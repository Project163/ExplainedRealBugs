diff --git a/clients/src/main/java/org/apache/kafka/common/utils/ThreadUtils.java b/clients/src/main/java/org/apache/kafka/common/utils/ThreadUtils.java
index 40ac9443cf..51cfe74fcd 100644
--- a/clients/src/main/java/org/apache/kafka/common/utils/ThreadUtils.java
+++ b/clients/src/main/java/org/apache/kafka/common/utils/ThreadUtils.java
@@ -70,6 +70,9 @@ public class ThreadUtils {
      */
     public static void shutdownExecutorServiceQuietly(ExecutorService executorService,
                                                       long timeout, TimeUnit timeUnit) {
+        if (executorService == null) {
+            return;
+        }
         executorService.shutdown(); // Disable new tasks from being submitted
         try {
             // Wait a while for existing tasks to terminate
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTask.java
index 4f9e0936ee..e5a05d975e 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTask.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTask.java
@@ -38,6 +38,7 @@ import org.apache.kafka.connect.header.Header;
 import org.apache.kafka.connect.header.Headers;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
 import org.apache.kafka.connect.runtime.errors.ErrorReporter;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.errors.Stage;
 import org.apache.kafka.connect.runtime.errors.ToleranceType;
@@ -157,6 +158,7 @@ public abstract class AbstractWorkerSourceTask extends WorkerTask {
 
     /**
      * Invoked when a record given to {@link Producer#send(ProducerRecord, Callback)} has failed with a non-retriable error.
+     * @param context the context for this record
      * @param synchronous whether the error occurred during the invocation of {@link Producer#send(ProducerRecord, Callback)}.
      *                    If {@code false}, indicates that the error was reported asynchronously by the producer by a {@link Callback}
      * @param producerRecord the {@link ProducerRecord} that the producer failed to send; never null
@@ -165,6 +167,7 @@ public abstract class AbstractWorkerSourceTask extends WorkerTask {
      *          via {@link Callback} after the call to {@link Producer#send(ProducerRecord, Callback)} completed
      */
     protected abstract void producerSendFailed(
+            ProcessingContext<SourceRecord> context,
             boolean synchronous,
             ProducerRecord<byte[], byte[]> producerRecord,
             SourceRecord preTransformRecord,
@@ -397,10 +400,10 @@ public abstract class AbstractWorkerSourceTask extends WorkerTask {
         final SourceRecordWriteCounter counter =
                 toSend.size() > 0 ? new SourceRecordWriteCounter(toSend.size(), sourceTaskMetricsGroup) : null;
         for (final SourceRecord preTransformRecord : toSend) {
-            retryWithToleranceOperator.sourceRecord(preTransformRecord);
-            final SourceRecord record = transformationChain.apply(preTransformRecord);
-            final ProducerRecord<byte[], byte[]> producerRecord = convertTransformedRecord(record);
-            if (producerRecord == null || retryWithToleranceOperator.failed()) {
+            ProcessingContext<SourceRecord> context = new ProcessingContext<>(preTransformRecord);
+            final SourceRecord record = transformationChain.apply(context, preTransformRecord);
+            final ProducerRecord<byte[], byte[]> producerRecord = convertTransformedRecord(context, record);
+            if (producerRecord == null || context.failed()) {
                 counter.skipRecord();
                 recordDropped(preTransformRecord);
                 processed++;
@@ -422,7 +425,7 @@ public abstract class AbstractWorkerSourceTask extends WorkerTask {
                                 log.error("{} failed to send record to {}: ", AbstractWorkerSourceTask.this, topic, e);
                             }
                             log.trace("{} Failed record: {}", AbstractWorkerSourceTask.this, preTransformRecord);
-                            producerSendFailed(false, producerRecord, preTransformRecord, e);
+                            producerSendFailed(context, false, producerRecord, preTransformRecord, e);
                             if (retryWithToleranceOperator.getErrorToleranceType() == ToleranceType.ALL) {
                                 counter.skipRecord();
                                 submittedRecord.ifPresent(SubmittedRecords.SubmittedRecord::ack);
@@ -454,7 +457,7 @@ public abstract class AbstractWorkerSourceTask extends WorkerTask {
                 log.trace("{} Failed to send {} with unrecoverable exception: ", this, producerRecord, e);
                 throw e;
             } catch (KafkaException e) {
-                producerSendFailed(true, producerRecord, preTransformRecord, e);
+                producerSendFailed(context, true, producerRecord, preTransformRecord, e);
             }
             processed++;
             recordDispatched(preTransformRecord);
@@ -481,20 +484,20 @@ public abstract class AbstractWorkerSourceTask extends WorkerTask {
      * @return the producer record which can sent over to Kafka. A null is returned if the input is null or
      * if an error was encountered during any of the converter stages.
      */
-    protected ProducerRecord<byte[], byte[]> convertTransformedRecord(SourceRecord record) {
+    protected ProducerRecord<byte[], byte[]> convertTransformedRecord(ProcessingContext<SourceRecord> context, SourceRecord record) {
         if (record == null) {
             return null;
         }
 
-        RecordHeaders headers = retryWithToleranceOperator.execute(() -> convertHeaderFor(record), Stage.HEADER_CONVERTER, headerConverter.getClass());
+        RecordHeaders headers = retryWithToleranceOperator.execute(context, () -> convertHeaderFor(record), Stage.HEADER_CONVERTER, headerConverter.getClass());
 
-        byte[] key = retryWithToleranceOperator.execute(() -> keyConverter.fromConnectData(record.topic(), headers, record.keySchema(), record.key()),
+        byte[] key = retryWithToleranceOperator.execute(context, () -> keyConverter.fromConnectData(record.topic(), headers, record.keySchema(), record.key()),
                 Stage.KEY_CONVERTER, keyConverter.getClass());
 
-        byte[] value = retryWithToleranceOperator.execute(() -> valueConverter.fromConnectData(record.topic(), headers, record.valueSchema(), record.value()),
+        byte[] value = retryWithToleranceOperator.execute(context, () -> valueConverter.fromConnectData(record.topic(), headers, record.valueSchema(), record.value()),
                 Stage.VALUE_CONVERTER, valueConverter.getClass());
 
-        if (retryWithToleranceOperator.failed()) {
+        if (context.failed()) {
             return null;
         }
 
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTask.java
index 5a123f2cd1..14be0b1ba3 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTask.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTask.java
@@ -29,6 +29,7 @@ import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
 import org.apache.kafka.connect.runtime.errors.ErrorReporter;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.apache.kafka.connect.source.SourceTask;
@@ -203,6 +204,7 @@ class ExactlyOnceWorkerSourceTask extends AbstractWorkerSourceTask {
 
     @Override
     protected void producerSendFailed(
+            ProcessingContext<SourceRecord> context,
             boolean synchronous,
             ProducerRecord<byte[], byte[]> producerRecord,
             SourceRecord preTransformRecord,
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java
index c35c3d3de4..a6b23d8abd 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java
@@ -20,6 +20,7 @@ import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.common.record.TimestampType;
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.header.Header;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.sink.SinkRecord;
 
 /**
@@ -29,30 +30,31 @@ import org.apache.kafka.connect.sink.SinkRecord;
  */
 public class InternalSinkRecord extends SinkRecord {
 
-    private final ConsumerRecord<byte[], byte[]> originalRecord;
+    private final ProcessingContext<ConsumerRecord<byte[], byte[]>> context;
 
-    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {
+    public InternalSinkRecord(ProcessingContext<ConsumerRecord<byte[], byte[]>> context, SinkRecord record) {
         super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),
                 record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),
-                record.timestampType(), record.headers(), originalRecord.topic(), originalRecord.partition(),
-                originalRecord.offset());
-        this.originalRecord = originalRecord;
+                record.timestampType(), record.headers(), context.original().topic(),
+                context.original().partition(), context.original().offset());
+        this.context = context;
     }
 
-    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,
+    protected InternalSinkRecord(ProcessingContext<ConsumerRecord<byte[], byte[]>> context, String topic,
                                  int partition, Schema keySchema, Object key, Schema valueSchema,
                                  Object value, long kafkaOffset, Long timestamp,
                                  TimestampType timestampType, Iterable<Header> headers) {
         super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers,
-                originalRecord.topic(), originalRecord.partition(), originalRecord.offset());
-        this.originalRecord = originalRecord;
+                context.original().topic(), context.original().partition(),
+                context.original().offset());
+        this.context = context;
     }
 
     @Override
     public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key,
                                 Schema valueSchema, Object value, Long timestamp,
                                 Iterable<Header> headers) {
-        return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,
+        return new InternalSinkRecord(context, topic, kafkaPartition, keySchema, key,
                 valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers);
     }
 
@@ -67,12 +69,12 @@ public class InternalSinkRecord extends SinkRecord {
     }
 
     /**
-    * Return the original consumer record that this sink record represents.
-    *
-    * @return the original consumer record; never null
-    */
-    public ConsumerRecord<byte[], byte[]> originalRecord() {
-        return originalRecord;
+     * Return the context used to process this record
+     *
+     * @return the processing context; never null
+     */
+    public ProcessingContext<ConsumerRecord<byte[], byte[]>> context() {
+        return context;
     }
 }
 
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TransformationChain.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TransformationChain.java
index 435d2d802e..b0f10fe39e 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TransformationChain.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TransformationChain.java
@@ -17,6 +17,7 @@
 package org.apache.kafka.connect.runtime;
 
 import org.apache.kafka.connect.connector.ConnectRecord;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.errors.Stage;
 import org.apache.kafka.connect.transforms.Transformation;
@@ -42,7 +43,7 @@ public class TransformationChain<R extends ConnectRecord<R>> implements AutoClos
         this.retryWithToleranceOperator = retryWithToleranceOperator;
     }
 
-    public R apply(R record) {
+    public R apply(ProcessingContext<?> context, R record) {
         if (transformationStages.isEmpty()) return record;
 
         for (final TransformationStage<R> transformationStage : transformationStages) {
@@ -51,7 +52,7 @@ public class TransformationChain<R extends ConnectRecord<R>> implements AutoClos
             log.trace("Applying transformation {} to {}",
                 transformationStage.transformClass().getName(), record);
             // execute the operation
-            record = retryWithToleranceOperator.execute(() -> transformationStage.apply(current), Stage.TRANSFORMATION, transformationStage.transformClass());
+            record = retryWithToleranceOperator.execute(context, () -> transformationStage.apply(current), Stage.TRANSFORMATION, transformationStage.transformClass());
 
             if (record == null) break;
         }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java
index 0f2e8f6eb2..165d065f2b 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java
@@ -42,6 +42,7 @@ import org.apache.kafka.connect.header.Headers;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
 import org.apache.kafka.connect.runtime.errors.ErrorReporter;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.errors.Stage;
 import org.apache.kafka.connect.runtime.errors.WorkerErrantRecordReporter;
@@ -313,7 +314,6 @@ class WorkerSinkTask extends WorkerTask {
     protected void initializeAndStart() {
         SinkConnectorConfig.validate(taskConfig);
         retryWithToleranceOperator.reporters(errorReportersSupplier.get());
-
         if (SinkConnectorConfig.hasTopicsConfig(taskConfig)) {
             List<String> topics = SinkConnectorConfig.parseTopicsList(taskConfig);
             consumer.subscribe(topics, new HandleRebalance());
@@ -508,9 +508,9 @@ class WorkerSinkTask extends WorkerTask {
             log.trace("{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}",
                     this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp());
 
-            retryWithToleranceOperator.consumerRecord(msg);
+            ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(msg);
 
-            SinkRecord transRecord = convertAndTransformRecord(msg);
+            SinkRecord transRecord = convertAndTransformRecord(context, msg);
 
             origOffsets.put(
                     new TopicPartition(msg.topic(), msg.partition()),
@@ -529,16 +529,16 @@ class WorkerSinkTask extends WorkerTask {
         sinkTaskMetricsGroup.recordConsumedOffsets(origOffsets);
     }
 
-    private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg) {
-        SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> keyConverter.toConnectData(msg.topic(), msg.headers(), msg.key()),
+    private SinkRecord convertAndTransformRecord(ProcessingContext<ConsumerRecord<byte[], byte[]>> context, final ConsumerRecord<byte[], byte[]> msg) {
+        SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(context, () -> keyConverter.toConnectData(msg.topic(), msg.headers(), msg.key()),
                 Stage.KEY_CONVERTER, keyConverter.getClass());
 
-        SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> valueConverter.toConnectData(msg.topic(), msg.headers(), msg.value()),
+        SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(context, () -> valueConverter.toConnectData(msg.topic(), msg.headers(), msg.value()),
                 Stage.VALUE_CONVERTER, valueConverter.getClass());
 
-        Headers headers = retryWithToleranceOperator.execute(() -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());
+        Headers headers = retryWithToleranceOperator.execute(context, () -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());
 
-        if (retryWithToleranceOperator.failed()) {
+        if (context.failed()) {
             return null;
         }
 
@@ -557,12 +557,12 @@ class WorkerSinkTask extends WorkerTask {
         }
 
         // Apply the transformations
-        SinkRecord transformedRecord = transformationChain.apply(origRecord);
+        SinkRecord transformedRecord = transformationChain.apply(context, origRecord);
         if (transformedRecord == null) {
             return null;
         }
         // Error reporting will need to correlate each sink record with the original consumer record
-        return new InternalSinkRecord(msg, transformedRecord);
+        return new InternalSinkRecord(context, transformedRecord);
     }
 
     private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {
@@ -601,9 +601,8 @@ class WorkerSinkTask extends WorkerTask {
             task.put(new ArrayList<>(messageBatch));
             // if errors raised from the operator were swallowed by the task implementation, an
             // exception needs to be thrown to kill the task indicating the tolerance was exceeded
-            if (retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {
-                throw new ConnectException("Tolerance exceeded in error handler",
-                    retryWithToleranceOperator.error());
+            if (workerErrantRecordReporter != null) {
+                workerErrantRecordReporter.maybeThrowAsyncError();
             }
             recordBatch(messageBatch.size());
             sinkTaskMetricsGroup.recordPut(time.milliseconds() - start);
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java
index e1ea9efa9b..deafd40ecc 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java
@@ -22,6 +22,7 @@ import org.apache.kafka.clients.producer.RecordMetadata;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.runtime.errors.ErrorReporter;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.storage.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
@@ -155,6 +156,7 @@ class WorkerSourceTask extends AbstractWorkerSourceTask {
 
     @Override
     protected void producerSendFailed(
+            ProcessingContext<SourceRecord> context,
             boolean synchronous,
             ProducerRecord<byte[], byte[]> producerRecord,
             SourceRecord preTransformRecord,
@@ -174,9 +176,9 @@ class WorkerSourceTask extends AbstractWorkerSourceTask {
             );
             // executeFailed here allows the use of existing logging infrastructure/configuration
             retryWithToleranceOperator.executeFailed(
+                    context,
                     Stage.KAFKA_PRODUCE,
                     WorkerSourceTask.class,
-                    preTransformRecord,
                     e
             );
             commitTaskRecord(preTransformRecord, null);
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java
index 4781136c2f..51ca9a4bb5 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java
@@ -121,20 +121,22 @@ public class DeadLetterQueueReporter implements ErrorReporter {
     /**
      * Write the raw records into a Kafka topic and return the producer future.
      *
-     * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.
+     * @param context processing context containing the raw record at {@link ProcessingContext#original()}.
      * @return the future associated with the writing of this record; never null
      */
-    public Future<RecordMetadata> report(ProcessingContext context) {
+    @SuppressWarnings("unchecked")
+    public Future<RecordMetadata> report(ProcessingContext<?> context) {
         if (dlqTopicName.isEmpty()) {
             return CompletableFuture.completedFuture(null);
         }
         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();
 
-        ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();
-        if (originalMessage == null) {
+        if (!(context.original() instanceof ConsumerRecord)) {
             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();
             return CompletableFuture.completedFuture(null);
         }
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> sinkContext = (ProcessingContext<ConsumerRecord<byte[], byte[]>>) context;
+        ConsumerRecord<byte[], byte[]> originalMessage = sinkContext.original();
 
         ProducerRecord<byte[], byte[]> producerRecord;
         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {
@@ -146,7 +148,7 @@ public class DeadLetterQueueReporter implements ErrorReporter {
         }
 
         if (connConfig.isDlqContextHeadersEnabled()) {
-            populateContextHeaders(producerRecord, context);
+            populateContextHeaders(producerRecord, sinkContext);
         }
 
         return this.kafkaProducer.send(producerRecord, (metadata, exception) -> {
@@ -158,12 +160,12 @@ public class DeadLetterQueueReporter implements ErrorReporter {
     }
 
     // Visible for testing
-    void populateContextHeaders(ProducerRecord<byte[], byte[]> producerRecord, ProcessingContext context) {
+    void populateContextHeaders(ProducerRecord<byte[], byte[]> producerRecord, ProcessingContext<ConsumerRecord<byte[], byte[]>> context) {
         Headers headers = producerRecord.headers();
-        if (context.consumerRecord() != null) {
-            headers.add(ERROR_HEADER_ORIG_TOPIC, toBytes(context.consumerRecord().topic()));
-            headers.add(ERROR_HEADER_ORIG_PARTITION, toBytes(context.consumerRecord().partition()));
-            headers.add(ERROR_HEADER_ORIG_OFFSET, toBytes(context.consumerRecord().offset()));
+        if (context.original() != null) {
+            headers.add(ERROR_HEADER_ORIG_TOPIC, toBytes(context.original().topic()));
+            headers.add(ERROR_HEADER_ORIG_PARTITION, toBytes(context.original().partition()));
+            headers.add(ERROR_HEADER_ORIG_OFFSET, toBytes(context.original().offset()));
         }
 
         headers.add(ERROR_HEADER_CONNECTOR_NAME, toBytes(connectorTaskId.connector()));
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java
index f9bc2f2360..d890426681 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java
@@ -31,7 +31,7 @@ public interface ErrorReporter extends AutoCloseable {
      * @param context the processing context (cannot be null).
      * @return future result from the producer sending a record to Kafka.
      */
-    Future<RecordMetadata> report(ProcessingContext context);
+    Future<RecordMetadata> report(ProcessingContext<?> context);
 
     @Override
     default void close() { }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java
index 63793975ff..fc71fcc71c 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java
@@ -54,7 +54,7 @@ public class LogReporter implements ErrorReporter {
      * @param context the processing context.
      */
     @Override
-    public Future<RecordMetadata> report(ProcessingContext context) {
+    public Future<RecordMetadata> report(ProcessingContext<?> context) {
         if (!connConfig.enableErrorLog()) {
             return COMPLETED;
         }
@@ -69,7 +69,7 @@ public class LogReporter implements ErrorReporter {
     }
 
     // Visible for testing
-    String message(ProcessingContext context) {
+    String message(ProcessingContext<?> context) {
         return String.format("Error encountered in task %s. %s", id,
                 context.toString(connConfig.includeRecordDetailsInErrorLog()));
     }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java
index b49c93c053..05f4a94f94 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java
@@ -17,34 +17,17 @@
 package org.apache.kafka.connect.runtime.errors;
 
 import org.apache.kafka.clients.consumer.ConsumerRecord;
-import org.apache.kafka.clients.producer.RecordMetadata;
 import org.apache.kafka.common.record.TimestampType;
-import org.apache.kafka.connect.errors.ConnectException;
-import org.apache.kafka.connect.runtime.errors.WorkerErrantRecordReporter.ErrantRecordFuture;
 import org.apache.kafka.connect.source.SourceRecord;
 
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.Future;
-import java.util.stream.Collectors;
-
 /**
- * Contains all the metadata related to the currently evaluating operation. Only one instance of this class is meant
- * to exist per task in a JVM.
+ * Contains all the metadata related to the currently evaluating operation, and associated with a particular
+ * sink or source record from the consumer or task, respectively. This class is not thread safe, and so once an
+ * instance is passed to a new thread, it should no longer be accessed by the previous thread.
  */
-class ProcessingContext implements AutoCloseable {
-
-    private Collection<ErrorReporter> reporters = Collections.emptyList();
-
-    private ConsumerRecord<byte[], byte[]> consumedMessage;
-    private SourceRecord sourceRecord;
+public class ProcessingContext<T> {
 
-    /**
-     * The following fields need to be reset every time a new record is seen.
-     */
+    private final T original;
 
     private Stage position;
     private Class<?> klass;
@@ -52,47 +35,18 @@ class ProcessingContext implements AutoCloseable {
     private Throwable error;
 
     /**
-     * Reset the internal fields before executing operations on a new record.
-     */
-    private void reset() {
-        attempt = 0;
-        position = null;
-        klass = null;
-        error = null;
-    }
-
-    /**
-     * Set the record consumed from Kafka in a sink connector.
-     *
-     * @param consumedMessage the record
+     * Construct a context associated with the processing of a particular record
+     * @param original The original record before processing, as received from either Kafka or a Source Task
      */
-    public void consumerRecord(ConsumerRecord<byte[], byte[]> consumedMessage) {
-        this.consumedMessage = consumedMessage;
-        reset();
+    public ProcessingContext(T original) {
+        this.original = original;
     }
 
     /**
-     * @return the record consumed from Kafka. could be null
-     */
-    public ConsumerRecord<byte[], byte[]> consumerRecord() {
-        return consumedMessage;
-    }
-
-    /**
-     * @return the source record being processed.
-     */
-    public SourceRecord sourceRecord() {
-        return sourceRecord;
-    }
-
-    /**
-     * Set the source record being processed in the connect pipeline.
-     *
-     * @param record the source record
+     * @return The original record before processing, as received from either Kafka or a Source Task
      */
-    public void sourceRecord(SourceRecord record) {
-        this.sourceRecord = record;
-        reset();
+    public T original() {
+        return original;
     }
 
     /**
@@ -136,26 +90,6 @@ class ProcessingContext implements AutoCloseable {
         executingClass(klass);
     }
 
-    /**
-     * Report errors. Should be called only if an error was encountered while executing the operation.
-     *
-     * @return a errant record future that potentially aggregates the producer futures
-     */
-    public Future<Void> report() {
-        if (reporters.size() == 1) {
-            return new ErrantRecordFuture(Collections.singletonList(reporters.iterator().next().report(this)));
-        }
-
-        List<Future<RecordMetadata>> futures = reporters.stream()
-                .map(r -> r.report(this))
-                .filter(f -> !f.isDone())
-                .collect(Collectors.toList());
-        if (futures.isEmpty()) {
-            return CompletableFuture.completedFuture(null);
-        }
-        return new ErrantRecordFuture(futures);
-    }
-
     @Override
     public String toString() {
         return toString(false);
@@ -168,11 +102,13 @@ class ProcessingContext implements AutoCloseable {
         builder.append("' with class '");
         builder.append(executingClass() == null ? "null" : executingClass().getName());
         builder.append('\'');
-        if (includeMessage && sourceRecord() != null) {
+        T original = original();
+        if (includeMessage && original instanceof SourceRecord) {
             builder.append(", where source record is = ");
-            builder.append(sourceRecord());
-        } else if (includeMessage && consumerRecord() != null) {
-            ConsumerRecord<byte[], byte[]> msg = consumerRecord();
+            builder.append(original);
+        } else if (includeMessage && original instanceof ConsumerRecord) {
+            @SuppressWarnings("unchecked")
+            ConsumerRecord<byte[], byte[]> msg = (ConsumerRecord<byte[], byte[]>) original;
             builder.append(", where consumed record is ");
             builder.append("{topic='").append(msg.topic()).append('\'');
             builder.append(", partition=").append(msg.partition());
@@ -223,30 +159,4 @@ class ProcessingContext implements AutoCloseable {
     public boolean failed() {
         return error() != null;
     }
-
-    /**
-     * Set the error reporters for this connector.
-     *
-     * @param reporters the error reporters (should not be null).
-     */
-    public void reporters(Collection<ErrorReporter> reporters) {
-        Objects.requireNonNull(reporters);
-        this.reporters = reporters;
-    }
-
-    @Override
-    public void close() {
-        ConnectException e = null;
-        for (ErrorReporter reporter : reporters) {
-            try {
-                reporter.close();
-            } catch (Throwable t) {
-                e = e != null ? e : new ConnectException("Failed to close all reporters");
-                e.addSuppressed(t);
-            }
-        }
-        if (e != null) {
-            throw e;
-        }
-    }
 }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java
index 6262f19e19..ee13ad015d 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java
@@ -16,23 +16,26 @@
  */
 package org.apache.kafka.connect.runtime.errors;
 
-import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.producer.RecordMetadata;
 import org.apache.kafka.common.config.ConfigException;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.errors.RetriableException;
 import org.apache.kafka.connect.runtime.ConnectorConfig;
-import org.apache.kafka.connect.source.SourceRecord;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Future;
 import java.util.concurrent.ThreadLocalRandom;
 import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
 
 /**
  * Attempt to recover a failed operation with retries and tolerance limits.
@@ -80,37 +83,44 @@ public class RetryWithToleranceOperator implements AutoCloseable {
     private final ErrorHandlingMetrics errorHandlingMetrics;
     private final CountDownLatch stopRequestedLatch;
     private volatile boolean stopping;   // indicates whether the operator has been asked to stop retrying
-
-    protected final ProcessingContext context;
+    private List<ErrorReporter> reporters;
 
     public RetryWithToleranceOperator(long errorRetryTimeout, long errorMaxDelayInMillis,
                                       ToleranceType toleranceType, Time time, ErrorHandlingMetrics errorHandlingMetrics) {
-        this(errorRetryTimeout, errorMaxDelayInMillis, toleranceType, time, errorHandlingMetrics, new ProcessingContext(), new CountDownLatch(1));
+        this(errorRetryTimeout, errorMaxDelayInMillis, toleranceType, time, errorHandlingMetrics, new CountDownLatch(1));
     }
 
     RetryWithToleranceOperator(long errorRetryTimeout, long errorMaxDelayInMillis,
                                ToleranceType toleranceType, Time time, ErrorHandlingMetrics errorHandlingMetrics,
-                               ProcessingContext context, CountDownLatch stopRequestedLatch) {
+                               CountDownLatch stopRequestedLatch) {
         this.errorRetryTimeout = errorRetryTimeout;
         this.errorMaxDelayInMillis = errorMaxDelayInMillis;
         this.errorToleranceType = toleranceType;
         this.time = time;
         this.errorHandlingMetrics = errorHandlingMetrics;
-        this.context = context;
         this.stopRequestedLatch = stopRequestedLatch;
         this.stopping = false;
+        this.reporters = Collections.emptyList();
     }
 
-    public synchronized Future<Void> executeFailed(Stage stage, Class<?> executingClass,
-                                      ConsumerRecord<byte[], byte[]> consumerRecord,
-                                      Throwable error) {
-
+    /**
+     * Inform this class that some external operation has already failed. This is used when the control flow does not
+     * allow for the operation to be started and stopped within the scope of a single {@link Operation}, and the
+     * {@link #execute(ProcessingContext, Operation, Stage, Class)} method cannot be used.
+     *
+     * @param context The {@link ProcessingContext} used to hold state about this operation
+     * @param stage The logical stage within the overall pipeline of the operation that has failed
+     * @param executingClass The class containing the operation implementation that failed
+     * @param error The error which caused the operation to fail
+     * @return A future which resolves when this failure has been persisted by all {@link ErrorReporter} instances
+     * @throws ConnectException if the operation is not tolerated, and the overall pipeline should stop
+     */
+    public Future<Void> executeFailed(ProcessingContext<?> context, Stage stage, Class<?> executingClass, Throwable error) {
         markAsFailed();
-        context.consumerRecord(consumerRecord);
         context.currentContext(stage, executingClass);
         context.error(error);
         errorHandlingMetrics.recordFailure();
-        Future<Void> errantRecordFuture = context.report();
+        Future<Void> errantRecordFuture = report(context);
         if (!withinToleranceLimits()) {
             errorHandlingMetrics.recordError();
             throw new ConnectException("Tolerance exceeded in error handler", error);
@@ -118,59 +128,75 @@ public class RetryWithToleranceOperator implements AutoCloseable {
         return errantRecordFuture;
     }
 
-    public synchronized Future<Void> executeFailed(Stage stage, Class<?> executingClass,
-                                                   SourceRecord sourceRecord,
-                                                   Throwable error) {
-
-        markAsFailed();
-        context.sourceRecord(sourceRecord);
-        context.currentContext(stage, executingClass);
-        context.error(error);
-        errorHandlingMetrics.recordFailure();
-        Future<Void> errantRecordFuture = context.report();
-        if (!withinToleranceLimits()) {
-            errorHandlingMetrics.recordError();
-            throw new ConnectException("Tolerance exceeded in Source Worker error handler", error);
+    /**
+     * Report an error to all configured {@link ErrorReporter} instances.
+     * @param context The context containing details of the error to report
+     * @return A future which resolves when this failure has been persisted by all {@link ErrorReporter} instances
+     */
+    // Visible for testing
+    synchronized Future<Void> report(ProcessingContext<?> context) {
+        if (reporters.size() == 1) {
+            return new WorkerErrantRecordReporter.ErrantRecordFuture(Collections.singletonList(reporters.iterator().next().report(context)));
         }
-        return errantRecordFuture;
+        List<Future<RecordMetadata>> futures = reporters.stream()
+                .map(r -> r.report(context))
+                .filter(f -> !f.isDone())
+                .collect(Collectors.toList());
+        if (futures.isEmpty()) {
+            return CompletableFuture.completedFuture(null);
+        }
+        return new WorkerErrantRecordReporter.ErrantRecordFuture(futures);
     }
 
     /**
-     * Execute the recoverable operation. If the operation is already in a failed state, then simply return
-     * with the existing failure.
+     * Attempt to execute an operation. Handles retriable and tolerated exceptions thrown by the operation. This is
+     * used for small blocking operations which can be represented as an {@link Operation}. For operations which do
+     * not fit this interface, see {@link #executeFailed(ProcessingContext, Stage, Class, Throwable)}
+     *
+     * <p>If any error is already present in the context, return null without modifying the context.
+     * <p>Retries are allowed if the operator is still running, and this operation is within the error retry timeout.
+     * <p>Tolerable exceptions are different for each stage, and encoded in {@link #TOLERABLE_EXCEPTIONS}
+     * <p>This method mutates the passed-in {@link ProcessingContext} with the number of attempts made to execute the
+     * operation, and the last error encountered if no attempt was successful.
      *
+     * @param context The {@link ProcessingContext} used to hold state about this operation
      * @param operation the recoverable operation
+     * @param stage The logical stage within the overall pipeline of the operation that has failed
+     * @param executingClass The class containing the operation implementation that failed
      * @param <V> return type of the result of the operation.
-     * @return result of the operation
+     * @return result of the operation, or null if a prior exception occurred, or the operation only threw retriable or tolerable exceptions
+     * @throws ConnectException wrapper if any non-tolerated exception was thrown by the operation
      */
-    public synchronized <V> V execute(Operation<V> operation, Stage stage, Class<?> executingClass) {
-        context.currentContext(stage, executingClass);
-
+    public <V> V execute(ProcessingContext<?> context, Operation<V> operation, Stage stage, Class<?> executingClass) {
         if (context.failed()) {
             log.debug("ProcessingContext is already in failed state. Ignoring requested operation.");
             return null;
         }
-
+        context.currentContext(stage, executingClass);
         try {
             Class<? extends Exception> ex = TOLERABLE_EXCEPTIONS.getOrDefault(context.stage(), RetriableException.class);
-            return execAndHandleError(operation, ex);
+            return execAndHandleError(context, operation, ex);
         } finally {
             if (context.failed()) {
                 errorHandlingMetrics.recordError();
-                context.report();
+                report(context);
             }
         }
     }
 
     /**
-     * Attempt to execute an operation. Retry if a {@link RetriableException} is raised. Re-throw everything else.
+     * Attempt to execute an operation. Handles retriable exceptions raised by the operation.
+     * <p>Retries are allowed if the operator is still running, and this operation is within the error retry timeout.
+     * <p>This method mutates the passed-in {@link ProcessingContext} with the number of attempts made to execute the
+     * operation, and the last error encountered if no attempt was successful.
      *
+     * @param context The {@link ProcessingContext} used to hold state about this operation
      * @param operation the operation to be executed.
      * @param <V> the return type of the result of the operation.
-     * @return the result of the operation.
-     * @throws Exception rethrow if a non-retriable Exception is thrown by the operation
+     * @return the result of the operation if it succeeded, or null if the operation only threw retriable exceptions
+     * @throws Exception rethrow if any non-retriable exception was thrown by the operation
      */
-    protected <V> V execAndRetry(Operation<V> operation) throws Exception {
+    protected <V> V execAndRetry(ProcessingContext<?> context, Operation<V> operation) throws Exception {
         int attempt = 0;
         long startTime = time.milliseconds();
         long deadline = (errorRetryTimeout >= 0) ? startTime + errorRetryTimeout : Long.MAX_VALUE;
@@ -201,17 +227,20 @@ public class RetryWithToleranceOperator implements AutoCloseable {
     }
 
     /**
-     * Execute a given operation multiple times (if needed), and tolerate certain exceptions.
-     * Visible for testing.
+     * Attempt to execute an operation. Handles retriable and tolerated exceptions thrown by the operation.
+     * <p>This method mutates the passed-in {@link ProcessingContext} with the number of attempts made to execute the
+     * operation, and the last error encountered if no attempt was successful.
      *
      * @param operation the operation to be executed.
-     * @param tolerated the class of exceptions which can be tolerated.
+     * @param tolerated the class of exceptions which can be tolerated if errors.tolerance=all
      * @param <V> The return type of the result of the operation.
-     * @return the result of the operation
+     * @return the result of the operation, or null if the operation only threw retriable or tolerable exceptions
+     * @throws ConnectException wrapper if any non-tolerated exception was thrown by the operation
      */
-    protected <V> V execAndHandleError(Operation<V> operation, Class<? extends Exception> tolerated) {
+    // Visible for testing
+    protected <V> V execAndHandleError(ProcessingContext<?> context, Operation<V> operation, Class<? extends Exception> tolerated) {
         try {
-            V result = execAndRetry(operation);
+            V result = execAndRetry(context, operation);
             if (context.failed()) {
                 markAsFailed();
                 errorHandlingMetrics.recordSkipped();
@@ -236,7 +265,7 @@ public class RetryWithToleranceOperator implements AutoCloseable {
     }
 
     // Visible for testing
-    void markAsFailed() {
+    synchronized void markAsFailed() {
         errorHandlingMetrics.recordErrorTimestamp();
         totalFailures++;
     }
@@ -293,7 +322,6 @@ public class RetryWithToleranceOperator implements AutoCloseable {
                 ", errorToleranceType=" + errorToleranceType +
                 ", totalFailures=" + totalFailures +
                 ", time=" + time +
-                ", context=" + context +
                 '}';
     }
 
@@ -303,48 +331,14 @@ public class RetryWithToleranceOperator implements AutoCloseable {
      * @param reporters the error reporters (should not be null).
      */
     public synchronized void reporters(List<ErrorReporter> reporters) {
-        this.context.reporters(reporters);
-    }
-
-    /**
-     * Set the source record being processed in the connect pipeline.
-     *
-     * @param preTransformRecord the source record
-     */
-    public synchronized void sourceRecord(SourceRecord preTransformRecord) {
-        this.context.sourceRecord(preTransformRecord);
-    }
-
-    /**
-     * Set the record consumed from Kafka in a sink connector.
-     *
-     * @param consumedMessage the record
-     */
-    public synchronized void consumerRecord(ConsumerRecord<byte[], byte[]> consumedMessage) {
-        this.context.consumerRecord(consumedMessage);
-    }
-
-    /**
-     * @return true, if the last operation encountered an error; false otherwise
-     */
-    public synchronized boolean failed() {
-        return this.context.failed();
-    }
-
-    /**
-     * Returns the error encountered when processing the current stage.
-     *
-     * @return the error encountered when processing the current stage
-     */
-    public synchronized Throwable error() {
-        return this.context.error();
+        this.reporters = Objects.requireNonNull(reporters, "reporters");
     }
 
     /**
      * This will stop any further retries for operations.
      * This will also mark any ongoing operations that are currently backing off for retry as failed.
      * This can be called from a separate thread to break out of retry/backoff loops in
-     * {@link #execAndRetry(Operation)}
+     * {@link #execAndRetry(ProcessingContext, Operation)}
      */
     public void triggerStop() {
         stopping = true;
@@ -353,6 +347,18 @@ public class RetryWithToleranceOperator implements AutoCloseable {
 
     @Override
     public synchronized void close() {
-        this.context.close();
+        ConnectException e = null;
+        for (ErrorReporter reporter : reporters) {
+            try {
+                reporter.close();
+            } catch (Throwable t) {
+                e = e != null ? e : new ConnectException("Failed to close all reporters");
+                e.addSuppressed(t);
+            }
+        }
+        reporters = Collections.emptyList();
+        if (e != null) {
+            throw e;
+        }
     }
 }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java
index ed48f79158..4edc93a4ad 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java
@@ -43,6 +43,7 @@ import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicReference;
 import java.util.stream.Collectors;
 
 public class WorkerErrantRecordReporter implements ErrantRecordReporter {
@@ -56,6 +57,7 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {
 
     // Visible for testing
     protected final ConcurrentMap<TopicPartition, List<Future<Void>>> futures;
+    private final AtomicReference<Throwable> taskPutException;
 
     public WorkerErrantRecordReporter(
         RetryWithToleranceOperator retryWithToleranceOperator,
@@ -68,16 +70,17 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {
         this.valueConverter = valueConverter;
         this.headerConverter = headerConverter;
         this.futures = new ConcurrentHashMap<>();
+        this.taskPutException = new AtomicReference<>();
     }
 
     @Override
     public Future<Void> report(SinkRecord record, Throwable error) {
-        ConsumerRecord<byte[], byte[]> consumerRecord;
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context;
 
         // Most of the records will be an internal sink record, but the task could potentially
         // report modified or new records, so handle both cases
         if (record instanceof InternalSinkRecord) {
-            consumerRecord = ((InternalSinkRecord) record).originalRecord();
+            context = ((InternalSinkRecord) record).context();
         } else {
             // Generate a new consumer record from the modified sink record. We prefer
             // to send the original consumer record (pre-transformed) to the DLQ,
@@ -101,15 +104,17 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {
             int keyLength = key != null ? key.length : -1;
             int valLength = value != null ? value.length : -1;
 
-            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),
+            ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),
                 record.kafkaOffset(), record.timestamp(), record.timestampType(), keyLength,
                 valLength, key, value, headers, Optional.empty());
+            context = new ProcessingContext<>(consumerRecord);
         }
 
-        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT, SinkTask.class, consumerRecord, error);
+        Future<Void> future = retryWithToleranceOperator.executeFailed(context, Stage.TASK_PUT, SinkTask.class, error);
+        taskPutException.compareAndSet(null, error);
 
         if (!future.isDone()) {
-            TopicPartition partition = new TopicPartition(consumerRecord.topic(), consumerRecord.partition());
+            TopicPartition partition = new TopicPartition(context.original().topic(), context.original().partition());
             futures.computeIfAbsent(partition, p -> new ArrayList<>()).add(future);
         }
         return future;
@@ -154,6 +159,12 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {
                 .collect(Collectors.toList());
     }
 
+    public synchronized void maybeThrowAsyncError() {
+        if (taskPutException.get() != null && !retryWithToleranceOperator.withinToleranceLimits()) {
+            throw new ConnectException("Tolerance exceeded in error handler", taskPutException.get());
+        }
+    }
+
     /**
      * Wrapper class to aggregate producer futures and abstract away the record metadata from the
      * Connect user.
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java
index 251c67cef6..bcac3505f5 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java
@@ -18,6 +18,7 @@
 package org.apache.kafka.connect.integration;
 
 import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.utils.ThreadUtils;
 import org.apache.kafka.connect.connector.Task;
 import org.apache.kafka.connect.sink.ErrantRecordReporter;
 import org.apache.kafka.connect.sink.SinkRecord;
@@ -25,6 +26,9 @@ import org.apache.kafka.connect.sink.SinkRecord;
 import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
 
 public class ErrantRecordSinkConnector extends MonitorableSinkConnector {
 
@@ -35,6 +39,7 @@ public class ErrantRecordSinkConnector extends MonitorableSinkConnector {
 
     public static class ErrantRecordSinkTask extends MonitorableSinkTask {
         private ErrantRecordReporter reporter;
+        private ExecutorService executorService;
 
         public ErrantRecordSinkTask() {
             super();
@@ -44,6 +49,12 @@ public class ErrantRecordSinkConnector extends MonitorableSinkConnector {
         public void start(Map<String, String> props) {
             super.start(props);
             reporter = context.errantRecordReporter();
+            executorService = Executors.newSingleThreadExecutor();
+        }
+
+        @Override
+        public void stop() {
+            ThreadUtils.shutdownExecutorServiceQuietly(executorService, 4, TimeUnit.SECONDS);
         }
 
         @Override
@@ -54,7 +65,16 @@ public class ErrantRecordSinkConnector extends MonitorableSinkConnector {
                     .computeIfAbsent(rec.topic(), v -> new HashMap<>())
                     .computeIfAbsent(rec.kafkaPartition(), v -> new TopicPartition(rec.topic(), rec.kafkaPartition()));
                 committedOffsets.put(tp, committedOffsets.getOrDefault(tp, 0) + 1);
-                reporter.report(rec, new Throwable());
+                Throwable error = new Throwable();
+                // Test synchronous and asynchronous reporting, allowing for re-ordering the errant reports
+                if (rec.originalKafkaOffset() % 2 == 0) {
+                    reporter.report(rec, error);
+                } else {
+                    executorService.submit(() -> {
+                        Thread.yield();
+                        reporter.report(rec, error);
+                    });
+                }
             }
         }
     }
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrorHandlingIntegrationTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrorHandlingIntegrationTest.java
index 55479e6d4f..1a76956be6 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrorHandlingIntegrationTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrorHandlingIntegrationTest.java
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.connect.integration;
 
+import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.consumer.ConsumerRecords;
 import org.apache.kafka.common.config.ConfigDef;
@@ -38,10 +39,13 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.TimeUnit;
 
 import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX;
 import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG;
 import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG;
 import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_RETRY_TIMEOUT_CONFIG;
@@ -75,9 +79,7 @@ public class ErrorHandlingIntegrationTest {
     private static final String DLQ_TOPIC = "my-connector-errors";
     private static final String CONNECTOR_NAME = "error-conn";
     private static final String TASK_ID = "error-conn-0";
-    private static final int NUM_RECORDS_PRODUCED = 20;
-    private static final int EXPECTED_CORRECT_RECORDS = 19;
-    private static final int EXPECTED_INCORRECT_RECORDS = 1;
+    private static final int NUM_RECORDS_PRODUCED = 1000;
     private static final int NUM_TASKS = 1;
     private static final long CONNECTOR_SETUP_DURATION_MS = TimeUnit.SECONDS.toMillis(60);
     private static final long CONSUME_MAX_DURATION_MS = TimeUnit.SECONDS.toMillis(30);
@@ -136,7 +138,7 @@ public class ErrorHandlingIntegrationTest {
         props.put(ERRORS_RETRY_TIMEOUT_CONFIG, "1000");
 
         // set expected records to successfully reach the task
-        connectorHandle.taskHandle(TASK_ID).expectedRecords(EXPECTED_CORRECT_RECORDS);
+        connectorHandle.taskHandle(TASK_ID).expectedRecords(NUM_RECORDS_PRODUCED - FaultyPassthrough.EXPECTED_INCORRECT_RECORDS);
 
         connect.configureConnector(CONNECTOR_NAME, props);
         connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS,
@@ -168,7 +170,7 @@ public class ErrorHandlingIntegrationTest {
 
         // consume failed records from dead letter queue topic
         log.info("Consuming records from test topic");
-        ConsumerRecords<byte[], byte[]> messages = connect.kafka().consume(EXPECTED_INCORRECT_RECORDS, CONSUME_MAX_DURATION_MS, DLQ_TOPIC);
+        ConsumerRecords<byte[], byte[]> messages = connect.kafka().consume(FaultyPassthrough.EXPECTED_INCORRECT_RECORDS, CONSUME_MAX_DURATION_MS, DLQ_TOPIC);
         for (ConsumerRecord<byte[], byte[]> recs : messages) {
             log.debug("Consumed record (key={}, value={}) from dead letter queue topic {}",
                     new String(recs.key()), new String(recs.value()), DLQ_TOPIC);
@@ -194,6 +196,8 @@ public class ErrorHandlingIntegrationTest {
         props.put(CONNECTOR_CLASS_CONFIG, ErrantRecordSinkConnector.class.getSimpleName());
         props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
         props.put(TOPICS_CONFIG, "test-topic");
+        // Restrict the size of each poll so that the records are delivered across multiple polls
+        props.put(CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "5");
         props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
         props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
 
@@ -213,7 +217,7 @@ public class ErrorHandlingIntegrationTest {
         props.put(ERRORS_RETRY_TIMEOUT_CONFIG, "1000");
 
         // set expected records to successfully reach the task
-        connectorHandle.taskHandle(TASK_ID).expectedRecords(EXPECTED_CORRECT_RECORDS);
+        connectorHandle.taskHandle(TASK_ID).expectedRecords(NUM_RECORDS_PRODUCED);
 
         connect.configureConnector(CONNECTOR_NAME, props);
         connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS,
@@ -245,7 +249,12 @@ public class ErrorHandlingIntegrationTest {
 
         // consume failed records from dead letter queue topic
         log.info("Consuming records from test topic");
-        ConsumerRecords<byte[], byte[]> messages = connect.kafka().consume(EXPECTED_INCORRECT_RECORDS, CONSUME_MAX_DURATION_MS, DLQ_TOPIC);
+        Set<String> keys = new HashSet<>();
+        for (ConsumerRecord<byte[], byte[]> rec : connect.kafka().consume(NUM_RECORDS_PRODUCED, CONSUME_MAX_DURATION_MS, DLQ_TOPIC)) {
+            String k = new String(rec.key());
+            keys.add(k);
+        }
+        assertEquals(NUM_RECORDS_PRODUCED, keys.size());
 
         connect.deleteConnector(CONNECTOR_NAME);
         connect.assertions().assertConnectorDoesNotExist(CONNECTOR_NAME,
@@ -297,6 +306,8 @@ public class ErrorHandlingIntegrationTest {
          * An arbitrary id which causes this transformation to fail with a {@link RetriableException}.
          */
         static final int BAD_RECORD_VAL = 7;
+        // Number of records that throw unrecoverable errors from this transformation.
+        private static final int EXPECTED_INCORRECT_RECORDS = 1;
 
         private boolean shouldFail = true;
 
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTaskTest.java
index 6b557b7758..6645f0b7f5 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTaskTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTaskTest.java
@@ -38,6 +38,7 @@ import org.apache.kafka.connect.header.ConnectHeaders;
 import org.apache.kafka.connect.integration.MonitorableSourceConnector;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
 import org.apache.kafka.connect.runtime.errors.ErrorReporter;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest;
 import org.apache.kafka.connect.runtime.isolation.Plugins;
@@ -668,9 +669,9 @@ public class AbstractWorkerSourceTaskTest {
         expectConvertHeadersAndKeyValue(emptyHeaders(), TOPIC);
         expectTaskGetTopic();
 
-        when(transformationChain.apply(eq(record1))).thenReturn(null);
-        when(transformationChain.apply(eq(record2))).thenReturn(null);
-        when(transformationChain.apply(eq(record3))).thenReturn(record3);
+        when(transformationChain.apply(any(), eq(record1))).thenReturn(null);
+        when(transformationChain.apply(any(), eq(record2))).thenReturn(null);
+        when(transformationChain.apply(any(), eq(record3))).thenReturn(record3);
 
         TopicPartitionInfo topicPartitionInfo = new TopicPartitionInfo(0, null, Collections.emptyList(), Collections.emptyList());
         TopicDescription topicDesc = new TopicDescription(TOPIC, false, Collections.singletonList(topicPartitionInfo));
@@ -689,9 +690,9 @@ public class AbstractWorkerSourceTaskTest {
 
         // Ensure that the first two records that were filtered out by the transformation chain
         // aren't re-processed when we retry the call to sendRecords()
-        verify(transformationChain, times(1)).apply(eq(record1));
-        verify(transformationChain, times(1)).apply(eq(record2));
-        verify(transformationChain, times(2)).apply(eq(record3));
+        verify(transformationChain, times(1)).apply(any(), eq(record1));
+        verify(transformationChain, times(1)).apply(any(), eq(record2));
+        verify(transformationChain, times(2)).apply(any(), eq(record3));
     }
 
     @Test
@@ -817,8 +818,8 @@ public class AbstractWorkerSourceTaskTest {
     }
 
     private void expectApplyTransformationChain() {
-        when(transformationChain.apply(any(SourceRecord.class)))
-                .thenAnswer(AdditionalAnswers.returnsFirstArg());
+        when(transformationChain.apply(any(), any(SourceRecord.class)))
+                .thenAnswer(AdditionalAnswers.returnsSecondArg());
     }
 
     private RecordHeaders emptyHeaders() {
@@ -874,7 +875,7 @@ public class AbstractWorkerSourceTaskTest {
             }
 
             @Override
-            protected void producerSendFailed(boolean synchronous, ProducerRecord<byte[], byte[]> producerRecord, SourceRecord preTransformRecord, Exception e) {
+            protected void producerSendFailed(ProcessingContext<SourceRecord> context, boolean synchronous, ProducerRecord<byte[], byte[]> producerRecord, SourceRecord preTransformRecord, Exception e) {
             }
 
             @Override
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTaskTest.java
index 6aa5844dd2..0a35dd314c 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTaskTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ExactlyOnceWorkerSourceTaskTest.java
@@ -1101,8 +1101,8 @@ public class ExactlyOnceWorkerSourceTaskTest {
     }
 
     private void expectApplyTransformationChain() {
-        when(transformationChain.apply(any()))
-                .thenAnswer(invocation -> invocation.getArgument(0));
+        when(transformationChain.apply(any(), any()))
+                .thenAnswer(invocation -> invocation.getArgument(1));
     }
 
     private void expectTaskGetTopic() {
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/InternalSinkRecordTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/InternalSinkRecordTest.java
index bc49583202..28ffcfd8d0 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/InternalSinkRecordTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/InternalSinkRecordTest.java
@@ -18,6 +18,7 @@ package org.apache.kafka.connect.runtime;
 
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.connect.header.Header;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.sink.SinkRecord;
 import org.junit.Test;
 import org.junit.runner.RunWith;
@@ -38,7 +39,8 @@ public class InternalSinkRecordTest {
     public void testNewRecordHeaders() {
         SinkRecord sinkRecord = new SinkRecord(TOPIC, 0, null, null, null, null, 10);
         ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>("test-topic", 0, 10, null, null);
-        InternalSinkRecord internalSinkRecord = new InternalSinkRecord(consumerRecord, sinkRecord);
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        InternalSinkRecord internalSinkRecord = new InternalSinkRecord(context, sinkRecord);
         assertTrue(internalSinkRecord.headers().isEmpty());
         assertTrue(sinkRecord.headers().isEmpty());
 
@@ -52,7 +54,8 @@ public class InternalSinkRecordTest {
         String transformedTopic = "transformed-test-topic";
         SinkRecord sinkRecord = new SinkRecord(transformedTopic, 0, null, null, null, null, 10);
         ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>(TOPIC, 0, 10, null, null);
-        InternalSinkRecord internalSinkRecord = new InternalSinkRecord(consumerRecord, sinkRecord);
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        InternalSinkRecord internalSinkRecord = new InternalSinkRecord(context, sinkRecord);
 
         assertEquals(TOPIC, internalSinkRecord.originalTopic());
         assertEquals(0, internalSinkRecord.originalKafkaPartition().intValue());
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java
index 179a5c62ae..52a5614058 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java
@@ -2164,7 +2164,7 @@ public class WorkerSinkTaskTest {
 
     private void expectTransformation(final int numMessages, final String topicPrefix) {
         final Capture<SinkRecord> recordCapture = EasyMock.newCapture();
-        EasyMock.expect(transformationChain.apply(EasyMock.capture(recordCapture)))
+        EasyMock.expect(transformationChain.apply(EasyMock.anyObject(), EasyMock.capture(recordCapture)))
                 .andAnswer(() -> {
                     SinkRecord origRecord = recordCapture.getValue();
                     return topicPrefix != null && !topicPrefix.isEmpty()
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java
index 4b6af05b07..dffc687f2a 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java
@@ -29,6 +29,7 @@ import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.errors.ConnectException;
+import org.apache.kafka.connect.runtime.errors.ProcessingContext;
 import org.apache.kafka.connect.storage.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
@@ -224,8 +225,10 @@ public class WorkerSinkTaskThreadedTest {
             for (SinkRecord rec : recs) {
                 SinkRecord referenceSinkRecord
                         = new SinkRecord(TOPIC, PARTITION, KEY_SCHEMA, KEY, VALUE_SCHEMA, VALUE, FIRST_OFFSET + offset, TIMESTAMP, TIMESTAMP_TYPE);
-                InternalSinkRecord referenceInternalSinkRecord =
-                        new InternalSinkRecord(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + offset, null, null), referenceSinkRecord);
+                ConsumerRecord<byte[], byte[]> referenceConsumerRecord
+                        = new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + offset, null, null);
+                ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(referenceConsumerRecord);
+                InternalSinkRecord referenceInternalSinkRecord = new InternalSinkRecord(context, referenceSinkRecord);
                 assertEquals(referenceInternalSinkRecord, rec);
                 offset++;
             }
@@ -589,7 +592,7 @@ public class WorkerSinkTaskThreadedTest {
         });
         when(keyConverter.toConnectData(TOPIC, emptyHeaders(), RAW_KEY)).thenReturn(new SchemaAndValue(KEY_SCHEMA, KEY));
         when(valueConverter.toConnectData(TOPIC, emptyHeaders(), RAW_VALUE)).thenReturn(new SchemaAndValue(VALUE_SCHEMA, VALUE));
-        when(transformationChain.apply(any(SinkRecord.class))).thenAnswer(AdditionalAnswers.returnsFirstArg());
+        when(transformationChain.apply(any(), any(SinkRecord.class))).thenAnswer(AdditionalAnswers.returnsSecondArg());
     }
 
     @SuppressWarnings("SameParameterValue")
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java
index dd347faca8..4236342f8a 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java
@@ -603,7 +603,7 @@ public class WorkerSourceTaskTest {
         workerTask.toSend = Arrays.asList(record1, record2);
         assertThrows(ConnectException.class, () -> workerTask.sendRecords());
 
-        verify(transformationChain, times(2)).apply(any(SourceRecord.class));
+        verify(transformationChain, times(2)).apply(any(), any(SourceRecord.class));
         verify(keyConverter, times(2)).fromConnectData(anyString(), any(Headers.class), eq(KEY_SCHEMA), eq(KEY));
         verify(valueConverter, times(2)).fromConnectData(anyString(), any(Headers.class), eq(RECORD_SCHEMA), eq(RECORD));
     }
@@ -835,8 +835,8 @@ public class WorkerSourceTaskTest {
     }
 
     private void expectApplyTransformationChain() {
-        when(transformationChain.apply(any(SourceRecord.class)))
-                .thenAnswer(AdditionalAnswers.returnsFirstArg());
+        when(transformationChain.apply(any(), any(SourceRecord.class)))
+                .thenAnswer(AdditionalAnswers.returnsSecondArg());
     }
 
     private void expectTaskGetTopic() {
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java
index a46dc3d5d2..0cab373eb3 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java
@@ -107,7 +107,7 @@ public class ErrorReporterTest {
         DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(
                 producer, config(emptyMap()), TASK_ID, errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
 
         // since topic name is empty, this method should be a NOOP and producer.send() should
         // not be called.
@@ -120,7 +120,7 @@ public class ErrorReporterTest {
         DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(
                 producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
 
         when(producer.send(any(), any())).thenReturn(metadata);
 
@@ -134,7 +134,7 @@ public class ErrorReporterTest {
         DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(
                 producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
 
         when(producer.send(any(), any())).thenReturn(metadata);
 
@@ -157,7 +157,7 @@ public class ErrorReporterTest {
     public void testLogOnDisabledLogReporter() {
         LogReporter logReporter = new LogReporter(TASK_ID, config(emptyMap()), errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
         context.error(new RuntimeException());
 
         // reporting a context without an error should not cause any errors.
@@ -169,7 +169,7 @@ public class ErrorReporterTest {
     public void testLogOnEnabledLogReporter() {
         LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, "true")), errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
         context.error(new RuntimeException());
 
         // reporting a context without an error should not cause any errors.
@@ -181,7 +181,7 @@ public class ErrorReporterTest {
     public void testLogMessageWithNoRecords() {
         LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, "true")), errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
 
         String msg = logReporter.message(context);
         assertEquals("Error encountered in task job-0. Executing stage 'KEY_CONVERTER' with class " +
@@ -196,7 +196,7 @@ public class ErrorReporterTest {
 
         LogReporter logReporter = new LogReporter(TASK_ID, config(props), errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
 
         String msg = logReporter.message(context);
         assertEquals("Error encountered in task job-0. Executing stage 'KEY_CONVERTER' with class " +
@@ -212,7 +212,7 @@ public class ErrorReporterTest {
 
         LogReporter logReporter = new LogReporter(TASK_ID, config(props), errorHandlingMetrics);
 
-        ProcessingContext context = processingContext();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = processingContext();
 
         String msg = logReporter.message(context);
         assertEquals("Error encountered in task job-0. Executing stage 'KEY_CONVERTER' with class " +
@@ -239,8 +239,8 @@ public class ErrorReporterTest {
         props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, "true");
         DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);
 
-        ProcessingContext context = new ProcessingContext();
-        context.consumerRecord(new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes()));
+        ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes());
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
         context.currentContext(Stage.TRANSFORMATION, Transformation.class);
         context.error(new ConnectException("Test Exception"));
 
@@ -267,8 +267,8 @@ public class ErrorReporterTest {
         props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, "true");
         DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);
 
-        ProcessingContext context = new ProcessingContext();
-        context.consumerRecord(new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes()));
+        ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes());
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
         context.currentContext(Stage.TRANSFORMATION, Transformation.class);
         context.error(new NullPointerException());
 
@@ -295,8 +295,8 @@ public class ErrorReporterTest {
         props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, "true");
         DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);
 
-        ProcessingContext context = new ProcessingContext();
-        context.consumerRecord(new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes()));
+        ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes());
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
         context.currentContext(Stage.TRANSFORMATION, Transformation.class);
         context.error(new ConnectException("Test Exception"));
 
@@ -319,9 +319,9 @@ public class ErrorReporterTest {
         return new String(producerRecord.headers().lastHeader(headerSuffix).value());
     }
 
-    private ProcessingContext processingContext() {
-        ProcessingContext context = new ProcessingContext();
-        context.consumerRecord(new ConsumerRecord<>(TOPIC, 5, 100, new byte[]{'a', 'b'}, new byte[]{'x'}));
+    private ProcessingContext<ConsumerRecord<byte[], byte[]>> processingContext() {
+        ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>(TOPIC, 5, 100, new byte[]{'a', 'b'}, new byte[]{'x'});
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
         context.currentContext(Stage.KEY_CONVERTER, JsonConverter.class);
         return context;
     }
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ProcessingContextTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ProcessingContextTest.java
deleted file mode 100644
index 89f101382e..0000000000
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ProcessingContextTest.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.connect.runtime.errors;
-
-import org.apache.kafka.clients.producer.RecordMetadata;
-import org.apache.kafka.common.TopicPartition;
-import org.junit.Test;
-
-import java.util.List;
-import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.Future;
-import java.util.stream.Collectors;
-import java.util.stream.IntStream;
-
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-public class ProcessingContextTest {
-
-    @Test
-    public void testReportWithSingleReporter() {
-        testReport(1);
-    }
-
-    @Test
-    public void testReportWithMultipleReporters() {
-        testReport(2);
-    }
-
-    private void testReport(int numberOfReports) {
-        ProcessingContext context = new ProcessingContext();
-        List<CompletableFuture<RecordMetadata>> fs = IntStream.range(0, numberOfReports).mapToObj(i -> new CompletableFuture<RecordMetadata>()).collect(Collectors.toList());
-        context.reporters(IntStream.range(0, numberOfReports).mapToObj(i -> (ErrorReporter) c -> fs.get(i)).collect(Collectors.toList()));
-        Future<Void> result = context.report();
-        fs.forEach(f -> {
-            assertFalse(result.isDone());
-            f.complete(new RecordMetadata(new TopicPartition("t", 0), 0, 0, 0, 0, 0));
-        });
-        assertTrue(result.isDone());
-    }
-}
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java
index 6abb6221b5..a4c40ec0b4 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java
@@ -18,6 +18,8 @@ package org.apache.kafka.connect.runtime.errors;
 
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.producer.RecordMetadata;
+import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.metrics.Sensor;
 import org.apache.kafka.common.utils.MockTime;
 import org.apache.kafka.common.utils.SystemTime;
@@ -44,13 +46,10 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
+import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicReference;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
@@ -124,8 +123,9 @@ public class RetryWithToleranceOperatorTest {
         RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(0,
             ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, SYSTEM, errorHandlingMetrics);
 
-        retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,
-            SinkTask.class, consumerRecord, new Throwable());
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        retryWithToleranceOperator.executeFailed(context, Stage.TASK_PUT,
+            SinkTask.class, new Throwable());
     }
 
     @Test
@@ -133,8 +133,9 @@ public class RetryWithToleranceOperatorTest {
         RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(0,
             ERRORS_RETRY_MAX_DELAY_DEFAULT, NONE, SYSTEM, errorHandlingMetrics);
 
-        assertThrows(ConnectException.class, () -> retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,
-            SinkTask.class, consumerRecord, new Throwable()));
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        assertThrows(ConnectException.class, () -> retryWithToleranceOperator.executeFailed(context, Stage.TASK_PUT,
+            SinkTask.class, new Throwable()));
     }
 
     @Test
@@ -189,11 +190,12 @@ public class RetryWithToleranceOperatorTest {
 
     private void testHandleExceptionInStage(Stage type, Exception ex) {
         RetryWithToleranceOperator retryWithToleranceOperator = setupExecutor();
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
         Operation<?> exceptionThrower = () -> {
             throw ex;
         };
-        retryWithToleranceOperator.execute(exceptionThrower, type, RetryWithToleranceOperator.class);
-        assertTrue(retryWithToleranceOperator.failed());
+        retryWithToleranceOperator.execute(context, exceptionThrower, type, RetryWithToleranceOperator.class);
+        assertTrue(context.failed());
     }
 
     private RetryWithToleranceOperator setupExecutor() {
@@ -224,7 +226,7 @@ public class RetryWithToleranceOperatorTest {
     public void execAndHandleRetriableError(long errorRetryTimeout, int numRetriableExceptionsThrown, List<Long> expectedWaits, Exception e, boolean successExpected) throws Exception {
         MockTime time = new MockTime(0, 0, 0);
         CountDownLatch exitLatch = mock(CountDownLatch.class);
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(errorRetryTimeout, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time, errorHandlingMetrics, new ProcessingContext(), exitLatch);
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(errorRetryTimeout, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time, errorHandlingMetrics, exitLatch);
 
         OngoingStubbing<String> mockOperationCall = when(mockOperation.call());
         for (int i = 0; i < numRetriableExceptionsThrown; i++) {
@@ -241,13 +243,14 @@ public class RetryWithToleranceOperatorTest {
             });
         }
 
-        String result = retryWithToleranceOperator.execAndHandleError(mockOperation, Exception.class);
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        String result = retryWithToleranceOperator.execAndHandleError(context, mockOperation, Exception.class);
 
         if (successExpected) {
-            assertFalse(retryWithToleranceOperator.failed());
+            assertFalse(context.failed());
             assertEquals("Success", result);
         } else {
-            assertTrue(retryWithToleranceOperator.failed());
+            assertTrue(context.failed());
         }
 
         verifyNoMoreInteractions(exitLatch);
@@ -258,12 +261,13 @@ public class RetryWithToleranceOperatorTest {
     public void testExecAndHandleNonRetriableError() throws Exception {
         MockTime time = new MockTime(0, 0, 0);
         CountDownLatch exitLatch = mock(CountDownLatch.class);
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(6000, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time, errorHandlingMetrics, new ProcessingContext(), exitLatch);
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(6000, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time, errorHandlingMetrics, exitLatch);
 
         when(mockOperation.call()).thenThrow(new Exception("Test"));
 
-        String result = retryWithToleranceOperator.execAndHandleError(mockOperation, Exception.class);
-        assertTrue(retryWithToleranceOperator.failed());
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        String result = retryWithToleranceOperator.execAndHandleError(context, mockOperation, Exception.class);
+        assertTrue(context.failed());
         assertNull(result);
 
         // expect no call to exitLatch.await() which is only called during the retry backoff
@@ -275,7 +279,7 @@ public class RetryWithToleranceOperatorTest {
     public void testExitLatch() throws Exception {
         MockTime time = new MockTime(0, 0, 0);
         CountDownLatch exitLatch = mock(CountDownLatch.class);
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(-1, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time, errorHandlingMetrics, new ProcessingContext(), exitLatch);
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(-1, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time, errorHandlingMetrics, exitLatch);
         when(mockOperation.call()).thenThrow(new RetriableException("test"));
 
         when(exitLatch.await(300L, TimeUnit.MILLISECONDS)).thenAnswer(i -> {
@@ -298,8 +302,9 @@ public class RetryWithToleranceOperatorTest {
 
         // expect no more calls to exitLatch.await() after retryWithToleranceOperator.triggerStop() is called
 
-        retryWithToleranceOperator.execAndHandleError(mockOperation, Exception.class);
-        assertTrue(retryWithToleranceOperator.failed());
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        retryWithToleranceOperator.execAndHandleError(context, mockOperation, Exception.class);
+        assertTrue(context.failed());
         assertEquals(4500L, time.milliseconds());
         verify(exitLatch).countDown();
         verifyNoMoreInteractions(exitLatch);
@@ -309,7 +314,7 @@ public class RetryWithToleranceOperatorTest {
     public void testBackoffLimit() throws Exception {
         MockTime time = new MockTime(0, 0, 0);
         CountDownLatch exitLatch = mock(CountDownLatch.class);
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(5, 5000, NONE, time, errorHandlingMetrics, new ProcessingContext(), exitLatch);
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(5, 5000, NONE, time, errorHandlingMetrics, exitLatch);
 
         when(exitLatch.await(300, TimeUnit.MILLISECONDS)).thenAnswer(i -> {
             time.sleep(300);
@@ -392,90 +397,29 @@ public class RetryWithToleranceOperatorTest {
     }
 
     @Test
-    public void testThreadSafety() throws Throwable {
-        long runtimeMs = 5_000;
-        int numThreads = 10;
-        // Check that multiple threads using RetryWithToleranceOperator concurrently
-        // can't corrupt the state of the ProcessingContext
-        AtomicReference<Throwable> failed = new AtomicReference<>(null);
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(0,
-                ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, SYSTEM, errorHandlingMetrics, new ProcessingContext() {
-                    private final AtomicInteger count = new AtomicInteger();
-                    private final AtomicInteger attempt = new AtomicInteger();
-
-                    @Override
-                    public void error(Throwable error) {
-                        if (count.getAndIncrement() > 0) {
-                            failed.compareAndSet(null, new AssertionError("Concurrent call to error()"));
-                        }
-                        super.error(error);
-                    }
-
-                    @Override
-                    public Future<Void> report() {
-                        if (count.getAndSet(0) > 1) {
-                            failed.compareAndSet(null, new AssertionError("Concurrent call to error() in report()"));
-                        }
-
-                        return super.report();
-                    }
-
-                    @Override
-                    public void currentContext(Stage stage, Class<?> klass) {
-                        this.attempt.set(0);
-                        super.currentContext(stage, klass);
-                    }
-
-                    @Override
-                    public void attempt(int attempt) {
-                        if (!this.attempt.compareAndSet(attempt - 1, attempt)) {
-                            failed.compareAndSet(null, new AssertionError(
-                                    "Concurrent call to attempt(): Attempts should increase monotonically " +
-                                            "within the scope of a given currentContext()"));
-                        }
-                        super.attempt(attempt);
-                    }
-                }, new CountDownLatch(1));
-
-        ExecutorService pool = Executors.newFixedThreadPool(numThreads);
-        List<? extends Future<?>> futures = IntStream.range(0, numThreads).boxed()
-                .map(id ->
-                        pool.submit(() -> {
-                            long t0 = System.currentTimeMillis();
-                            long i = 0;
-                            while (true) {
-                                if (++i % 10000 == 0 && System.currentTimeMillis() > t0 + runtimeMs) {
-                                    break;
-                                }
-                                if (failed.get() != null) {
-                                    break;
-                                }
-                                try {
-                                    if (id < numThreads / 2) {
-                                        retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,
-                                                SinkTask.class, consumerRecord, new Throwable()).get();
-                                    } else {
-                                        retryWithToleranceOperator.execute(() -> null, Stage.TRANSFORMATION,
-                                                SinkTask.class);
-                                    }
-                                } catch (Exception e) {
-                                    failed.compareAndSet(null, e);
-                                }
-                            }
-                        }))
-                .collect(Collectors.toList());
-        pool.shutdown();
-        pool.awaitTermination((long) (1.5 * runtimeMs), TimeUnit.MILLISECONDS);
-        futures.forEach(future -> {
-            try {
-                future.get();
-            } catch (Exception e) {
-                failed.compareAndSet(null, e);
-            }
+    public void testReportWithSingleReporter() {
+        testReport(1);
+    }
+
+    @Test
+    public void testReportWithMultipleReporters() {
+        testReport(2);
+    }
+
+    private void testReport(int numberOfReports) {
+        MockTime time = new MockTime(0, 0, 0);
+        CountDownLatch exitLatch = mock(CountDownLatch.class);
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(-1, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time, errorHandlingMetrics, exitLatch);
+        ConsumerRecord<byte[], byte[]> consumerRecord = new ConsumerRecord<>("t", 0, 0, null, null);
+        List<CompletableFuture<RecordMetadata>> fs = IntStream.range(0, numberOfReports).mapToObj(i -> new CompletableFuture<RecordMetadata>()).collect(Collectors.toList());
+        List<ErrorReporter> reporters = IntStream.range(0, numberOfReports).mapToObj(i -> (ErrorReporter) c -> fs.get(i)).collect(Collectors.toList());
+        retryWithToleranceOperator.reporters(reporters);
+        ProcessingContext<ConsumerRecord<byte[], byte[]>> context = new ProcessingContext<>(consumerRecord);
+        Future<Void> result = retryWithToleranceOperator.report(context);
+        fs.forEach(f -> {
+            assertFalse(result.isDone());
+            f.complete(new RecordMetadata(new TopicPartition("t", 0), 0, 0, 0, 0, 0));
         });
-        Throwable exception = failed.get();
-        if (exception != null) {
-            throw exception;
-        }
+        assertTrue(result.isDone());
     }
 }
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporterTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporterTest.java
index 084bf9c57f..d7b7f9822b 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporterTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporterTest.java
@@ -39,7 +39,6 @@ import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertThrows;
 import static org.junit.Assert.assertTrue;
 import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 
@@ -50,6 +49,7 @@ public class WorkerErrantRecordReporterTest {
 
     @Mock private Converter converter;
     @Mock private HeaderConverter headerConverter;
+    @Mock private ProcessingContext<ConsumerRecord<byte[], byte[]>> context;
     @Mock private InternalSinkRecord record;
     @Mock private ErrorHandlingMetrics errorHandlingMetrics;
     @Mock private ErrorReporter errorReporter;
@@ -82,8 +82,7 @@ public class WorkerErrantRecordReporterTest {
     private void testReport(boolean errorsTolerated) {
         initializeReporter(errorsTolerated);
         when(errorReporter.report(any())).thenReturn(CompletableFuture.completedFuture(null));
-        @SuppressWarnings("unchecked") ConsumerRecord<byte[], byte[]> consumerRecord = mock(ConsumerRecord.class);
-        when(record.originalRecord()).thenReturn(consumerRecord);
+        when(record.context()).thenReturn(context);
 
         if (errorsTolerated) {
             reporter.report(record, new Throwable());
