diff --git a/core/src/main/scala/kafka/consumer/SimpleConsumer.scala b/core/src/main/scala/kafka/consumer/SimpleConsumer.scala
index 6ed3858945..10a2eefe06 100644
--- a/core/src/main/scala/kafka/consumer/SimpleConsumer.scala
+++ b/core/src/main/scala/kafka/consumer/SimpleConsumer.scala
@@ -20,6 +20,7 @@ package kafka.consumer
 import kafka.api._
 import kafka.network._
 import kafka.utils._
+import kafka.common.ErrorMapping
 
 /**
  * A consumer of kafka messages
@@ -111,8 +112,10 @@ class SimpleConsumer( val host: String,
    */
   def getOffsetsBefore(topic: String, partition: Int, time: Long, maxNumOffsets: Int): Array[Long] = {
     val request = new OffsetRequest(topic, partition, time, maxNumOffsets)
-    val response = sendRequest(request)
-    OffsetResponse.readFrom(response.buffer).offsets
+    val offsetResponse = OffsetResponse.readFrom(sendRequest(request).buffer)
+    // try to throw exception based on global error codes
+    ErrorMapping.maybeThrowException(offsetResponse.errorCode)
+    offsetResponse.offsets
   }
 
   private def getOrMakeConnection() {
diff --git a/core/src/main/scala/kafka/javaapi/consumer/ConsumerConnector.java b/core/src/main/scala/kafka/javaapi/consumer/ConsumerConnector.java
index afb6b0ae54..9ac8860d68 100644
--- a/core/src/main/scala/kafka/javaapi/consumer/ConsumerConnector.java
+++ b/core/src/main/scala/kafka/javaapi/consumer/ConsumerConnector.java
@@ -18,13 +18,14 @@
 package kafka.javaapi.consumer;
 
 
-import java.util.List;
-import java.util.Map;
 import kafka.consumer.KafkaStream;
 import kafka.consumer.TopicFilter;
 import kafka.message.Message;
 import kafka.serializer.Decoder;
 
+import java.util.List;
+import java.util.Map;
+
 public interface ConsumerConnector {
   /**
    *  Create a list of MessageStreams of type T for each topic.
diff --git a/core/src/main/scala/kafka/producer/ProducerPool.scala b/core/src/main/scala/kafka/producer/ProducerPool.scala
index 75c127ae32..9d742f56d5 100644
--- a/core/src/main/scala/kafka/producer/ProducerPool.scala
+++ b/core/src/main/scala/kafka/producer/ProducerPool.scala
@@ -20,7 +20,7 @@ package kafka.producer
 import kafka.cluster.Broker
 import java.util.Properties
 import org.I0Itec.zkclient.ZkClient
-import kafka.utils.{ZkUtils, Utils, Logging}
+import kafka.utils.{ZkUtils, Logging}
 import collection.mutable.HashMap
 import java.lang.Object
 import kafka.common.{UnavailableProducerException, NoBrokersForPartitionException}
diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala
index b970cdb1aa..7b450a70fc 100644
--- a/core/src/main/scala/kafka/server/KafkaApis.scala
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala
@@ -168,8 +168,6 @@ class KafkaApis(val requestChannel: RequestChannel,
         BrokerTopicStat.getBrokerTopicStat(topicData.topic).recordBytesIn(partitionData.messages.sizeInBytes)
         BrokerTopicStat.getBrokerAllTopicStat.recordBytesIn(partitionData.messages.sizeInBytes)
         try {
-          // TODO: should use replicaManager for ensurePartitionLeaderOnThisBroker?
-          // although this ties in with KAFKA-352
           kafkaZookeeper.ensurePartitionLeaderOnThisBroker(topicData.topic, partitionData.partition)
           val log = logManager.getOrCreateLog(topicData.topic, partitionData.partition)
           log.append(partitionData.messages.asInstanceOf[ByteBufferMessageSet])
@@ -357,8 +355,20 @@ class KafkaApis(val requestChannel: RequestChannel,
     val offsetRequest = OffsetRequest.readFrom(request.request.buffer)
     if(requestLogger.isTraceEnabled)
       requestLogger.trace("Offset request " + offsetRequest.toString)
-    val offsets = logManager.getOffsets(offsetRequest)
-    val response = new OffsetResponse(offsetRequest.versionId, offsets)
+    var response: OffsetResponse = null
+    try {
+      kafkaZookeeper.ensurePartitionLeaderOnThisBroker(offsetRequest.topic, offsetRequest.partition)
+      val offsets = logManager.getOffsets(offsetRequest)
+      response = new OffsetResponse(offsetRequest.versionId, offsets)
+    }catch {
+      case ioe: IOException =>
+        fatal("Halting due to unrecoverable I/O error while handling producer request: " + ioe.getMessage, ioe)
+        System.exit(1)
+      case e =>
+        warn("Error while responding to offset request", e)
+        response = new OffsetResponse(offsetRequest.versionId, Array.empty[Long],
+          ErrorMapping.codeFor(e.getClass.asInstanceOf[Class[Throwable]]).toShort)
+    }
     requestChannel.sendResponse(new RequestChannel.Response(request, new BoundedByteBufferSend(response)))
   }
 
@@ -367,7 +377,6 @@ class KafkaApis(val requestChannel: RequestChannel,
    */
   def handleTopicMetadataRequest(request: RequestChannel.Request) {
     val metadataRequest = TopicMetadataRequest.readFrom(request.request.buffer)
-
     if(requestLogger.isTraceEnabled)
       requestLogger.trace("Topic metadata request " + metadataRequest.toString())
 
@@ -395,7 +404,6 @@ class KafkaApis(val requestChannel: RequestChannel,
           }
       }
     }
-    info("Sending response for topic metadata request")
     val response = new TopicMetaDataResponse(metadataRequest.versionId, topicsMetadata.toSeq)
     requestChannel.sendResponse(new RequestChannel.Response(request, new BoundedByteBufferSend(response)))
   }
diff --git a/core/src/main/scala/kafka/server/KafkaZooKeeper.scala b/core/src/main/scala/kafka/server/KafkaZooKeeper.scala
index 1423b28503..7bf60170f3 100644
--- a/core/src/main/scala/kafka/server/KafkaZooKeeper.scala
+++ b/core/src/main/scala/kafka/server/KafkaZooKeeper.scala
@@ -25,7 +25,7 @@ import org.I0Itec.zkclient.{IZkDataListener, IZkChildListener, IZkStateListener,
 import kafka.admin.AdminUtils
 import java.lang.{Thread, IllegalStateException}
 import collection.mutable.HashSet
-import kafka.common.{InvalidPartitionException, NoLeaderForPartitionException, NotLeaderForPartitionException, KafkaZookeeperClient}
+import kafka.common._
 
 /**
  * Handles the server's interaction with zookeeper. The server needs to register the following paths:
@@ -41,8 +41,8 @@ class KafkaZooKeeper(config: KafkaConfig,
 
   val brokerIdPath = ZkUtils.BrokerIdsPath + "/" + config.brokerId
   private var zkClient: ZkClient = null
-  private val leaderChangeListener = new LeaderChangeListener
-  private val topicPartitionsChangeListener = new TopicChangeListener
+  private var leaderChangeListener: LeaderChangeListener = null
+  private var topicPartitionsChangeListener: TopicChangeListener = null
   private var stateChangeHandler: StateChangeCommandHandler = null
 
   private val topicListenerLock = new Object
@@ -52,6 +52,8 @@ class KafkaZooKeeper(config: KafkaConfig,
     /* start client */
     info("connecting to ZK: " + config.zkConnect)
     zkClient = KafkaZookeeperClient.getZookeeperClient(config)
+    leaderChangeListener = new LeaderChangeListener
+    topicPartitionsChangeListener = new TopicChangeListener
     startStateChangeCommandHandler()
     zkClient.subscribeStateChanges(new SessionExpireListener)
     registerBrokerInZk()
@@ -112,9 +114,8 @@ class KafkaZooKeeper(config: KafkaConfig,
   }
 
   def ensurePartitionLeaderOnThisBroker(topic: String, partition: Int) {
-    // TODO: KAFKA-352 first check if this topic exists in the cluster
-//    if(!topicPartitionsChangeListener.doesTopicExistInCluster(topic))
-//      throw new UnknownTopicException("Topic %s doesn't exist in the cluster".format(topic))
+    if(!topicPartitionsChangeListener.doesTopicExistInCluster(topic))
+      throw new UnknownTopicException("Topic %s doesn't exist in the cluster".format(topic))
     // check if partition id is invalid
     if(partition < 0)
       throw new InvalidPartitionException("Partition %d is invalid".format(partition))
@@ -287,6 +288,8 @@ class KafkaZooKeeper(config: KafkaConfig,
 
   class TopicChangeListener extends IZkChildListener with Logging {
     private val allTopics = new HashSet[String]()
+    // read existing topics, if any
+    allTopics ++= ZkUtils.getAllTopics(zkClient)
 
     @throws(classOf[Exception])
     def handleChildChange(parentPath : String, curChilds : java.util.List[String]) {
@@ -297,17 +300,20 @@ class KafkaZooKeeper(config: KafkaConfig,
         val newTopics = currentChildren -- allTopics
         val deletedTopics = allTopics -- currentChildren
         allTopics.clear()
-        allTopics ++ currentChildren
+        allTopics ++= currentChildren
 
         debug("New topics: [%s]. Deleted topics: [%s]".format(newTopics.mkString(","), deletedTopics.mkString(",")))
+        debug("Current topics in the cluster: [%s]".format(allTopics.mkString(",")))
         handleNewTopics(newTopics.toSeq)
         // TODO: Handle topic deletions
-        //handleDeletedTopics(deletedTopics.toSeq)
+        // handleDeletedTopics(deletedTopics.toSeq)
       }
     }
 
     def doesTopicExistInCluster(topic: String): Boolean = {
-      allTopics.contains(topic)
+      topicListenerLock.synchronized {
+        allTopics.contains(topic)
+      }
     }
   }
 
diff --git a/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala b/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala
index 1137f36c01..5e37e3f105 100644
--- a/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala
+++ b/core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala
@@ -32,7 +32,7 @@ import kafka.zk.ZooKeeperTestHarness
 import org.scalatest.junit.JUnit3Suite
 import scala.collection._
 import kafka.admin.CreateTopicCommand
-import kafka.common.{InvalidPartitionException, NotLeaderForPartitionException, FetchRequestFormatException, OffsetOutOfRangeException}
+import kafka.common.{InvalidPartitionException, FetchRequestFormatException, OffsetOutOfRangeException}
 
 /**
  * End to end tests of the primitive apis against a local server
diff --git a/core/src/test/scala/unit/kafka/log/LogOffsetTest.scala b/core/src/test/scala/unit/kafka/log/LogOffsetTest.scala
index 23e2156d19..780b667f7c 100644
--- a/core/src/test/scala/unit/kafka/log/LogOffsetTest.scala
+++ b/core/src/test/scala/unit/kafka/log/LogOffsetTest.scala
@@ -30,6 +30,7 @@ import kafka.zk.ZooKeeperTestHarness
 import org.scalatest.junit.JUnit3Suite
 import kafka.admin.CreateTopicCommand
 import kafka.api.{FetchRequestBuilder, OffsetRequest}
+import kafka.common.UnknownTopicException
 
 object LogOffsetTest {
   val random = new Random()  
@@ -62,6 +63,16 @@ class LogOffsetTest extends JUnit3Suite with ZooKeeperTestHarness {
     super.tearDown()
   }
 
+  @Test
+  def testGetOffsetsForUnknownTopic() {
+    try {
+      simpleConsumer.getOffsetsBefore("foo", 0, OffsetRequest.LatestTime, 10)
+      fail("Should fail with UnknownTopicException since topic foo was never created")
+    }catch {
+      case e: UnknownTopicException => // this is ok
+    }
+  }
+
   @Test
   def testGetOffsetsBeforeLatestTime() {
     val topicPartition = "kafka-" + 0
@@ -104,14 +115,14 @@ class LogOffsetTest extends JUnit3Suite with ZooKeeperTestHarness {
     topicLogDir.mkdir
 
     val topic = topicPartition.split("-").head
-    val part = Integer.valueOf(topicPartition.split("-").last).intValue
 
     // setup brokers in zookeeper as owners of partitions for this test
     CreateTopicCommand.createTopic(zkClient, topic, 1, 1, "1")
+    TestUtils.waitUntilLeaderIsElected(zkClient, topic, 0, 500)
 
     var offsetChanged = false
     for(i <- 1 to 14) {
-      val consumerOffsets = simpleConsumer.getOffsetsBefore(topic, part,
+      val consumerOffsets = simpleConsumer.getOffsetsBefore(topic, 0,
         OffsetRequest.EarliestTime, 1)
 
       if(consumerOffsets(0) == 1) {
diff --git a/core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala b/core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala
index ca7962c9ee..49a54be758 100644
--- a/core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala
+++ b/core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala
@@ -132,7 +132,7 @@ class SyncProducerTest extends JUnit3Suite with KafkaServerTestHarness {
     Assert.assertEquals(request.correlationId, response.correlationId)
     Assert.assertEquals(response.errors.length, response.offsets.length)
     Assert.assertEquals(3, response.errors.length)
-    response.errors.foreach(Assert.assertEquals(ErrorMapping.NoLeaderForPartitionCode.toShort, _))
+    response.errors.foreach(Assert.assertEquals(ErrorMapping.UnknownTopicCode.toShort, _))
     response.offsets.foreach(Assert.assertEquals(-1L, _))
 
     // #2 - test that we get correct offsets when partition is owned by broker
@@ -141,7 +141,6 @@ class SyncProducerTest extends JUnit3Suite with KafkaServerTestHarness {
     CreateTopicCommand.createTopic(zkClient, "topic3", 1, 1)
     TestUtils.waitUntilLeaderIsElected(zkClient, "topic3", 0, 500)
 
-    Thread.sleep(500)
     val response2 = producer.send(request)
     Assert.assertNotNull(response2)
     Assert.assertEquals(request.correlationId, response2.correlationId)
@@ -154,8 +153,8 @@ class SyncProducerTest extends JUnit3Suite with KafkaServerTestHarness {
     Assert.assertEquals(messages.sizeInBytes, response2.offsets(0))
     Assert.assertEquals(messages.sizeInBytes, response2.offsets(2))
 
-    // the middle message should have been rejected because broker doesn't lead partition
-    Assert.assertEquals(ErrorMapping.NoLeaderForPartitionCode.toShort, response2.errors(1))
+    // the middle message should have been rejected because the topic does not exist
+    Assert.assertEquals(ErrorMapping.UnknownTopicCode.toShort, response2.errors(1))
     Assert.assertEquals(-1, response2.offsets(1))
   }
 
@@ -180,7 +179,7 @@ class SyncProducerTest extends JUnit3Suite with KafkaServerTestHarness {
 
     val t1 = SystemTime.milliseconds
     try {
-      val response2 = producer.send(request)
+      producer.send(request)
       Assert.fail("Should have received timeout exception since request handling is stopped.")
     } catch {
       case e: SocketTimeoutException => /* success */
@@ -191,4 +190,28 @@ class SyncProducerTest extends JUnit3Suite with KafkaServerTestHarness {
     // make sure we don't wait fewer than timeoutMs for a response
     Assert.assertTrue((t2-t1) >= timeoutMs)
   }
+
+  @Test
+  def testProduceRequestForUnknownTopic() {
+    val server = servers.head
+    val props = new Properties()
+    props.put("host", "localhost")
+    props.put("port", server.socketServer.port.toString)
+    props.put("buffer.size", "102400")
+    props.put("connect.timeout.ms", "300")
+    props.put("reconnect.interval", "500")
+    props.put("max.message.size", "100")
+
+    val producer = new SyncProducer(new SyncProducerConfig(props))
+    val messages = new ByteBufferMessageSet(NoCompressionCodec, new Message(messageBytes))
+
+    val request = TestUtils.produceRequestWithAcks(Array("topic1", "topic2", "topic3"), Array(0), messages, 1)
+    val response = producer.send(request)
+
+    Assert.assertNotNull(response)
+    Assert.assertEquals(request.correlationId, response.correlationId)
+    Assert.assertEquals(response.errors.length, response.offsets.length)
+    Assert.assertEquals(3, response.errors.length)
+    response.errors.foreach(Assert.assertEquals(ErrorMapping.UnknownTopicCode.toShort, _))
+  }
 }
diff --git a/core/src/test/scala/unit/kafka/server/ServerShutdownTest.scala b/core/src/test/scala/unit/kafka/server/ServerShutdownTest.scala
index e1ceb2dfb1..6052ef2d0a 100644
--- a/core/src/test/scala/unit/kafka/server/ServerShutdownTest.scala
+++ b/core/src/test/scala/unit/kafka/server/ServerShutdownTest.scala
@@ -20,7 +20,6 @@ import java.io.File
 import kafka.consumer.SimpleConsumer
 import java.util.Properties
 import org.junit.Test
-import org.scalatest.junit.JUnitSuite
 import junit.framework.Assert._
 import kafka.message.{Message, ByteBufferMessageSet}
 import org.scalatest.junit.JUnit3Suite
