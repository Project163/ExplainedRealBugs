diff --git a/core/src/test/java/kafka/server/MetadataVersionConfigValidatorTest.java b/core/src/test/java/kafka/server/MetadataVersionConfigValidatorTest.java
index a776e3fe10..daa0aacca7 100644
--- a/core/src/test/java/kafka/server/MetadataVersionConfigValidatorTest.java
+++ b/core/src/test/java/kafka/server/MetadataVersionConfigValidatorTest.java
@@ -47,7 +47,7 @@ public class MetadataVersionConfigValidatorTest {
             .numBytes(88)
             .build();
     public static final MetadataProvenance TEST_PROVENANCE =
-            new MetadataProvenance(50, 3, 8000);
+            new MetadataProvenance(50, 3, 8000, true);
 
     void testWith(MetadataVersion metadataVersion, KafkaConfig config, FaultHandler faultHandler) throws Exception {
         try (MetadataVersionConfigValidator validator = new MetadataVersionConfigValidator(config, faultHandler)) {
diff --git a/core/src/test/java/kafka/server/handlers/DescribeTopicPartitionsRequestHandlerTest.java b/core/src/test/java/kafka/server/handlers/DescribeTopicPartitionsRequestHandlerTest.java
index b0d635c226..643d9d333f 100644
--- a/core/src/test/java/kafka/server/handlers/DescribeTopicPartitionsRequestHandlerTest.java
+++ b/core/src/test/java/kafka/server/handlers/DescribeTopicPartitionsRequestHandlerTest.java
@@ -495,7 +495,7 @@ class DescribeTopicPartitionsRequestHandlerTest {
     void updateKraftMetadataCache(KRaftMetadataCache kRaftMetadataCache, List<ApiMessage> records) {
         MetadataImage image = kRaftMetadataCache.currentImage();
         MetadataImage partialImage = new MetadataImage(
-            new MetadataProvenance(100L, 10, 1000L),
+            new MetadataProvenance(100L, 10, 1000L, true),
             image.features(),
             ClusterImage.EMPTY,
             image.topics(),
@@ -508,7 +508,7 @@ class DescribeTopicPartitionsRequestHandlerTest {
         );
         MetadataDelta delta = new MetadataDelta.Builder().setImage(partialImage).build();
         records.stream().forEach(record -> delta.replay(record));
-        kRaftMetadataCache.setImage(delta.apply(new MetadataProvenance(100L, 10, 1000L)));
+        kRaftMetadataCache.setImage(delta.apply(new MetadataProvenance(100L, 10, 1000L, true)));
     }
 
     private RequestChannel.Request buildRequest(AbstractRequest request,
diff --git a/core/src/test/scala/unit/kafka/server/ControllerRegistrationManagerTest.scala b/core/src/test/scala/unit/kafka/server/ControllerRegistrationManagerTest.scala
index 203f821322..52c90cc93a 100644
--- a/core/src/test/scala/unit/kafka/server/ControllerRegistrationManagerTest.scala
+++ b/core/src/test/scala/unit/kafka/server/ControllerRegistrationManagerTest.scala
@@ -117,7 +117,7 @@ class ControllerRegistrationManagerTest {
         }
       }
     }
-    val provenance = new MetadataProvenance(100, 200, 300)
+    val provenance = new MetadataProvenance(100, 200, 300, true)
     val newImage = delta.apply(provenance)
     val manifest = if (!prevImage.features().metadataVersion().equals(metadataVersion)) {
       new SnapshotManifest(provenance, 1000)
diff --git a/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala b/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala
index 4e12e93d7b..4e70652494 100644
--- a/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala
+++ b/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala
@@ -64,7 +64,7 @@ object MetadataCacheTest {
         // contains no brokers, but which contains the previous partitions.
         val image = c.currentImage()
         val partialImage = new MetadataImage(
-          new MetadataProvenance(100L, 10, 1000L),
+          new MetadataProvenance(100L, 10, 1000L, true),
           image.features(),
           ClusterImage.EMPTY,
           image.topics(),
@@ -129,7 +129,7 @@ object MetadataCacheTest {
           toRecords(topic).foreach(delta.replay)
         }
         records.foreach(delta.replay)
-        c.setImage(delta.apply(new MetadataProvenance(100L, 10, 1000L)))
+        c.setImage(delta.apply(new MetadataProvenance(100L, 10, 1000L, true)))
       }
       case _ => throw new RuntimeException("Unsupported cache type")
     }
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala
index 8682532214..1f654c0909 100644
--- a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala
+++ b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala
@@ -6359,7 +6359,7 @@ class ReplicaManagerTest {
       MetadataVersion.latestProduction(),
       ZkMigrationState.NONE)
     new MetadataImage(
-      new MetadataProvenance(100L, 10, 1000L),
+      new MetadataProvenance(100L, 10, 1000L, true),
       featuresImageLatest,
       ClusterImageTest.IMAGE1,
       topicsImage,
diff --git a/metadata/src/main/java/org/apache/kafka/image/MetadataProvenance.java b/metadata/src/main/java/org/apache/kafka/image/MetadataProvenance.java
index 57ce8efa6d..9e0414d8c8 100644
--- a/metadata/src/main/java/org/apache/kafka/image/MetadataProvenance.java
+++ b/metadata/src/main/java/org/apache/kafka/image/MetadataProvenance.java
@@ -27,20 +27,23 @@ import java.util.Objects;
  * Information about the source of a metadata image.
  */
 public final class MetadataProvenance {
-    public static final MetadataProvenance EMPTY = new MetadataProvenance(-1L, -1, -1L);
+    public static final MetadataProvenance EMPTY = new MetadataProvenance(-1L, -1, -1L, false);
 
     private final long lastContainedOffset;
     private final int lastContainedEpoch;
     private final long lastContainedLogTimeMs;
+    private final boolean isOffsetBatchAligned;
 
     public MetadataProvenance(
         long lastContainedOffset,
         int lastContainedEpoch,
-        long lastContainedLogTimeMs
+        long lastContainedLogTimeMs,
+        boolean isOffsetBatchAligned
     ) {
         this.lastContainedOffset = lastContainedOffset;
         this.lastContainedEpoch = lastContainedEpoch;
         this.lastContainedLogTimeMs = lastContainedLogTimeMs;
+        this.isOffsetBatchAligned = isOffsetBatchAligned;
     }
 
     public OffsetAndEpoch snapshotId() {
@@ -59,6 +62,13 @@ public final class MetadataProvenance {
         return lastContainedLogTimeMs;
     }
 
+    /**
+     * Returns whether lastContainedOffset is the last offset in a record batch
+     */
+    public boolean isOffsetBatchAligned() {
+        return isOffsetBatchAligned;
+    }
+
     /**
      * Returns the name that a snapshot with this provenance would have.
      */
@@ -72,14 +82,16 @@ public final class MetadataProvenance {
         MetadataProvenance other = (MetadataProvenance) o;
         return lastContainedOffset == other.lastContainedOffset &&
             lastContainedEpoch == other.lastContainedEpoch &&
-            lastContainedLogTimeMs == other.lastContainedLogTimeMs;
+            lastContainedLogTimeMs == other.lastContainedLogTimeMs &&
+            isOffsetBatchAligned == other.isOffsetBatchAligned;
     }
 
     @Override
     public int hashCode() {
         return Objects.hash(lastContainedOffset,
             lastContainedEpoch,
-            lastContainedLogTimeMs);
+            lastContainedLogTimeMs,
+            isOffsetBatchAligned);
     }
 
     @Override
@@ -88,6 +100,7 @@ public final class MetadataProvenance {
             "lastContainedOffset=" + lastContainedOffset +
             ", lastContainedEpoch=" + lastContainedEpoch +
             ", lastContainedLogTimeMs=" + lastContainedLogTimeMs +
+            ", isOffsetBatchAligned=" + isOffsetBatchAligned +
             ")";
     }
 }
diff --git a/metadata/src/main/java/org/apache/kafka/image/loader/MetadataBatchLoader.java b/metadata/src/main/java/org/apache/kafka/image/loader/MetadataBatchLoader.java
index 97d1a1f9c0..6dca7caf4d 100644
--- a/metadata/src/main/java/org/apache/kafka/image/loader/MetadataBatchLoader.java
+++ b/metadata/src/main/java/org/apache/kafka/image/loader/MetadataBatchLoader.java
@@ -143,7 +143,8 @@ public class MetadataBatchLoader {
             // 1) this is not the first record in this batch
             // 2) this is not the first batch since last emitting a delta
             if (transactionState == TransactionState.STARTED_TRANSACTION && (indexWithinBatch > 0 || numBatches > 0)) {
-                MetadataProvenance provenance = new MetadataProvenance(lastOffset, lastEpoch, lastContainedLogTimeMs);
+                // Accumulated delta is aligned on batch boundaries iff the BeginTransactionRecord is the first record in a batch
+                MetadataProvenance provenance = new MetadataProvenance(lastOffset, lastEpoch, lastContainedLogTimeMs, indexWithinBatch == 0);
                 LogDeltaManifest manifest = LogDeltaManifest.newBuilder()
                     .provenance(provenance)
                     .leaderAndEpoch(leaderAndEpoch)
@@ -152,9 +153,10 @@ public class MetadataBatchLoader {
                     .numBytes(numBytes)     // This will be zero if we have not yet read a batch
                     .build();
                 if (log.isDebugEnabled()) {
-                    log.debug("handleCommit: Generated a metadata delta between {} and {} from {} batch(es) in {} us.",
+                    log.debug("handleCommit: Generated a metadata delta between {} and {} from {} batch(es) in {} us. The delta is {}.",
                             image.offset(), manifest.provenance().lastContainedOffset(),
-                            manifest.numBatches(), NANOSECONDS.toMicros(manifest.elapsedNs()));
+                            manifest.numBatches(), NANOSECONDS.toMicros(manifest.elapsedNs()),
+                            provenance.isOffsetBatchAligned() ? "batch aligned" : "not batch aligned");
                 }
                 applyDeltaAndUpdate(delta, manifest);
                 transactionState = TransactionState.STARTED_TRANSACTION;
@@ -178,8 +180,8 @@ public class MetadataBatchLoader {
      * Flush the metadata accumulated in this batch loader if not in the middle of a transaction. The
      * flushed metadata will be passed to the {@link MetadataUpdater} configured for this class.
      */
-    public void maybeFlushBatches(LeaderAndEpoch leaderAndEpoch) {
-        MetadataProvenance provenance = new MetadataProvenance(lastOffset, lastEpoch, lastContainedLogTimeMs);
+    public void maybeFlushBatches(LeaderAndEpoch leaderAndEpoch, boolean isOffsetBatchAligned) {
+        MetadataProvenance provenance = new MetadataProvenance(lastOffset, lastEpoch, lastContainedLogTimeMs, isOffsetBatchAligned);
         LogDeltaManifest manifest = LogDeltaManifest.newBuilder()
             .provenance(provenance)
             .leaderAndEpoch(leaderAndEpoch)
diff --git a/metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java b/metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java
index 5702b992aa..ff118a06ab 100644
--- a/metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java
+++ b/metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java
@@ -363,7 +363,7 @@ public class MetadataLoader implements RaftClient.Listener<ApiMessageAndVersion>
                     metrics.updateBatchSize(batch.records().size());
                     metrics.updateBatchProcessingTimeNs(elapsedNs);
                 }
-                batchLoader.maybeFlushBatches(currentLeaderAndEpoch);
+                batchLoader.maybeFlushBatches(currentLeaderAndEpoch, true);
             } catch (Throwable e) {
                 // This is a general catch-all block where we don't expect to end up;
                 // failure-prone operations should have individual try/catch blocks around them.
@@ -434,7 +434,7 @@ public class MetadataLoader implements RaftClient.Listener<ApiMessageAndVersion>
         }
         delta.finishSnapshot();
         MetadataProvenance provenance = new MetadataProvenance(reader.lastContainedLogOffset(),
-                reader.lastContainedLogEpoch(), reader.lastContainedLogTimestamp());
+                reader.lastContainedLogEpoch(), reader.lastContainedLogTimestamp(), true);
         return new SnapshotManifest(provenance,
                 time.nanoseconds() - startNs);
     }
diff --git a/metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java b/metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java
index e427fbe61d..501fb2ee4d 100644
--- a/metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java
+++ b/metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java
@@ -242,16 +242,17 @@ public class SnapshotGenerator implements MetadataPublisher {
         bytesSinceLastSnapshot += manifest.numBytes();
         if (bytesSinceLastSnapshot >= maxBytesSinceLastSnapshot) {
             if (eventQueue.isEmpty()) {
-                scheduleEmit("we have replayed at least " + maxBytesSinceLastSnapshot +
-                    " bytes", newImage);
+                maybeScheduleEmit("we have replayed at least " + maxBytesSinceLastSnapshot +
+                    " bytes", newImage, manifest.provenance().isOffsetBatchAligned());
             } else if (log.isTraceEnabled()) {
                 log.trace("Not scheduling bytes-based snapshot because event queue is not empty yet.");
             }
         } else if (maxTimeSinceLastSnapshotNs != 0 &&
                 (time.nanoseconds() - lastSnapshotTimeNs >= maxTimeSinceLastSnapshotNs)) {
             if (eventQueue.isEmpty()) {
-                scheduleEmit("we have waited at least " +
-                    TimeUnit.NANOSECONDS.toMinutes(maxTimeSinceLastSnapshotNs) + " minute(s)", newImage);
+                maybeScheduleEmit("we have waited at least " +
+                    TimeUnit.NANOSECONDS.toMinutes(maxTimeSinceLastSnapshotNs) +
+                    " minute(s)", newImage, manifest.provenance().isOffsetBatchAligned());
             } else if (log.isTraceEnabled()) {
                 log.trace("Not scheduling time-based snapshot because event queue is not empty yet.");
             }
@@ -260,18 +261,21 @@ public class SnapshotGenerator implements MetadataPublisher {
         }
     }
 
-    void scheduleEmit(
+    void maybeScheduleEmit(
         String reason,
-        MetadataImage image
+        MetadataImage image,
+        boolean isOffsetBatchAligned
     ) {
-        resetSnapshotCounters();
-        eventQueue.append(() -> {
-            String currentDisabledReason = disabledReason.get();
-            if (currentDisabledReason != null) {
-                log.error("Not emitting {} despite the fact that {} because snapshots are " +
-                    "disabled; {}", image.provenance().snapshotName(), reason,
-                        currentDisabledReason);
-            } else {
+        String currentDisabledReason = disabledReason.get();
+        if (currentDisabledReason != null) {
+            log.error("Not emitting {} despite the fact that {} because snapshots are " +
+                "disabled; {}", image.provenance().snapshotName(), reason, currentDisabledReason);
+        } else if (!isOffsetBatchAligned) {
+            log.debug("Not emitting {} despite the fact that {} because snapshots are " +
+                "disabled; {}", image.provenance().snapshotName(), reason, "metadata image is not batch aligned");
+        } else {
+            eventQueue.append(() -> {
+                resetSnapshotCounters();
                 log.info("Creating new KRaft snapshot file {} because {}.",
                         image.provenance().snapshotName(), reason);
                 try {
@@ -279,8 +283,8 @@ public class SnapshotGenerator implements MetadataPublisher {
                 } catch (Throwable e) {
                     faultHandler.handleFault("KRaft snapshot file generation error", e);
                 }
-            }
-        });
+            });
+        }
     }
 
     public void beginShutdown() {
diff --git a/metadata/src/test/java/org/apache/kafka/image/MetadataImageTest.java b/metadata/src/test/java/org/apache/kafka/image/MetadataImageTest.java
index b75ca8330f..c189335a32 100644
--- a/metadata/src/test/java/org/apache/kafka/image/MetadataImageTest.java
+++ b/metadata/src/test/java/org/apache/kafka/image/MetadataImageTest.java
@@ -41,7 +41,7 @@ public class MetadataImageTest {
 
     static {
         IMAGE1 = new MetadataImage(
-            new MetadataProvenance(100, 4, 2000),
+            new MetadataProvenance(100, 4, 2000, true),
             FeaturesImageTest.IMAGE1,
             ClusterImageTest.IMAGE1,
             TopicsImageTest.IMAGE1,
@@ -66,7 +66,7 @@ public class MetadataImageTest {
         RecordTestUtils.replayAll(DELTA1, DelegationTokenImageTest.DELTA1_RECORDS);
 
         IMAGE2 = new MetadataImage(
-            new MetadataProvenance(200, 5, 4000),
+            new MetadataProvenance(200, 5, 4000, true),
             FeaturesImageTest.IMAGE2,
             ClusterImageTest.IMAGE2,
             TopicsImageTest.IMAGE2,
diff --git a/metadata/src/test/java/org/apache/kafka/image/loader/MetadataBatchLoaderTest.java b/metadata/src/test/java/org/apache/kafka/image/loader/MetadataBatchLoaderTest.java
index 311aac0098..e0656c8e26 100644
--- a/metadata/src/test/java/org/apache/kafka/image/loader/MetadataBatchLoaderTest.java
+++ b/metadata/src/test/java/org/apache/kafka/image/loader/MetadataBatchLoaderTest.java
@@ -46,8 +46,10 @@ import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertNotNull;
 import static org.junit.jupiter.api.Assertions.assertNull;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 public class MetadataBatchLoaderTest {
 
@@ -159,11 +161,12 @@ public class MetadataBatchLoaderTest {
         assertEquals(0, updater.updates);
         batchLoader.loadBatch(batch3, LEADER_AND_EPOCH);
         assertEquals(0, updater.updates);
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(1, updater.updates);
         assertNotNull(updater.latestImage.topics().getTopic("foo"));
         assertEquals(18, updater.latestImage.provenance().lastContainedOffset());
         assertEquals(2, updater.latestImage.provenance().lastContainedEpoch());
+        assertTrue(updater.latestImage.provenance().isOffsetBatchAligned());
     }
 
     @Test
@@ -191,14 +194,16 @@ public class MetadataBatchLoaderTest {
         batchLoader.resetToImage(MetadataImage.EMPTY);
         batchLoader.loadBatch(batch1, LEADER_AND_EPOCH);
         assertEquals(0, updater.updates);
+        // batch1 is flushed in this loadBatch call
         batchLoader.loadBatch(batch2, LEADER_AND_EPOCH);
         assertEquals(1, updater.updates);
+        assertTrue(updater.latestImage.provenance().isOffsetBatchAligned());
         assertNull(updater.latestImage.topics().getTopic("bar"));
         batchLoader.loadBatch(batch3, LEADER_AND_EPOCH);
         assertEquals(1, updater.updates);
         batchLoader.loadBatch(batch4, LEADER_AND_EPOCH);
         assertEquals(1, updater.updates);
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertNotNull(updater.latestImage.topics().getTopic("bar"));
         assertEquals(20, updater.latestImage.provenance().lastContainedOffset());
         assertEquals(4, updater.latestImage.provenance().lastContainedEpoch());
@@ -207,19 +212,19 @@ public class MetadataBatchLoaderTest {
         updater.reset();
         batchLoader.resetToImage(MetadataImage.EMPTY);
         batchLoader.loadBatch(batch1, LEADER_AND_EPOCH);
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(1, updater.updates);
 
         batchLoader.loadBatch(batch2, LEADER_AND_EPOCH);
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(1, updater.updates);
 
         batchLoader.loadBatch(batch3, LEADER_AND_EPOCH);
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(1, updater.updates);
 
         batchLoader.loadBatch(batch4, LEADER_AND_EPOCH);
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(2, updater.updates);
     }
 
@@ -249,7 +254,7 @@ public class MetadataBatchLoaderTest {
             "Encountered BeginTransactionRecord while already in a transaction",
             faultHandler.firstException().getCause().getMessage()
         );
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(0, updater.updates);
     }
 
@@ -281,7 +286,7 @@ public class MetadataBatchLoaderTest {
             "Encountered EndTransactionRecord without having seen a BeginTransactionRecord",
             faultHandler.firstException().getCause().getMessage()
         );
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(1, updater.updates);
         assertNotNull(updater.latestImage.topics().getTopic("bar"));
     }
@@ -314,7 +319,7 @@ public class MetadataBatchLoaderTest {
             "Encountered AbortTransactionRecord without having seen a BeginTransactionRecord",
             faultHandler.firstException().getCause().getMessage()
         );
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(1, updater.updates);
         assertNotNull(updater.latestImage.topics().getTopic("bar"));
     }
@@ -355,15 +360,18 @@ public class MetadataBatchLoaderTest {
         assertEquals(1, updater.updates);
         assertEquals(0, updater.latestManifest.numBytes());
         assertEquals(15, updater.latestImage.provenance().lastContainedOffset());
+        // The first transaction is flushed in the middle of the batch, the offset flushed is not batch-aligned
+        assertFalse(updater.latestImage.provenance().isOffsetBatchAligned());
         assertEquals(42, updater.latestImage.provenance().lastContainedEpoch());
 
         assertNotNull(updater.latestImage.topics().getTopic("foo"));
         assertNull(updater.latestImage.topics().getTopic("bar"));
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(2, updater.updates);
         assertEquals(100, updater.latestManifest.numBytes());
         assertEquals(20, updater.latestImage.provenance().lastContainedOffset());
         assertEquals(42, updater.latestImage.provenance().lastContainedEpoch());
+        assertTrue(updater.latestImage.provenance().isOffsetBatchAligned());
         assertNotNull(updater.latestImage.topics().getTopic("foo"));
         assertNotNull(updater.latestImage.topics().getTopic("bar"));
     }
@@ -392,9 +400,10 @@ public class MetadataBatchLoaderTest {
         assertEquals(0, updater.latestManifest.numBytes());
         assertEquals(18, updater.latestImage.provenance().lastContainedOffset());
         assertEquals(42, updater.latestImage.provenance().lastContainedEpoch());
+        assertFalse(updater.latestImage.provenance().isOffsetBatchAligned());
         assertNotNull(updater.latestImage.topics().getTopic("foo"));
         assertNull(updater.latestImage.topics().getTopic("bar"));
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
         assertEquals(3, updater.updates);
         assertEquals(100, updater.latestManifest.numBytes());
         assertEquals(26, updater.latestImage.provenance().lastContainedOffset());
@@ -429,7 +438,7 @@ public class MetadataBatchLoaderTest {
                 20, 4, 0, 10, TXN_END_SINGLETON), LEADER_AND_EPOCH);
         }
         assertEquals(0, updater.updates);
-        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH);
+        batchLoader.maybeFlushBatches(LEADER_AND_EPOCH, true);
 
         // Regardless of end/abort, we should publish an updated MetadataProvenance and manifest
         assertEquals(50, updater.latestManifest.numBytes());
@@ -442,4 +451,29 @@ public class MetadataBatchLoaderTest {
             assertNotNull(updater.latestImage.topics().getTopic("bar"));
         }
     }
+
+    @Test
+    public void testTransactionAlignmentOnBatchBoundary() {
+        List<ApiMessageAndVersion> batchRecords = new ArrayList<>();
+        batchRecords.addAll(noOpRecords(3));
+        batchRecords.addAll(TOPIC_TXN_BATCH_1);
+        batchRecords.addAll(TOPIC_TXN_BATCH_2);
+        batchRecords.addAll(noOpRecords(3));
+
+        MockMetadataUpdater updater = new MockMetadataUpdater();
+        MockFaultHandler faultHandler = new MockFaultHandler("testMultipleTransactionsInOneBatch");
+        MetadataBatchLoader batchLoader = loadSingleBatch(updater, faultHandler, batchRecords);
+
+        assertEquals(1, updater.updates);
+        assertEquals(0, updater.latestManifest.numBytes());
+        assertEquals(12, updater.latestImage.provenance().lastContainedOffset());
+        assertFalse(updater.latestImage.provenance().isOffsetBatchAligned());
+
+        batchLoader.loadBatch(Batch.data(
+                22, 42, 0, 10, TXN_BEGIN_SINGLETON), LEADER_AND_EPOCH);
+        assertEquals(2, updater.updates);
+        assertEquals(100, updater.latestManifest.numBytes());
+        assertEquals(21, updater.latestImage.provenance().lastContainedOffset());
+        assertTrue(updater.latestImage.provenance().isOffsetBatchAligned());
+    }
 }
diff --git a/metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java b/metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java
index 42d73c8798..ce87c57a91 100644
--- a/metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java
+++ b/metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java
@@ -253,7 +253,7 @@ public class MetadataLoaderTest {
             loader.installPublishers(singletonList(publisher)).get();
             if (loadSnapshot) {
                 MockSnapshotReader snapshotReader = new MockSnapshotReader(
-                    new MetadataProvenance(200, 100, 4000),
+                    new MetadataProvenance(200, 100, 4000, true),
                     singletonList(
                         Batch.control(
                             200,
@@ -306,7 +306,7 @@ public class MetadataLoaderTest {
             loader.installPublishers(publishers.subList(0, 2)).get();
             loader.removeAndClosePublisher(publishers.get(1)).get();
             MockSnapshotReader snapshotReader = MockSnapshotReader.fromRecordLists(
-                new MetadataProvenance(100, 50, 2000),
+                new MetadataProvenance(100, 50, 2000, true),
                 singletonList(singletonList(new ApiMessageAndVersion(
                     new FeatureLevelRecord().
                         setName(MetadataVersion.FEATURE_NAME).
@@ -347,7 +347,7 @@ public class MetadataLoaderTest {
             assertEquals(200L, loader.lastAppliedOffset());
             loadEmptySnapshot(loader, 300);
             assertEquals(300L, loader.lastAppliedOffset());
-            assertEquals(new SnapshotManifest(new MetadataProvenance(300, 100, 4000), 3000000L),
+            assertEquals(new SnapshotManifest(new MetadataProvenance(300, 100, 4000, true), 3000000L),
                 publishers.get(0).latestSnapshotManifest);
             assertEquals(MetadataVersion.MINIMUM_KRAFT_VERSION,
                 loader.metrics().currentMetadataVersion());
@@ -364,7 +364,7 @@ public class MetadataLoaderTest {
         long offset
     ) throws Exception {
         MockSnapshotReader snapshotReader = new MockSnapshotReader(
-            new MetadataProvenance(offset, 100, 4000),
+            new MetadataProvenance(offset, 100, 4000, true),
             singletonList(
                 Batch.control(
                     200,
@@ -482,7 +482,7 @@ public class MetadataLoaderTest {
         assertTrue(publishers.get(0).closed);
         assertEquals(
             LogDeltaManifest.newBuilder()
-                .provenance(new MetadataProvenance(300, 100, 4000))
+                .provenance(new MetadataProvenance(300, 100, 4000, true))
                 .leaderAndEpoch(LeaderAndEpoch.UNKNOWN)
                 .numBatches(1)
                 .elapsedNs(0L)
@@ -508,7 +508,7 @@ public class MetadataLoaderTest {
                 build()) {
             loader.installPublishers(publishers).get();
             loader.handleLoadSnapshot(MockSnapshotReader.fromRecordLists(
-                new MetadataProvenance(200, 100, 4000), asList(
+                new MetadataProvenance(200, 100, 4000, true), asList(
                     singletonList(new ApiMessageAndVersion(new FeatureLevelRecord().
                         setName(MetadataVersion.FEATURE_NAME).
                         setFeatureLevel(IBP_3_3_IV1.featureLevel()), (short) 0)),
@@ -578,7 +578,7 @@ public class MetadataLoaderTest {
         long offset
     ) throws Exception {
         loader.handleLoadSnapshot(MockSnapshotReader.fromRecordLists(
-                new MetadataProvenance(offset, 100, 4000), asList(
+                new MetadataProvenance(offset, 100, 4000, true), asList(
                         singletonList(new ApiMessageAndVersion(new FeatureLevelRecord().
                                 setName(MetadataVersion.FEATURE_NAME).
                                 setFeatureLevel(IBP_3_3_IV1.featureLevel()), (short) 0)),
@@ -594,7 +594,7 @@ public class MetadataLoaderTest {
         long offset
     ) throws Exception {
         loader.handleLoadSnapshot(MockSnapshotReader.fromRecordLists(
-                new MetadataProvenance(offset, 100, 4000), asList(
+                new MetadataProvenance(offset, 100, 4000, true), asList(
                         singletonList(new ApiMessageAndVersion(new FeatureLevelRecord().
                                 setName(MetadataVersion.FEATURE_NAME).
                                 setFeatureLevel(IBP_3_3_IV2.featureLevel()), (short) 0)),
@@ -769,7 +769,7 @@ public class MetadataLoaderTest {
 
             // loading a snapshot discards any in-flight transaction
             loader.handleLoadSnapshot(MockSnapshotReader.fromRecordLists(
-                new MetadataProvenance(600, 101, 4000), singletonList(
+                new MetadataProvenance(600, 101, 4000, true), singletonList(
                     singletonList(new ApiMessageAndVersion(new TopicRecord().
                         setName("foo").
                         setTopicId(Uuid.fromString("Uum7sfhHQP-obSvfywmNUA")), (short) 0))
diff --git a/metadata/src/test/java/org/apache/kafka/image/loader/metrics/MetadataLoaderMetricsTest.java b/metadata/src/test/java/org/apache/kafka/image/loader/metrics/MetadataLoaderMetricsTest.java
index 64d501bb40..e26087951d 100644
--- a/metadata/src/test/java/org/apache/kafka/image/loader/metrics/MetadataLoaderMetricsTest.java
+++ b/metadata/src/test/java/org/apache/kafka/image/loader/metrics/MetadataLoaderMetricsTest.java
@@ -105,7 +105,7 @@ public class MetadataLoaderMetricsTest {
     public void testUpdateLastAppliedImageProvenance() {
         MetricsRegistry registry = new MetricsRegistry();
         try (FakeMetadataLoaderMetrics fakeMetrics = new FakeMetadataLoaderMetrics(registry)) {
-            MetadataProvenance provenance = new MetadataProvenance(1L, 2, 3L);
+            MetadataProvenance provenance = new MetadataProvenance(1L, 2, 3L, true);
             fakeMetrics.metrics.updateLastAppliedImageProvenance(provenance);
             assertEquals(provenance, fakeMetrics.provenance.get());
         }
diff --git a/metadata/src/test/java/org/apache/kafka/image/publisher/BrokerRegistrationTrackerTest.java b/metadata/src/test/java/org/apache/kafka/image/publisher/BrokerRegistrationTrackerTest.java
index eac84c8a76..2e03587a7c 100644
--- a/metadata/src/test/java/org/apache/kafka/image/publisher/BrokerRegistrationTrackerTest.java
+++ b/metadata/src/test/java/org/apache/kafka/image/publisher/BrokerRegistrationTrackerTest.java
@@ -55,7 +55,7 @@ public class BrokerRegistrationTrackerTest {
         MetadataImage image = MetadataImage.EMPTY;
 
         void onMetadataUpdate(MetadataDelta delta) {
-            MetadataProvenance provenance = new MetadataProvenance(0, 0, 0);
+            MetadataProvenance provenance = new MetadataProvenance(0, 0, 0, true);
             image = delta.apply(provenance);
             LogDeltaManifest manifest = new LogDeltaManifest.Builder().
                 provenance(provenance).
diff --git a/metadata/src/test/java/org/apache/kafka/image/publisher/ControllerRegistrationsPublisherTest.java b/metadata/src/test/java/org/apache/kafka/image/publisher/ControllerRegistrationsPublisherTest.java
index 8013d29994..4d47580313 100644
--- a/metadata/src/test/java/org/apache/kafka/image/publisher/ControllerRegistrationsPublisherTest.java
+++ b/metadata/src/test/java/org/apache/kafka/image/publisher/ControllerRegistrationsPublisherTest.java
@@ -60,7 +60,7 @@ public class ControllerRegistrationsPublisherTest {
 
     private static final MetadataImage TEST_IMAGE;
 
-    private static final MetadataProvenance PROVENANCE = new MetadataProvenance(100L, 10, 2000L);
+    private static final MetadataProvenance PROVENANCE = new MetadataProvenance(100L, 10, 2000L, true);
 
     static {
         TEST_DELTA = new MetadataDelta.Builder().build();
@@ -79,7 +79,7 @@ public class ControllerRegistrationsPublisherTest {
         ControllerRegistrationsPublisher publisher = new ControllerRegistrationsPublisher();
         if (fromSnapshot) {
             publisher.onMetadataUpdate(TEST_DELTA, TEST_IMAGE,
-                new SnapshotManifest(new MetadataProvenance(100L, 10, 2000L), 100L));
+                new SnapshotManifest(new MetadataProvenance(100L, 10, 2000L, true), 100L));
         } else {
             publisher.onMetadataUpdate(TEST_DELTA, TEST_IMAGE,
                 LogDeltaManifest.newBuilder().
diff --git a/metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java b/metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java
index 955a73bd1e..313c2bc14e 100644
--- a/metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java
+++ b/metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java
@@ -81,13 +81,22 @@ public class SnapshotGeneratorTest {
 
     static LogDeltaManifest.Builder logDeltaManifestBuilder() {
         return LogDeltaManifest.newBuilder()
-            .provenance(MetadataProvenance.EMPTY)
+            .provenance(new MetadataProvenance(-1L, -1, -1L, true))
             .leaderAndEpoch(LeaderAndEpoch.UNKNOWN)
             .numBatches(1)
             .elapsedNs(100)
             .numBytes(100);
     }
 
+    static LogDeltaManifest.Builder notBatchAlignedLogDeltaManifestBuilder() {
+        return LogDeltaManifest.newBuilder()
+                .provenance(MetadataProvenance.EMPTY)
+                .leaderAndEpoch(LeaderAndEpoch.UNKNOWN)
+                .numBatches(1)
+                .elapsedNs(100)
+                .numBytes(100);
+    }
+
     private static final MetadataDelta TEST_DELTA;
 
     static {
@@ -122,6 +131,49 @@ public class SnapshotGeneratorTest {
         faultHandler.maybeRethrowFirstException();
     }
 
+    @Test
+    public void testNoSnapshotCreatedWhenLastOffsetIsNotBatchAligned() throws Exception {
+        MockFaultHandler faultHandler = new MockFaultHandler("SnapshotGenerator");
+        MockEmitter emitter = new MockEmitter();
+        try (SnapshotGenerator generator = new SnapshotGenerator.Builder(emitter).
+                setFaultHandler(faultHandler).
+                setMaxBytesSinceLastSnapshot(200).
+                setMaxTimeSinceLastSnapshotNs(TimeUnit.DAYS.toNanos(10)).
+                build()) {
+            // None of these log delta batches should trigger a snapshot since their offset is not batch aligned.
+            generator.publishLogDelta(TEST_DELTA, TEST_IMAGE, notBatchAlignedLogDeltaManifestBuilder().build());
+            generator.publishLogDelta(TEST_DELTA, TEST_IMAGE, notBatchAlignedLogDeltaManifestBuilder().build());
+            generator.publishLogDelta(TEST_DELTA, TEST_IMAGE, notBatchAlignedLogDeltaManifestBuilder().build());
+            assertEquals(Collections.emptyList(), emitter.images());
+            emitter.setReady();
+        }
+        assertEquals(Collections.emptyList(), emitter.images());
+        faultHandler.maybeRethrowFirstException();
+    }
+
+    @Test
+    public void testSnapshotsCreatedAgainWhenLastOffsetIsAligned() throws Exception {
+        MockFaultHandler faultHandler = new MockFaultHandler("SnapshotGenerator");
+        MockEmitter emitter = new MockEmitter();
+        MetadataImage batchAlignedImage = TEST_DELTA.apply(
+                new MetadataProvenance(-1L, -1, -1L, true));
+        try (SnapshotGenerator generator = new SnapshotGenerator.Builder(emitter).
+                setFaultHandler(faultHandler).
+                setMaxBytesSinceLastSnapshot(100).
+                setMaxTimeSinceLastSnapshotNs(TimeUnit.DAYS.toNanos(10)).
+                build()) {
+            // These should not be published despite meeting the max bytes threshold since they are not batch aligned.
+            generator.publishLogDelta(TEST_DELTA, TEST_IMAGE, notBatchAlignedLogDeltaManifestBuilder().build());
+            generator.publishLogDelta(TEST_DELTA, TEST_IMAGE, notBatchAlignedLogDeltaManifestBuilder().build());
+            // This snapshot should get published since it is batch aligned.
+            generator.publishLogDelta(TEST_DELTA, batchAlignedImage, logDeltaManifestBuilder().build());
+            assertEquals(Collections.emptyList(), emitter.images());
+            emitter.setReady();
+        }
+        assertEquals(Collections.singletonList(batchAlignedImage), emitter.images());
+        faultHandler.maybeRethrowFirstException();
+    }
+
     @Test
     public void testSnapshotsDisabled() throws Exception {
         MockFaultHandler faultHandler = new MockFaultHandler("SnapshotGenerator");
diff --git a/server/src/test/java/org/apache/kafka/server/metrics/BrokerServerMetricsTest.java b/server/src/test/java/org/apache/kafka/server/metrics/BrokerServerMetricsTest.java
index 17cf0fbde2..3b85670525 100644
--- a/server/src/test/java/org/apache/kafka/server/metrics/BrokerServerMetricsTest.java
+++ b/server/src/test/java/org/apache/kafka/server/metrics/BrokerServerMetricsTest.java
@@ -76,7 +76,8 @@ public final class BrokerServerMetricsTest {
             brokerMetrics.updateLastAppliedImageProvenance(new MetadataProvenance(
                     expectedValue,
                     brokerMetrics.lastAppliedImageProvenance().get().lastContainedEpoch(),
-                    brokerMetrics.lastAppliedTimestamp()));
+                    brokerMetrics.lastAppliedTimestamp(),
+                    true));
             assertEquals((double) expectedValue, offsetMetric.metricValue());
         }
     }
@@ -99,7 +100,8 @@ public final class BrokerServerMetricsTest {
             brokerMetrics.updateLastAppliedImageProvenance(new MetadataProvenance(
                     brokerMetrics.lastAppliedOffset(),
                     brokerMetrics.lastAppliedImageProvenance().get().lastContainedEpoch(),
-                    timestamp));
+                    timestamp,
+                    true));
             assertEquals((double) timestamp, timestampMetric.metricValue());
             assertEquals((double) time.milliseconds() - timestamp, lagMetric.metricValue());
         }
