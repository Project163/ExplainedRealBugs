diff --git a/core/src/main/scala/kafka/server/ControllerServer.scala b/core/src/main/scala/kafka/server/ControllerServer.scala
index 282787ac49..5ba4fc3dcb 100644
--- a/core/src/main/scala/kafka/server/ControllerServer.scala
+++ b/core/src/main/scala/kafka/server/ControllerServer.scala
@@ -28,7 +28,7 @@ import kafka.server.QuotaFactory.QuotaManagers
 
 import scala.collection.immutable
 import kafka.server.metadata.{ClientQuotaMetadataManager, DynamicClientQuotaPublisher, DynamicConfigPublisher, ScramPublisher}
-import kafka.utils.{CoreUtils, Logging}
+import kafka.utils.{CoreUtils, Logging, PasswordEncoder}
 import kafka.zk.{KafkaZkClient, ZkMigrationClient}
 import org.apache.kafka.common.config.ConfigException
 import org.apache.kafka.common.message.ApiMessageType.ListenerType
@@ -245,7 +245,15 @@ class ControllerServer(
 
       if (config.migrationEnabled) {
         val zkClient = KafkaZkClient.createZkClient("KRaft Migration", time, config, KafkaServer.zkClientConfigFromKafkaConfig(config))
-        val migrationClient = new ZkMigrationClient(zkClient)
+        val zkConfigEncoder = config.passwordEncoderSecret match {
+          case Some(secret) => PasswordEncoder.encrypting(secret,
+            config.passwordEncoderKeyFactoryAlgorithm,
+            config.passwordEncoderCipherAlgorithm,
+            config.passwordEncoderKeyLength,
+            config.passwordEncoderIterations)
+          case None => PasswordEncoder.noop()
+        }
+        val migrationClient = new ZkMigrationClient(zkClient, zkConfigEncoder)
         val propagator: LegacyPropagator = new MigrationPropagator(config.nodeId, config)
         val migrationDriver = new KRaftMigrationDriver(
           config.nodeId,
diff --git a/core/src/main/scala/kafka/zk/ZkMigrationClient.scala b/core/src/main/scala/kafka/zk/ZkMigrationClient.scala
index e13609d7fb..33e9dfb334 100644
--- a/core/src/main/scala/kafka/zk/ZkMigrationClient.scala
+++ b/core/src/main/scala/kafka/zk/ZkMigrationClient.scala
@@ -18,8 +18,8 @@ package kafka.zk
 
 import kafka.api.LeaderAndIsr
 import kafka.controller.{LeaderIsrAndControllerEpoch, ReplicaAssignment}
-import kafka.server.{ConfigEntityName, ConfigType, ZkAdminManager}
-import kafka.utils.Logging
+import kafka.server.{ConfigEntityName, ConfigType, DynamicBrokerConfig, ZkAdminManager}
+import kafka.utils.{Logging, PasswordEncoder}
 import kafka.zk.TopicZNode.TopicIdReplicaAssignment
 import kafka.zookeeper._
 import org.apache.kafka.common.config.ConfigResource
@@ -45,7 +45,10 @@ import scala.jdk.CollectionConverters._
  * Migration client in KRaft controller responsible for handling communication to Zookeeper and
  * the ZkBrokers present in the cluster.
  */
-class ZkMigrationClient(zkClient: KafkaZkClient) extends MigrationClient with Logging {
+class ZkMigrationClient(
+  zkClient: KafkaZkClient,
+  zkConfigEncoder: PasswordEncoder
+) extends MigrationClient with Logging {
 
   override def getOrCreateMigrationRecoveryState(initialState: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {
     zkClient.createTopLevelPaths()
@@ -135,20 +138,26 @@ class ZkMigrationClient(zkClient: KafkaZkClient) extends MigrationClient with Lo
   }
 
   def migrateBrokerConfigs(recordConsumer: Consumer[util.List[ApiMessageAndVersion]]): Unit = {
-    val brokerEntities = zkClient.getAllEntitiesWithConfig(ConfigType.Broker)
     val batch = new util.ArrayList[ApiMessageAndVersion]()
+
+    val brokerEntities = zkClient.getAllEntitiesWithConfig(ConfigType.Broker)
     zkClient.getEntitiesConfigs(ConfigType.Broker, brokerEntities.toSet).foreach { case (broker, props) =>
       val brokerResource = if (broker == ConfigEntityName.Default) {
         ""
       } else {
         broker
       }
-      props.forEach { case (key: Object, value: Object) =>
+      props.asScala.foreach { case (key, value) =>
+        val newValue = if (DynamicBrokerConfig.isPasswordConfig(key))
+          zkConfigEncoder.decode(value).value
+        else
+          value
+
         batch.add(new ApiMessageAndVersion(new ConfigRecord()
           .setResourceType(ConfigResource.Type.BROKER.id)
           .setResourceName(brokerResource)
-          .setName(key.toString)
-          .setValue(value.toString), 0.toShort))
+          .setName(key)
+          .setValue(newValue), 0.toShort))
       }
     }
     if (!batch.isEmpty) {
diff --git a/core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala b/core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala
index 231bf01c91..e1c9ffbcdd 100644
--- a/core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala
+++ b/core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala
@@ -22,7 +22,7 @@ import kafka.test.annotation.{ClusterConfigProperty, ClusterTest, Type}
 import kafka.test.junit.ClusterTestExtensions
 import kafka.test.junit.ZkClusterInvocationContext.ZkClusterInstance
 import kafka.testkit.{KafkaClusterTestKit, TestKitNodes}
-import kafka.utils.TestUtils
+import kafka.utils.{PasswordEncoder, TestUtils}
 import org.apache.kafka.clients.admin.{Admin, AlterClientQuotasResult, AlterConfigOp, AlterConfigsResult, ConfigEntry, NewTopic}
 import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
 import org.apache.kafka.common.Uuid
@@ -88,7 +88,18 @@ class ZkMigrationIntegrationTest {
     admin.alterClientQuotas(quotas)
 
     val zkClient = clusterInstance.asInstanceOf[ZkClusterInstance].getUnderlying().zkClient
-    val migrationClient = new ZkMigrationClient(zkClient)
+    val kafkaConfig = clusterInstance.asInstanceOf[ZkClusterInstance].getUnderlying.servers.head.config
+    val zkConfigEncoder = kafkaConfig.passwordEncoderSecret match {
+      case Some(secret) =>
+        PasswordEncoder.encrypting(secret,
+          kafkaConfig.passwordEncoderKeyFactoryAlgorithm,
+          kafkaConfig.passwordEncoderCipherAlgorithm,
+          kafkaConfig.passwordEncoderKeyLength,
+          kafkaConfig.passwordEncoderIterations)
+      case None => PasswordEncoder.noop()
+    }
+
+    val migrationClient = new ZkMigrationClient(zkClient, zkConfigEncoder)
     var migrationState = migrationClient.getOrCreateMigrationRecoveryState(ZkMigrationLeadershipState.EMPTY)
     migrationState = migrationState.withNewKRaftController(3000, 42)
     migrationState = migrationClient.claimControllerLeadership(migrationState)
diff --git a/core/src/test/scala/unit/kafka/zk/ZkMigrationClientTest.scala b/core/src/test/scala/unit/kafka/zk/ZkMigrationClientTest.scala
index 77e133a703..3bf4ec9d13 100644
--- a/core/src/test/scala/unit/kafka/zk/ZkMigrationClientTest.scala
+++ b/core/src/test/scala/unit/kafka/zk/ZkMigrationClientTest.scala
@@ -19,10 +19,12 @@ package kafka.zk
 import kafka.api.LeaderAndIsr
 import kafka.controller.LeaderIsrAndControllerEpoch
 import kafka.coordinator.transaction.ProducerIdManager
-import kafka.server.{ConfigType, QuorumTestHarness, ZkAdminManager}
+import kafka.server.{ConfigType, KafkaConfig, QuorumTestHarness, ZkAdminManager}
+import kafka.utils.PasswordEncoder
 import org.apache.kafka.common.config.{ConfigResource, TopicConfig}
 import org.apache.kafka.common.{TopicPartition, Uuid}
 import org.apache.kafka.common.config.internals.QuotaConfigs
+import org.apache.kafka.common.config.types.Password
 import org.apache.kafka.common.errors.ControllerMovedException
 import org.apache.kafka.common.metadata.{ConfigRecord, MetadataRecordType, ProducerIdsRecord}
 import org.apache.kafka.common.quota.ClientQuotaEntity
@@ -49,12 +51,25 @@ class ZkMigrationClientTest extends QuorumTestHarness {
 
   private var migrationState: ZkMigrationLeadershipState = _
 
+  private val SECRET = "secret"
+
+  private val encoder: PasswordEncoder = {
+    val encoderProps = new Properties()
+    encoderProps.put(KafkaConfig.ZkConnectProp, "localhost:1234") // Get around the config validation
+    encoderProps.put(KafkaConfig.PasswordEncoderSecretProp, SECRET) // Zk secret to encrypt the
+    val encoderConfig = new KafkaConfig(encoderProps)
+    PasswordEncoder.encrypting(encoderConfig.passwordEncoderSecret.get,
+      encoderConfig.passwordEncoderKeyFactoryAlgorithm,
+      encoderConfig.passwordEncoderCipherAlgorithm,
+      encoderConfig.passwordEncoderKeyLength,
+      encoderConfig.passwordEncoderIterations)
+  }
+
   @BeforeEach
   override def setUp(testInfo: TestInfo): Unit = {
     super.setUp(testInfo)
     zkClient.createControllerEpochRaw(1)
-
-    migrationClient = new ZkMigrationClient(zkClient)
+    migrationClient = new ZkMigrationClient(zkClient, encoder)
     migrationState = initialMigrationState
     migrationState = migrationClient.getOrCreateMigrationRecoveryState(migrationState)
    }
@@ -74,6 +89,37 @@ class ZkMigrationClientTest extends QuorumTestHarness {
     assertEquals(0, batches.size())
   }
 
+  @Test
+  def testMigrationBrokerConfigs(): Unit = {
+    val brokers = new java.util.ArrayList[Integer]()
+    val batches = new java.util.ArrayList[java.util.List[ApiMessageAndVersion]]()
+
+    // Create some configs and persist in Zk.
+    val props = new Properties()
+    props.put(KafkaConfig.DefaultReplicationFactorProp, "1") // normal config
+    props.put(KafkaConfig.SslKeystorePasswordProp, encoder.encode(new Password(SECRET))) // sensitive config
+    zkClient.setOrCreateEntityConfigs(ConfigType.Broker, "1", props)
+
+    migrationClient.readAllMetadata(batch => batches.add(batch), brokerId => brokers.add(brokerId))
+    assertEquals(0, brokers.size())
+    assertEquals(1, batches.size())
+    assertEquals(2, batches.get(0).size)
+
+    batches.get(0).forEach(record => {
+      val message = record.message().asInstanceOf[ConfigRecord]
+      val name = message.name
+      val value = message.value
+
+      assertTrue(props.containsKey(name))
+      // If the config is senstive, compare it to the decoded value.
+      if (name == KafkaConfig.SslKeystorePasswordProp) {
+        assertEquals(SECRET, value)
+      } else {
+        assertEquals(props.getProperty(name), value)
+      }
+    })
+  }
+
   @Test
   def testEmptyWrite(): Unit = {
     val (zkVersion, responses) = zkClient.retryMigrationRequestsUntilConnected(Seq(), migrationState)
