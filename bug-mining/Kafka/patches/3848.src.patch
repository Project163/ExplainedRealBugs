diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java
index 2e00e34ebd..fe9a3c6304 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java
@@ -68,6 +68,7 @@ public abstract class Cast<R extends ConnectRecord<R>> implements Transformation
                     + "or value (<code>" + Value.class.getName() + "</code>).";
 
     public static final String SPEC_CONFIG = "spec";
+    public static final String REPLACE_NULL_WITH_DEFAULT_CONFIG = "replace.null.with.default";
 
     public static final ConfigDef CONFIG_DEF = new ConfigDef()
             .define(SPEC_CONFIG, ConfigDef.Type.LIST, ConfigDef.NO_DEFAULT_VALUE, new ConfigDef.Validator() {
@@ -89,7 +90,12 @@ public abstract class Cast<R extends ConnectRecord<R>> implements Transformation
             ConfigDef.Importance.HIGH,
             "List of fields and the type to cast them to of the form field1:type,field2:type to cast fields of "
                     + "Maps or Structs. A single type to cast the entire value. Valid types are int8, int16, int32, "
-                    + "int64, float32, float64, boolean, and string. Note that binary fields can only be cast to string.");
+                    + "int64, float32, float64, boolean, and string. Note that binary fields can only be cast to string.")
+            .define(REPLACE_NULL_WITH_DEFAULT_CONFIG,
+                    ConfigDef.Type.BOOLEAN,
+                    true,
+                    ConfigDef.Importance.MEDIUM,
+                    "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.");
 
     private static final String PURPOSE = "cast types";
 
@@ -112,6 +118,7 @@ public abstract class Cast<R extends ConnectRecord<R>> implements Transformation
     private Map<String, Schema.Type> casts;
     private Schema.Type wholeValueCastType;
     private Cache<Schema, Schema> schemaUpdateCache;
+    private boolean replaceNullWithDefault;
 
     @Override
     public String version() {
@@ -124,6 +131,7 @@ public abstract class Cast<R extends ConnectRecord<R>> implements Transformation
         casts = parseFieldTypes(config.getList(SPEC_CONFIG));
         wholeValueCastType = casts.get(WHOLE_VALUE_CAST);
         schemaUpdateCache = new SynchronizedCache<>(new LRUCache<>(16));
+        replaceNullWithDefault = config.getBoolean(REPLACE_NULL_WITH_DEFAULT_CONFIG);
     }
 
     @Override
@@ -176,7 +184,7 @@ public abstract class Cast<R extends ConnectRecord<R>> implements Transformation
 
         final Struct updatedValue = new Struct(updatedSchema);
         for (Field field : value.schema().fields()) {
-            final Object origFieldValue = value.get(field);
+            final Object origFieldValue = getFieldValue(value, field);
             final Schema.Type targetType = casts.get(field.name());
             final Object newFieldValue = targetType != null ? castValueToType(field.schema(), origFieldValue, targetType) : origFieldValue;
             log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
@@ -185,6 +193,13 @@ public abstract class Cast<R extends ConnectRecord<R>> implements Transformation
         return newRecord(record, updatedSchema, updatedValue);
     }
 
+    private Object getFieldValue(Struct value, Field field) {
+        if (replaceNullWithDefault) {
+            return value.get(field);
+        }
+        return value.getWithoutDefault(field.name());
+    }
+
     private Schema getOrBuildSchema(Schema valueSchema) {
         Schema updatedSchema = schemaUpdateCache.get(valueSchema);
         if (updatedSchema != null)
diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ExtractField.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ExtractField.java
index ebfded643d..89f6d28c5e 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ExtractField.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ExtractField.java
@@ -41,21 +41,28 @@ public abstract class ExtractField<R extends ConnectRecord<R>> implements Transf
                     + "or value (<code>" + Value.class.getName() + "</code>).";
 
     private static final String FIELD_CONFIG = "field";
+    private static final String REPLACE_NULL_WITH_DEFAULT_CONFIG = "replace.null.with.default";
 
     public static final ConfigDef CONFIG_DEF = FieldSyntaxVersion.appendConfigTo(
-        new ConfigDef()
-            .define(
-                FIELD_CONFIG,
-                ConfigDef.Type.STRING,
-                ConfigDef.NO_DEFAULT_VALUE,
-                ConfigDef.Importance.MEDIUM,
-                "Field name to extract."
-            ));
+            new ConfigDef()
+                    .define(
+                            FIELD_CONFIG,
+                            ConfigDef.Type.STRING,
+                            ConfigDef.NO_DEFAULT_VALUE,
+                            ConfigDef.Importance.MEDIUM,
+                            "Field name to extract."
+                    )
+                    .define(REPLACE_NULL_WITH_DEFAULT_CONFIG,
+                            ConfigDef.Type.BOOLEAN,
+                            true,
+                            ConfigDef.Importance.MEDIUM,
+                            "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used."));
 
     private static final String PURPOSE = "field extraction";
 
     private SingleFieldPath fieldPath;
     private String originalPath;
+    private boolean replaceNullWithDefault;
 
     @Override
     public String version() {
@@ -67,6 +74,7 @@ public abstract class ExtractField<R extends ConnectRecord<R>> implements Transf
         final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);
         originalPath = config.getString(FIELD_CONFIG);
         fieldPath = new SingleFieldPath(originalPath, FieldSyntaxVersion.fromConfig(config));
+        replaceNullWithDefault = config.getBoolean(REPLACE_NULL_WITH_DEFAULT_CONFIG);
     }
 
     @Override
@@ -83,7 +91,7 @@ public abstract class ExtractField<R extends ConnectRecord<R>> implements Transf
                 throw new IllegalArgumentException("Unknown field: " + originalPath);
             }
 
-            return newRecord(record, field.schema(), value == null ? null : fieldPath.valueFrom(value));
+            return newRecord(record, field.schema(), value == null ? null : fieldPath.valueFrom(value, replaceNullWithDefault));
         }
     }
 
diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java
index 992414ac41..1773233394 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java
@@ -49,6 +49,7 @@ public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transfor
     public static final String OPERATION_FIELD = "operation";
     private static final String MOVE_OPERATION = "move";
     private static final String COPY_OPERATION = "copy";
+    public static final String REPLACE_NULL_WITH_DEFAULT_FIELD = "replace.null.with.default";
 
     public static final String OVERVIEW_DOC =
             "Moves or copies fields in the key/value of a record into that record's headers. " +
@@ -70,7 +71,9 @@ public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transfor
             .define(OPERATION_FIELD, ConfigDef.Type.STRING, NO_DEFAULT_VALUE,
                     ConfigDef.ValidString.in(MOVE_OPERATION, COPY_OPERATION), ConfigDef.Importance.HIGH,
                     "Either <code>move</code> if the fields are to be moved to the headers (removed from the key/value), " +
-                            "or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).");
+                            "or <code>copy</code> if the fields are to be copied to the headers (retained in the key/value).")
+            .define(REPLACE_NULL_WITH_DEFAULT_FIELD, ConfigDef.Type.BOOLEAN, true, ConfigDef.Importance.MEDIUM,
+                    "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.");
 
     private final Cache<Schema, Schema> moveSchemaCache = new SynchronizedCache<>(new LRUCache<>(16));
     enum Operation {
@@ -105,6 +108,8 @@ public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transfor
 
     private Operation operation;
 
+    private boolean replaceNullWithDefault;
+
     @Override
     public R apply(R record) {
         Object operatingValue = operatingValue(record);
@@ -131,7 +136,7 @@ public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transfor
             updatedSchema = moveSchema(operatingSchema);
             updatedValue = new Struct(updatedSchema);
             for (Field field : updatedSchema.fields()) {
-                updatedValue.put(field, value.get(field.name()));
+                updatedValue.put(field, getFieldValue(value, field));
             }
         } else {
             updatedSchema = operatingSchema;
@@ -140,7 +145,7 @@ public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transfor
         for (int i = 0; i < fields.size(); i++) {
             String fieldName = fields.get(i);
             String headerName = headers.get(i);
-            Object fieldValue = value.schema().field(fieldName) != null ? value.get(fieldName) : null;
+            Object fieldValue = value.schema().field(fieldName) != null ? getFieldValue(value, value.schema().field(fieldName)) : null;
             Schema fieldSchema = operatingSchema.field(fieldName).schema();
             updatedHeaders.add(headerName, fieldValue, fieldSchema);
         }
@@ -178,6 +183,13 @@ public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transfor
         return newRecord(record, null, updatedValue, updatedHeaders);
     }
 
+    private Object getFieldValue(Struct value, Field field) {
+        if (replaceNullWithDefault) {
+            return value.get(field.name());
+        }
+        return value.getWithoutDefault(field.name());
+    }
+
     protected abstract Object operatingValue(R record);
     protected abstract Schema operatingSchema(R record);
     protected abstract R newRecord(R record, Schema updatedSchema, Object updatedValue, Iterable<Header> updatedHeaders);
@@ -240,5 +252,6 @@ public abstract class HeaderFrom<R extends ConnectRecord<R>> implements Transfor
                     FIELDS_FIELD, HEADERS_FIELD));
         }
         operation = Operation.fromName(config.getString(OPERATION_FIELD));
+        replaceNullWithDefault = config.getBoolean(REPLACE_NULL_WITH_DEFAULT_FIELD);
     }
 }
diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertField.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertField.java
index ddc29c2e2c..695dc2ab55 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertField.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertField.java
@@ -54,6 +54,7 @@ public abstract class InsertField<R extends ConnectRecord<R>> implements Transfo
         String TIMESTAMP_FIELD = "timestamp.field";
         String STATIC_FIELD = "static.field";
         String STATIC_VALUE = "static.value";
+        String REPLACE_NULL_WITH_DEFAULT = "replace.null.with.default";
     }
 
     private static final String OPTIONALITY_DOC = "Suffix with <code>!</code> to make this a required field, or <code>?</code> to keep it optional (the default).";
@@ -70,7 +71,9 @@ public abstract class InsertField<R extends ConnectRecord<R>> implements Transfo
             .define(ConfigName.STATIC_FIELD, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM,
                     "Field name for static data field. " + OPTIONALITY_DOC)
             .define(ConfigName.STATIC_VALUE, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM,
-                    "Static field value, if field name configured.");
+                    "Static field value, if field name configured.")
+            .define(ConfigName.REPLACE_NULL_WITH_DEFAULT, ConfigDef.Type.BOOLEAN, true, ConfigDef.Importance.MEDIUM,
+                    "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.");
 
     private static final String PURPOSE = "field insertion";
 
@@ -103,6 +106,7 @@ public abstract class InsertField<R extends ConnectRecord<R>> implements Transfo
     private InsertionSpec timestampField;
     private InsertionSpec staticField;
     private String staticValue;
+    private boolean replaceNullWithDefault;
 
     private Cache<Schema, Schema> schemaUpdateCache;
 
@@ -120,6 +124,7 @@ public abstract class InsertField<R extends ConnectRecord<R>> implements Transfo
         timestampField = InsertionSpec.parse(config.getString(ConfigName.TIMESTAMP_FIELD));
         staticField = InsertionSpec.parse(config.getString(ConfigName.STATIC_FIELD));
         staticValue = config.getString(ConfigName.STATIC_VALUE);
+        replaceNullWithDefault = config.getBoolean(ConfigName.REPLACE_NULL_WITH_DEFAULT);
 
         if (topicField == null && partitionField == null && offsetField == null && timestampField == null && staticField == null) {
             throw new ConfigException("No field insertion configured");
@@ -179,7 +184,7 @@ public abstract class InsertField<R extends ConnectRecord<R>> implements Transfo
         final Struct updatedValue = new Struct(updatedSchema);
 
         for (Field field : value.schema().fields()) {
-            updatedValue.put(field.name(), value.get(field));
+            updatedValue.put(field.name(), getFieldValue(value, field));
         }
 
         if (topicField != null) {
@@ -227,6 +232,13 @@ public abstract class InsertField<R extends ConnectRecord<R>> implements Transfo
         return builder.build();
     }
 
+    private Object getFieldValue(Struct value, Field field) {
+        if (replaceNullWithDefault) {
+            return value.get(field);
+        }
+        return value.getWithoutDefault(field.name());
+    }
+
     @Override
     public void close() {
         schemaUpdateCache = null;
diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/MaskField.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/MaskField.java
index abca5917d3..c3a45d9170 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/MaskField.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/MaskField.java
@@ -52,13 +52,16 @@ public abstract class MaskField<R extends ConnectRecord<R>> implements Transform
 
     private static final String FIELDS_CONFIG = "fields";
     private static final String REPLACEMENT_CONFIG = "replacement";
+    private static final String REPLACE_NULL_WITH_DEFAULT_CONFIG = "replace.null.with.default";
 
     public static final ConfigDef CONFIG_DEF = new ConfigDef()
             .define(FIELDS_CONFIG, ConfigDef.Type.LIST, ConfigDef.NO_DEFAULT_VALUE, new NonEmptyListValidator(),
                     ConfigDef.Importance.HIGH, "Names of fields to mask.")
             .define(REPLACEMENT_CONFIG, ConfigDef.Type.STRING, null, new ConfigDef.NonEmptyString(),
                     ConfigDef.Importance.LOW, "Custom value replacement, that will be applied to all"
-                            + " 'fields' values (numeric or non-empty string values only).");
+                            + " 'fields' values (numeric or non-empty string values only).")
+            .define(REPLACE_NULL_WITH_DEFAULT_CONFIG, ConfigDef.Type.BOOLEAN, true, ConfigDef.Importance.MEDIUM,
+                    "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.");
 
     private static final String PURPOSE = "mask fields";
 
@@ -91,6 +94,7 @@ public abstract class MaskField<R extends ConnectRecord<R>> implements Transform
 
     private Set<String> maskedFields;
     private String replacement;
+    private boolean replaceNullWithDefault;
 
     @Override
     public String version() {
@@ -102,6 +106,7 @@ public abstract class MaskField<R extends ConnectRecord<R>> implements Transform
         final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);
         maskedFields = new HashSet<>(config.getList(FIELDS_CONFIG));
         replacement = config.getString(REPLACEMENT_CONFIG);
+        replaceNullWithDefault = config.getBoolean(REPLACE_NULL_WITH_DEFAULT_CONFIG);
     }
 
     @Override
@@ -126,12 +131,19 @@ public abstract class MaskField<R extends ConnectRecord<R>> implements Transform
         final Struct value = requireStruct(operatingValue(record), PURPOSE);
         final Struct updatedValue = new Struct(value.schema());
         for (Field field : value.schema().fields()) {
-            final Object origFieldValue = value.get(field);
+            final Object origFieldValue = getFieldValue(value, field);
             updatedValue.put(field, maskedFields.contains(field.name()) ? masked(origFieldValue) : origFieldValue);
         }
         return newRecord(record, updatedValue);
     }
 
+    private Object getFieldValue(Struct value, Field field) {
+        if (replaceNullWithDefault) {
+            return value.get(field);
+        }
+        return value.getWithoutDefault(field.name());
+    }
+
     private Object masked(Object value) {
         if (value == null) {
             return null;
diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/SetSchemaMetadata.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/SetSchemaMetadata.java
index 84505d5939..86ec11f5fd 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/SetSchemaMetadata.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/SetSchemaMetadata.java
@@ -44,14 +44,18 @@ public abstract class SetSchemaMetadata<R extends ConnectRecord<R>> implements T
     private interface ConfigName {
         String SCHEMA_NAME = "schema.name";
         String SCHEMA_VERSION = "schema.version";
+        String REPLACE_NULL_WITH_DEFAULT = "replace.null.with.default";
     }
 
     public static final ConfigDef CONFIG_DEF = new ConfigDef()
             .define(ConfigName.SCHEMA_NAME, ConfigDef.Type.STRING, null, ConfigDef.Importance.HIGH, "Schema name to set.")
-            .define(ConfigName.SCHEMA_VERSION, ConfigDef.Type.INT, null, ConfigDef.Importance.HIGH, "Schema version to set.");
+            .define(ConfigName.SCHEMA_VERSION, ConfigDef.Type.INT, null, ConfigDef.Importance.HIGH, "Schema version to set.")
+            .define(ConfigName.REPLACE_NULL_WITH_DEFAULT, ConfigDef.Type.BOOLEAN, true, ConfigDef.Importance.MEDIUM,
+                    "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.");
 
     private String schemaName;
     private Integer schemaVersion;
+    private boolean replaceNullWithDefault;
 
     @Override
     public String version() {
@@ -63,6 +67,7 @@ public abstract class SetSchemaMetadata<R extends ConnectRecord<R>> implements T
         final SimpleConfig config = new SimpleConfig(CONFIG_DEF, configs);
         schemaName = config.getString(ConfigName.SCHEMA_NAME);
         schemaVersion = config.getInt(ConfigName.SCHEMA_VERSION);
+        replaceNullWithDefault = config.getBoolean(ConfigName.REPLACE_NULL_WITH_DEFAULT);
 
         if (schemaName == null && schemaVersion == null) {
             throw new ConfigException("Neither schema name nor version configured");
@@ -96,6 +101,13 @@ public abstract class SetSchemaMetadata<R extends ConnectRecord<R>> implements T
         return newRecord(record, updatedSchema);
     }
 
+    private Object getFieldValue(Struct value, Field field) {
+        if (replaceNullWithDefault) {
+            return value.get(field);
+        }
+        return value.getWithoutDefault(field.name());
+    }
+
     @Override
     public ConfigDef config() {
         return CONFIG_DEF;
@@ -166,13 +178,13 @@ public abstract class SetSchemaMetadata<R extends ConnectRecord<R>> implements T
      * @return the original key or value object if it does not reference the old schema, or
      * a copy of the key or value object with updated references to the new schema.
      */
-    protected static Object updateSchemaIn(Object keyOrValue, Schema updatedSchema) {
+    protected Object updateSchemaIn(Object keyOrValue, Schema updatedSchema) {
         if (keyOrValue instanceof Struct) {
             Struct origStruct = (Struct) keyOrValue;
             Struct newStruct = new Struct(updatedSchema);
             for (Field field : updatedSchema.fields()) {
                 // assume both schemas have exact same fields with same names and schemas ...
-                newStruct.put(field, origStruct.get(field));
+                newStruct.put(field, getFieldValue(origStruct, field));
             }
             return newStruct;
         }
diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampConverter.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampConverter.java
index 0ef9292686..957cc3e1fe 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampConverter.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampConverter.java
@@ -68,6 +68,8 @@ public abstract class TimestampConverter<R extends ConnectRecord<R>> implements
     public static final String UNIX_PRECISION_CONFIG = "unix.precision";
     private static final String UNIX_PRECISION_DEFAULT = "milliseconds";
 
+    public static final String REPLACE_NULL_WITH_DEFAULT_CONFIG = "replace.null.with.default";
+
     private static final String PURPOSE = "converting timestamp formats";
 
     private static final String TYPE_STRING = "string";
@@ -104,7 +106,10 @@ public abstract class TimestampConverter<R extends ConnectRecord<R>> implements
                     ConfigDef.Importance.LOW,
                     "The desired Unix precision for the timestamp: seconds, milliseconds, microseconds, or nanoseconds. " +
                             "Used to generate the output when type=unix or used to parse the input if the input is a Long." +
-                            "Note: This SMT will cause precision loss during conversions from, and to, values with sub-millisecond components.");
+                            "Note: This SMT will cause precision loss during conversions from, and to, values with sub-millisecond components.")
+            .define(REPLACE_NULL_WITH_DEFAULT_CONFIG, ConfigDef.Type.BOOLEAN, true, ConfigDef.Importance.MEDIUM,
+                    "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.");
+
 
     private interface TimestampTranslator {
         /**
@@ -287,7 +292,7 @@ public abstract class TimestampConverter<R extends ConnectRecord<R>> implements
     }
     private Config config;
     private Cache<Schema, Schema> schemaUpdateCache;
-
+    private boolean replaceNullWithDefault;
 
     @Override
     public void configure(Map<String, ?> configs) {
@@ -297,6 +302,7 @@ public abstract class TimestampConverter<R extends ConnectRecord<R>> implements
         String formatPattern = simpleConfig.getString(FORMAT_CONFIG);
         final String unixPrecision = simpleConfig.getString(UNIX_PRECISION_CONFIG);
         schemaUpdateCache = new SynchronizedCache<>(new LRUCache<>(16));
+        replaceNullWithDefault = simpleConfig.getBoolean(REPLACE_NULL_WITH_DEFAULT_CONFIG);
 
         if (type.equals(TYPE_STRING) && Utils.isBlank(formatPattern)) {
             throw new ConfigException("TimestampConverter requires format option to be specified when using string timestamps");
@@ -415,15 +421,22 @@ public abstract class TimestampConverter<R extends ConnectRecord<R>> implements
         for (Field field : value.schema().fields()) {
             final Object updatedFieldValue;
             if (field.name().equals(config.field)) {
-                updatedFieldValue = convertTimestamp(value.get(field), timestampTypeFromSchema(field.schema()));
+                updatedFieldValue = convertTimestamp(getFieldValue(value, field), timestampTypeFromSchema(field.schema()));
             } else {
-                updatedFieldValue = value.get(field);
+                updatedFieldValue = getFieldValue(value, field);
             }
             updatedValue.put(field.name(), updatedFieldValue);
         }
         return updatedValue;
     }
 
+    private Object getFieldValue(Struct value, Field field) {
+        if (replaceNullWithDefault) {
+            return value.get(field);
+        }
+        return value.getWithoutDefault(field.name());
+    }
+
     private R applySchemaless(R record) {
         Object rawValue = operatingValue(record);
         if (rawValue == null || config.field.isEmpty()) {
diff --git a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/field/SingleFieldPath.java b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/field/SingleFieldPath.java
index 62b14336cd..326a844025 100644
--- a/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/field/SingleFieldPath.java
+++ b/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/field/SingleFieldPath.java
@@ -167,6 +167,11 @@ public class SingleFieldPath {
      * If object is not found, then {@code null} is returned.
      */
     public Object valueFrom(Struct struct) {
+
+        return valueFrom(struct, true);
+    }
+
+    public Object valueFrom(Struct struct, boolean withDefault) {
         if (struct == null) return null;
 
         Struct current = struct;
@@ -180,8 +185,9 @@ public class SingleFieldPath {
             if (current == null) return null;
         }
 
-        if (current.schema().field(lastStep()) != null) {
-            return current.get(lastStep());
+        String lastStep = lastStep();
+        if (current.schema().field(lastStep) != null) {
+            return withDefault ? current.get(lastStep) : current.getWithoutDefault(lastStep);
         } else {
             return null;
         }
diff --git a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/CastTest.java b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/CastTest.java
index a70fac1d1d..e79e163b46 100644
--- a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/CastTest.java
+++ b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/CastTest.java
@@ -32,6 +32,9 @@ import org.apache.kafka.connect.source.SourceRecord;
 
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
 
 import java.math.BigDecimal;
 import java.nio.ByteBuffer;
@@ -42,6 +45,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
+import java.util.stream.Stream;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertNull;
@@ -54,6 +58,12 @@ public class CastTest {
     private static final long MILLIS_PER_HOUR = TimeUnit.HOURS.toMillis(1);
     private static final long MILLIS_PER_DAY = TimeUnit.DAYS.toMillis(1);
 
+    public static Stream<Arguments> data() {
+        return Stream.of(
+                Arguments.of(false, null),
+                Arguments.of(true, "10")
+        );
+    }
     @AfterEach
     public void teardown() {
         xformKey.close();
@@ -100,6 +110,23 @@ public class CastTest {
         assertEquals(original, transformed);
     }
 
+    @ParameterizedTest
+    @MethodSource("data")
+    public void castFieldWithDefaultValueRecordWithSchema(boolean replaceNullWithDefault, Object expectedValue) {
+        Map<String, Object> configs = new HashMap<>();
+        configs.put(Cast.SPEC_CONFIG, "magic:string");
+        configs.put(Cast.REPLACE_NULL_WITH_DEFAULT_CONFIG, replaceNullWithDefault);
+        xformValue.configure(configs);
+        Schema structSchema = SchemaBuilder.struct()
+                .field("magic", SchemaBuilder.int32().optional().defaultValue(10).build()).build();
+        SourceRecord original = new SourceRecord(null, null, "topic", 0,
+                Schema.STRING_SCHEMA, "key", structSchema, new Struct(structSchema).put("magic", null));
+        SourceRecord transformed = xformValue.apply(original);
+
+        assertEquals(Type.STRING, transformed.valueSchema().field("magic").schema().type());
+        assertEquals(expectedValue, ((Struct) transformed.value()).getWithoutDefault("magic"));
+    }
+
     @Test
     public void castNullValueRecordSchemaless() {
         xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "foo:int64"));
diff --git a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ExtractFieldTest.java b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ExtractFieldTest.java
index 2ac2f91829..ff11ffe4e8 100644
--- a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ExtractFieldTest.java
+++ b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ExtractFieldTest.java
@@ -25,29 +25,42 @@ import org.apache.kafka.connect.transforms.field.FieldSyntaxVersion;
 
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
 
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.stream.Stream;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertNull;
 import static org.junit.jupiter.api.Assertions.fail;
 
 public class ExtractFieldTest {
-    private final ExtractField<SinkRecord> xform = new ExtractField.Key<>();
+    private final ExtractField<SinkRecord> xformKey = new ExtractField.Key<>();
+    private final ExtractField<SinkRecord> xformValue = new ExtractField.Value<>();
+
+    public static Stream<Arguments> data() {
+        return Stream.of(
+                Arguments.of(false, null),
+                Arguments.of(true, 42)
+        );
+    }
 
     @AfterEach
     public void teardown() {
-        xform.close();
+        xformKey.close();
+        xformValue.close();
     }
 
     @Test
     public void schemaless() {
-        xform.configure(Collections.singletonMap("field", "magic"));
+        xformKey.configure(Collections.singletonMap("field", "magic"));
 
         final SinkRecord record = new SinkRecord("test", 0, null, Collections.singletonMap("magic", 42), null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertNull(transformedRecord.keySchema());
         assertEquals(42, transformedRecord.key());
@@ -58,11 +71,11 @@ public class ExtractFieldTest {
         Map<String, String> configs = new HashMap<>();
         configs.put(FieldSyntaxVersion.FIELD_SYNTAX_VERSION_CONFIG, FieldSyntaxVersion.V2.name());
         configs.put("field", "magic.foo");
-        xform.configure(configs);
+        xformKey.configure(configs);
 
         final Map<String, Object> key = Collections.singletonMap("magic", Collections.singletonMap("foo", 42));
         final SinkRecord record = new SinkRecord("test", 0, null, key, null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertNull(transformedRecord.keySchema());
         assertEquals(42, transformedRecord.key());
@@ -70,11 +83,11 @@ public class ExtractFieldTest {
 
     @Test
     public void nullSchemaless() {
-        xform.configure(Collections.singletonMap("field", "magic"));
+        xformKey.configure(Collections.singletonMap("field", "magic"));
 
         final Map<String, Object> key = null;
         final SinkRecord record = new SinkRecord("test", 0, null, key, null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertNull(transformedRecord.keySchema());
         assertNull(transformedRecord.key());
@@ -82,12 +95,12 @@ public class ExtractFieldTest {
 
     @Test
     public void withSchema() {
-        xform.configure(Collections.singletonMap("field", "magic"));
+        xformKey.configure(Collections.singletonMap("field", "magic"));
 
         final Schema keySchema = SchemaBuilder.struct().field("magic", Schema.INT32_SCHEMA).build();
         final Struct key = new Struct(keySchema).put("magic", 42);
         final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertEquals(Schema.INT32_SCHEMA, transformedRecord.keySchema());
         assertEquals(42, transformedRecord.key());
@@ -98,13 +111,13 @@ public class ExtractFieldTest {
         Map<String, String> configs = new HashMap<>();
         configs.put(FieldSyntaxVersion.FIELD_SYNTAX_VERSION_CONFIG, FieldSyntaxVersion.V2.name());
         configs.put("field", "magic.foo");
-        xform.configure(configs);
+        xformKey.configure(configs);
 
         final Schema fooSchema = SchemaBuilder.struct().field("foo", Schema.INT32_SCHEMA).build();
         final Schema keySchema = SchemaBuilder.struct().field("magic", fooSchema).build();
         final Struct key = new Struct(keySchema).put("magic", new Struct(fooSchema).put("foo", 42));
         final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertEquals(Schema.INT32_SCHEMA, transformedRecord.keySchema());
         assertEquals(42, transformedRecord.key());
@@ -112,12 +125,12 @@ public class ExtractFieldTest {
 
     @Test
     public void testNullWithSchema() {
-        xform.configure(Collections.singletonMap("field", "magic"));
+        xformKey.configure(Collections.singletonMap("field", "magic"));
 
         final Schema keySchema = SchemaBuilder.struct().field("magic", Schema.INT32_SCHEMA).optional().build();
         final Struct key = null;
         final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertEquals(Schema.INT32_SCHEMA, transformedRecord.keySchema());
         assertNull(transformedRecord.key());
@@ -125,10 +138,10 @@ public class ExtractFieldTest {
 
     @Test
     public void nonExistentFieldSchemalessShouldReturnNull() {
-        xform.configure(Collections.singletonMap("field", "nonexistent"));
+        xformKey.configure(Collections.singletonMap("field", "nonexistent"));
 
         final SinkRecord record = new SinkRecord("test", 0, null, Collections.singletonMap("magic", 42), null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertNull(transformedRecord.keySchema());
         assertNull(transformedRecord.key());
@@ -139,11 +152,11 @@ public class ExtractFieldTest {
         Map<String, String> configs = new HashMap<>();
         configs.put(FieldSyntaxVersion.FIELD_SYNTAX_VERSION_CONFIG, FieldSyntaxVersion.V2.name());
         configs.put("field", "magic.nonexistent");
-        xform.configure(configs);
+        xformKey.configure(configs);
 
         final Map<String, Object> key = Collections.singletonMap("magic", Collections.singletonMap("foo", 42));
         final SinkRecord record = new SinkRecord("test", 0, null, key, null, null, 0);
-        final SinkRecord transformedRecord = xform.apply(record);
+        final SinkRecord transformedRecord = xformKey.apply(record);
 
         assertNull(transformedRecord.keySchema());
         assertNull(transformedRecord.key());
@@ -151,14 +164,14 @@ public class ExtractFieldTest {
 
     @Test
     public void nonExistentFieldWithSchemaShouldFail() {
-        xform.configure(Collections.singletonMap("field", "nonexistent"));
+        xformKey.configure(Collections.singletonMap("field", "nonexistent"));
 
         final Schema keySchema = SchemaBuilder.struct().field("magic", Schema.INT32_SCHEMA).build();
         final Struct key = new Struct(keySchema).put("magic", 42);
         final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);
 
         try {
-            xform.apply(record);
+            xformKey.apply(record);
             fail("Expected exception wasn't raised");
         } catch (IllegalArgumentException iae) {
             assertEquals("Unknown field: nonexistent", iae.getMessage());
@@ -170,7 +183,7 @@ public class ExtractFieldTest {
         Map<String, String> configs = new HashMap<>();
         configs.put(FieldSyntaxVersion.FIELD_SYNTAX_VERSION_CONFIG, FieldSyntaxVersion.V2.name());
         configs.put("field", "magic.nonexistent");
-        xform.configure(configs);
+        xformKey.configure(configs);
 
         final Schema fooSchema = SchemaBuilder.struct().field("foo", Schema.INT32_SCHEMA).build();
         final Schema keySchema = SchemaBuilder.struct().field("magic", fooSchema).build();
@@ -178,7 +191,7 @@ public class ExtractFieldTest {
         final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);
 
         try {
-            xform.apply(record);
+            xformKey.apply(record);
             fail("Expected exception wasn't raised");
         } catch (IllegalArgumentException iae) {
             assertEquals("Unknown field: magic.nonexistent", iae.getMessage());
@@ -187,7 +200,53 @@ public class ExtractFieldTest {
 
     @Test
     public void testExtractFieldVersionRetrievedFromAppInfoParser() {
-        assertEquals(AppInfoParser.getVersion(), xform.version());
+        assertEquals(AppInfoParser.getVersion(), xformKey.version());
+    }
+
+    @ParameterizedTest
+    @MethodSource("data")
+    public void testUnsetOptionalKey(boolean replaceNullWithDefault, Object expectedValue) {
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "optional_with_default");
+        config.put("replace.null.with.default", replaceNullWithDefault);
+
+        xformKey.configure(config);
+
+        final Schema keySchema = SchemaBuilder.struct()
+                .field("optional_with_default", SchemaBuilder.int32().optional().defaultValue(42).build())
+                .build();
+        final Struct key = new Struct(keySchema).put("optional_with_default", null);
+
+        final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);
+
+        final SinkRecord transformedRecord = xformKey.apply(record);
+        Integer extractedValue = (Integer) transformedRecord.key();
+
+        assertEquals(expectedValue, extractedValue);
+    }
+
+    @ParameterizedTest
+    @MethodSource("data")
+    public void testUnsetOptionalField(boolean replaceNullWithDefault, Object expectedValue) {
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "optional_with_default");
+        config.put("replace.null.with.default", replaceNullWithDefault);
+
+        xformValue.configure(config);
+
+        final Schema valueSchema = SchemaBuilder.struct()
+                .field("optional_with_default", SchemaBuilder.int32().optional().defaultValue(42).build())
+                .build();
+        final Struct value = new Struct(valueSchema).put("optional_with_default", null);
+
+        final SinkRecord record = new SinkRecord("test", 0, null, null, valueSchema, value, 0);
+
+        final SinkRecord transformedRecord = xformValue.apply(record);
+        Integer extractedValue = (Integer) transformedRecord.value();
+
+        assertEquals(expectedValue, extractedValue);
     }
 
 }
diff --git a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java
index 91efd3fb3e..da9e358432 100644
--- a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java
+++ b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java
@@ -149,7 +149,7 @@ public class HeaderFromTest {
                         .withField("field1", STRING_SCHEMA, "field1-value")
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("header1", STRING_SCHEMA, "existing-value"),
-                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.COPY,
+                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.COPY, true,
                     new RecordBuilder()
                         .withField("field1", STRING_SCHEMA, "field1-value")
                         .withField("field2", STRING_SCHEMA, "field2-value")
@@ -164,7 +164,7 @@ public class HeaderFromTest {
                         .withField("field1", STRING_SCHEMA, "field1-value")
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("header1", STRING_SCHEMA, "existing-value"),
-                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.MOVE,
+                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.MOVE, true,
                     new RecordBuilder()
                         // field1 got moved
                         .withField("field2", STRING_SCHEMA, "field2-value")
@@ -179,7 +179,7 @@ public class HeaderFromTest {
                         .withField("field1", STRING_SCHEMA, "field1-value")
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("inserted1", STRING_SCHEMA, "existing-value"),
-                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.COPY,
+                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.COPY, true,
                     new RecordBuilder()
                         .withField("field1", STRING_SCHEMA, "field1-value")
                         .withField("field2", STRING_SCHEMA, "field2-value")
@@ -194,7 +194,7 @@ public class HeaderFromTest {
                         .withField("field1", STRING_SCHEMA, "field1-value")
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("inserted1", STRING_SCHEMA, "existing-value"),
-                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.MOVE,
+                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.MOVE, true,
                     new RecordBuilder()
                         // field1 got moved
                         .withField("field2", STRING_SCHEMA, "field2-value")
@@ -211,7 +211,7 @@ public class HeaderFromTest {
                         .withField("field1", schema, struct)
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("header1", STRING_SCHEMA, "existing-value"),
-                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.COPY,
+                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.COPY, true,
                     new RecordBuilder()
                         .withField("field1", schema, struct)
                         .withField("field2", STRING_SCHEMA, "field2-value")
@@ -226,7 +226,7 @@ public class HeaderFromTest {
                         .withField("field1", schema, struct)
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("header1", STRING_SCHEMA, "existing-value"),
-                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.MOVE,
+                    singletonList("field1"), singletonList("inserted1"), HeaderFrom.Operation.MOVE, true,
                     new RecordBuilder()
                         // field1 got moved
                         .withField("field2", STRING_SCHEMA, "field2-value")
@@ -242,7 +242,7 @@ public class HeaderFromTest {
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("header1", STRING_SCHEMA, "existing-value"),
                     // two headers from the same field
-                    asList("field1", "field1"), asList("inserted1", "inserted2"), HeaderFrom.Operation.MOVE,
+                    asList("field1", "field1"), asList("inserted1", "inserted2"), HeaderFrom.Operation.MOVE, true,
                     new RecordBuilder()
                         // field1 got moved
                         .withField("field2", STRING_SCHEMA, "field2-value")
@@ -259,22 +259,53 @@ public class HeaderFromTest {
                         .withField("field2", STRING_SCHEMA, "field2-value")
                         .addHeader("header1", STRING_SCHEMA, "existing-value"),
                     // two headers from the same field
-                    asList("field1", "field2"), asList("inserted1", "inserted1"), HeaderFrom.Operation.MOVE,
+                    asList("field1", "field2"), asList("inserted1", "inserted1"), HeaderFrom.Operation.MOVE, true,
                     new RecordBuilder()
                         // field1 and field2 got moved
                         .addHeader("header1", STRING_SCHEMA, "existing-value")
                         .addHeader("inserted1", STRING_SCHEMA, "field1-value")
                         .addHeader("inserted1", STRING_SCHEMA, "field2-value")
                 ));
+            result.add(
+                    Arguments.of(
+                            "Copy null field without default",
+                            testKeyTransform,
+                            new RecordBuilder()
+                                    .withField("field1", SchemaBuilder.string().defaultValue("default").optional().build(), "field1-value")
+                                    .withField("field2", SchemaBuilder.string().defaultValue("default").optional().build(), null)
+                                    .addHeader("header1", STRING_SCHEMA, "existing-value"),
+                            asList("field1", "field2"), asList("inserted1", "inserted2"), HeaderFrom.Operation.COPY, false,
+                            new RecordBuilder()
+                                    .withField("field1", SchemaBuilder.string().defaultValue("default").optional().build(), "field1-value")
+                                    .withField("field2", SchemaBuilder.string().defaultValue("default").optional().build(), null)
+                                    .addHeader("header1", STRING_SCHEMA, "existing-value")
+                                    .addHeader("inserted1", SchemaBuilder.string().defaultValue("default").optional().build(), "field1-value")
+                                    .addHeader("inserted2", SchemaBuilder.string().defaultValue("default").optional().build(), null)
+                    ));
+            result.add(
+                    Arguments.of(
+                            "Move null field without default",
+                            testKeyTransform,
+                            new RecordBuilder()
+                                    .withField("field1", SchemaBuilder.string().defaultValue("default").optional().build(), "field1-value")
+                                    .withField("field2", SchemaBuilder.string().defaultValue("default").optional().build(), null)
+                                    .addHeader("header1", STRING_SCHEMA, "existing-value"),
+                            asList("field1", "field2"), asList("inserted1", "inserted2"), HeaderFrom.Operation.MOVE, false,
+                            new RecordBuilder()
+                                    .addHeader("header1", STRING_SCHEMA, "existing-value")
+                                    .addHeader("inserted1", SchemaBuilder.string().defaultValue("default").optional().build(), "field1-value")
+                                    .addHeader("inserted2", SchemaBuilder.string().defaultValue("default").optional().build(), null)
+                    ));
         }
         return result;
     }
 
-    private Map<String, Object> config(List<String> headers, List<String> transformFields, HeaderFrom.Operation operation) {
+    private Map<String, Object> config(List<String> headers, List<String> transformFields, HeaderFrom.Operation operation, boolean useFieldDefault) {
         Map<String, Object> result = new HashMap<>();
         result.put(HeaderFrom.HEADERS_FIELD, headers);
         result.put(HeaderFrom.FIELDS_FIELD, transformFields);
         result.put(HeaderFrom.OPERATION_FIELD, operation.toString());
+        result.put(HeaderFrom.REPLACE_NULL_WITH_DEFAULT_FIELD, useFieldDefault);
         return result;
     }
 
@@ -283,11 +314,11 @@ public class HeaderFromTest {
     public void schemaless(String description,
                            boolean keyTransform,
                            RecordBuilder originalBuilder,
-                           List<String> transformFields, List<String> headers1, HeaderFrom.Operation operation,
+                           List<String> transformFields, List<String> headers1, HeaderFrom.Operation operation, boolean replaceNullWithDefault,
                            RecordBuilder expectedBuilder) {
         HeaderFrom<SourceRecord> xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();
 
-        xform.configure(config(headers1, transformFields, operation));
+        xform.configure(config(headers1, transformFields, operation, replaceNullWithDefault));
         ConnectHeaders headers = new ConnectHeaders();
         headers.addString("existing", "existing-value");
 
@@ -302,10 +333,10 @@ public class HeaderFromTest {
     public void withSchema(String description,
                            boolean keyTransform,
                            RecordBuilder originalBuilder,
-                           List<String> transformFields, List<String> headers1, HeaderFrom.Operation operation,
+                           List<String> transformFields, List<String> headers1, HeaderFrom.Operation operation, boolean replaceNullWithDefault,
                            RecordBuilder expectedBuilder) {
         HeaderFrom<SourceRecord> xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();
-        xform.configure(config(headers1, transformFields, operation));
+        xform.configure(config(headers1, transformFields, operation, replaceNullWithDefault));
         ConnectHeaders headers = new ConnectHeaders();
         headers.addString("existing", "existing-value");
         Headers expect = headers.duplicate();
@@ -322,7 +353,7 @@ public class HeaderFromTest {
     @ParameterizedTest
     @ValueSource(booleans = {true, false})
     public void invalidConfigExtraHeaderConfig(boolean keyTransform) {
-        Map<String, Object> config = config(singletonList("foo"), asList("foo", "bar"), HeaderFrom.Operation.COPY);
+        Map<String, Object> config = config(singletonList("foo"), asList("foo", "bar"), HeaderFrom.Operation.COPY, true);
         HeaderFrom<?> xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();
         assertThrows(ConfigException.class, () -> xform.configure(config));
     }
@@ -330,7 +361,7 @@ public class HeaderFromTest {
     @ParameterizedTest
     @ValueSource(booleans = {true, false})
     public void invalidConfigExtraFieldConfig(boolean keyTransform) {
-        Map<String, Object> config = config(asList("foo", "bar"), singletonList("foo"), HeaderFrom.Operation.COPY);
+        Map<String, Object> config = config(asList("foo", "bar"), singletonList("foo"), HeaderFrom.Operation.COPY, true);
         HeaderFrom<?> xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();
         assertThrows(ConfigException.class, () -> xform.configure(config));
     }
@@ -338,7 +369,7 @@ public class HeaderFromTest {
     @ParameterizedTest
     @ValueSource(booleans = {true, false})
     public void invalidConfigEmptyHeadersAndFieldsConfig(boolean keyTransform) {
-        Map<String, Object> config = config(emptyList(), emptyList(), HeaderFrom.Operation.COPY);
+        Map<String, Object> config = config(emptyList(), emptyList(), HeaderFrom.Operation.COPY, true);
         HeaderFrom<?> xform = keyTransform ? new HeaderFrom.Key<>() : new HeaderFrom.Value<>();
         assertThrows(ConfigException.class, () -> xform.configure(config));
     }
diff --git a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertFieldTest.java b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertFieldTest.java
index f9754aa5a5..cb48fdd810 100644
--- a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertFieldTest.java
+++ b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertFieldTest.java
@@ -26,11 +26,15 @@ import org.apache.kafka.connect.source.SourceRecord;
 
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
 
 import java.util.Collections;
 import java.util.Date;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.stream.Stream;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertNull;
@@ -41,6 +45,13 @@ public class InsertFieldTest {
     private final InsertField<SourceRecord> xformKey = new InsertField.Key<>();
     private final InsertField<SourceRecord> xformValue = new InsertField.Value<>();
 
+    public static Stream<Arguments> data() {
+        return Stream.of(
+                Arguments.of(false, null),
+                Arguments.of(true, 42L)
+        );
+    }
+
     @AfterEach
     public void teardown() {
         xformValue.close();
@@ -210,4 +221,35 @@ public class InsertFieldTest {
 
         assertEquals(xformKey.version(), xformValue.version());
     }
+
+    @ParameterizedTest
+    @MethodSource("data")
+    public void testUnsetOptionalField(boolean replaceNullWithDefault, Object expectedValue) {
+
+        final Map<String, Object> props = new HashMap<>();
+        props.put("topic.field", "topic_field!");
+        props.put("partition.field", "partition_field");
+        props.put("timestamp.field", "timestamp_field?");
+        props.put("static.field", "instance_id");
+        props.put("static.value", "my-instance-id");
+        props.put("replace.null.with.default", replaceNullWithDefault);
+
+        xformValue.configure(props);
+
+        Schema magicFieldSchema = SchemaBuilder.int64().optional().defaultValue(42L).build();
+        final Schema simpleStructSchema = SchemaBuilder.struct().name("name").version(1).doc("doc").field("magic_with_default", magicFieldSchema).build();
+        final Struct simpleStruct = new Struct(simpleStructSchema).put("magic_with_default", null);
+
+        final SourceRecord record = new SourceRecord(null, null, "test", 0, null, null, simpleStructSchema, simpleStruct, 789L);
+        final SourceRecord transformedRecord = xformValue.apply(record);
+
+        assertEquals(simpleStructSchema.name(), transformedRecord.valueSchema().name());
+        assertEquals(simpleStructSchema.version(), transformedRecord.valueSchema().version());
+        assertEquals(simpleStructSchema.doc(), transformedRecord.valueSchema().doc());
+
+        assertEquals(magicFieldSchema, transformedRecord.valueSchema().field("magic_with_default").schema());
+        assertEquals(expectedValue, ((Struct) transformedRecord.value()).getInt64("magic_with_default"));
+
+    }
+
 }
diff --git a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/MaskFieldTest.java b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/MaskFieldTest.java
index 1f5a9641d2..05989af572 100644
--- a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/MaskFieldTest.java
+++ b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/MaskFieldTest.java
@@ -62,6 +62,7 @@ public class MaskFieldTest {
             .field("decimal", Decimal.schema(0))
             .field("array", SchemaBuilder.array(Schema.INT32_SCHEMA))
             .field("map", SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA))
+            .field("withDefault", SchemaBuilder.string().optional().defaultValue("default").build())
             .build();
     private static final Map<String, Object> VALUES = new HashMap<>();
     private static final Struct VALUES_WITH_SCHEMA = new Struct(SCHEMA);
@@ -97,6 +98,7 @@ public class MaskFieldTest {
         VALUES_WITH_SCHEMA.put("decimal", new BigDecimal(42));
         VALUES_WITH_SCHEMA.put("array", Arrays.asList(1, 2, 3));
         VALUES_WITH_SCHEMA.put("map", Collections.singletonMap("what", "what"));
+        VALUES_WITH_SCHEMA.put("withDefault", null);
     }
 
     private static MaskField<SinkRecord> transform(List<String> fields, String replacement) {
@@ -104,6 +106,7 @@ public class MaskFieldTest {
         Map<String, Object> props = new HashMap<>();
         props.put("fields", fields);
         props.put("replacement", replacement);
+        props.put("replace.null.with.default", false);
         xform.configure(props);
         return xform;
     }
@@ -181,6 +184,7 @@ public class MaskFieldTest {
         assertEquals(BigDecimal.ZERO, updatedValue.get("decimal"));
         assertEquals(Collections.emptyList(), updatedValue.get("array"));
         assertEquals(Collections.emptyMap(), updatedValue.get("map"));
+        assertEquals(null, updatedValue.getWithoutDefault("withDefault"));
     }
 
     @Test
diff --git a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/SetSchemaMetadataTest.java b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/SetSchemaMetadataTest.java
index 46f8be7788..5f0e51559b 100644
--- a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/SetSchemaMetadataTest.java
+++ b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/SetSchemaMetadataTest.java
@@ -26,10 +26,14 @@ import org.apache.kafka.connect.sink.SinkRecord;
 
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
 
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.stream.Stream;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertNull;
@@ -39,6 +43,13 @@ import static org.junit.jupiter.api.Assertions.assertThrows;
 public class SetSchemaMetadataTest {
     private final SetSchemaMetadata<SinkRecord> xform = new SetSchemaMetadata.Value<>();
 
+    public static Stream<Arguments> data() {
+        return Stream.of(
+                Arguments.of(false, null),
+                Arguments.of(true, "default")
+        );
+    }
+
     @AfterEach
     public void teardown() {
         xform.close();
@@ -57,7 +68,7 @@ public class SetSchemaMetadataTest {
         xform.configure(Collections.singletonMap("schema.version", 42));
         final SinkRecord record = new SinkRecord("", 0, null, null, SchemaBuilder.struct().build(), null, 0);
         final SinkRecord updatedRecord = xform.apply(record);
-        assertEquals(Integer.valueOf(42), updatedRecord.valueSchema().version());
+        assertEquals(42, updatedRecord.valueSchema().version());
     }
 
     @Test
@@ -73,7 +84,7 @@ public class SetSchemaMetadataTest {
         final SinkRecord updatedRecord = xform.apply(record);
 
         assertEquals("foo", updatedRecord.valueSchema().name());
-        assertEquals(Integer.valueOf(42), updatedRecord.valueSchema().version());
+        assertEquals(42, updatedRecord.valueSchema().version());
     }
 
     @Test
@@ -99,10 +110,42 @@ public class SetSchemaMetadataTest {
         final SinkRecord updatedRecord = xform.apply(record);
 
         assertEquals("foo", updatedRecord.valueSchema().name());
-        assertEquals(Integer.valueOf(42), updatedRecord.valueSchema().version());
+        assertEquals(42, updatedRecord.valueSchema().version());
+
+        // Make sure the struct's schema and fields all point to the new schema
+        assertMatchingSchema((Struct) updatedRecord.value(), updatedRecord.valueSchema());
+    }
+
+    @ParameterizedTest
+    @MethodSource("data")
+    public void schemaNameAndVersionUpdateWithStructAndNullField(boolean replaceNullWithDefault, Object expectedValue) {
+        final String fieldName1 = "f1";
+        final String fieldName2 = "f2";
+        final int fieldValue2 = 1;
+        final Schema schema = SchemaBuilder.struct()
+                .name("my.orig.SchemaDefn")
+                .field(fieldName1, SchemaBuilder.string().defaultValue("default").optional().build())
+                .field(fieldName2, Schema.INT32_SCHEMA)
+                .build();
+        final Struct value = new Struct(schema).put(fieldName1, null).put(fieldName2, fieldValue2);
+
+        final Map<String, Object> props = new HashMap<>();
+        props.put("schema.name", "foo");
+        props.put("schema.version", "42");
+        props.put("replace.null.with.default", replaceNullWithDefault);
+        xform.configure(props);
+
+        final SinkRecord record = new SinkRecord("", 0, null, null, schema, value, 0);
+
+        final SinkRecord updatedRecord = xform.apply(record);
+
+        assertEquals("foo", updatedRecord.valueSchema().name());
+        assertEquals(42, updatedRecord.valueSchema().version());
 
         // Make sure the struct's schema and fields all point to the new schema
         assertMatchingSchema((Struct) updatedRecord.value(), updatedRecord.valueSchema());
+
+        assertEquals(expectedValue, ((Struct) updatedRecord.value()).getWithoutDefault(fieldName1));
     }
 
     @Test
@@ -142,20 +185,20 @@ public class SetSchemaMetadataTest {
                                       .field(fieldName2, Schema.INT32_SCHEMA)
                                       .build();
 
-        Struct newValue = (Struct) SetSchemaMetadata.updateSchemaIn(value, newSchema);
+        Struct newValue = (Struct) xform.updateSchemaIn(value, newSchema);
         assertMatchingSchema(newValue, newSchema);
     }
 
     @Test
     public void updateSchemaOfNonStruct() {
         Object value = 1;
-        Object updatedValue = SetSchemaMetadata.updateSchemaIn(value, Schema.INT32_SCHEMA);
+        Object updatedValue = xform.updateSchemaIn(value, Schema.INT32_SCHEMA);
         assertSame(value, updatedValue);
     }
 
     @Test
     public void updateSchemaOfNull() {
-        Object updatedValue = SetSchemaMetadata.updateSchemaIn(null, Schema.INT32_SCHEMA);
+        Object updatedValue = xform.updateSchemaIn(null, Schema.INT32_SCHEMA);
         assertNull(updatedValue);
     }
 
diff --git a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampConverterTest.java b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampConverterTest.java
index fb3f3be626..a807ad1fc2 100644
--- a/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampConverterTest.java
+++ b/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampConverterTest.java
@@ -29,6 +29,9 @@ import org.apache.kafka.connect.source.SourceRecord;
 
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
 
 import java.util.Calendar;
 import java.util.Collections;
@@ -36,6 +39,7 @@ import java.util.GregorianCalendar;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.TimeZone;
+import java.util.stream.Stream;
 
 import static org.apache.kafka.connect.transforms.util.Requirements.requireStruct;
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
@@ -87,6 +91,12 @@ public class TimestampConverterTest {
         DATE_PLUS_TIME_STRING = "1970 01 02 00 00 01 234 UTC";
     }
 
+    public static Stream<Arguments> data() {
+        return Stream.of(
+                Arguments.of(false, null),
+                Arguments.of(true, EPOCH.getTime())
+        );
+    }
 
     // Configuration
 
@@ -554,6 +564,36 @@ public class TimestampConverterTest {
         assertEquals("test", ((Struct) transformed.value()).get("other"));
     }
 
+    @ParameterizedTest
+    @MethodSource("data")
+    public void testWithSchemaNullFieldWithDefaultConversion(boolean replaceNullWithDefault, Object expectedValue) {
+        Map<String, Object> config = new HashMap<>();
+        config.put(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp");
+        config.put(TimestampConverter.FIELD_CONFIG, "ts");
+        config.put(TimestampConverter.REPLACE_NULL_WITH_DEFAULT_CONFIG, false);
+        xformValue.configure(config);
+
+        // ts field is a unix timestamp
+        Schema structWithTimestampFieldSchema = SchemaBuilder.struct()
+                .field("ts", SchemaBuilder.int64().optional().defaultValue(0L).build())
+                .field("other", Schema.STRING_SCHEMA)
+                .build();
+        Struct original = new Struct(structWithTimestampFieldSchema);
+        original.put("ts", null);
+        original.put("other", "test");
+
+        SourceRecord transformed = xformValue.apply(createRecordWithSchema(structWithTimestampFieldSchema, original));
+
+        Schema expectedSchema = SchemaBuilder.struct()
+                .field("ts", Timestamp.builder().optional().build())
+                .field("other", Schema.STRING_SCHEMA)
+                .build();
+
+        assertEquals(expectedSchema, transformed.valueSchema());
+        assertEquals(null, ((Struct) transformed.value()).get("ts"));
+        assertEquals("test", ((Struct) transformed.value()).get("other"));
+    }
+
     @Test
     public void testWithSchemaFieldConversion_Micros() {
         Map<String, String> config = new HashMap<>();
