diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java
index 05cf66ad07..32610b583c 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java
@@ -220,7 +220,7 @@ public class PartitionGroup {
 
     void close() {
         clear();
-        partitionQueues.clear();
+
         streamTime = RecordQueue.UNKNOWN;
     }
 
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java
index 00601a7e92..0beade90ec 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java
@@ -47,7 +47,7 @@ public class RecordQueue {
     private final ArrayDeque<ConsumerRecord<byte[], byte[]>> fifoQueue;
 
     private StampedRecord headRecord = null;
-    private long partitionTime = RecordQueue.UNKNOWN;
+    private long partitionTime;
 
     private final Sensor droppedRecordsSensor;
 
@@ -74,6 +74,7 @@ public class RecordQueue {
             droppedRecordsSensor
         );
         this.log = logContext.logger(RecordQueue.class);
+        setPartitionTime(UNKNOWN);
     }
 
     void setPartitionTime(final long partitionTime) {
@@ -166,6 +167,7 @@ public class RecordQueue {
     public void clear() {
         fifoQueue.clear();
         headRecord = null;
+        setPartitionTime(UNKNOWN);
     }
 
     private void updateHead() {
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
index fc6dd9cbb1..1b08177ab3 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
@@ -421,7 +421,7 @@ public class StoreChangelogReader implements ChangelogReader {
                 log.warn("Encountered {} fetching records from restore consumer for partitions {}, it is likely that " +
                     "the consumer's position has fallen out of the topic partition offset range because the topic was " +
                     "truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing" +
-                    "it later.", e.getClass().getName(), e.partitions());
+                    " it later.", e.getClass().getName(), e.partitions());
 
                 final Map<TaskId, Set<TopicPartition>> taskWithCorruptedChangelogs = new HashMap<>();
                 for (final TopicPartition partition : e.partitions()) {
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java
index ef31d84a1d..d38c52dcb8 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java
@@ -380,38 +380,42 @@ public class PartitionGroupTest {
     }
 
     @Test
-    public void shouldEmpyPartitionsOnClean() {
+    public void shouldEmptyPartitionsOnClear() {
         final List<ConsumerRecord<byte[], byte[]>> list = Arrays.asList(
             new ConsumerRecord<>("topic", 1, 1L, recordKey, recordValue),
             new ConsumerRecord<>("topic", 1, 3L, recordKey, recordValue),
             new ConsumerRecord<>("topic", 1, 5L, recordKey, recordValue));
         group.addRawRecords(partition1, list);
+        group.nextRecord(new PartitionGroup.RecordInfo());
+        group.nextRecord(new PartitionGroup.RecordInfo());
 
         group.clear();
 
         assertThat(group.numBuffered(), equalTo(0));
-        assertThat(group.streamTime(), equalTo(RecordQueue.UNKNOWN));
+        assertThat(group.streamTime(), equalTo(3L));
         assertThat(group.nextRecord(new PartitionGroup.RecordInfo()), equalTo(null));
+        assertThat(group.partitionTimestamp(partition1), equalTo(RecordQueue.UNKNOWN));
 
         group.addRawRecords(partition1, list);
     }
 
+    @Test
     public void shouldCleanPartitionsOnClose() {
         final List<ConsumerRecord<byte[], byte[]>> list = Arrays.asList(
             new ConsumerRecord<>("topic", 1, 1L, recordKey, recordValue),
             new ConsumerRecord<>("topic", 1, 3L, recordKey, recordValue),
             new ConsumerRecord<>("topic", 1, 5L, recordKey, recordValue));
         group.addRawRecords(partition1, list);
+        group.nextRecord(new PartitionGroup.RecordInfo());
 
         group.close();
 
         assertThat(group.numBuffered(), equalTo(0));
         assertThat(group.streamTime(), equalTo(RecordQueue.UNKNOWN));
         assertThat(group.nextRecord(new PartitionGroup.RecordInfo()), equalTo(null));
+        assertThat(group.partitionTimestamp(partition1), equalTo(RecordQueue.UNKNOWN));
 
-        final IllegalStateException exception = assertThrows(
-            IllegalStateException.class,
-            () -> group.addRawRecords(partition1, list));
-        assertThat("Partition topic-1 not found.", equalTo(exception.getMessage()));
+        // The partition1 should still be able to find.
+        assertThat(group.addRawRecords(partition1, list), equalTo(3));
     }
 }
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java
index c98950ebc1..fb13d45fcd 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java
@@ -180,6 +180,7 @@ public class RecordQueueTest {
         assertTrue(queue.isEmpty());
         assertEquals(0, queue.size());
         assertEquals(RecordQueue.UNKNOWN, queue.headRecordTimestamp());
+        assertEquals(RecordQueue.UNKNOWN, queue.partitionTime());
         assertNull(queue.headRecordOffset());
 
         // re-insert the three records with 4, 5, 6
