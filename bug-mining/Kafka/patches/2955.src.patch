diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java b/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java
index 6eefb897b8..650b69795d 100644
--- a/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java
@@ -527,7 +527,12 @@ public class MockProducer<K, V> implements Producer<K, V> {
         }
 
         public void complete(RuntimeException e) {
-            result.set(e == null ? offset : -1L, RecordBatch.NO_TIMESTAMP, e);
+            if (e == null) {
+                result.set(offset, RecordBatch.NO_TIMESTAMP, null);
+            } else {
+                result.set(-1, RecordBatch.NO_TIMESTAMP, index -> e);
+            }
+
             if (callback != null) {
                 if (e == null)
                     callback.onCompletion(metadata, null);
diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadata.java b/clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadata.java
index d1a643b319..a9665a991b 100644
--- a/clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadata.java
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadata.java
@@ -30,7 +30,7 @@ import java.util.concurrent.TimeoutException;
 public final class FutureRecordMetadata implements Future<RecordMetadata> {
 
     private final ProduceRequestResult result;
-    private final long relativeOffset;
+    private final int batchIndex;
     private final long createTimestamp;
     private final Long checksum;
     private final int serializedKeySize;
@@ -38,10 +38,10 @@ public final class FutureRecordMetadata implements Future<RecordMetadata> {
     private final Time time;
     private volatile FutureRecordMetadata nextRecordMetadata = null;
 
-    public FutureRecordMetadata(ProduceRequestResult result, long relativeOffset, long createTimestamp,
+    public FutureRecordMetadata(ProduceRequestResult result, int batchIndex, long createTimestamp,
                                 Long checksum, int serializedKeySize, int serializedValueSize, Time time) {
         this.result = result;
-        this.relativeOffset = relativeOffset;
+        this.batchIndex = batchIndex;
         this.createTimestamp = createTimestamp;
         this.checksum = checksum;
         this.serializedKeySize = serializedKeySize;
@@ -94,8 +94,9 @@ public final class FutureRecordMetadata implements Future<RecordMetadata> {
     }
 
     RecordMetadata valueOrError() throws ExecutionException {
-        if (this.result.error() != null)
-            throw new ExecutionException(this.result.error());
+        RuntimeException exception = this.result.error(batchIndex);
+        if (exception != null)
+            throw new ExecutionException(exception);
         else
             return value();
     }
@@ -107,7 +108,7 @@ public final class FutureRecordMetadata implements Future<RecordMetadata> {
     RecordMetadata value() {
         if (nextRecordMetadata != null)
             return nextRecordMetadata.value();
-        return new RecordMetadata(result.topicPartition(), this.result.baseOffset(), this.relativeOffset,
+        return new RecordMetadata(result.topicPartition(), this.result.baseOffset(), this.batchIndex,
                                   timestamp(), this.checksum, this.serializedKeySize, this.serializedValueSize);
     }
 
diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProduceRequestResult.java b/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProduceRequestResult.java
index 1e8c787963..9077b107ab 100644
--- a/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProduceRequestResult.java
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProduceRequestResult.java
@@ -22,7 +22,7 @@ import org.apache.kafka.common.record.RecordBatch;
 
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
-
+import java.util.function.Function;
 
 /**
  * A class that models the future completion of a produce request for a single partition. There is one of these per
@@ -36,7 +36,7 @@ public class ProduceRequestResult {
 
     private volatile Long baseOffset = null;
     private volatile long logAppendTime = RecordBatch.NO_TIMESTAMP;
-    private volatile RuntimeException error;
+    private volatile Function<Integer, RuntimeException> errorsByIndex;
 
     /**
      * Create an instance of this class.
@@ -52,12 +52,12 @@ public class ProduceRequestResult {
      *
      * @param baseOffset The base offset assigned to the record
      * @param logAppendTime The log append time or -1 if CreateTime is being used
-     * @param error The error that occurred if there was one, or null
+     * @param errorsByIndex Function mapping the batch index to the exception, or null if the response was successful
      */
-    public void set(long baseOffset, long logAppendTime, RuntimeException error) {
+    public void set(long baseOffset, long logAppendTime, Function<Integer, RuntimeException> errorsByIndex) {
         this.baseOffset = baseOffset;
         this.logAppendTime = logAppendTime;
-        this.error = error;
+        this.errorsByIndex = errorsByIndex;
     }
 
     /**
@@ -110,8 +110,12 @@ public class ProduceRequestResult {
     /**
      * The error thrown (generally on the server) while processing this request
      */
-    public RuntimeException error() {
-        return error;
+    public RuntimeException error(int batchIndex) {
+        if (errorsByIndex == null) {
+            return null;
+        } else {
+            return errorsByIndex.apply(batchIndex);
+        }
     }
 
     /**
diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java b/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java
index cfd3a6794c..6f76c799b2 100644
--- a/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java
@@ -17,7 +17,6 @@
 package org.apache.kafka.clients.producer.internals;
 
 import org.apache.kafka.clients.producer.Callback;
-import org.apache.kafka.common.utils.ProducerIdAndEpoch;
 import org.apache.kafka.clients.producer.RecordMetadata;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.errors.RecordBatchTooLargeException;
@@ -32,6 +31,7 @@ import org.apache.kafka.common.record.Record;
 import org.apache.kafka.common.record.RecordBatch;
 import org.apache.kafka.common.record.TimestampType;
 import org.apache.kafka.common.requests.ProduceResponse;
+import org.apache.kafka.common.utils.ProducerIdAndEpoch;
 import org.apache.kafka.common.utils.Time;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -42,8 +42,10 @@ import java.util.ArrayList;
 import java.util.Deque;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Objects;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.Function;
 
 import static org.apache.kafka.common.record.RecordBatch.MAGIC_VALUE_V2;
 import static org.apache.kafka.common.record.RecordBatch.NO_TIMESTAMP;
@@ -156,16 +158,46 @@ public final class ProducerBatch {
             throw new IllegalStateException("Batch has already been completed in final state " + finalState.get());
 
         log.trace("Aborting batch for partition {}", topicPartition, exception);
-        completeFutureAndFireCallbacks(ProduceResponse.INVALID_OFFSET, RecordBatch.NO_TIMESTAMP, exception);
+        completeFutureAndFireCallbacks(ProduceResponse.INVALID_OFFSET, RecordBatch.NO_TIMESTAMP, index -> exception);
     }
 
     /**
-     * Return `true` if {@link #done(long, long, RuntimeException)} has been invoked at least once, `false` otherwise.
+     * Check if the batch has been completed (either successfully or exceptionally).
+     * @return `true` if the batch has been completed, `false` otherwise.
      */
     public boolean isDone() {
         return finalState() != null;
     }
 
+    /**
+     * Complete the batch successfully.
+     * @param baseOffset The base offset of the messages assigned by the server
+     * @param logAppendTime The log append time or -1 if CreateTime is being used
+     * @return true if the batch was completed as a result of this call, and false
+     *   if it had been completed previously
+     */
+    public boolean complete(long baseOffset, long logAppendTime) {
+        return done(baseOffset, logAppendTime, null, null);
+    }
+
+    /**
+     * Complete the batch exceptionally. The provided top-level exception will be used
+     * for each record future contained in the batch.
+     *
+     * @param topLevelException top-level partition error
+     * @param recordExceptions Record exception function mapping batchIndex to the respective record exception
+     * @return true if the batch was completed as a result of this call, and false
+     *   if it had been completed previously
+     */
+    public boolean completeExceptionally(
+        RuntimeException topLevelException,
+        Function<Integer, RuntimeException> recordExceptions
+    ) {
+        Objects.requireNonNull(topLevelException);
+        Objects.requireNonNull(recordExceptions);
+        return done(ProduceResponse.INVALID_OFFSET, RecordBatch.NO_TIMESTAMP, topLevelException, recordExceptions);
+    }
+
     /**
      * Finalize the state of a batch. Final state, once set, is immutable. This function may be called
      * once or twice on a batch. It may be called twice if
@@ -181,20 +213,25 @@ public final class ProducerBatch {
      *
      * @param baseOffset The base offset of the messages assigned by the server
      * @param logAppendTime The log append time or -1 if CreateTime is being used
-     * @param exception The exception that occurred (or null if the request was successful)
+     * @param topLevelException The exception that occurred (or null if the request was successful)
+     * @param recordExceptions Record exception function mapping batchIndex to the respective record exception
      * @return true if the batch was completed successfully and false if the batch was previously aborted
      */
-    public boolean done(long baseOffset, long logAppendTime, RuntimeException exception) {
-        final FinalState tryFinalState = (exception == null) ? FinalState.SUCCEEDED : FinalState.FAILED;
-
+    private boolean done(
+        long baseOffset,
+        long logAppendTime,
+        RuntimeException topLevelException,
+        Function<Integer, RuntimeException> recordExceptions
+    ) {
+        final FinalState tryFinalState = (topLevelException == null) ? FinalState.SUCCEEDED : FinalState.FAILED;
         if (tryFinalState == FinalState.SUCCEEDED) {
             log.trace("Successfully produced messages to {} with base offset {}.", topicPartition, baseOffset);
         } else {
-            log.trace("Failed to produce messages to {} with base offset {}.", topicPartition, baseOffset, exception);
+            log.trace("Failed to produce messages to {} with base offset {}.", topicPartition, baseOffset, topLevelException);
         }
 
         if (this.finalState.compareAndSet(null, tryFinalState)) {
-            completeFutureAndFireCallbacks(baseOffset, logAppendTime, exception);
+            completeFutureAndFireCallbacks(baseOffset, logAppendTime, recordExceptions);
             return true;
         }
 
@@ -215,20 +252,26 @@ public final class ProducerBatch {
         return false;
     }
 
-    private void completeFutureAndFireCallbacks(long baseOffset, long logAppendTime, RuntimeException exception) {
+    private void completeFutureAndFireCallbacks(
+        long baseOffset,
+        long logAppendTime,
+        Function<Integer, RuntimeException> recordExceptions
+    ) {
         // Set the future before invoking the callbacks as we rely on its state for the `onCompletion` call
-        produceFuture.set(baseOffset, logAppendTime, exception);
+        produceFuture.set(baseOffset, logAppendTime, recordExceptions);
 
         // execute callbacks
-        for (Thunk thunk : thunks) {
+        for (int i = 0; i < thunks.size(); i++) {
             try {
-                if (exception == null) {
-                    RecordMetadata metadata = thunk.future.value();
-                    if (thunk.callback != null)
+                Thunk thunk = thunks.get(i);
+                if (thunk.callback != null) {
+                    if (recordExceptions == null) {
+                        RecordMetadata metadata = thunk.future.value();
                         thunk.callback.onCompletion(metadata, null);
-                } else {
-                    if (thunk.callback != null)
+                    } else {
+                        RuntimeException exception = recordExceptions.apply(i);
                         thunk.callback.onCompletion(null, exception);
+                    }
                 }
             } catch (Exception e) {
                 log.error("Error executing user-provided callback on message for topic-partition '{}'", topicPartition, e);
@@ -280,7 +323,7 @@ public final class ProducerBatch {
             batch.closeForRecordAppends();
         }
 
-        produceFuture.set(ProduceResponse.INVALID_OFFSET, NO_TIMESTAMP, new RecordBatchTooLargeException());
+        produceFuture.set(ProduceResponse.INVALID_OFFSET, NO_TIMESTAMP, index -> new RecordBatchTooLargeException());
         produceFuture.done();
 
         if (hasSequence()) {
@@ -420,8 +463,8 @@ public final class ProducerBatch {
      * Abort the record builder and reset the state of the underlying buffer. This is used prior to aborting
      * the batch with {@link #abort(RuntimeException)} and ensures that no record previously appended can be
      * read. This is used in scenarios where we want to ensure a batch ultimately gets aborted, but in which
-     * it is not safe to invoke the completion callbacks (e.g. because we are holding a lock,
-     * {@link RecordAccumulator#abortBatches()}).
+     * it is not safe to invoke the completion callbacks (e.g. because we are holding a lock, such as
+     * when aborting batches in {@link RecordAccumulator}).
      */
     public void abortRecordAppends() {
         recordsBuilder.abort();
diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java b/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java
index 8f2549343c..8b31094fda 100644
--- a/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java
@@ -24,6 +24,8 @@ import org.apache.kafka.clients.Metadata;
 import org.apache.kafka.clients.NetworkClientUtils;
 import org.apache.kafka.clients.RequestCompletionHandler;
 import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.InvalidRecordException;
+import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.MetricName;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.TopicPartition;
@@ -60,10 +62,9 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
+import java.util.function.Function;
 import java.util.stream.Collectors;
 
-import static org.apache.kafka.common.record.RecordBatch.NO_TIMESTAMP;
-
 /**
  * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata
  * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes.
@@ -380,7 +381,7 @@ public class Sender implements Runnable {
         for (ProducerBatch expiredBatch : expiredBatches) {
             String errorMessage = "Expiring " + expiredBatch.recordCount + " record(s) for " + expiredBatch.topicPartition
                 + ":" + (now - expiredBatch.createdMs) + " ms has passed since batch creation";
-            failBatch(expiredBatch, -1, NO_TIMESTAMP, new TimeoutException(errorMessage), false);
+            failBatch(expiredBatch, new TimeoutException(errorMessage), false);
             if (transactionManager != null && expiredBatch.inRetry()) {
                 // This ensures that no new batches are drained until the current in flight batches are fully resolved.
                 transactionManager.markSequenceUnresolved(expiredBatch);
@@ -635,17 +636,10 @@ public class Sender implements Runnable {
                 // The only thing we can do is to return success to the user and not return a valid offset and timestamp.
                 completeBatch(batch, response);
             } else {
-                final RuntimeException exception;
-                if (error == Errors.TOPIC_AUTHORIZATION_FAILED)
-                    exception = new TopicAuthorizationException(Collections.singleton(batch.topicPartition.topic()));
-                else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED)
-                    exception = new ClusterAuthorizationException("The producer is not authorized to do idempotent sends");
-                else
-                    exception = error.exception(response.errorMessage);
                 // tell the user the result of their request. We only adjust sequence numbers if the batch didn't exhaust
                 // its retries -- if it did, we don't know whether the sequence number was accepted or not, and
                 // thus it is not safe to reassign the sequence.
-                failBatch(batch, response, exception, batch.attempts() < this.retries);
+                failBatch(batch, response, batch.attempts() < this.retries);
             }
             if (error.exception() instanceof InvalidMetadataException) {
                 if (error.exception() instanceof UnknownTopicOrPartitionException) {
@@ -689,30 +683,87 @@ public class Sender implements Runnable {
             transactionManager.handleCompletedBatch(batch, response);
         }
 
-        if (batch.done(response.baseOffset, response.logAppendTime, null)) {
+        if (batch.complete(response.baseOffset, response.logAppendTime)) {
             maybeRemoveAndDeallocateBatch(batch);
         }
     }
 
     private void failBatch(ProducerBatch batch,
                            ProduceResponse.PartitionResponse response,
-                           RuntimeException exception,
                            boolean adjustSequenceNumbers) {
-        failBatch(batch, response.baseOffset, response.logAppendTime, exception, adjustSequenceNumbers);
+        final RuntimeException topLevelException;
+        if (response.error == Errors.TOPIC_AUTHORIZATION_FAILED)
+            topLevelException = new TopicAuthorizationException(Collections.singleton(batch.topicPartition.topic()));
+        else if (response.error == Errors.CLUSTER_AUTHORIZATION_FAILED)
+            topLevelException = new ClusterAuthorizationException("The producer is not authorized to do idempotent sends");
+        else
+            topLevelException = response.error.exception(response.errorMessage);
+
+        if (response.recordErrors == null || response.recordErrors.isEmpty()) {
+            failBatch(batch, topLevelException, adjustSequenceNumbers);
+        } else {
+            Map<Integer, RuntimeException> recordErrorMap = new HashMap<>(response.recordErrors.size());
+            for (ProduceResponse.RecordError recordError : response.recordErrors) {
+                // The API leaves us with some awkwardness interpreting the errors in the response.
+                // We cannot differentiate between different error cases (such as INVALID_TIMESTAMP)
+                // from the single error code at the partition level, so instead we use INVALID_RECORD
+                // for all failed records and rely on the message to distinguish the cases.
+                final String errorMessage;
+                if (recordError.message != null) {
+                    errorMessage = recordError.message;
+                } else if (response.errorMessage != null) {
+                    errorMessage = response.errorMessage;
+                } else {
+                    errorMessage = response.error.message();
+                }
+
+                // If the batch contained only a single record error, then we can unambiguously
+                // use the exception type corresponding to the partition-level error code.
+                if (response.recordErrors.size() == 1) {
+                    recordErrorMap.put(recordError.batchIndex, response.error.exception(errorMessage));
+                } else {
+                    recordErrorMap.put(recordError.batchIndex, new InvalidRecordException(errorMessage));
+                }
+            }
+
+            Function<Integer, RuntimeException> recordExceptions = batchIndex -> {
+                RuntimeException exception = recordErrorMap.get(batchIndex);
+                if (exception != null) {
+                    return exception;
+                } else {
+                    // If the response contains record errors, then the records which failed validation
+                    // will be present in the response. To avoid confusion for the remaining records, we
+                    // return a generic exception.
+                    return new KafkaException("Failed to append record because it was part of a batch " +
+                        "which had one more more invalid records");
+                }
+            };
+
+            failBatch(batch, topLevelException, recordExceptions, adjustSequenceNumbers);
+        }
     }
 
-    private void failBatch(ProducerBatch batch,
-                           long baseOffset,
-                           long logAppendTime,
-                           RuntimeException exception,
-                           boolean adjustSequenceNumbers) {
+    private void failBatch(
+        ProducerBatch batch,
+        RuntimeException topLevelException,
+        boolean adjustSequenceNumbers
+    ) {
+        failBatch(batch, topLevelException, batchIndex -> topLevelException, adjustSequenceNumbers);
+    }
+
+    private void failBatch(
+        ProducerBatch batch,
+        RuntimeException topLevelException,
+        Function<Integer, RuntimeException> recordExceptions,
+        boolean adjustSequenceNumbers
+    ) {
         if (transactionManager != null) {
-            transactionManager.handleFailedBatch(batch, exception, adjustSequenceNumbers);
+            transactionManager.handleFailedBatch(batch, topLevelException, adjustSequenceNumbers);
         }
 
         this.sensors.recordErrors(batch.topicPartition.topic(), batch.recordCount);
 
-        if (batch.done(baseOffset, logAppendTime, exception)) {
+        if (batch.completeExceptionally(topLevelException, recordExceptions)) {
             maybeRemoveAndDeallocateBatch(batch);
         }
     }
diff --git a/clients/src/main/resources/common/message/ProduceRequest.json b/clients/src/main/resources/common/message/ProduceRequest.json
index 121cd42f44..90900af580 100644
--- a/clients/src/main/resources/common/message/ProduceRequest.json
+++ b/clients/src/main/resources/common/message/ProduceRequest.json
@@ -41,7 +41,7 @@
     { "name": "Acks", "type": "int16", "versions": "0+",
       "about": "The number of acknowledgments the producer requires the leader to have received before considering a request complete. Allowed values: 0 for no acknowledgments, 1 for only the leader and -1 for the full ISR." },
     { "name": "TimeoutMs", "type": "int32", "versions": "0+",
-      "about": "The timeout to await a response in miliseconds." },
+      "about": "The timeout to await a response in milliseconds." },
     { "name": "TopicData", "type": "[]TopicProduceData", "versions": "0+",
       "about": "Each topic to produce to.", "fields": [
       { "name": "Name", "type": "string", "versions": "0+", "entityType": "topicName", "mapKey": true,
diff --git a/clients/src/main/resources/common/message/ProduceResponse.json b/clients/src/main/resources/common/message/ProduceResponse.json
index d6e5566eab..0c47f6d938 100644
--- a/clients/src/main/resources/common/message/ProduceResponse.json
+++ b/clients/src/main/resources/common/message/ProduceResponse.json
@@ -53,10 +53,10 @@
           "about": "The log start offset." },
         { "name": "RecordErrors", "type": "[]BatchIndexAndErrorMessage", "versions": "8+", "ignorable": true,
           "about": "The batch indices of records that caused the batch to be dropped", "fields": [
-          { "name":  "BatchIndex", "type": "int32", "versions":  "8+",
-            "about":  "The batch index of the record that cause the batch to be dropped" },
+          { "name": "BatchIndex", "type": "int32", "versions":  "8+",
+            "about": "The batch index of the record that cause the batch to be dropped" },
           { "name": "BatchIndexErrorMessage", "type": "string", "default": "null", "versions": "8+", "nullableVersions": "8+",
-            "about":  "The error message of the record that caused the batch to be dropped"}
+            "about": "The error message of the record that caused the batch to be dropped"}
         ]},
         { "name":  "ErrorMessage", "type": "string", "default": "null", "versions": "8+", "nullableVersions": "8+", "ignorable":  true,
           "about":  "The global error message summarizing the common root cause of the records that caused the batch to be dropped"}
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/RecordSendTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/RecordSendTest.java
index 3af658bbac..b7cfd4e583 100644
--- a/clients/src/test/java/org/apache/kafka/clients/producer/RecordSendTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/RecordSendTest.java
@@ -39,7 +39,7 @@ public class RecordSendTest {
 
     private final TopicPartition topicPartition = new TopicPartition("test", 0);
     private final long baseOffset = 45;
-    private final long relOffset = 5;
+    private final int relOffset = 5;
 
     /**
      * Test that waiting on a request that never completes times out
@@ -89,7 +89,12 @@ public class RecordSendTest {
             public void run() {
                 try {
                     sleep(timeout);
-                    request.set(baseOffset, RecordBatch.NO_TIMESTAMP, error);
+                    if (error == null) {
+                        request.set(baseOffset, RecordBatch.NO_TIMESTAMP, null);
+                    } else {
+                        request.set(-1L, RecordBatch.NO_TIMESTAMP, index -> error);
+                    }
+
                     request.done();
                 } catch (InterruptedException e) { }
             }
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerBatchTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerBatchTest.java
index af867ba17d..b6ee58245a 100644
--- a/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerBatchTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerBatchTest.java
@@ -29,12 +29,18 @@ import org.apache.kafka.common.record.MemoryRecordsBuilder;
 import org.apache.kafka.common.record.Record;
 import org.apache.kafka.common.record.RecordBatch;
 import org.apache.kafka.common.record.TimestampType;
+import org.apache.kafka.test.TestUtils;
 import org.junit.jupiter.api.Test;
 
 import java.nio.ByteBuffer;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Deque;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 import java.util.concurrent.ExecutionException;
+import java.util.function.Function;
 
 import static org.apache.kafka.common.record.RecordBatch.MAGIC_VALUE_V0;
 import static org.apache.kafka.common.record.RecordBatch.MAGIC_VALUE_V1;
@@ -43,6 +49,7 @@ import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertNotNull;
 import static org.junit.jupiter.api.Assertions.assertNull;
+import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 import static org.junit.jupiter.api.Assertions.fail;
 
@@ -50,7 +57,7 @@ public class ProducerBatchTest {
 
     private final long now = 1488748346917L;
 
-    private final MemoryRecordsBuilder memoryRecordsBuilder = MemoryRecords.builder(ByteBuffer.allocate(128),
+    private final MemoryRecordsBuilder memoryRecordsBuilder = MemoryRecords.builder(ByteBuffer.allocate(512),
             CompressionType.NONE, TimestampType.CREATE_TIME, 128);
 
     @Test
@@ -75,8 +82,8 @@ public class ProducerBatchTest {
         assertNull(callback.metadata);
 
         // subsequent completion should be ignored
-        assertFalse(batch.done(500L, 2342342341L, null));
-        assertFalse(batch.done(-1, -1, new KafkaException()));
+        assertFalse(batch.complete(500L, 2342342341L));
+        assertFalse(batch.completeExceptionally(new KafkaException(), index -> new KafkaException()));
         assertEquals(1, callback.invocations);
 
         assertTrue(future.isDone());
@@ -121,18 +128,11 @@ public class ProducerBatchTest {
         ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), memoryRecordsBuilder, now);
         MockCallback callback = new MockCallback();
         FutureRecordMetadata future = batch.tryAppend(now, null, new byte[10], Record.EMPTY_HEADERS, callback, now);
-        batch.done(500L, 10L, null);
+        batch.complete(500L, 10L);
         assertEquals(1, callback.invocations);
         assertNull(callback.exception);
         assertNotNull(callback.metadata);
-
-        try {
-            batch.done(1000L, 20L, null);
-            fail("Expected exception from done");
-        } catch (IllegalStateException e) {
-            // expected
-        }
-
+        assertThrows(IllegalStateException.class, () -> batch.complete(1000L, 20L));
         RecordMetadata recordMetadata = future.get();
         assertEquals(500L, recordMetadata.offset());
         assertEquals(10L, recordMetadata.timestamp());
@@ -266,6 +266,57 @@ public class ProducerBatchTest {
         assertNull(batch.tryAppend(now + 1, null, new byte[10], Record.EMPTY_HEADERS, null, now + 1));
     }
 
+    @Test
+    public void testCompleteExceptionallyWithRecordErrors() {
+        int recordCount = 5;
+        RuntimeException topLevelException = new RuntimeException();
+
+        Map<Integer, RuntimeException> recordExceptionMap = new HashMap<>();
+        recordExceptionMap.put(0, new RuntimeException());
+        recordExceptionMap.put(3, new RuntimeException());
+
+        Function<Integer, RuntimeException> recordExceptions = batchIndex ->
+            recordExceptionMap.getOrDefault(batchIndex, topLevelException);
+
+        testCompleteExceptionally(recordCount, topLevelException, recordExceptions);
+    }
+
+    @Test
+    public void testCompleteExceptionallyWithNullRecordErrors() {
+        int recordCount = 5;
+        RuntimeException topLevelException = new RuntimeException();
+        assertThrows(NullPointerException.class, () ->
+            testCompleteExceptionally(recordCount, topLevelException, null));
+    }
+
+    private void testCompleteExceptionally(
+        int recordCount,
+        RuntimeException topLevelException,
+        Function<Integer, RuntimeException> recordExceptions
+    ) {
+        ProducerBatch batch = new ProducerBatch(
+            new TopicPartition("topic", 1),
+            memoryRecordsBuilder,
+            now
+        );
+
+        List<FutureRecordMetadata> futures = new ArrayList<>(recordCount);
+        for (int i = 0; i < recordCount; i++) {
+            futures.add(batch.tryAppend(now, null, new byte[10], Record.EMPTY_HEADERS, null, now));
+        }
+        assertEquals(recordCount, batch.recordCount);
+
+        batch.completeExceptionally(topLevelException, recordExceptions);
+        assertTrue(batch.isDone());
+
+        for (int i = 0; i < futures.size(); i++) {
+            FutureRecordMetadata future = futures.get(i);
+            RuntimeException caughtException = TestUtils.assertFutureThrows(future, RuntimeException.class);
+            RuntimeException expectedException = recordExceptions.apply(i);
+            assertEquals(expectedException, caughtException);
+        }
+    }
+
     private static class MockCallback implements Callback {
         private int invocations = 0;
         private RecordMetadata metadata;
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java
index 3608aabdeb..9cc350abc1 100644
--- a/clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java
@@ -754,7 +754,7 @@ public class RecordAccumulatorTest {
         drained = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, time.milliseconds());
         assertFalse(drained.isEmpty());
         assertFalse(drained.get(node1.id()).isEmpty());
-        drained.get(node1.id()).get(0).done(acked.get(), 100L, null);
+        drained.get(node1.id()).get(0).complete(acked.get(), 100L);
         assertEquals(1, acked.get(), "The first message should have been acked.");
         assertTrue(future1.isDone());
         assertEquals(0, future1.get().offset());
@@ -762,7 +762,7 @@ public class RecordAccumulatorTest {
         drained = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, time.milliseconds());
         assertFalse(drained.isEmpty());
         assertFalse(drained.get(node1.id()).isEmpty());
-        drained.get(node1.id()).get(0).done(acked.get(), 100L, null);
+        drained.get(node1.id()).get(0).complete(acked.get(), 100L);
         assertEquals(2, acked.get(), "Both message should have been acked.");
         assertTrue(future2.isDone());
         assertEquals(1, future2.get().offset());
@@ -1010,7 +1010,7 @@ public class RecordAccumulatorTest {
                         // release the resource of the original big batch.
                         numSplit++;
                     } else {
-                        batch.done(0L, 0L, null);
+                        batch.complete(0L, 0L);
                     }
                     accum.deallocate(batch);
                 }
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java
index c98ac632a0..52e801f87b 100644
--- a/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java
@@ -25,6 +25,7 @@ import org.apache.kafka.clients.NodeApiVersions;
 import org.apache.kafka.clients.producer.Callback;
 import org.apache.kafka.clients.producer.RecordMetadata;
 import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.InvalidRecordException;
 import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.MetricName;
 import org.apache.kafka.common.MetricNameTemplate;
@@ -44,6 +45,8 @@ import org.apache.kafka.common.message.ApiMessageType;
 import org.apache.kafka.common.message.EndTxnResponseData;
 import org.apache.kafka.common.message.InitProducerIdResponseData;
 import org.apache.kafka.common.message.ProduceRequestData;
+import org.apache.kafka.common.message.ProduceResponseData;
+import org.apache.kafka.common.message.ProduceResponseData.BatchIndexAndErrorMessage;
 import org.apache.kafka.common.metrics.KafkaMetric;
 import org.apache.kafka.common.metrics.MetricConfig;
 import org.apache.kafka.common.metrics.Metrics;
@@ -86,6 +89,7 @@ import org.junit.jupiter.api.Timeout;
 import org.mockito.InOrder;
 
 import java.nio.ByteBuffer;
+import java.util.Arrays;
 import java.util.Collections;
 import java.util.Deque;
 import java.util.HashMap;
@@ -2450,6 +2454,48 @@ public class SenderTest {
         }
     }
 
+    @Test
+    public void testRecordErrorPropagatedToApplication() throws InterruptedException {
+        int recordCount = 5;
+
+        setup();
+
+        Map<Integer, FutureRecordMetadata> futures = new HashMap<>(recordCount);
+        for (int i = 0; i < recordCount; i++) {
+            futures.put(i, appendToAccumulator(tp0));
+        }
+
+        sender.runOnce();  // send request
+        assertEquals(1, client.inFlightRequestCount());
+        assertEquals(1, sender.inFlightBatches(tp0).size());
+
+        OffsetAndError offsetAndError = new OffsetAndError(-1L, Errors.INVALID_RECORD, Arrays.asList(
+            new BatchIndexAndErrorMessage().setBatchIndex(0).setBatchIndexErrorMessage("0"),
+            new BatchIndexAndErrorMessage().setBatchIndex(2).setBatchIndexErrorMessage("2"),
+            new BatchIndexAndErrorMessage().setBatchIndex(3)
+        ));
+
+        client.respond(produceResponse(Collections.singletonMap(tp0, offsetAndError)));
+        sender.runOnce();
+
+        for (Map.Entry<Integer, FutureRecordMetadata> futureEntry : futures.entrySet()) {
+            FutureRecordMetadata future = futureEntry.getValue();
+            assertTrue(future.isDone());
+
+            KafkaException exception = TestUtils.assertFutureThrows(future, KafkaException.class);
+            Integer index = futureEntry.getKey();
+            if (index == 0 || index == 2) {
+                assertTrue(exception instanceof InvalidRecordException);
+                assertEquals(index.toString(), exception.getMessage());
+            } else if (index == 3) {
+                assertTrue(exception instanceof InvalidRecordException);
+                assertEquals(Errors.INVALID_RECORD.message(), exception.getMessage());
+            } else {
+                assertEquals(KafkaException.class, exception.getClass());
+            }
+        }
+    }
+
     @Test
     public void testWhenFirstBatchExpireNoSendSecondBatchIfGuaranteeOrder() throws InterruptedException {
         long deliveryTimeoutMs = 1500L;
@@ -2867,13 +2913,25 @@ public class SenderTest {
         };
     }
 
-    class OffsetAndError {
-        long offset;
-        Errors error;
-        OffsetAndError(long offset, Errors error) {
+    private static class OffsetAndError {
+        final long offset;
+        final Errors error;
+        final List<BatchIndexAndErrorMessage> recordErrors;
+
+        OffsetAndError(
+            long offset,
+            Errors error,
+            List<BatchIndexAndErrorMessage> recordErrors
+        ) {
             this.offset = offset;
             this.error = error;
+            this.recordErrors = recordErrors;
+        }
+
+        OffsetAndError(long offset, Errors error) {
+            this(offset, error, Collections.emptyList());
         }
+
     }
 
     private FutureRecordMetadata appendToAccumulator(TopicPartition tp) throws InterruptedException {
@@ -2893,16 +2951,29 @@ public class SenderTest {
         return new ProduceResponse(partResp, throttleTimeMs);
     }
 
-    @SuppressWarnings("deprecation")
     private ProduceResponse produceResponse(Map<TopicPartition, OffsetAndError> responses) {
-        Map<TopicPartition, ProduceResponse.PartitionResponse> partResponses = new LinkedHashMap<>();
+        ProduceResponseData data = new ProduceResponseData();
+
         for (Map.Entry<TopicPartition, OffsetAndError> entry : responses.entrySet()) {
-            ProduceResponse.PartitionResponse response = new ProduceResponse.PartitionResponse(entry.getValue().error,
-                    entry.getValue().offset, RecordBatch.NO_TIMESTAMP, -1);
-            partResponses.put(entry.getKey(), response);
+            TopicPartition topicPartition = entry.getKey();
+            ProduceResponseData.TopicProduceResponse topicData = data.responses().find(topicPartition.topic());
+            if (topicData == null) {
+                topicData = new ProduceResponseData.TopicProduceResponse().setName(topicPartition.topic());
+                data.responses().add(topicData);
+            }
+
+            OffsetAndError offsetAndError = entry.getValue();
+            ProduceResponseData.PartitionProduceResponse partitionData =
+                new ProduceResponseData.PartitionProduceResponse()
+                    .setIndex(topicPartition.partition())
+                    .setBaseOffset(offsetAndError.offset)
+                    .setErrorCode(offsetAndError.error.code())
+                    .setRecordErrors(offsetAndError.recordErrors);
+
+            topicData.partitionResponses().add(partitionData);
         }
-        return new ProduceResponse(partResponses);
 
+        return new ProduceResponse(data);
     }
     private ProduceResponse produceResponse(TopicPartition tp, long offset, Errors error, int throttleTimeMs) {
         return produceResponse(tp, offset, error, throttleTimeMs, -1L, null);
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java
index 9f097fc423..a14d29522c 100644
--- a/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java
@@ -595,7 +595,7 @@ public class TransactionManagerTest {
         long b1AppendTime = time.milliseconds();
         ProduceResponse.PartitionResponse b1Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        b1.done(500L, b1AppendTime, null);
+        b1.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(b1, b1Response);
 
         // We get an UNKNOWN_PRODUCER_ID, so bump the epoch and set sequence numbers back to 0
@@ -3084,12 +3084,12 @@ public class TransactionManagerTest {
         long b1AppendTime = time.milliseconds();
         ProduceResponse.PartitionResponse t0b1Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp0b1.done(500L, b1AppendTime, null);
+        tp0b1.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp0b1, t0b1Response);
 
         ProduceResponse.PartitionResponse t1b1Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp1b1.done(500L, b1AppendTime, null);
+        tp1b1.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp1b1, t1b1Response);
 
         // We bump the epoch and set sequence numbers back to 0
@@ -3134,7 +3134,7 @@ public class TransactionManagerTest {
         // After successfully retrying, there should be no in-flight batches for tp1 and the sequence should be 0
         t1b2Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp1b2.done(500L, b1AppendTime, null);
+        tp1b2.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp1b2, t1b2Response);
 
         transactionManager.maybeUpdateProducerIdAndEpoch(tp1);
@@ -3149,7 +3149,7 @@ public class TransactionManagerTest {
 
         ProduceResponse.PartitionResponse t1b3Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp1b3.done(500L, b1AppendTime, null);
+        tp1b3.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp1b3, t1b3Response);
 
         transactionManager.maybeUpdateProducerIdAndEpoch(tp1);
@@ -3208,12 +3208,12 @@ public class TransactionManagerTest {
         long b1AppendTime = time.milliseconds();
         ProduceResponse.PartitionResponse t0b1Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp0b1.done(500L, b1AppendTime, null);
+        tp0b1.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp0b1, t0b1Response);
 
         ProduceResponse.PartitionResponse t1b1Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp1b1.done(500L, b1AppendTime, null);
+        tp1b1.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp1b1, t1b1Response);
 
         // We bump the epoch and set sequence numbers back to 0
@@ -3258,7 +3258,7 @@ public class TransactionManagerTest {
         // After successfully retrying, there should be no in-flight batches for tp1 and the sequence should be 0
         t1b2Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp1b2.done(500L, b1AppendTime, null);
+        tp1b2.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp1b2, t1b2Response);
 
         transactionManager.maybeUpdateProducerIdAndEpoch(tp1);
@@ -3273,7 +3273,7 @@ public class TransactionManagerTest {
 
         ProduceResponse.PartitionResponse t1b3Response = new ProduceResponse.PartitionResponse(
                 Errors.NONE, 500L, b1AppendTime, 0L);
-        tp1b3.done(500L, b1AppendTime, null);
+        tp1b3.complete(500L, b1AppendTime);
         transactionManager.handleCompletedBatch(tp1b3, t1b3Response);
 
         assertFalse(transactionManager.hasInflightBatches(tp1));
diff --git a/core/src/test/scala/integration/kafka/api/PlaintextProducerSendTest.scala b/core/src/test/scala/integration/kafka/api/PlaintextProducerSendTest.scala
index 526f7266a1..38febbcd7a 100644
--- a/core/src/test/scala/integration/kafka/api/PlaintextProducerSendTest.scala
+++ b/core/src/test/scala/integration/kafka/api/PlaintextProducerSendTest.scala
@@ -100,7 +100,6 @@ class PlaintextProducerSendTest extends BaseProducerSendTest {
       val e = assertThrows(classOf[ExecutionException],
         () => producer.send(new ProducerRecord(topic, 0, System.currentTimeMillis() - 1001, "key".getBytes, "value".getBytes)).get()).getCause
       assertTrue(e.isInstanceOf[InvalidTimestampException])
-      assertEquals("One or more records have been rejected due to invalid timestamp", e.getMessage)
     } finally {
       producer.close()
     }
@@ -111,7 +110,6 @@ class PlaintextProducerSendTest extends BaseProducerSendTest {
       val e = assertThrows(classOf[ExecutionException],
         () => compressedProducer.send(new ProducerRecord(topic, 0, System.currentTimeMillis() - 1001, "key".getBytes, "value".getBytes)).get()).getCause
       assertTrue(e.isInstanceOf[InvalidTimestampException])
-      assertEquals("One or more records have been rejected due to invalid timestamp", e.getMessage)
     } finally {
       compressedProducer.close()
     }
