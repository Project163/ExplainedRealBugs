<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:10:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6834] log cleaner should handle the case when the size of a message set is larger than the max message size</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6834</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5316&quot; title=&quot;Log cleaning can increase message size and cause cleaner to crash with buffer overflow&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5316&quot;&gt;&lt;del&gt;KAFKA-5316&lt;/del&gt;&lt;/a&gt;, we added the logic to allow a message (set) larger than the per topic message size to be written to the log during log cleaning. However, the buffer size in the log cleaner is still bounded by the per topic message size. This can cause the log cleaner to die and cause the broker to run out of disk space.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13155810">KAFKA-6834</key>
            <summary>log cleaner should handle the case when the size of a message set is larger than the max message size</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsivaram">Rajini Sivaram</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                    </labels>
                <created>Fri, 27 Apr 2018 21:51:13 +0000</created>
                <updated>Thu, 13 Feb 2020 05:06:51 +0000</updated>
                            <resolved>Wed, 9 May 2018 11:22:05 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16457130" author="junrao" created="Fri, 27 Apr 2018 21:53:37 +0000"  >&lt;p&gt;To fix this, we need to handle&#160;the cleaner buffer to grow up to the size of a single message set in the log.&lt;/p&gt;</comment>
                            <comment id="16458810" author="cmccabe" created="Mon, 30 Apr 2018 17:49:38 +0000"  >&lt;p&gt;Good catch!&lt;/p&gt;

&lt;p&gt;Maybe we should validate the CRC of the overlarge message batch before enlarging the buffer, just to make sure we&apos;re not allocating memory based on corrupt data.&lt;/p&gt;</comment>
                            <comment id="16460689" author="githubbot" created="Wed, 2 May 2018 08:16:31 +0000"  >&lt;p&gt;rajinisivaram opened a new pull request #4953: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6834&quot; title=&quot;log cleaner should handle the case when the size of a message set is larger than the max message size&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6834&quot;&gt;&lt;del&gt;KAFKA-6834&lt;/del&gt;&lt;/a&gt;: Handle compaction with batches bigger than max.message.bytes&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4953&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4953&lt;/a&gt;&lt;/p&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16468693" author="githubbot" created="Wed, 9 May 2018 10:46:39 +0000"  >&lt;p&gt;rajinisivaram closed pull request #4953: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6834&quot; title=&quot;log cleaner should handle the case when the size of a message set is larger than the max message size&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6834&quot;&gt;&lt;del&gt;KAFKA-6834&lt;/del&gt;&lt;/a&gt;: Handle compaction with batches bigger than max.message.bytes&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4953&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4953&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/ByteBufferLogInputStream.java b/clients/src/main/java/org/apache/kafka/common/record/ByteBufferLogInputStream.java&lt;br/&gt;
index 22f417f8dda..7f91f266158 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/ByteBufferLogInputStream.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/ByteBufferLogInputStream.java&lt;br/&gt;
@@ -21,6 +21,7 @@&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;/p&gt;

&lt;p&gt;+import static org.apache.kafka.common.record.Records.HEADER_SIZE_UP_TO_MAGIC;&lt;br/&gt;
 import static org.apache.kafka.common.record.Records.LOG_OVERHEAD;&lt;br/&gt;
 import static org.apache.kafka.common.record.Records.MAGIC_OFFSET;&lt;br/&gt;
 import static org.apache.kafka.common.record.Records.SIZE_OFFSET;&lt;br/&gt;
@@ -40,9 +41,33 @@&lt;/p&gt;

&lt;p&gt;     public MutableRecordBatch nextBatch() throws IOException &lt;/p&gt;
{
         int remaining = buffer.remaining();
-        if (remaining &amp;lt; LOG_OVERHEAD)
+
+        Integer batchSize = nextBatchSize();
+        if (batchSize == null || remaining &amp;lt; batchSize)
             return null;
 
+        byte magic = buffer.get(buffer.position() + MAGIC_OFFSET);
+
+        ByteBuffer batchSlice = buffer.slice();
+        batchSlice.limit(batchSize);
+        buffer.position(buffer.position() + batchSize);
+
+        if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V1)
+            return new DefaultRecordBatch(batchSlice);
+        else
+            return new AbstractLegacyRecordBatch.ByteBufferLegacyRecordBatch(batchSlice);
+    }
&lt;p&gt;+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Validates the header of the next batch and returns batch size.&lt;br/&gt;
+     * @return next batch size including LOG_OVERHEAD if buffer contains header up to&lt;br/&gt;
+     *         magic byte, null otherwise&lt;br/&gt;
+     * @throws CorruptRecordException if record size or magic is invalid&lt;br/&gt;
+     */&lt;br/&gt;
+    Integer nextBatchSize() throws CorruptRecordException {&lt;br/&gt;
+        int remaining = buffer.remaining();&lt;br/&gt;
+        if (remaining &amp;lt; LOG_OVERHEAD)&lt;br/&gt;
+            return null;&lt;br/&gt;
         int recordSize = buffer.getInt(buffer.position() + SIZE_OFFSET);&lt;br/&gt;
         // V0 has the smallest overhead, stricter checking is done later&lt;br/&gt;
         if (recordSize &amp;lt; LegacyRecord.RECORD_OVERHEAD_V0)&lt;br/&gt;
@@ -52,23 +77,13 @@ public MutableRecordBatch nextBatch() throws IOException &lt;/p&gt;
{
             throw new CorruptRecordException(String.format(&quot;Record size %d exceeds the largest allowable message size (%d).&quot;,
                     recordSize, maxMessageSize));
 
-        int batchSize = recordSize + LOG_OVERHEAD;
-        if (remaining &amp;lt; batchSize)
+        if (remaining &amp;lt; HEADER_SIZE_UP_TO_MAGIC)
             return null;
 
         byte magic = buffer.get(buffer.position() + MAGIC_OFFSET);
-
-        ByteBuffer batchSlice = buffer.slice();
-        batchSlice.limit(batchSize);
-        buffer.position(buffer.position() + batchSize);
-
         if (magic &amp;lt; 0 || magic &amp;gt; RecordBatch.CURRENT_MAGIC_VALUE)
             throw new CorruptRecordException(&quot;Invalid magic found in record: &quot; + magic);
 
-        if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V1)
-            return new DefaultRecordBatch(batchSlice);
-        else
-            return new AbstractLegacyRecordBatch.ByteBufferLegacyRecordBatch(batchSlice);
+        return recordSize + LOG_OVERHEAD;
     }
&lt;p&gt;-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
index ea6aa4ce3a9..eb4e31b6e58 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;/p&gt;

&lt;p&gt; import org.apache.kafka.common.KafkaException;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.errors.CorruptRecordException;&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter.BatchRetention;&lt;br/&gt;
 import org.apache.kafka.common.utils.ByteBufferOutputStream;&lt;br/&gt;
 import org.apache.kafka.common.utils.CloseableIterator;&lt;br/&gt;
@@ -117,6 +118,18 @@ public int validBytes() &lt;/p&gt;
{
         return downConvert(batches(), toMagic, firstOffset, time);
     }

&lt;p&gt;+    /**&lt;br/&gt;
+     * Validates the header of the first batch and returns batch size.&lt;br/&gt;
+     * @return first batch size including LOG_OVERHEAD if buffer contains header up to&lt;br/&gt;
+     *         magic byte, null otherwise&lt;br/&gt;
+     * @throws CorruptRecordException if record size or magic is invalid&lt;br/&gt;
+     */&lt;br/&gt;
+    public Integer firstBatchSize() &lt;/p&gt;
{
+        if (buffer.remaining() &amp;lt; HEADER_SIZE_UP_TO_MAGIC)
+            return null;
+        return new ByteBufferLogInputStream(buffer, Integer.MAX_VALUE).nextBatchSize();
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Filter the records into the provided ByteBuffer.&lt;br/&gt;
      *&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java&lt;br/&gt;
index e1409e052f6..61d8a00865b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java&lt;br/&gt;
@@ -17,6 +17,7 @@&lt;br/&gt;
 package org.apache.kafka.common.record;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.errors.CorruptRecordException;&lt;br/&gt;
 import org.apache.kafka.common.header.internals.RecordHeaders;&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter.BatchRetention;&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
@@ -794,6 +795,47 @@ public void testFilterToPreservesLogAppendTime() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testNextBatchSize() {&lt;br/&gt;
+        ByteBuffer buffer = ByteBuffer.allocate(2048);&lt;br/&gt;
+        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression,&lt;br/&gt;
+                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, pid, epoch, firstSequence);&lt;br/&gt;
+        builder.append(10L, null, &quot;abc&quot;.getBytes());&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+&lt;br/&gt;
+        buffer.flip();&lt;br/&gt;
+        int size = buffer.remaining();&lt;br/&gt;
+        MemoryRecords records = MemoryRecords.readableRecords(buffer);&lt;br/&gt;
+        assertEquals(size, records.firstBatchSize().intValue());&lt;br/&gt;
+        assertEquals(0, buffer.position());&lt;br/&gt;
+&lt;br/&gt;
+        buffer.limit(1); // size not in buffer&lt;br/&gt;
+        assertEquals(null, records.firstBatchSize());&lt;br/&gt;
+        buffer.limit(Records.LOG_OVERHEAD); // magic not in buffer&lt;br/&gt;
+        assertEquals(null, records.firstBatchSize());&lt;br/&gt;
+        buffer.limit(Records.HEADER_SIZE_UP_TO_MAGIC); // payload not in buffer&lt;br/&gt;
+        assertEquals(size, records.firstBatchSize().intValue());&lt;br/&gt;
+&lt;br/&gt;
+        buffer.limit(size);&lt;br/&gt;
+        byte magic = buffer.get(Records.MAGIC_OFFSET);&lt;br/&gt;
+        buffer.put(Records.MAGIC_OFFSET, (byte) 10);&lt;br/&gt;
+        try &lt;/p&gt;
{
+            records.firstBatchSize();
+            fail(&quot;Did not fail with corrupt magic&quot;);
+        }
&lt;p&gt; catch (CorruptRecordException e) &lt;/p&gt;
{
+            // Expected exception
+        }&lt;br/&gt;
+        buffer.put(Records.MAGIC_OFFSET, magic);&lt;br/&gt;
+&lt;br/&gt;
+        buffer.put(Records.SIZE_OFFSET + 3, (byte) 0);&lt;br/&gt;
+        try {
+            records.firstBatchSize();
+            fail(&quot;Did not fail with corrupt size&quot;);
+        } catch (CorruptRecordException e) {+            // Expected exception+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     @Parameterized.Parameters(name = &quot;&lt;/p&gt;
{index}
&lt;p&gt; magic=&lt;/p&gt;
{0}
&lt;p&gt;, firstOffset=&lt;/p&gt;
{1}
&lt;p&gt;, compressionType=&lt;/p&gt;
{2}
&lt;p&gt;&quot;)&lt;br/&gt;
     public static Collection&amp;lt;Object[]&amp;gt; data() {&lt;br/&gt;
         List&amp;lt;Object[]&amp;gt; values = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogCleaner.scala b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
index ee31274f6c9..aa7cfe276c4 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
@@ -32,7 +32,7 @@ import org.apache.kafka.common.record._&lt;br/&gt;
 import org.apache.kafka.common.utils.Time&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.config.ConfigException&lt;br/&gt;
-import org.apache.kafka.common.errors.KafkaStorageException&lt;br/&gt;
+import org.apache.kafka.common.errors.&lt;/p&gt;
{CorruptRecordException, KafkaStorageException}
&lt;p&gt; import org.apache.kafka.common.record.MemoryRecords.RecordFilter&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter.BatchRetention&lt;/p&gt;

&lt;p&gt;@@ -621,13 +621,46 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;       // if we read bytes but didn&apos;t get even one complete batch, our I/O buffer is too small, grow it and try again&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// `result.bytesRead` contains bytes from the `messagesRead` and any discarded batches.&lt;br/&gt;
+      // `result.bytesRead` contains bytes from `messagesRead` and any discarded batches.&lt;br/&gt;
       if (readBuffer.limit() &amp;gt; 0 &amp;amp;&amp;amp; result.bytesRead == 0)&lt;/li&gt;
	&lt;li&gt;growBuffers(maxLogMessageSize)&lt;br/&gt;
+        growBuffersOrFail(sourceRecords, position, maxLogMessageSize, records)&lt;br/&gt;
     }&lt;br/&gt;
     restoreBuffers()&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Grow buffers to process next batch of records from `sourceRecords.` Buffers are doubled in size&lt;br/&gt;
+   * up to a maximum of `maxLogMessageSize`. In some scenarios, a record could be bigger than the&lt;br/&gt;
+   * current maximum size configured for the log. For example:&lt;br/&gt;
+   *   1. A compacted topic using compression may contain a message set slightly larger than max.message.bytes&lt;br/&gt;
+   *   2. max.message.bytes of a topic could have been reduced after writing larger messages&lt;br/&gt;
+   * In these cases, grow the buffer to hold the next batch.&lt;br/&gt;
+   */&lt;br/&gt;
+  private def growBuffersOrFail(sourceRecords: FileRecords,&lt;br/&gt;
+                                position: Int,&lt;br/&gt;
+                                maxLogMessageSize: Int,&lt;br/&gt;
+                                memoryRecords: MemoryRecords): Unit = {&lt;br/&gt;
+&lt;br/&gt;
+    val maxSize = if (readBuffer.capacity &amp;gt;= maxLogMessageSize) {&lt;br/&gt;
+      val nextBatchSize = memoryRecords.firstBatchSize&lt;br/&gt;
+      val logDesc = s&quot;log segment ${sourceRecords.file} at position $position&quot;&lt;br/&gt;
+      if (nextBatchSize == null)&lt;br/&gt;
+        throw new IllegalStateException(s&quot;Could not determine next batch size for $logDesc&quot;)&lt;br/&gt;
+      if (nextBatchSize &amp;lt;= 0)&lt;br/&gt;
+        throw new IllegalStateException(s&quot;Invalid batch size $nextBatchSize for $logDesc&quot;)&lt;br/&gt;
+      if (nextBatchSize &amp;lt;= readBuffer.capacity)&lt;br/&gt;
+        throw new IllegalStateException(s&quot;Batch size $nextBatchSize &amp;lt; buffer size ${readBuffer.capacity}, but not processed for $logDesc&quot;)&lt;br/&gt;
+      val bytesLeft = sourceRecords.channel.size - position&lt;br/&gt;
+      if (nextBatchSize &amp;gt; bytesLeft)&lt;br/&gt;
+        throw new CorruptRecordException(s&quot;Log segment may be corrupt, batch size $nextBatchSize &amp;gt; $bytesLeft bytes left in segment for $logDesc&quot;)&lt;br/&gt;
+      nextBatchSize.intValue&lt;br/&gt;
+    } else&lt;br/&gt;
+      maxLogMessageSize&lt;br/&gt;
+&lt;br/&gt;
+    growBuffers(maxSize)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   private def shouldDiscardBatch(batch: RecordBatch,&lt;br/&gt;
                                  transactionMetadata: CleanedTransactionMetadata,&lt;br/&gt;
                                  retainTxnMarkers: Boolean): Boolean = &lt;/p&gt;
{
@@ -844,7 +877,7 @@ private[log] class Cleaner(val id: Int,
 
       // if we didn&apos;t read even one complete message, our read buffer may be too small
       if(position == startPosition)
-        growBuffers(maxLogMessageSize)
+        growBuffersOrFail(segment.log, position, maxLogMessageSize, records)
     }
&lt;p&gt;     restoreBuffers()&lt;br/&gt;
     false&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
index edc1744bfca..537c561b387 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;/p&gt;

&lt;p&gt; package kafka.log&lt;/p&gt;

&lt;p&gt;-import java.io.File&lt;br/&gt;
+import java.io.&lt;/p&gt;
{File, RandomAccessFile}
&lt;p&gt; import java.nio._&lt;br/&gt;
 import java.nio.file.Paths&lt;br/&gt;
 import java.util.Properties&lt;br/&gt;
@@ -26,6 +26,7 @@ import kafka.common._&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{BrokerTopicStats, LogDirFailureChannel}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.errors.CorruptRecordException&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
@@ -500,6 +501,78 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     assertEquals(shouldRemain, keysInLog(log))
   }

&lt;p&gt;+  /**&lt;br/&gt;
+   * Test log cleaning with logs containing messages larger than topic&apos;s max message size&lt;br/&gt;
+   */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testMessageLargerThanMaxMessageSize() &lt;/p&gt;
{
+    val (log, offsetMap) = createLogWithMessagesLargerThanMaxSize(largeMessageSize = 1024 * 1024)
+
+    val cleaner = makeCleaner(Int.MaxValue, maxMessageSize=1024)
+    cleaner.cleanSegments(log, Seq(log.logSegments.head), offsetMap, 0L, new CleanerStats)
+    val shouldRemain = keysInLog(log).filter(k =&amp;gt; !offsetMap.map.containsKey(k.toString))
+    assertEquals(shouldRemain, keysInLog(log))
+  }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Test log cleaning with logs containing messages larger than topic&apos;s max message size&lt;br/&gt;
+   * where header is corrupt&lt;br/&gt;
+   */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testMessageLargerThanMaxMessageSizeWithCorruptHeader() {&lt;br/&gt;
+    val (log, offsetMap) = createLogWithMessagesLargerThanMaxSize(largeMessageSize = 1024 * 1024)&lt;br/&gt;
+    val file = new RandomAccessFile(log.logSegments.head.log.file, &quot;rw&quot;)&lt;br/&gt;
+    file.seek(Records.MAGIC_OFFSET)&lt;br/&gt;
+    file.write(0xff)&lt;br/&gt;
+    file.close()&lt;br/&gt;
+&lt;br/&gt;
+    val cleaner = makeCleaner(Int.MaxValue, maxMessageSize=1024)&lt;br/&gt;
+    intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;CorruptRecordException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
+      cleaner.cleanSegments(log, Seq(log.logSegments.head), offsetMap, 0L, new CleanerStats)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Test log cleaning with logs containing messages larger than topic&apos;s max message size&lt;br/&gt;
+   * where message size is corrupt and larger than bytes available in log segment.&lt;br/&gt;
+   */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testCorruptMessageSizeLargerThanBytesAvailable() {&lt;br/&gt;
+    val (log, offsetMap) = createLogWithMessagesLargerThanMaxSize(largeMessageSize = 1024 * 1024)&lt;br/&gt;
+    val file = new RandomAccessFile(log.logSegments.head.log.file, &quot;rw&quot;)&lt;br/&gt;
+    file.setLength(1024)&lt;br/&gt;
+    file.close()&lt;br/&gt;
+&lt;br/&gt;
+    val cleaner = makeCleaner(Int.MaxValue, maxMessageSize=1024)&lt;br/&gt;
+    intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;CorruptRecordException&amp;#93;&lt;/span&gt; {+      cleaner.cleanSegments(log, Seq(log.logSegments.head), offsetMap, 0L, new CleanerStats)+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
+  def createLogWithMessagesLargerThanMaxSize(largeMessageSize: Int): (Log, FakeOffsetMap) = &lt;/p&gt;
{
+    val logProps = new Properties()
+    logProps.put(LogConfig.SegmentBytesProp, largeMessageSize * 16: java.lang.Integer)
+    logProps.put(LogConfig.MaxMessageBytesProp, largeMessageSize * 2: java.lang.Integer)
+
+    val log = makeLog(config = LogConfig.fromProps(logConfig.originals, logProps))
+
+    while(log.numberOfSegments &amp;lt; 2)
+      log.appendAsLeader(record(log.logEndOffset.toInt, Array.fill(largeMessageSize)(0: Byte)), leaderEpoch = 0)
+    val keysFound = keysInLog(log)
+    assertEquals(0L until log.logEndOffset, keysFound)
+
+    // Decrease the log&apos;s max message size
+    logProps.put(LogConfig.MaxMessageBytesProp, largeMessageSize / 2: java.lang.Integer)
+    log.config = LogConfig.fromProps(logConfig.originals, logProps)
+
+    // pretend we have the following keys
+    val keys = immutable.ListSet(1, 3, 5, 7, 9)
+    val map = new FakeOffsetMap(Int.MaxValue)
+    keys.foreach(k =&amp;gt; map.put(key(k), Long.MaxValue))
+
+    (log, map)
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testCleaningWithDeletes(): Unit = {&lt;br/&gt;
     val cleaner = makeCleaner(Int.MaxValue)&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="17035922" author="zhangzs" created="Thu, 13 Feb 2020 05:06:51 +0000"  >&lt;p&gt;kafka2.12_0.11.0.3 log clean tread stoped&#65292;throw exception info e.g&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// code placeholder
&lt;/span&gt;java.lang.IllegalStateException: This log contains a message larger than maximum allowable size of 1000000.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13150779">KAFKA-6762</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 39 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3t4gv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>hachikuji</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>