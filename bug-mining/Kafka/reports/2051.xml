<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:15:03 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7400] Compacted topic segments that precede the log start offset are not cleaned up</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7400</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;LogManager.cleanupLogs currently checks if a topic is compacted, and skips any deletion if it is. This means that if the log start offset increases, log segments that precede the start offset will never be deleted. The log cleanup logic should be improved to delete these segments even for compacted topics.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13184405">KAFKA-7400</key>
            <summary>Compacted topic segments that precede the log start offset are not cleaned up</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bob-barrett">Bob Barrett</assignee>
                                    <reporter username="bob-barrett">Bob Barrett</reporter>
                        <labels>
                    </labels>
                <created>Tue, 11 Sep 2018 16:55:23 +0000</created>
                <updated>Fri, 21 Sep 2018 20:32:49 +0000</updated>
                            <resolved>Fri, 21 Sep 2018 20:32:49 +0000</resolved>
                                    <version>1.1.0</version>
                    <version>1.1.1</version>
                    <version>2.0.0</version>
                                    <fixVersion>2.1.0</fixVersion>
                                    <component>log</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16613825" author="githubbot" created="Thu, 13 Sep 2018 17:25:59 +0000"  >&lt;p&gt;bob-barrett opened a new pull request #5646: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7400&quot; title=&quot;Compacted topic segments that precede the log start offset are not cleaned up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7400&quot;&gt;&lt;del&gt;KAFKA-7400&lt;/del&gt;&lt;/a&gt;: Compacted topic segments that precede the log start offse&#8230;&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5646&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5646&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   &#8230;t are not cleaned up&lt;/p&gt;

&lt;p&gt;   Currently we don&apos;t delete any log segments if the cleanup policy doesn&apos;t include delete. This patch changes the behavior to delete log segments that fully precede the log start offset even when deletion is not enabled. Tested with unit tests to verify that LogManager.cleanupLogs now cleans logs with cleanup.policy=compact and that Log.deleteOldSegments deletes segments that preced the start offset regardless of the cleanup policy.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16624132" author="githubbot" created="Fri, 21 Sep 2018 20:31:48 +0000"  >&lt;p&gt;junrao closed pull request #5646: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7400&quot; title=&quot;Compacted topic segments that precede the log start offset are not cleaned up&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7400&quot;&gt;&lt;del&gt;KAFKA-7400&lt;/del&gt;&lt;/a&gt;: Compacted topic segments that precede the log start offse&#8230;&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5646&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5646&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index afe151d69b6..c9b877bdca9 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -1353,12 +1353,17 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Delete any log segments that have either expired due to time based retention&lt;/li&gt;
	&lt;li&gt;* or because the log size is &amp;gt; retentionSize&lt;br/&gt;
+   * If topic deletion is enabled, delete any log segments that have either expired due to time based retention&lt;br/&gt;
+   * or because the log size is &amp;gt; retentionSize.&lt;br/&gt;
+   *&lt;br/&gt;
+   * Whether or not deletion is enabled, delete any log segments that are before the log start offset&lt;br/&gt;
    */&lt;br/&gt;
   def deleteOldSegments(): Int = {&lt;/li&gt;
	&lt;li&gt;if (!config.delete) return 0&lt;/li&gt;
	&lt;li&gt;deleteRetentionMsBreachedSegments() + deleteRetentionSizeBreachedSegments() + deleteLogStartOffsetBreachedSegments()&lt;br/&gt;
+    if (config.delete) 
{
+      deleteRetentionMsBreachedSegments() + deleteRetentionSizeBreachedSegments() + deleteLogStartOffsetBreachedSegments()
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      deleteLogStartOffsetBreachedSegments()
+    }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def deleteRetentionMsBreachedSegments(): Int = &lt;/p&gt;
{
diff --git a/core/src/main/scala/kafka/log/LogCleanerManager.scala b/core/src/main/scala/kafka/log/LogCleanerManager.scala
index 83d902f952a..680fa94e33e 100755
--- a/core/src/main/scala/kafka/log/LogCleanerManager.scala
+++ b/core/src/main/scala/kafka/log/LogCleanerManager.scala
@@ -171,12 +171,13 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],
   }

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Find any logs that have compact and delete enabled&lt;br/&gt;
+    * Find any logs that have compaction enabled. Include logs without delete enabled, as they may have segments&lt;br/&gt;
+    * that precede the start offset.&lt;br/&gt;
     */&lt;br/&gt;
   def deletableLogs(): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, Log)&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     inLock(lock) {&lt;br/&gt;
       val toClean = logs.filter 
{ case (topicPartition, log) =&amp;gt;
-        !inProgress.contains(topicPartition) &amp;amp;&amp;amp; isCompactAndDelete(log)
+        !inProgress.contains(topicPartition) &amp;amp;&amp;amp; log.config.compact
       }
&lt;p&gt;       toClean.foreach &lt;/p&gt;
{ case (tp, _) =&amp;gt; inProgress.put(tp, LogCleaningInProgress) }
&lt;p&gt;       toClean&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala&lt;br/&gt;
index 8cb2f9ec874..3653e282383 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala&lt;br/&gt;
@@ -77,17 +77,17 @@ class LogCleanerManagerTest extends JUnitSuite with Logging {&lt;br/&gt;
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* When looking for logs with segments ready to be deleted we shouldn&apos;t consider&lt;/li&gt;
	&lt;li&gt;* logs with cleanup.policy=compact as they shouldn&apos;t have segments truncated.&lt;br/&gt;
+    * When looking for logs with segments ready to be deleted we should consider&lt;br/&gt;
+    * logs with cleanup.policy=compact because they may have segments from before the log start offset&lt;br/&gt;
     */&lt;br/&gt;
   @Test&lt;/li&gt;
	&lt;li&gt;def testLogsWithSegmentsToDeleteShouldNotConsiderCleanupPolicyCompactLogs(): Unit = {&lt;br/&gt;
+  def testLogsWithSegmentsToDeleteShouldConsiderCleanupPolicyCompactLogs(): Unit = 
{
     val records = TestUtils.singletonRecords(&quot;test&quot;.getBytes, key=&quot;test&quot;.getBytes)
     val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)
     val cleanerManager: LogCleanerManager = createCleanerManager(log)
 
     val readyToDelete = cleanerManager.deletableLogs().size
-    assertEquals(&quot;should have 1 logs ready to be deleted&quot;, 0, readyToDelete)
+    assertEquals(&quot;should have 1 logs ready to be deleted&quot;, 1, readyToDelete)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogManagerTest.scala b/core/src/test/scala/unit/kafka/log/LogManagerTest.scala&lt;br/&gt;
index 38d6f71c7d0..ae8bc01fab2 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogManagerTest.scala&lt;br/&gt;
@@ -190,13 +190,26 @@ class LogManagerTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Ensures that LogManager only runs on logs with cleanup.policy=delete&lt;br/&gt;
+    * Ensures that LogManager doesn&apos;t run on logs with cleanup.policy=compact,delete&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;LogCleaner.CleanerThread handles all logs where compaction is enabled.&lt;br/&gt;
     */&lt;br/&gt;
   @Test&lt;br/&gt;
   def testDoesntCleanLogsWithCompactDeletePolicy() 
{
+    testDoesntCleanLogs(LogConfig.Compact + &quot;,&quot; + LogConfig.Delete)
+  }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * Ensures that LogManager doesn&apos;t run on logs with cleanup.policy=compact&lt;br/&gt;
+    * LogCleaner.CleanerThread handles all logs where compaction is enabled.&lt;br/&gt;
+    */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testDoesntCleanLogsWithCompactPolicy() &lt;/p&gt;
{
+    testDoesntCleanLogs(LogConfig.Compact)
+  }
&lt;p&gt;+&lt;br/&gt;
+  private def testDoesntCleanLogs(policy: String) {&lt;br/&gt;
     val logProps = new Properties()&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;logProps.put(LogConfig.CleanupPolicyProp, LogConfig.Compact + &quot;,&quot; + LogConfig.Delete)&lt;br/&gt;
+    logProps.put(LogConfig.CleanupPolicyProp, policy)&lt;br/&gt;
     val log = logManager.getOrCreateLog(new TopicPartition(name, 0), LogConfig.fromProps(logConfig.originals, logProps))&lt;br/&gt;
     var offset = 0L&lt;br/&gt;
     for (_ &amp;lt;- 0 until 200) {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index 9a9bc613585..1381bc66fe4 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -2746,6 +2746,32 @@ class LogTest 
{
     assertEquals(&quot;There should be 1 segment remaining&quot;, 1, log.numberOfSegments)
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def shouldDeleteStartOffsetBreachedSegmentsWhenPolicyDoesNotIncludeDelete(): Unit = &lt;/p&gt;
{
+    def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes, key = &quot;test&quot;.getBytes, timestamp = 10L)
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000, cleanupPolicy = &quot;compact&quot;)
+
+    // Create log with start offset ahead of the first log segment
+    val log = createLog(logDir, logConfig, brokerTopicStats, logStartOffset = 5L)
+
+    // append some messages to create some segments
+    for (_ &amp;lt;- 0 until 15)
+      log.appendAsLeader(createRecords, leaderEpoch = 0)
+
+    // Three segments should be created, with the first one entirely preceding the log start offset
+    assertEquals(3, log.logSegments.count(_ =&amp;gt; true))
+    assertTrue(log.logSegments.slice(1, 2).head.baseOffset &amp;lt;= log.logStartOffset)
+
+    // The first segment, which is entirely before the log start offset, should be deleted
+    // Of the remaining the segments, the first can overlap the log start offset and the rest must have a base offset
+    // greater than the start offset
+    log.onHighWatermarkIncremented(log.logEndOffset)
+    log.deleteOldSegments()
+    assertEquals(&quot;There should be 2 segments remaining&quot;, 2, log.numberOfSegments)
+    assertTrue(log.logSegments.head.baseOffset &amp;lt;= log.logStartOffset)
+    assertTrue(log.logSegments.tail.forall(s =&amp;gt; s.baseOffset &amp;gt; log.logStartOffset))
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldApplyEpochToMessageOnAppendIfLeader() {&lt;br/&gt;
     val records = (0 until 50).toArray.map(id =&amp;gt; new SimpleRecord(id.toString.getBytes))&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16624135" author="junrao" created="Fri, 21 Sep 2018 20:32:49 +0000"  >&lt;p&gt;Merged the PR to trunk.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 8 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3xz1z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>