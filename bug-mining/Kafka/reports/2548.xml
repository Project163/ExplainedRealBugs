<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:22:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-8803] Stream will not start due to TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-8803</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;One streams app is consistently failing at startup with the following exception:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-08-14 17:02:29,568 ERROR --- [2ce1b-StreamThread-2] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36. This might happen &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the broker is slow to respond, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the network connection to the broker was interrupted, or &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; similar circumstances arise. You can increase producer parameter `max.block.ms` to increase &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; timeout.
org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000milliseconds &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; awaiting InitProducerId
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These same brokers are used by many other streams without any issue, including some in the very same processes for the stream which consistently throws this exception.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;UPDATE 08/16:&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The very first instance of this error is August 13th 2019, 17:03:36.754 and it happened for 4 different streams. For 3 of these streams, the error only happened once, and then the stream recovered. For the 4th stream, the error has continued to happen, and continues to happen now.&lt;/p&gt;

&lt;p&gt;I looked up the broker logs for this time, and see that at August 13th 2019, 16:47:43, two of four brokers started reporting messages like this, for multiple partitions:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-13 20:47:43,658&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0&amp;#93;&lt;/span&gt; Retrying leaderEpoch request for partition xxx-1 as the leader reported an error: UNKNOWN_LEADER_EPOCH (kafka.server.ReplicaFetcherThread)&lt;/p&gt;

&lt;p&gt;The UNKNOWN_LEADER_EPOCH messages continued for some time, and then stopped, here is a view of the count of these messages over time:&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12977810/12977810_screenshot-1.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;However, as noted, the stream task timeout error continues to happen.&lt;/p&gt;

&lt;p&gt;I use the static consumer group protocol with Kafka 2.3.0 clients and 2.3.0 broker. The broker has a patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8773&quot; title=&quot;Static membership protocol borks on re-used group id&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8773&quot;&gt;KAFKA-8773&lt;/a&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13250832">KAFKA-8803</key>
            <summary>Stream will not start due to TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="guozhang">Guozhang Wang</assignee>
                                    <reporter username="rocketraman">Raman Gupta</reporter>
                        <labels>
                    </labels>
                <created>Wed, 14 Aug 2019 17:23:29 +0000</created>
                <updated>Sat, 26 Aug 2023 11:54:24 +0000</updated>
                            <resolved>Mon, 8 Jun 2020 21:21:07 +0000</resolved>
                                                    <fixVersion>2.3.2</fixVersion>
                    <fixVersion>2.4.2</fixVersion>
                    <fixVersion>2.5.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>22</watches>
                                                                                                                <comments>
                            <comment id="16910380" author="bbejeck" created="Mon, 19 Aug 2019 12:59:46 +0000"  >&lt;p&gt;Hi&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Thanks for reporting this.&lt;/p&gt;

&lt;p&gt;Can you share the broker logs and streams logs from when this occurred?&lt;/p&gt;

&lt;p&gt;-Bill&lt;/p&gt;</comment>
                            <comment id="16911022" author="rocketraman" created="Tue, 20 Aug 2019 06:03:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; I have attached the logs of all the broker processes, as well as the client application containing this stream, as well as some other streams, from the period when this problem first started. The file is tab-delimited, with the first column being the Kubernetes pod name (will contain kafka-x for the broker logs, or cis-x for the client logs), and the second column being the log message. The logs from the different processes are interleaved by timestamp. Let me know if that is sufficient.&lt;/p&gt;

&lt;p&gt;A few streams experienced the timeout error as you can see from the logs, but most of them recovered. The one that has not is &quot;dev-cisSegmenter-stream&quot;.&lt;/p&gt;</comment>
                            <comment id="16911395" author="bbejeck" created="Tue, 20 Aug 2019 14:19:02 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;&#160;I&apos;ll take a look over the logs.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;BTW you mention the broker having a patch for&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8773&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8773&lt;/a&gt;, but as far as I know, no one merged a PR fixing the issue.&#160; Were you thinking of another issue maybe?&lt;/p&gt;</comment>
                            <comment id="16911420" author="rocketraman" created="Tue, 20 Aug 2019 14:43:08 +0000"  >&lt;p&gt;You&apos;re right, I had the incorrect issue reference. I meant issue &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8715&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8715&lt;/a&gt; (patch &lt;a href=&quot;https://github.com/apache/kafka/pull/7116&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/7116&lt;/a&gt;, although note that my patch is actually an earlier version of this pull request that used timestamp &amp;#8211; I don&apos;t believe its relevant here though).&lt;/p&gt;</comment>
                            <comment id="16911429" author="bbejeck" created="Tue, 20 Aug 2019 14:49:27 +0000"  >&lt;p&gt;Thanks for clarifying!&lt;/p&gt;</comment>
                            <comment id="16915946" author="rocketraman" created="Mon, 26 Aug 2019 17:01:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; Any updates on this? The stream still won&apos;t start.&lt;/p&gt;</comment>
                            <comment id="16918675" author="bbejeck" created="Thu, 29 Aug 2019 14:41:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;&#160;sorry I&apos;ve been tied up with some other things, I&apos;ll take a look by COB today.&lt;/p&gt;</comment>
                            <comment id="16918695" author="rocketraman" created="Thu, 29 Aug 2019 15:07:33 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt;. Currently the stream is in the same state so if additional debugging information is needed, I can probably still get it. However, very soon I&apos;ll need to reset the environment and move on, as this stream has been down a long time.&lt;/p&gt;</comment>
                            <comment id="16918868" author="bbejeck" created="Thu, 29 Aug 2019 18:44:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think what is going is when the broker is experiencing&#160;a &lt;tt&gt;UNKNOWN_LEADER_EPOCH&lt;/tt&gt; error.&#160; But by the time the broker recovers and stabilizes more than 60 seconds has elapsed.&#160; The &lt;tt&gt;initProducerId&lt;/tt&gt; request is controlled by the &lt;tt&gt;max.block.ms&lt;/tt&gt; configuration.&#160; Try bumping up that value to something higher (I don&apos;t have a great suggestion, 5-10 minutes) and see if that helps.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
props.put(StreamsConfig.producerPrefix(ProducerConfig.MAX_BLOCK_MS_CONFIG), 600000);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;HTH,&lt;/p&gt;

&lt;p&gt;Bill&lt;/p&gt;</comment>
                            <comment id="16919744" author="rocketraman" created="Fri, 30 Aug 2019 17:23:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; I&apos;ll try that but one thing that seems odd to me: why would the next time my app restarts, the broker experience the same UNKNOWN_LEADER_EPOCH error? Is that somehow caused by the client?&lt;/p&gt;</comment>
                            <comment id="16919838" author="rocketraman" created="Fri, 30 Aug 2019 19:20:36 +0000"  >&lt;p&gt;Changing the `max.block.ms` configuration did seem to fix the problem. I&apos;m still curious to understand more about why this error kept occurring, as per my last comment. Also, should the default value of this config be higher?&lt;/p&gt;</comment>
                            <comment id="16919856" author="bbejeck" created="Fri, 30 Aug 2019 19:59:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;,&#160;I&apos;m glad to hear that setting the config helped.&#160;&lt;/p&gt;

&lt;p&gt;I have a theory about why the error kept happening, but I want to confirm my suspicions first.&#160;&lt;/p&gt;

&lt;p&gt;Picking a reasonable default value for configurations is always tricky, but IMHO, it&apos;s better to leave it at an &quot;optimistic&quot; value and adjust when needed.&lt;/p&gt;</comment>
                            <comment id="16919877" author="rocketraman" created="Fri, 30 Aug 2019 20:46:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; Hmm, I was just looking over the logs again. I thought the stream had recovered on that setting change, but in reality the brokers / stream seem to have recovered on their own. The last time the TimeoutException occurred was Aug 26th 13:04. I did also disable these streams to avoid a crash-loop in the process at that time, but today those streams did run fine without the max.block.ms change. Here is the timeline:&lt;/p&gt;

&lt;p&gt;Aug 20 16:31 - disabled stream&lt;/p&gt;

&lt;p&gt;Aug 26 12:52 - stream enabled, issue still happening&lt;/p&gt;

&lt;p&gt;Aug 26 13:04 - disabled stream again&lt;/p&gt;

&lt;p&gt;Aug 30 14:18 - stream enabled, issue now resolved (no max.block.ms change)&lt;/p&gt;

&lt;p&gt;Let me know if you want me to dig into any logs between any of the above times, but I wanted to point this behavior out in case it impacts your working theory.&lt;/p&gt;</comment>
                            <comment id="16919908" author="bbejeck" created="Fri, 30 Aug 2019 21:54:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;&#160;I think what happened is that the brokers fully recovered from the errors you saw before and the initProducerId requests were fulfilled in time.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I don&apos;t think it will be necessary for you to dig into it anymore.&#160; If it&apos;s ok with you, I&apos;ll go ahead and close this as not an issue then.&lt;/p&gt;</comment>
                            <comment id="16920004" author="rocketraman" created="Sat, 31 Aug 2019 04:00:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; If you want to close it go ahead, however, I don&apos;t really consider any situation in which a stream takes 17 days to recover normal, when using the default settings. Furthermore, the documentation for `max.block.ms` does not in any way cover this situation. It says:&lt;/p&gt;

&lt;p&gt;&amp;gt; These methods can be blocked either because the buffer is full or metadata unavailable.&lt;/p&gt;

&lt;p&gt;Neither of these was true in this situation. Furthermore the error message says: &quot;This might happen if the broker is slow to respond, if the network connection to the broker was interrupted, or if similar circumstances arise.&quot; Note that these situations explicitly refer to performance and networking problems, and do not mention that the broker state for this particular stream could be causing the issue.&lt;/p&gt;

&lt;p&gt;Furthermore, I still don&apos;t see why the broker would continue to experience the same UNKNOWN_LEADER_EPOCH error over the course of 17 days. Shouldn&apos;t the broker&apos;s recover on their own and the stream successfully reconnect once they do? Any situation in which the client is somehow causing this error to continue to happen for 17 days is in my opinion a bug (especially given I had even turned off this stream for about 6 of these 17 days, and still the brokers did not recover during this period).&lt;/p&gt;

&lt;p&gt;Given all that, it seems to me there are still lots of unexplained behavior here, and it doesn&apos;t make sense to me to close the issue.&lt;/p&gt;</comment>
                            <comment id="16955262" author="rocketraman" created="Sat, 19 Oct 2019 19:42:12 +0000"  >&lt;p&gt;And now suddenly I have this same problem again... this is super-frustrating.&lt;/p&gt;</comment>
                            <comment id="16955393" author="rocketraman" created="Sun, 20 Oct 2019 05:24:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; Now the error is happening again, for two different streams than the stream which was failing with this error before. Both of streams now experiencing issus have also been running just fine until now, and changing `max.block.ms` for them. I still get the same error message. After setting:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
props.put(StreamsConfig.producerPrefix(ProducerConfig.MAX_BLOCK_MS_CONFIG), 1200000);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;it takes longer for the error to occur but after 20 minutes it still does:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-10-19 22:10:52,910 ERROR --- [c892e-StreamThread-1] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: task [0_1] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_1. This might happen &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the broker is slow to respond, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the network connection to the broker was interrupted, or &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; similar circumstances arise. You can increase producer parameter `max.block.ms` to increase &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; timeout
org.apache.kafka.common.errors.TimeoutException: Timeout expired after 1200000milliseconds &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; awaiting InitProducerId&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So at this point, short of waiting 17 days for the stream to finally recover on its own as it did before, I don&apos;t know how to solve this, other than ditching Kafka entirely, which admittedly, is an idea looking better and better.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16955404" author="rocketraman" created="Sun, 20 Oct 2019 06:11:30 +0000"  >&lt;p&gt;I&apos;ve restarted my entire Kafka cluster, one node at a time, and that seems to have &quot;solved&quot; the problem. I have no idea what happened here, but one or more Kafka brokers must have been &quot;broken&quot; somehow.&lt;/p&gt;</comment>
                            <comment id="16955549" author="guozhang" created="Sun, 20 Oct 2019 17:24:34 +0000"  >&lt;p&gt;At the time when you observed this issue, any error entries you&apos;ve seen on the broker side?&lt;/p&gt;</comment>
                            <comment id="16956165" author="rocketraman" created="Mon, 21 Oct 2019 14:48:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; I&apos;ve provided all the logs, including broker side, as attachments to this issue. The main thing that seemed irregular was `UNKNOWN_LEADER_EPOCH` errors.&lt;/p&gt;</comment>
                            <comment id="16969090" author="timvanlaer" created="Thu, 7 Nov 2019 09:40:56 +0000"  >&lt;p&gt;I ran into the same issue. &lt;/p&gt;

&lt;p&gt;One stream instance (the one dealing with partition 52) kept failing with:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_52, processor=KSTREAM-SOURCE-0000000000, topic=xyz.entries-internal.0, partition=52, offset=5151450, stacktrace=org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000milliseconds &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; awaiting InitProducerId

  at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:380) ~[timeline-aligner.jar:?]
  at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.process(AssignedStreamsTasks.java:199) ~[timeline-aligner.jar:?]
  at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:425) ~[timeline-aligner.jar:?]
  at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:912) ~[timeline-aligner.jar:?]
  at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:819) ~[timeline-aligner.jar:?]
  at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:788) ~[timeline-aligner.jar:?]
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000milliseconds &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; awaiting InitProducerId
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It was automatically restarted every time, but it kept failing (even after stopping the whole group).&lt;/p&gt;

&lt;p&gt;Yesterday two brokers throw a UNKNOWN_LEADER_EPOCH error and after that, the client started to get into troubles. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2019-11-06 11:53:42,499] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=3] Retrying leaderEpoch request &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition xyz.entries-internal.0-52 as the leader reported an error: UNKNOWN_LEADER_EPOCH (kafka.server.ReplicaFetcherThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2019-11-06 10:06:56,652] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=3] Retrying leaderEpoch request &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition xyz.entries-internal.0-52 as the leader reported an error: UNKNOWN_LEADER_EPOCH (kafka.server.ReplicaFetcherThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Meta:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Kafka Streams 2.3.1,&lt;/li&gt;
	&lt;li&gt;Broker: patched: 2.3.1 without &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8724&quot; title=&quot;log cleaner thread dies when attempting to clean a __consumer_offsets partition after upgrade from 2.0-&amp;gt;2.3&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8724&quot;&gt;&lt;del&gt;KAFKA-8724&lt;/del&gt;&lt;/a&gt; (see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9133&quot; title=&quot;LogCleaner thread dies with: currentLog cannot be empty on an unexpected exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-9133&quot;&gt;&lt;del&gt;KAFKA-9133&lt;/del&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I will give the &lt;tt&gt;max.block.ms&lt;/tt&gt; a shot, but we&apos;re first trying a rolling restart of the brokers.&lt;/p&gt;</comment>
                            <comment id="16969161" author="timvanlaer" created="Thu, 7 Nov 2019 10:53:07 +0000"  >&lt;p&gt;I can confirm: a rolling restart of the brokers resolved the issue. &lt;/p&gt;

&lt;p&gt;It&apos;s hard to tell as our rolling restart is automated, but I have the impression a restart of the group coordinator is enough to do the trick. Would that make sense?&lt;/p&gt;</comment>
                            <comment id="16969488" author="bchen225242" created="Thu, 7 Nov 2019 18:36:40 +0000"  >&lt;p&gt;You mean &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=timvanlaer&quot; class=&quot;user-hover&quot; rel=&quot;timvanlaer&quot;&gt;timvanlaer&lt;/a&gt;&#160;transaction coordinator right? Group coordinator is handling the consumer related logic&lt;/p&gt;</comment>
                            <comment id="16969532" author="timvanlaer" created="Thu, 7 Nov 2019 20:08:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; good point, I guess so. Any way to find the transaction coordinator broker for this particular consumer?&lt;/p&gt;</comment>
                            <comment id="16977577" author="rocketraman" created="Tue, 19 Nov 2019 15:54:00 +0000"  >&lt;p&gt;We had this problem again today, and checked on each node restart whether the problem was fixed. It went away after restarting the third of four nodes.&lt;/p&gt;</comment>
                            <comment id="16983504" author="bchen225242" created="Wed, 27 Nov 2019 13:28:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;&#160;Hey Raman, I&apos;m currently investigating this issue. Just to confirm, all the EOS apps are affected but non-EOS ones are all doing fine during this time?&lt;/p&gt;</comment>
                            <comment id="16983584" author="rocketraman" created="Wed, 27 Nov 2019 14:38:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt;  actually only one eos app is affected (but seemingly random ones, not the same one every time). The rest of the EOS and non-EOS apps are fine.&lt;/p&gt;</comment>
                            <comment id="16984069" author="bchen225242" created="Thu, 28 Nov 2019 01:34:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt;&#160;Thanks, so the issue happens spontaneously without any server/client side change? Do you see any workload change on broker during that time, or leadership change?&lt;/p&gt;</comment>
                            <comment id="16984073" author="bchen225242" created="Thu, 28 Nov 2019 01:39:50 +0000"  >&lt;p&gt;To be more specific, quoted from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;```&lt;br/&gt;
we could look at request metrics to see if the delay is present&#160;&lt;br/&gt;
for example, if there is a big request queue delay, then that probably means the broker is overloaded&lt;br/&gt;
```&lt;/p&gt;</comment>
                            <comment id="16984248" author="rocketraman" created="Thu, 28 Nov 2019 09:03:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; IIRC, it seems to happen randomly to specific streams upon application startup. Once it happens, the stream usually does not recover on its own unless the brokers are restarted, even after restarting the application.&lt;/p&gt;

&lt;p&gt;I doubt the issue is broker overload. Note also that most of the streams in my system are totally fine &amp;#8211; when the issue (seemingly) randomly happens, it happens only to specific streams. If it was a broker overload issue, I would expect more streams to be affected right? Plus these brokers are really not doing that much.&lt;/p&gt;

&lt;p&gt;That being said I can still provide the info. When you say &quot;request metrics&quot; what metrics are you referring to? The producer metrics?&lt;/p&gt;

&lt;p&gt;Right now I don&apos;t have any streams with this issue, but I can make a note to check specific things the next time it happens.&lt;/p&gt;</comment>
                            <comment id="16985854" author="githubbot" created="Mon, 2 Dec 2019 06:37:52 +0000"  >&lt;p&gt;abbccdda commented on pull request #7766: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8803&quot; title=&quot;Stream will not start due to TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8803&quot;&gt;&lt;del&gt;KAFKA-8803&lt;/del&gt;&lt;/a&gt;: &lt;span class=&quot;error&quot;&gt;&amp;#91;Streams fix&amp;#93;&lt;/span&gt; retry for initTxn&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/7766&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/7766&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Currently the initTransactions() API on StreamTask will throw uncaught timeout exception. It is not a desirable outcome as we normally would expect to retry upon timeout exception as this could indicate some temporary server side unavailability. Failing immediately is not ideal.&lt;/p&gt;

&lt;p&gt;   The fix is that if we hit non-fatal exception,  stream task keeps retrying `initTransactions` until succeed.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16989995" author="githubbot" created="Fri, 6 Dec 2019 17:19:10 +0000"  >&lt;p&gt;abbccdda commented on pull request #7766: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8803&quot; title=&quot;Stream will not start due to TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8803&quot;&gt;&lt;del&gt;KAFKA-8803&lt;/del&gt;&lt;/a&gt;: &lt;span class=&quot;error&quot;&gt;&amp;#91;Streams fix&amp;#93;&lt;/span&gt; retry for initTxn&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/7766&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/7766&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16990054" author="rocketraman" created="Fri, 6 Dec 2019 18:18:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; I see the pull request adds a retry. That is fine and good, but I think that won&apos;t solve this problem. I already effectively have retries because my app, upon encountering this error, crashes with a fatal error and is restarted automatically, at which point it retries. If you look at the original posts above, you will see that some of my streams retried every couple of minutes over the course of multiple weeks to restart with no success, so adding a retry in the client isn&apos;t going to solve this problem.&lt;/p&gt;

&lt;p&gt;You have to figure out what the underlying problem in the broker is that is causing the problem in the first place.&lt;/p&gt;</comment>
                            <comment id="16990286" author="guozhang" created="Sat, 7 Dec 2019 02:34:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=timvanlaer&quot; class=&quot;user-hover&quot; rel=&quot;timvanlaer&quot;&gt;timvanlaer&lt;/a&gt; Based on the current logs / discussions we have, one possible scenario that I can conclude is that, during a leader migration, if the leaderAndIsr metadata did not reach the new leader (either ever, or not in time), hence the client -&amp;gt; broker interaction becomes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;client: &quot;txn coordinator, give me the new producer id please&quot;&lt;/li&gt;
	&lt;li&gt;broker: &quot;I do not know I&apos;m the new leader of this partition, hence I think I&apos;m not the txn coordinator; you need to ask someone else&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And eventually it will time out.&lt;/p&gt;

&lt;p&gt;What still makes me wonder is that, how long does this phase last. Since in practice such leaderAndIsr metadata propagation can be late, but would eventually arrive. What you folks describe here is that, even after you restart the producer and send initTxnId, it still does not succeed, and only after the brokers are bounced. So it&apos;s not clear to me if you bumped into &quot;metadata NEVER arrived&quot;, or &quot;metadata is delayed by too long&quot; since in the latter case, you need to either increase the max.block.time or just retry until succeed.&lt;/p&gt;

&lt;p&gt;If you believe it is the former case, could you give me the full broker log during the whole period when clients cannot succeed in initPID but timeout.&lt;/p&gt;</comment>
                            <comment id="16991244" author="timvanlaer" created="Mon, 9 Dec 2019 08:44:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; Well, hard to say but to me it looked like the state was indefinitely (no recovery at all). Only after restarting the txn coordinator, the application started processing again. So it is indeed as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; described, without intervention, the application nor the broker recovers itself.&lt;/p&gt;

&lt;p&gt;I&apos;m sorry, but I already lost the logs from that particular moment. The issue occurred only once and never appeared since then... &lt;/p&gt;
</comment>
                            <comment id="16991711" author="rocketraman" created="Mon, 9 Dec 2019 15:44:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; Yes, the state is indefinite until the brokers are bounced. I believe I posted the complete broker logs earlier &amp;#8211; what is missing from them that you need?&lt;/p&gt;

&lt;p&gt;I&apos;ve had this issue happen to me three times, but as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=timvanlaer&quot; class=&quot;user-hover&quot; rel=&quot;timvanlaer&quot;&gt;timvanlaer&lt;/a&gt; says, it does not happen consistently, nor do I know how to reproduce it.&lt;/p&gt;</comment>
                            <comment id="16993026" author="guozhang" created="Tue, 10 Dec 2019 22:57:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; I&apos;ve looked into the logs you shared, and here are some findings:&lt;/p&gt;

&lt;p&gt;1. There are a total of 9 TimeoutExceptions from the client side, the first two are on task 3 and 7, while the rest of them are on task 36. Also the task 3 / 7 failures seem not been handled with auto creation / resumption of the instance, while task 36&apos;s timeout are triggering auto creation / resumption and are migrating between thread 1, 2, and 3.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
cis-55d8b78cf-zqr4q	2019-08-13 21:04:10,501 ERROR --- [540a9-StreamThread-1] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_3] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_3.
cis-55d8b78cf-2ws25	2019-08-13 21:04:10,867 ERROR --- [111fd-StreamThread-1] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_7] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_7.
cis-55d8b78cf-zqr4q	2019-08-13 21:06:01,930 ERROR --- [d950d-StreamThread-1] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36.
cis-55d8b78cf-zqr4q	2019-08-13 21:09:21,640 ERROR --- [daa53-StreamThread-1] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36.
cis-55d8b78cf-zqr4q	2019-08-13 21:11:19,263 ERROR --- [f4cfc-StreamThread-1] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36.
cis-55d8b78cf-2ws25	2019-08-13 21:13:27,471 ERROR --- [e2227-StreamThread-2] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36.
cis-55d8b78cf-zqr4q	2019-08-13 21:14:28,419 ERROR --- [249f0-StreamThread-2] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36.
cis-55d8b78cf-2ws25	2019-08-13 21:16:37,501 ERROR --- [a6e65-StreamThread-2] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36.
cis-55d8b78cf-zqr4q	2019-08-13 21:18:35,811 ERROR --- [ad405-StreamThread-3] org.apa.kaf.str.pro.&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;.StreamTask                : task [0_36] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_36.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2. The first two errors auto recovered later, as the txn coordinator successfully completed future initProducerId requests:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
kafka-2	[2019-08-13 21:04:28,820] INFO [TransactionCoordinator id=2] Initialized transactionalId dev-cisFileIndexer-stream-0_7 with producerId 121077 and producer epoch 21 on partition __transaction_state-18 (kafka.coordinator.transaction.TransactionCoordinator)
kafka-2	[2019-08-13 21:04:29,448] INFO [TransactionCoordinator id=2] Initialized transactionalId dev-cisFileIndexer-stream-0_3 with producerId 121071 and producer epoch 21 on partition __transaction_state-14 (kafka.coordinator.transaction.TransactionCoordinator)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And although it is not directly deductionable from the logs, I believe it is because there are a bunch of topic-partitions (especially __transaction_state-XX partitions) and the broker is swamped handling those migrations and did not respond to the initProducerId requests in time, and hence the timeout exception, these failures successfully recovers as the log indicates.&lt;/p&gt;

&lt;p&gt;3. For task 0_36 though, they never recovers, but the corresponding txn topic-partition, which is 21, is actually healthy on broker-1 and it is responsible for both task 81 and task 36, and we can see successful entries for task 81 before and after the period of task 36&apos;s timeout:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
kafka-1	[2019-08-13 21:05:01,486] INFO [TransactionCoordinator id=1] Initialized transactionalId dev-cisSegmenter-stream-0_81 with producerId 120078 and producer epoch 9 on partition __transaction_state-21 (kafka.coordinator.transaction.TransactionCoordinator)
kafka-1	[2019-08-13 21:10:18,817] INFO [TransactionCoordinator id=1] Initialized transactionalId dev-cisSegmenter-stream-0_81 with producerId 120078 and producer epoch 12 on partition __transaction_state-21 (kafka.coordinator.transaction.TransactionCoordinator)
kafka-1	[2019-08-13 21:17:35,514] INFO [TransactionCoordinator id=1] Initialized transactionalId dev-cisSegmenter-stream-0_81 with producerId 120078 and producer epoch 16 on partition __transaction_state-21 (kafka.coordinator.transaction.TransactionCoordinator)
kafka-1	[2019-08-13 21:19:45,073] INFO [TransactionCoordinator id=1] Initialized transactionalId dev-cisSegmenter-stream-0_81 with producerId 120078 and producer epoch 17 on partition __transaction_state-21 (kafka.coordinator.transaction.TransactionCoordinator)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since you mentioned after bouncing the brokers handles this, while at the same time the partition is healthy and can handle other tasks, I suspect it is blocked on finishing the current dangling txn for the same txn-id from &quot;task 0_36&quot; as in &lt;tt&gt;handleEndTransaction&lt;/tt&gt; which needs to call &lt;tt&gt;appendTransactionToLog&lt;/tt&gt; to append the abort txn entry, but without more logs I cannot investigate further at this point.&lt;/p&gt;

&lt;p&gt;I&apos;m wondering which version of Kafka are you using on the broker side?&lt;/p&gt;</comment>
                            <comment id="16993219" author="rocketraman" created="Wed, 11 Dec 2019 05:46:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; As per the OP, broker is 2.3.0 (with a patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8715&quot; title=&quot;Static consumer cannot join group due to ERROR in broker&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8715&quot;&gt;&lt;del&gt;KAFKA-8715&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="16993230" author="apurva" created="Wed, 11 Dec 2019 06:14:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;gt;&#160; I suspect it is blocked on finishing the current dangling txn for the same txn-id from &quot;task 0_36&quot; as in &lt;tt&gt;handleEndTransaction&lt;/tt&gt; which needs to call &lt;tt&gt;appendTransactionToLog&lt;/tt&gt; to append the abort txn entry,&lt;/p&gt;

&lt;p&gt;Are you suggesting the broker gets into a state where it waits seemingly indefinitely for a dangling transaction so that only bouncing the broker allows the app to recover?&#160;&lt;/p&gt;</comment>
                            <comment id="16993795" author="guozhang" created="Wed, 11 Dec 2019 18:24:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurva&quot; class=&quot;user-hover&quot; rel=&quot;apurva&quot;&gt;apurva&lt;/a&gt; this is just one possible explanation of it. But without more logs I cannot verify if it is the case.&lt;/p&gt;

&lt;p&gt;I also did a quick search on bug fixes of 2.3.1, and found this one: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8635&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8635&lt;/a&gt; but that is related to &lt;tt&gt;retry.backoff.ms&lt;/tt&gt; since if it is the default value, 100ms, it should not cause 60 seconds or more delay. So at the moment the more reasonable causes I can think of is a dangling txn to be aborted, but somehow the prepare-abort record takes forever to complete.&lt;/p&gt;</comment>
                            <comment id="17016128" author="rocketraman" created="Wed, 15 Jan 2020 16:22:05 +0000"  >&lt;p&gt;This happened to me again today. Recycling 3 of 4 brokers fixed the issue.&lt;/p&gt;</comment>
                            <comment id="17016236" author="rocketraman" created="Wed, 15 Jan 2020 19:03:21 +0000"  >&lt;p&gt;Looks like this is not just a dev-ops issue, from what I&apos;m seeing here, it looks like &lt;b&gt;the EXACTLY_ONCE guarantee of the stream that failed to start with this error is being violated by Kafka&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;After restarting all my brokers I was expecting the stream to start where it left off and process all input messages. However, the stream offset was already past a message that had never been processed. This is a huge problem.&lt;/p&gt;</comment>
                            <comment id="17017287" author="rocketraman" created="Thu, 16 Jan 2020 16:40:15 +0000"  >&lt;p&gt;Very very strange. Following up on my last message, after a client restart, my stream is showing a `-` for multiple partition offsets (no offsets committed), even though every partition is assigned a consumer. The stream does not read any of the messages on that partition until more messages are sent to it. At that point it reads those messages and updates the offset. However, I certainly have messages in that topic that were &lt;b&gt;never&lt;/b&gt; processed by my EXACTLY_ONCE stream. It all seems related to this issue, and this is very scary &amp;#8211; now I&apos;m wondering what else hasn&apos;t been processed from Kafka.&lt;/p&gt;</comment>
                            <comment id="17017311" author="rocketraman" created="Thu, 16 Jan 2020 17:09:27 +0000"  >&lt;p&gt;Is it possible this is related in the opposite direction? i.e. the offsets expired due to `offsets.retention.minutes`, and then because of that this issue is triggered for the stream for which offsets were expired? I don&apos;t see why that would be the case in this situation, because the stream has been running continuously, and since the consumer group was never empty for more than 7 days, I see no reason why the offsets should have been expired. However, I throw this out there for consideration.&lt;/p&gt;</comment>
                            <comment id="17018259" author="rocketraman" created="Fri, 17 Jan 2020 19:29:34 +0000"  >&lt;p&gt;And it happened again today with the same stream. Offsets were / are not expired so never mind my last theory.&lt;/p&gt;</comment>
                            <comment id="17018847" author="panpan.liu" created="Sun, 19 Jan 2020 09:29:33 +0000"  >&lt;p&gt;I have met the same problem.&lt;/p&gt;

&lt;p&gt;kafka broker: 2.0.0&lt;/p&gt;

&lt;p&gt;kafka client/stream: 2.3.0 (enabled EOS)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Before I restarted kafka broker, I found that the __transaction_state disk volumn were very big and nerver cleaned from 2018.&lt;/p&gt;

&lt;p&gt;And then I restarted kafka brokers one by one until the broker loading fully.&lt;/p&gt;

&lt;p&gt;At the same time, at the client side, I found that the client was rebalancing, or even stop consuming forever . The longest waiting time was about one hour. I restart the client and continue to consuming then.&lt;/p&gt;

&lt;p&gt;At the broker side, I found&#160; is was reloading __transacion_state topic data, and deleting the expired data.&#160; The slowest broker costed over one hour to loading .&lt;/p&gt;

&lt;p&gt;&#160;It cost me above 5 hours to restart 5 brokers. It was very slow to load transaction data.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;So I guess that the main reason is __transaction_state huge large and causing load transaction slowing. (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9142&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-9142&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17019171" author="guozhang" created="Mon, 20 Jan 2020 01:42:08 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=panpan.liu&quot; class=&quot;user-hover&quot; rel=&quot;panpan.liu&quot;&gt;panpan.liu&lt;/a&gt; We are very actively working on this issue right now, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; is leading on the investigation. So far we&apos;ve reproduced similar issues locally and observed multiple scenarios that can cause timeout exception: i.e. if you print out the timeout exception&apos;s message it may be different depending on which round trip triggers the timeout.&lt;/p&gt;

&lt;p&gt;We will also look into &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9142&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-9142&lt;/a&gt; but one note is that from our local reproduced scenario we do not need to let the brokers to run very long (i.e. accumulate enough txn logs without deleting) to trigger this, so there&apos;re likely other reasons that can also cause the timeout.&lt;/p&gt;</comment>
                            <comment id="17021982" author="oleksii.boiko" created="Thu, 23 Jan 2020 11:29:54 +0000"  >&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;We are facing same issue. The frequency of issue reproducing was increased after migrating to Kafka-streams 2.3&lt;br/&gt;
 For our case increase was caused by changing commit behaviour in Kafka-streams. Previously(we used 2.0.1), &quot;commit offsets&quot; was executed only in case when new record was consumed on source node but in version 2.3 &quot;commit offset&quot; executes on each &quot;punctuate&quot; call even no changes were made. We have punctuator with 1s wall-clock scheduler. As the the result commit offsets operations count was grown. Additionally to this we had detected that error(on broker side) which causes transctional id stuck&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.IllegalStateException: TransactionalId &amp;lt;id&amp;gt; failed transition to state TxnTransitMetadata(producerId=61432, producerEpoch=0, txnTimeoutMs=60000, txnState=Ongoing, topicPartitions=Set(__consumer_offsets-10), txnStartTimestamp=1577934186261, txnLastUpdateTimestamp=1577934186261) due to unexpected metadata&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;appears almost every day&#160; at the same time and&#160;txnStartTimestamp of previous transaction state is younger than&#160;txnStartTimestamp of transaction target state&lt;/p&gt;

&lt;p&gt;If I&apos;m correct that means that transaction can not be transferred to &quot;Ongoing&quot; state and as the result it never expires due to only &quot;Ongoing&quot; transactions can be expired&lt;br/&gt;
 kafka.coordinator.transaction.TransactionStateManager#timedOutTransactions&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;def timedOutTransactions(): Iterable[TransactionalIdAndProducerIdEpoch] = {
    val now = time.milliseconds()
    inReadLock(stateLock) {
      transactionMetadataCache.filter { case (txnPartitionId, _) =&amp;gt;
        !leavingPartitions.exists(_.txnPartitionId == txnPartitionId)
      }.flatMap { case (_, entry) =&amp;gt;
        entry.metadataPerTransactionalId.filter { case (_, txnMetadata) =&amp;gt;
          if (txnMetadata.pendingTransitionInProgress) {
            false
          } else {
            txnMetadata.state match {
              case Ongoing =&amp;gt;
                txnMetadata.txnStartTimestamp + txnMetadata.txnTimeoutMs &amp;lt; now
              case _ =&amp;gt; false
            }
          }
        }.map { case (txnId, txnMetadata) =&amp;gt;
          TransactionalIdAndProducerIdEpoch(txnId, txnMetadata.producerId, txnMetadata.producerEpoch)
        }
      }
    }
  }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Based on this inputs we had found that time of issue reproducing is the time of ntp synchronization.&#160;&lt;br/&gt;
 Our broker timer goes forward in comparison with ntp and have up to +2 seconds per 24 hour and ntp sync rollbacks this delta.&lt;/p&gt;

&lt;p&gt;After disabling ntp sync issue was gone&lt;br/&gt;
 We had found that&#160;similar issue was already fixed previously but it does not cover all the cases &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5415&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-5415&lt;/a&gt;&lt;br/&gt;
 There is one more place where timestamp comparison exists&lt;/p&gt;

&lt;p&gt;kafka.coordinator.transaction.TransactionMetadata#completeTransitionTo&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;case Ongoing =&amp;gt; // from addPartitions if (!validProducerEpoch(transitMetadata) || !topicPartitions.subsetOf(transitMetadata.topicPartitions) || txnTimeoutMs != transitMetadata.txnTimeoutMs || txnStartTimestamp &amp;gt; transitMetadata.txnStartTimestamp) { throwStateTransitionFailure(transitMetadata) } else { txnStartTimestamp = transitMetadata.txnStartTimestamp addPartitions(transitMetadata.topicPartitions) }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17022001" author="oleksii.boiko" created="Thu, 23 Jan 2020 12:00:46 +0000"  >&lt;p&gt;Original exception full stacktrace(Broker 2.0.1)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; [2020-01-02 03:03:06,262] ERROR [KafkaApi-0] Error when handling request {transactional_id=&amp;lt;id&amp;gt;&amp;gt;,producer_id=61432,producer_epoch=0,group_id=&amp;lt;group-id&amp;gt;&amp;gt;} (kafka.server.KafkaApis)
java.lang.IllegalStateException: TransactionalId &amp;lt;id&amp;gt;&amp;gt; failed transition to state TxnTransitMetadata(producerId=61432, producerEpoch=0, txnTimeoutMs=60000, txnState=Ongoing, topicPartitions=Set(__consumer_offsets-10), txnStartTimestamp=1577934186261, txnLastUpdateTimestamp=1577934186261) due to unexpected metadata
    at kafka.coordinator.transaction.TransactionMetadata.throwStateTransitionFailure(TransactionMetadata.scala:390)
    at kafka.coordinator.transaction.TransactionMetadata.completeTransitionTo(TransactionMetadata.scala:326)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$kafka$coordinator$transaction$TransactionStateManager$$updateCacheCallback$1$1.apply$mcV$sp(TransactionStateManager.scala:542)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$kafka$coordinator$transaction$TransactionStateManager$$updateCacheCallback$1$1.apply(TransactionStateManager.scala:534)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$kafka$coordinator$transaction$TransactionStateManager$$updateCacheCallback$1$1.apply(TransactionStateManager.scala:534)
    at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
    at kafka.coordinator.transaction.TransactionMetadata.inLock(TransactionMetadata.scala:172)
    at kafka.coordinator.transaction.TransactionStateManager.kafka$coordinator$transaction$TransactionStateManager$$updateCacheCallback$1(TransactionStateManager.scala:533)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$appendTransactionToLog$1$$anonfun$apply$mcV$sp$11.apply(TransactionStateManager.scala:628)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$appendTransactionToLog$1$$anonfun$apply$mcV$sp$11.apply(TransactionStateManager.scala:628)
    at kafka.server.DelayedProduce.onComplete(DelayedProduce.scala:129)
    at kafka.server.DelayedOperation.forceComplete(DelayedOperation.scala:70)
    at kafka.server.DelayedProduce.tryComplete(DelayedProduce.scala:110)
    at kafka.server.DelayedOperationPurgatory.tryCompleteElseWatch(DelayedOperation.scala:232)
    at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:495)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$appendTransactionToLog$1.apply$mcV$sp(TransactionStateManager.scala:622)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$appendTransactionToLog$1.apply(TransactionStateManager.scala:599)
    at kafka.coordinator.transaction.TransactionStateManager$$anonfun$appendTransactionToLog$1.apply(TransactionStateManager.scala:599)
    at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
    at kafka.utils.CoreUtils$.inReadLock(CoreUtils.scala:257)
    at kafka.coordinator.transaction.TransactionStateManager.appendTransactionToLog(TransactionStateManager.scala:593)
    at kafka.coordinator.transaction.TransactionCoordinator.handleAddPartitionsToTransaction(TransactionCoordinator.scala:272)
    at kafka.server.KafkaApis.handleAddOffsetsToTxnRequest(KafkaApis.scala:1852)
    at kafka.server.KafkaApis.handle(KafkaApis.scala:138)
    at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
    at java.lang.Thread.run(Thread.java:748)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17022192" author="rocketraman" created="Thu, 23 Jan 2020 15:28:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=oleksii.boiko&quot; class=&quot;user-hover&quot; rel=&quot;oleksii.boiko&quot;&gt;oleksii.boiko&lt;/a&gt;&apos;s last message inspired me to check for `IllegalStateException` in our logs. I don&apos;t see the same error as he, but I do see 12 of these errors on our `kafka-2` broker a few hours before the last timeout error we experienced on Jan 15th &#8211; these errors always seem to occur on stream restart. The `kafka-2` broker is the one which is the same broker restarted after which the stream recovered.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2020-01-15 09:27:08,689] ERROR Uncaught exception in scheduled task &lt;span class=&quot;code-quote&quot;&gt;&apos;load-txns-&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;-partition-__transaction_state-22&apos;&lt;/span&gt; (kafka.utils.KafkaScheduler)
 java.lang.IllegalStateException: The metadata cache &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; txn partition 22 has already exist with epoch 567 and 9 entries &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; trying to add to it; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; should not happen
 at kafka.coordinator.transaction.TransactionStateManager.addLoadedTransactionsToCache(TransactionStateManager.scala:369)
 at kafka.coordinator.transaction.TransactionStateManager.$anonfun$loadTransactionsForTxnTopicPartition$3(TransactionStateManager.scala:394)
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
 at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
 at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
 at kafka.coordinator.transaction.TransactionStateManager.loadTransactions$1(TransactionStateManager.scala:393)
 at kafka.coordinator.transaction.TransactionStateManager.$anonfun$loadTransactionsForTxnTopicPartition$7(TransactionStateManager.scala:426)
 at kafka.utils.KafkaScheduler.$anonfun$schedule$2(KafkaScheduler.scala:114)
 at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:65)
 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
 at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
 at java.base/java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:834)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17057191" author="rocketraman" created="Wed, 11 Mar 2020 16:40:17 +0000"  >&lt;p&gt;Any progress on this? Still happening regularly for us.&lt;/p&gt;</comment>
                            <comment id="17057241" author="guozhang" created="Wed, 11 Mar 2020 17:31:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; Yes here are the current status:&lt;/p&gt;

&lt;p&gt;1) On the streams side, I&apos;ve merged a PR &lt;a href=&quot;https://github.com/apache/kafka/pull/8060&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/8060&lt;/a&gt; which would handle the timeout exception gracefully and retry, it is merged in trunk and hence would be in the 2.6.0 release (you can also try it out compiling from trunk).&lt;/p&gt;

&lt;p&gt;2) The root cause of this long init-pid timing out, as far as we&apos;ve investigated so far, is due to broker side partition availability and hence the txn record cannot be successfully written (note since the txn record is critical determining the state of the txn broker&apos;s require ack=all with min.isr in this append), and hence the current dangling txn cannot be aborted and thus the new PID cannot be granted. We&apos;re still working on the broker-side improvement to reduce the likelihood of this scenario that may cause timeout.&lt;/p&gt;</comment>
                            <comment id="17057262" author="rocketraman" created="Wed, 11 Mar 2020 17:44:49 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;. I&apos;m a little confused by your response. Regarding the timeout and timeout retry, as mentioned previously in this issue, I don&apos;t think that is going to help. We currently let-it-crash in this situation, and retry on the next startup, and when this error happens, these retries do not work until the brokers are restarted. Whatever state the brokers are in, is not fixed by retries nor by longer timeouts.&lt;/p&gt;</comment>
                            <comment id="17057269" author="guozhang" created="Wed, 11 Mar 2020 17:49:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=oleksii.boiko&quot; class=&quot;user-hover&quot; rel=&quot;oleksii.boiko&quot;&gt;oleksii.boiko&lt;/a&gt; indeed had a great observation about one root cause of it: in `prepareAddPartitions` the txnStartTimestamp is updated as updateTimestamp, and due to time shift which may indeed be smaller if timer goes backwards. We should have a follow-up PR as in &lt;a href=&quot;https://github.com/apache/kafka/pull/3286&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3286&lt;/a&gt; to release this check. I will send a short PR shortly. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; I&apos;m still investigating your observation of &quot;The metadata cache for txn partition 22 has already exist with epoch 567 and 9 entries while trying to add to it; this should not happen&quot;, stay tuned.&lt;/p&gt;

&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17057276" author="githubbot" created="Wed, 11 Mar 2020 17:56:27 +0000"  >&lt;p&gt;guozhangwang commented on pull request #8278: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8803&quot; title=&quot;Stream will not start due to TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8803&quot;&gt;&lt;del&gt;KAFKA-8803&lt;/del&gt;&lt;/a&gt;: Remove timestamp check in completeTransitionTo&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/8278&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/8278&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   In `prepareAddPartitions` the txnStartTimestamp could be updated as updateTimestamp, which is assumed to be always larger then the original startTimestamp. However, due to ntp time shift the timer may go backwards and hence the newStartTimestamp be smaller than the original one. Then later in completeTransitionTo the time check would fail with an IllegalStateException, and the txn would not transit to Ongoing.&lt;/p&gt;

&lt;p&gt;   An indirect result of this, is that this txn would NEVER be expired anymore because only Ongoing ones would be checked for expiration.&lt;/p&gt;

&lt;p&gt;   We should do the same as in &lt;a href=&quot;https://github.com/apache/kafka/pull/3286&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3286&lt;/a&gt; to remove this check.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="17057279" author="guozhang" created="Wed, 11 Mar 2020 17:59:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/pull/8269&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/8269&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17057281" author="rocketraman" created="Wed, 11 Mar 2020 18:00:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; FYI we had this happen again today, and this time I did NOT see any errors similar to &quot;The metadata cache for txn partition 22 has already exist with epoch 567 and 9 entries while trying to add to it; this should not happen&quot; error, so it may or may not be related (probably not).&lt;/p&gt;</comment>
                            <comment id="17057285" author="guozhang" created="Wed, 11 Mar 2020 18:08:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&quot;Re: I don&apos;t think that is going to help. We currently let-it-crash in this situation, and retry on the next startup, and when this error happens, these retries do not work until the brokers are restarted.&quot;&lt;/p&gt;

&lt;p&gt;I guess my previous reply is a bit misleading. What I meant is that from our investigation, some initPID timeout are actually transient &amp;#8211; it is just taking longer than 5 minutes &amp;#8211; due to the partitions unavailable and hence abortion record cannot be written and replicated, and hence the transition to abort-prepare cannot be completed. In this case retries would help.&lt;/p&gt;

&lt;p&gt;In other cases, we also observe that the timeout is actually permanent (this is I believe you&apos;ve encountered), and that we believe is a broker-side issue that we are still investigating, as I tried to explain in 2).&lt;/p&gt;</comment>
                            <comment id="17057291" author="guozhang" created="Wed, 11 Mar 2020 18:15:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; Also about your reported IllegalStateException, I just found that in later versions we have actually removed that illegal-state check as a warning: &lt;a href=&quot;https://github.com/apache/kafka/pull/7840&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/7840&lt;/a&gt;, this is as part of 2.5.0.&lt;/p&gt;</comment>
                            <comment id="17057296" author="guozhang" created="Wed, 11 Mar 2020 18:20:31 +0000"  >&lt;p&gt;Raman, could you share the broker-side logs (everything, the server logs, controller logs, anything you have) so that we can help investigating? Did you see the IllegalStateExceptio that Oleskii saw?&lt;/p&gt;</comment>
                            <comment id="17057352" author="rocketraman" created="Wed, 11 Mar 2020 20:05:53 +0000"  >&lt;p&gt;Here are the logs from this morning, bracketing the problem by an hour or so on each side, and includes logs from the broker shutdowns and restarts. The first instance of `UNKNOWN_LEADER_EPOCH` is at 14:38:36. &lt;/p&gt;

&lt;p&gt;Our client-side streams were failing from 14:59: to 16:40, the first instance of the error is at 14:59:41. The error always seems to occur for the first time after the client restarts, so its quite possible its related to a streams shutdown process, although I don&apos;t think so because the UNKNOWN_LEADER_EPOCH happened earlier. It seems like whatever the issue was occurred on the broker side, but as long as the client side didn&apos;t restart, things were ok. Just a theory though.&lt;/p&gt;

&lt;p&gt;There is no IllegalStateException as seen by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=oleksii.boiko&quot; class=&quot;user-hover&quot; rel=&quot;oleksii.boiko&quot;&gt;oleksii.boiko&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996468/12996468_logs-20200311.txt.gz&quot; title=&quot;logs-20200311.txt.gz attached to KAFKA-8803&quot;&gt;logs-20200311.txt.gz&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;  &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12996469/12996469_logs-client-20200311.txt.gz&quot; title=&quot;logs-client-20200311.txt.gz attached to KAFKA-8803&quot;&gt;logs-client-20200311.txt.gz&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; &lt;/p&gt;</comment>
                            <comment id="17058444" author="guozhang" created="Fri, 13 Mar 2020 05:57:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; I took some time on the logs, and I now wonder if you could try out the just released 2.4.1 which has this ticket resolved: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9144&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-9144&lt;/a&gt;. From the logs I suspect it was likely triggered.&lt;/p&gt;</comment>
                            <comment id="17060403" author="ableegoldman" created="Mon, 16 Mar 2020 17:59:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&#160;do you think the bug(s) you&apos;ve discovered also explain the &quot;Timeout expired while initializing transactional state&quot; issue reported&#160; in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8858&quot; title=&quot;Kafka Streams - Failed to Rebalance Error and stream consumer stuck for some reason&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8858&quot;&gt;&lt;del&gt;KAFKA-8858&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="17061205" author="githubbot" created="Tue, 17 Mar 2020 21:40:09 +0000"  >&lt;p&gt;guozhangwang commented on pull request #8278: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8803&quot; title=&quot;Stream will not start due to TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8803&quot;&gt;&lt;del&gt;KAFKA-8803&lt;/del&gt;&lt;/a&gt;: Remove timestamp check in completeTransitionTo&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/8278&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/8278&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="17062177" author="guozhang" created="Thu, 19 Mar 2020 00:40:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; I think 8858 and 8803 are referring to the same issue: in Kafka 2.1 we log it as&lt;/p&gt;

&lt;p&gt;```&lt;br/&gt;
try {&lt;br/&gt;
            if (initTransactionsResult.await(maxBlockTimeMs, TimeUnit.MILLISECONDS)) &lt;/p&gt;
{
                initTransactionsResult = null;
            }
&lt;p&gt; else &lt;/p&gt;
{
                throw new TimeoutException(&quot;Timeout expired while initializing transactional state in &quot; + maxBlockTimeMs + &quot;ms.&quot;);
            }
&lt;p&gt;        } catch (InterruptedException e) &lt;/p&gt;
{
            throw new InterruptException(&quot;Initialize transactions interrupted.&quot;, e);
        }
&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;whereas in 2.3 we removed that log4j entry and just have:&lt;/p&gt;

&lt;p&gt;```&lt;br/&gt;
TransactionalRequestResult result = transactionManager.initializeTransactions();&lt;br/&gt;
sender.wakeup();&lt;br/&gt;
result.await(maxBlockTimeMs, TimeUnit.MILLISECONDS);&lt;br/&gt;
```&lt;/p&gt;</comment>
                            <comment id="17062200" author="guozhang" created="Thu, 19 Mar 2020 01:35:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; Here&apos;s my current investigation described below:&lt;/p&gt;

&lt;p&gt;There are three tasks that get bumped into this timeout issue:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;test-fileAnalyzer-metadataAnalysis-stream-0_30&lt;/tt&gt; is on &lt;tt&gt;__transaction_state-45&lt;/tt&gt; partition&lt;br/&gt;
&lt;tt&gt;test-fileAnalyzer-initialAnalysis-stream-0_15&lt;/tt&gt; is on &lt;tt&gt;__transaction_state-13&lt;/tt&gt; partition&lt;br/&gt;
&lt;tt&gt;test-fileAnalyzer-initialAnalysis-stream-0_6&lt;/tt&gt; is on &lt;tt&gt;__transaction_state-45&lt;/tt&gt; partition&lt;/p&gt;

&lt;p&gt;Both of these two partitions has a leader migration before the first occurrence of the timeout (around 14:40) to broker 1; from broker&apos;s logs there seems no issue after the partition migration besides the `UNKNOWN_LEADER_EPOCH` but that seems transient, since eventually the replica fetchers from broker2 and broker3 can proceed with the log truncation as well. Without looking into the actual data logs and see if epoch has indeed been regressed or not.&lt;/p&gt;</comment>
                            <comment id="17062286" author="rocketraman" created="Thu, 19 Mar 2020 04:54:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; I plan on upgrading to 2.4.1 (from 2.3.0) within the next few days.&lt;/p&gt;

&lt;p&gt;Regarding your last post, what is the conclusion you are drawing?&lt;/p&gt;</comment>
                            <comment id="17062874" author="guozhang" created="Thu, 19 Mar 2020 19:09:51 +0000"  >&lt;p&gt;I guess what I tried to convey is that it&apos;s not 100 percent verified that your bumped issue is &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9144&quot; title=&quot;Early expiration of producer state can cause coordinator epoch to regress&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-9144&quot;&gt;&lt;del&gt;KAFKA-9144&lt;/del&gt;&lt;/a&gt; for sure. There maybe other root causes that are yet to be dug out, but still if you upgraded to 2.4.1 and did not see this anymore that can also tell us something &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="17065190" author="guozhang" created="Mon, 23 Mar 2020 23:48:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; We found another cause that can let txn to be trapped inside CONCURRENT_TXN state, and hence making initPID always timed out: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9749&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-9749&lt;/a&gt; A fix would be pushed very soon (probably in the next 24 hours or so) so if you can apply that patch before rolling out the new version that should help too.&lt;/p&gt;</comment>
                            <comment id="17076895" author="rocketraman" created="Tue, 7 Apr 2020 04:38:20 +0000"  >&lt;p&gt;We are on 2.4.1 now, locally patched with the fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9749&quot; title=&quot;TransactionMarkerRequestCompletionHandler should treat storage exceptions as retriable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-9749&quot;&gt;&lt;del&gt;KAFKA-9749&lt;/del&gt;&lt;/a&gt;. So far so good.&lt;/p&gt;</comment>
                            <comment id="17077638" author="guozhang" created="Tue, 7 Apr 2020 23:02:54 +0000"  >&lt;p&gt;Great to hear! Please let us know if there&apos;s any new issues coming up.&lt;/p&gt;

&lt;p&gt;If it has been stable for say, 2 weeks, could you go ahead and resolve this ticket with fixed versions &quot;2.5.0, 2.3.2, 2.4.2&quot;? Thanks.&lt;/p&gt;</comment>
                            <comment id="17097060" author="waykarp" created="Thu, 30 Apr 2020 23:49:09 +0000"  >&lt;p&gt;We are seeing this issue quite often in client 2.3.1 and not easily reproducible.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ERROR TransactionMetadata(transactionalId=TID_edf30911-dcb9-4b1e-a6fc-b9aa91bc23f1, producerId=45014, producerEpoch=0, txnTimeoutMs=60000, state=CompleteCommit, pendingState=Some(Ongoing), topicPartitions=Set(), txnStartTimestamp=1588262781066, txnLastUpdateTimestamp=1588262781067)&apos;s transition to TxnTransitMetadata(producerId=45014, producerEpoch=0, txnTimeoutMs=60000, txnState=Ongoing, topicPartitions=Set(JOBS-0),txnStartTimestamp=1588262780837, txnLastUpdateTimestamp=1588262780837) failed: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; should not happen (kafka.coordinator.transaction.TransactionMetadata)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Can anyone suggest a way to reproduce this issue?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="17097539" author="guozhang" created="Fri, 1 May 2020 17:35:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=waykarp&quot; class=&quot;user-hover&quot; rel=&quot;waykarp&quot;&gt;waykarp&lt;/a&gt; It&apos;s a bit tricky to reproduce since we&apos;ve found and fixed at least three different root causes of it, e.g. one of them is related to possible time-shifts on local wall-clocks, which makes it even harder to reproduce in a production environment. Also note that all of the found issues are not actually on the client side, but on the broker side.&lt;/p&gt;

&lt;p&gt;So I&apos;d suggest upgrading your server side to the newly released 2.5.0 and see if it helps avoiding this scenario.&lt;/p&gt;</comment>
                            <comment id="17105691" author="rocketraman" created="Tue, 12 May 2020 19:43:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; Just ran into this problem again, with 2.4.1 + patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9749&quot; title=&quot;TransactionMarkerRequestCompletionHandler should treat storage exceptions as retriable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-9749&quot;&gt;&lt;del&gt;KAFKA-9749&lt;/del&gt;&lt;/a&gt;. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="17105739" author="guozhang" created="Tue, 12 May 2020 21:02:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; Did cherry-pick this commit as well? &lt;a href=&quot;https://github.com/apache/kafka/commit/9d68b8e3db2df135c799fc9523c99570d1ed6a26&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/commit/9d68b8e3db2df135c799fc9523c99570d1ed6a26&lt;/a&gt; It is only in 2.5.0+&lt;/p&gt;</comment>
                            <comment id="17105745" author="rocketraman" created="Tue, 12 May 2020 21:12:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; No, you never mentioned that commit above AFAICS.&lt;/p&gt;</comment>
                            <comment id="17128662" author="guozhang" created="Mon, 8 Jun 2020 21:20:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; Have you seen it again since then?&lt;/p&gt;</comment>
                            <comment id="17182553" author="eleanore0102" created="Sun, 23 Aug 2020 01:27:50 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;,&#160;&lt;/p&gt;

&lt;p&gt;I am using&#160;&lt;a href=&quot;https://beam.apache.org/documentation/io/built-in/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Apache Beam KafkaIO&lt;/a&gt; to read from kafka topics and publish to kafka topics.&lt;/p&gt;

&lt;p&gt;Recently I changed to enable transaction:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &amp;lt;K, V&amp;gt; Producer&amp;lt;K, V&amp;gt; initializeExactlyOnceProducer(
      WriteRecords&amp;lt;K, V&amp;gt; spec, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; producerName) {

    Map&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;&amp;gt; producerConfig = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HashMap&amp;lt;&amp;gt;(spec.getProducerConfig());
    producerConfig.putAll(
        ImmutableMap.of(
            ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
            spec.getKeySerializer(),
            ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
            spec.getValueSerializer(),
            ProducerSpEL.ENABLE_IDEMPOTENCE_CONFIG,
            &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;,
            ProducerSpEL.TRANSACTIONAL_ID_CONFIG,
            producerName));

    Producer&amp;lt;K, V&amp;gt; producer =
        spec.getProducerFactoryFn() != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
            ? spec.getProducerFactoryFn().apply(producerConfig)
            : &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; KafkaProducer&amp;lt;&amp;gt;(producerConfig);

    ProducerSpEL.initTransactions(producer);
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; producer;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I keeps on getting &lt;b&gt;org.apache.kafka.common.errors.TimeoutException: Timeout expired after 10000milliseconds while awaiting InitProducerId&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;My kafka client is 2.6.0, also tried 2.3.0, and kafka broker is 2.3.0.&lt;/p&gt;

&lt;p&gt;Reading from the above comments, it seems the broker requires to be upgraded to 2.5.0+, is my understanding correct? &lt;/p&gt;</comment>
                            <comment id="17192086" author="vkorna" created="Tue, 8 Sep 2020 09:42:57 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;We have recently enabled EOS and run in to this issue. Our clients 2.4.1 and broker 2.5.1.&lt;/p&gt;

&lt;p&gt;We have tried to increase max.block.ms but it did not help (the latest value was 900 seconds).&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Below is what we have in the logs on the client side:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[ERROR] [2020-09-08 08:45:11.111] [ks-node-v2-c23b6c0a-6d43-43a9-b6a2-5212dd70d849-StreamThread-1] [StreamTask:] stream-thread [ks-node-v2-c23b6c0a-6d43-43a9-b6a2-5212dd70d849-StreamThread-1] task [1_96] Timeout exception caught when initializing transactions &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 1_96. This might happen &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the broker is slow to respond, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the network connection to the broker was interrupted, or &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; similar circumstances arise. You can increase producer parameter max.block.ms to increase &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; timeout.
org.apache.kafka.common.errors.TimeoutException: Timeout expired after 900000milliseconds &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; awaiting InitProducerId
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[ERROR] [2020-09-08 08:45:11.111] [ks-node-v2-c23b6c0a-6d43-43a9-b6a2-5212dd70d849-StreamThread-1] [StreamThread:] stream-thread [ks-node-v2-c23b6c0a-6d43-43a9-b6a2-5212dd70d849-StreamThread-1] Error caught during partition assignment, will abort the current process and re-&lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; at the end of rebalance
org.apache.kafka.streams.errors.StreamsException: stream-thread [ks-node-v2-c23b6c0a-6d43-43a9-b6a2-5212dd70d849-StreamThread-1] task [1_96] Failed to initialize task 1_96 due to timeout.
	at org.apache.kafka.streams.processor.internals.StreamTask.initializeTransactions(StreamTask.java:969) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamTask.&amp;lt;init&amp;gt;(StreamTask.java:254) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamTask.&amp;lt;init&amp;gt;(StreamTask.java:176) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamThread$TaskCreator.createTask(StreamThread.java:355) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamThread$TaskCreator.createTask(StreamThread.java:313) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamThread$AbstractTaskCreator.createTasks(StreamThread.java:298) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.TaskManager.addNewActiveTasks(TaskManager.java:160) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.TaskManager.createTasks(TaskManager.java:120) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsAssigned(StreamsRebalanceListener.java:77) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsAssigned(ConsumerCoordinator.java:272) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:400) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:421) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:340) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:471) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1231) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:843) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:743) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:698) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:671) ~[ks-node-v2.jar:0.98.3-KS241-SNAPSHOT]
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired after 900000milliseconds &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; awaiting InitProducerId
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;My expectation is it should work with clients 2.4.1 as the fixes are on broker side which are in 2.5.1 already. Is that correct?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17195982" author="msilb" created="Tue, 15 Sep 2020 08:43:40 +0000"  >&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;Is there any update on this? We are facing the same issue on startup of the Streams application which causes the Streams thread to die:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2020-09-15 08:38:24.828 ERROR 1 --- [-StreamThread-1] o.a.k.s.processor.internals.StreamTask   : stream-thread [iec-x07-baseline-pc-data-storage-due-pattern-787033d4-355e-4dfc-83e0-eb1574d7d705-StreamThread-1] task [0_0] Timeout exception caught when initializing transactions for task 0_0. This might happen if the broker is slow to respond, if the network connection to the broker was interrupted, or if similar circumstances arise. You can increase producer parameter `max.block.ms` to increase this timeout.

org.apache.kafka.common.errors.TimeoutException: Timeout expired after 60000milliseconds while awaiting InitProducerId

2020-09-15 08:38:24.829 ERROR 1 --- [-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [iec-x07-baseline-pc-data-storage-due-pattern-787033d4-355e-4dfc-83e0-eb1574d7d705-StreamThread-1] Error caught during partition assignment, will abort the current process and re-throw at the end of rebalance

org.apache.kafka.streams.errors.StreamsException: stream-thread [iec-x07-baseline-pc-data-storage-due-pattern-787033d4-355e-4dfc-83e0-eb1574d7d705-StreamThread-1] task [0_0] Failed to initialize task 0_0 due to timeout.
        at org.apache.kafka.streams.processor.internals.StreamTask.initializeTransactions(StreamTask.java:923) ~[kafka-streams-2.5.0.jar!/:na]
        at org.apache.kafka.streams.processor.internals.StreamTask.&amp;lt;init&amp;gt;(StreamTask.java:206) ~[kafka-streams-2.5.0.jar!/:na]
        at org.apache.kafka.streams.processor.internals.StreamTask.&amp;lt;init&amp;gt;(StreamTask.java:115) ~[kafka-streams-2.5.0.jar!/:na]
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Client version: 2.5.0&lt;br/&gt;
Server version: 2.3.1&lt;/p&gt;</comment>
                            <comment id="17196442" author="bpuxk@yahoo.com" created="Tue, 15 Sep 2020 17:50:46 +0000"  >&lt;p&gt;Hi, we have similar issue too. kafka broker version is 2.3.1. but we are planning to upgrade to 2.5.1 soon. want to confirm is this issue fixed in 2.5.1? thanks.&#160;&lt;/p&gt;</comment>
                            <comment id="17196476" author="guozhang" created="Tue, 15 Sep 2020 18:49:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bpuxk%40yahoo.com&quot; class=&quot;user-hover&quot; rel=&quot;bpuxk@yahoo.com&quot;&gt;bpuxk@yahoo.com&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=msilb&quot; class=&quot;user-hover&quot; rel=&quot;msilb&quot;&gt;msilb&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vkorna&quot; class=&quot;user-hover&quot; rel=&quot;vkorna&quot;&gt;vkorna&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eleanore0102&quot; class=&quot;user-hover&quot; rel=&quot;eleanore0102&quot;&gt;eleanore0102&lt;/a&gt; Sorry for getting late on you folks.&lt;/p&gt;

&lt;p&gt;So far all the bug-related timeout exceptions have been nailed since the 2.5 release, on both broker and client side, which means to be on the safer side, it is recommended that you upgrade both your broker and your client to 2.5.1 and verify if you still see these issues.&lt;/p&gt;</comment>
                            <comment id="17199235" author="msilb" created="Mon, 21 Sep 2020 08:11:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; thanks, we will try to upgrade server to 2.5 when time permits. For now we were able to fix the issue with a rolling broker restart as it was suggested above.&lt;/p&gt;</comment>
                            <comment id="17213887" author="pdeole" created="Wed, 14 Oct 2020 12:55:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&#160;I recently faced this issue on our lab. The weird thing is some of the events are getting processed by the stream while other are getting this error (probably some events might be consumed by different streams app)&#160;&lt;/p&gt;

&lt;p&gt;We use following versions:&lt;/p&gt;

&lt;p&gt;Kafka broker 2.5.0&#160;&lt;/p&gt;

&lt;p&gt;Kafka streams and client 2.5.0&lt;/p&gt;

&lt;p&gt;So do i need to upgrade to 2.5.1 version for both i.e. broker as well as clients ?&lt;/p&gt;</comment>
                            <comment id="17214412" author="pdeole" created="Thu, 15 Oct 2020 03:54:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&#160;any update on above? we are on confluent 5.5.0 which internally use apache kafka 2.5.0.&#160;&lt;/p&gt;

&lt;p&gt;Do we need to upgrade to 2.5.1 version broker?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;By the way, I could observe that the issue is only with 1 of the streams application, where we have 3 streams applications. The other 2 don&apos;t have the error.&lt;/p&gt;</comment>
                            <comment id="17221813" author="guozhang" created="Tue, 27 Oct 2020 22:36:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pdeole&quot; class=&quot;user-hover&quot; rel=&quot;pdeole&quot;&gt;pdeole&lt;/a&gt; I was OOO and just saw your message. So far all issues that are found related to this observed symptom is on the broker side, but in 2.5.0 they should be all fixed. If you see other issues they may be related to other PRs not listed here. I think upgrading to 2.5.1 is worthy but I cannot tell if it would definitely fix your issue.&lt;/p&gt;</comment>
                            <comment id="17271283" author="nvolynets" created="Mon, 25 Jan 2021 12:33:57 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Just bumped with the same / similar issue (read all comments above from the beginning).&lt;/p&gt;

&lt;p&gt;Env:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Kafka Broker --&amp;gt; 2.5.0&lt;/li&gt;
	&lt;li&gt;Kafka Client --&amp;gt; 2.5.1&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;App:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Kafka producer / consumer API&lt;/li&gt;
	&lt;li&gt;Simple transactional producer/consumer&lt;/li&gt;
	&lt;li&gt;Aim to achieve exactly_once delivery&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Just wondering why this issue status has been moved to &lt;b&gt;RESOLVED&lt;/b&gt; with fix version &lt;b&gt;2.5.0&lt;/b&gt; as there are multiple comments / complains above that issue is still exists (where Kafka Broker version &amp;gt;= 2.5.0):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8803?focusedCommentId=17214412&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17214412&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;comment 1&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8803?focusedCommentId=17213887&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17213887&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;comment 2&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8803?focusedCommentId=17192086&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17192086&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;comment 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;?&lt;/p&gt;

&lt;p&gt;What do I need to do if I have bumped with the same issue ? Ask to reopen this one (with providing all the details) or create new one (but it can be closed as duplication of this RESOLVED one) ?&lt;/p&gt;</comment>
                            <comment id="17271767" author="ableegoldman" created="Tue, 26 Jan 2021 01:33:17 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nvolynets&quot; class=&quot;user-hover&quot; rel=&quot;nvolynets&quot;&gt;nvolynets&lt;/a&gt;&#160;thanks for the report. All of the known bugs related to this error should be fixed in 2.5.0, so if you&apos;re still running into this then we should definitely have an open ticket. You can reopen this one if you&apos;d like, or open a new one and just link it to this &#8211; up to you. We&apos;ll make sure it doesn&apos;t get closed as a duplicate.&lt;/p&gt;

&lt;p&gt;I will note that there&apos;s at least one other bug which could cause this, that was found somewhat recently. However it only shows up in a very specific case: when you have max.in.flight.connections set to 1 and only one broker in the bootstrap servers list. See &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-10520&quot; title=&quot;InitProducerId may be blocked if least loaded node is not ready to send&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-10520&quot;&gt;&lt;del&gt;KAFKA-10520&lt;/del&gt;&lt;/a&gt;&#160;if that sounds relevant.&lt;/p&gt;

&lt;p&gt;Of course you can also hit this as a transient issue in an unstable environment, in which case you should try fiddling with the configs as suggested in the error message and/or implementing catch + retry logic. But if you still see this error repeatedly then there&apos;s probably yet another undiscovered bug and we should have a ticket to track it&lt;/p&gt;</comment>
                            <comment id="17272019" author="nvolynets" created="Tue, 26 Jan 2021 10:09:55 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Thanks for quick reply.&lt;/p&gt;

&lt;p&gt;About&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All of the known bugs related to this error should be fixed ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Just updated my env to (to be confident that this issue is 100% fixed):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Kafka Broker --&amp;gt; 2.6.0&lt;/li&gt;
	&lt;li&gt;Kafka Client --&amp;gt; 2.5.1&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Additionally set &quot;max.block.ms&quot; to 5 mins; but essentially it will only delay the issue if it is still exists.&lt;/p&gt;

&lt;p&gt;Regarding&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-10520&quot; title=&quot;InitProducerId may be blocked if least loaded node is not ready to send&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-10520&quot;&gt;&lt;del&gt;KAFKA-10520&lt;/del&gt;&lt;/a&gt; ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Looks like as my use case too. Forced to set &quot;max.in.flight.connections&quot; to 1 (otherwise ordering will be not preserved).&lt;br/&gt;
And yeah only single broker in bootstrap servers list.&lt;/p&gt;

&lt;p&gt;To filter out this issue will set &quot;max.in.flight.connections&quot; to 5 (temporary).&lt;/p&gt;

&lt;p&gt;About&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we should definitely have an open ticket&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;With all changes mentioned above. If I will bump with this issue again then will create corresponding issue. Probably new one to decrease the level of confusion.&lt;/p&gt;

&lt;p&gt;Regarding&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;... as suggested in the error message and/or implementing catch + retry logic ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The main problem is not to implement custom / own &quot;catch + retry&quot; logic.&lt;br/&gt;
Problem is:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;that producer hangs infinitely to publish new messages / events&lt;/li&gt;
	&lt;li&gt;only Kafka Broker restart resolves issue - and yes 17 hours of recovery (for streaming app) as mentioned above by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rocketraman&quot; class=&quot;user-hover&quot; rel=&quot;rocketraman&quot;&gt;rocketraman&lt;/a&gt; is the same as infinite.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basically it blocks to use Kafka transactions (transactional API) &amp;amp; all things around it (exactly_once delivery guarantee &amp;amp; etc.).&lt;/p&gt;</comment>
                            <comment id="17272388" author="ableegoldman" created="Tue, 26 Jan 2021 20:18:31 +0000"  >&lt;p&gt;Ah, yeah, then it sounds like you may be hitting&#160;&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-10520&quot; title=&quot;InitProducerId may be blocked if least loaded node is not ready to send&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-10520&quot;&gt;&lt;del&gt;KAFKA-10520&lt;/del&gt;&lt;/a&gt;. I know that setting &quot;max.in.flight.connections&quot; to a larger value is probably not an option if ordering matters in your application so I&apos;d recommend either adding another broker to the bootstrap server list or just upgrading your client to 2.7, whichever is possible.&lt;/p&gt;

&lt;p&gt;I&apos;ll go ahead and link to that ticket in this one since it&apos;s kind of hard to discover at the moment. Let me know if your experiment with setting &quot;max.in.flight.connections&quot; to 5 solves the problem for you and confirms that it&apos;s&#160;&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-10520&quot; title=&quot;InitProducerId may be blocked if least loaded node is not ready to send&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-10520&quot;&gt;&lt;del&gt;KAFKA-10520&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17274511" author="nvolynets" created="Fri, 29 Jan 2021 15:16:08 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;JFI, as was promised. Sorry that doing it with so big delay.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Long story short&lt;/b&gt;&lt;br/&gt;
Confirmed that I&apos;ve bumped with &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-10520&quot; title=&quot;InitProducerId may be blocked if least loaded node is not ready to send&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-10520&quot;&gt;&lt;del&gt;KAFKA-10520&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Issue is not reproducible:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Kafka Client --&amp;gt; 2.5.1&lt;/li&gt;
	&lt;li&gt;Kafka Broker --&amp;gt; 2.6.0
	&lt;ul&gt;
		&lt;li&gt;with &quot;max.in.flight.connections&quot;=5&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Moved forward with adding more Kafka brokers (as you suggested):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;just bumped with issue on local test env (where single Kafka broker)&lt;/li&gt;
	&lt;li&gt;so, not an issue for prod&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks a lot!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13254243">KAFKA-8858</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13266382">KAFKA-9144</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13274893">KAFKA-9307</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13293396">KAFKA-9749</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13329185">KAFKA-10520</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13406494">KAFKA-13375</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13272597">KAFKA-9274</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12996468" name="logs-20200311.txt.gz" size="2291552" author="rocketraman" created="Wed, 11 Mar 2020 20:05:26 +0000"/>
                            <attachment id="12996469" name="logs-client-20200311.txt.gz" size="161334" author="rocketraman" created="Wed, 11 Mar 2020 20:05:25 +0000"/>
                            <attachment id="12978028" name="logs.txt.gz" size="1728254" author="rocketraman" created="Tue, 20 Aug 2019 06:02:59 +0000"/>
                            <attachment id="12977810" name="screenshot-1.png" size="21353" author="rocketraman" created="Fri, 16 Aug 2019 16:29:00 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 41 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z05o28:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>