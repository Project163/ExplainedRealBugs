<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:38:11 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-914] Deadlock between initial rebalance and watcher-triggered rebalances</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-914</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Summary doesn&apos;t give the full picture and the fetcher-manager/fetcher-thread                                                                                                                                &lt;br/&gt;
code is very complex so it&apos;s a bit hard to articulate the following very                                                                                                                                    &lt;br/&gt;
clearly. I will try and describe the sequence that results in a deadlock                                                                                                                                    &lt;br/&gt;
when starting up a large number of consumers at around the same time:                                                                                                                                       &lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;When a consumer&apos;s createMessageStream method is called, it initiates an&lt;br/&gt;
  initial inline rebalance.                                                                                                                                                                                 &lt;/li&gt;
	&lt;li&gt;However, before the above initial rebalance actually begins, a ZK watch&lt;br/&gt;
  may trigger (due to some other consumers starting up) and initiate a                                                                                                                                      &lt;br/&gt;
  rebalance. This happens successfully so fetchers start and start filling                                                                                                                                  &lt;br/&gt;
  up the chunk queues.                                                                                                                                                                                      &lt;/li&gt;
	&lt;li&gt;Another watch triggers and initiates yet another rebalance. This rebalance&lt;br/&gt;
  attempt tries to close the fetchers. Before the fetchers are stopped, we                                                                                                                                  &lt;br/&gt;
  shutdown the leader-finder-thread to prevent new fetchers from being                                                                                                                                      &lt;br/&gt;
  started.                                                                                                                                                                                                  &lt;/li&gt;
	&lt;li&gt;The shutdown is accomplished by interrupting the leader-finder-thread and&lt;br/&gt;
  then awaiting its shutdown latch.                                                                                                                                                                         &lt;/li&gt;
	&lt;li&gt;If the leader-finder-thread still has a partition without leader to&lt;br/&gt;
  process and tries to add a fetcher for it, it will get an exception                                                                                                                                       &lt;br/&gt;
  (InterruptedException if acquiring the partitionMapLock or                                                                                                                                                &lt;br/&gt;
  ClosedByInterruptException if performing an offset request). If we get an                                                                                                                                 &lt;br/&gt;
  InterruptedException the thread&apos;s interrupted flag is cleared.                                                                                                                                            &lt;/li&gt;
	&lt;li&gt;However, the leader-finder-thread may have multiple partitions without&lt;br/&gt;
  leader that it is currently processing. So if the interrupted flag is                                                                                                                                     &lt;br/&gt;
  cleared and the leader-finder-thread tries to add a fetcher for a another                                                                                                                                 &lt;br/&gt;
  partition, it does not receive an InterruptedException when it tries to                                                                                                                                   &lt;br/&gt;
  acquire the partitionMapLock. It can end up blocking indefinitely at that                                                                                                                                 &lt;br/&gt;
  point.                                                                                                                                                                                                    &lt;/li&gt;
	&lt;li&gt;The problem is that until now, the createMessageStream&apos;s initial inline&lt;br/&gt;
  rebalance has not yet returned - it is blocked on the rebalance lock                                                                                                                                      &lt;br/&gt;
  waiting on the second watch-triggered rebalance to complete. i.e., the                                                                                                                                    &lt;br/&gt;
  consumer iterators have not been created yet and thus the fetcher queues                                                                                                                                  &lt;br/&gt;
  get filled up. &lt;span class=&quot;error&quot;&gt;&amp;#91;td1&amp;#93;&lt;/span&gt;                                                                                                                                                                                      &lt;/li&gt;
	&lt;li&gt;As a result, processPartitionData (which holds on to the partitionMapLock)&lt;br/&gt;
  in one or more fetchers will be blocked trying to enqueue into a full                                                                                                                                     &lt;br/&gt;
  chunk queue.&lt;span class=&quot;error&quot;&gt;&amp;#91;td2&amp;#93;&lt;/span&gt;                                                                                                                                                                                         &lt;/li&gt;
	&lt;li&gt;So the leader-finder-thread cannot finish adding fetchers for the&lt;br/&gt;
  remaining partitions without leader and thus cannot shutdown.                                                                                                                                             &lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;One way to fix would be to let the exception from the leader-finder-thread                                                                                                                                  &lt;br/&gt;
propagate outside if the leader-finder-thread is currently shutting down -                                                                                                                                  &lt;br/&gt;
and avoid the subsequent (unnecessary) attempt to add a fetcher and lock                                                                                                                                    &lt;br/&gt;
partitionMapLock. There may be more elegant fixes (such as rewriting the                                                                                                                                    &lt;br/&gt;
whole consumer manager logic) but obviously we want to avoid extensive                                                                                                                                      &lt;br/&gt;
changes at this point in 0.8.                                                                                                                                                                               &lt;/p&gt;

&lt;p&gt;Relevant portions of the thread-dump are here:                                                                                                                                                              &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;td1&amp;#93;&lt;/span&gt; createMessageStream&apos;s initial inline rebalance (blocked on the ongoing                                                                                                                                &lt;br/&gt;
watch-triggered rebalance)                                                                                                                                                                                  &lt;/p&gt;

&lt;p&gt;2013-05-20_17:50:13.04848 &quot;main&quot; prio=10 tid=0x00007f5960008000 nid=0x3772 waiting for monitor entry &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f59666c3000&amp;#93;&lt;/span&gt;                                                                                   &lt;br/&gt;
2013-05-20_17:50:13.04848    java.lang.Thread.State: BLOCKED (on object monitor)                                                                                                                            &lt;br/&gt;
2013-05-20_17:50:13.04848       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:368)                                                     &lt;br/&gt;
2013-05-20_17:50:13.04849       - waiting to lock &amp;lt;0x00007f58637dfe10&amp;gt; (a java.lang.Object)                                                                                                                 &lt;br/&gt;
2013-05-20_17:50:13.04849       at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:678)                          &lt;br/&gt;
2013-05-20_17:50:13.04850       at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.&amp;lt;init&amp;gt;(ZookeeperConsumerConnector.scala:712)                                                            &lt;br/&gt;
2013-05-20_17:50:13.04850       at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:140)                                                             &lt;br/&gt;
2013-05-20_17:50:13.04850       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)                                                                                                          &lt;br/&gt;
2013-05-20_17:50:13.04850       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)                                                                                                          &lt;br/&gt;
2013-05-20_17:50:13.04850       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)                                                                                         &lt;br/&gt;
2013-05-20_17:50:13.04851       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)                                                                                         &lt;br/&gt;
2013-05-20_17:50:13.04851       at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)                                                                                           &lt;br/&gt;
2013-05-20_17:50:13.04851       at scala.collection.immutable.List.foreach(List.scala:45)                                                                                                                   &lt;br/&gt;
2013-05-20_17:50:13.04851       at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)                                                                                                    &lt;br/&gt;
2013-05-20_17:50:13.04852       at scala.collection.immutable.List.map(List.scala:45)                                                                                                                       &lt;br/&gt;
2013-05-20_17:50:13.04852       at kafka.tools.MirrorMaker$.main(MirrorMaker.scala:118)                                                                                                                     &lt;br/&gt;
2013-05-20_17:50:13.04852       at kafka.tools.MirrorMaker.main(MirrorMaker.scala)                                                                                                                          &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;td2&amp;#93;&lt;/span&gt; A consumer fetcher thread blocked on full queue.                                                                                                                                                      &lt;/p&gt;

&lt;p&gt;2013-05-20_17:50:13.04703 &quot;ConsumerFetcherThread-xxxx-1368836182178-2009023c-0-3248&quot; prio=10 tid=0x00007f57a4010800 nid=0x3920 waiting on condition [0x00                                                   &lt;br/&gt;
007f58316ae000]                                                                                                                                                                                             &lt;br/&gt;
2013-05-20_17:50:13.04703    java.lang.Thread.State: WAITING (parking)                                                                                                                                      &lt;br/&gt;
2013-05-20_17:50:13.04703       at sun.misc.Unsafe.park(Native Method)                                                                                                                                      &lt;br/&gt;
2013-05-20_17:50:13.04704       - parking to wait for  &amp;lt;0x00007f586381d6c0&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)                                                       &lt;br/&gt;
2013-05-20_17:50:13.04704       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)                                                                                                        &lt;br/&gt;
2013-05-20_17:50:13.04704       at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)                                                        &lt;br/&gt;
2013-05-20_17:50:13.04704       at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:306)                                                                                               &lt;br/&gt;
2013-05-20_17:50:13.04704       at kafka.consumer.PartitionTopicInfo.enqueue(PartitionTopicInfo.scala:60)                                                                                                   &lt;br/&gt;
2013-05-20_17:50:13.04705       at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:50)                                                                                &lt;br/&gt;
2013-05-20_17:50:13.04706       at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:131)                                                                 &lt;br/&gt;
2013-05-20_17:50:13.04707       at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:112)                                                                 &lt;br/&gt;
2013-05-20_17:50:13.04708       at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)                                                                                                               &lt;br/&gt;
2013-05-20_17:50:13.04709       at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:112)                                                                                  &lt;br/&gt;
2013-05-20_17:50:13.04709       at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88)                                                                                                &lt;br/&gt;
2013-05-20_17:50:13.04709       at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)                                                                                                          &lt;br/&gt;
2                                                                                                                                                                                                           &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;td3&amp;#93;&lt;/span&gt; Second watch-triggered rebalance                                                                                                                                                                      &lt;/p&gt;

&lt;p&gt;2013-05-20_17:50:13.04725 &quot;xxxx-1368836182178-2009023c_watcher_executor&quot; prio=10 tid=0x00007f5960777800 nid=0x37af waiting on condition [0x00007f58318b00                                                   &lt;br/&gt;
00]                                                                                                                                                                                                         &lt;br/&gt;
2013-05-20_17:50:13.04725    java.lang.Thread.State: WAITING (parking)                                                                                                                                      &lt;br/&gt;
2013-05-20_17:50:13.04726       at sun.misc.Unsafe.park(Native Method)                                                                                                                                      &lt;br/&gt;
2013-05-20_17:50:13.04726       - parking to wait for  &amp;lt;0x00007f5863728de8&amp;gt; (a java.util.concurrent.CountDownLatch$Sync)                                                                                    &lt;br/&gt;
2013-05-20_17:50:13.04726       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)                                                                                                        &lt;br/&gt;
2013-05-20_17:50:13.04727       at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)                                                         &lt;br/&gt;
2013-05-20_17:50:13.04727       at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)                                                  &lt;br/&gt;
2013-05-20_17:50:13.04728       at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)                                                   &lt;br/&gt;
2013-05-20_17:50:13.04728       at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)                                                                                                       &lt;br/&gt;
2013-05-20_17:50:13.04729       at kafka.utils.ShutdownableThread.shutdown(ShutdownableThread.scala:36)                                                                                                     &lt;br/&gt;
2013-05-20_17:50:13.04729       at kafka.consumer.ConsumerFetcherManager.stopConnections(ConsumerFetcherManager.scala:125)                                                                                  &lt;br/&gt;
2013-05-20_17:50:13.04730       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$closeFetchersForQueues(ZookeeperConsumerCo&lt;br/&gt;
nnector.scala:486)                                                                                                                                                                                          &lt;br/&gt;
2013-05-20_17:50:13.04730       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.closeFetchers(ZookeeperConsumerConnector.scala:523)                                                       &lt;br/&gt;
2013-05-20_17:50:13.04731       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala&lt;br/&gt;
:420)                                                                                                                                                                                                       &lt;br/&gt;
2013-05-20_17:50:13.04731       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:373)                            &lt;br/&gt;
2013-05-20_17:50:13.04732       at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)                                                                                             &lt;br/&gt;
2013-05-20_17:50:13.04733       at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)                                                                                                 &lt;br/&gt;
2013-05-20_17:50:13.04733       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:368)                                                     &lt;br/&gt;
2013-05-20_17:50:13.04733       - locked &amp;lt;0x00007f58637dfe10&amp;gt; (a java.lang.Object)                                                                                                                          &lt;br/&gt;
2013-05-20_17:50:13.04734       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anon$1.run(ZookeeperConsumerConnector.scala:325)                                                         &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;td4&amp;#93;&lt;/span&gt; leader-finder-thread still trying to process partitions without leader, blocked on the partitionMapLock held by processPartitionData in td2.                                                          &lt;/p&gt;

&lt;p&gt;2013-05-20_17:50:13.04712 &quot;xxxx-1368836182178-2009023c-leader-finder-thread&quot; prio=10 tid=0x00007f57b0027800 nid=0x38d8 waiting on condition [0x00007f5831                                                   &lt;br/&gt;
7af000]                                                                                                                                                                                                     &lt;br/&gt;
2013-05-20_17:50:13.04712    java.lang.Thread.State: WAITING (parking)                                                                                                                                      &lt;br/&gt;
2013-05-20_17:50:13.04713       at sun.misc.Unsafe.park(Native Method)                                                                                                                                      &lt;br/&gt;
2013-05-20_17:50:13.04713       - parking to wait for  &amp;lt;0x00007f586375e3d8&amp;gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)                                                                        &lt;br/&gt;
2013-05-20_17:50:13.04713       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)                                                                                                        &lt;br/&gt;
2013-05-20_17:50:13.04714       at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)                                                         &lt;br/&gt;
2013-05-20_17:50:13.04714       at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:867)                                                        &lt;br/&gt;
2013-05-20_17:50:13.04717       at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1201)                                                         &lt;br/&gt;
2013-05-20_17:50:13.04718       at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:312)                                                                                       &lt;br/&gt;
2013-05-20_17:50:13.04718       at kafka.server.AbstractFetcherThread.addPartition(AbstractFetcherThread.scala:173)                                                                                         &lt;br/&gt;
2013-05-20_17:50:13.04719       at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:48)                                                                                          &lt;br/&gt;
2013-05-20_17:50:13.04719       - locked &amp;lt;0x00007f586374b040&amp;gt; (a java.lang.Object)                                                                                                                          &lt;br/&gt;
2013-05-20_17:50:13.04719       at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$4.apply(ConsumerFetcherManager.scala:83)                                                        &lt;br/&gt;
2013-05-20_17:50:13.04720       at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$4.apply(ConsumerFetcherManager.scala:79)                                                        &lt;br/&gt;
2013-05-20_17:50:13.04721       at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)                                                                                              &lt;br/&gt;
2013-05-20_17:50:13.04721       at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)                                                                                              &lt;br/&gt;
2013-05-20_17:50:13.04721       at scala.collection.Iterator$class.foreach(Iterator.scala:631)                                                                                                              &lt;br/&gt;
2013-05-20_17:50:13.04722       at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)                                                                                                  &lt;br/&gt;
2013-05-20_17:50:13.04723       at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)                                                                                               &lt;br/&gt;
2013-05-20_17:50:13.04723       at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)                                                                                                          &lt;br/&gt;
2013-05-20_17:50:13.04723       at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)                                                                                                               &lt;br/&gt;
2013-05-20_17:50:13.04724       at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread.doWork(ConsumerFetcherManager.scala:79)                                                                         &lt;br/&gt;
2013-05-20_17:50:13.04724       at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)                                                                                                          &lt;/p&gt;</description>
                <environment></environment>
        <key id="12648572">KAFKA-914</key>
            <summary>Deadlock between initial rebalance and watcher-triggered rebalances</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jjkoshy">Joel Jacob Koshy</reporter>
                        <labels>
                    </labels>
                <created>Tue, 21 May 2013 07:38:37 +0000</created>
                <updated>Sat, 13 Feb 2016 01:20:08 +0000</updated>
                            <resolved>Wed, 22 May 2013 23:27:16 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13663146" author="jjkoshy" created="Tue, 21 May 2013 17:18:45 +0000"  >&lt;p&gt;One more point: &lt;span class=&quot;error&quot;&gt;&amp;#91;td3&amp;#93;&lt;/span&gt; above does not need to originate from a watcher-triggered rebalance. The initial rebalance can also run into the same deadlock. i.e., as long as one or more watcher-triggered rebalances succeed and start fetchers prior to the initial rebalance, we may end up in this wedged state. E.g., on another instance I saw &lt;span class=&quot;error&quot;&gt;&amp;#91;td3&amp;#93;&lt;/span&gt; but on the main thread:&lt;/p&gt;

&lt;p&gt;2013-05-21_17:07:14.34308 &quot;main&quot; prio=10 tid=0x00007f5e34008000 nid=0x4e49 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f5e3b410000&amp;#93;&lt;/span&gt;&lt;br/&gt;
2013-05-21_17:07:14.34308    java.lang.Thread.State: WAITING (parking)&lt;br/&gt;
2013-05-21_17:07:14.34309       at sun.misc.Unsafe.park(Native Method)&lt;br/&gt;
2013-05-21_17:07:14.34309       - parking to wait for  &amp;lt;0x00007f5d36d99fa0&amp;gt; (a java.util.concurrent.CountDownLatch$Sync)&lt;br/&gt;
2013-05-21_17:07:14.34309       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)&lt;br/&gt;
2013-05-21_17:07:14.34310       at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)&lt;br/&gt;
2013-05-21_17:07:14.34310       at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)&lt;br/&gt;
2013-05-21_17:07:14.34310       at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)&lt;br/&gt;
2013-05-21_17:07:14.34311       at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)&lt;br/&gt;
2013-05-21_17:07:14.34312       at kafka.utils.ShutdownableThread.shutdown(ShutdownableThread.scala:36)&lt;br/&gt;
2013-05-21_17:07:14.34313       at kafka.consumer.ConsumerFetcherManager.stopConnections(ConsumerFetcherManager.scala:125)&lt;br/&gt;
2013-05-21_17:07:14.34313       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$closeFetchersForQueues(ZookeeperConsumerCo&lt;br/&gt;
nnector.scala:486)&lt;br/&gt;
2013-05-21_17:07:14.34313       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.closeFetchers(ZookeeperConsumerConnector.scala:523)&lt;br/&gt;
2013-05-21_17:07:14.34314       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala&lt;br/&gt;
:420)&lt;br/&gt;
2013-05-21_17:07:14.34314       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:373)&lt;br/&gt;
2013-05-21_17:07:14.34315       at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)&lt;br/&gt;
2013-05-21_17:07:14.34315       at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)&lt;br/&gt;
2013-05-21_17:07:14.34316       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:368)&lt;br/&gt;
2013-05-21_17:07:14.34316       - locked &amp;lt;0x00007f5d36d4b2e0&amp;gt; (a java.lang.Object)&lt;br/&gt;
2013-05-21_17:07:14.34317       at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:678)&lt;br/&gt;
2013-05-21_17:07:14.34317       at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.&amp;lt;init&amp;gt;(ZookeeperConsumerConnector.scala:712)&lt;br/&gt;
2013-05-21_17:07:14.34318       at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:140)&lt;br/&gt;
2013-05-21_17:07:14.34318       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)&lt;br/&gt;
2013-05-21_17:07:14.34318       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)&lt;br/&gt;
2013-05-21_17:07:14.34319       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
2013-05-21_17:07:14.34319       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
2013-05-21_17:07:14.34319       at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)&lt;br/&gt;
2013-05-21_17:07:14.34320       at scala.collection.immutable.List.foreach(List.scala:45)&lt;br/&gt;
2013-05-21_17:07:14.34320       at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)&lt;br/&gt;
2013-05-21_17:07:14.34320       at scala.collection.immutable.List.map(List.scala:45)&lt;br/&gt;
2013-05-21_17:07:14.34321       at kafka.tools.MirrorMaker$.main(MirrorMaker.scala:118)&lt;br/&gt;
2013-05-21_17:07:14.34322       at kafka.tools.MirrorMaker.main(MirrorMaker.scala)&lt;/p&gt;</comment>
                            <comment id="13663446" author="jjkoshy" created="Tue, 21 May 2013 21:48:49 +0000"  >&lt;p&gt;Patch with the mentioned fix.&lt;/p&gt;

&lt;p&gt;1 - I added comments with some detail since the manager/fetcher/connector interaction is very tricky.&lt;br/&gt;
2 - Passing through throwables while shutting down. The isRunning check is probably unnecessary, but safer to keep.&lt;br/&gt;
3 - Made the following changes to the mirrormaker - I can put that in a separate jira as well.&lt;br/&gt;
  a - Currently if no streams are created, the mirrormaker doesn&apos;t quit. Setting streams to empty/nil fixes that issue.&lt;br/&gt;
  b - If a consumer-side exception (e.g., iterator timeout) gets thrown the mirror-maker does not exit. Addressed this by awaiting on the consumer threads at the end of the main method.&lt;/p&gt;
</comment>
                            <comment id="13663605" author="junrao" created="Tue, 21 May 2013 23:51:01 +0000"  >&lt;p&gt;Thanks for the patch. Looks good. +1. One minor comment: The following statement in the catch clause in MirrorMaker is unnecessary.&lt;br/&gt;
        streams = Nil&lt;/p&gt;</comment>
                            <comment id="13664474" author="nehanarkhede" created="Wed, 22 May 2013 19:59:02 +0000"  >&lt;p&gt;Thanks for the patch! Good catch, +1&lt;/p&gt;</comment>
                            <comment id="13664681" author="jjkoshy" created="Wed, 22 May 2013 23:27:16 +0000"  >&lt;p&gt;Thanks for the review. Committed after removing the unnecessary assignment in MirrorMaker.&lt;/p&gt;</comment>
                            <comment id="15145519" author="rekhajoshm" created="Fri, 12 Feb 2016 23:14:17 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
Facing similar issue; raised in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3238&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3238&lt;/a&gt;&lt;br/&gt;
Thanks&lt;br/&gt;
Rekha&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12584179" name="KAFKA-914-v1.patch" size="5798" author="jjkoshy" created="Tue, 21 May 2013 21:48:49 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>328927</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 40 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1kr1j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>329269</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>