<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:07 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-46] Commit thread, ReplicaFetcherThread for intra-cluster replication</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-46</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We need to implement the commit thread at the leader and the fetcher thread at the follower for replication the data from the leader.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12514683">KAFKA-46</key>
            <summary>Commit thread, ReplicaFetcherThread for intra-cluster replication</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nehanarkhede">Neha Narkhede</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Jul 2011 21:32:20 +0000</created>
                <updated>Fri, 1 Jun 2012 20:58:51 +0000</updated>
                            <resolved>Fri, 1 Jun 2012 20:58:51 +0000</resolved>
                                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="13201633" author="prashanth.menon" created="Mon, 6 Feb 2012 21:48:11 +0000"  >&lt;p&gt;Should this be broken down into two sub tasks: One for the commiting thread on the leader and one for the fetcher thread on the follower?&lt;/p&gt;</comment>
                            <comment id="13213817" author="junrao" created="Wed, 22 Feb 2012 18:19:37 +0000"  >&lt;p&gt;Some notes on the implementation:&lt;/p&gt;

&lt;p&gt;1. If required_acks for a produce request is not 0 or 1, the commit thread will put the request in a DelayedQueue and adds it to a watch list for that topic/partition (similar implementation as long poll in kafka-48).&lt;br/&gt;
2. There will be an &quot;offset watcher&quot; per topic/partition. The watcher fires everytime a follower in the in-sync set advances the offset. Every time the watcher fires, it checks from the head of watcher list and see if any produce request can be satisfied on this partition. If so, marks this partition as satisfied, if all partitions of the produce requests are satisfied, dequeue the request an send back the ack. &lt;br/&gt;
3. How to add/delete follower from in-sync set? We can run a background insync-check thread that does the following:&lt;br/&gt;
for each topic/partition&lt;br/&gt;
  get and save the offset of each partition of the leader replica&lt;br/&gt;
  wait for KeepInSyncTime&lt;br/&gt;
  check if any insync replica hasn&apos;t caught up to the saved offset, if so, drop it out of insync set (need to fire the corresponding offset watcher) &lt;br/&gt;
  check if any replica (not in insync set) has caught up to the saved offset, if so, add it to insync set.&lt;/p&gt;

&lt;p&gt;If we follow this approach, we can have 2 subtasks that implement &quot;offset watcher&quot; and insync-check thread. We can also have 1 separate subtask for the FetcherThread and another for the commit thread.&lt;/p&gt;</comment>
                            <comment id="13220583" author="nehanarkhede" created="Fri, 2 Mar 2012 01:35:11 +0000"  >&lt;p&gt;&amp;gt;&amp;gt; Should this be broken down into two sub tasks: One for the commiting thread on the leader and one for the fetcher thread on the follower? &lt;/p&gt;

&lt;p&gt;Lets not worry about that for now. I&apos;ll take care of this JIRA, once we resolve the other JIRAs. This one has many other dependencies.&lt;/p&gt;</comment>
                            <comment id="13237724" author="nehanarkhede" created="Sat, 24 Mar 2012 23:30:28 +0000"  >&lt;p&gt;Started work on this, will upload a patch, once &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-45&quot; title=&quot;Broker startup, leader election, becoming a leader/follower for intra-cluster replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-45&quot;&gt;&lt;del&gt;KAFKA-45&lt;/del&gt;&lt;/a&gt; is resolved&lt;/p&gt;</comment>
                            <comment id="13268057" author="nehanarkhede" created="Fri, 4 May 2012 02:55:50 +0000"  >&lt;p&gt;Attaching a draft patch for message replication. This is just to give a high level overview of the changes involved and is by no means ready to be committed. So no need for a detailed review. There are probably a few bugs lurking around. Since the changes are pretty significant, I was hoping to get some early feedback. &lt;/p&gt;

&lt;p&gt;1. Added ReplicaFetcherThread that reads data from the leader and appends to local replica log&lt;/p&gt;

&lt;p&gt;2. Added highwatermark maintenance at the leader and the follower. The highwatermark is checkpointed in the partition directory in a file named highwatermark.&lt;/p&gt;

&lt;p&gt;3. Added ISR maintenance logic on the leader. This involves possibly expanding the ISR while handling a fetch request from a follower. &lt;/p&gt;

&lt;p&gt;4. Also added ISRExpirationThread that tracks the highwatermark update time for all partitions that the leader owns and shrinks it if (hw update time + keep.in.sync.time.ms) &amp;lt; current time. &lt;/p&gt;

&lt;p&gt;5. Note that to get this patch going, I had to put in code to cover &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-302&quot; title=&quot;Implement the become leader and become follower state change operations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-302&quot;&gt;&lt;del&gt;KAFKA-302&lt;/del&gt;&lt;/a&gt;. I would encourage a more detailed review for the becomeLeader() and becomeFollower() APIs. We would either like to check it in from this patch, or if Prashanth has some patch, review that one too. &lt;/p&gt;

&lt;p&gt;I will probably add v1 patch with unit tests early next week. &lt;/p&gt;

&lt;p&gt;Also, I would like to check this in parts, if possible. Starting with probably &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-302&quot; title=&quot;Implement the become leader and become follower state change operations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-302&quot;&gt;&lt;del&gt;KAFKA-302&lt;/del&gt;&lt;/a&gt;, then the actual message replication logic. But that is open for discussion as well. &lt;/p&gt;
</comment>
                            <comment id="13269296" author="prashanth.menon" created="Sun, 6 May 2012 21:52:59 +0000"  >&lt;p&gt;I can comment on point 5, related to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-302&quot; title=&quot;Implement the become leader and become follower state change operations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-302&quot;&gt;&lt;del&gt;KAFKA-302&lt;/del&gt;&lt;/a&gt;.  After a first pass this is what I&apos;ve got, let me know what you think:&lt;/p&gt;

&lt;p&gt;1. In KafkaZooKeeper.leaderElection, if the replica isn&apos;t elected as the leader, it should issue a becomeFollower after reading who became the leader from ZK.&lt;br/&gt;
2. In KafkaServer.becomeLeader, the leader should probably update the ISR and CUR list by performing the same type of logic as the ISRExpirationThread does.  If the intention was to rely on the ISRET to perform this asynchronously, I think it&apos;ll need to be modified to update the partition&apos;s CUR list along with the ISR.&lt;br/&gt;
3. In ISRExpirationThread, you&apos;ll need to add the slowest partition back into the queue in every case.&lt;br/&gt;
4. It seems like there are two ways to update a leader replica&apos;s HW, either through Replica.hw itself or Partition.leaderHW.  To avoid confusion, can we simplify and provide only one API through which all clients to perform this?  The latter seems to do the same thing but just update the hw update time.  &lt;br/&gt;
5. Can the leo and hw methods in Replica make use of the isLocal method?  The logic is a little more clear this way, IMO.&lt;br/&gt;
6. In KafkaApis.handleFetchRequest, it looks the maybeAddReplicaToISR call is unncessary and is rolled into readMessageSets?  Any reason we need it there?&lt;br/&gt;
7. KafkaApis.readMessageSets should probably verify that the leader for the partition exists on the broker.&lt;/p&gt;

&lt;p&gt;In general, I&apos;m not entirely sold on the ISRExpirationThread.  From my point of view, there is a function that, given a partition, determines whether its ISR/CUR list needs to be updated in-memory and in ZK.  Right now, there is a single thread that uses a heap to pick off the partitions with the oldest update times, waits for expiry if necessary, then updates accordingly.  I&apos;m wondering if it&apos;s possible instead to leverage a scheduled executor that appropriately schedules the execution of the above function on a given partition based on the same criteria (the partition&apos;s HW updated time); when the task actually executes, it&apos;s possible the hw would have moved, making the task an no-op.  The benefit there is simplicity, added concurrency and a slightly more accurate/real-time reflection of the ISR list in ZK meaning a possible reduction in message loss during leadership changes?&lt;/p&gt;</comment>
                            <comment id="13271005" author="nehanarkhede" created="Wed, 9 May 2012 01:51:23 +0000"  >&lt;p&gt;Prashanth, &lt;br/&gt;
Thanks for reviewing the patch, in detail. Like I mentioned earlier, this is just a draft patch and will clearly miss some details like you&apos;ve pointed out. I&apos;m looking more for high level feedback on data structures used, any ideas for refactoring etc.&lt;/p&gt;</comment>
                            <comment id="13271009" author="junrao" created="Wed, 9 May 2012 02:04:03 +0000"  >&lt;p&gt;Some comments on the draft.&lt;/p&gt;

&lt;p&gt;High level:&lt;br/&gt;
1. We should consider whether to have 1 HW checkpoint file per partition vs 1 HW checkpoint file for all partitions. The benefit of the latter is fewer file writes during checkpoint and fewer file reads during broker startup. Also, to avoid corrupting the checkpointed file, we should probably first write the file to a tmp file and rename to the actual checkpointed file. This probably can be done in a separate jira.&lt;/p&gt;

&lt;p&gt;2. The benefit of using an ISRExpirationThread is that it&apos;s relatively simple since there is 1 thread doing all the ISR expiration. One drawback I can see is that idle partitions are still constantly checked by the thread. This may or may not be a big concern.&lt;/p&gt;

&lt;p&gt;Low level:&lt;br/&gt;
3. KafkaApis:&lt;br/&gt;
3.1 Agreed with #6 in Prashanth&apos;s comment. Probably don&apos;t need to call maybeAddReplicaToISR directly from handlFetchRequest.&lt;br/&gt;
3.2 A subtle issue is that we should probably wait until a (replica) fetch request is successful before updating the follower replica&apos;s LEO. This is because during an unclean failover (no live brokers in ISR), the offset of the first fetch request from a follower may not be valid.&lt;br/&gt;
3.3 We need to update ISR in ZK and in memory atomically since the ISR can be expanded and shrunk from different threads.&lt;/p&gt;

&lt;p&gt;4. Partition:&lt;br/&gt;
4.1 We probably don&apos;t need to add reassignedReplicas in the patch and can add it later when we get to kafka-42, if necessary.&lt;br/&gt;
4.2 We probably don&apos;t need both catchUpReplicas and assignedReplicas since we can always derive one from another together with ISR.&lt;br/&gt;
4.3 Do we need to maintain a HashMap of &amp;lt;replica_id., Replica&amp;gt;, instead of a set of replicas for faster lookup? This may not be a big deal since the replica set is small.&lt;br/&gt;
4.4 Should we keep highWatermarkUpdateTime in Log where the HW is stored?&lt;/p&gt;

&lt;p&gt;5. Replica:&lt;br/&gt;
5.1 leo(), if log is present, we should return l.leo not l.getHighwaterMark.&lt;/p&gt;

&lt;p&gt;6. KafkaConfig: All follower related properties should be probably be prefixed with &quot;follower&quot;.&lt;/p&gt;

&lt;p&gt;7. Log:&lt;br/&gt;
7.1 recoverUptoLastCheckpointedHW(): if there are k+1 log segment files need to be truncated, we should delete the last k and truncate the first one.&lt;/p&gt;</comment>
                            <comment id="13279161" author="nehanarkhede" created="Fri, 18 May 2012 20:28:22 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Regarding your high level comment -&lt;/p&gt;

&lt;p&gt;If you use a scheduled thread pool executor to schedule every partition greadily, you would be spinning up a bunch of threads that might not do any work, since the priority of the items in the queue keeps changing. Also, you will need to change the priority of an already scheduled task, which might not be possible to do. What&apos;s more, if the threadpool maxes out, a partition that requires immediate ISR expiration might not get scheduled on time. &lt;/p&gt;

&lt;p&gt;I&apos;m guessing it is unlikely that all the partitions on a node expire at the same time. Even if they do, it might take maybe a few seconds for the last partition to shrink its ISR, which is not a big deal. In reality, there would be very few partitions, maybe 1 or 2, that need to shrink their ISR due to slower followers. That&apos;s why a single thread seems to suffice for handling the ISR expiration for all partitions. &lt;/p&gt;

&lt;p&gt;Regarding your detailed review comments -&lt;/p&gt;

&lt;p&gt;1. Fixed that&lt;br/&gt;
2. That is a good suggestion. I&apos;ve attempted a refactoring, let me know if you have more feedback on it. The ISR thread updates the ISR in ZK AND in memory cache, where as the become leader API is passed in the latest ISR read from ZK and it just has to update its cache with that state, on becoming a leader. So, I wrapped up the cache + Zk update in an updateLeaderAndISR API in KafkaServer. This will be used by the ISR expiration thread. The become leader should not be updating anything in Zk, it should just be reading from Zk and updating its cache. &lt;br/&gt;
3. The patch already does it at the end of the while loop. Or do you mean something else ?&lt;br/&gt;
5. Yes&lt;br/&gt;
6. The replica should be added to the ISR as soon as the fetch request is received by the server. I intended to add the replica to the ISR even if it might get ended up in the purgatory waiting for additional data. Ideally, would like to get rid of it from the readMessageSets() API.&lt;br/&gt;
7. Good point. Added that.&lt;/p&gt;

&lt;p&gt;Jun&lt;/p&gt;

&lt;p&gt;Regarding your high level comments -&lt;/p&gt;

&lt;p&gt;1. Yes, I kept it simple in this patch, since the main goal is to get the message replication to work. Persisting all the HW in one file would definitely be a better approach and can be another JIRA&lt;br/&gt;
2. Idle partitions will cause one delete and one insert into the priority queue. It doesn&apos;t look like an issue, but could be resolved by adding a TTL to items in the queue. Idle partitions will expire from the queue and will be added back to the queue when the leader receives a produce request for that partition. However, I&apos;d like to push that to later, since it does not improve correctness and does not look like a performance issue. &lt;/p&gt;

&lt;p&gt;Regarding your detailed review comments -&lt;/p&gt;

&lt;p&gt;3.2 Good point. I think that is better too.&lt;br/&gt;
3.3 Yes, this is one of those &#8220;details&#8221; that I didn&apos;t include in the draft patch.&lt;br/&gt;
4.1 Removed it&lt;br/&gt;
4.2 Removed CUR&lt;br/&gt;
4.3 Kept it simple since replication factors that make sense in production are typically 3-5.&lt;br/&gt;
4.4 Yes, but when will it be used ?&lt;/p&gt;

&lt;p&gt;5.1 Good catch !&lt;/p&gt;

&lt;p&gt;6. That exists in the patch. Did I miss any ?&lt;br/&gt;
7. Good point. Fixed it&lt;/p&gt;</comment>
                            <comment id="13280225" author="junrao" created="Mon, 21 May 2012 15:40:09 +0000"  >&lt;p&gt;Thanks for the patch. Overall, a very encouraging patch given the complexity of this jira. Some comments:&lt;/p&gt;

&lt;p&gt;From previous reviews:&lt;br/&gt;
Were 4.1 and 4.2 addressed in the patch? I still see CUR and reassignedReplicas.&lt;br/&gt;
For 4.4, I think highWatermarkUpdateTime can be used as described in 15.2 below.&lt;br/&gt;
For 6, I meant that all local variable names should also be prefixed with follower.&lt;/p&gt;

&lt;p&gt;New review comments:&lt;br/&gt;
11. KafkaApis:&lt;br/&gt;
11.1 handleFetchRequest(): if the leader of one partition is not on this broker, we reject the whole request. Ideally, we should just send the error code for that partition in the response and fulfill the rest of the request. &lt;br/&gt;
11.2 handleFetchRequest() and readMessageSets(): If the leader is not on this broker, we should probably return a new type of error like NotLeaderException, instead of using InvalidPartionException or throwing IllegalStateException. &lt;br/&gt;
11.3 readMessageSets(): add a comment of what -1 means for replicaId&lt;/p&gt;

&lt;p&gt;12. ReplicaManager:&lt;br/&gt;
12.1 remove unused imports&lt;br/&gt;
12.2 maybeIncrementLeaderHW(): if(newHw &amp;lt; oldHw) should be if(newHw &amp;gt; oldHw)&lt;br/&gt;
12.3 We will need to either synchronize the methods in this class or use ConcurrentHashMap for allReplicas since allReplicas can be read and updated concurrently.&lt;/p&gt;

&lt;p&gt;13. Replica:&lt;br/&gt;
13.1 hw(): to be consistent, we should probably throw IllegalStateException, instead of InvalidPartitionException.&lt;/p&gt;

&lt;p&gt;14. KafkaServer:&lt;br/&gt;
14.1 There are a couple of TODOs. Will they be addressed in this jira or separate jiras?&lt;/p&gt;

&lt;p&gt;15. ISRExpirationThread:&lt;br/&gt;
15.1 It seems that when the time expires, we always update the ISR. We should only update ISR if it actually shrinks.&lt;br/&gt;
15.2 Currently, we take a replica out of ISR if its LEO is less than leaderHW after keepInSync time. We probably should use the following condition:&lt;br/&gt;
  leaderHW - r.leo &amp;gt; keepInSyncBytes || currentTime - r.highWatermarkUpdateTime &amp;gt; keepInSyncTime&lt;br/&gt;
  The first condition handles a slow follower and the second condition handles a stuck follower.&lt;br/&gt;
15.3 I think we can potentially get rid of the inner while loop by putting all the logic when time expires in a if statement and the awaitUtil part in the else clause of the if statement.&lt;br/&gt;
15.4 Also, instead of using a priority queue and keep adding and deleting partitions into the queue, would it be simpler to have the thread just check the isInsyncCondition for each partition every keepInSyncTime?&lt;/p&gt;

&lt;p&gt;16. LogDisk: recoverUptoLastCheckpointedHW(): &lt;br/&gt;
16.1 The second condition in&lt;br/&gt;
          segments.view.find(segment =&amp;gt;  lastKnownHW &amp;gt;= segment.start &amp;amp;&amp;amp; lastKnownHW &amp;lt; segment.size) &lt;br/&gt;
       seems incorrect. It seems that you want to use &quot;lastKnownHW &amp;lt; segment.messageSet.getEndOffset&quot;&lt;br/&gt;
16.2 The files of all deleted segments should be deleted like that in LogManager.cleanupExpiredSegments().&lt;/p&gt;

&lt;p&gt;17. LogOffsetTest:&lt;br/&gt;
17.1 There is no need to keep testEmptyLogs(), since we have a test that covers fetching from a non-existing topic using SimpleConsumer.&lt;/p&gt;

&lt;p&gt;18. PrimitiveApiTest:&lt;br/&gt;
18.1 testConsumerNotExistTopic(): we probably shouldn&apos;t create the topic in this case.&lt;/p&gt;

&lt;p&gt;19. ProducerTest:&lt;br/&gt;
19.1 testZKSendToNewTopic(): Which should fix the comment that says &quot;Available partition ids should be 0, 1, 2 and 3&quot; since there is only 1 partition created.&lt;/p&gt;

&lt;p&gt;20. ReplicaFetchTest:&lt;br/&gt;
20.1 Since the test is already using in-memory log, we can remove TODO in testReplicaFetcherThread().&lt;br/&gt;
20.2 testReplicaFetcherThreadI(): Instead of sleeping and then checking log.get.getLogEndOffset, could we create a utility method that keeps checking until LEO reaches certain value up to a certain max wait time? Maybe we should make a more general util that waits up to a certain amount of time until a condition is satisfied. &lt;/p&gt;</comment>
                            <comment id="13281413" author="jkreps" created="Wed, 23 May 2012 05:29:37 +0000"  >&lt;p&gt;Comments&lt;br/&gt;
Some of these we discussed in person but I wanted to post them here for anyone else following along. A number of these are really just structural or naming comments. When we get closer to final form I will do a pass on trying to understand all the logic and see if I can find any corner cases, but I haven&apos;t done that yet. As a result I am not really sure if I understand everything I am commenting on, so take it all with a grain of salt.&lt;/p&gt;

&lt;p&gt;1. This patch pays really good attention to code structure and testability which is awesome since we are adding gobs of hard logic. Nice to see things getting cleaner as we do this.&lt;br/&gt;
2. For some reason I preferred just having Log instead of DiskLog and MemoryLog. I feel like doing it twice tends to lead to similar logic in both. I do like the idea of lightening unit tests. I wonder if a helper method to set up a log wouldn&apos;t be good enough, though? Not sure if this is a rational preference or just inertia on my part, though, so feel free to ignore. If we do separate out a Log trait I feel we should clean up that interface a bit it is kind of a disaster right now (probably we should do that regardless).&lt;br/&gt;
3. Maybe HW mark should move out of Log since now it really indicates something about the state of other servers and Log is meant to be just a simple stand alone log implementation.&lt;br/&gt;
4. I think expanding out some of the acronyms in public methods would be nice: i.e. highWatermark and logEndOffset. Having a concise local variable name is helpful but for the poor person trying to learn the code i think the slightly more verbose name is helpful. If you prefer the more concise naming then just having good javadoc that explains the abbreviations would be good.&lt;br/&gt;
5. Consider making Replica just be a dumb pojos (posos?) and move the ReplicaFetcherThread elsewhere.&lt;br/&gt;
6. We currently have Partition and BrokerPartitionInfo. We should clarify why both of these and make the naming make sense. To me a partition is logically just (topic, partId). I think BrokerPartitionInfo is really a bit hard to understand, though that is unrelated to this patch. Partition is more like the broker&apos;s information about replicas of that partition. Also both Partition and Replica are in the cluster package which was originally shared by client and server. Now with all the additional stuff this is really part of the server only, right? Probably we should change the package name...?&lt;br/&gt;
7. I sent a separate email on setters/getters. I think overall there area a lot of setter/getter methods. We should pick a style for these and go with that uniformly (we haven&apos;t been consistent so far). &lt;br/&gt;
8. I think we should figure out a general strategy for managing the zookeeper interactions. I think it is wise to wrap up the zookeeper dependency but having everything in one class is too much. Maybe the way to go is to have generic ZkUtils and reusable infra like ZkQueue and then split KafkaZookeeper into the logical functions it covers.&lt;br/&gt;
9. For the config I recommend the prefix &quot;replication&quot; or &quot;replica&quot; instead of &quot;follower&quot; (e.g. replication.socket.timeout.ms), I think this is more clear to someone who hasn&apos;t read about the internals of our replication design and doesn&apos;t know the terminology.&lt;br/&gt;
10. A bit of internals have spilled into KafkaServer, such as locking, ISR management, etc. I think KafkaServer should just interact with the main subsystems of kafka in a very abstract way. I think the core problem is that we need to think more about the functionality and API of ReplicaManager. To my mind the replica manager should be the one running the ISR maintence, doing its own locking, etc. To me the main subsystems are (1) logs managed by the LogManager, (2) the network layer wrapped up by SocketServer, (3) the request processor pool (4) and now the ReplicaManager or ReplicationManager or whatever we want to call it.&lt;br/&gt;
11. ISRMaintenceThread--I think you are right that we will need to be very prompt about handling ISR expiration since this is effectively part of our failure detection time. It might be good, though to just stick in the polling loop Jun suggested for now, and then come back to optimize it later (even though we almost certain will have to), just to reduce the scope in this iteration. &lt;br/&gt;
12. Also make sure the ISR thread either uses the Utils.newThread helper or handles the common gotchas (thread name, set daemon properly, set uncaught exception handler). Also think through the details of the lifecycle.&lt;br/&gt;
13. We have a lot of threads that basically run in a loop and use an isRunning atomic boolean and count down latch. You added two but I think we had a few others. Consider factoring this out into a helper runnable that these can extend. Verifying the lifecycle details for each is kind of a pain and it pretty easy to either not cleanly shutdown all the threads or block indefinitely or whatever.&lt;br/&gt;
14. The changes in KafkaApis seem kind of brute forced. ensureLeaderOnThisBroker and the massive expansion of logic in readMessageSets seems like we are just brute forcing through this problem. We need to find a way to structure this into methods that make sense and don&apos;t reach into the internals of other parts of the system. readMessageSet is already doing crazy funky stuff that needs to be fixed. I think restructuring readMessageSet will help with some of the problems, and the rest can maybe be solved by pushing all the replica/leo/isr logic here into ReplicaManager. Basically the API level should just say ReplicaManager.leaderForPartition(id) as part of the request validation and ReplicaManager.recordFollowerPosition(...) and move all the other details out of the KafkaApis. Not sure if I understand this well enough for that to make sense...&lt;br/&gt;
15. We should always return the hw mark in the PartitionData, right? This way we can do monitoring on the consumers. Currently it looks like we only do this for replicas.&lt;br/&gt;
16. Name for ReplicaManager.makeSurePartitionExists and ReplicaManager.assurePartitionExists doesn&apos;t really call out the difference. I would recommend calling them getOrCreatePartition() and ensureExists()&lt;br/&gt;
17. Overall I would think through the public API for ReplicaManager. I think it may be possible to move much more replica/replication/partitioning logic under this classes wrapper and out of other parts of the system which would be good. &lt;/p&gt;</comment>
                            <comment id="13281718" author="junrao" created="Wed, 23 May 2012 16:43:28 +0000"  >&lt;p&gt;Jay&apos;s comments remind another thing:&lt;/p&gt;

&lt;p&gt;21. The follower&apos;s HW should be min(follower LEO, leader HW). This is to handle the case that a follower is still catching up.&lt;/p&gt;</comment>
                            <comment id="13283880" author="nehanarkhede" created="Sat, 26 May 2012 01:59:50 +0000"  >&lt;p&gt;Thanks for the great feedback ! This is probably the largest jira for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-50&quot; title=&quot;kafka intra-cluster replication support&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-50&quot;&gt;&lt;del&gt;KAFKA-50&lt;/del&gt;&lt;/a&gt;, I&apos;ve done my best to include the review changes in this JIRA. I will file separate JIRAs to include other review suggestions. Let me attempt to describe the changes made in this patch -&lt;/p&gt;

&lt;p&gt;1. Simplied the ISR maintenance logic to iterate through the partitions every keepInSyncTimeMs ms. I guess the overhead is O&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; for isr expiration and O(1) for replica fetch requests. This seems reasonable since keepInSyncTimeMs is expected to be in the order of several seconds and replica fetch requests are easily more frequent than that.&lt;br/&gt;
2. Fixed ISR expiration logic to remove a slow follower as well as a stuck follower from the ISR&lt;br/&gt;
2. Moved replication specific logic inside ReplicaManager and Partition. So KafkaApis and KafkaServer have minimum replication specific code&lt;br/&gt;
3. Removed InMemoryLog, I guess that was an over optimization&lt;br/&gt;
4. Kept the high watermarks in a separate file, will fix it in a separate JIRA to contain the changes in this JIRA. &lt;/p&gt;</comment>
                            <comment id="13283882" author="nehanarkhede" created="Sat, 26 May 2012 02:08:06 +0000"  >&lt;p&gt;Regarding Jun&apos;s comments -&lt;/p&gt;

&lt;p&gt;4, 6: Done&lt;/p&gt;

&lt;p&gt;New review comments:&lt;br/&gt;
11. KafkaApis:&lt;br/&gt;
11.1 Makes sense&lt;br/&gt;
11.2 Added a new exception class NotLeaderForPartitionException. We can improve the naming going forward.&lt;br/&gt;
11.3 Done&lt;/p&gt;

&lt;p&gt;12. ReplicaManager:&lt;br/&gt;
12.1 Done&lt;br/&gt;
12.2 Good catch &lt;br/&gt;
12.3 Ideally, I would like to get rid of allReplicas, maybe do it differently. I&apos;m thinking of fixing this in another JIRA. Let me know if you prefer fixing it in this one.&lt;/p&gt;

&lt;p&gt;13. Replica:&lt;br/&gt;
13.1 Done&lt;/p&gt;

&lt;p&gt;14. KafkaServer:&lt;br/&gt;
14.1 Removed the TODOs. They are addressed.&lt;/p&gt;

&lt;p&gt;15. ISRExpirationThread:&lt;br/&gt;
15.1 Done&lt;br/&gt;
15.2 I&apos;ve included logic for handling slow and stuck followers, and unit tested it.&lt;br/&gt;
15.3 It has completely disappeared now.&lt;br/&gt;
15.4 Agreed&lt;/p&gt;

&lt;p&gt;16. LogDisk: recoverUptoLastCheckpointedHW():&lt;br/&gt;
16.1 That&apos;s a good point.&lt;br/&gt;
16.2 Done&lt;/p&gt;

&lt;p&gt;17. LogOffsetTest:&lt;br/&gt;
17.1 Deleted it&lt;/p&gt;

&lt;p&gt;18. PrimitiveApiTest:&lt;br/&gt;
18.1 testConsumerNotExistTopic() Actually I&apos;m not too sure this test makes sense in the replication branch. Is this testing that the server returns some meaningful error code if it receives a request for an unknown topic ? If yes, maybe we don&apos;t need a consumer to test that logic. I haven&apos;t fixed this test, maybe we can think more on what exactly we want to test here. &lt;/p&gt;

&lt;p&gt;19. ProducerTest:&lt;br/&gt;
19.1 testZKSendToNewTopic(): Done&lt;/p&gt;

&lt;p&gt;20. ReplicaFetchTest:&lt;br/&gt;
20.1 Right&lt;br/&gt;
20.2 testReplicaFetcherThreadI(): That is a good suggestion. I&apos;d like to clean up unit tests and add related helper APIs, maybe in another JIRA.&lt;/p&gt;</comment>
                            <comment id="13283886" author="jkreps" created="Sat, 26 May 2012 02:15:12 +0000"  >&lt;p&gt;Indeed, this is replication! The rest of it is just a simple matter of handling failures and a little tooling. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Very nicely done.&lt;/p&gt;</comment>
                            <comment id="13283888" author="nehanarkhede" created="Sat, 26 May 2012 02:18:22 +0000"  >&lt;p&gt;Jay,&lt;/p&gt;

&lt;p&gt;Thanks for thinking through the code structure, I&apos;ve included more refactoring changes in this patch. Some of the suggestions are orthogonal to this patch and I&apos;d prefer to fix it in another JIRA, given the complexity of this patch. Maybe I can create a &apos;refactoring&apos; JIRA after this one to cover some of these -&lt;/p&gt;

&lt;p&gt;2. Makes sense. I guess that was an over optimization.&lt;br/&gt;
3. This is a good suggestion, al though would prefer keeping it to refactoring JIRA&lt;br/&gt;
4. Picked descriptive names&lt;br/&gt;
5. Somehow I like the idea of wrapping up enough logic inside Replica to figure out if it is a follower or leader. ReplicaFetcherThread inside Replica allows that. Al though, I&apos;m not sure that is the best way to achieve it.&lt;br/&gt;
6. Yeah, probably something to think about. Will move it to the refactoring JIRA&lt;br/&gt;
7. I like Option 4 there, hoping that can be fixed in a separate JIRA&lt;br/&gt;
8. Yeah, I moved some zookeeper client access to ReplicaManager so that all replication specific logic can be moved there.&lt;br/&gt;
9. Changed configs to replication.*&lt;br/&gt;
11. Simplified the ISR expiration. Looks better now.&lt;br/&gt;
12. Hmm, Utils.newThread returns Thread, but I think it is useful to use some APIs specific to ReplicaFetcherThread like getIfFollowerAndLeader(). But I see your point here. Given a choice, it is always better to use a helper method. I set the daemon property and the thread handles all Throwables. &lt;br/&gt;
13. Yeah, this is a good suggestion. This also fits in generic refactoring category that can be fixed separately.&lt;br/&gt;
14. This is another great suggestion. Please see the included patch if you like it. &lt;br/&gt;
15. Fixed it&lt;br/&gt;
16. Fixed it&lt;br/&gt;
17. Yeah, this will keep changing with the v3 code. Will be good to keep this in mind though.&lt;/p&gt;

&lt;p&gt;Overall, I liked your refactoring suggestions, and I might have been lazy to describe all of the changes I made here. Will really appreciate it if you can read through the new patch and suggest improvements. I&apos;m fine with working through more in this patch itself, if you feel that works better. &lt;/p&gt;</comment>
                            <comment id="13283891" author="nehanarkhede" created="Sat, 26 May 2012 02:28:58 +0000"  >&lt;p&gt;Filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt; for improving the high watermark maintenance&lt;br/&gt;
Filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-351&quot; title=&quot;Refactor some new components introduced for replication &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-351&quot;&gt;&lt;del&gt;KAFKA-351&lt;/del&gt;&lt;/a&gt; to cover the refactoring suggestions. &lt;/p&gt;

&lt;p&gt;Now, we need some serious system testing for all this code ! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13284570" author="junrao" created="Mon, 28 May 2012 23:05:54 +0000"  >&lt;p&gt;Thanks for patch v2. To help people review the code, I summarized the logic of handling the produce and fetch request on the server in this wiki: &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Handling+Produce+and+Fetch+Requests+in+KafkaApi&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Handling+Produce+and+Fetch+Requests+in+KafkaApi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Some new comments:&lt;br/&gt;
21. ISRExpirationThread: It seems that this class is no longer used. Let&apos;s remove it.&lt;/p&gt;

&lt;p&gt;22. Partition.getOutOfSyncReplicas(): The first condition doesn&apos;t seem to implement what&apos;s in the comment. It doesn&apos;t check the leader&apos;s leo update time. Also, the condition specified in the comment doesn&apos;t seem sufficient. Suppose that the leader gets 100 bytes of more data, after which no more data is coming. A follower gets the first 50 bytes and then stopped. The follower&apos;s leo has been updated after the leader&apos;s leo was last updated. However, we still need to take the follower out of ISR. How about changing the condition to: select replicas whose leo is less than the leo of leader and whose leo hasn&apos;t been updated for keepInsyncTime. &lt;/p&gt;

&lt;p&gt;23. ReplicaManager:&lt;br/&gt;
23.1 makeLeader(): remove comment &quot;also add this partition to the ISR expiration priority queue&quot;&lt;br/&gt;
23.2 makeFollower(): If a follower switches leader, we should stop the old FetchThread before starting the new one.&lt;/p&gt;

&lt;p&gt;24. Replica.leoUpdateTime(): use logEndOffsetUpdateTime to be consistent.&lt;/p&gt;

&lt;p&gt;25. KafkaConfig: Let&apos;s keep the variable name and property name consistent. If we choose to use replication as the prefix for property name, use the same prefix for variable names.&lt;/p&gt;

&lt;p&gt;26. KafkaServer: To be consistent, we should probably name becomeLeader and becomeFollower as makeLeader and makeFollower, respectively.&lt;/p&gt;

&lt;p&gt;27. Log.recoverUptoLastCheckpointedHW(): not sure if comment 16.2 is addressed. Removed segments are not physically deleted.&lt;/p&gt;

&lt;p&gt;28. ISRExpirationTest:&lt;br/&gt;
28.1 testISRExpirationForSlowFollowers(): the comment says set leo of remote replica to sth like 2, but the code set it to 4.&lt;br/&gt;
28.2 testISRExpirationForStuckFollowers() and testISRExpirationForSlowFollowers(): is Thread.sleep() really needed? testISRExpirationForMultiplePartitions() didn&apos;t seem to use Thread.sleep().&lt;/p&gt;

&lt;p&gt;29. PrimitiveApiTest.testConsumerNotExistTopic(): I think this test is just to make sure that the client can get the error code on a non-existing topic.&lt;/p&gt;

&lt;p&gt;30. TestUtils:follower.socket.timeout.ms is now renamed to replication.socket.timeout.ms&lt;/p&gt;</comment>
                            <comment id="13285347" author="nehanarkhede" created="Wed, 30 May 2012 02:35:42 +0000"  >&lt;p&gt;Updated patch to address Jun&apos;s suggestions -&lt;/p&gt;

&lt;p&gt;1. Fixed the ISR expiration for stuck followers case&lt;br/&gt;
2. HW maintenance work is postponed to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
3. System test (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-341&quot; title=&quot;Create a new single host system test to validate all replicas on 0.8 branch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-341&quot;&gt;&lt;del&gt;KAFKA-341&lt;/del&gt;&lt;/a&gt;), that tests message replication without failures, works on this patch&lt;/p&gt;

&lt;p&gt;More detailed comments -&lt;/p&gt;

&lt;p&gt;21. Removed it&lt;/p&gt;

&lt;p&gt;22. Partition.getOutOfSyncReplicas(): Good catch ! Fixed the logic and added another test case for this.&lt;/p&gt;

&lt;p&gt;23. ReplicaManager: Done&lt;/p&gt;

&lt;p&gt;24. Replica: Changed the name to logEndOffsetUpdateTime()&lt;/p&gt;

&lt;p&gt;25. KafkaConfig: Changed the variable and config names to replica.*&lt;/p&gt;

&lt;p&gt;26. KafkaServer: Well, become* makes sense on the entity that is changing its state (Replica), make*, I thought made sense on the actor (KafkaServer). But that is just a matter of personal taste &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;27. There are some nitty gritty details about HW maintenance that I would like to fix as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;28. ISRExpirationTest: Done&lt;/p&gt;

&lt;p&gt;29. PrimitiveApiTest.testConsumerNotExistTopic(): I think the right fix is to throw a descriptive exception is UnknownTopicException when a client makes a produce/consume request for a topic that has never been created. Filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-351&quot; title=&quot;Refactor some new components introduced for replication &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-351&quot;&gt;&lt;del&gt;KAFKA-351&lt;/del&gt;&lt;/a&gt; to fix it.&lt;/p&gt;

&lt;p&gt;30. TestUtils:Fixed it&lt;/p&gt;</comment>
                            <comment id="13286011" author="junrao" created="Wed, 30 May 2012 20:32:42 +0000"  >&lt;p&gt;+ 1 from me for patch v3. Let&apos;s see if there are more comments from others.&lt;/p&gt;

&lt;p&gt;For 27, it&apos;s ok to resolve this in kafka-350. Could you update the jira so that we remember all the changes that need to be made? Ditto for kafka-351.&lt;/p&gt;</comment>
                            <comment id="13287082" author="nehanarkhede" created="Fri, 1 Jun 2012 01:30:24 +0000"  >&lt;p&gt;Attaching an updated patch that includes the rebase changes from &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-348&quot; title=&quot;rebase 0.8 branch from trunk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-348&quot;&gt;&lt;del&gt;KAFKA-348&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, updated the follow up JIRAs - &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-351&quot; title=&quot;Refactor some new components introduced for replication &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-351&quot;&gt;&lt;del&gt;KAFKA-351&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13287087" author="junrao" created="Fri, 1 Jun 2012 01:42:01 +0000"  >&lt;p&gt;+1 for patch v4.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12514687">KAFKA-50</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12546172">KAFKA-302</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12525557" name="kafka-46-draft.patch" size="79321" author="nehanarkhede" created="Fri, 4 May 2012 02:55:50 +0000"/>
                            <attachment id="12528144" name="kafka-46-v1.patch" size="138802" author="nehanarkhede" created="Fri, 18 May 2012 20:28:21 +0000"/>
                            <attachment id="12529841" name="kafka-46-v2.patch" size="142792" author="nehanarkhede" created="Sat, 26 May 2012 01:59:50 +0000"/>
                            <attachment id="12530254" name="kafka-46-v3.patch" size="127410" author="nehanarkhede" created="Wed, 30 May 2012 19:00:42 +0000"/>
                            <attachment id="12530496" name="kafka-46-v4.patch" size="132664" author="nehanarkhede" created="Fri, 1 Jun 2012 01:42:28 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>67069</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 25 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i09m7r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>54032</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>