<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:54:27 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-3565] Producer&apos;s throughput lower with compressed data after KIP-31/32</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-3565</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Relative offsets were introduced by KIP-31 so that the broker does not have to recompress data (this was previously required after offsets were assigned). The implicit assumption is that reducing CPU usage required by recompression would mean that producer throughput for compressed data would increase.&lt;/p&gt;

&lt;p&gt;However, this doesn&apos;t seem to be the case:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Commit: eee95228fabe1643baa016a2d49fb0a9fe2c66bd (one before KIP-31/32)
test_id:    2016-04-15--012.kafkatest.tests.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.acks=1.message_size=100.compression_type=snappy
status:     PASS
run time:   59.030 seconds
{&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 519418.343653, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 49.54}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Full results: &lt;a href=&quot;https://gist.github.com/ijuma/0afada4ff51ad6a5ac2125714d748292&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/ijuma/0afada4ff51ad6a5ac2125714d748292&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Commit: fa594c811e4e329b6e7b897bce910c6772c46c0f (KIP-31/32)
test_id:    2016-04-15--013.kafkatest.tests.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.acks=1.message_size=100.compression_type=snappy
status:     PASS
run time:   1 minute 0.243 seconds
{&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 427308.818848, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 40.75}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Full results: &lt;a href=&quot;https://gist.github.com/ijuma/e49430f0548c4de5691ad47696f5c87d&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/ijuma/e49430f0548c4de5691ad47696f5c87d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The difference for the uncompressed case is smaller (and within what one would expect given the additional size overhead caused by the timestamp field):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Commit: eee95228fabe1643baa016a2d49fb0a9fe2c66bd (one before KIP-31/32)
test_id:    2016-04-15--010.kafkatest.tests.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.acks=1.message_size=100
status:     PASS
run time:   1 minute 4.176 seconds
{&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 321018.17747, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 30.61}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Full results: &lt;a href=&quot;https://gist.github.com/ijuma/5fec369d686751a2d84debae8f324d4f&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/ijuma/5fec369d686751a2d84debae8f324d4f&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Commit: fa594c811e4e329b6e7b897bce910c6772c46c0f (KIP-31/32)
test_id:    2016-04-15--014.kafkatest.tests.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.acks=1.message_size=100
status:     PASS
run time:   1 minute 5.079 seconds
{&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 291777.608696, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 27.83}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Full results: &lt;a href=&quot;https://gist.github.com/ijuma/1d35bd831ff9931448b0294bd9b787ed&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/ijuma/1d35bd831ff9931448b0294bd9b787ed&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12959296">KAFKA-3565</key>
            <summary>Producer&apos;s throughput lower with compressed data after KIP-31/32</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="ijuma">Ismael Juma</reporter>
                        <labels>
                    </labels>
                <created>Fri, 15 Apr 2016 21:30:51 +0000</created>
                <updated>Fri, 2 Jun 2017 09:50:34 +0000</updated>
                            <resolved>Sun, 15 May 2016 16:04:46 +0000</resolved>
                                                    <fixVersion>0.10.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15243695" author="ijuma" created="Fri, 15 Apr 2016 21:31:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, do you think you can look into this? We need to understand what&apos;s happening here before we can release 0.10.0.0. cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gwenshap&quot; class=&quot;user-hover&quot; rel=&quot;gwenshap&quot;&gt;gwenshap&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15243699" author="ijuma" created="Fri, 15 Apr 2016 21:33:29 +0000"  >&lt;p&gt;By the way, the results with compression include the following change to `ProducerPerformance`:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/pull/1225/files#diff-34eb560f812ed25c56bd774d3360e4b8R66&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1225/files#diff-34eb560f812ed25c56bd774d3360e4b8R66&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15243840" author="becket_qin" created="Fri, 15 Apr 2016 23:30:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; What is your other settings? e.g. max.in.flight.requests, number of partitions, batch size, etc.&lt;/p&gt;

&lt;p&gt;I happen to be conducting some performance tuning for the producers. The results I noticed are different from what you saw:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Producer before KIP31/32

./kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic test_3_replica_1_partition --num-records 100000 --record-size 10000 --throughput 10000 --valueBound 50000 --producer-props bootstrap.servers=CLUSTER_WIHT_KIP31/32 acks=1 max.in.flight.requests.per.connection=1 batch.size=50000 compression.type=gzip client.id=becket_gzip
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/core/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/tools/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/api/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/runtime/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/file/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/json/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
5370 records sent, 1074.0 records/sec (10.24 MB/sec), 14.3 ms avg latency, 165.0 max latency.
5875 records sent, 1175.0 records/sec (11.21 MB/sec), 13.1 ms avg latency, 34.0 max latency.
5977 records sent, 1195.2 records/sec (11.40 MB/sec), 12.7 ms avg latency, 96.0 max latency.
6000 records sent, 1199.8 records/sec (11.44 MB/sec), 12.7 ms avg latency, 79.0 max latency.
6115 records sent, 1223.0 records/sec (11.66 MB/sec), 12.0 ms avg latency, 34.0 max latency.
5967 records sent, 1193.4 records/sec (11.38 MB/sec), 12.5 ms avg latency, 32.0 max latency.
5948 records sent, 1189.6 records/sec (11.34 MB/sec), 12.7 ms avg latency, 35.0 max latency.
5939 records sent, 1187.8 records/sec (11.33 MB/sec), 12.9 ms avg latency, 78.0 max latency.
5941 records sent, 1188.2 records/sec (11.33 MB/sec), 12.7 ms avg latency, 32.0 max latency.
5947 records sent, 1188.9 records/sec (11.34 MB/sec), 12.7 ms avg latency, 31.0 max latency.
6016 records sent, 1203.2 records/sec (11.47 MB/sec), 12.5 ms avg latency, 44.0 max latency.
6049 records sent, 1209.6 records/sec (11.54 MB/sec), 12.3 ms avg latency, 32.0 max latency.
6042 records sent, 1208.4 records/sec (11.52 MB/sec), 12.3 ms avg latency, 33.0 max latency.
5961 records sent, 1191.5 records/sec (11.36 MB/sec), 12.6 ms avg latency, 32.0 max latency.
5875 records sent, 1174.5 records/sec (11.20 MB/sec), 12.9 ms avg latency, 94.0 max latency.
5730 records sent, 1145.5 records/sec (10.92 MB/sec), 13.0 ms avg latency, 111.0 max latency.
100000 records sent, 1181.851489 records/sec (11.27 MB/sec), 12.76 ms avg latency, 165.00 ms max latency, 12 ms 50th, 21 ms 95th, 27 ms 99th, 47 ms 99.9th.

./kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic test_3_replica_1_partition --num-records 100000 --record-size 10000 --throughput 10000 --valueBound 50000 --producer-props bootstrap.servers=CLUSTER_BEFORE_KIP31/32 acks=1 max.in.flight.requests.per.connection=1 batch.size=50000 compression.type=gzip client.id=becket_gzip
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/core/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/tools/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/api/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/runtime/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/file/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/json/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
5390 records sent, 1078.0 records/sec (10.28 MB/sec), 13.6 ms avg latency, 192.0 max latency.
5970 records sent, 1194.0 records/sec (11.39 MB/sec), 12.8 ms avg latency, 31.0 max latency.
6067 records sent, 1213.2 records/sec (11.57 MB/sec), 12.2 ms avg latency, 32.0 max latency.
6151 records sent, 1230.0 records/sec (11.73 MB/sec), 12.0 ms avg latency, 53.0 max latency.
6143 records sent, 1228.1 records/sec (11.71 MB/sec), 12.0 ms avg latency, 32.0 max latency.
6045 records sent, 1209.0 records/sec (11.53 MB/sec), 12.4 ms avg latency, 66.0 max latency.
6104 records sent, 1220.6 records/sec (11.64 MB/sec), 11.9 ms avg latency, 31.0 max latency.
6057 records sent, 1210.9 records/sec (11.55 MB/sec), 11.9 ms avg latency, 30.0 max latency.
6053 records sent, 1210.6 records/sec (11.55 MB/sec), 11.7 ms avg latency, 31.0 max latency.
6102 records sent, 1220.2 records/sec (11.64 MB/sec), 11.9 ms avg latency, 30.0 max latency.
6144 records sent, 1228.8 records/sec (11.72 MB/sec), 11.8 ms avg latency, 30.0 max latency.
6135 records sent, 1227.0 records/sec (11.70 MB/sec), 11.5 ms avg latency, 28.0 max latency.
6141 records sent, 1228.0 records/sec (11.71 MB/sec), 11.8 ms avg latency, 31.0 max latency.
6138 records sent, 1227.6 records/sec (11.71 MB/sec), 11.6 ms avg latency, 32.0 max latency.
6102 records sent, 1219.9 records/sec (11.63 MB/sec), 12.0 ms avg latency, 33.0 max latency.
6024 records sent, 1204.6 records/sec (11.49 MB/sec), 11.7 ms avg latency, 84.0 max latency.
100000 records sent, 1208.955946 records/sec (11.53 MB/sec), 12.02 ms avg latency, 192.00 ms max latency, 11 ms 50th, 20 ms 95th, 25 ms 99th, 31 ms 99.9th.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I use the producer with KIP31/32&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Producer with KIP31/32

./kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic test_3_replica_1_partition --num-records 100000 --record-size 10000 --throughput 10000 --valueBound 50000 --producer-props bootstrap.servers=CLUSTER_WITH_KIP31/32 acks=1 max.in.flight.requests.per.connection=1 batch.size=50000 compression.type=gzip client.id=becket_gzip
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/core/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/tools/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/api/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/runtime/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/file/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/json/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
5444 records sent, 1088.8 records/sec (10.38 MB/sec), 9.5 ms avg latency, 158.0 max latency.
6254 records sent, 1250.8 records/sec (11.93 MB/sec), 7.2 ms avg latency, 54.0 max latency.
6448 records sent, 1289.3 records/sec (12.30 MB/sec), 6.9 ms avg latency, 24.0 max latency.
6429 records sent, 1285.8 records/sec (12.26 MB/sec), 6.9 ms avg latency, 14.0 max latency.
6456 records sent, 1291.2 records/sec (12.31 MB/sec), 6.9 ms avg latency, 31.0 max latency.
6382 records sent, 1276.4 records/sec (12.17 MB/sec), 7.0 ms avg latency, 47.0 max latency.
6365 records sent, 1272.7 records/sec (12.14 MB/sec), 6.9 ms avg latency, 12.0 max latency.
6454 records sent, 1290.8 records/sec (12.31 MB/sec), 6.9 ms avg latency, 27.0 max latency.
6457 records sent, 1291.4 records/sec (12.32 MB/sec), 6.8 ms avg latency, 12.0 max latency.
6301 records sent, 1259.7 records/sec (12.01 MB/sec), 6.9 ms avg latency, 19.0 max latency.
6492 records sent, 1298.4 records/sec (12.38 MB/sec), 6.9 ms avg latency, 12.0 max latency.
6426 records sent, 1284.9 records/sec (12.25 MB/sec), 6.9 ms avg latency, 13.0 max latency.
6419 records sent, 1283.5 records/sec (12.24 MB/sec), 6.9 ms avg latency, 37.0 max latency.
6370 records sent, 1273.7 records/sec (12.15 MB/sec), 7.0 ms avg latency, 50.0 max latency.
6431 records sent, 1286.2 records/sec (12.27 MB/sec), 6.8 ms avg latency, 12.0 max latency.
100000 records sent, 1269.051638 records/sec (12.10 MB/sec), 7.07 ms avg latency, 158.00 ms max latency, 7 ms 50th, 9 ms 95th, 12 ms 99th, 26 ms 99.9th.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It seems both throughput and latency is improved. Similar results for message size 100B&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Producer with KIP31/32

./kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic test_3_replica_1_partition --num-records 10000000 --record-size 100 --throughput 100000 --valueBound 50000 --producer-props bootstrap.servers=CLUSTER_WITH_KIP31/32 acks=1 max.in.flight.requests.per.connection=1 batch.size=50000 compression.type=gzip client.id=becket_gzip
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/core/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/tools/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/api/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/runtime/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/file/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/gitlikafka/connect/json/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
236641 records sent, 47328.2 records/sec (4.51 MB/sec), 4.5 ms avg latency, 170.0 max latency.
241964 records sent, 48383.1 records/sec (4.61 MB/sec), 4.3 ms avg latency, 16.0 max latency.
233744 records sent, 46748.8 records/sec (4.46 MB/sec), 4.3 ms avg latency, 23.0 max latency.
243816 records sent, 48763.2 records/sec (4.65 MB/sec), 4.3 ms avg latency, 15.0 max latency.
249216 records sent, 49843.2 records/sec (4.75 MB/sec), 4.4 ms avg latency, 43.0 max latency.
247266 records sent, 49453.2 records/sec (4.72 MB/sec), 4.3 ms avg latency, 32.0 max latency.
238498 records sent, 47699.6 records/sec (4.55 MB/sec), 4.3 ms avg latency, 31.0 max latency.
244814 records sent, 48962.8 records/sec (4.67 MB/sec), 4.3 ms avg latency, 27.0 max latency.
243154 records sent, 48630.8 records/sec (4.64 MB/sec), 4.4 ms avg latency, 51.0 max latency.
246016 records sent, 49203.2 records/sec (4.69 MB/sec), 4.3 ms avg latency, 55.0 max latency.
237093 records sent, 47418.6 records/sec (4.52 MB/sec), 4.3 ms avg latency, 25.0 max latency.
238039 records sent, 47607.8 records/sec (4.54 MB/sec), 4.4 ms avg latency, 43.0 max latency.
215768 records sent, 43153.6 records/sec (4.12 MB/sec), 4.3 ms avg latency, 15.0 max latency.
226158 records sent, 45231.6 records/sec (4.31 MB/sec), 4.3 ms avg latency, 15.0 max latency.

Producer before KIP31/32

./kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic test_3_replica_1_partition --num-records 10000000 --record-size 100 --throughput 100000 --valuound 50000 --producer-props bootstrap.servers=CLUSTER_BEFORE_KIP31/32 acks=1 max.in.flight.requests.per.connection=1 batch.size=50000 compression.type=gzip client.id=becket_gzip
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/core/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/tools/build/dependant-libs-2.10.6/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/api/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/runtime/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/file/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/jqin/workspace/temp/gitlikafka/connect/json/build/dependant-libs/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
210949 records sent, 42189.8 records/sec (4.02 MB/sec), 7.2 ms avg latency, 243.0 max latency.
220946 records sent, 44189.2 records/sec (4.21 MB/sec), 6.5 ms avg latency, 19.0 max latency.
213472 records sent, 42694.4 records/sec (4.07 MB/sec), 6.7 ms avg latency, 100.0 max latency.
215634 records sent, 43126.8 records/sec (4.11 MB/sec), 6.7 ms avg latency, 107.0 max latency.
197060 records sent, 39404.1 records/sec (3.76 MB/sec), 6.4 ms avg latency, 20.0 max latency.
207942 records sent, 41588.4 records/sec (3.97 MB/sec), 6.7 ms avg latency, 76.0 max latency.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15243850" author="becket_qin" created="Fri, 15 Apr 2016 23:35:02 +0000"  >&lt;p&gt;The producer performance I ran was with similar tweaks you made but with an adjustable range of integer (0 to valueBound) to get different compression ratio.&lt;/p&gt;</comment>
                            <comment id="15243862" author="ijuma" created="Fri, 15 Apr 2016 23:45:55 +0000"  >&lt;p&gt;Interesting. Good to know that there are cases where the new code does better. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I ran `benchmark_test.py` on d2.2xlarge AWS instances (8 vCPUs, Xeon E5-2676v3 2.4 Ghz , 61 GB of RAM, 6 x 2000 GB HDD, High networking), the file includes all the settings used:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/tests/kafkatest/benchmarks/core/benchmark_test.py#L72&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/tests/kafkatest/benchmarks/core/benchmark_test.py#L72&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15244946" author="becket_qin" created="Sun, 17 Apr 2016 22:22:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; A few comments:&lt;/p&gt;

&lt;p&gt;1. The performance improvement introduced in KIP31/32 is on the broker side. KIP32 introduced 8 more bytes to the message, so it is expected the none compressed throughput to be less in terms of the user bytes.&lt;/p&gt;

&lt;p&gt;2. When we test throughput, it is important to know that which specific part we consider as the bottleneck. In this benchmark, because we have 6 partitions it is very likely the bottleneck would become the user thread of the producer. If that is the case, for 100B message, KIP32 introduced another 8 bytes for timestamp, so for each messages there are 8 more bytes to compress. Even if the broker avoids re-compression, it is simply waiting for the producer to producer more data so it has no impact on the throughput. If we want to see whether the broker is doing better, we should check the latency and I think we will see the difference.&lt;/p&gt;

&lt;p&gt;3. Snappy is much faster than gzip in terms or compression and recompression, so the elimination of re-compression might not have as prominent impact as gzip on the broker side.&lt;/p&gt;

&lt;p&gt;So in the above benchmark, I don&apos;t think it actually measures the throughput of the broker, but only measures the the max throughput of one single user thread in the producer, which is expected to be worse than before because we have an extra 8 bytes overhead when the message size is 100B.&lt;/p&gt;

&lt;p&gt;BTW, we should add the number of caller thread option to the ProducerPerformance (currently I usually a tweaked version of ProducerPerformance), I think we can do this in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3554&quot; title=&quot;Generate actual data with specific compression ratio and add multi-thread support in the ProducerPerformance tool.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3554&quot;&gt;KAFKA-3554&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15245007" author="ijuma" created="Mon, 18 Apr 2016 00:27:15 +0000"  >&lt;p&gt;Becket, &lt;/p&gt;

&lt;p&gt;1. Yes, the non-compressed throughput change is expected, I just included it for reference (as mentioned in the issue description).&lt;/p&gt;

&lt;p&gt;2. The additional 8 bytes don&apos;t explain the magnitude of the slowdown for the compressed case though (which is quite a bit higher than the non-compressed case).&lt;/p&gt;

&lt;p&gt;3. Yes, I tested gzip internally first for the reason you mention and the results were similar (Jun suggested to try snappy too and to keep things simple I only mentioned the snappy results).&lt;/p&gt;

&lt;p&gt;I paste the results for gzip with 3 producers that I computed earlier (note that this is 0.9 versus trunk, but KIP-31/32 is the main difference):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Test name                      version    parameters                 records/s         MB/s       Difference 
test_producer_throughput	0.9	  acks=1, message_size=1000, num_producers=3	124653.624	118.87	- 
test_producer_throughput	trunk     acks=1, message_size=1000, num_producers=3	102308.689	97.57	-17.93
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15246460" author="junrao" created="Mon, 18 Apr 2016 20:20:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, in the benchmark result that Ismael posted, each message is 1000-byte. So, it&apos;s surprising to see a performance degradation of 17% just because of the addition of an 8-byte per message overhead. The result seems different from the test that you have. Between the two tests, it seems they differ on how random the data is, the number of partitions and the message size. Could you do some more tests to see if (1) if you can reproduce the degradation from Ismael&apos;s test locally (2) what specific configurations are causing the degradation (e.g., randomness of data , # partitions)? The degradation may only happen in corner cases, but we need to understand this. Would you have time to help figure this out before releasing 0.10.0? Thanks,&lt;/p&gt;</comment>
                            <comment id="15246565" author="becket_qin" created="Mon, 18 Apr 2016 21:20:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; I see. So we do know that the throughput of a single-user-thread producer will be lower compared with 0.9, but we are trying to understand why the throughput seems even lower than our expectation considering the amount of overhead introduced in KIP-32.&lt;/p&gt;

&lt;p&gt;I did the following test:&lt;br/&gt;
1. Launch a one broker cluster running 0.9&lt;br/&gt;
2. Launch another one-broker cluster running trunk&lt;br/&gt;
3. Using tweaked 0.9 ProducerPerformance and trunk ProducerPerformance to producer to an 8-partition topic.&lt;/p&gt;

&lt;p&gt;I am not able to reproduce the result you had for gzip.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;./kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic becket_test_1_replica_8_partition --num-records 500000 --record-size 1000 --throughput 100000 --valueBound 50000 --producer-props bootstrap.servers=localhost:9092 acks=1 max.in.flight.requests.per.connection=1 compression.type=gzip batch.size=500000 client.id=becket

The result form 0.9:
500000 records sent, 3734.548306 records/sec (3.56 MB/sec), 368.73 ms avg latency, 790.00 ms max latency, 368 ms 50th, 535 ms 95th, 597 ms 99th, 723 ms 99.9th.

The result from trunk:
500000 records sent, 11028.276501 records/sec (10.52 MB/sec), 4.08 ms avg latency, 148.00 ms max latency, 4 ms 50th, 6 ms 95th, 9 ms 99th, 57 ms 99.9th.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results of snappy with 100B messages are followling&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;./kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic becket_test_1_replica_8_partition --num-records 100000000 --record-size 100 --throughput 10000 --valueBound 50000 --producer-props bootstrap.servers=localhost:9092 acks=1 max.in.flight.requests.per.connection=1 compression.type=snappy batch.size=500000 client.id=becket

The result from 0.9:
100000000 records sent, 358709.649648 records/sec (34.21 MB/sec), 22.84 ms avg latency, 388.00 ms max latency, 21 ms 50th, 30 ms 95th, 44 ms 99th, 237 ms 99.9th.

The result from trunk:
100000000 records sent, 272133.279995 records/sec (25.95 MB/sec), 13.96 ms avg latency, 1057.00 ms max latency, 9 ms 50th, 26 ms 95th, 145 ms 99th, 915 ms 99.9th.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I took a closer look at the ProducerPerformance metrics, there are a few differences with and w/o KIP-31/32.&lt;br/&gt;
1. The batch size: 212721(w) vs 475194(w/o)&lt;br/&gt;
2. The request rate: 134(w) vs 81(w/o)&lt;br/&gt;
3. record queue time: 4.9 ms(w) vs 18(w/o)&lt;/p&gt;

&lt;p&gt;This indicates that in general the sender thread is running more iterations after KIP31/32 due to the smaller latency from the broker on trunk (in fact I think this is the metric we should care about most). That also means more batches are rolled out, and more lock grabbing. Those things can impact the throughput for a single user thread. While the throughput of a single user thread is important, if we take the producer as a system, there are too many factors that can affect that. One thing I notice on the producer performance is that you have to tune it. e.g. if I change the configuration on the trunk ProducerPerformance to batch.size=100000 and linger.ms=100. The result I got is similar to the 0.9 result. &lt;br/&gt;
&lt;tt&gt;100000000 records sent, 349094.971287 records/sec (33.29 MB/sec), 25.34 ms avg latency, 540.00 ms max latency, 23 ms 50th, 31 ms 95th, 107 ms 99th, 388 ms 99.9th.&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;I think we can say that with KIP31/32&lt;br/&gt;
1. the brokers are able to handle the requests much faster so the throughput of the broker increased.&lt;br/&gt;
2. each user thread of producer might be slower because of the 8-bytes overhead. But user can increase user threads or tune the producer to get better throughput.&lt;/p&gt;</comment>
                            <comment id="15246765" author="ijuma" created="Mon, 18 Apr 2016 23:25:29 +0000"  >&lt;p&gt;Interesting about the tuning that may be required due to the lower broker latency. To add to what Jun said, another difference in the results I posted is that they were executed over a network (not localhost) with 3 brokers. For the gzip result, it involved 3 producers as well. These details can sometimes be important, for example &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2950&quot; title=&quot;Performance regression in producer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2950&quot;&gt;&lt;del&gt;KAFKA-2950&lt;/del&gt;&lt;/a&gt; was only visible if there were multiple brokers.&lt;/p&gt;

&lt;p&gt;Given what you have found so far, the questions I have are:&lt;/p&gt;

&lt;p&gt;1. Is the slowdown in each producer user thread solely explained by the additional 8 bytes or is there something else as well (as we saw in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2950&quot; title=&quot;Performance regression in producer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2950&quot;&gt;&lt;del&gt;KAFKA-2950&lt;/del&gt;&lt;/a&gt;, some of the producer code is performance sensitive)?&lt;br/&gt;
2. Will existing users with compressed topics suffer a slowdown when they move to 0.10.0 unless they tune their producers or are the producer settings in `benchmark_test.py` unrealistic? If the former, do we need to add something to the upgrade notes?&lt;/p&gt;</comment>
                            <comment id="15247198" author="becket_qin" created="Tue, 19 Apr 2016 05:18:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; I tried a few more things including &lt;br/&gt;
1) tweaking compressor buffer size&lt;br/&gt;
2) using acks=0 which should eliminate the broker impact. &lt;br/&gt;
3) Change the random integer range between 50 and 50000&lt;/p&gt;

&lt;p&gt;I was still not able to reproduce the performance gap between 0.9 and trunk as Ismael observed.&lt;/p&gt;

&lt;p&gt;To answer Ismael&apos;s questions:&lt;br/&gt;
1. I am not sure about the reason for the slowdown you saw. Is this steadily reproduceable? I am curious about where the actual bottleneck is. &lt;br/&gt;
I ran a few tests with 3 producers, using ack=0 to ignore the broker. I compared the results between git hash eee95228fabe1643baa016a2d49fb0a9fe2c66bd and trunk. In most cases, trunk seems even performing better than 0.9.&lt;/p&gt;

&lt;p&gt;2. It really depends, the existing users may or may not suffer a slowdown when they move to 0.10.0. If the bottleneck of the producing is on the broker, they will see improvement. In most cases, if they switch to an 0.10.0 producer, it seems they can have some performance improvement. But if the users have already max out the CPU they may see the lower throughput.&lt;/p&gt;

&lt;p&gt;I agree with Jun that it is important to understand why the gap is there if it is stably reproduceable. It would be helpful if we can get the following metrics when running the tests:&lt;br/&gt;
1. actual batch size&lt;br/&gt;
2. actual request size&lt;br/&gt;
3. request latency&lt;br/&gt;
4. request rate&lt;br/&gt;
5. records queue time&lt;/p&gt;

&lt;p&gt;To reduce the variable factors, I recommend we use 1 producer, 1 partition, acks=1, batch.size=500K, linger.ms=0 (replication factor should not matter here). &lt;br/&gt;
The variable parameters are: max.in.flight.requests.per.connection=1 and 100, random integer range from 10 to 50000, compression.type=gzip and snappy.&lt;/p&gt;

&lt;p&gt;I will run the tests tomorrow and let you know the results I saw. Please let me know if you think we should add more test parameters. Thanks.&lt;/p&gt;</comment>
                            <comment id="15248198" author="jkreps" created="Tue, 19 Apr 2016 17:09:49 +0000"  >&lt;p&gt;I think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; noted a good possible explanation. The dominate thing for single-client producer performance is the efficiency of batching. With the default setting the producer only batches optimistically so counter-intuitively when the server get&apos;s slows, there is more batching, and and good batching results in much lower CPU since there are fewer requests, but also better compression which also improves CPU. So bad performance leads to good performance. This is super counter-intuitive but basically making the server slower can result in dynamic improvements in batching that make it faster! You can also reproduce this with in flight requests, setting this higher can actually lead to much lower performance. I think one test to confirm this hypothesis would be to attempt to reproduce the problem with linger.ms=10 to force maximum batching to occur and see if the difference goes away or not.&lt;/p&gt;</comment>
                            <comment id="15249181" author="becket_qin" created="Wed, 20 Apr 2016 03:10:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt; Good example of max.in.flight.requests.per.connection. I will add the linger.ms into the parameter.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; I was pulled into some production issue today and did not get time to finish running the tests. I will do it tomorrow. It would be great if Ismael can also check if the performance gap is stably reproduceable. Thanks.&lt;/p&gt;</comment>
                            <comment id="15249346" author="ijuma" created="Wed, 20 Apr 2016 06:16:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, the results were reproducible when I ran them last time. However, I launched a new AWS cluster and ran a set of tests twice yesterday to double-check. I&apos;ll try to post the results here before I catch my flight from London to San Francisco in a few hours (I will then be unavailable for about half a day).&lt;/p&gt;</comment>
                            <comment id="15249601" author="ijuma" created="Wed, 20 Apr 2016 10:18:13 +0000"  >&lt;p&gt;I ran the tests a couple of times with various settings to check if my previous results are reprocible and included three linger_ms parameters (0ms, 10ms, 100ms).&lt;/p&gt;

&lt;p&gt;I paste below one configuration and will follow-up with full results.&lt;/p&gt;

&lt;p&gt;Test name and base parameters: test_producer_throughput with replication_factor=3, message_size=100, replication_factor=3, num_producers=1, acks=1&lt;/p&gt;

&lt;p&gt;Additional parameters: linger_ms=0&lt;/p&gt;

&lt;p&gt;Run 1&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;no compression 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 315361.137218, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 30.08}
no compression trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 297798.313734, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 28.4}
snappy 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 553246.908491, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 52.76}
snappy trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 577280.430108, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 55.05}
gzip 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 77354.44643, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 7.38}
gzip trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 62830.118903, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 5.99}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run 2&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;no compression 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 315955.037665, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 30.13}
no compression trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 300464.965301, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 28.65}
snappy 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 613146.185473, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 58.47}
snappy trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 566080.556727, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 53.99}
gzip 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 79531.701825, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 7.58}
gzip trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 64608.501011, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 6.16}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Additional parameters: linger_ms=10&lt;/p&gt;

&lt;p&gt;Run 1&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;no compression 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 321710.690316, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 30.68}
no compression trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 295894.400353, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 28.22}
snappy 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 626892.573564, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 59.79}
snappy trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 583555.217391, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 55.65}
gzip 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 101564.66137, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 9.69}
gzip trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 93290.957114, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 8.9}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run 2&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;no compression 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 322871.541977, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 30.79}
no compression trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 297139.03033, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 28.34}
snappy 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 655040.019522, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 62.47}
snappy trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 584571.864111, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 55.75}
gzip 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 106699.817156, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 10.18}
gzip trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 93577.145646, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 8.92}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Additional parameters: linger_ms=100&lt;/p&gt;

&lt;p&gt;Run 1&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;no compression 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 318958.412548, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 30.42}
no compression trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 289574.325782, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 27.62}
snappy 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 654401.267674, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 62.41}
snappy trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 533244.735797, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 50.85}
gzip 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 108845.754602, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 10.38}
gzip trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 95630.708942, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 9.12}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run 2&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;no compression 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 322561.163182, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 30.76}
no compression trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 291524.10947, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 27.8}
snappy 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 626599.906629, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 59.76}
snappy trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 568719.067797, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 54.24}
gzip 0.9.0.1: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 108660.70272, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 10.36}
gzip trunk: {&lt;span class=&quot;code-quote&quot;&gt;&quot;records_per_sec&quot;&lt;/span&gt;: 94786.511299, &lt;span class=&quot;code-quote&quot;&gt;&quot;mb_per_sec&quot;&lt;/span&gt;: 9.04}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15251422" author="becket_qin" created="Thu, 21 Apr 2016 07:04:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I just finished the parameterized test on the code with and without KIP31/32. &lt;br/&gt;
Here is the result:&lt;br/&gt;
&lt;a href=&quot;https://docs.google.com/spreadsheets/d/1DR13ng6ZMRFki9QCepvsDcT2hfE1sjodql9zR_y2XKs/edit?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://docs.google.com/spreadsheets/d/1DR13ng6ZMRFki9QCepvsDcT2hfE1sjodql9zR_y2XKs/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A few explanation on the test.&lt;/p&gt;

&lt;p&gt;The parameters I used are the following:&lt;br/&gt;
max.in.flight.requests.per.connection=1, 5&lt;br/&gt;
valueBound=500, 5000, 50000&lt;br/&gt;
linger.ms=0, 10, 100&lt;br/&gt;
recordSize=100, 1000&lt;br/&gt;
compression.type=gzip, snappy&lt;/p&gt;

&lt;p&gt;*The tests ran with 1 partition, 1 replica, batch.size=500000, acks=1. &lt;br/&gt;
*The 0.9 test broker and the trunk test broker were running on the same standalone machine. The machine was almost idle except for running the two brokers.&lt;br/&gt;
*The producers were running on another test box. The RTT between producer and broker is negligible (the two machines are only 3 feet away from each other)&lt;br/&gt;
*The test runs sequentially, i.e. there is only one producer running during each test.&lt;/p&gt;

&lt;p&gt;In the test result table, the result of the trunk comes first. Out of all the 72 runs, 0.9 wins in the following configuration combinations:&lt;br/&gt;
1. max.in.flight.requests.per.connection=5, valueBound=500, linger.ms=0, messageSize=100, compression.type=gzip&lt;br/&gt;
2. max.in.flight.requests.per.connection=5, valueBound=500, linger.ms=0, messageSize=1000, compression.type=gzip&lt;br/&gt;
3. max.in.flight.requests.per.connection=5, valueBound=500, linger.ms=10, messageSize=100, compression.type=gzip&lt;br/&gt;
4. max.in.flight.requests.per.connection=5, valueBound=500, linger.ms=10, messageSize=1000, compression.type=gzip&lt;br/&gt;
5. max.in.flight.requests.per.connection=5, valueBound=500, linger.ms=100, messageSize=1000, compression.type=gzip&lt;br/&gt;
6. max.in.flight.requests.per.connection=5, valueBound=5000, linger.ms=100, messageSize=100, compression.type=gzip&lt;/p&gt;

&lt;p&gt;The common thing about these combination is that the valueBound is small. This seems indicating that the more compressible the message is the bigger negative impact would the 8-bytes overhead have.&lt;/p&gt;

&lt;p&gt;There are many other interesting things can be seen from the detailed metrics in the table. But for now with the tests, it seems there is no obvious unexpected performance issue with KIP-31/32. Please let me know if you think there is something I missed. Thanks.&lt;/p&gt;</comment>
                            <comment id="15252069" author="jkreps" created="Thu, 21 Apr 2016 15:36:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; Yeah your data eliminates my guess. If the problem were the change in batching you wouldn&apos;t see impact in the non-compressed case.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; I don&apos;t think we have access to that google doc, but I&apos;m not sure how intuitive that is to me, maybe you can explain it more? For one thing the timestamps themselves should be ultra compressible, how much bigger is the resulting log with timestamps? Also, can we look at actual hprof output for one case where there is a degradation and see which methods the change comes from and confirm it is consistent with that theory?&lt;/p&gt;

&lt;p&gt;One thing we noted in the last release was that System.currentTimeMillis() is pretty expensive if it is in the critical path of send(). I wonder if the difference remains if we manually set the timestamp to some fixed value and don&apos;t do it dynamically with the system call?&lt;/p&gt;</comment>
                            <comment id="15252123" author="becket_qin" created="Thu, 21 Apr 2016 16:09:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt; The google doc should be accessible now. &lt;/p&gt;

&lt;p&gt;I think the change in uncompressed case is expected because of the 8-bytes additional cost. The MB/s reported in ProducerPerformance seems based on the actual message size which does not include the message header overhead.&lt;/p&gt;

&lt;p&gt;I benchmarked the compression of bounded random integers in the producer with jmh:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Benchmark                         (bufferSize)  (recordSize)  (valueBound)  Mode  Cnt       Score        Error  Units
ClientMicroBenchMark.compression          1024           100           500  avgt   20  152745.539 &#177; 198435.342  ns/op
ClientMicroBenchMark.compression          1024           100          5000  avgt   20  106698.904 &#177; 111624.120  ns/op
ClientMicroBenchMark.compression          1024           100         50000  avgt   20  104670.802 &#177; 110694.704  ns/op
ClientMicroBenchMark.compression          1024          1000           500  avgt   20  169223.272 &#177; 180063.271  ns/op
ClientMicroBenchMark.compression          1024          1000          5000  avgt   20  118949.514 &#177; 122875.686  ns/op
ClientMicroBenchMark.compression          1024          1000         50000  avgt   20  130193.581 &#177; 140485.913  ns/op
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I benchmarked System.currentTimeMillis() some time ago as well, it took single digit nano seconds. So It seems ignorable compared with compression time.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; test, it seems the gap is around 8-10% in the throughput of user bytes when message size is 100. Is this different from what we saw before which has 17% gap? What was the integer range of the test? And how many records were sent during the test? One thing I noticed is that it takes a few seconds for the throughput to become stable, so if the tests finished very quickly, the results may not be quite accurate. The tests I ran adjusted the records number dynamically to let the testing time be at least 15 seconds.&lt;/p&gt;</comment>
                            <comment id="15252139" author="jkreps" created="Thu, 21 Apr 2016 16:18:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; Yeah I think you are saying something like &quot;previously a 100 byte record had 16 bytes of overhead, now it has 24 so naturally it&apos;s slower&quot;. The reason I find that surprising is because in the 100 byte range we have always been bound by the per-record overhead not the raw throughput. You can see this in that in this range we max out at around 30MB/sec versus with larger records we max out closer to 100MB/sec. But for small records the raw number of them per second is crazy high so any per-record work, no matter how small, even little things like acquiring a lock or checking the clock, can have outsized performance impact since we&apos;re trying to do it 800k-1m times per second. We saw this last release where we added a few clock checks and it lead to a very significant perf drop in the producer. This magnification means adding something that is only 100ns to the per-record overhead, which normally is in the range of &quot;free&quot;, is magnified to something like a 10% overhead in the small record regime.&lt;/p&gt;</comment>
                            <comment id="15252152" author="jkreps" created="Thu, 21 Apr 2016 16:24:48 +0000"  >&lt;p&gt;Oh yeah, I think the critical question is what is the bottleneck? In the cases where there is a degradation does it come from the server, the producer I/O thread, or the sending thread. I think the producer has metrics that cover this.&lt;/p&gt;</comment>
                            <comment id="15252352" author="becket_qin" created="Thu, 21 Apr 2016 17:58:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt; Yes, I agree that &quot;free&quot; calls can make a difference when called millions of times. We can try to use a fixed timestamp to verify this. If that is the case theoretically we should see a bigger difference if more records are sent. That means when snappy is used 0.9 should have a bigger advantage because it sends significantly more records per sec, but the cases where 0.9 wins in my tests are all using gzip.&lt;/p&gt;

&lt;p&gt;Your batching hypothesis may still stand. In all the tests, 0.9 is sending less but bigger batches. And trunk is send more but smaller batches. In the cases that 0.9 wins, it looks the bigger batch helped. Let me run the test again and also get the records per request and compression ratio.&lt;/p&gt;</comment>
                            <comment id="15254550" author="becket_qin" created="Fri, 22 Apr 2016 19:57:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I ran the tests a few more times and updated the results in the previous google sheet. The 8th run was using a fixed timestamp 0L for trunk.&lt;/p&gt;

&lt;p&gt;A brief summary of the cases where 0.9 wins in the eight runs we have is following.&lt;br/&gt;
(&quot;2nd&quot; means 0.9 wins, followed by the configurations, followed by the trunk throughput and 0.9 throughput in MB/sec, followed by the throughput difference in percentage.)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Run 1:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (2.04   &amp;lt;  2.38,   16%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=1000,  compression.type=gzip    (3.18   &amp;lt;  3.49,   9%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=gzip    (2.43   &amp;lt;  2.46,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=1000,  compression.type=gzip    (3.16   &amp;lt;  3.49,   10%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=1000,  compression.type=gzip    (3.09   &amp;lt;  3.49,   12%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100,  messageSize=100,   compression.type=gzip    (2.52   &amp;lt;  2.55,   1%)

Run 2:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (1.82   &amp;lt;  2.00,   9%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=snappy  (19.32  &amp;lt;  20.37,  5%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=gzip    (1.80   &amp;lt;  1.86,   3%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=gzip    (1.87   &amp;lt;  2.01,   7%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=10,   messageSize=100,   compression.type=gzip    (2.01   &amp;lt;  2.14,   6%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100,  messageSize=100,   compression.type=gzip    (2.05   &amp;lt;  2.13,   3%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,    messageSize=100,   compression.type=gzip    (2.25   &amp;lt;  2.28,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,    messageSize=100,   compression.type=snappy  (22.14  &amp;lt;  23.38,  5%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=100,   compression.type=gzip    (2.23   &amp;lt;  2.29,   2%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=100,   compression.type=snappy  (22.68  &amp;lt;  23.55,  3%)

Run 3:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (1.88   &amp;lt;  2.04,   8%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=gzip    (1.86   &amp;lt;  2.02,   8%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=snappy  (20.23  &amp;lt;  20.44,  1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=gzip    (1.97   &amp;lt;  2.01,   2%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=snappy  (20.34  &amp;lt;  21.15,  3%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,    messageSize=100,   compression.type=gzip    (2.07   &amp;lt;  2.13,   2%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=10,   messageSize=100,   compression.type=gzip    (2.06   &amp;lt;  2.09,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100,  messageSize=100,   compression.type=gzip    (2.06   &amp;lt;  2.14,   3%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,    messageSize=100,   compression.type=gzip    (2.20   &amp;lt;  2.30,   4%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,    messageSize=100,   compression.type=snappy  (21.04  &amp;lt;  23.68,  12%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=100,   compression.type=snappy  (22.91  &amp;lt;  23.39,  2%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=1000,  compression.type=snappy  (38.99  &amp;lt;  39.80,  2%)

Run 4:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (1.82   &amp;lt;  2.03,   11%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=snappy  (16.74  &amp;lt;  20.99,  25%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=gzip    (1.74   &amp;lt;  2.05,   17%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=snappy  (17.12  &amp;lt;  21.15,  23%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=gzip    (1.72   &amp;lt;  2.04,   18%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=snappy  (18.42  &amp;lt;  20.64,  12%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,    messageSize=100,   compression.type=gzip    (1.95   &amp;lt;  2.08,   6%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=10,   messageSize=100,   compression.type=gzip    (1.90   &amp;lt;  2.13,   12%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100,  messageSize=100,   compression.type=gzip    (1.90   &amp;lt;  2.14,   12%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,    messageSize=100,   compression.type=gzip    (2.27   &amp;lt;  2.30,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=100,   compression.type=gzip    (2.26   &amp;lt;  2.29,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100,  messageSize=100,   compression.type=gzip    (2.17   &amp;lt;  2.27,   4%)

Run 5:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (1.96   &amp;lt;  2.06,   5%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=snappy  (21.01  &amp;lt;  21.33,  1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=gzip    (1.94   &amp;lt;  2.03,   4%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=gzip    (1.89   &amp;lt;  1.99,   5%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100,  messageSize=100,   compression.type=gzip    (2.07   &amp;lt;  2.12,   2%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,    messageSize=100,   compression.type=gzip    (2.21   &amp;lt;  2.26,   2%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=100,   compression.type=gzip    (2.21   &amp;lt;  2.29,   3%)

Run 6:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (2.00   &amp;lt;  2.03,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=snappy  (20.99  &amp;lt;  21.05,  0%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=gzip    (1.86   &amp;lt;  2.01,   8%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=10,   messageSize=100,   compression.type=gzip    (2.06   &amp;lt;  2.14,   3%)

Run 7:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (1.81   &amp;lt;  2.08,   14%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=snappy  (20.46  &amp;lt;  21.15,  3%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=gzip    (1.91   &amp;lt;  2.04,   6%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=snappy  (20.90  &amp;lt;  21.07,  0%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=gzip    (1.92   &amp;lt;  2.10,   9%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,    messageSize=100,   compression.type=gzip    (2.28   &amp;lt;  2.29,   0%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100,  messageSize=100,   compression.type=gzip    (2.23   &amp;lt;  2.29,   2%)

Run 8:
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,    messageSize=100,   compression.type=gzip    (1.95   &amp;lt;  2.04,   4%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=gzip    (1.96   &amp;lt;  1.99,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=10,   messageSize=100,   compression.type=snappy  (19.19  &amp;lt;  21.03,  9%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=gzip    (1.91   &amp;lt;  2.02,   5%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100,  messageSize=100,   compression.type=snappy  (19.54  &amp;lt;  20.65,  5%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,    messageSize=100,   compression.type=gzip    (2.01   &amp;lt;  2.14,   6%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=10,   messageSize=100,   compression.type=gzip    (2.09   &amp;lt;  2.12,   1%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100,  messageSize=100,   compression.type=gzip    (2.03   &amp;lt;  2.12,   4%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=100,   compression.type=gzip    (2.20   &amp;lt;  2.28,   3%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=10,   messageSize=100,   compression.type=snappy  (23.38  &amp;lt;  24.08,  2%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100,  messageSize=100,   compression.type=gzip    (2.19   &amp;lt;  2.29,   4%)
2nd:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100,  messageSize=100,   compression.type=snappy  (21.38  &amp;lt;  22.53,  5%)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In almost all the cases where 0.9 wins, max.in.flight.requests.per.connection=5, messageSize=100. In these cases, the throughput depends more on the user thread because the sender thread is pipelining. &lt;/p&gt;

&lt;p&gt;It seems that we can say if the producers are sending small messages and the user thread is the bottleneck, the trunk producer will have less throughput compared with 0.9. The magnitude of the throughput reduce fluctuates.&lt;/p&gt;

&lt;p&gt;I am not sure if we should change the default setting in the producer. We may be able to change the default setting for the producer but It seems for best performance the users need to tune the producer anyways.&lt;/p&gt;</comment>
                            <comment id="15256788" author="ijuma" created="Mon, 25 Apr 2016 18:54:41 +0000"  >&lt;p&gt;Thanks for your work on this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;. I finally have a bit of time to look into this again. You asked:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;From Ismael Juma test, it seems the gap is around 8-10% in the throughput of user bytes when message size is 100. Is this different from what we saw before which has 17% gap? What was the integer range of the test? And how many records were sent during the test? One thing I noticed is that it takes a few seconds for the throughput to become stable, so if the tests finished very quickly, the results may not be quite accurate. The tests I ran adjusted the records number dynamically to let the testing time be at least 15 seconds.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The 17% gap was for 1000 randomly generated bytes, whereas the 8-10% gap was for 100 bytes with more repetition to make the data more compressible. It seems like there are a couple of things that could affect the reliability of results when we run tests via ducktape:&lt;/p&gt;

&lt;p&gt;1. They tend to take between 1 and 2 seconds. You are suggesting that we need to run them for longer.&lt;br/&gt;
2. We don&apos;t necessarily select the same nodes when running the tests again (eg if we have a cluster of 8 nodes, the first test may use nodes 1, 2 and 3 while the second would use 4, 5 and 6).]&lt;/p&gt;

&lt;p&gt;I will now look into your results in more detail.&lt;/p&gt;</comment>
                            <comment id="15264905" author="gwenshap" created="Fri, 29 Apr 2016 22:37:11 +0000"  >&lt;p&gt;This is marked as critical. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; - are you planning to get it to 0.10.0.0? If not, change the fixVersion.&lt;/p&gt;</comment>
                            <comment id="15265530" author="becket_qin" created="Sat, 30 Apr 2016 22:21:50 +0000"  >&lt;p&gt;[ &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gwenshap&quot; class=&quot;user-hover&quot; rel=&quot;gwenshap&quot;&gt;gwenshap&lt;/a&gt; I ran a few tests and was not able to reproduce the issue. It is not clear yet at this point what caused the performance gap we saw. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;, do you have any update? ]&lt;/p&gt;</comment>
                            <comment id="15267803" author="junrao" created="Tue, 3 May 2016 00:25:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the results. It seems that the overall producer throughput can be degraded in trunk in some of the cases. The degradation seems to be related to batching and/or compression ratio, which is a bit hard to fully understand.&lt;/p&gt;

&lt;p&gt;A simpler test is probably to compare the producer throughput between trunk and 0.9.0 when the batching size is the same. To do that, I was thinking that we can set a large batch.size and linger.ms and explicitly call flush() in ProducerPerformance after a certain number of records has been sent. If we can show that the throughput in trunk in this setting is better than 0.9.0 when compression is enabled, we can be confident that the changes in KIP-31/KIP-32 didn&apos;t directly cause a performance degradation.&lt;/p&gt;</comment>
                            <comment id="15268242" author="becket_qin" created="Tue, 3 May 2016 07:07:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I updated the results in the Google sheet. The results are in test_run9 and test_run10&lt;/p&gt;

&lt;p&gt;The linger.ms was run with 0 and 10000 ms. The batch size is set to 1000000. The modified test will flush when the batch is almost full (calculated by BatchSize/RecordSize - 10). Each test scenario runs at least 30 seconds. &lt;/p&gt;

&lt;p&gt;Trunk performs better than 0.9 in all the tested cases. (max.in.flight.requests actually does not matter here, though.)&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;run 9:
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=gzip    (2.19   &amp;gt;  1.83,   19%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=snappy  (18.17  &amp;gt;  14.96,  21%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=gzip    (4.32   &amp;gt;  2.74,   57%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=snappy  (28.66  &amp;gt;  20.59,  39%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.61   &amp;gt;  1.09,   47%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=snappy  (10.77  &amp;gt;  8.41,   28%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.56   &amp;gt;  1.51,   69%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=snappy  (21.37  &amp;gt;  16.24,  31%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=gzip    (2.36   &amp;gt;  1.89,   24%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=snappy  (19.25  &amp;gt;  14.99,  28%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.13   &amp;gt;  2.89,   77%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=snappy  (32.24  &amp;gt;  21.95,  46%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.69   &amp;gt;  1.13,   49%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=snappy  (12.34  &amp;gt;  9.61,   28%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.68   &amp;gt;  1.61,   66%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=snappy  (23.70  &amp;gt;  18.03,  31%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=gzip    (2.58   &amp;gt;  2.04,   26%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=snappy  (19.44  &amp;gt;  16.20,  20%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.65   &amp;gt;  2.97,   90%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=snappy  (33.82  &amp;gt;  26.91,  25%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.74   &amp;gt;  1.17,   48%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=snappy  (12.67  &amp;gt;  10.60,  19%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.61   &amp;gt;  1.59,   64%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=snappy  (24.06  &amp;gt;  21.71,  10%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=gzip    (2.06   &amp;gt;  1.88,   9%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=snappy  (16.77  &amp;gt;  13.71,  22%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=gzip    (3.90   &amp;gt;  2.73,   42%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=snappy  (29.85  &amp;gt;  19.11,  56%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.62   &amp;gt;  1.09,   48%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=snappy  (11.18  &amp;gt;  8.68,   28%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.53   &amp;gt;  1.51,   67%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=snappy  (21.45  &amp;gt;  16.14,  32%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=gzip    (2.28   &amp;gt;  2.01,   13%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=snappy  (18.67  &amp;gt;  14.18,  31%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.28   &amp;gt;  2.95,   78%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=snappy  (34.24  &amp;gt;  22.01,  55%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.61   &amp;gt;  1.14,   41%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=snappy  (12.95  &amp;gt;  9.82,   31%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.68   &amp;gt;  1.63,   64%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=snappy  (24.54  &amp;gt;  18.31,  34%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=gzip    (2.50   &amp;gt;  2.17,   15%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=snappy  (18.78  &amp;gt;  15.79,  18%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.51   &amp;gt;  3.14,   75%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=snappy  (35.34  &amp;gt;  28.92,  22%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.75   &amp;gt;  1.15,   52%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=snappy  (13.41  &amp;gt;  11.02,  21%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.62   &amp;gt;  1.58,   65%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=snappy  (25.40  &amp;gt;  22.47,  13%)


run 10:
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=gzip    (2.15   &amp;gt;  1.81,   18%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=snappy  (18.72  &amp;gt;  14.84,  26%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=gzip    (3.94   &amp;gt;  2.74,   43%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=snappy  (29.69  &amp;gt;  20.33,  46%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.64   &amp;gt;  1.09,   50%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=snappy  (10.93  &amp;gt;  8.41,   29%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.50   &amp;gt;  1.51,   65%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=snappy  (21.25  &amp;gt;  16.19,  31%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=gzip    (2.41   &amp;gt;  1.88,   28%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=snappy  (18.96  &amp;gt;  14.97,  26%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.22   &amp;gt;  2.89,   80%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=snappy  (32.47  &amp;gt;  22.00,  47%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.74   &amp;gt;  1.12,   55%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=snappy  (11.86  &amp;gt;  9.60,   23%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.77   &amp;gt;  1.63,   69%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=snappy  (24.10  &amp;gt;  17.71,  36%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=gzip    (2.57   &amp;gt;  1.99,   29%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=snappy  (19.95  &amp;gt;  16.77,  18%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.63   &amp;gt;  2.98,   88%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=snappy  (32.94  &amp;gt;  27.33,  20%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.73   &amp;gt;  1.14,   51%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=snappy  (12.82  &amp;gt;  10.85,  18%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.60   &amp;gt;  1.58,   64%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=snappy  (24.29  &amp;gt;  21.69,  11%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=gzip    (2.01   &amp;gt;  1.90,   5%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=snappy  (17.93  &amp;gt;  13.49,  32%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=gzip    (3.92   &amp;gt;  2.74,   43%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=snappy  (28.23  &amp;gt;  19.01,  48%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.67   &amp;gt;  1.07,   56%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=snappy  (11.10  &amp;gt;  8.66,   28%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.48   &amp;gt;  1.52,   63%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=snappy  (21.07  &amp;gt;  16.24,  29%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=gzip    (2.30   &amp;gt;  2.00,   15%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=snappy  (18.85  &amp;gt;  14.39,  30%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.20   &amp;gt;  2.90,   79%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=snappy  (32.94  &amp;gt;  21.86,  50%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.54   &amp;gt;  1.14,   35%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=snappy  (13.09  &amp;gt;  9.65,   35%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.68   &amp;gt;  1.63,   64%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=snappy  (24.80  &amp;gt;  18.33,  35%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=gzip    (2.49   &amp;gt;  2.15,   15%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=snappy  (17.77  &amp;gt;  15.84,  12%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.72   &amp;gt;  3.00,   90%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=snappy  (33.26  &amp;gt;  28.44,  16%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.73   &amp;gt;  1.15,   50%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=snappy  (13.14  &amp;gt;  11.11,  18%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.61   &amp;gt;  1.57,   66%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=snappy  (25.34  &amp;gt;  22.63,  11%)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15268985" author="junrao" created="Tue, 3 May 2016 16:25:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the new results. Are the stats in test_run9 and test_run10 accurate? Since we are doing manual flushing now, I expect batch-size-avg to be more or less the same between trunk and 0.9.0 in each test. However, the numbers are still quite different.&lt;/p&gt;</comment>
                            <comment id="15268998" author="becket_qin" created="Tue, 3 May 2016 16:31:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; The tests run above have two different linger.ms settings. For the tests that linger.ms is set to 10000, the batch sizes seem almost the same.&lt;/p&gt;</comment>
                            <comment id="15269113" author="junrao" created="Tue, 3 May 2016 17:12:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the explanation. The results now make sense to me. In your test, the batch sizes with explicit flush are all around 500KB. Could you do another round of flush tests with a smaller batch size (say around 40KB)? Also, for completeness, could you run the consumer performance test on the data published with explicit flush? This will help us understand if there is any performance degradation on the consumer side when messages are compressed in the same batch size.&lt;/p&gt;</comment>
                            <comment id="15269491" author="becket_qin" created="Tue, 3 May 2016 20:12:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; Thanks for help looking into this. I updated run 11 in the Google sheet. The batch size was around 40 - 50K. The summary is following. It looks trunk still performs better.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=gzip    (2.04   &amp;gt;  1.36,   50%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=snappy  (11.43  &amp;gt;  8.05,   41%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=gzip    (3.75   &amp;gt;  2.12,   76%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=snappy  (16.33  &amp;gt;  11.28,  44%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.40   &amp;gt;  0.99,   41%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=snappy  (9.16   &amp;gt;  7.37,   24%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.48   &amp;gt;  1.43,   73%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=snappy  (16.04  &amp;gt;  10.91,  47%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=gzip    (2.28   &amp;gt;  1.51,   50%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=snappy  (11.59  &amp;gt;  8.66,   33%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=gzip    (4.50   &amp;gt;  2.23,   101%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=snappy  (17.36  &amp;gt;  12.14,  42%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.55   &amp;gt;  1.04,   49%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=snappy  (10.41  &amp;gt;  7.66,   35%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.58   &amp;gt;  1.51,   70%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=snappy  (15.57  &amp;gt;  11.32,  37%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=gzip    (2.36   &amp;gt;  1.58,   49%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=snappy  (11.96  &amp;gt;  9.51,   25%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=gzip    (5.14   &amp;gt;  2.41,   113%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=snappy  (16.90  &amp;gt;  14.75,  14%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.30   &amp;gt;  1.07,   21%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=snappy  (10.31  &amp;gt;  8.34,   23%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.53   &amp;gt;  1.54,   64%)
1st:  max.in.flight.requests.per.connection=1,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=snappy  (15.84  &amp;gt;  13.59,  16%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=gzip    (1.73   &amp;gt;  1.42,   21%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=100,   compression.type=snappy  (9.20   &amp;gt;  7.66,   20%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=gzip    (2.90   &amp;gt;  2.12,   36%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=0,       messageSize=1000,  compression.type=snappy  (13.85  &amp;gt;  10.25,  35%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.52   &amp;gt;  1.00,   52%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=100,   compression.type=snappy  (10.64  &amp;gt;  7.60,   40%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.43   &amp;gt;  1.44,   68%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=500,    linger.ms=100000,  messageSize=1000,  compression.type=snappy  (15.75  &amp;gt;  10.98,  43%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=gzip    (1.75   &amp;gt;  1.64,   6%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=100,   compression.type=snappy  (10.31  &amp;gt;  8.79,   17%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=gzip    (4.04   &amp;gt;  2.58,   56%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=0,       messageSize=1000,  compression.type=snappy  (16.35  &amp;gt;  12.24,  33%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.51   &amp;gt;  1.04,   45%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=100,   compression.type=snappy  (11.53  &amp;gt;  8.45,   36%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.59   &amp;gt;  1.53,   69%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=5000,   linger.ms=100000,  messageSize=1000,  compression.type=snappy  (17.44  &amp;gt;  12.34,  41%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=gzip    (2.07   &amp;gt;  1.69,   22%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=100,   compression.type=snappy  (11.16  &amp;gt;  8.87,   25%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=gzip    (4.40   &amp;gt;  2.83,   55%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=0,       messageSize=1000,  compression.type=snappy  (17.57  &amp;gt;  14.70,  19%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=gzip    (1.63   &amp;gt;  1.07,   52%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=100,   compression.type=snappy  (11.91  &amp;gt;  9.20,   29%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=gzip    (2.65   &amp;gt;  1.52,   74%)
1st:  max.in.flight.requests.per.connection=5,  valueBound=50000,  linger.ms=100000,  messageSize=1000,  compression.type=snappy  (18.06  &amp;gt;  15.17,  19%)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15269805" author="junrao" created="Tue, 3 May 2016 23:06:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the new results. Could you also test consumer performance and share the results?&lt;/p&gt;</comment>
                            <comment id="15269836" author="guozhang" created="Tue, 3 May 2016 23:27:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; JIRA title? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15269854" author="becket_qin" created="Tue, 3 May 2016 23:47:04 +0000"  >&lt;p&gt;Woops...Touched the the yubi key accidentally...&lt;/p&gt;</comment>
                            <comment id="15270062" author="becket_qin" created="Wed, 4 May 2016 03:09:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; The result with consumer performance test is updated in test run 13. The consumer performance test just consumes from the topic that the producer performance just produced to.&lt;/p&gt;</comment>
                            <comment id="15271185" author="junrao" created="Wed, 4 May 2016 18:47:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the results. As I was looking at the results with linger.ms=10000, I was expecting the consumer throughput to be more or less the same between trunk and 0.9.0 since the batch sizes are about the same in those tests. Half of the results are actually like that. However, the other half looks a bit weird. It seems that trunk can be either much better or worse than 0.9.0. Do you have a good explanation on those? How reliable are those numbers? &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;max.in.flight.requests.per.connection=1, valueBound=500, linger.ms=100000, messageSize=1000, compression.type=gzip     (30.4 &amp;gt; 16.0)
max.in.flight.requests.per.connection=1, valueBound=500, linger.ms=100000, messageSize=1000, compression.type=snappy (47.2 &amp;lt; 61.0)
max.in.flight.requests.per.connection=1, valueBound=5000, linger.ms=100000, messageSize=100, compression.type=gzip     (47.7 &amp;gt; 33.3)
max.in.flight.requests.per.connection=1, valueBound=5000, linger.ms=100000, messageSize=100, compression.type=snappy (28.6 &amp;gt; 21.8)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15271795" author="becket_qin" created="Thu, 5 May 2016 02:11:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I noticed that the consumer tests finished very quickly because the consumption is much faster than producing. So if a producer produces for 30 seconds, it is possible that the consumer consumes everything in 3 seconds. This might cause the test result to be inaccurate. I will re-run the test with more data and let the consumers to consume for longer time.&lt;/p&gt;</comment>
                            <comment id="15272619" author="becket_qin" created="Thu, 5 May 2016 16:53:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I ran the tests again with more data and it looks the result is stable now. The results are updated in run 13-15.&lt;/p&gt;

&lt;p&gt;Most of the results are similar or reasonable between trunk and 0.9. But the difference between trunk and 0.9 is still bigger than expected in the following two cases. Especially in the first case where the message size is 1000. I am not sure if this is related but this discrepancy only shows up when compression codec is snappy and value bound is 500. I can run snappy decompression test to see if that is the issue. &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;max.in.flight.requests.per.connection=1, valueBound=500, linger.ms=100000, messageSize=1000, compression.type=snappy

start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
22:54:28:955, 22:54:41:990, 953.6743, 73.1626, 1000000, 76716.5324 
23:19:08:786, 23:19:19:701, 953.6743, 87.3728, 1000000, 91617.0408
----------------------
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
00:35:27:626, 00:35:40:751, 953.6743, 72.6609, 1000000, 76190.4762 
00:59:55:306, 01:00:06:217, 953.6743, 87.4048, 1000000, 91650.6278
----------------------
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
23:45:07:404, 23:45:20:463, 953.6743, 73.0281, 1000000, 76575.5418 
00:09:32:282, 00:09:43:315, 953.6743, 86.4384, 1000000, 90637.1794

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;max.in.flight.requests.per.connection=1, valueBound=500, linger.ms=100000, messageSize=100, compression.type=snappy

start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
22:51:24:002, 22:51:43:158, 953.6743, 49.7846, 10000000, 522029.6513 
23:14:43:458, 23:14:59:696, 953.6743, 58.7310, 10000000, 615839.3891
----------------------
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
00:32:23:976, 00:32:43:008, 953.6743, 50.1090, 10000000, 525430.8533 
00:55:30:602, 00:55:46:507, 953.6743, 59.9607, 10000000, 628733.1028
----------------------
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
23:42:03:559, 23:42:22:788, 953.6743, 49.5956, 10000000, 520047.8444 
00:05:09:039, 00:05:25:073, 953.6743, 59.4783, 10000000, 623674.6913

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15272750" author="junrao" created="Thu, 5 May 2016 18:10:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the latest consumer results. Yes, the snappy results are a bit weird. I don&apos;t know how to explain it. It may be useful to see if the difference is in the decompression cost. Also, I am wondering what the results will look like for lz4.&lt;/p&gt;</comment>
                            <comment id="15274917" author="junrao" created="Fri, 6 May 2016 23:32:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, any new findings on the consumer performance? Thanks.&lt;/p&gt;</comment>
                            <comment id="15274969" author="becket_qin" created="Sat, 7 May 2016 00:24:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I didn&apos;t have time to run the test yet. I will investigate this during this weekend. Thanks.&lt;/p&gt;</comment>
                            <comment id="15275431" author="becket_qin" created="Sun, 8 May 2016 00:29:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I think I figured out the reason why 0.9 consumer has better performance than trunk. It is because the recompression on the broker side in 0.9 is more efficient than the streaming compression on the producer side.&lt;/p&gt;

&lt;p&gt;For the setting using snappy compression, message size 100B, valuebound 500, both trunk and 0.9 reports the same batch size on the producer side.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Producer_Test
Select_Rate:	784.10	689.02
Batch_Size_Avg:	10625.79	10204.10
Request_Size_Avg:	85144.37	81771.16
Request_Latency_Avg:	4.41	6.77
Request_Rate:	114.30	99.33
Records_Per_Request_Avg:	801.00	801.00
Record_Queue_Time:	4.09	3.07
Compression_Rate_Avg:	0.79	0.81
92395.823709 records/sec (8.81 MB/sec), 6.52 ms avg latency, 436.00 ms max latency, 6 ms 50th, 9 ms 95th, 9 ms 99th, 17 ms 99.9th
79507.056251 records/sec (7.58 MB/sec), 8.43 ms avg latency, 220.00 ms max latency, 8 ms 50th, 11 ms 95th, 11 ms 99th, 18 ms 99.9th.
Consumer_Test
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
16:14:48:796, 16:15:07:793, 953.6743, 50.2013, 10000000, 526398.9051 
16:17:17:637, 16:17:33:701, 953.6743, 59.3672, 10000000, 622509.9602
----------------------
max.in.flight.requests.per.connection=1, valueBound=500, linger.ms=100000, messageSize=100, compression.type=snappy
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But after I dump the log on the broker side, after recompression the shallow messages on 0.9 broker become ~8K but while the trunk broker still has ~10K shallow message. &lt;/p&gt;

&lt;p&gt;I ran the tests with lz4 as well. The results is updated in test run 16 and 17. I did not see the issue of snappy. Although after broker side recompression the sizes of the shallow messages change a little but they are roughly the same as the producer side batch size.&lt;/p&gt;

&lt;p&gt;I did not see this problem when value bound is 5000. So it seems the batch compression on the broker side the better compression ratio of snappy for certain data pattern is the reason of the performance gap we saw in the test. I listed below the batch sizes before and after recompression for snappy with different settings:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Producer Batch Size Avg:           10204.49
Broker batch size:  ~8.0K
----------------------
max.in.flight.requests.per.connection=1, valueBound=500, linger.ms=100000, messageSize=100, compression.type=gzip

Producer Batch Size Avg:           9107.23
Broker batch size: ~6.6K
----------------------
max.in.flight.requests.per.connection=1, valueBound=500, linger.ms=100000, messageSize=1000, compression.type=snappy

Producer Batch Size Avg:           11457.56
Broker batch size: ~10.5K
----------------------
max.in.flight.requests.per.connection=1, valueBound=5000, linger.ms=100000, messageSize=100, compression.type=snappy

Producer Batch Size Avg:           10429.08
Broker batch size: ~9.4K
----------------------
max.in.flight.requests.per.connection=1, valueBound=5000, linger.ms=100000, messageSize=1000, compression.type=snappy
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="15275793" author="junrao" created="Sun, 8 May 2016 23:40:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the latest analysis. The different data size on the broker can definitely explain the consumer performance difference. In both the producer and the broker, we append messages to the SnappyOutputStream one at a time. So both are done in a streaming fashion. One difference is that SnappyOutputStream in the producer is configured with a buffer size that matches batch.size. In the broker, SnappyOutputStream doesn&apos;t specify a buffer size and always uses the default 32KB. This could affect the compression ratio. One way to verify this is to change the code in 0.9.0 so that we can configure the same buffer size when creating SnappyOutputStream and see if that equalizes the data size.&lt;/p&gt;</comment>
                            <comment id="15275847" author="ijuma" created="Mon, 9 May 2016 01:43:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, would you be able to submit a PR that adds a note to the upgrade page summarising the findings from this JIRA? I think the most important aspect to mention is that the reduced latency by the broker has an impact on batch size, which can affect throughput. So, people who care about throughput should test their workload and tune the producer settings once again. The other thing that may be good to mention is that the message timestamps introduce a bit of overhead in a few scenarios.&lt;/p&gt;</comment>
                            <comment id="15275924" author="becket_qin" created="Mon, 9 May 2016 04:29:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; Yes, you are right. I changed the producer side output buffer size to 32K for trunk code. After that the shallow message size on trunk broker also became ~8K for valueBound=500 and message size = 1000. The consumer throughput of trunk also improves and seems reasonable now. Should we just change the producer compressor to use the default buffer size for all the compression codec? Is there any concern?&lt;/p&gt;</comment>
                            <comment id="15275930" author="guozhang" created="Mon, 9 May 2016 04:40:12 +0000"  >&lt;p&gt;Just curious what is the &lt;tt&gt;batch.size&lt;/tt&gt; in your test code above?&lt;/p&gt;</comment>
                            <comment id="15275934" author="becket_qin" created="Mon, 9 May 2016 04:47:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; We should absolutely mention the 8 bytes overhead. And I agree that we should let users know that producers need to be tuned to get the best performance regardless of the version. &lt;/p&gt;

&lt;p&gt;But I am not sure if the lower latency or smaller batch size really caused the lower throughput, even though theoretically there could be a negative impact. From test result run2 - run8, it looks that the lower message throughput on trunk was primarily caused by the the 8 bytes timestamp overhead - because almost in all the winning cases for 0.9, message size is 100. So mentioning that in the upgrade doc seems not quite solid.&lt;/p&gt;</comment>
                            <comment id="15275948" author="becket_qin" created="Mon, 9 May 2016 04:59:13 +0000"  >&lt;p&gt;BTW, I just created &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3677&quot; title=&quot;Add a producer performance tuning tool&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3677&quot;&gt;KAFKA-3677&lt;/a&gt; to provide a tool to help user tune the producer performance.&lt;/p&gt;</comment>
                            <comment id="15275957" author="junrao" created="Mon, 9 May 2016 05:13:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for confirming this. I guess defaulting the buffer size in snappy using batch.size may be reasonable. It&apos;s just that in your test, the producer is not hitting batch.size, which was set to 1000000. Perhaps &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; can comment more on how to pick buffer size in snappy.&lt;/p&gt;</comment>
                            <comment id="15275959" author="junrao" created="Mon, 9 May 2016 05:16:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, also, it seems that you patched ProducerPerformance with valueBound, which could be useful. Will you be including that in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3677&quot; title=&quot;Add a producer performance tuning tool&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3677&quot;&gt;KAFKA-3677&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15275970" author="becket_qin" created="Mon, 9 May 2016 05:33:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; Yes, the value bound was supposed to be included in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3554&quot; title=&quot;Generate actual data with specific compression ratio and add multi-thread support in the ProducerPerformance tool.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3554&quot;&gt;KAFKA-3554&lt;/a&gt;. I linked &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3677&quot; title=&quot;Add a producer performance tuning tool&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3677&quot;&gt;KAFKA-3677&lt;/a&gt; to that ticket.&lt;/p&gt;</comment>
                            <comment id="15275971" author="becket_qin" created="Mon, 9 May 2016 05:39:46 +0000"  >&lt;p&gt;The batch size was 80K. It is not quite clear to me how this actually works. Intuitively it seems the larger buffer size should give a better compression ratio. But after reducing the buffer size to 32K, the compression ratio actually improved.&lt;/p&gt;</comment>
                            <comment id="15275974" author="becket_qin" created="Mon, 9 May 2016 05:43:11 +0000"  >&lt;p&gt;Actually, never mind. The buffer size is not the batch size, but the message buffer size. So in this case it was 100B + message overhead. Now that makes sense. So it looks that we might want to just use the default buffer size of 32K?&lt;/p&gt;</comment>
                            <comment id="15277023" author="guozhang" created="Mon, 9 May 2016 20:59:32 +0000"  >&lt;p&gt;Actually, in the producer&apos;s GZIP and SNAPPY compression code, the buffer size is set as &lt;tt&gt;COMPRESSION_DEFAULT_BUFFER_SIZE&lt;/tt&gt; but not configured batch.size, which is 1K. So resetting the block size to 32K would probably help in the compression ratio.&lt;/p&gt;</comment>
                            <comment id="15277072" author="becket_qin" created="Mon, 9 May 2016 21:24:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; It seems that we are using the default buffer size to append the Int, Long and other message overhead bytes. But when we compress the messages, we actually use &lt;tt&gt;Record.write()&lt;/tt&gt; which uses the message buffer size to do the compression. I am not sure why we are compressing in two different ways.&lt;/p&gt;</comment>
                            <comment id="15277142" author="guozhang" created="Mon, 9 May 2016 22:02:15 +0000"  >&lt;p&gt;You are referring to &lt;tt&gt;new Compressor(buffer, CompressionType.NONE, buffer.capacity());&lt;/tt&gt; in &lt;tt&gt;Record.write()&lt;/tt&gt; right? That function actually does not compress the data anymore, as it just writes the &quot;compressed&quot; bytes with the compression type it used to compress the raw bytes. So in the calling function you can see that we always use &lt;tt&gt;CompressionType.NONE&lt;/tt&gt; to write this single shallow message.&lt;/p&gt;</comment>
                            <comment id="15279536" author="junrao" created="Wed, 11 May 2016 04:49:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;, do you want to patch the producer to use the default buffer size (instead of 1KB) for snappy compressor?&lt;/p&gt;</comment>
                            <comment id="15279541" author="guozhang" created="Wed, 11 May 2016 04:51:43 +0000"  >&lt;p&gt;I can do that, under this JIRA or file a separate one?&lt;/p&gt;</comment>
                            <comment id="15280213" author="junrao" created="Wed, 11 May 2016 14:30:09 +0000"  >&lt;p&gt;Since that&apos;s an existing issue, perhaps file a new jira?&lt;/p&gt;</comment>
                            <comment id="15280721" author="becket_qin" created="Wed, 11 May 2016 20:03:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; I think it makes sense to just use 32K as default buffer size. I ran some preliminary tests. It seems 32K compressor buffer size gave better throughput also. The downside is that there will probably be a larger memory footprint on the producer side because this is a per partition buffer. In some extreme cases such as mirror maker, we may need to bump up JVM heap size.&lt;/p&gt;</comment>
                            <comment id="15280863" author="guozhang" created="Wed, 11 May 2016 21:31:15 +0000"  >&lt;p&gt;Yes, that is a valid concern. Maybe we can add a note in the upgrade section accordingly, cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gwenshap&quot; class=&quot;user-hover&quot; rel=&quot;gwenshap&quot;&gt;gwenshap&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15280933" author="gwenshap" created="Wed, 11 May 2016 22:39:03 +0000"  >&lt;p&gt;32K per partition sounds pretty minor to me. &lt;br/&gt;
You&apos;d need 30,000 partitions (for single MM instance!) before this starts making a real dent in the RAM.&lt;br/&gt;
In the common case of 1000-5000 partitions, we are talking about 32-150M, which is nothing.&lt;/p&gt;

&lt;p&gt;Adding a note is a good idea, but include the numbers, so this won&apos;t look like a huge deal.&lt;/p&gt;</comment>
                            <comment id="15281192" author="becket_qin" created="Thu, 12 May 2016 04:24:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gwenshap&quot; class=&quot;user-hover&quot; rel=&quot;gwenshap&quot;&gt;gwenshap&lt;/a&gt; I am not sure if 32K is minor. &lt;/p&gt;

&lt;p&gt;Typically a mirror maker producer will be producing to all the topics in the target cluster. A 40 node cluster with 1000 partitions each gives 40,000 partitions. In that case, the compression buffer will take 1.2GB of memory. As an indirect indication, we usually see the accumulator memory footprint to be somewhere between 600MB ~ 900MB when batch size is 16K. This size is roughly the #partitions * batchSize assuming there are only one or two batches in the queue of each partition.&lt;/p&gt;</comment>
                            <comment id="15281213" author="gwenshap" created="Thu, 12 May 2016 05:00:49 +0000"  >&lt;p&gt;This is consistent with my calculations, except that 90% of the clusters out there have fewer than 5000 partitions for the entire cluster.&lt;/p&gt;

&lt;p&gt;As long as we show users which memory increase to expect (+1GB in worst case), it is all good.&lt;/p&gt;

&lt;p&gt;P.S.&lt;br/&gt;
Most of my customers have more than a single MM instance for a cluster (for throughput and HA), and due to the way consumer partition-assignment works, each producer only produces to a subset of the partitions that the consumers on the same instance got assigned (so usually just 1/3rd of the cluster)&lt;/p&gt;</comment>
                            <comment id="15281735" author="becket_qin" created="Thu, 12 May 2016 16:47:28 +0000"  >&lt;p&gt;Thanks for the clarification, Gwen. &lt;/p&gt;

&lt;p&gt;Just curious, by &quot;each producer only produces to a subset of the partitions that the consumers on the same instance got assigned&quot;, do you mean people are using customized MirrorMakerMessageHandler to do partition to partition copy? The default MirrorMakerMessageHandler just maintains the key and value of the messages. It does not pass the source partition information to the producer. So even if the consumers of a mirror maker process are only assigned one partition of a topic, it seems the messages consumed from that source partition will still go to all the partitions in the target cluster.&lt;/p&gt;</comment>
                            <comment id="15281777" author="gwenshap" created="Thu, 12 May 2016 17:19:45 +0000"  >&lt;p&gt;Why do I need to override the handler?&lt;/p&gt;

&lt;p&gt;The hash function is consistent between the clusters, i.e. all the keys in partition 0 in the source were hashed and mapped to partition 0. Given same number of partitions in target (which is very common), the same key will always get hashed to partition 0 again, right?&lt;/p&gt;

&lt;p&gt;So if I&apos;m reading from specific subset of partitions and don&apos;t modify the keys, I should end up writing to same subset. &lt;/p&gt;</comment>
                            <comment id="15281803" author="becket_qin" created="Thu, 12 May 2016 17:30:32 +0000"  >&lt;p&gt;Got it. Yes, if the messages are keyed messages and the partition numbers are the same between source and destination clusters, there is no need to have a customized message handler.&lt;/p&gt;</comment>
                            <comment id="15283872" author="junrao" created="Sun, 15 May 2016 16:04:46 +0000"  >&lt;p&gt;Issue resolved by pull request 1372&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/1372&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1372&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13071849">KAFKA-5236</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 27 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2w78f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>