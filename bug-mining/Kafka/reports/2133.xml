<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:16:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7786] Fast update of leader epoch may stall partition fetching due to FENCED_LEADER_EPOCH</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7786</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;KIP-320/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7395&quot; title=&quot;Add fencing to replication protocol (KIP-320)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7395&quot;&gt;&lt;del&gt;KAFKA-7395&lt;/del&gt;&lt;/a&gt; Added FENCED_LEADER_EPOCH error response&#160;to a OffsetsForLeaderEpoch request if the epoch in the request is lower than the broker&apos;s leader epoch. ReplicaFetcherThread builds a OffsetsForLeaderEpoch request under &lt;em&gt;partitionMapLock&lt;/em&gt;, sends the request outside the lock, and then processes the response under &lt;em&gt;partitionMapLock&lt;/em&gt;. The broker may receive LeaderAndIsr with the same leader but with the next leader epoch, remove and add partition to the fetcher thread (with partition state reflecting the updated leader epoch) &#8211; all while the&#160;OffsetsForLeaderEpoch request (with the old leader epoch) is still outstanding/ waiting for the lock to process the&#160;OffsetsForLeaderEpoch response. As a result, partition gets removed from partitionStates and this broker will not fetch for this partition until the next LeaderAndIsr which may take a while. We will see log message like this:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-12-23 07:23:04,802&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0&amp;#93;&lt;/span&gt; Partition test_topic-17 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)&lt;/p&gt;

&lt;p&gt;We saw this happen with&#160;kafkatest.tests.core.reassign_partitions_test.ReassignPartitionsTest.test_reassign_partitions.bounce_brokers=True.reassign_from_offset_zero=True. This test&#160;does partition re-assignment while bouncing 2 out of 4 total brokers. When the failure happen, each bounced broker was also a controller. Because of re-assignment, the controller updates leader epoch without updating the leader on controller change or on broker startup, so we see several leader epoch changes without the leader change, which increases the likelihood of the race condition described above.&lt;/p&gt;

&lt;p&gt;Here is exact events that happen in this test (around the failure):&lt;/p&gt;

&lt;p&gt;We have 4 brokers Brokers 1, 2, 3, 4.&#160;Partition re-assignment&#160;is started for test_topic-17 &lt;span class=&quot;error&quot;&gt;&amp;#91;2, 4, 1&amp;#93;&lt;/span&gt;&#160; &#8212;&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;3, 1, 2&amp;#93;&lt;/span&gt;. At time t0, leader of test_topic-17 is broker 2.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;clean shutdown of broker 3, which is also a controller&lt;/li&gt;
	&lt;li&gt;broker 4 becomes controller, continues re-assignment and updates leader epoch for test_topic-17 to 6 (with same leader)&lt;/li&gt;
	&lt;li&gt;broker 2 (leader of test_topic-17) receives new leader epoch: &#8220;test_topic-17 starts at Leader Epoch 6 from offset 1388. Previous Leader Epoch was: 5&#8221;&lt;/li&gt;
	&lt;li&gt;broker 3 is started again after clean shutdown&lt;/li&gt;
	&lt;li&gt;controller sees broker 3 startup, and sends LeaderAndIsr(leader epoch 6) to broker 3&lt;/li&gt;
	&lt;li&gt;controller updates leader epoch to 7&lt;/li&gt;
	&lt;li&gt;broker 2 (leader of test_topic-17) receives LeaderAndIsr for leader epoch 7: &#8220;test_topic-17 starts at Leader Epoch 7 from offset 1974. Previous Leader Epoch was: 6&#8221;&lt;/li&gt;
	&lt;li&gt;broker 3 receives LeaderAndIsr for test_topic-17 and leader epoch 6 from controller: &#8220;Added fetcher to broker BrokerEndPoint(id=2) for leader epoch 6&#8221; and sends OffsetsForLeaderEpoch request to broker 2&lt;/li&gt;
	&lt;li&gt;broker 3 receives LeaderAndIsr for test_topic-17 and leader epoch 7 from controller; removes fetcher thread and adds fetcher thread + executes AbstractFetcherThread.addPartitions() which updates partition state with leader epoch 7&lt;/li&gt;
	&lt;li&gt;broker 3 receives FENCED_LEADER_EPOCH in response to OffsetsForLeaderEpoch(leader epoch 6), because the leader received LeaderAndIsr for leader epoch 7 before it got OffsetsForLeaderEpoch(leader epoch 6) from broker 3. As a result, it removes partition from partitionStates and it does not fetch until controller updates leader epoch and sends LeaderAndIsr for this partition to broker 3. The test fails, because re-assignment does not finish on time (due to broker 3 not fetching).&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;One way to address this is possibly add more state to&#160;PartitionFetchState. However,&#160;we may introduce other race condition. A cleaner way, I think, is to return leader epoch in the&#160;OffsetsForLeaderEpoch response with&#160;FENCED_LEADER_EPOCH error, and then ignore the error if partition state contains a higher leader epoch. The advantage is less state maintenance, but disadvantage is it requires bumping inter-broker protocol.&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;%C2%A0&quot;&gt;&lt;/a&gt;&#160;&lt;/h1&gt;</description>
                <environment></environment>
        <key id="13207619">KAFKA-7786</key>
            <summary>Fast update of leader epoch may stall partition fetching due to FENCED_LEADER_EPOCH</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="apovzner">Anna Povzner</assignee>
                                    <reporter username="apovzner">Anna Povzner</reporter>
                        <labels>
                    </labels>
                <created>Fri, 4 Jan 2019 07:12:37 +0000</created>
                <updated>Wed, 9 Jan 2019 19:27:59 +0000</updated>
                            <resolved>Wed, 9 Jan 2019 19:27:59 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16736415" author="junrao" created="Mon, 7 Jan 2019 22:00:39 +0000"  >&lt;p&gt;Great find, Anna. About the fix. I am not sure that we need the protocol change. We know the leader epoch used in&#160;OffsetsForLeaderEpoch request. We can just pass that along with the&#160;OffsetsForLeaderEpoch response to&#160;maybeTruncate(). If the leaderEpoch in partitionStates has changed, we simply ignore the response and retry the&#160;OffsetsForLeaderEpoch request with the new leader epoch.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16736416" author="githubbot" created="Mon, 7 Jan 2019 22:00:43 +0000"  >&lt;p&gt;apovzner commented on pull request #6101: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7786&quot; title=&quot;Fast update of leader epoch may stall partition fetching due to FENCED_LEADER_EPOCH&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7786&quot;&gt;&lt;del&gt;KAFKA-7786&lt;/del&gt;&lt;/a&gt;: Ignore OffsetsForLeaderEpoch response if leader epoch changed while request in flight&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/6101&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6101&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   There is a race condition in ReplicaFetcherThread, where we can update PartitionFetchState with the new leader epoch (same leader) before handling the OffsetsForLeaderEpoch response with FENCED_LEADER_EPOCH error which causes removing partition from partitionStates, which in turn causes no fetching until the next LeaderAndIsr. &lt;/p&gt;

&lt;p&gt;   Our system test kafkatest.tests.core.reassign_partitions_test.ReassignPartitionsTest.test_reassign_partitions.bounce_brokers=True.reassign_from_offset_zero=True failed 3 times due to this error in the last couple of months. Since this test is already able to test this condition, not adding any more tests.&lt;/p&gt;

&lt;p&gt;   Also added toString() implementation to PartitionData, because some log messages did not show useful info which I found while investigating the above system test failure.&lt;/p&gt;

&lt;p&gt;   cc @hachikuji who suggested the fix.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16736420" author="apovzner" created="Mon, 7 Jan 2019 22:03:34 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, thanks for the comment. I recently talked to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; and he suggested the same fix, with no protocol change. I just opened a PR: &lt;a href=&quot;https://github.com/apache/kafka/pull/6101.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6101.&lt;/a&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16738557" author="githubbot" created="Wed, 9 Jan 2019 19:09:52 +0000"  >&lt;p&gt;hachikuji commented on pull request #6101: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7786&quot; title=&quot;Fast update of leader epoch may stall partition fetching due to FENCED_LEADER_EPOCH&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7786&quot;&gt;&lt;del&gt;KAFKA-7786&lt;/del&gt;&lt;/a&gt;: Ignore OffsetsForLeaderEpoch response if leader epoch changed while request in flight&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/6101&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6101&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 44 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|u00ibs:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>