<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:44:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-2477] Replicas spuriously deleting all segments in partition</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-2477</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We&apos;re seeing some strange behaviour in brokers: a replica will sometimes schedule all segments in a partition for deletion, and then immediately start replicating them back, triggering our check for under-replicating topics.&lt;/p&gt;

&lt;p&gt;This happens on average a couple of times a week, for different brokers and topics.&lt;/p&gt;

&lt;p&gt;We have per-topic retention.ms and retention.bytes configuration, the topics where we&apos;ve seen this happen are hitting the size limit.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12858972">KAFKA-2477</key>
            <summary>Replicas spuriously deleting all segments in partition</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="becket_qin">Jiangjie Qin</assignee>
                                    <reporter username="hakon">H&#229;kon Hitland</reporter>
                        <labels>
                    </labels>
                <created>Wed, 26 Aug 2015 12:59:06 +0000</created>
                <updated>Wed, 14 Oct 2015 20:05:00 +0000</updated>
                            <resolved>Thu, 8 Oct 2015 04:59:34 +0000</resolved>
                                    <version>0.8.2.1</version>
                                    <fixVersion>0.9.0.0</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="14713079" author="hakon" created="Wed, 26 Aug 2015 13:04:34 +0000"  >&lt;p&gt;Attached example log from a broker&lt;/p&gt;</comment>
                            <comment id="14715280" author="becket_qin" created="Wed, 26 Aug 2015 18:22:38 +0000"  >&lt;p&gt;Maybe related to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2143&quot; title=&quot;Replicas get ahead of leader and fail&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2143&quot;&gt;&lt;del&gt;KAFKA-2143&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14715518" author="hakon" created="Wed, 26 Aug 2015 21:08:13 +0000"  >&lt;p&gt;Thanks for the reply. Checking the logs, we did get the &quot;Error when processing fetch request&quot; error in the leader mentioned in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2143&quot; title=&quot;Replicas get ahead of leader and fail&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2143&quot;&gt;&lt;del&gt;KAFKA-2143&lt;/del&gt;&lt;/a&gt;, so it could be the same issue.&lt;/p&gt;

&lt;p&gt;I don&apos;t see anything in our logs about a leader change, so I don&apos;t think it is caused by an unclean election, like some of the comments suggest.&lt;/p&gt;</comment>
                            <comment id="14715604" author="becket_qin" created="Wed, 26 Aug 2015 22:03:39 +0000"  >&lt;p&gt;What is the partition replication factor?&lt;br/&gt;
Also, can you search for &quot;start offset&quot; in the server log of the broker who truncates its log?&lt;/p&gt;</comment>
                            <comment id="14715742" author="hakon" created="Wed, 26 Aug 2015 23:33:37 +0000"  >&lt;p&gt;We use a replication factor of 3.&lt;br/&gt;
The only line with &quot;start offset&quot; that day is the one in the attached log:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-08-24 18:32:32,299&amp;#93;&lt;/span&gt; WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaFetcherThread-3-0&amp;#93;&lt;/span&gt;, Replica 3 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;log.event,3&amp;#93;&lt;/span&gt; reset its fetch offset from 10200597616 to current leader 0&apos;s start offset 10200597616 (kafka.server.ReplicaFetcherThread)&lt;/p&gt;

&lt;p&gt;e: the leader error reads:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-08-24 18:32:32,145&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 0&amp;#93;&lt;/span&gt;: Error when processing fetch request for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;log.event,3&amp;#93;&lt;/span&gt; offset 10349592111 from follower with correlation id 141609587. Possible cause: Request for offset 10349592111 but we only have log segments in the range 10200597616 to 10349592109. (kafka.server.ReplicaManager)&lt;/p&gt;</comment>
                            <comment id="14720419" author="junrao" created="Fri, 28 Aug 2015 19:10:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hakon&quot; class=&quot;user-hover&quot; rel=&quot;hakon&quot;&gt;hakon&lt;/a&gt;, the leader changes will be logged in the controller log. Do you see anything there? Also, we have a jmx metrics kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec&lt;br/&gt;
 that tells you if there is any unclean leader election.&lt;/p&gt;</comment>
                            <comment id="14723259" author="hakon" created="Mon, 31 Aug 2015 09:26:29 +0000"  >&lt;p&gt;&quot;kafka.controller:name=UncleanLeaderElectionsPerSec,type=ControllerStats&quot; has Count = 0 on all brokers.&lt;/p&gt;

&lt;p&gt;There is nothing in any controller.log from that day except for one broker checking leader imbalance.&lt;br/&gt;
(&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-08-24 00:02:32,650&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;Controller 0&amp;#93;&lt;/span&gt;: checking need to trigger partition rebalance (kafka.controller.KafkaController)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-08-24 00:02:32,684&amp;#93;&lt;/span&gt; DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Controller 0&amp;#93;&lt;/span&gt;: preferred replicas by broker Map(&lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt;) (kafka.controller.KafkaController)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-08-24 00:02:32,684&amp;#93;&lt;/span&gt; DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Controller 0&amp;#93;&lt;/span&gt;: topics not in preferred replica Map() (kafka.controller.KafkaController)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-08-24 00:02:32,685&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;Controller 0&amp;#93;&lt;/span&gt;: leader imbalance ratio for broker 2 is 0.000000 (kafka.controller.KafkaController)&lt;br/&gt;
, etc.)&lt;/p&gt;</comment>
                            <comment id="14724559" author="junrao" created="Tue, 1 Sep 2015 01:40:37 +0000"  >&lt;p&gt;Interesting, so there is nothing in broker 0&apos;s log about truncating the log for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;log.event,3&amp;#93;&lt;/span&gt;? &lt;/p&gt;

&lt;p&gt;It seems that this is reproducible. Would it be possible for you to enable TRACE level logging for class kafka.log.Log on every broker. This will log the new log end offset after each message append. When this issue happens again, we can verify if it is indeed that the follower uses a fetch offset that&apos;s larger than the leader&apos;s log end offset.&lt;/p&gt;</comment>
                            <comment id="14727933" author="hakon" created="Wed, 2 Sep 2015 20:07:16 +0000"  >&lt;p&gt;I don&apos;t think enabling trace logging would be practical in our production environment, unfortunately.&lt;/p&gt;

&lt;p&gt;We see the error regularly in production, but I haven&apos;t been able to reproduce it locally.&lt;/p&gt;</comment>
                            <comment id="14730102" author="junrao" created="Fri, 4 Sep 2015 00:33:35 +0000"  >&lt;p&gt;Could you then try the following? In the above situation, go to broker 0&apos;s log dir for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;log.event,3&amp;#93;&lt;/span&gt;. Get the name of the last log segment (the .log file). Then run the following&lt;br/&gt;
bin/kafka-run-class.sh kafka.tools.DumpLogSegments &lt;span class=&quot;error&quot;&gt;&amp;#91;logsegmentname&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This will print out the offset of each message. In the normal case, those offsets should be monotonically increasing. Could you check if there is any out of sequence offsets in the output especially close to 10349592109?&lt;/p&gt;</comment>
                            <comment id="14730713" author="hakon" created="Fri, 4 Sep 2015 12:24:31 +0000"  >&lt;p&gt;I don&apos;t see any out of sequence offsets.&lt;br/&gt;
Here are a couple of recent examples.&lt;br/&gt;
If I run with --deep-iteration, all offsets are present and sequential.&lt;br/&gt;
The result on the replica is identical to the leader.&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-09-02 23:43:03,379&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 0&amp;#93;&lt;/span&gt;: Error when processing fetch request for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;log.event,3&amp;#93;&lt;/span&gt; offset 10591627212 from follower with correlation id 391785394. Possible cause: Request for offset 10591627212 but we only have log segments in the range 10444248800 to 10591627211. (kafka.server.ReplicaManager)&lt;/p&gt;

&lt;p&gt;offset: 10591627210 position: 994954613 isvalid: true payloadsize: 674 magic: 0 compresscodec: SnappyCompressionCodec crc: 4144791071&lt;br/&gt;
offset: 10591627211 position: 994955313 isvalid: true payloadsize: 1255 magic: 0 compresscodec: SnappyCompressionCodec crc: 1011806998&lt;br/&gt;
offset: 10591627213 position: 994956594 isvalid: true payloadsize: 1460 magic: 0 compresscodec: SnappyCompressionCodec crc: 4145284502&lt;br/&gt;
offset: 10591627215 position: 994958080 isvalid: true payloadsize: 1719 magic: 0 compresscodec: SnappyCompressionCodec crc: 444418110&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-09-03 11:44:02,483&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 3&amp;#93;&lt;/span&gt;: Error when processing fetch request for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;log.count,5&amp;#93;&lt;/span&gt; offset 69746066284 from follower with correlation id 239821628. Possible cause: Request for offset 69746066284 but we only have log segments in the range 68788206610 to 69746066280. (kafka.server.ReplicaManager)&lt;/p&gt;

&lt;p&gt;offset: 69746066278 position: 464897345 isvalid: true payloadsize: 674 magic: 0 compresscodec: SnappyCompressionCodec crc: 3013732329&lt;br/&gt;
offset: 69746066279 position: 464898045 isvalid: true payloadsize: 234 magic: 0 compresscodec: SnappyCompressionCodec crc: 3286064200&lt;br/&gt;
offset: 69746066283 position: 464898305 isvalid: true payloadsize: 486 magic: 0 compresscodec: SnappyCompressionCodec crc: 747917524&lt;br/&gt;
offset: 69746066285 position: 464898817 isvalid: true payloadsize: 342 magic: 0 compresscodec: SnappyCompressionCodec crc: 4283754786&lt;br/&gt;
offset: 69746066286 position: 464899185 isvalid: true payloadsize: 233 magic: 0 compresscodec: SnappyCompressionCodec crc: 2129213572&lt;/p&gt;</comment>
                            <comment id="14731210" author="junrao" created="Fri, 4 Sep 2015 18:34:24 +0000"  >&lt;p&gt;Thanks. Then the log looks normal. The only thing that I can recommend now is to try reproducing the issue locally and apply the trace level logging.&lt;/p&gt;

&lt;p&gt;Also, since you are using snappy, you may want to apply the fixes in 0.8.2.2 (&lt;a href=&quot;https://people.apache.org/~junrao/kafka-0.8.2.2-candidate1/RELEASE_NOTES.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://people.apache.org/~junrao/kafka-0.8.2.2-candidate1/RELEASE_NOTES.html&lt;/a&gt;) once it&apos;s out. They may not be related to the issue that you are seeing here though.&lt;/p&gt;</comment>
                            <comment id="14734921" author="hakon" created="Tue, 8 Sep 2015 14:47:29 +0000"  >&lt;p&gt;Attached trace log from leader. Filtered to only lines for the relevant partition.&lt;/p&gt;</comment>
                            <comment id="14734928" author="hakon" created="Tue, 8 Sep 2015 14:51:13 +0000"  >&lt;p&gt;I was able to enable trace logging on a production server, and have captured logs from the leader when the error happens.&lt;/p&gt;

&lt;p&gt;It looks like the attempted read happens right before the log is actually appended. I don&apos;t see any other abnormal behaviour.&lt;/p&gt;

&lt;p&gt;Looking at the code in question, I think I have an idea of how it might happen:&lt;/p&gt;

&lt;p&gt;kafka.log.Log uses a lock to synchronize writes, but not reads.&lt;/p&gt;

&lt;p&gt;Assume a write W1 has gotten as far as FileMessageSet.append() and has just executed _size.getAndAdd(written)&lt;/p&gt;

&lt;p&gt;Now a concurrent read R1 comes in. In FileMessageSet.read(), it can get a new message set with end = math.min(this.start + position + size, sizeInBytes()). This includes the message that was just written in W1.&lt;/p&gt;

&lt;p&gt;The read finishes, and a new read R2 starts. R2 tries to continue from W1, but in Log.read() it finds that startOffset is larger than nextOffsetMetadata.messageOffset and throws an exception.&lt;br/&gt;
(By the way, Log.read() can potentially read nextOffsetMetadata multiple times, with no guarantee that it hasn&apos;t changed. It&apos;s not obvious to me that this is correct.)&lt;/p&gt;

&lt;p&gt;Finally, W1 updates nextOffsetMetadata in Log.updateLogEndOffset(), too late for R2 which has already triggered a log truncation on the replica.&lt;/p&gt;

&lt;p&gt;Some possible solutions:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Synchronize access to nextOffsetMetadata in Log.read()&lt;/li&gt;
	&lt;li&gt;Clamp reads in Log.read() to never go beyond the current message offset.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14735015" author="becket_qin" created="Tue, 8 Sep 2015 15:41:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hakon&quot; class=&quot;user-hover&quot; rel=&quot;hakon&quot;&gt;hakon&lt;/a&gt; Yes, that&apos;s correct.&lt;/p&gt;

&lt;p&gt;The log appending does the following two things:&lt;br/&gt;
1. Append message to log&lt;br/&gt;
2. Update Log.nextOffsetMetadata.messageOffset.&lt;br/&gt;
If two follower reads come between 1 and 2. There will be a out of range exception. I think the fix is to read up to Log.nextOffsetMetadata.messageOffset for replicas instead of max size.&lt;/p&gt;

&lt;p&gt;Are you interested in submitting a patch?&lt;/p&gt;</comment>
                            <comment id="14735134" author="hakon" created="Tue, 8 Sep 2015 16:49:54 +0000"  >&lt;p&gt;I don&apos;t think I can provide a patch at the moment, I would appreciate if someone more familiar with the code fixed it.&lt;/p&gt;</comment>
                            <comment id="14735152" author="becket_qin" created="Tue, 8 Sep 2015 16:57:58 +0000"  >&lt;p&gt;No worries. I can do that &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14741274" author="githubbot" created="Fri, 11 Sep 2015 18:11:57 +0000"  >&lt;p&gt;Github user becketqin closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/204&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/204&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14741275" author="githubbot" created="Fri, 11 Sep 2015 18:11:59 +0000"  >&lt;p&gt;GitHub user becketqin reopened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/204&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/204&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt;: Fix a race condition between log append and fetch that causes OffsetOutOfRangeException.&lt;/p&gt;

&lt;p&gt;    Tried two fixes. I prefer the second approach because it saves an additional offset search.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/becketqin/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/becketqin/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/204.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/204.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #204&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit e7610fb69a4007ae661a768635e930355c8caa76&lt;br/&gt;
Author: Jiangjie Qin &amp;lt;becket.qin@gmail.com&amp;gt;&lt;br/&gt;
Date:   2015-09-11T02:17:12Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt;: Fix a race condition between log append and fetch that causes OffsetOutOfRangeException&lt;/p&gt;

&lt;p&gt;commit 45364d76e756fc6075924b3a07651b7fbbcc391a&lt;br/&gt;
Author: Jiangjie Qin &amp;lt;becket.qin@gmail.com&amp;gt;&lt;br/&gt;
Date:   2015-09-11T03:06:35Z&lt;/p&gt;

&lt;p&gt;    A second fix that avoids an addition offset search&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14744401" author="becket_qin" created="Mon, 14 Sep 2015 22:23:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I just marked this ticket for 0.9. The patch is small. Will you be able to take a look? &lt;/p&gt;</comment>
                            <comment id="14941999" author="junrao" created="Sat, 3 Oct 2015 00:35:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hakon&quot; class=&quot;user-hover&quot; rel=&quot;hakon&quot;&gt;hakon&lt;/a&gt;, thanks a lot for the update. This seems like a real issue. Also, the point that you made on  &quot;Log.read() can potentially read nextOffsetMetadata multiple times&quot; is also relevant. In Log.read(), we have the following code:&lt;/p&gt;

&lt;p&gt;    // check if the offset is valid and in range&lt;br/&gt;
    val next = nextOffsetMetadata.messageOffset&lt;br/&gt;
    if(startOffset == next)&lt;br/&gt;
      return FetchDataInfo(nextOffsetMetadata, MessageSet.Empty)&lt;/p&gt;

&lt;p&gt;This seems wrong. If nextOffsetMetadata changes after the if test, we could return a larger fetchOffsetMetadata in FetchDataInfo that we should. This will potentially affect the computation of things like isr. In this case, we should get a reference of nextOffsetMetadata first and use that to do the if test and as the return value.&lt;/p&gt;

&lt;p&gt;Log.read() also references nextOffsetMetadata again in the last line. I am not sure if the comment is correct. The last message will never be deleted, so it seems that we can never reach the last statement.&lt;/p&gt;

&lt;p&gt;    // okay we are beyond the end of the last segment with no data fetched although the start offset is in range,&lt;br/&gt;
    // this can happen when all messages with offset larger than start offsets have been deleted.&lt;br/&gt;
    // In this case, we will return the empty set with log end offset metadata&lt;br/&gt;
    FetchDataInfo(nextOffsetMetadata, MessageSet.Empty)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, do you want to fix nextOffsetMetadata in your patch too?&lt;/p&gt;</comment>
                            <comment id="14944398" author="becket_qin" created="Tue, 6 Oct 2015 02:03:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I&apos;ll fix the nexOffsetMetadata in the patch.&lt;/p&gt;

&lt;p&gt;Would the following case be what the last line was trying to address?&lt;/p&gt;

&lt;p&gt;1. Leader is on broker 1, HW=X, LEO=Y, Y &amp;gt; X&lt;br/&gt;
2. A fetch request from follower goes to broker 1 to fetch from offset Z. Assume X &amp;lt; Z &amp;lt; Y.&lt;br/&gt;
3. Broker 1 proceeds with fetch request and enters Log.read()&lt;br/&gt;
4. Leader migration occurs, log on broker 1 got truncated to X.&lt;/p&gt;

&lt;p&gt;In this case, because the FetchRequest has passed the leader check before leader migration, no NotLeaderForPartitionException would be thrown. Also because the read does not grab any lock, the log truncation might occur before the actual message search occur.&lt;/p&gt;</comment>
                            <comment id="14948073" author="junrao" created="Thu, 8 Oct 2015 04:59:34 +0000"  >&lt;p&gt;Issue resolved by pull request 204&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/204&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/204&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14948074" author="githubbot" created="Thu, 8 Oct 2015 04:59:36 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/204&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/204&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14948076" author="junrao" created="Thu, 8 Oct 2015 05:01:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=H%C3%A5kon+Hitland&quot; class=&quot;user-hover&quot; rel=&quot;H&#229;kon Hitland&quot;&gt;H&#229;kon Hitland&lt;/a&gt;, the fix is committed in trunk. Do you think you can test this out in a test environment?&lt;/p&gt;</comment>
                            <comment id="14952113" author="cpsoman" created="Sun, 11 Oct 2015 00:00:30 +0000"  >&lt;p&gt;We&apos;re hitting this issue quite often (every 15-20 mins) and is a problem since its eating up the already scarce disk / Network resource. At the moment we&apos;re running 0.8.2.0. Is there any plan to backport this patch given the severity of the issue ?&lt;/p&gt;</comment>
                            <comment id="14952120" author="ewencp" created="Sun, 11 Oct 2015 00:49:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cpsoman&quot; class=&quot;user-hover&quot; rel=&quot;cpsoman&quot;&gt;cpsoman&lt;/a&gt; Interesting, the only other org I had heard this affecting was seeing it at approximately the same frequency as the original report &amp;#8211; maybe once or twice a week. It seemed random, so the events could sometimes clump together (e.g. see two in one day), but the average rate was pretty low. It was an annoyance rather than a serious issue.&lt;/p&gt;

&lt;p&gt;The patch does seem to apply trivially to 0.8.2.2.&lt;/p&gt;</comment>
                            <comment id="14952127" author="cpsoman" created="Sun, 11 Oct 2015 01:47:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt; I&apos;m attaching the screenshot of Max lag observed for the different brokers which describes the behaviour. &lt;/p&gt;

&lt;p&gt;Also here&apos;s the pertinent log:&lt;/p&gt;

&lt;p&gt;========&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-10-10 22:17:17,759&amp;#93;&lt;/span&gt; 3171793337 &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-request-handler-0&amp;#93;&lt;/span&gt; WARN  kafka.server.ReplicaManager  - &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 70&amp;#93;&lt;/span&gt;: Error when processing fetch request for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;...topic...,62&amp;#93;&lt;/span&gt; offset 91963211 from follower with correlation id 176614372. Possible cause: Request for offset 91963211 but we only have log segments in the range 55923986 to 91963210.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-10-10 22:17:17,759&amp;#93;&lt;/span&gt; 3171793337 &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-request-handler-4&amp;#93;&lt;/span&gt; WARN  kafka.server.ReplicaManager  - &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 70&amp;#93;&lt;/span&gt;: Error when processing fetch request for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;...topic...,62&amp;#93;&lt;/span&gt; offset 91963211 from follower with correlation id 152788081. Possible cause: Request for offset 91963211 but we only have log segments in the range 55923986 to 91963210.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-10-10 22:17:20,256&amp;#93;&lt;/span&gt; 3171795834 &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-scheduler-4&amp;#93;&lt;/span&gt; INFO  kafka.cluster.Partition  - Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;...topic...,62&amp;#93;&lt;/span&gt; on broker 70: Shrinking ISR for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;hp.event.user.driver_app.experiment,62&amp;#93;&lt;/span&gt; from 70,69,71 to 70&lt;br/&gt;
========&lt;/p&gt;</comment>
                            <comment id="14952128" author="cpsoman" created="Sun, 11 Oct 2015 01:50:05 +0000"  >&lt;p&gt;Btw, this doesn&apos;t happen everywhere but is definitely seen in our biggest cluster (with way more partitions / node). Maybe it has something to do with scale ?&lt;/p&gt;</comment>
                            <comment id="14952600" author="becket_qin" created="Mon, 12 Oct 2015 04:15:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cpsoman&quot; class=&quot;user-hover&quot; rel=&quot;cpsoman&quot;&gt;cpsoman&lt;/a&gt; I think the likelihood of the issue is related to the scale as you observed, because there would be potentially more threads trying to read/write from the same log segment.&lt;/p&gt;

&lt;p&gt;It looks there are a few other patches on the files touched by this patch since 0.8.2.0. However, I checked the code of 0.8.2.0, it seems the code blocks related to this patch are still the same as the latest trunk. So you should be able to patch 0.8.2.0 easily although the patch itself may not apply.&lt;/p&gt;</comment>
                            <comment id="14954511" author="ewencp" created="Tue, 13 Oct 2015 06:56:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cpsoman&quot; class=&quot;user-hover&quot; rel=&quot;cpsoman&quot;&gt;cpsoman&lt;/a&gt; Beyond applying to 0.8.2.0 with the patch, any reason not to update to 0.8.2.2 and apply the patch on top of that, where it definitely applies cleanly? It looks like 8 patches, and some of the patches on top of 0.8.2.0 are likely to be useful if you might have a large number of partitions or use snappy compression, among other key fixes. Maybe you&apos;re not hitting any of the critical fixes in those releases, but since they&apos;re low risk maybe catching up with the latest release and only having a minor patch would simplify things?&lt;/p&gt;</comment>
                            <comment id="14957645" author="cpsoman" created="Wed, 14 Oct 2015 20:05:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt; Totally agree. Its just that the current 0.8.2.0 version we&apos;re using : has been &quot;tampered&quot; with. I see a lot of commits from our previous team here and I need to be careful not to break anything. At the moment, I&apos;ve applied the patch on top of our current version and tested in staging. I&apos;ll be rolling it out on our biggest cluster soon to validate whether it works.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310060">
                    <name>Container</name>
                                            <outwardlinks description="contains">
                                        <issuelink>
            <issuekey id="12903095">KAFKA-2621</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12766003" name="Screen Shot 2015-10-10 at 6.54.44 PM.png" size="706296" author="cpsoman" created="Sun, 11 Oct 2015 01:47:22 +0000"/>
                            <attachment id="12752473" name="kafka_log.txt" size="14424" author="hakon" created="Wed, 26 Aug 2015 13:04:34 +0000"/>
                            <attachment id="12754651" name="kafka_log_trace.txt" size="24940" author="hakon" created="Tue, 8 Sep 2015 14:47:29 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 5 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2jefb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>