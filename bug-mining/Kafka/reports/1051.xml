<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:54:10 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-725] Broker Exception: Attempt to read with a maximum offset less than start offset</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-725</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;I have a simple consumer that&apos;s reading from a single topic/partition pair. Running it seems to trigger these messages on the broker periodically:&lt;/p&gt;

&lt;p&gt;2013/01/22 23:04:54.936 ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaApis&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-request-handler-4&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; []  &lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaApi-466&amp;#93;&lt;/span&gt; error when processing request (MyTopic,4,7951732,2097152)&lt;br/&gt;
java.lang.IllegalArgumentException: Attempt to read with a maximum offset (7951715) less than the start offset (7951732).&lt;br/&gt;
        at kafka.log.LogSegment.read(LogSegment.scala:105)&lt;br/&gt;
        at kafka.log.Log.read(Log.scala:390)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:372)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:330)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:326)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:326)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$maybeUnblockDelayedFetchRequests$2.apply(KafkaApis.scala:165)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$maybeUnblockDelayedFetchRequests$2.apply(KafkaApis.scala:164)&lt;br/&gt;
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)&lt;br/&gt;
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)&lt;br/&gt;
        at kafka.server.KafkaApis.maybeUnblockDelayedFetchRequests(KafkaApis.scala:164)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$handleProducerRequest$2.apply(KafkaApis.scala:186)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$handleProducerRequest$2.apply(KafkaApis.scala:185)&lt;br/&gt;
        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)&lt;br/&gt;
        at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:185)&lt;br/&gt;
        at kafka.server.KafkaApis.handle(KafkaApis.scala:58)&lt;br/&gt;
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/p&gt;

&lt;p&gt;When I shut the consumer down, I don&apos;t see the exceptions anymore.&lt;/p&gt;

&lt;p&gt;This is the code that my consumer is running:&lt;br/&gt;
          while(true) {&lt;br/&gt;
            // we believe the consumer to be connected, so try and use it for a fetch request&lt;br/&gt;
            val request = new FetchRequestBuilder()&lt;br/&gt;
              .addFetch(topic, partition, nextOffset, fetchSize)&lt;br/&gt;
              .maxWait(Int.MaxValue)&lt;br/&gt;
              // TODO for super high-throughput, might be worth waiting for more bytes&lt;br/&gt;
              .minBytes(1)&lt;br/&gt;
              .build&lt;/p&gt;

&lt;p&gt;            debug(&quot;Fetching messages for stream %s and offset %s.&quot; format (streamPartition, nextOffset))&lt;br/&gt;
            val messages = connectedConsumer.fetch(request)&lt;br/&gt;
            debug(&quot;Fetch complete for stream %s and offset %s. Got messages: %s&quot; format (streamPartition, nextOffset, messages))&lt;br/&gt;
            if (messages.hasError) &lt;/p&gt;
{
              warn(&quot;Got error code from broker for %s: %s. Shutting down consumer to trigger a reconnect.&quot; format (streamPartition, messages.errorCode(topic, partition)))
              ErrorMapping.maybeThrowException(messages.errorCode(topic, partition))
            }
&lt;p&gt;            messages.messageSet(topic, partition).foreach(msg =&amp;gt; &lt;/p&gt;
{
              watchers.foreach(_.onMessagesReady(msg.offset.toString, msg.message.payload))
              nextOffset = msg.nextOffset
            }
&lt;p&gt;)&lt;br/&gt;
          }&lt;/p&gt;

&lt;p&gt;Any idea what might be causing this error?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12628820">KAFKA-725</key>
            <summary>Broker Exception: Attempt to read with a maximum offset less than start offset</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srdo">Stig Rohde D&#248;ssing</assignee>
                                    <reporter username="criccomini">Chris Riccomini</reporter>
                        <labels>
                    </labels>
                <created>Tue, 22 Jan 2013 23:16:57 +0000</created>
                <updated>Fri, 6 May 2016 12:59:09 +0000</updated>
                            <resolved>Fri, 6 May 2016 12:59:08 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.10.0.0</fixVersion>
                                    <component>log</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>16</watches>
                                                                                                                <comments>
                            <comment id="13560246" author="nehanarkhede" created="Wed, 23 Jan 2013 00:43:14 +0000"  >&lt;p&gt;This looks exactly like &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-698&quot; title=&quot;broker may expose uncommitted data to a consumer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-698&quot;&gt;&lt;del&gt;KAFKA-698&lt;/del&gt;&lt;/a&gt;. Chris, did you try with a Kafka cluster that includes a fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-698&quot; title=&quot;broker may expose uncommitted data to a consumer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-698&quot;&gt;&lt;del&gt;KAFKA-698&lt;/del&gt;&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="14235782" author="lokeshbirla" created="Fri, 5 Dec 2014 17:45:56 +0000"  >&lt;p&gt;Neha,&lt;/p&gt;

&lt;p&gt;I still see this issue in 0.8.1.1.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1806&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-1806&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14273861" author="diwakar" created="Mon, 12 Jan 2015 18:01:44 +0000"  >&lt;p&gt;Neha,&lt;/p&gt;

&lt;p&gt;we have 6 brokers and 131 partitions per topic(replication factor : 3 ) and recently updated to kafka_2.10-0.8.2.0 and facing similar issue causing lot of below errors.Due to this it seems like producers are unable to produce to kafka successfully.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2015-01-11 05:21:56.604-0700&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 2&amp;#93;&lt;/span&gt;: Error when processing fetch request for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;application-access,13&amp;#93;&lt;/span&gt; offset 42748276 from consumer with correlation id 4974. Possible cause: Attempt to read with a maximum offset (42748275) less than the start offset (42748276). &lt;/p&gt;


&lt;p&gt;Any solution available to fix this.&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Diwakar&lt;/p&gt;</comment>
                            <comment id="15111680" author="mauzhang" created="Fri, 22 Jan 2016 00:25:06 +0000"  >&lt;p&gt;Is anyone still looking at this issue ? We have run into this exception on a 4-node kafka_2.10-0.8.2.1 cluster where 4 producers produce data with throughput of 17k messages/s on each node.&lt;/p&gt;</comment>
                            <comment id="15111747" author="gwenshap" created="Fri, 22 Jan 2016 01:29:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt; has somewhat similar symptoms. Perhaps you are running into that? You can try applying the patch and checking if it fixes your issue.&lt;/p&gt;</comment>
                            <comment id="15111990" author="becket_qin" created="Fri, 22 Jan 2016 06:17:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gwenshap&quot; class=&quot;user-hover&quot; rel=&quot;gwenshap&quot;&gt;gwenshap&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mauzhang&quot; class=&quot;user-hover&quot; rel=&quot;mauzhang&quot;&gt;mauzhang&lt;/a&gt; Not sure if it is related to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt;. &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt; should only affect replica fetchers, and we never set maxOffset for replica fetchers. The error log here seems caused by a regular consumers trying to fetch beyond high watermark. But this should not affect producing.&lt;/p&gt;</comment>
                            <comment id="15149886" author="mauzhang" created="Wed, 17 Feb 2016 05:19:46 +0000"  >&lt;p&gt;I can reproduce this on 0.9.0.0. The error log is &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2016-01-28 16:12:32,840&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 1&amp;#93;&lt;/span&gt;: Error processing fetch operation on partition &lt;span class=&quot;error&quot;&gt;&amp;#91;ad-events,1&amp;#93;&lt;/span&gt; offset 75510318 (kafka.server.ReplicaManager)&lt;/p&gt;

&lt;p&gt;I also print the sent offset from producer &lt;/p&gt;

&lt;p&gt;time   partition offset &lt;br/&gt;
16:12:32.840   1   75510318&lt;/p&gt;

&lt;p&gt;It seems the offset is produced and consumed at the same time. &lt;/p&gt;</comment>
                            <comment id="15149970" author="mauzhang" created="Wed, 17 Feb 2016 06:22:14 +0000"  >&lt;p&gt;I can reproduce this on 0.9.0.0. The error log is &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2016-01-28 16:12:32,840&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 1&amp;#93;&lt;/span&gt;: Error processing fetch operation on partition &lt;span class=&quot;error&quot;&gt;&amp;#91;ad-events,1&amp;#93;&lt;/span&gt; offset 75510318 (kafka.server.ReplicaManager)&lt;/p&gt;

&lt;p&gt;I also print the sent offset from producer &lt;/p&gt;

&lt;p&gt;time   partition offset &lt;br/&gt;
16:12:32.840   1   75510318&lt;/p&gt;

&lt;p&gt;It seems the offset is produced and consumed at the same time. &lt;/p&gt;</comment>
                            <comment id="15221673" author="srdo" created="Fri, 1 Apr 2016 13:14:26 +0000"  >&lt;p&gt;We&apos;re seeing this on 8.2.2. I&apos;m not sure Log/LogSegment handles the high watermark as gracefully as they maybe could.&lt;/p&gt;

&lt;p&gt;My guess at how it&apos;s happening:&lt;br/&gt;
Assume a replica set of at least 2.&lt;br/&gt;
A consumer (in our case the Storm KafkaSpout) reads up to the end of the committed log, say up to message 5. &lt;br/&gt;
The leader for the relevant partition then receives one or more messages (6). &lt;br/&gt;
Before the new message(s) are replicated, the consumer increments its offset and fetches (from 6). &lt;br/&gt;
The leader receives the fetch, sets the maxOffset for read to the high watermark (5), and compares the end of the log to the requested offset (see &lt;a href=&quot;https://github.com/apache/kafka/blob/0.9.0.1/core/src/main/scala/kafka/log/Log.scala#L482&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/0.9.0.1/core/src/main/scala/kafka/log/Log.scala#L482&lt;/a&gt;). This check passes because the end of the log is at 6.&lt;br/&gt;
When the read on LogSegment is reached, it will error out when the maxOffset is smaller than the start offset, which causes this error log. &lt;a href=&quot;https://github.com/apache/kafka/blob/0.9.0.1/core/src/main/scala/kafka/log/LogSegment.scala#L146&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/0.9.0.1/core/src/main/scala/kafka/log/LogSegment.scala#L146&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Maybe the check in Log should include whether the maxOffset is larger than offset as well?&lt;/p&gt;

&lt;p&gt;Edit: Or maybe Kafka should allow the fetch to wait until the requested offset is available, similar to how minBytes can be waited for?&lt;/p&gt;</comment>
                            <comment id="15222909" author="githubbot" created="Sat, 2 Apr 2016 14:56:11 +0000"  >&lt;p&gt;GitHub user srdo opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1178&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;: Change behavior of Log/LogSegment when attempting read on an offset that&apos;s above high watermark.&lt;/p&gt;

&lt;p&gt;    This should make Log.read act the same when startOffset is larger than maxOffset as it would if startOffset was larger than logEndOffset. The current behavior can result in an IllegalArgumentException from LogSegment if a consumer attempts to fetch an offset above the high watermark which is present in the leader&apos;s log. It seems more correct if Log.read presents the view of the log to consumers as if it simply ended at maxOffset (high watermark).&lt;/p&gt;

&lt;p&gt;    I&apos;ve tried to describe an example scenario of this happening here &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725?focusedCommentId=15221673&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221673&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-725?focusedCommentId=15221673&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221673&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;m not sure I understand why ReplicaManager sets maxOffset to the high watermark, and not high watermark + 1. Isn&apos;t the high watermark the last committed message, and readable by consumers?&lt;/p&gt;

&lt;p&gt;    Tests passed for me locally on second try, seems like it just hit a flaky test.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/srdo/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/srdo/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1178.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1178.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1178&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 5c7d583ec1af0892e9fadc4bbdcbeaa94390524e&lt;br/&gt;
Author: Stig Rohde D&#248;ssing &amp;lt;sdo@it-minds.dk&amp;gt;&lt;br/&gt;
Date:   2016-04-02T12:20:50Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;: Throw OffsetOutOfRangeException when reading from Log with maxOffset &amp;gt; startOffset&lt;/p&gt;

&lt;p&gt;commit 5546433916d49b30b0869964a779e1af189be0ce&lt;br/&gt;
Author: Stig Rohde D&#248;ssing &amp;lt;sdo@it-minds.dk&amp;gt;&lt;br/&gt;
Date:   2016-04-02T13:37:22Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;: Return empty message set if reading from Log with maxOffset+1 == startOffset&lt;/p&gt;

&lt;p&gt;commit 5808b31828d3703729569476217880971bf279af&lt;br/&gt;
Author: Stig Rohde D&#248;ssing &amp;lt;sdo@it-minds.dk&amp;gt;&lt;br/&gt;
Date:   2016-04-02T14:09:40Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;: Return only message offset when reading one beyond maxOffset&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15222995" author="srdo" created="Sat, 2 Apr 2016 18:27:45 +0000"  >&lt;p&gt;Nevermind this, I misunderstood the high watermark to be the last committed offset. It seems to be the last committed offset + 1. There&apos;s still a minor issue if a consumer requests an offset that&apos;s in the log but above the high watermark, which the PR should fix.&lt;/p&gt;</comment>
                            <comment id="15223456" author="githubbot" created="Sun, 3 Apr 2016 18:47:56 +0000"  >&lt;p&gt;Github user srdo closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1178&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15223457" author="githubbot" created="Sun, 3 Apr 2016 18:47:58 +0000"  >&lt;p&gt;GitHub user srdo reopened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1178&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;: Change behavior of Log/LogSegment when attempting read on an offset that&apos;s above high watermark.&lt;/p&gt;

&lt;p&gt;    This should make Log.read act the same when startOffset is larger than maxOffset as it would if startOffset was larger than logEndOffset. The current behavior can result in an IllegalArgumentException from LogSegment if a consumer attempts to fetch an offset above the high watermark which is present in the leader&apos;s log. It seems more correct if Log.read presents the view of the log to consumers as if it simply ended at maxOffset (high watermark).&lt;/p&gt;

&lt;p&gt;    I&apos;ve tried to describe an example scenario of this happening here &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725?focusedCommentId=15221673&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221673&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-725?focusedCommentId=15221673&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221673&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;m not sure I understand why ReplicaManager sets maxOffset to the high watermark, and not high watermark + 1. Isn&apos;t the high watermark the last committed message, and readable by consumers?&lt;/p&gt;

&lt;p&gt;    Tests passed for me locally on second try, seems like it just hit a flaky test.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/srdo/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/srdo/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1178.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1178.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1178&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit c7bab99b77b71c73380d473facda1138799e42a6&lt;br/&gt;
Author: Stig Rohde D&#248;ssing &amp;lt;sdo@it-minds.dk&amp;gt;&lt;br/&gt;
Date:   2016-04-02T12:20:50Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;: Throw OffsetOutOfRangeException when reading from Log with maxOffset &amp;gt; startOffset&lt;/p&gt;

&lt;p&gt;commit 4f5b415651ec45d3040c22393d24293de4f2cfd0&lt;br/&gt;
Author: Stig Rohde D&#248;ssing &amp;lt;sdo@it-minds.dk&amp;gt;&lt;br/&gt;
Date:   2016-04-02T23:29:02Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;: Put check for HW from consumer in ReplicaManager.readFromLocalLog instead of Log.read&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15231622" author="guozhang" created="Fri, 8 Apr 2016 04:37:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Srdo&quot; class=&quot;user-hover&quot; rel=&quot;srdo&quot;&gt;Srdo&lt;/a&gt; I think your reasoning still makes sense, because a producer usually produces in batches. So following your case, after receiving a batch of 5 messages, the log end offset could then be 5 + 5 + 1 = 11, but before it was replicated to follower the high watermark is still 5 + 1 = 6. Hence this check will fail.&lt;/p&gt;</comment>
                            <comment id="15231850" author="muditcse" created="Fri, 8 Apr 2016 08:23:59 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;We are seeing below exception in our kafka logs on one of the broker id.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2016-04-08 07:59:58,486&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 3&amp;#93;&lt;/span&gt;: Error processing fetch operation on partition &lt;span class=&quot;error&quot;&gt;&amp;#91;subscribed_product_logs,17&amp;#93;&lt;/span&gt; offset 483780 (kafka.server.ReplicaManager)&lt;br/&gt;
java.lang.IllegalStateException: Failed to read complete buffer for targetOffset 483780 startPosition 958861516 in /kafka/kafka-logs/subscribed_product_logs-17/00000000000000378389.log&lt;br/&gt;
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:133)&lt;br/&gt;
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:105)&lt;br/&gt;
        at kafka.log.LogSegment.read(LogSegment.scala:126)&lt;br/&gt;
        at kafka.log.Log.read(Log.scala:506)&lt;br/&gt;
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:536)&lt;br/&gt;
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:507)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;br/&gt;
        at kafka.server.ReplicaManager.readFromLocalLog(ReplicaManager.scala:507)&lt;br/&gt;
        at kafka.server.ReplicaManager.fetchMessages(ReplicaManager.scala:462)&lt;br/&gt;
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:431)&lt;br/&gt;
        at kafka.server.KafkaApis.handle(KafkaApis.scala:69)&lt;br/&gt;
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:60)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2016-04-08 07:59:58,486&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica Manager on Broker 3&amp;#93;&lt;/span&gt;: Error processing fetch operation on partition &lt;span class=&quot;error&quot;&gt;&amp;#91;subscribed_product_logs,8&amp;#93;&lt;/span&gt; offset 592637 (kafka.server.ReplicaManager)&lt;br/&gt;
java.lang.IllegalStateException: Failed to read complete buffer for targetOffset 592637 startPosition 780606424 in /kafka/kafka-logs/subscribed_product_logs-8/00000000000000505731.log&lt;br/&gt;
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:133)&lt;br/&gt;
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:105)&lt;br/&gt;
        at kafka.log.LogSegment.read(LogSegment.scala:126)&lt;br/&gt;
        at kafka.log.Log.read(Log.scala:506)&lt;br/&gt;
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:536)&lt;br/&gt;
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:507)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;br/&gt;
        at kafka.server.ReplicaManager.readFromLocalLog(ReplicaManager.scala:507)&lt;br/&gt;
        at kafka.server.ReplicaManager.fetchMessages(ReplicaManager.scala:462)&lt;br/&gt;
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:431)&lt;br/&gt;
        at kafka.server.KafkaApis.handle(KafkaApis.scala:69)&lt;br/&gt;
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:60)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;seems this is related to same bug.Any update on this?&lt;/p&gt;</comment>
                            <comment id="15232469" author="githubbot" created="Fri, 8 Apr 2016 16:45:23 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1178&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15232470" author="guozhang" created="Fri, 8 Apr 2016 16:45:28 +0000"  >&lt;p&gt;Issue resolved by pull request 1178&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/1178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1178&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15232503" author="srdo" created="Fri, 8 Apr 2016 17:14:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; Makes sense. I&apos;m wondering if it would be better for the request to be put into purgatory then? If the request hits inbetween the high watermark and the end of the log, we can reasonably expect that offset to be readable shortly, while if the client gets a general OffsetOutOfRangeException, it might make more sense for the client to restart at either end of the log.&lt;/p&gt;

&lt;p&gt;What I mean is basically, what does proper handling of this exception look like on the client-side now?&lt;/p&gt;</comment>
                            <comment id="15232514" author="guozhang" created="Fri, 8 Apr 2016 17:22:39 +0000"  >&lt;p&gt;Usually client code should gracefully handle OffsetOutOfRangeException by requesting the current log end offset and retry fetching. Note that this handling would just make this exception potentially being triggered multiple times until the data is replicated complete and HW advanced; in most cases this is fine as it is only transient. But I agree that a more ideal solution is to park the request in purgatory so that we can reduce round trips retrying in this case as well.&lt;/p&gt;</comment>
                            <comment id="15235016" author="srdo" created="Mon, 11 Apr 2016 12:39:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; Okay, returning the error should be fine then. I can&apos;t really think of a case where this error can happen if the client is well behaved and unclean leader election is turned off. If the client never increments its offset by more than 1 past the most recently consumed message, it shouldn&apos;t be possible for it to request an offset higher than the high watermark, since the most recent offset it can have consumed is HW - 1.&lt;/p&gt;</comment>
                            <comment id="15270789" author="junrao" created="Wed, 4 May 2016 15:10:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Srdo&quot; class=&quot;user-hover&quot; rel=&quot;srdo&quot;&gt;Srdo&lt;/a&gt;, thanks for the patch. It&apos;s still not very clear to me how a consumer can trigger the IllegalArgumentException even without the patch. The broker only returns messages up to the high watermark (HW) to the consumer. So, the offset from a consumer should always be &amp;lt;= HW. The problem can only occur if a consumer uses an offset &amp;gt; HW, but &amp;lt;= the log end offset, which should never happen in a normal consumer.&lt;/p&gt;</comment>
                            <comment id="15271051" author="becket_qin" created="Wed, 4 May 2016 17:39:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; I asked this question in the RB. It seems the consumer in this case is not the Java consumer. Theoretically a java consumer can only fetch beyond HW when unclean leader election occurs.&lt;/p&gt;</comment>
                            <comment id="15271854" author="junrao" created="Thu, 5 May 2016 04:06:46 +0000"  >&lt;p&gt;Reopen this jira since the fix exposes a new issue. When the leader switches (say due to leader balancing), the new leader&apos;s HW can actually be smaller than the previous leader&apos;s HW since HW is propagated asynchronously. The new leader&apos;s log end offset is &amp;gt;= than the previous leader&apos;s HW and eventually its HW will move to its log end offset. Before that happens, if a consumer fetches data using previous leader&apos;s HW, with the patch, the consumer will get OffsetOutOfRangeException and thus has to reset the offset, which is bad. Without the patch, the consumer will get an empty response instead.&lt;/p&gt;

&lt;p&gt;So, it seems that we should revert the changes in this patch.&lt;/p&gt;</comment>
                            <comment id="15272490" author="guozhang" created="Thu, 5 May 2016 15:09:29 +0000"  >&lt;p&gt;Jun, thanks for pointing it out. While reverting this change, I&apos;m thinking we should change &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogSegment.scala#L147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogSegment.scala#L147&lt;/a&gt; to return empty response instead of throw exceptions as well. What do you think?&lt;/p&gt;</comment>
                            <comment id="15272591" author="junrao" created="Thu, 5 May 2016 16:37:08 +0000"  >&lt;p&gt;Yes, I agree. If the requested offset is &amp;gt; MaxOffset, it&apos;s better to just return an empty response instead of throwing an IllegalStateException. We can add a comment on why we want to do that. Also, while you are there, could you fix the following comment above LogSegment.read()? maxPosition is not optional.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;@param maxPosition An optional maximum position in the log segment that should be exposed for read.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15272872" author="githubbot" created="Thu, 5 May 2016 19:04:26 +0000"  >&lt;p&gt;GitHub user guozhangwang opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1327&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1327&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    HOTFIX: follow-up on &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt; to remove the check and return empty response instead of throw exceptions&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/guozhangwang/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/guozhangwang/kafka&lt;/a&gt; K725r&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1327.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1327.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1327&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 9fdfe9ce1a0242f78775cbc5e24fc4a059a07296&lt;br/&gt;
Author: Guozhang Wang &amp;lt;wangguoz@gmail.com&amp;gt;&lt;br/&gt;
Date:   2016-05-05T19:03:30Z&lt;/p&gt;

&lt;p&gt;    follow-up on &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-725&quot; title=&quot;Broker Exception: Attempt to read with a maximum offset less than start offset&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-725&quot;&gt;&lt;del&gt;KAFKA-725&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15272948" author="srdo" created="Thu, 5 May 2016 19:54:06 +0000"  >&lt;p&gt;Thanks for fixing this. The scenario Jun describes is probably a better match for the times we saw the exception originally. We&apos;re using Storm&apos;s storm-kafka component to consume, and it shouldn&apos;t go beyond the HW if the HW never moves backwards. It seems plausible that the logs coincided with leader failover for us.&lt;/p&gt;</comment>
                            <comment id="15273337" author="githubbot" created="Thu, 5 May 2016 23:55:51 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1327&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1327&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15274000" author="ijuma" created="Fri, 6 May 2016 12:59:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&apos;s PR was merged to trunk and 0.10.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12628852">KAFKA-727</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12933839">FLINK-3288</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>308298</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 28 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1b1l3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>272555</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>guozhang</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>