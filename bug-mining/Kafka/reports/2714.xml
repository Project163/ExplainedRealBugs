<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:24:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-9891] Invalid state store content after task migration with exactly_once and standby replicas</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-9891</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We have a simple command id deduplication mechanism (very similar to the one from Kafka Streams examples) based on Kafka Streams State Stores. It stores command ids from the past hour in &lt;em&gt;persistentWindowStore&lt;/em&gt;. We encountered a problem with the store if there&apos;s an exception thrown later in that topology.&lt;br/&gt;
 We run 3 nodes using docker, each with multiple threads set for this particular Streams Application.&lt;/p&gt;

&lt;p&gt;The business flow is as follows (performed within a single subtopology):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&#160;a valid command is sent with command id (&lt;em&gt;mnl_cmd_31bad7e5-35e7-490e-89f0-616fe967111f&lt;/em&gt;). NODE 1 is running an active task 1_2.&#160;First node in the topology analyses if this is a duplicate by checking in the state store (&lt;em&gt;COMMAND_ID_STORE&lt;/em&gt;), if not puts the command id in the state store and processes the command properly.&lt;/li&gt;
	&lt;li&gt;an invalid command is sent with the same key but new command id (&lt;em&gt;mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc&lt;/em&gt;). Again, check for the duplicated command id is performed, it&apos;s not a duplicate, command id is put into the state store. Next node in the topology throws an exception which causes an error on NODE 1 for task 1_2. As a result, transaction is aborted, offsets are not committed. I double checked for the changelog topic - relevant messages are not committed. Therefore, the changelog topic contains only the first command id&#160;&lt;em&gt;mnl_cmd_31bad7e5-35e7-490e-89f0-616fe967111f,&lt;/em&gt;&#160;and not the one which caused a failure.&lt;/li&gt;
	&lt;li&gt;in the meantime a standby task 1_2 running on NODE 3 replicated&#160;&lt;em&gt;mnl_cmd_31bad7e5-35e7-490e-89f0-616fe967111f&lt;/em&gt; command id into a local&#160;&lt;em&gt;COMMAND_ID_STORE&lt;/em&gt;&lt;/li&gt;
	&lt;li&gt;standby task 1_2 on NODE 3 Thread-2 takes over the task as an active one. It checks if this command id is a duplicate - no, it isn&apos;t - tries to process the faulty command and throws an exception. Again, transaction aborted, all looks fine.&lt;/li&gt;
	&lt;li&gt;NODE 3 Thread-1 takes over. It checks for the duplicate. To our surprise, &lt;b&gt;it is a duplicate!&lt;/b&gt;&#160;Even though the transaction has been aborted and the changelog doesn&apos;t contain this command id:&#160;&lt;em&gt;mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;After digging into the Streams logs and some discussion on (&lt;a href=&quot;https://stackoverflow.com/questions/61247789/invalid-state-store-content-after-aborted-transaction-with-exactly-once-and-stan&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Stack Overflow&lt;/a&gt;) we concluded it has something to do with checkpoint files. Here are the detailed logs relevant to checkpoint files.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
NODE_3 2020-04-15 21:06:14.470 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Checkpointable offsets read from checkpoint: {}
NODE_3 2020-04-15 21:06:19.413 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXProcessor-COMMAND_ID_STORE-changelog at checkpoint &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
NODE_3 2020-04-15 21:06:28.470 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Checkpointable offsets read from checkpoint: {}
NODE_3 2020-04-15 21:06:29.634 TRACE 1 --- [-StreamThread-2] o.a.k.s.s.internals.OffsetCheckpoint : Writing tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp
NODE_3 2020-04-15 21:06:29.640 TRACE 1 --- [-StreamThread-2] o.a.k.s.s.internals.OffsetCheckpoint : Swapping tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint
NODE_3 2020-04-15 21:11:15.909 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Writing tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp
NODE_3 2020-04-15 21:11:15.912 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Swapping tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp /tmp/kafka-streams/XXXXProcessor/1_2/.checkpointNODE_1 log1:2020-04-15 21:11:33.942 DEBUG 1 --- [-StreamThread-2] c.g.f.c.s.validation.CommandIdValidator : CommandId: mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc is not a duplicate.NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Writing tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp
NODE_3 2020-04-15 21:11:47.233 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Swapping tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint
NODE_3 2020-04-15 21:11:49.075 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXProcessor-COMMAND_ID_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:11:49.436 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.StoreChangelogReader : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Found checkpoint 1 from changelog XXXXProcessor-COMMAND_ID_STORE-changelog-2 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; store COMMAND_ID_STORE.NODE_3 2020-04-15 21:11:52.023 DEBUG 1 --- [-StreamThread-2] c.g.f.c.s.validation.CommandIdValidator : CommandId: mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc is not a duplicate.
NODE_3 2020-04-15 21:11:53.683 ERROR 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Failed to process stream task 1_2 due to the following error: java.lang.RuntimeExceptionNODE_3 2020-04-15 21:12:05.346 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXProcessor-COMMAND_ID_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:12:05.562 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.StoreChangelogReader : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] Found checkpoint 1 from changelog XXXXProcessor-COMMAND_ID_STORE-changelog-2 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; store COMMAND_ID_STORE.NODE_3 2020-04-15 21:12:06.424 WARN 1 --- [-StreamThread-1] c.g.f.c.s.validation.CommandIdValidator : Command duplicate detected. Command id mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It seems that on NODE_3 there&apos;s a standby task 1_2 running on T-2, it replicates a first valid command, thus creating a checkpoint file. Invalid command causes an error on NODE_1, then NODE_3 T-2 takes over the task. It finds the checkpoint file (which is fine), and starts to process the invalid command. It crashes, same node T-1 takes over, finds the checkpoint file &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/warning.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, thinks state store is clean (apparently it&apos;s not as it contains state modified by T-2) and finds a duplicated command id.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;We use Java 11, kafka clients 4.1 and spring-kafka 2.4.5. We rolled back for a moment to kafka clients 2.3.1 and the problem persists.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;We performed more tests with configuration changes and after changing `num.standby.replicas = 1` to `num.standby.replicas = 0` the problem disappeared. It is also resolved when changing the store to &lt;em&gt;inMemoryWindowStore.&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;In the SO question you can find the relevant java code. I don&apos;t have a sample project to share at the moment which replicates the problem, but it is easily repeatable in our project.&lt;/p&gt;

&lt;p&gt;Such behaviour can have serious implications on business logic, in our case accidentally skipped messages without properly processing them.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13299686">KAFKA-9891</key>
            <summary>Invalid state store content after task migration with exactly_once and standby replicas</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mjsax">Matthias J. Sax</assignee>
                                    <reporter username="mateuszjadczyk">Mateusz Jadczyk</reporter>
                        <labels>
                    </labels>
                <created>Mon, 20 Apr 2020 12:47:33 +0000</created>
                <updated>Fri, 19 Jun 2020 22:16:14 +0000</updated>
                            <resolved>Fri, 19 Jun 2020 20:01:21 +0000</resolved>
                                    <version>2.3.1</version>
                    <version>2.4.1</version>
                    <version>2.5.0</version>
                                    <fixVersion>2.5.1</fixVersion>
                    <fixVersion>2.6.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="17088044" author="bchen225242" created="Mon, 20 Apr 2020 19:59:41 +0000"  >&lt;p&gt;Thanks for the report, will take a look this week.&lt;/p&gt;</comment>
                            <comment id="17092295" author="bchen225242" created="Sat, 25 Apr 2020 16:56:16 +0000"  >&lt;p&gt;I made a try to replicate the scenario with integration test, but 2.4 didn&apos;t fail. Could you take a look and see if my implementation replicates what you suggested?&lt;/p&gt;</comment>
                            <comment id="17094313" author="mateuszjadczyk" created="Tue, 28 Apr 2020 08:59:12 +0000"  >&lt;p&gt;Thanks for looking into it. I looked into the test (see comments in the PR), played a bit with it and enabled some more logging and I may have more insights.&lt;br/&gt;
First of all, are you sure your test uses 2.4 clients? We used 2.4.1 clients and this broker image&#160;confluentinc/cp-zookeeper:5.3.1. I see that you use&#160;StoreQueryParameters in the code which is not available in Streams 2.4.1 and also&#160;ProcessorStateManager implementation changed a lot.&lt;/p&gt;

&lt;p&gt;I also revised the logs I included in the ticket and may have a new finding. The flow is:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;NODE 1 T-2 has active task 1_2.&lt;/li&gt;
	&lt;li&gt;NODE 3 &lt;b&gt;T-1&lt;/b&gt; has standy task 1_2.&lt;/li&gt;
	&lt;li&gt;NODE 1 T-2 crashes&lt;/li&gt;
	&lt;li&gt;NODE 3 &lt;b&gt;T-2&lt;/b&gt; takes over, T-1 (which had a standby task) is assigned other task, standby task 1_2 is revoked.&lt;/li&gt;
	&lt;li&gt;NODE 2 T1 has standby task 1_2&lt;/li&gt;
	&lt;li&gt;NODE 3 T-2 crashes&lt;/li&gt;
	&lt;li&gt;NODE 3 T-1 takes over&lt;/li&gt;
	&lt;li&gt;NODE2 T-1 standby task 1_2 is revoked.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The crucial takeaway here is that if we focus on strictly NODE 3, we can see that the task 1_2 was not taken over by a thread T-1 with standby task, but rather T-2. I guess that&apos;s how this version of TaskAssignor works. Digging deeper I checked what exactly happened when standby task was revoked on T-1, and active task was starting on T-2.&lt;br/&gt;
So this is T-1 having standby task revoked:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;NODE_3 2020-04-15 21:11:47.024  INFO 1 --- [-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] State transition from RUNNING to PARTITIONS_ASSIGNED
NODE_3 2020-04-15 21:11:47.027 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.AssignedStandbyTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] Closing revoked standby tasks {1_2=[mnl.xxxx.command-2, xxxx.command-2]}
NODE_3 2020-04-15 21:11:47.027 DEBUG 1 --- [-StreamThread-1] o.a.k.s.processor.internals.StandbyTask  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing
NODE_3 2020-04-15 21:11:47.027 TRACE 1 --- [-StreamThread-1] o.a.k.s.processor.internals.StandbyTask  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Committing
NODE_3 2020-04-15 21:11:47.027 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Flushing all stores registered in the state manager
NODE_3 2020-04-15 21:11:47.032 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Flushing store COMMAND_ID_STORE
NODE_3 2020-04-15 21:11:47.194 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Flushing store XXXX_STATE_STORE
NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Checkpointable offsets updated with restored offsets: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Checkpointable offsets updated with active acked offsets: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Writing checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:47.296 TRACE 1 --- [-StreamThread-1] o.a.k.s.processor.internals.StandbyTask  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing state manager
NODE_3 2020-04-15 21:11:47.296 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing its state manager and all the registered state stores
NODE_3 2020-04-15 21:11:47.298 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing storage engine COMMAND_ID_STORE
NODE_3 2020-04-15 21:11:47.388 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing storage engine XXXX_STATE_STORE
NODE_3 2020-04-15 21:11:47.455 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.internals.StateDirectory       : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] Released state dir lock for task 1_2&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And this is T-2 with active task starting:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;NODE_3 2020-04-15 21:11:46.556 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.TaskManager  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] New active tasks to be created: {1_2=[mnl.xxxx.command-2, xxxx.command-2]}
NODE_3 2020-04-15 21:11:46.697 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Checkpointable offsets read from checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:46.703 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Created state store manager for task 1_2
NODE_3 2020-04-15 21:11:46.950  INFO 1 --- [-StreamThread-2] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Creating producer client for task 1_2
        client.id = XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer
NODE_3 2020-04-15 21:11:47.137  INFO 1 --- [-StreamThread-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Instantiated a transactional producer.
NODE_3 2020-04-15 21:11:47.389  INFO 1 --- [-StreamThread-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
NODE_3 2020-04-15 21:11:47.403  INFO 1 --- [-StreamThread-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Overriding the default acks to all since idempotence is enabled.
NODE_3 2020-04-15 21:11:47.472 DEBUG 1 --- [-2-1_2-producer] o.a.k.clients.producer.internals.Sender  : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Starting Kafka producer I/O thread.
NODE_3 2020-04-15 21:11:47.599  INFO 1 --- [-2-1_2-producer] org.apache.kafka.clients.Metadata        : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Cluster ID: UoTd5Q9HQsKwSUpY3eABQA
NODE_3 2020-04-15 21:11:47.670 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Register global stores []
NODE_3 2020-04-15 21:11:47.742 DEBUG 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Transition from state UNINITIALIZED to INITIALIZING
NODE_3 2020-04-15 21:11:47.742  INFO 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] ProducerId set to -1 with epoch -1
NODE_3 2020-04-15 21:11:47.742 DEBUG 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Enqueuing transactional request InitProducerIdRequestData(transactionalId=&apos;XXXXCommandProcessor-1_2&apos;, transactionTimeoutMs=60000)
NODE_3 2020-04-15 21:11:47.744 DEBUG 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Enqueuing transactional request FindCoordinatorRequestData(key=&apos;XXXXCommandProcessor-1_2&apos;, keyType=1)
NODE_3 2020-04-15 21:11:47.790 DEBUG 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Enqueuing transactional request InitProducerIdRequestData(transactionalId=&apos;XXXXCommandProcessor-1_2&apos;, transactionTimeoutMs=60000)
NODE_3 2020-04-15 21:11:47.855 DEBUG 1 --- [-2-1_2-producer] o.a.k.clients.producer.internals.Sender  : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Sending transactional request FindCoordinatorRequestData(key=&apos;XXXXCommandProcessor-1_2&apos;, keyType=1) to node kafka-3:39092 (id: 3 rack: rack-a)
NODE_3 2020-04-15 21:11:48.000 DEBUG 1 --- [-2-1_2-producer] o.a.k.clients.producer.internals.Sender  : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Sending transactional request InitProducerIdRequestData(transactionalId=&apos;XXXXCommandProcessor-1_2&apos;, transactionTimeoutMs=60000) to node kafka-1:19092 (id: 1 rack: null)
NODE_3 2020-04-15 21:11:48.133  INFO 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] ProducerId set to 1014 with epoch 3
NODE_3 2020-04-15 21:11:48.146 DEBUG 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Transition from state INITIALIZING to READY
NODE_3 2020-04-15 21:11:48.148 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Created task 1_2 with assigned partitions [mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.150  INFO 1 --- [-StreamThread-2] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
NODE_3 2020-04-15 21:11:48.150 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.TaskManager  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Pausing all active task partitions until the underlying state stores are ready
NODE_3 2020-04-15 21:11:48.152 TRACE 1 --- [-StreamThread-2] o.a.k.s.processor.internals.TaskManager  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Pausing partitions: [zzzzzz_state-1, mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.152 DEBUG 1 --- [-StreamThread-2] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-consumer, groupId=XXXXCommandProcessor] Pausing partitions [zzzzzz_state-1, mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.153  INFO 1 --- [-StreamThread-2] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] partition assignment took 1599 ms.
NODE_3 2020-04-15 21:11:48.192 DEBUG 1 --- [-StreamThread-2] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-consumer, groupId=XXXXCommandProcessor] Fetching committed offsets for partitions: [mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.316 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Initializing stream tasks [1_2]
NODE_3 2020-04-15 21:11:48.326  INFO 1 --- [-StreamThread-2] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-consumer, groupId=XXXXCommandProcessor] Found no committed offset for partition mnl.xxxx.command-2
NODE_3 2020-04-15 21:11:48.365 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Updating store offset limit with {mnl.xxxx.command-2=0, xxxx.command-2=0}
NODE_3 2020-04-15 21:11:48.371 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] A committed timestamp was detected: setting the partition time of partition xxxx.command-2 to 1586985068274 in stream task 1_2
NODE_3 2020-04-15 21:11:48.371 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] No committed offset for partition mnl.xxxx.command-2, therefore no timestamp can be found for this partition
NODE_3 2020-04-15 21:11:48.393 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing state stores
NODE_3 2020-04-15 21:11:48.410 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.internals.StateDirectory       : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Acquired state dir lock for task 1_2
NODE_3 2020-04-15 21:11:48.410 TRACE 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing state stores
NODE_3 2020-04-15 21:11:48.410 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing store COMMAND_ID_STORE
NODE_3 2020-04-15 21:11:49.075 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Registering state store COMMAND_ID_STORE to its state manager
NODE_3 2020-04-15 21:11:49.075 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXCommandProcessor-COMMAND_ID_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:11:49.076 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Added restorer for changelog XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2
NODE_3 2020-04-15 21:11:49.076 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing store XXXX_STATE_STORE
NODE_3 2020-04-15 21:11:49.349 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Registering state store XXXX_STATE_STORE to its state manager
NODE_3 2020-04-15 21:11:49.349 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store XXXX_STATE_STORE from changelog topic XXXXCommandProcessor-XXXX_STATE_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:11:49.436 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Found checkpoint 1 from changelog XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2 for store COMMAND_ID_STORE.NODE_3 2020-04-15 21:11:50.082 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Stream task 1_2 cannot resume processing yet since some of its changelog partitions have not completed restoring: [XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2]NODE_3 2020-04-15 21:11:50.627 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Transitioning stream task 1_2 to running
NODE_3 2020-04-15 21:11:50.627 TRACE 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing processor nodes of the topology
NODE_3 2020-04-15 21:11:51.389 DEBUG 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Transition from state READY to IN_TRANSACTION
NODE_3 2020-04-15 21:11:51.399 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Stream task 1_2 completed restoration as all its changelog partitions [XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2] have been applied to restore state
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now compare timestamps:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;NODE_3 2020-04-15 21:11:46.697 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Checkpointable offsets read from checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}

NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Writing checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It seems that the standby task on T-1 hasn&apos;t finished shutdown yet, and active task on T-2 is already starting. If you look at the code corresponding to these 2 particular logs:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;log.trace(&quot;Checkpointable offsets read from checkpoint: {}&quot;, initialLoadedCheckpoints);

if (eosEnabled) {
    // with EOS enabled, there should never be a checkpoint file _during_ processing.
    // delete the checkpoint file after loading its stored offsets.
    checkpointFile.delete();
    checkpointFile = null;
}&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;log.trace(&quot;Writing checkpoint: {}&quot;, checkpointFileCache);
try {
    checkpointFile.write(checkpointFileCache);
} catch (final IOException e) {
    log.warn(&quot;Failed to write offset checkpoint file to [{}]&quot;, checkpointFile, e);
}&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Can it be a race condition, that a new active task reads the file, saves content in memory, then deletes the checkpoint file. 0.5 sec after that standby task when shutting down writes the checkpoint file, and eventually it&apos;s there although it shouldn&apos;t.&lt;br/&gt;
Is this checkpoint file locked in any way? It&apos;s also a bit misleading that there are no logs when this files is deleted, so it&apos;s hard to debug.&lt;/p&gt;

&lt;p&gt;I am aware it may be hard to reproduce in a test due to timing, but maybe with exactly the same topology and this strange rebalancing scenario it could happen. Or some kind of unit test?&lt;br/&gt;
I also noticed that the piece of code operating on state store and checkpoint files has been recently rewritten, but it&apos;s hard to tell if this scenario can still happen.&lt;/p&gt;

&lt;p&gt;Let me know if I can help any other way. I attached detailed logs showing the afore-mentioned failover scenario.&lt;/p&gt;</comment>
                            <comment id="17099743" author="mateuszjadczyk" created="Tue, 5 May 2020 10:00:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; Hi, is any follow-up planned for this?&lt;/p&gt;</comment>
                            <comment id="17100020" author="bchen225242" created="Tue, 5 May 2020 15:47:21 +0000"  >&lt;p&gt;We had some offline discussion, and plan to revise the integration test PR as well. We want to test out on trunk first, and then backport to 2.4 to ensure the regression doesn&apos;t carry over.&lt;/p&gt;</comment>
                            <comment id="17100021" author="bchen225242" created="Tue, 5 May 2020 15:50:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&#160;on the other hand, if you are willing to work on this ticket, feel free to take over, as I realize that you have better context of this problem in 2.4 than I do.&lt;/p&gt;</comment>
                            <comment id="17100135" author="mateuszjadczyk" created="Tue, 5 May 2020 17:51:59 +0000"  >&lt;p&gt;I don&apos;t think I have enough knowledge to propose a fix, I&apos;m afraid. One thing is debugging, another one making it work &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17100156" author="bchen225242" created="Tue, 5 May 2020 18:11:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&#160;Lol, no worry, thanks a lot for the insight you provided so far, will revise the integration test according to them.&lt;/p&gt;</comment>
                            <comment id="17102271" author="bchen225242" created="Fri, 8 May 2020 05:28:21 +0000"  >&lt;p&gt;I don&apos;t see the full log for 3 nodes you mentioned, where did you attach them? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17102668" author="mateuszjadczyk" created="Fri, 8 May 2020 15:22:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; &lt;br/&gt;
Maybe I didn&apos;t. You can now find them in attachments.&#160;&lt;br/&gt;
I have also full-full trace logs but would need some time to remove sensitive data from them. So first I need to know that what I provided so far is not enough.&lt;/p&gt;</comment>
                            <comment id="17110947" author="mateuszjadczyk" created="Tue, 19 May 2020 07:59:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt;&#160;have you had time to look some more into it?&lt;/p&gt;</comment>
                            <comment id="17111343" author="bchen225242" created="Tue, 19 May 2020 16:47:05 +0000"  >&lt;p&gt;Sorry I was oncall last week. Will take another look this week. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17114862" author="bchen225242" created="Sat, 23 May 2020 16:04:56 +0000"  >&lt;p&gt;I made another attempt to reproduce the issue on local. Basically I started 3 stream instances with 2 threads each, where the instance gets standby task assigned uses thread-1:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2020-05-23 08:57:55,050&amp;#93;&lt;/span&gt; INFO stream-thread &lt;span class=&quot;error&quot;&gt;&amp;#91;client-2-StreamThread-1&amp;#93;&lt;/span&gt; partition assignment took 1 ms.&lt;span class=&quot;error&quot;&gt;&amp;#91;2020-05-23 08:57:55,050&amp;#93;&lt;/span&gt; INFO stream-thread &lt;span class=&quot;error&quot;&gt;&amp;#91;client-2-StreamThread-1&amp;#93;&lt;/span&gt; partition assignment took 1 ms. currently assigned active tasks: [] currently assigned standby tasks: &lt;span class=&quot;error&quot;&gt;&amp;#91;0_0&amp;#93;&lt;/span&gt; revoked active tasks: [] revoked standby tasks: []&lt;/p&gt;

&lt;p&gt;After the first instance gets crushed, the same stream thread gets the active task, which is different from what you observed:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2020-05-23 08:58:01,183&amp;#93;&lt;/span&gt; INFO stream-thread &lt;span class=&quot;error&quot;&gt;&amp;#91;client-2-StreamThread-1&amp;#93;&lt;/span&gt; partition assignment took 115 ms.&lt;span class=&quot;error&quot;&gt;&amp;#91;2020-05-23 08:58:01,183&amp;#93;&lt;/span&gt; INFO stream-thread &lt;span class=&quot;error&quot;&gt;&amp;#91;client-2-StreamThread-1&amp;#93;&lt;/span&gt; partition assignment took 115 ms. currently assigned active tasks: &lt;span class=&quot;error&quot;&gt;&amp;#91;0_0&amp;#93;&lt;/span&gt; currently assigned standby tasks: [] revoked active tasks: [] revoked standby tasks: &lt;span class=&quot;error&quot;&gt;&amp;#91;0_0&amp;#93;&lt;/span&gt;&#160;&lt;/p&gt;

&lt;p&gt;on the other hand, I looked at the codepath and I didn&apos;t find the possibility you mentioned, as the state manager creates with the task and there is no shared struct between threads. Right now I think I&apos;m blocked on further investigation until I could find a reliable way to reproduce the scenario. Is this issue still recurring on your side?&lt;/p&gt;</comment>
                            <comment id="17115006" author="mjsax" created="Sun, 24 May 2020 01:45:14 +0000"  >&lt;p&gt;Why not create two instances with one thread each? This way, we can control where active/standbys are assigned? And i think the issue is not with any shared stuff between threads: Assume a single partition input topic and a single stateful task:&lt;/p&gt;

&lt;p&gt;Instance one holds the active task; instance two holds the standby. Let instance one crash (this leaves a dirty state on local disc; no checkpoint file is present though to indicate that it&apos;s dirty). The active is moved to instance two. There is no standby at this point (as there is not enough &quot;capacity&quot;). Restart instance one; it will get the standby assigned (at this point, the issue occurs: the standby does not wipe out the local state, even if there is no checkpoint file and resumes with the corrupted state &#8211; note, that I believe that this issue is is fixed in trunk already, so you need to use 2.3 or 2.4 to reproduce &#8211; not sure if the fix is in 2.5). Now let instance two crash (or stop gracefully). The active task is migrated back to instance one and now uses the corrupted state for further processing.&lt;/p&gt;</comment>
                            <comment id="17115324" author="mateuszjadczyk" created="Sun, 24 May 2020 14:33:45 +0000"  >&lt;p&gt;I think I reproduce it in&#160;&lt;a href=&quot;https://github.com/mateuszjadczykDna/kafka/tree/KAFKA-9891-integration-test-2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/mateuszjadczykDna/kafka/tree/KAFKA-9891-integration-test-2&#160;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I branched from an older version, and the setup contains 3 instances, 2 input topics, 2 threads each.&lt;/p&gt;

&lt;p&gt;It seems it doesn&apos;t happen 100% of the runs.&#160;I attached to this ticket logs from 2 failed runs.&#160;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/13003867/13003867_failedtest&quot; title=&quot;failedtest attached to KAFKA-9891&quot;&gt;failedtest&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;My worry is that if it seems to be fixed in the newest version (2.5), it may be due to changed rebalancing, and not actually fixing the real issue.&lt;/p&gt;</comment>
                            <comment id="17115352" author="mateuszjadczyk" created="Sun, 24 May 2020 15:43:56 +0000"  >&lt;p&gt;BTW in this test (&lt;em&gt;failedtest3&lt;/em&gt;) it also turns out to be happening during the switch between T-1 standby 0_1 to T-2 active 0_1 :&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2020-05-24 17:24:56,442] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO task [0_1] Checkpointable offsets read from checkpoint: {test-app-dedup-store-changelog-1=1} (org.apache.kafka.st
reams.processor.internals.ProcessorStateManager:113)
[2020-05-24 17:24:56,442] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-1 INFO standby-task [0_1] Writing checkpoint: {test-app-dedup-store-changelog-1=1} (org.apache.kafka.streams.processor.i
nternals.ProcessorStateManager:358)
[2020-05-24 17:24:56,443] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO task [0_1] eosEnabled checkpointFile.delete() (org.apache.kafka.streams.processor.internals.ProcessorStateManager
:118)
streams.state.internals.OffsetCheckpoint:185)
ssor.internals.ProcessorStateManager:313)
sorStateManager:123)
[2020-05-24 17:24:56,443] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO delete offset checkpoint /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint, exists: &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; (org.apache.kafka.s
treams.state.internals.OffsetCheckpoint:185)
[2020-05-24 17:24:56,443] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-1 TRACE Writing tmp checkpoint file /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint.tmp (org.apache.kafka.streams
.state.internals.OffsetCheckpoint:81)
teManager:317)
        currently assigned active tasks: []
        currently assigned standby tasks: [0_0, 0_2]
        revoked active tasks: []
        revoked standby tasks: []
 (org.apache.kafka.streams.processor.internals.StreamThread:96)
[2020-05-24 17:24:56,444] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 DEBUG task [0_1] Created state store manager &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_1 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:123)
[2020-05-24 17:24:56,444] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO stream-thread [test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2] Creating producer client &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task 0_1 (org.apache.kafka.streams.processor.internals.StreamThread:370)
[2020-05-24 17:24:56,444] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-1 TRACE Swapping tmp checkpoint file /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint.tmp /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint (org.apache.kafka.streams.state.internals.OffsetCheckpoint:97)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;T-2 deletes the checkpoint but T-1 manages to write it back during the shutdown. So it ends up with the file which shouldn&apos;t be there. That&apos;s probably also the reason why the test is flashing - this timing is quite specific.&lt;br/&gt;
Hope you find a fix now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17115366" author="mateuszjadczyk" created="Sun, 24 May 2020 16:30:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; &lt;br/&gt;
Maybe this could make this test more reliable (&lt;a href=&quot;https://github.com/mateuszjadczykDna/kafka/compare/KAFKA-9891-integration-test-2-extra-wait&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KAFKA-9891-integration-test-2-extra-wait&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://github.com/mateuszjadczykDna/kafka/commit/e0304b48a47a07aa12ab55b7abd1ddf6cfef828b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/mateuszjadczykDna/kafka/commit/e0304b48a47a07aa12ab55b7abd1ddf6cfef828b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I saw your new commits after changing the test on my own, but I use 2 topics (maybe it causes another tasks assignment than in your branch) and I changed failure assertion as the fail() in exception handler didn&apos;t seem to work. Let me know if it fails for you.&lt;/p&gt;</comment>
                            <comment id="17115954" author="mateuszjadczyk" created="Mon, 25 May 2020 11:46:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; if this is true&#160;&lt;br/&gt;
&amp;gt; &lt;em&gt;Restart instance one; it will get the standby assigned (at this point, the issue occurs: the standby does not wipe out the local state, even if there is no checkpoint file and resumes with the corrupted state&lt;/em&gt;&lt;br/&gt;
then it&apos;s another case.&lt;/p&gt;

&lt;p&gt;For the case I&apos;m talking about you need 2 threads and a specific rebalancing: T1 with standby task and then T2 taking over even though it wasn&apos;t a standby (I don&apos;t know the rebalancing algorithm that well but as I understand it has something to do with balances traffic across all instances, that&apos;s why you may need 2 topics to reproduce it). &lt;b&gt;In this scenario T1 hasn&apos;t finished shutting down and T2 is already starting up. And both of them operate on the same disk state directory - that&apos;s the shared part between threads.&lt;/b&gt;&#160;&lt;br/&gt;
Before we feel safe enough to enable standby tasks again, we need to be sure it&apos;s fixed, either here or in 2.5&lt;/p&gt;</comment>
                            <comment id="17117158" author="mjsax" created="Tue, 26 May 2020 22:40:45 +0000"  >&lt;p&gt;Even if two instances run on the same server and use the same state directory, a task (active or standby) should get a lock on the task directory first. And thus, only after the lock is released by one thread, the other thread can grab it. &#8211; Thus, T2 cannot really start before T1 finished it&apos;s shut down (or there is another bug in the locking code...).&lt;/p&gt;</comment>
                            <comment id="17117509" author="mateuszjadczyk" created="Wed, 27 May 2020 08:30:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; If I&apos;m not mistaken, these logs indicate a bug, as both threads (same instance) operate on &lt;em&gt;/tmp/kafka-streams/instance-2/test-app/0_1/&lt;/em&gt; at the same time&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9891?focusedCommentId=17115352&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17115352&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-9891?focusedCommentId=17115352&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17115352&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17119899" author="bchen225242" created="Fri, 29 May 2020 19:58:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&#160;I ran your example again, and could see that the assignment scenario reproduced with &amp;gt; 1 topic partition.&#160;The part I&apos;m confusing is that you used 0 ms as offset commit interval, as this would actually cause the poison record very likely to commit and replicated. Could you elaborate?&lt;/p&gt;</comment>
                            <comment id="17120253" author="mateuszjadczyk" created="Sat, 30 May 2020 13:57:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; I think I wanted to be sure that keyOne is committed ASAP. If it&apos;s set too high, then keyOne is not committed = nothing on changelog topic = nothing to do for a  standby task, right?&lt;/p&gt;

&lt;p&gt;Nevertheless, I changed it to default (100ms) on &lt;a href=&quot;https://github.com/mateuszjadczykDna/kafka/compare/KAFKA-9891-integration-test-2-extra-wait&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/mateuszjadczykDna/kafka/compare/KAFKA-9891-integration-test-2-extra-wait&lt;/a&gt; and it still fails.&lt;/p&gt;</comment>
                            <comment id="17120333" author="mateuszjadczyk" created="Sat, 30 May 2020 18:10:34 +0000"  >&lt;p&gt;On trunk I can&apos;t seem to reproduce this particular partitions reassignment, but I noticed that directory locking before reading checkpoint was introduced in Feb 2020 by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; which should prevent such a race condition. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; is this the fix you had in mind? Will this be a part of 2.6?&lt;/p&gt;</comment>
                            <comment id="17121149" author="bchen225242" created="Mon, 1 Jun 2020 16:35:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&#160;If we commit keyOne, then it &lt;b&gt;should&lt;/b&gt;&#160;be materialized inside the standby state store copy, and the duplicate key check will see it, correct? Will sync with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&#160;on whether to backport this checkpoint lock to 2.4.&lt;/p&gt;</comment>
                            <comment id="17123542" author="mateuszjadczyk" created="Tue, 2 Jun 2020 09:09:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; yes, it should be materialized. Duplicate key will be however performed only once for keyOne (during the very first processing), as this is thrown only for the poisonKey:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IllegalStateException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Throw on &quot;&lt;/span&gt; + poisonKey + &lt;span class=&quot;code-quote&quot;&gt;&quot; to trigger rebalance&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17123544" author="mateuszjadczyk" created="Tue, 2 Jun 2020 09:10:46 +0000"  >&lt;p&gt;The reason we need at least one materialized key is that we then have something on the changelog topic and some checkpoint files are used which mess things up.&lt;/p&gt;</comment>
                            <comment id="17124296" author="bchen225242" created="Tue, 2 Jun 2020 20:36:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&#160;that&apos;s right, so we should only check duplicate of `poisonKey` instead for exception, like:&lt;/p&gt;



&lt;p&gt;if (storeIterator.hasNext() &amp;amp;&amp;amp; key.equals(poisonKey)) &lt;/p&gt;
{

&#160; &#160; throw new IllegalStateException(&quot;Caught a duplicate key &quot; + key);

}

&lt;p&gt;WDYT?&lt;/p&gt;</comment>
                            <comment id="17124787" author="mateuszjadczyk" created="Wed, 3 Jun 2020 09:14:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; well, it shouldn&apos;t occur for either of the keys, but this will pinpoint the poisonKey.&lt;br/&gt;
Btw we&apos;re currently not using standby replicas because of this bug and we probably could wait until 2.6. Not sure about all other kafka streams users though..&lt;/p&gt;</comment>
                            <comment id="17125457" author="bchen225242" created="Thu, 4 Jun 2020 01:26:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt;&#160;I see, glad you guys have a workaround. I already synced with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&#160;offline to bring in another pair of eye to see if we could find more leads here.&lt;/p&gt;</comment>
                            <comment id="17128389" author="vvcephei" created="Mon, 8 Jun 2020 15:33:45 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt; ,&lt;/p&gt;

&lt;p&gt;I see you&apos;re just added this as a blocker for 2.5.1. Is it a regression?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;-John&lt;/p&gt;</comment>
                            <comment id="17128646" author="mjsax" created="Mon, 8 Jun 2020 20:54:48 +0000"  >&lt;p&gt;I am not 100% sure. But even if not, it might be severe enough to treat as blocker?&lt;/p&gt;</comment>
                            <comment id="17138091" author="mjsax" created="Wed, 17 Jun 2020 04:25:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mateuszjadczyk&quot; class=&quot;user-hover&quot; rel=&quot;mateuszjadczyk&quot;&gt;mateuszjadczyk&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt;: I think I was able to write a test that reproduces the issue in 2.5. Cf&#160;&lt;a href=&quot;https://github.com/apache/kafka/pull/8886&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/8886&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Please let me know what you think.&lt;/p&gt;</comment>
                            <comment id="17138942" author="mjsax" created="Thu, 18 Jun 2020 01:34:12 +0000"  >&lt;p&gt;PR&#160;&lt;a href=&quot;https://github.com/apache/kafka/pull/8890&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/8890&lt;/a&gt;&#160;for `trunk` (and `2.6`) seem to confirm that it&apos;s already fixed there.&lt;/p&gt;</comment>
                            <comment id="17139963" author="mateuszjadczyk" created="Thu, 18 Jun 2020 20:00:14 +0000"  >&lt;p&gt;LGTM thanks for looking into it and making sure that 2.5.1/2.6 should be safe to use&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="13003867" name="failedtest" size="844239" author="mateuszjadczyk" created="Sun, 24 May 2020 14:31:29 +0000"/>
                            <attachment id="13003868" name="failedtest2" size="877462" author="mateuszjadczyk" created="Sun, 24 May 2020 14:31:29 +0000"/>
                            <attachment id="13003874" name="failedtest3" size="925107" author="mateuszjadczyk" created="Sun, 24 May 2020 15:44:15 +0000"/>
                            <attachment id="13003873" name="failedtest3_bug" size="2463" author="mateuszjadczyk" created="Sun, 24 May 2020 15:44:15 +0000"/>
                            <attachment id="13002422" name="state_store_operations.txt" size="25261" author="mateuszjadczyk" created="Fri, 8 May 2020 15:20:41 +0000"/>
                            <attachment id="13002423" name="tasks_assignment.txt" size="8499" author="mateuszjadczyk" created="Fri, 8 May 2020 15:20:41 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 21 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0duig:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>