<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:13:52 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7164] Follower should truncate after every leader epoch change</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7164</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Currently we skip log truncation for followers if a LeaderAndIsr request is received, but the leader does not change. This can lead to log divergence if the follower missed a leader change before the current known leader was reelected. Basically the problem is that the leader may truncate its own log prior to becoming leader again, so the follower would need to reconcile its log again.&lt;/p&gt;

&lt;p&gt;For example, suppose that we have three replicas: r1, r2, and r3. Initially, r1 is the leader in epoch 0 and writes one record at offset 0. r3 replicates this successfully.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
r1: 
  status: leader
  epoch: 0
  log: [{id: 0, offset: 0, epoch:0}]
r2: 
  status: follower
  epoch: 0
  log: []
r3: 
  status: follower
  epoch: 0
  log: [{id: 0, offset: 0, epoch:0}]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Suppose then that r2 becomes leader in epoch 1. r1 notices the leader change and truncates, but r3 for whatever reason, does not.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
r1: 
  status: follower
  epoch: 1
  log: []
r2: 
  status: leader
  epoch: 1
  log: []
r3: 
  status: follower
  epoch: 0
  log: [{offset: 0, epoch:0}]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now suppose that r2 fails and r1 becomes the leader in epoch 2. Immediately it writes a new record:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
r1: 
  status: leader
  epoch: 2
  log: [{id: 1, offset: 0, epoch:2}]
r2: 
  status: follower
  epoch: 2
  log: []
r3: 
  status: follower
  epoch: 0
  log: [{id: 0, offset: 0, epoch:0}]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the replica continues fetching with the old epoch, we can have log divergence as noted in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6880&quot; title=&quot;Zombie replicas must be fenced&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6880&quot;&gt;&lt;del&gt;KAFKA-6880&lt;/del&gt;&lt;/a&gt;. However, if r3 successfully receives the new LeaderAndIsr request which updates the epoch to 2, but skips the truncation, then the logs will stay inconsistent.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13172210">KAFKA-7164</key>
            <summary>Follower should truncate after every leader epoch change</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bob-barrett">Bob Barrett</assignee>
                                    <reporter username="hachikuji">Jason Gustafson</reporter>
                        <labels>
                    </labels>
                <created>Sat, 14 Jul 2018 17:24:03 +0000</created>
                <updated>Tue, 28 Aug 2018 18:09:42 +0000</updated>
                            <resolved>Mon, 13 Aug 2018 06:30:43 +0000</resolved>
                                                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16563169" author="githubbot" created="Tue, 31 Jul 2018 05:52:45 +0000"  >&lt;p&gt;bob-barrett opened a new pull request #5436: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7164&quot; title=&quot;Follower should truncate after every leader epoch change&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7164&quot;&gt;&lt;del&gt;KAFKA-7164&lt;/del&gt;&lt;/a&gt;: Follower should truncate after every leader epoch change&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5436&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5436&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Currently, we skip the steps to make a replica a follower if the leader does not change, inlcuding truncating the follower log if necessary. This can cause problems if the follower has missed one or more leader updates. Change the logic to only skip the steps if the new epoch is the same or one greater than the old epoch. Tested with unit tests that verify the behavior of Partition.scala and that show log truncation when the follower&apos;s log is ahead of the leader&apos;s, the follower has missed an epoch update, and the follower receives a LeaderAndIsrRequest making it a follower&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16577880" author="githubbot" created="Mon, 13 Aug 2018 06:30:12 +0000"  >&lt;p&gt;hachikuji closed pull request #5436: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7164&quot; title=&quot;Follower should truncate after every leader epoch change&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7164&quot;&gt;&lt;del&gt;KAFKA-7164&lt;/del&gt;&lt;/a&gt;: Follower should truncate after every leader epoch change&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5436&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5436&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index 154a8f969c5..a92340f2a4b 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -326,12 +326,15 @@ class Partition(val topic: String,&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Make the local replica the follower by setting the new leader and ISR to empty&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  If the leader replica id does not change, return false to indicate the replica manager&lt;br/&gt;
+   *  If the leader replica id does not change and the new epoch is equal or one&lt;br/&gt;
+   *  greater (that is, no updates have been missed), return false to indicate to the&lt;br/&gt;
+    * replica manager that state is already correct and the become-follower steps can be skipped&lt;br/&gt;
    */&lt;br/&gt;
   def makeFollower(controllerId: Int, partitionStateInfo: LeaderAndIsrRequest.PartitionState, correlationId: Int): Boolean = {&lt;br/&gt;
     inWriteLock(leaderIsrUpdateLock) {&lt;br/&gt;
       val newAssignedReplicas = partitionStateInfo.basePartitionState.replicas.asScala.map(_.toInt)&lt;/li&gt;
	&lt;li&gt;val newLeaderBrokerId: Int = partitionStateInfo.basePartitionState.leader&lt;br/&gt;
+      val newLeaderBrokerId = partitionStateInfo.basePartitionState.leader&lt;br/&gt;
+      val oldLeaderEpoch = leaderEpoch&lt;br/&gt;
       // record the epoch of the controller that made the leadership decision. This is useful while updating the isr&lt;br/&gt;
       // to maintain the decision maker controller&apos;s epoch in the zookeeper path&lt;br/&gt;
       controllerEpoch = partitionStateInfo.basePartitionState.controllerEpoch&lt;br/&gt;
@@ -343,7 +346,9 @@ class Partition(val topic: String,&lt;br/&gt;
       leaderEpoch = partitionStateInfo.basePartitionState.leaderEpoch&lt;br/&gt;
       zkVersion = partitionStateInfo.basePartitionState.zkVersion&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (leaderReplicaIdOpt.isDefined &amp;amp;&amp;amp; leaderReplicaIdOpt.get == newLeaderBrokerId) {&lt;br/&gt;
+      // If the leader is unchanged and the epochs are no more than one change apart, indicate that no follower changes are required&lt;br/&gt;
+      // Otherwise, we missed a leader epoch update, which means the leader&apos;s log may have been truncated prior to the current epoch.&lt;br/&gt;
+      if (leaderReplicaIdOpt.contains(newLeaderBrokerId) &amp;amp;&amp;amp; (leaderEpoch == oldLeaderEpoch || leaderEpoch == oldLeaderEpoch + 1)) 
{
         false
       }
&lt;p&gt;       else {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
index 41bdefddb76..96c1147c9bf 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
@@ -32,9 +32,11 @@ import org.apache.kafka.common.errors.ReplicaNotAvailableException&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
+import org.apache.kafka.common.requests.LeaderAndIsrRequest&lt;br/&gt;
 import org.junit.
{After, Before, Test}
&lt;p&gt; import org.junit.Assert._&lt;br/&gt;
 import org.scalatest.Assertions.assertThrows&lt;br/&gt;
+&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; class PartitionTest {&lt;br/&gt;
@@ -207,6 +209,27 @@ class PartitionTest {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testMakeFollowerWithNoLeaderIdChange(): Unit = &lt;/p&gt;
{
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)
+
+    // Start off as follower
+    var partitionStateInfo = new LeaderAndIsrRequest.PartitionState(0, 1, 1, List[Integer](0, 1, 2).asJava, 1, List[Integer](0, 1, 2).asJava, false)
+    partition.makeFollower(0, partitionStateInfo, 0)
+
+    // Request with same leader and epoch increases by more than 1, perform become-follower steps
+    partitionStateInfo = new LeaderAndIsrRequest.PartitionState(0, 1, 3, List[Integer](0, 1, 2).asJava, 1, List[Integer](0, 1, 2).asJava, false)
+    assertTrue(partition.makeFollower(0, partitionStateInfo, 1))
+
+    // Request with same leader and epoch increases by only 1, skip become-follower steps
+    partitionStateInfo = new LeaderAndIsrRequest.PartitionState(0, 1, 4, List[Integer](0, 1, 2).asJava, 1, List[Integer](0, 1, 2).asJava, false)
+    assertFalse(partition.makeFollower(0, partitionStateInfo, 2))
+
+    // Request with same leader and same epoch, skip become-follower steps
+    partitionStateInfo = new LeaderAndIsrRequest.PartitionState(0, 1, 4, List[Integer](0, 1, 2).asJava, 1, List[Integer](0, 1, 2).asJava, false)
+    assertFalse(partition.makeFollower(0, partitionStateInfo, 2))
+  }
&lt;p&gt;+&lt;br/&gt;
   def createRecords(records: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;SimpleRecord&amp;#93;&lt;/span&gt;, baseOffset: Long, partitionLeaderEpoch: Int = 0): MemoryRecords = {&lt;br/&gt;
     val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))&lt;br/&gt;
     val builder = MemoryRecords.builder(&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
index 56d4b7919e4..171bcf3528c 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
@@ -19,21 +19,26 @@ package kafka.server&lt;/p&gt;

&lt;p&gt; import java.io.File&lt;br/&gt;
 import java.util.Properties&lt;br/&gt;
+import java.util.concurrent.&lt;/p&gt;
{CountDownLatch, TimeUnit}
&lt;p&gt; import java.util.concurrent.atomic.AtomicBoolean&lt;/p&gt;

&lt;p&gt;-import kafka.log.LogConfig&lt;br/&gt;
+import kafka.log.&lt;/p&gt;
{Log, LogConfig, LogManager, ProducerStateManager}
&lt;p&gt; import kafka.utils.&lt;/p&gt;
{MockScheduler, MockTime, TestUtils}
&lt;p&gt; import TestUtils.createBroker&lt;br/&gt;
+import kafka.cluster.BrokerEndPoint&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.util.ReplicaFetcherMockBlockingSend&lt;br/&gt;
 import kafka.utils.timer.MockTimer&lt;br/&gt;
 import kafka.zk.KafkaZkClient&lt;br/&gt;
 import org.I0Itec.zkclient.ZkClient&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.protocol.&lt;/p&gt;
{ApiKeys, Errors}
&lt;p&gt; import org.apache.kafka.common.record._&lt;br/&gt;
-import org.apache.kafka.common.requests.&lt;/p&gt;
{IsolationLevel, LeaderAndIsrRequest}
&lt;p&gt;+import org.apache.kafka.common.requests.&lt;/p&gt;
{EpochEndOffset, IsolationLevel, LeaderAndIsrRequest}
&lt;p&gt; import org.apache.kafka.common.requests.ProduceResponse.PartitionResponse&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest.PartitionData&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
+import org.apache.kafka.common.utils.Time&lt;br/&gt;
 import org.apache.kafka.common.&lt;/p&gt;
{Node, TopicPartition}
&lt;p&gt; import org.apache.zookeeper.data.Stat&lt;br/&gt;
 import org.easymock.EasyMock&lt;br/&gt;
@@ -51,6 +56,11 @@ class ReplicaManagerTest {&lt;br/&gt;
   var zkClient: ZkClient = _&lt;br/&gt;
   var kafkaZkClient: KafkaZkClient = _&lt;/p&gt;

&lt;p&gt;+  // Constants defined for readability&lt;br/&gt;
+  val zkVersion = 0&lt;br/&gt;
+  val correlationId = 0&lt;br/&gt;
+  var controllerEpoch = 0&lt;br/&gt;
+&lt;br/&gt;
   @Before&lt;br/&gt;
   def setUp() {&lt;br/&gt;
     zkClient = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ZkClient&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -504,6 +514,216 @@ class ReplicaManagerTest {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  /**&lt;br/&gt;
+    * If a partition becomes a follower and the leader is unchanged it should check for truncation&lt;br/&gt;
+    * if the epoch has increased by more than one (which suggests it has missed an update)&lt;br/&gt;
+    */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testBecomeFollowerWhenLeaderIsUnchangedButMissedLeaderUpdate() &lt;/p&gt;
{
+    val topicPartition = 0
+    val followerBrokerId = 0
+    val leaderBrokerId = 1
+    val controllerId = 0
+    val controllerEpoch = 0
+    var leaderEpoch = 1
+    val aliveBrokerIds = Seq[Integer] (followerBrokerId, leaderBrokerId)
+    val countDownLatch = new CountDownLatch(1)
+
+    // Prepare the mocked components for the test
+    val (replicaManager, mockLogMgr) = prepareReplicaManagerAndLogManager(
+      topicPartition, followerBrokerId, leaderBrokerId, countDownLatch, expectTruncation = true)
+
+    // Initialize partition state to follower, with leader = 1, leaderEpoch = 1
+    val partition = replicaManager.getOrCreatePartition(new TopicPartition(topic, topicPartition))
+    partition.getOrCreateReplica(followerBrokerId)
+    partition.makeFollower(controllerId,
+      leaderAndIsrPartitionState(leaderEpoch, leaderBrokerId, aliveBrokerIds),
+      correlationId)
+
+    // Make local partition a follower - because epoch increased by more than 1, truncation should
+    // trigger even though leader does not change
+    leaderEpoch += 2
+    val leaderAndIsrRequest0 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion,
+      controllerId, controllerEpoch,
+      collection.immutable.Map(new TopicPartition(topic, topicPartition) -&amp;gt;
+        leaderAndIsrPartitionState(leaderEpoch, leaderBrokerId, aliveBrokerIds)).asJava,
+      Set(new Node(followerBrokerId, &quot;host1&quot;, 0),
+        new Node(leaderBrokerId, &quot;host2&quot;, 1)).asJava).build()
+    replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest0,
+      (_, followers) =&amp;gt; assertEquals(followerBrokerId, followers.head.partitionId))
+    assertTrue(countDownLatch.await(1000L, TimeUnit.MILLISECONDS))
+
+    // Truncation should have happened once
+    EasyMock.verify(mockLogMgr)
+  }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * If a partition becomes a follower and the leader is unchanged but no epoch update&lt;br/&gt;
+    * has been missed, it should not check for truncation&lt;br/&gt;
+    */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testDontBecomeFollowerWhenNoMissedLeaderUpdate() &lt;/p&gt;
{
+    val topicPartition = 0
+    val followerBrokerId = 0
+    val leaderBrokerId = 1
+    val controllerId = 0
+    var leaderEpoch = 1
+    val aliveBrokerIds = Seq[Integer] (followerBrokerId, leaderBrokerId)
+    val countDownLatch = new CountDownLatch(1)
+
+    // Prepare the mocked components for the test
+    val (replicaManager, mockLogMgr) = prepareReplicaManagerAndLogManager(
+      topicPartition, followerBrokerId, leaderBrokerId, countDownLatch, expectTruncation = false)
+
+    // Initialize partition state to follower, with leader = 1, leaderEpoch = 1
+    val partition = replicaManager.getOrCreatePartition(new TopicPartition(topic, topicPartition))
+    partition.getOrCreateReplica(followerBrokerId)
+    partition.makeFollower(controllerId,
+      leaderAndIsrPartitionState(leaderEpoch, leaderBrokerId, aliveBrokerIds),
+      correlationId)
+
+    // Make local partition a follower - because epoch did not change, truncation should not trigger
+    val leaderAndIsrRequest0 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion,
+      controllerId, controllerEpoch,
+      collection.immutable.Map(new TopicPartition(topic, topicPartition) -&amp;gt;
+        leaderAndIsrPartitionState(leaderEpoch, leaderBrokerId, aliveBrokerIds)).asJava,
+      Set(new Node(followerBrokerId, &quot;host1&quot;, 0),
+        new Node(leaderBrokerId, &quot;host2&quot;, 1)).asJava).build()
+    replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest0,
+      (_, followers) =&amp;gt; assertTrue(followers.isEmpty))
+
+    // Make local partition a follower - because epoch increased by only 1 and leader did not change,
+    // truncation should not trigger
+    leaderEpoch += 1
+    val leaderAndIsrRequest1 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion,
+      controllerId, controllerEpoch,
+      collection.immutable.Map(new TopicPartition(topic, topicPartition) -&amp;gt;
+        leaderAndIsrPartitionState(leaderEpoch, leaderBrokerId, aliveBrokerIds)).asJava,
+      Set(new Node(followerBrokerId, &quot;host1&quot;, 0),
+        new Node(leaderBrokerId, &quot;host2&quot;, 1)).asJava).build()
+    replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest1,
+      (_, followers) =&amp;gt; assertTrue(followers.isEmpty))
+
+    // Truncation should not have happened
+    EasyMock.verify(mockLogMgr)
+  }
&lt;p&gt;+&lt;br/&gt;
+  private def prepareReplicaManagerAndLogManager(topicPartition: Int,&lt;br/&gt;
+                                                 followerBrokerId: Int,&lt;br/&gt;
+                                                 leaderBrokerId: Int,&lt;br/&gt;
+                                                 countDownLatch: CountDownLatch,&lt;br/&gt;
+                                                 expectTruncation: Boolean) : (ReplicaManager, LogManager) = {&lt;br/&gt;
+    val props = TestUtils.createBrokerConfig(0, TestUtils.MockZkConnect)&lt;br/&gt;
+    props.put(&quot;log.dir&quot;, TestUtils.tempRelativeDir(&quot;data&quot;).getAbsolutePath)&lt;br/&gt;
+    val config = KafkaConfig.fromProps(props)&lt;br/&gt;
+&lt;br/&gt;
+    // Setup mock local log to have leader epoch of 3 and offset of 10&lt;br/&gt;
+    val localLogOffset = 10&lt;br/&gt;
+    val offsetFromLeader = 5&lt;br/&gt;
+    val leaderEpochFromLeader = 3&lt;br/&gt;
+    val mockScheduler = new MockScheduler(time)&lt;br/&gt;
+    val mockBrokerTopicStats = new BrokerTopicStats&lt;br/&gt;
+    val mockLogDirFailureChannel = new LogDirFailureChannel(config.logDirs.size)&lt;br/&gt;
+    val mockLeaderEpochCache = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(mockLeaderEpochCache.latestEpoch()).andReturn(leaderEpochFromLeader)&lt;br/&gt;
+    EasyMock.expect(mockLeaderEpochCache.endOffsetFor(leaderEpochFromLeader))&lt;br/&gt;
+      .andReturn((leaderEpochFromLeader, localLogOffset))&lt;br/&gt;
+    EasyMock.replay(mockLeaderEpochCache)&lt;br/&gt;
+    val mockLog = new Log(&lt;br/&gt;
+      dir = new File(new File(config.logDirs.head), s&quot;$topic-0&quot;),&lt;br/&gt;
+      config = LogConfig(),&lt;br/&gt;
+      logStartOffset = 0L,&lt;br/&gt;
+      recoveryPoint = 0L,&lt;br/&gt;
+      scheduler = mockScheduler,&lt;br/&gt;
+      brokerTopicStats = mockBrokerTopicStats,&lt;br/&gt;
+      time = time,&lt;br/&gt;
+      maxProducerIdExpirationMs = 30000,&lt;br/&gt;
+      producerIdExpirationCheckIntervalMs = 30000,&lt;br/&gt;
+      topicPartition = new TopicPartition(topic, topicPartition),&lt;br/&gt;
+      producerStateManager = new ProducerStateManager(new TopicPartition(topic, topicPartition),&lt;br/&gt;
+        new File(new File(config.logDirs.head), s&quot;$topic-$topicPartition&quot;), 30000),&lt;br/&gt;
+      logDirFailureChannel = mockLogDirFailureChannel) &lt;/p&gt;
{
+
+      override def leaderEpochCache: LeaderEpochCache = mockLeaderEpochCache
+
+      override def logEndOffsetMetadata = LogOffsetMetadata(localLogOffset)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Expect to call LogManager.truncateTo exactly once&lt;br/&gt;
+    val mockLogMgr = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(mockLogMgr.liveLogDirs).andReturn(config.logDirs.map(new File(_).getAbsoluteFile)).anyTimes&lt;br/&gt;
+    EasyMock.expect(mockLogMgr.currentDefaultConfig).andReturn(LogConfig())&lt;br/&gt;
+    EasyMock.expect(mockLogMgr.getOrCreateLog(new TopicPartition(topic, topicPartition),&lt;br/&gt;
+      LogConfig(), isNew = false, isFuture = false)).andReturn(mockLog).anyTimes&lt;br/&gt;
+    if (expectTruncation) &lt;/p&gt;
{
+      EasyMock.expect(mockLogMgr.truncateTo(Map(new TopicPartition(topic, topicPartition) -&amp;gt; offsetFromLeader),
+        isFuture = false)).once
+    }
&lt;p&gt;+    EasyMock.replay(mockLogMgr)&lt;br/&gt;
+&lt;br/&gt;
+    val aliveBrokerIds = Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(followerBrokerId, leaderBrokerId)&lt;br/&gt;
+    val aliveBrokers = aliveBrokerIds.map(brokerId =&amp;gt; createBroker(brokerId, s&quot;host$brokerId&quot;, brokerId))&lt;br/&gt;
+&lt;br/&gt;
+    val metadataCache = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;MetadataCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(metadataCache.getAliveBrokers).andReturn(aliveBrokers).anyTimes&lt;br/&gt;
+    aliveBrokerIds.foreach &lt;/p&gt;
{ brokerId =&amp;gt;
+      EasyMock.expect(metadataCache.isBrokerAlive(EasyMock.eq(brokerId))).andReturn(true).anyTimes
+    }
&lt;p&gt;+    EasyMock.replay(metadataCache)&lt;br/&gt;
+&lt;br/&gt;
+    val timer = new MockTimer&lt;br/&gt;
+    val mockProducePurgatory = new DelayedOperationPurgatory&lt;span class=&quot;error&quot;&gt;&amp;#91;DelayedProduce&amp;#93;&lt;/span&gt;(&lt;br/&gt;
+      purgatoryName = &quot;Produce&quot;, timer, reaperEnabled = false)&lt;br/&gt;
+    val mockFetchPurgatory = new DelayedOperationPurgatory&lt;span class=&quot;error&quot;&gt;&amp;#91;DelayedFetch&amp;#93;&lt;/span&gt;(&lt;br/&gt;
+      purgatoryName = &quot;Fetch&quot;, timer, reaperEnabled = false)&lt;br/&gt;
+    val mockDeleteRecordsPurgatory = new DelayedOperationPurgatory&lt;span class=&quot;error&quot;&gt;&amp;#91;DelayedDeleteRecords&amp;#93;&lt;/span&gt;(&lt;br/&gt;
+      purgatoryName = &quot;DeleteRecords&quot;, timer, reaperEnabled = false)&lt;br/&gt;
+&lt;br/&gt;
+    // Mock network client to show leader offset of 5&lt;br/&gt;
+    val quota = QuotaFactory.instantiate(config, metrics, time, &quot;&quot;)&lt;br/&gt;
+    val blockingSend = new ReplicaFetcherMockBlockingSend(Map(new TopicPartition(topic, topicPartition) -&amp;gt;&lt;br/&gt;
+      new EpochEndOffset(leaderEpochFromLeader, offsetFromLeader)).asJava, BrokerEndPoint(1, &quot;host1&quot; ,1), time)&lt;br/&gt;
+    val replicaManager = new ReplicaManager(config, metrics, time, kafkaZkClient, mockScheduler, mockLogMgr,&lt;br/&gt;
+      new AtomicBoolean(false), quota, mockBrokerTopicStats,&lt;br/&gt;
+      metadataCache, mockLogDirFailureChannel, mockProducePurgatory, mockFetchPurgatory,&lt;br/&gt;
+      mockDeleteRecordsPurgatory, Option(this.getClass.getName)) {&lt;br/&gt;
+&lt;br/&gt;
+      override protected def createReplicaFetcherManager(metrics: Metrics,&lt;br/&gt;
+                                                     time: Time,&lt;br/&gt;
+                                                     threadNamePrefix: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                                     quotaManager: ReplicationQuotaManager): ReplicaFetcherManager = {&lt;br/&gt;
+        new ReplicaFetcherManager(config, this, metrics, time, threadNamePrefix, quotaManager) {&lt;br/&gt;
+&lt;br/&gt;
+          override def createFetcherThread(fetcherId: Int, sourceBroker: BrokerEndPoint): AbstractFetcherThread = {&lt;br/&gt;
+            new ReplicaFetcherThread(s&quot;ReplicaFetcherThread-$fetcherId&quot;, fetcherId,&lt;br/&gt;
+              sourceBroker, config, replicaManager, metrics, time, quota.follower, Some(blockingSend)) {&lt;br/&gt;
+&lt;br/&gt;
+              override def doWork() = &lt;/p&gt;
{
+                // In case the thread starts before the partition is added by AbstractFetcherManager,
+                // add it here (it&apos;s a no-op if already added)
+                addPartitions(Map(new TopicPartition(topic, topicPartition) -&amp;gt; 0L))
+                super.doWork()
+
+                // Shut the thread down after one iteration to avoid double-counting truncations
+                initiateShutdown()
+                countDownLatch.countDown()
+              }
&lt;p&gt;+            }&lt;br/&gt;
+          }&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    (replicaManager, mockLogMgr)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def leaderAndIsrPartitionState(leaderEpoch: Int,&lt;br/&gt;
+                                         leaderBrokerId: Int,&lt;br/&gt;
+                                         aliveBrokerIds: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;) : LeaderAndIsrRequest.PartitionState = &lt;/p&gt;
{
+    new LeaderAndIsrRequest.PartitionState(controllerEpoch, leaderBrokerId, leaderEpoch, aliveBrokerIds.asJava,
+      zkVersion, aliveBrokerIds.asJava, false)
+  }
&lt;p&gt;+&lt;br/&gt;
   private class CallbackResult&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; {&lt;br/&gt;
     private var value: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
     private var fun: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;T =&amp;gt; Unit&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
@@ -532,7 +752,8 @@ class ReplicaManagerTest {&lt;br/&gt;
   private def appendRecords(replicaManager: ReplicaManager,&lt;br/&gt;
                             partition: TopicPartition,&lt;br/&gt;
                             records: MemoryRecords,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;isFromClient: Boolean = true): CallbackResult&lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionResponse&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+                            isFromClient: Boolean = true,&lt;br/&gt;
+                            requiredAcks: Short = -1): CallbackResult&lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionResponse&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val result = new CallbackResult&lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionResponse&amp;#93;&lt;/span&gt;()&lt;br/&gt;
     def appendCallback(responses: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionResponse&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
       val response = responses.get(partition)&lt;br/&gt;
@@ -542,7 +763,7 @@ class ReplicaManagerTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     replicaManager.appendRecords(&lt;br/&gt;
       timeout = 1000,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;requiredAcks = -1,&lt;br/&gt;
+      requiredAcks = requiredAcks,&lt;br/&gt;
       internalTopicsAllowed = false,&lt;br/&gt;
       isFromClient = isFromClient,&lt;br/&gt;
       entriesPerPartition = Map(partition -&amp;gt; records),&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16578819" author="watsonmw" created="Mon, 13 Aug 2018 19:27:14 +0000"  >&lt;p&gt;Does this&#160;fix resolve the issue where a follower is continuously&#160;writing&#160;the following to server.log after losing and re-establishing its connection to zookeeper?&#160; I see this with&#160;Kafka 1.1.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2018-07-30 03:09:18,611] INFO [Partition __consumer_offsets-13 broker=1] Shrinking ISR from 1,2 to 1 (kafka.cluster.Partition)
[2018-07-30 03:09:18,614] INFO [Partition __consumer_offsets-13 broker=1] Cached zkVersion [127] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 14 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3vwfz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>