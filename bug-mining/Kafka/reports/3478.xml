<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:32:47 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-14639] Kafka CooperativeStickyAssignor revokes/assigns partition in one rebalance cycle</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-14639</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;I have an application that runs 6 consumers in parallel. I am getting some unexpected results when I use &lt;tt&gt;CooperativeStickyAssignor&lt;/tt&gt;. If I understand the mechanism correctly, if the consumer looses partition in one rebalance cycle, the partition should be assigned in the next rebalance cycle.&lt;/p&gt;

&lt;p&gt;This assumption is based on the&#160;&lt;a href=&quot;https://kafka.apache.org/31/javadoc/org/apache/kafka/clients/consumer/ConsumerPartitionAssignor.RebalanceProtocol.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;RebalanceProtocol&lt;/a&gt;&#160;documentation and few blog posts that describe the protocol, like&#160;&lt;a href=&quot;https://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this one&lt;/a&gt;&#160;on Confluent blog.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The assignor should not reassign any owned partitions immediately, but instead may indicate consumers the need for partition revocation so that the revoked partitions can be reassigned to other consumers in the next rebalance event. This is designed for sticky assignment logic which attempts to minimize partition reassignment with cooperative adjustments.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Any member that revoked partitions then rejoins the group, triggering a second rebalance so that its revoked partitions can be assigned. Until then, these partitions are unowned and unassigned.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;These are the logs from the application that uses&#160;&lt;tt&gt;protocol=&apos;cooperative-sticky&apos;&lt;/tt&gt;. In the same rebalance cycle (&lt;tt&gt;generationId=640&lt;/tt&gt;)&#160;&lt;tt&gt;partition 74&lt;/tt&gt;&#160;moves from&#160;&lt;tt&gt;consumer-3&lt;/tt&gt;&#160;to&#160;&lt;tt&gt;consumer-4&lt;/tt&gt;. I omitted the lines that are logged by the other 4 consumers.&lt;/p&gt;

&lt;p&gt;Mind that the log is in reverse(bottom to top)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2022-12-14 11:18:24 1 &#8212; [consumer-3] x.y.z.MyRebalanceHandler1 : New partition assignment: partition-59, seek to min common offset: 85120524
2022-12-14 11:18:24 1 &#8212; [consumer-3] x.y.z.MyRebalanceHandler2 : Partitions [partition-59] assigned successfully
2022-12-14 11:18:24 1 &#8212; [consumer-3] x.y.z.MyRebalanceHandler1 : Partitions assigned: [partition-59]
2022-12-14 11:18:24 1 &#8212; [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Adding newly assigned partitions: partition-59
2022-12-14 11:18:24 1 &#8212; [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Assignment(partitions=[partition-59])
2022-12-14 11:18:24 1 &#8212; [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Request joining group due to: need to revoke partitions [partition-26, partition-74] as indicated by the current assignment and re-join
2022-12-14 11:18:24 1 &#8212; [consumer-3] x.y.z.MyRebalanceHandler2 : Partitions [partition-26, partition-74] revoked successfully
2022-12-14 11:18:24 1 &#8212; [consumer-3] x.y.z.MyRebalanceHandler1 : Finished removing partition data
2022-12-14 11:18:24 1 &#8212; [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] (Re-)joining group
2022-12-14 11:18:24 1 &#8212; [consumer-4] x.y.z.MyRebalanceHandler1 : New partition assignment: partition-74, seek to min common offset: 107317730
2022-12-14 11:18:24 1 &#8212; [consumer-4] x.y.z.MyRebalanceHandler2 : Partitions [partition-74] assigned successfully
2022-12-14 11:18:24 1 &#8212; [consumer-4] x.y.z.MyRebalanceHandler1 : Partitions assigned: [partition-74]
2022-12-14 11:18:24 1 &#8212; [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Adding newly assigned partitions: partition-74
2022-12-14 11:18:24 1 &#8212; [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Assignment(partitions=[partition-74])
2022-12-14 11:18:24 1 &#8212; [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Request joining group due to: need to revoke partitions [partition-57] as indicated by the current assignment and re-join
2022-12-14 11:18:24 1 &#8212; [consumer-4] x.y.z.MyRebalanceHandler2 : Partitions [partition-57] revoked successfully
2022-12-14 11:18:24 1 &#8212; [consumer-4] x.y.z.MyRebalanceHandler1 : Finished removing partition data
2022-12-14 11:18:22 1 &#8212; [consumer-3] x.y.z.MyRebalanceHandler1 : Partitions revoked: [partition-26, partition-74]
2022-12-14 11:18:22 1 &#8212; [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Revoke previously assigned partitions partition-26, partition-74
2022-12-14 11:18:22 1 &#8212; [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-59]\n\tCurrent owned partitions: [partition-26, partition-74]\n\tAdded partitions (assigned - owned): [partition-59]\n\tRevoked partitions (owned - assigned): [partition-26, partition-74]
2022-12-14 11:18:22 1 &#8212; [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Successfully synced group in generation Generation{generationId=640, memberId=&lt;span class=&quot;code-quote&quot;&gt;&apos;partition-3-my-client-id-my-group-id-c31afd19-3f22-43cb-ad07-9088aa98d3af&apos;&lt;/span&gt;, protocol=&lt;span class=&quot;code-quote&quot;&gt;&apos;cooperative-sticky&apos;&lt;/span&gt;}
2022-12-14 11:18:22 1 &#8212; [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Successfully joined group with generation Generation{generationId=640, memberId=&lt;span class=&quot;code-quote&quot;&gt;&apos;partition-3-my-client-id-my-group-id-c31afd19-3f22-43cb-ad07-9088aa98d3af&apos;&lt;/span&gt;, protocol=&lt;span class=&quot;code-quote&quot;&gt;&apos;cooperative-sticky&apos;&lt;/span&gt;}
2022-12-14 11:18:22 1 &#8212; [consumer-4] x.y.z.MyRebalanceHandler1 : Partitions revoked: [partition-57]
2022-12-14 11:18:22 1 &#8212; [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Revoke previously assigned partitions partition-57
2022-12-14 11:18:22 1 &#8212; [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-74]\n\tCurrent owned partitions: [partition-57]\n\tAdded partitions (assigned - owned): [partition-74]\n\tRevoked partitions (owned - assigned): [partition-57]
2022-12-14 11:18:21 1 &#8212; [id-1] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Successfully synced group in generation Generation{generationId=640, memberId=&lt;span class=&quot;code-quote&quot;&gt;&apos;partition-4-my-client-id-my-group-id-ae2af665-edc9-4a8e-b658-98372d142477&apos;&lt;/span&gt;, protocol=&lt;span class=&quot;code-quote&quot;&gt;&apos;cooperative-sticky&apos;&lt;/span&gt;}
2022-12-14 11:18:21 1 &#8212; [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Successfully joined group with generation Generation{generationId=640, memberId=&lt;span class=&quot;code-quote&quot;&gt;&apos;partition-4-my-client-id-my-group-id-ae2af665-edc9-4a8e-b658-98372d142477&apos;&lt;/span&gt;, protocol=&lt;span class=&quot;code-quote&quot;&gt;&apos;cooperative-sticky&apos;&lt;/span&gt;} &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Is this expected?&lt;/p&gt;

&lt;p&gt;Kafka client version is 3.2.1.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13520189">KAFKA-14639</key>
            <summary>Kafka CooperativeStickyAssignor revokes/assigns partition in one rebalance cycle</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="pnee">Philip Nee</assignee>
                                    <reporter username="bojanblagojevic">Bojan Blagojevic</reporter>
                        <labels>
                    </labels>
                <created>Thu, 19 Jan 2023 11:51:17 +0000</created>
                <updated>Tue, 2 May 2023 11:48:08 +0000</updated>
                            <resolved>Fri, 28 Apr 2023 09:17:03 +0000</resolved>
                                    <version>3.2.1</version>
                                    <fixVersion>3.4.1</fixVersion>
                    <fixVersion>3.5.0</fixVersion>
                                    <component>clients</component>
                    <component>consumer</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="17678981" author="ableegoldman" created="Fri, 20 Jan 2023 04:27:19 +0000"  >&lt;p&gt;There&apos;s definitely something off, it almost looks like consumer-3 actually missed the gen 640 rebalance altogether &#8211; which would definitely explain the symptom you describe, as partition 74 would be free to assign to consumer-4 if consumer-3 didn&apos;t join the group in time to report partition-74 among its &quot;ownedPartitions&quot;.&#160;&lt;/p&gt;

&lt;p&gt;Reading from the bottom up, the first 3 lines show consumer-4 going through the entire rebalance and getting partition-74 assigned, all before consumer-3 even joins the group in gen 640. So that all adds up so far. What seems odd to me is that we then see consumer-3 appear to &quot;successfully join the group&quot; AND &quot;successfully sync the group&quot; for that same generation, which should not be the case if it missed the rebalance entirely &#8211; it should fail on JoinGroup if it truly did not manage to join the gen 640 rebalance in time, and certainly should fail the SyncGroup. So that does seem weird to me&lt;/p&gt;

&lt;p&gt;If you&apos;re able to reproduce this, could you try doing so with DEBUG logging enabled? In the meantime, I might be able to make more sense of what&apos;s going on if you can upload the full logs over this period of time&lt;/p&gt;</comment>
                            <comment id="17679899" author="JIRAUSER298710" created="Mon, 23 Jan 2023 17:09:55 +0000"  >&lt;p&gt;Thank you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; for the quick response.&lt;br/&gt;
&#160;&lt;br/&gt;
I just noticed that I forgot to add some details about the problem. 6 consumers that were mentioned were running on one of the 15 pods in Kubernetes cluster. Each pod spawns 6 consumers. Topic that is being consumed has 96 partitions.&lt;br/&gt;
&#160;&lt;br/&gt;
I only saw this behaviour once, on our production environment and I am not able to reproduce it. After the issue happened the consumers were reconfigured back to the previously used `&lt;tt&gt;RoundRobinAssignor&lt;/tt&gt;`.&lt;br/&gt;
&#160;&#160;&lt;br/&gt;
I have the logs from the `&lt;tt&gt;ConsumerCoordinator&lt;/tt&gt;` that did the assignment of the partitions. Log contains problematic generation 640 and three more that came before it. As in the first log excerpt I filtered out other consumers:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&quot;2022-12-14 11:18:11 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group at generation 638: {
consumer-4=Assignment(partitions=[partition-57]),
consumer-0=Assignment(partitions=[partition-2, partition-93]),
consumer-1=Assignment(partitions=[partition-10, partition-58]),
consumer-3=Assignment(partitions=[partition-26, partition-74]),
consumer-5=Assignment(partitions=[partition-65]),
consumer-2=Assignment(partitions=[partition-34]),
}&quot;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:18:11 1 --- [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-26, partition-74]\n\tCurrent owned partitions: [partition-26, partition-74]\n\tAdded partitions (assigned - owned): []\n\tRevoked partitions (owned - assigned): []&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:18:11 1 --- [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Assignment(partitions=[partition-26, partition-74])&quot;&lt;/span&gt;
&quot;2022-12-14 11:18:16 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group at generation 639: {
...
consumer-4=Assignment(partitions=[partition-57]),
consumer-0=Assignment(partitions=[partition-2, partition-93]),
consumer-1=Assignment(partitions=[partition-24]),
consumer-3=Assignment(partitions=[partition-26, partition-74]),
consumer-5=Assignment(partitions=[partition-88]),
consumer-2=Assignment(partitions=[partition-34]),
...
}&quot;
&quot;2022-12-14 11:18:21 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group at generation 640: {
...
consumer-4=Assignment(partitions=[partition-74]),
consumer-0=Assignment(partitions=[partition-2, partition-93]),
consumer-1=Assignment(partitions=[partition-21]),
consumer-3=Assignment(partitions=[partition-59]),
consumer-5=Assignment(partitions=[partition-82]),
consumer-2=Assignment(partitions=[partition-38]),
...
}&quot; 
&quot;2022-12-14 11:18:28   1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group at generation 641: {
...
consumer-4=Assignment(partitions=[partition-74]), 
consumer-0=Assignment(partitions=[partition-2, partition-93]), 
consumer-1=Assignment(partitions=[partition-21]), 
consumer-3=Assignment(partitions=[partition-59]), 
consumer-5=Assignment(partitions=[partition-82]), 
consumer-2=Assignment(partitions=[partition-38]), 
...
}&quot;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It seems that in the same generation partition-74 changed ownership from consumer-3 to consumer-4.&lt;br/&gt;
&#160;&lt;br/&gt;
I would expect:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;generation 640 -&amp;gt; consumer-3 looses partition-74&lt;/li&gt;
	&lt;li&gt;generation 641 -&amp;gt; consumer-4 acquires partition-74&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17680141" author="ableegoldman" created="Tue, 24 Jan 2023 08:41:11 +0000"  >&lt;p&gt;Thanks for the additional logs, that does indeed verify that both consumers participated in the same rebalance. My next guess would be that for some reason, consumer-3 had &quot;lost&quot; its partitions prior to joining the group in gen 640, in which case they would be allowed to be freely given away to another consumer in that same generation. Can you check and/or provide all the logs from consumer-3 between gen 639 and gen 640? Is there anything in there about resetting the generation, dropping out of the group, resetting the member id, anything at all like that?&#160;&lt;/p&gt;

&lt;p&gt;I also notice that the assignment changes drastically between gen 639 and 640, it&apos;s not &quot;sticky&quot; at all which should have been easy for the assignor to do if the previous assignment was something relatively simple like each consumer claiming exactly one or two partitions (only) and all from the same topic. Something fishy is definitely going on.&lt;/p&gt;

&lt;p&gt;The only other thing off the top of my head to check would be that every single consumer was configured with (only) the CooperativeStickyAssignor over the full period from gen 639 through the end of gen 640, or at least check the group leader (consumer-5 I think?) and consumers 3 &amp;amp; 4.&lt;/p&gt;

&lt;p&gt;I&apos;ll take a look at the assignor logic and see if anything jumps out at me on my end, but I have to say the complete lack of stickiness in the assignment from 639 to 640 is fairly perplexing and something I have never seen before with the CooperativeStickyAssignor. There have been some recents bugs related to rebalancing edge cases that have been fixed over the past few versions, so I may go back over those and see if anything was messed up by them. I did in fact discover one bug affecting rebalancing/assignment in the past few months which had been introduced by that series of fixes, so perhaps there is another.&lt;/p&gt;</comment>
                            <comment id="17680275" author="JIRAUSER298710" created="Tue, 24 Jan 2023 16:28:12 +0000"  >&lt;p&gt;Thank you again for the quick response. I will try to answer to your questions.&lt;/p&gt;
&lt;h5&gt;&lt;a name=&quot;Answers%3A&quot;&gt;&lt;/a&gt;Answers:&lt;/h5&gt;
&lt;p&gt;&lt;cite&gt;1. Can you check and/or provide all the logs from consumer-3 between gen 639 and gen 640? Is there anything in there about resetting the generation, dropping out of the group, resetting the member id, anything at all like that&lt;/cite&gt;?&lt;br/&gt;
I don&apos;t see nothing like that happening between gen 639 and gen 640. I attached the log excerpt related to few surrounding generations &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/13054790/13054790_consumers-jira.log&quot; title=&quot;consumers-jira.log attached to KAFKA-14639&quot;&gt;consumers-jira.log&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;2. The only other thing off the top of my head to check would be that every single consumer was configured with (only) the CooperativeStickyAssignor over the full period from gen 639 through the end of gen 640, or at least check the group leader (consumer-5 I think?) and consumers 3 &amp;amp; 4.&lt;/cite&gt;&lt;br/&gt;
The full logs are unfortunately expired but I am pretty sure that all the consumers were configured with only `CooperativeStickyAssignor`. They are part of Kuberenetes deployment in which all the pods share the configuration. I observed correct group leader behaviour when changing ownership of other partitions. I followed ownership changes when partition &lt;b&gt;partition-68&lt;/b&gt; is moved.&lt;br/&gt;
It belonged to the consumer partition-2-6b9db8686f-hswvn... in generation 639.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Final assignment of partitions to consumers:
partition-2-6b9db8686f-hswvn-bbcfa7e4-7d5b-4227-ad62-99e8cc6e176f=[partition-20, partition-68]
...
Finished assignment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group at generation 639:
partition-2-6b9db8686f-hswvn-bbcfa7e4-7d5b-4227-ad62-99e8cc6e176f=Assignment(partitions=[partition-20, partition-68]) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Between generation 639 and generation 640 new pod joins, pod-6b9db8686f-p87m9. One of the Kafka consumers that belongs to this pod, partition-3-6b9db8686f-p87m9..., in generation 640 gets the partition-68 as assigned and it is logged in AbstractStickyAssignor.constrainedAssign.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Final assignment of partitions to consumers:
partition-3-6b9db8686f-p87m9-737d9359-daa5-4d89-9e4c-40a433aa8c6c=[partition-68]&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Since this partition is changing ownership, it does not show up in the log of ConsumerCoordinator, which is expected.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Finished assignment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group at generation 640:
partition-3-6b9db8686f-p87m9-737d9359-daa5-4d89-9e4c-40a433aa8c6c=Assignment(partitions=[])&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And it gets assigned in generation 641:&lt;br/&gt;
&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Final assignment of partitions to consumers: partition-3-6b9db8686f-psfx4-5dd12c98-e698-44aa-9131-56281e798369=[partition-68] 
... 
Finished assignment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; group at generation 641: partition-3-6b9db8686f-psfx4-5dd12c98-e698-44aa-9131-56281e798369=Assignment(partitions=[partition-68])&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It gets assigned to a consumer which is different than the one determined in generation 640 but this does not break the rebalance barrier.&lt;/p&gt;
&lt;h5&gt;&lt;a name=&quot;Additionalnotes&quot;&gt;&lt;/a&gt;Additional notes&lt;/h5&gt;
&lt;p&gt;Not sure if it matters. I saw a several consumers logging:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
... org.apache.kafka.clients.Metadata ... Resetting the last seen epoch of partition partition-74 to 149 since the associated topicId changed from &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; to nixqUZnpQYWjY0RreaCczA&quot; &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;br/&gt;
I think that the HB thread(I am assuming this based on the &lt;a href=&quot;https://github.com/apache/kafka/blob/3.2.1/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1193&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;code I&apos;ve read&lt;/a&gt;) is requesting join on behalf of consumers. Again, not sure if it matters:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:48 1 --- [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:48 1 --- [consumer-2] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-2-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:48 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:48 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-2-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:47 1 --- [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:47 1 --- [consumer-1] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-1-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:47 1 --- [consumer-0] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-0-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:46 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-0-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:46 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:45 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:45 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-1-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing&quot;&lt;/span&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;2022-12-14 11:17:45 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing&quot;&lt;/span&gt; &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17704864" author="JIRAUSER283568" created="Sat, 25 Mar 2023 03:48:36 +0000"  >&lt;p&gt;A few issues I see...&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;why did consumer-4 revoke partition-57? Aren&apos;t partitions supposed to be sticky?&lt;/li&gt;
	&lt;li&gt;There are a lot of sync group failures, which makes me wonder if it is an exact problem described in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-13891&quot; title=&quot;sync group failed with rebalanceInProgress error cause rebalance many rounds in coopeartive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-13891&quot;&gt;&lt;del&gt;KAFKA-13891&lt;/del&gt;&lt;/a&gt; - the pod might be a slightly slower in sending out the sync group, and therefore result in out-of-date owned partitions.&lt;/li&gt;
	&lt;li&gt;The timing seems off, consumer-3 completed to join/sync after consumer-4 completed the revocation. Not sure if this is an issue with the logging.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;It seems like a lot of folks agree on making assignor better at handling partition ownership, across multiple generations, but no one seems to be implementing the fix.&lt;/p&gt;</comment>
                            <comment id="17705731" author="guozhang" created="Mon, 27 Mar 2023 23:29:57 +0000"  >&lt;p&gt;I looked at both JIRA tickets and I believe the root cause is &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-13891&quot; title=&quot;sync group failed with rebalanceInProgress error cause rebalance many rounds in coopeartive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-13891&quot;&gt;&lt;del&gt;KAFKA-13891&lt;/del&gt;&lt;/a&gt;, and this ticket is just a symptom of that issue. More specifically, I think there are two common scenarios here, and just to use the example&apos;s names for illustration purposes only:&lt;/p&gt;

&lt;p&gt;Scenario 1:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;in generation 639, partition-74 is &quot;decided&quot; to go to consumer-3 in the join-group. Note here I used the word &quot;decided&quot; since the assignment only completes after the Sync-group round trip. So this assignment is only logged on the leader, but consumer-3 has not learned about this new assignment in the sync-group yet.&lt;/li&gt;
	&lt;li&gt;then before the sync-group finishes, a new member joins (or more generally, some other event happens, such as what &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-13891&quot; title=&quot;sync group failed with rebalanceInProgress error cause rebalance many rounds in coopeartive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-13891&quot;&gt;&lt;del&gt;KAFKA-13891&lt;/del&gt;&lt;/a&gt; describes: some member received the sync-group earlier than others, and immediately sends another re-join group to trigger a new rebalance). The server-side group coordinator bumps up the generation to 640 for the new rebalance.&lt;/li&gt;
	&lt;li&gt;consumer-3 sends the sync-group for generation 639 and gets a REBALANCE_IN_PROGRESS, and hence it needs to re-join the group without knowing partition-74 was given to it in generation 639. In this case consumer-3 has never officially &quot;owned&quot; partition-74.&lt;/li&gt;
	&lt;li&gt;in generation 640, partition-74 is decided to consumer-4.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;From the callback&apos;s perspective, consumer-3 would not trigger the `onPartitionAssigned`, nor `onPartitionRevoked` for partition-74, since the assignment in generation 639 got obsoleted before it even gets notified and hence it does not ever get that partition ever. But this should still be okay since whoever previously owned partition-74 prior to generation 639, should still invoke the `onPartitionRevoked` for partition-74.&lt;/p&gt;

&lt;p&gt;Scenario 2 (pretty much what &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-13891&quot; title=&quot;sync group failed with rebalanceInProgress error cause rebalance many rounds in coopeartive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-13891&quot;&gt;&lt;del&gt;KAFKA-13891&lt;/del&gt;&lt;/a&gt;&apos;s description well stated):&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;prior to generation 639, partition-74 is already owned by consumer-3 and consumer-3 knows about it. The rebalance of generation 639 did not change that ownership, i.e. it&apos;s still assigned to consumer-3.&lt;/li&gt;
	&lt;li&gt;then before the sync-group finishes, a new member joins (or more generally, some other event happens, such as what &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-13891&quot; title=&quot;sync group failed with rebalanceInProgress error cause rebalance many rounds in coopeartive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-13891&quot;&gt;&lt;del&gt;KAFKA-13891&lt;/del&gt;&lt;/a&gt; describes: some member received the sync-group earlier than others, and immediately sends another re-join group to trigger a new rebalance). The server-side group coordinator bumps up the generation to 640 for the new rebalance.&lt;/li&gt;
	&lt;li&gt;consumer-3 sends the sync-group for generation 639 and gets a REBALANCE_IN_PROGRESS, and hence it needs to re-join the group with generation reset. In this case, its owned partitions are discarded by the broker-side coordinator since its generation is literally &quot;-1&quot;.&lt;/li&gt;
	&lt;li&gt;in generation 640, partition-74 is decided to consumer-4 with the perception that no one owns this partition in that generation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In this case, the `onPartitionRevoked` would be triggered on consumer 3 and `onPartitionAssigned` on consumer 4 at the same time, which is a bad case.&lt;/p&gt;</comment>
                            <comment id="17705734" author="guozhang" created="Mon, 27 Mar 2023 23:48:12 +0000"  >&lt;p&gt;As for the fix.. it&apos;s quite tough, since the ultimate and best solution is the new rebalance protocol KIP, and if we want to have a near-term fix, it has to be only touching on the client side, not the server side. That means the suggestions on the other ticket cannot apply.&lt;/p&gt;

&lt;p&gt;The trickiness is that, when a sync-group request got a REBALANCE_IN_PROGRESS error for generation X, it means that (let&apos;s take the conversion case of COORDINATOR_LOAD_IN_PROGRESS out for the moment) the consumer did not actually miss a rebalance, it&apos;s just that the next generation X+1 has started; HOWEVER, its current owned partitions is not from generation X, but from an older version, say X-1 (since it did not get the actual assignment for generation X), and hence when it sends out that owned partitions of generation X-1, it is likely to be discarded by the coordinators. Without fixing the broker-side, this trickiness is hard to resolve.&lt;/p&gt;

&lt;p&gt;I&apos;d suggest we coordinate with the folks working on the KIP (a.k.a. on the broker-side) for the fix at the near term. If everyone agrees this is a good issue to be remedied before the KIP is out, I&apos;d suggest we took the broker-change, a.k.a. the description in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-14016&quot; title=&quot;Revoke more partitions than expected in Cooperative rebalance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-14016&quot;&gt;&lt;del&gt;KAFKA-14016&lt;/del&gt;&lt;/a&gt; by Shawn. &lt;/p&gt;

&lt;p&gt;If that&apos;s considered a too short fix given the KIP, another fix could be, that on the assignor side, do not discard the old generation&apos;s owned partitions if those partitions are NOT claimed by anyone else &amp;#8212; admittedly this is a risky (think about a partition owned by host A in generation X, and then assigned to B in generation X+1 which host A missed, and then host B missed generation X+2 in which case it&apos;s given back to A, in which case we may see that partitions consuming offsets &quot;going backwards&quot;) fix but may give us the runway until the new rebalance protocol is out. Personally I do not like it and would only consider as a last resort.&lt;/p&gt;</comment>
                            <comment id="17706162" author="guozhang" created="Tue, 28 Mar 2023 22:13:21 +0000"  >&lt;p&gt;Had some chat with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pnee&quot; class=&quot;user-hover&quot; rel=&quot;pnee&quot;&gt;pnee&lt;/a&gt; today regarding the client-side fix, I think a more conservative fix may work, which is to only honor the old generation&apos;s owned partitions if 1) it&apos;s the only participant claiming to own this partition, 2) it&apos;s generation is no smaller than the current generation X minus 1. If its generation is X-2 or less, then the above mentioned risk would materialize.&lt;/p&gt;

&lt;p&gt;That being said, I still feel that the ultimate fix should touch on the broker side, and hence would better be for the new rebalance protocol KIP.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="13054790" name="consumers-jira.log" size="48143" author="bojanblagojevic" created="Tue, 24 Jan 2023 16:27:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 32 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1f5u0:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>dajac</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>