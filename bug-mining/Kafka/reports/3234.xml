<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:30:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-10888]  Sticky partition leads to uneven product msg, resulting in abnormal delays in some partitions</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-10888</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;&#160; 110 producers ,550 partitions ,550 consumers , 5 nodes Kafka cluster&lt;br/&gt;
&#160; The producer uses the nullkey+stick partitioner, the total production rate is about 100w tps&lt;br/&gt;
Observed partition delay is abnormal and message distribution is uneven, which leads to the maximum production and consumption delay of the partition with more messages &lt;br/&gt;
abnormal.&lt;/p&gt;

&lt;p&gt;&#160; I cannot find reason that stick will make the message distribution uneven at this production rate.&lt;br/&gt;
&#160; I can&apos;t switch to the round-robin partitioner, which will increase the delay and cpu cost. Is thathe stick partationer design cause uneven message distribution, or this is abnormal. How to solve it?&lt;/p&gt;

&lt;p&gt;&#160; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13017614/13017614_image-2020-12-24-21-09-47-692.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As shown in the picture, the uneven distribution is concentrated on some partitions and some brokers, there seems to be some rules.&lt;/p&gt;

&lt;p&gt;This problem does not only occur in one cluster, but in many high tps clusters,&lt;/p&gt;

&lt;p&gt;The problem is more obvious on the test cluster we built.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13017613/13017613_image-2020-12-24-21-10-24-407.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13347755">KAFKA-10888</key>
            <summary> Sticky partition leads to uneven product msg, resulting in abnormal delays in some partitions</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="alivshits">Artem Livshits</assignee>
                                    <reporter username="jr981008">jr</reporter>
                        <labels>
                    </labels>
                <created>Thu, 24 Dec 2020 13:12:21 +0000</created>
                <updated>Tue, 12 Jul 2022 21:07:34 +0000</updated>
                            <resolved>Fri, 6 May 2022 18:33:36 +0000</resolved>
                                    <version>2.4.1</version>
                                    <fixVersion>3.3.0</fixVersion>
                                    <component>clients</component>
                    <component>producer </component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="17254596" author="junrao" created="Thu, 24 Dec 2020 17:11:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jr981008&quot; class=&quot;user-hover&quot; rel=&quot;jr981008&quot;&gt;jr981008&lt;/a&gt;: Thanks for reporting this. There are a couple of threads in the dev mailing list that might be related to what&apos;s described in this jira.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/rae8d2d5587dae57ad9093a85181e0cb4256f10d1e57138ecdb3ef287%40%3Cdev.kafka.apache.org%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://lists.apache.org/thread.html/rae8d2d5587dae57ad9093a85181e0cb4256f10d1e57138ecdb3ef287%40%3Cdev.kafka.apache.org%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://lists.apache.org/list.html?dev@kafka.apache.org:lte=1M:kip-693&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://lists.apache.org/list.html?dev@kafka.apache.org:lte=1M:kip-693&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17285383" author="hachikuji" created="Tue, 16 Feb 2021 17:59:52 +0000"  >&lt;p&gt;We have found one cause of imbalance when the sticky partitioner is used. Basically the intuition behind the sticky partitioner breaks down a little bit when a small `linger.ms` is in use (sadly this is the default). The user is opting out of batching with this setting which means there is often little opportunity to fill batches before they get drained and sent. That leaves the door open to sustained imbalance in some cases.&lt;/p&gt;

&lt;p&gt;To see why, suppose that we have a producer writing to 3 partitions with linger.ms=0 and one partition slows down a little bit for some reason. It could be a leader change or some transient network issue. The producer will have to hold onto the batches for that partition until it becomes available. While it is holding onto those batches, additional batches will begin piling up. Each of these batches is likely to get filled because the producer is not ready to send to this partition yet.&lt;/p&gt;

&lt;p&gt;Consider this from the perspective of the sticky partitioner. Every time the slow partition gets selected, the producer will fill the batches completely. On the other hand, the remaining &quot;fast&quot; partitions will likely not get their batches filled because of the `linger.ms=0` setting. As soon as a single record is available, it might get sent. So more data ends up getting written to the partition that has already started to build a backlog. And even after the cause of the original slowness (e.g. leader change) gets resolved, it might take some time for this imbalance to recover. We believe this can even create a runaway effect if the partition cannot catch up with the handicap of the additional load.&lt;/p&gt;

&lt;p&gt;We analyzed one case where we thought this might be going on. Below I&apos;ve summarized the writes over a period of one hour to 3 partitions. Partition 0 here is the &quot;slow&quot; partition. All partitions get roughly the same number of batches, but the slow partition has much bigger batch sizes.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Partition TotalBatches TotalBytes TotalRecords BytesPerBatch RecordsPerBatch
0         1683         25953200   25228        15420.80      14.99        
1         1713         7836878    4622         4574.94       2.70
2         1711         7546212    4381         4410.41       2.56
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After restarting the application, the producer was healthy again. It just was not able to recover with the imbalanced workload.&lt;/p&gt;</comment>
                            <comment id="17286039" author="junrao" created="Wed, 17 Feb 2021 18:03:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;: Thanks for the analysis. One thing that led to this behavior is that sticky partitioner tries to distribute the data to different partitions in batches and the batches sometimes are not even in size as you pointed out. So, one way of fixing this is to have the sticky partitioner distribute the data in units that are always equal (e.g., a fixed amount of bytes based on batch size).&lt;/p&gt;</comment>
                            <comment id="17286089" author="hachikuji" created="Wed, 17 Feb 2021 19:03:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; That&apos;s a good observation. The partitioner knows the approximate size of each batch that gets sent, so it could take that into account before rotating partitions. We were thinking of approaches which involved the partitioner being aware of how many bytes were inflight, but propagating this information from the accumulator to the partitioner is kind of messy. &lt;/p&gt;

&lt;p&gt;Perhaps it is simpler for the partitioner to track at the level of total bytes sent to each partition. Maybe we could see this analogously to the tcp window size. For example, say you start with an expected size of 10 records (or 1k bytes or whatever). The partitioner keeps writing to the partition until this many records/bytes have been sent regardless of the batching. If the batch is not filled when the limit is reached, then we move onto the next partition, but we increase the batch size. On the other hand, if a batch gets sent before the limit is reached, we continue writing to the partition until the limit is reached and we decrease it when we move on to the next partition. In this way, we can keep a better balance between partitions.&lt;/p&gt;</comment>
                            <comment id="17391446" author="showuon" created="Mon, 2 Aug 2021 08:46:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;, I was thinking about your suggestion above, to be analogous to the tcp window size implementation. If I understand it correctly, this example should explain what your suggestion:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;topic A with partition 3:&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Suppose we use default `batch.size` and `linger.ms` setting (16K bytes, 0ms), and on average, we can send 2k bytes for each batch, and we set the default window size of 1k bytes. So,&lt;/p&gt;

&lt;p&gt;1st batch, we&apos;ll have: (window size = 1k)&lt;/p&gt;

&lt;p&gt;partition A-0, 1k bytes (reach the limit, so move to next partition, and increase to 2k bytes window size (suppose +/- 1k each time))&lt;/p&gt;

&lt;p&gt;partition A-1, 1k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;2nd batch, we&apos;ll have: (window size = 2k)&lt;/p&gt;

&lt;p&gt;partition A-2, 2k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;so far, we&apos;ll have&lt;/p&gt;

&lt;p&gt;partition A-0: 1k bytes&lt;/p&gt;

&lt;p&gt;partition A-1: 1k bytes&lt;/p&gt;

&lt;p&gt;partition A-2: 2k bytes&lt;/p&gt;

&lt;p&gt;and keep going.&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;Suppose partition A-0 slows down in next batch, with 6k bytes be sent:&lt;/p&gt;

&lt;p&gt;4th batch (window size = 2k) ,&lt;/p&gt;

&lt;p&gt;partition A-0, 2k bytes (reach the limit, so move to next partition, and increase to 2k bytes window size, so 4k bytes now )&lt;/p&gt;

&lt;p&gt;partition A-1, 4k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;5th batch (window size = 4k)&lt;/p&gt;

&lt;p&gt;partition A-2, 2k bytes (not reach the limit, keep sending to partition A-2 in next batch, decrease the window size to 2k bytes )&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;so far, we&apos;ll have&lt;/p&gt;

&lt;p&gt;partition A-0: 3k bytes&lt;/p&gt;

&lt;p&gt;partition A-1: 5k bytes&lt;/p&gt;

&lt;p&gt;partition A-2: 4k bytes&lt;/p&gt;

&lt;p&gt;and keep going.&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;I think this proposal might still cause uneven distribution as above example showed. Also, it will send to 2 or more batches in some cases(ex: 1st batch in above example) , which was originally sent to 1 batch only (and it&apos;s the spirit of the sticky partitioner, to stick to a partition before batch full to improve throughput).&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;So, I&apos;m proposing a way to make it evenly distribution and still keep the original sticky partitioner spirit: check the distribution status when all partitions are sent 1 batch. And have a threshold to see if we want to skip the exceeding partition in the following rounds, and how many rounds it should be skipped. Using the above example:&lt;/p&gt;

&lt;p&gt;==1st round, same result as using original sticky partitioner==&lt;/p&gt;

&lt;p&gt;1st batch&lt;/p&gt;

&lt;p&gt;partition A-0, 2k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;2nd batch&lt;/p&gt;

&lt;p&gt;partition A-1, 2k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;3rd batch:&lt;/p&gt;

&lt;p&gt;partition A-2, 2k bytes&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;so far, we&apos;ll have&lt;/p&gt;

&lt;p&gt;partition A-0: 2k bytes&lt;/p&gt;

&lt;p&gt;partition A-1: 2k bytes&lt;/p&gt;

&lt;p&gt;partition A-2: 2k bytes&lt;/p&gt;

&lt;p&gt;After all partitions sent 1 batch, we check if there&apos;s any partition batch size in this round is exceeding other partitions more than the threshold (ex: 70%), here, no, so keeps going&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;===2nd round, still have the same result as using original sticky partitioner===&lt;/p&gt;

&lt;p&gt;Suppose partition A-0 slows down in next batch, with 6k bytes be sent:&lt;/p&gt;

&lt;p&gt;4th batch&lt;/p&gt;

&lt;p&gt;partition A-0, 6k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;5th batch&lt;/p&gt;

&lt;p&gt;partition A-1, 2k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;6th batch&lt;/p&gt;

&lt;p&gt;partition A-2, 2k bytes&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;in this round, we&apos;ll have&lt;/p&gt;

&lt;p&gt;partition A-0: 6k bytes&lt;/p&gt;

&lt;p&gt;partition A-1: 2k bytes&lt;/p&gt;

&lt;p&gt;partition A-2: 2k bytes&lt;/p&gt;

&lt;p&gt;After this round, we check if there&apos;s any partition batch size in this round is exceeding other partitions more than the threshold (ex: 70%), here, we have partition A-0 exceeding with 4k bytes, and compute how many rounds it should be skipped: 4k / (2k * 0.7) = 2.8 =&amp;gt; only care the integer part, 2. So, partition A-0 should be skipped 2 rounds&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;So, we can imagine, after 3rd and 4th rounds, partition A-0 is skipped, we&apos;ll have balanced messages sent&lt;/p&gt;

&lt;p&gt;partition A-0: 6k bytes&lt;/p&gt;

&lt;p&gt;partition A-1: 6k bytes&lt;/p&gt;

&lt;p&gt;partition A-2: 6k bytes&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;

&lt;p&gt;(Sorry for the long response) Thank you.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17458258" author="JIRAUSER281546" created="Mon, 13 Dec 2021 10:25:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=showuon&quot; class=&quot;user-hover&quot; rel=&quot;showuon&quot;&gt;showuon&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;I propose a solution based on keeping the&#160; track of number of offsets/messages written to each partition.&#160;&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;At partitioner level, Store the number of offsets/messages written to each partition. (sizePerPartitionMap) and total(total offsets for a topic)&lt;/li&gt;
	&lt;li&gt;Before choosing next batch to write(onNextBatch()) . Use (sizePerPartitionMap) to blacklisting the available partitions which causes skewness(USING a configurable THRESHOLD %) . Choose next partition from list of available whitelisted partitions.&#160;&lt;/li&gt;
	&lt;li&gt;To configure sizePerPartitionMap. Use the callback method of producer.send() to update the sizePerPartitionMap and total .&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This way, we can skip slower partitions(blacklisted) for few rounds and ensure all partitions are roughly of equal size.&#160;&lt;/p&gt;</comment>
                            <comment id="17459675" author="showuon" created="Wed, 15 Dec 2021 06:26:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nk2242696%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;nk2242696@gmail.com&quot;&gt;nk2242696@gmail.com&lt;/a&gt; , for your information, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=alivshits&quot; class=&quot;user-hover&quot; rel=&quot;alivshits&quot;&gt;alivshits&lt;/a&gt; is working on &#160;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-794%3A+Strictly+Uniform+Sticky+Partitioner&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-794%3A+Strictly+Uniform+Sticky+Partitioner&lt;/a&gt; to fix this issue. Thanks.&lt;/p&gt;</comment>
                            <comment id="17533028" author="junrao" created="Fri, 6 May 2022 18:33:36 +0000"  >&lt;p&gt;merged the PR to trunk. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=alivshits&quot; class=&quot;user-hover&quot; rel=&quot;alivshits&quot;&gt;alivshits&lt;/a&gt; for the design, implementation and the testing.&lt;/p&gt;</comment>
                            <comment id="17561256" author="zhangzs" created="Fri, 1 Jul 2022 03:41:45 +0000"  >&lt;p&gt;goog job&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="13417022">KAFKA-13540</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                            <outwardlinks description="causes">
                                        <issuelink>
            <issuekey id="13468347">KAFKA-14020</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13017615" name="image-2020-12-24-21-05-02-800.png" size="96390" author="jr981008" created="Thu, 24 Dec 2020 13:06:42 +0000"/>
                            <attachment id="13017614" name="image-2020-12-24-21-09-47-692.png" size="66318" author="jr981008" created="Thu, 24 Dec 2020 13:11:26 +0000"/>
                            <attachment id="13017613" name="image-2020-12-24-21-10-24-407.png" size="50739" author="jr981008" created="Thu, 24 Dec 2020 13:12:03 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 19 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0lt7k:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>