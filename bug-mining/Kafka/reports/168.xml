<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:36:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-664] Kafka server threads die due to OOME during long running test</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-664</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;I set up a Kafka cluster with 5 brokers (JVM memory 512M) and set up a long running producer process that sends data to 100s of partitions continuously for ~15 hours. After ~4 hours of operation, few server threads (acceptor and processor) exited due to OOME -&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:24:44,355&amp;#93;&lt;/span&gt; ERROR OOME with size 1700161893 (kafka.network.BoundedByteBufferReceive)&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:24:44,356&amp;#93;&lt;/span&gt; ERROR Uncaught exception in thread &apos;kafka-acceptor&apos;: (kafka.utils.Utils$)&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:24:44,356&amp;#93;&lt;/span&gt; ERROR Uncaught exception in thread &apos;kafka-processor-9092-1&apos;: (kafka.utils.Utils$)&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:24:46,344&amp;#93;&lt;/span&gt; INFO Unable to reconnect to ZooKeeper service, session 0x13afd0753870103 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:24:46,344&amp;#93;&lt;/span&gt; INFO zookeeper state changed (Expired) (org.I0Itec.zkclient.ZkClient)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:24:46,344&amp;#93;&lt;/span&gt; INFO Initiating client connection, connectString=eat1-app309.corp:12913,eat1-app310.corp:12913,eat1-app311.corp:12913,eat1-app312.corp:12913,eat1-app313.corp:12913 sessionTimeout=15000 watcher=org.I0Itec.zkclient.ZkClient@19202d69 (org.apache.zookeeper.ZooKeeper)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:24:55,702&amp;#93;&lt;/span&gt; ERROR OOME with size 2001040997 (kafka.network.BoundedByteBufferReceive)&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:25:01,192&amp;#93;&lt;/span&gt; ERROR Uncaught exception in thread &apos;kafka-request-handler-0&apos;: (kafka.utils.Utils$)&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:25:08,739&amp;#93;&lt;/span&gt; INFO Opening socket connection to server eat1-app311.corp/172.20.72.75:12913 (org.apache.zookeeper.ClientCnxn)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:25:14,221&amp;#93;&lt;/span&gt; INFO Socket connection established to eat1-app311.corp/172.20.72.75:12913, initiating session (org.apache.zookeeper.ClientCnxn)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:25:17,943&amp;#93;&lt;/span&gt; INFO Client session timed out, have not heard from server in 3722ms for sessionid 0x0, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:25:19,805&amp;#93;&lt;/span&gt; ERROR error in loggedRunnable (kafka.utils.Utils$)&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 08:25:23,528&amp;#93;&lt;/span&gt; ERROR OOME with size 1853095936 (kafka.network.BoundedByteBufferReceive)&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;/p&gt;


&lt;p&gt;It seems like it runs out of memory while trying to read the producer request, but its unclear so far. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12622938">KAFKA-664</key>
            <summary>Kafka server threads die due to OOME during long running test</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jkreps">Jay Kreps</assignee>
                                    <reporter username="nehanarkhede">Neha Narkhede</reporter>
                        <labels>
                            <label>bugs</label>
                    </labels>
                <created>Fri, 7 Dec 2012 18:07:04 +0000</created>
                <updated>Wed, 6 Jan 2016 19:49:51 +0000</updated>
                            <resolved>Tue, 18 Dec 2012 00:42:57 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="13526587" author="nehanarkhede" created="Fri, 7 Dec 2012 18:08:47 +0000"  >&lt;p&gt;Attaching a thread dump that shows -&lt;/p&gt;

&lt;p&gt;1. 4 processor threads and the acceptor threads are dead&lt;br/&gt;
2. Rest of the processor threads have a full request queue, and they are waiting to add to the request queue.&lt;/p&gt;</comment>
                            <comment id="13526590" author="nehanarkhede" created="Fri, 7 Dec 2012 18:12:38 +0000"  >&lt;p&gt;Another observation - The server is probably GCing quite a lot, since I see the following in the server logs -&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-12-07 09:32:14,742&amp;#93;&lt;/span&gt; INFO Client session timed out, have not heard from server in 1204905ms for sessionid 0x23afd074d6600ea, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)&lt;/p&gt;

&lt;p&gt;The zookeeper session timeout is pretty high (15secs) and it is in the same DC as the Kafka cluster and the producer&lt;/p&gt;</comment>
                            <comment id="13526713" author="jkreps" created="Fri, 7 Dec 2012 19:46:25 +0000"  >&lt;p&gt;One pain of oom is that the thing leaking the memory is not necessarily the thing that gets the exception. Can you rerun with -XX:+HeapDumpOnOutOfMemoryError&lt;/p&gt;</comment>
                            <comment id="13526719" author="nehanarkhede" created="Fri, 7 Dec 2012 19:51:02 +0000"  >&lt;p&gt;Heap dump is here - &lt;a href=&quot;http://people.apache.org/~nehanarkhede/kafka-misc/kafka-0.8/heap-dump.tar.gz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://people.apache.org/~nehanarkhede/kafka-misc/kafka-0.8/heap-dump.tar.gz&lt;/a&gt;&lt;br/&gt;
Almost all the largest objects trace back to RequestPurgatory$ExpiredRequestReaper as the GC root.&lt;/p&gt;</comment>
                            <comment id="13526720" author="nehanarkhede" created="Fri, 7 Dec 2012 19:51:34 +0000"  >&lt;p&gt;I&apos;m re-running the tests with that option now&lt;/p&gt;</comment>
                            <comment id="13526805" author="jkreps" created="Fri, 7 Dec 2012 21:46:19 +0000"  >&lt;p&gt;Looks like the problem is in request purgatory--watchers aren&apos;t getting removed.&lt;/p&gt;</comment>
                            <comment id="13526808" author="nehanarkhede" created="Fri, 7 Dec 2012 21:47:19 +0000"  >&lt;p&gt;The root cause seems to be that watchersForKey map keeps growing. I see that we add keys to the map, but never actually delete them.&lt;/p&gt;</comment>
                            <comment id="13526876" author="jjkoshy" created="Fri, 7 Dec 2012 23:31:37 +0000"  >&lt;p&gt;To clarify, the map itself shouldn&apos;t grow indefinitely right? - i.e., if there are no new partitions the number of keys should be the same. I think the issue is that expired requests (for a key) are not removed from the list of outstanding requests for that key.&lt;/p&gt;</comment>
                            <comment id="13526883" author="jjkoshy" created="Fri, 7 Dec 2012 23:38:35 +0000"  >&lt;p&gt;Okay I&apos;m slightly confused. Even on expiration the request is marked as satisfied. So even if it is not removed from the watcher&apos;s list during expiration it will be removed on the next call to collectSatisfiedRequests - which in this case will be when the next produce request arrives to that partition. Which means this should only be due to low-volume partitions that are no longer growing. i.e., the replica fetcher would keep issuing fetch requests that keep expiring but never get removed from the list of pending requests in watchersFor(the-low-volume-partition).&lt;/p&gt;</comment>
                            <comment id="13526887" author="jkreps" created="Fri, 7 Dec 2012 23:44:39 +0000"  >&lt;p&gt;Another issue is that we are saving the full producer request in memory for as long as it is in purgatory. Not sure that is causing this, but that is pretty bad.&lt;/p&gt;</comment>
                            <comment id="13526973" author="nehanarkhede" created="Sat, 8 Dec 2012 01:55:27 +0000"  >&lt;p&gt;The problem was ever increasing requests in the watchersForKey map. Please look at the graph attached. In merely 40 minutes of running the broker, the number of requests in the purgatory map shot upto 4 million.&lt;br/&gt;
This can happen for very low volume topics since the replica fetcher requests keep entering this map, and since there are no more produce requests coming for those topics/partitions, no one ever removes those requests from the map. &lt;/p&gt;

&lt;p&gt;With Joel&apos;s help, hacked RequestPurgatory to force the cleanup of expired/satisfied requests by the expiry thread inside purgeSatisfied. Of course, a better solution is re-designing the purgatory data structure to point from the queue to the map, but that is a bigger change. I just want to get around this issue and continue performance testing.&lt;/p&gt;</comment>
                            <comment id="13527039" author="jjkoshy" created="Sat, 8 Dec 2012 05:19:38 +0000"  >&lt;p&gt;+1 &lt;/p&gt;

&lt;p&gt;Some minor comments:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We can probably remove the WatchersForKey gauge (or maybe keep it until the RequestPurgatory refactoring is done).&lt;/li&gt;
	&lt;li&gt;While I agree we should definitely refactor the RequestPurgatory to fix the inefficient scan, I think this approach is not as hacky as it sounds. i.e., on fetch request expiration, this now does what would have been done if a produce request to that key had arrived; so we can consider the overhead of this approach as sending additional produce requests to the affected partition at the rate of fetch expirations (which by default is 2/sec). We can optimize a bit more, by adding a threshold for cleanup. i.e., do the iteration and check/removal only if watchers.requests.size &amp;gt; threshold.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13527592" author="nehanarkhede" created="Sun, 9 Dec 2012 19:33:05 +0000"  >&lt;p&gt;Joel,&lt;/p&gt;

&lt;p&gt;Thanks for the quick review. I wasn&apos;t intending to checkin the draft patch, just threw it up here for people to take a look and give feedback. I&apos;m not sure I understand the purgatory enough to checkin the right fix, mostly interested in crossing this hurdle and proceed with perf testing &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Having said that, I tested the draft patch and as you can see from the attached screenshots, it doesn&apos;t seem to solve the problem. The memory consumption due to the requests in the watchersFor map and the corresponding GC activity is very high and keeps on increasing. &lt;/p&gt;</comment>
                            <comment id="13528217" author="jjkoshy" created="Mon, 10 Dec 2012 19:50:49 +0000"  >&lt;p&gt;Can you try this instead? The update should run as requests expire, not in the purge step - which only runs when a cleanup threshold is reached.&lt;/p&gt;</comment>
                            <comment id="13528429" author="nehanarkhede" created="Mon, 10 Dec 2012 23:30:51 +0000"  >&lt;p&gt;Joel, patch v2 works better and I see why v1 didn&apos;t work out. Thanks for the quick turnaround !&lt;/p&gt;</comment>
                            <comment id="13528707" author="junrao" created="Tue, 11 Dec 2012 06:08:35 +0000"  >&lt;p&gt;If the problem is due to an expired request not being removed from the request LinkedList in the watcher, then there should be at most 1 such outstanding request per topic/partition. So, if the number of topic/partition is fixed, the memory space taken by those outstanding requests should be bounded too, right? Not sure why this causes memory usage to keep going up.&lt;/p&gt;</comment>
                            <comment id="13528713" author="nehanarkhede" created="Tue, 11 Dec 2012 06:17:00 +0000"  >&lt;p&gt;For the current implementation, that is true only for the DelayQueue but not for the watchersForKey map. The memory consumption keeps on increasing because we never circle around to cleaning up the entries in the map. When entries are satisfied/expired, they only exit the queue and are &quot;marked&quot; as expired/satisfied.&lt;/p&gt;</comment>
                            <comment id="13528723" author="junrao" created="Tue, 11 Dec 2012 06:34:16 +0000"  >&lt;p&gt;Got it. So the issue is that for low volume topics, the fetch requests made by the followers keep got timed out. Those timeouted requests won&apos;t be removed from the request LinkedList in the watcher until the next produce request for that topic comes, which could be a long time.&lt;/p&gt;</comment>
                            <comment id="13528733" author="nehanarkhede" created="Tue, 11 Dec 2012 06:44:15 +0000"  >&lt;p&gt;That&apos;s correct. I&apos;m tempted to checkin v2 for now and wait for the purgatory refactor patch. Until then, we can probably keep the JIRA open. Thoughts ?&lt;/p&gt;</comment>
                            <comment id="13528948" author="jkreps" created="Tue, 11 Dec 2012 13:17:18 +0000"  >&lt;p&gt;This seems to iterate the whole list for every expiration. That seems a bit excessive, no? What about just doing a &quot;full clean&quot; every 100 expirations or something like that...?&lt;/p&gt;

&lt;p&gt;I agree we should do something like this rather than attempt a rewrite.&lt;/p&gt;</comment>
                            <comment id="13529313" author="jjkoshy" created="Tue, 11 Dec 2012 20:28:58 +0000"  >&lt;p&gt;Agreed that we should checkin this fix + the throttling (forgot to add that in v2) and open a separate jira to refactor the purgatory a bit.&lt;/p&gt;</comment>
                            <comment id="13529489" author="jjkoshy" created="Tue, 11 Dec 2012 23:42:52 +0000"  >&lt;p&gt;One problem with the updated patch is the following: multi-fetch requests could be satisfied (since minBytes is 1).&lt;br/&gt;
i.e., the request will be marked as satisfied and the expiration code path will not be executed. We should do the update&lt;br/&gt;
on expiration even if the request has been satisfied.&lt;/p&gt;

&lt;p&gt;So an additional &quot;catch-all&quot; that may make life easier for us is to have a global threshold of the requestsFor map - i.e.,&lt;br/&gt;
if its size exceeds a threshold (which will be checked on both expiration/checkSatisfied) then trigger a full cleanup - i.e.,&lt;br/&gt;
iterate over all entries in watchersFor and remove those that are satisfied.&lt;/p&gt;</comment>
                            <comment id="13530229" author="jjkoshy" created="Wed, 12 Dec 2012 19:20:32 +0000"  >&lt;p&gt;Here is a simpler and hopefully safer approach of using a global request counter and purging both the delay queue and watcher map. Also, I changed the existing gauge to just count the number of requests in purgatory (i.e., over both the delay queue and the map).&lt;/p&gt;</comment>
                            <comment id="13530254" author="jjkoshy" created="Wed, 12 Dec 2012 19:51:19 +0000"  >&lt;p&gt;Also, just filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-671&quot; title=&quot;DelayedProduce requests should not hold full producer request data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-671&quot;&gt;&lt;del&gt;KAFKA-671&lt;/del&gt;&lt;/a&gt; to address the issue that Jay pointed out.&lt;/p&gt;</comment>
                            <comment id="13530709" author="junrao" created="Thu, 13 Dec 2012 05:53:13 +0000"  >&lt;p&gt;Thanks for patch v3. +1 from me. One minor comment: Should we make CleanupInterval configurable?&lt;/p&gt;</comment>
                            <comment id="13531337" author="nehanarkhede" created="Thu, 13 Dec 2012 19:03:15 +0000"  >&lt;p&gt;Thanks for patch v3, Joel ! It works correctly, here are a few review suggestions -&lt;/p&gt;

&lt;p&gt;1. Can we bring back the exact count of delayed requests sitting in the purgatory ? Your patch removed that, added a metric for the total size of purgatory, which is good to have as well. But its inaccuracy is inversely proportional to the time till the next cleanup interval.&lt;br/&gt;
2. It seems like the cleanup is happening on the request handler thread. Can it happen on the background expiry reaper thread instead ? I understand the latency impact is minimal, but it still seems like a good idea to put extra processing on a background thread, which we already have&lt;br/&gt;
3. CleanupThreshold can be hard coded in the future. Until then, for performance tuning, it will be very convenient to have it configurable &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                            <comment id="13531344" author="jkreps" created="Thu, 13 Dec 2012 19:13:33 +0000"  >&lt;p&gt;+1 on doing cleanup in the expiry thread--expiry is a lot less latency sensitive.&lt;/p&gt;</comment>
                            <comment id="13532802" author="jjkoshy" created="Fri, 14 Dec 2012 23:50:03 +0000"  >&lt;p&gt;All good points - here is v4 with those changes.&lt;/p&gt;</comment>
                            <comment id="13534388" author="nehanarkhede" created="Mon, 17 Dec 2012 22:40:33 +0000"  >&lt;p&gt;+1 on v4&lt;/p&gt;</comment>
                            <comment id="13534468" author="jjkoshy" created="Tue, 18 Dec 2012 00:42:57 +0000"  >&lt;p&gt;Committed on 0.8&lt;/p&gt;</comment>
                            <comment id="15085389" author="bhaskarv82@gmail.com" created="Wed, 6 Jan 2016 10:42:16 +0000"  >&lt;p&gt;I am seeing similar errors on Kafka which is running on version 0.8.2.&lt;/p&gt;</comment>
                            <comment id="15085427" author="ijuma" created="Wed, 6 Jan 2016 11:25:54 +0000"  >&lt;p&gt;Vijay, you should probably file a new issue and include as much information as possible. Also, it would be interesting to know if this still happens with 0.9.0.0 (if possible).&lt;/p&gt;</comment>
                            <comment id="15086165" author="bhaskarv82@gmail.com" created="Wed, 6 Jan 2016 19:49:51 +0000"  >&lt;p&gt;Thanks for the suggestion, Ismael.&lt;br/&gt;
I have created &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3071&quot; title=&quot;Kafka Server 0.8.2 ERROR OOME with siz&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3071&quot;&gt;&lt;del&gt;KAFKA-3071&lt;/del&gt;&lt;/a&gt; ID.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12560617" name="KAFKA-664-v3.patch" size="6711" author="jjkoshy" created="Wed, 12 Dec 2012 19:20:32 +0000"/>
                            <attachment id="12561061" name="KAFKA-664-v4.patch" size="11910" author="jjkoshy" created="Fri, 14 Dec 2012 23:50:03 +0000"/>
                            <attachment id="12560112" name="Screen Shot 2012-12-09 at 11.22.50 AM.png" size="43587" author="nehanarkhede" created="Sun, 9 Dec 2012 19:33:05 +0000"/>
                            <attachment id="12560113" name="Screen Shot 2012-12-09 at 11.23.09 AM.png" size="36972" author="nehanarkhede" created="Sun, 9 Dec 2012 19:33:05 +0000"/>
                            <attachment id="12560114" name="Screen Shot 2012-12-09 at 11.31.29 AM.png" size="26869" author="nehanarkhede" created="Sun, 9 Dec 2012 19:33:05 +0000"/>
                            <attachment id="12560258" name="kafka-664-draft-2.patch" size="1997" author="jjkoshy" created="Mon, 10 Dec 2012 19:50:49 +0000"/>
                            <attachment id="12560004" name="kafka-664-draft.patch" size="1575" author="nehanarkhede" created="Sat, 8 Dec 2012 01:55:27 +0000"/>
                            <attachment id="12559911" name="thread-dump.log" size="20247" author="nehanarkhede" created="Fri, 7 Dec 2012 18:08:47 +0000"/>
                            <attachment id="12560005" name="watchersForKey.png" size="9119" author="nehanarkhede" created="Sat, 8 Dec 2012 01:55:27 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>296525</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 45 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i149yn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>233077</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>