<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:16:31 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-2334] Prevent HW from going back during leader failover </title>
                <link>https://issues.apache.org/jira/browse/KAFKA-2334</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Consider the following scenario:&lt;/p&gt;

&lt;p&gt;0. Kafka use replication factor of 2, with broker B1 as the leader, and B2 as the follower. &lt;br/&gt;
1. A producer keep sending to Kafka with ack=-1.&lt;br/&gt;
2. A consumer repeat issuing ListOffset request to Kafka.&lt;/p&gt;

&lt;p&gt;And the following sequence:&lt;/p&gt;

&lt;p&gt;0. B1 current log-end-offset (LEO) 0, HW-offset 0; and same with B2.&lt;br/&gt;
1. B1 receive a ProduceRequest of 100 messages, append to local log (LEO becomes 100) and hold the request in purgatory.&lt;br/&gt;
2. B1 receive a FetchRequest starting at offset 0 from follower B2, and returns the 100 messages.&lt;br/&gt;
3. B2 append its received message to local log (LEO becomes 100).&lt;br/&gt;
4. B1 receive another FetchRequest starting at offset 100 from B2, knowing that B2&apos;s LEO has caught up to 100, and hence update its own HW, and satisfying the ProduceRequest in purgatory, and sending the FetchResponse with HW 100 back to B2 ASYNCHRONOUSLY.&lt;br/&gt;
5. B1 successfully sends the ProduceResponse to the producer, and then fails, hence the FetchResponse did not reach B2, whose HW remains 0.&lt;/p&gt;

&lt;p&gt;From the consumer&apos;s point of view, it could first see the latest offset of 100 (from B1), and then see the latest offset of 0 (from B2), and then the latest offset gradually catch up to 100.&lt;/p&gt;

&lt;p&gt;This is because we use HW to guard the ListOffset and Fetch-from-ordinary-consumer.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12845078">KAFKA-2334</key>
            <summary>Prevent HW from going back during leader failover </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mumrah">David Arthur</assignee>
                                    <reporter username="guozhang">Guozhang Wang</reporter>
                        <labels>
                            <label>reliability</label>
                    </labels>
                <created>Tue, 14 Jul 2015 21:03:46 +0000</created>
                <updated>Tue, 8 Jan 2019 21:05:13 +0000</updated>
                            <resolved>Fri, 14 Dec 2018 21:55:15 +0000</resolved>
                                    <version>0.8.2.1</version>
                                    <fixVersion>2.2.0</fixVersion>
                                    <component>replication</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="14627040" author="guozhang" created="Tue, 14 Jul 2015 21:06:04 +0000"  >&lt;p&gt;One possible solution to this issue is to let the new leader only become available (i.e. start accepting Produce / Fetch requests for the partition) after its HW caught up with its LEO. This will likely increase the unavailability latency a bit, in practice it should not cause much performance implication since most of the time its HW == LEO, and even not it will quickly catch up. The tricky part is how to implement it without introducing too much logic complexity on the broker side.&lt;/p&gt;</comment>
                            <comment id="15009364" author="fullung" created="Tue, 17 Nov 2015 19:47:14 +0000"  >&lt;p&gt;Hitting this issue in production with 0.8.2.1 on a large cluster. Fix appreciated. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15022117" author="jinxing6042@126.com" created="Mon, 23 Nov 2015 13:59:50 +0000"  >&lt;p&gt;after receiving LeaderAndIsrRequest, Broker B2 will finally call &quot; Partition::makeLeader&quot;, part of code is as below:&lt;br/&gt;
...&lt;br/&gt;
     zkVersion = leaderAndIsr.zkVersion&lt;br/&gt;
      leaderReplicaIdOpt = Some(localBrokerId)&lt;br/&gt;
      // construct the high watermark metadata for the new leader replica&lt;br/&gt;
      val newLeaderReplica = getReplica().get&lt;br/&gt;
      newLeaderReplica.convertHWToLocalOffsetMetadata()&lt;br/&gt;
      // reset log end offset for remote replicas&lt;br/&gt;
      assignedReplicas.foreach(r =&amp;gt; if (r.brokerId != localBrokerId) r.logEndOffset = LogOffsetMetadata.UnknownOffsetMetadata)&lt;br/&gt;
      // we may need to increment high watermark since ISR could be down to 1&lt;br/&gt;
      maybeIncrementLeaderHW(newLeaderReplica)&lt;br/&gt;
      if (topic == OffsetManager.OffsetsTopicName)&lt;br/&gt;
        offsetManager.loadOffsetsFromLog(partitionId)&lt;br/&gt;
...&lt;br/&gt;
I can tell Broker B2 will first set &apos;leaderReplicaIdOpt = Some(localBrokerId)&apos;, and then try to update high watermark;&lt;br/&gt;
by setting leaderReplicaIdOpt, Broker B2 will be available for consumer(if the consumer send fetchReqeust, there will be no NotLeaderForPartitionException);&lt;br/&gt;
In the short interval which after &apos;leaderReplicaIdOpt = Some(localBrokerId)&apos; and before setting up hw, what the consumer get is the &quot;gone back&quot; hw;&lt;br/&gt;
If my understanding is wright, just reverse the order of setting up leaderReplicaIdOpt and updating high watermark will fix this issue;&lt;br/&gt;
am I wrong ?&lt;/p&gt;</comment>
                            <comment id="15032642" author="guozhang" created="Mon, 30 Nov 2015 22:46:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jinxing6042%40126.com&quot; class=&quot;user-hover&quot; rel=&quot;jinxing6042@126.com&quot;&gt;jinxing6042@126.com&lt;/a&gt; The problem with this scenario is not related to LeaderAndISRRequest but in follower&apos;s FetchResponse, which will make the follower to update HW according to the returned value:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala#L120&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala#L120&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If the Fetch Response is not returned from the previous leader, then this follower will not update its HW and when itself becoming the new leader, the HW will effectively &quot;go back&quot; from the consumer point of view.&lt;/p&gt;</comment>
                            <comment id="16030772" author="ijuma" created="Wed, 31 May 2017 07:28:51 +0000"  >&lt;p&gt;Is this still an issue?&lt;/p&gt;</comment>
                            <comment id="16341941" author="guozhang" created="Sat, 27 Jan 2018 04:21:18 +0000"  >&lt;p&gt;This is fixed in another approach: KIP-101.&lt;/p&gt;</comment>
                            <comment id="16347426" author="cmccabe" created="Wed, 31 Jan 2018 19:18:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;: The fix here is not KIP-101, but KIP-207.  KIP-207 was accepted, but not implemented yet.  There&apos;s some discussion here: &lt;a href=&quot;https://www.mail-archive.com/dev@kafka.apache.org/msg81074.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.mail-archive.com/dev@kafka.apache.org/msg81074.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16532067" author="elevy" created="Tue, 3 Jul 2018 23:33:59 +0000"  >&lt;p&gt;I think we may&#160;hit this in our cluster while running a Flink application. &#160;One broker became disconnected from ZK but recovered in 20 seconds or so. &#160;But during this period a single subtask in a Flink job consuming from Kafka came to believe that its fetch offset was out-of-range:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;June 30th 2018, 09:35:01.711 Fetch offset 2340400514 is out of range for partition topic-124, resetting offset&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As the Kafka consumer was configured with &lt;tt&gt;auto.offsets.reset&lt;/tt&gt; set to &lt;tt&gt;earliest&lt;/tt&gt;, that caused that single partition to be reprocessed from the earliest offset.&lt;/p&gt;</comment>
                            <comment id="16532094" author="guozhang" created="Wed, 4 Jul 2018 00:40:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cmccabe&quot; class=&quot;user-hover&quot; rel=&quot;cmccabe&quot;&gt;cmccabe&lt;/a&gt; Is KIP-207 already implemented? I saw the vote passes and it is targeted for 2.0.0 but did not saw the corresponding PR.&lt;/p&gt;</comment>
                            <comment id="16707611" author="githubbot" created="Mon, 3 Dec 2018 18:11:02 +0000"  >&lt;p&gt;mumrah opened a new pull request #5991: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2334&quot; title=&quot;Prevent HW from going back during leader failover &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2334&quot;&gt;&lt;del&gt;KAFKA-2334&lt;/del&gt;&lt;/a&gt; Guard against non-monotonic offsets in the client&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5991&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5991&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   After a recent leader election, the leaders high-water mark might lag behind the offset at the beginning of the new epoch (as well as the previous leader&apos;s HW). This can lead to offsets going backwards from a client perspective, which is confusing and leads to strange behavior in some clients.&lt;/p&gt;

&lt;p&gt;   This change causes Partition#fetchOffsetForTimestamp to throw an exception to indicate the offsets are not yet available from the leader. For new clients, a new OFFSET_NOT_AVAILABLE error is added. For existing clients, a LEADER_NOT_AVAILABLE is thrown.&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16721816" author="githubbot" created="Fri, 14 Dec 2018 21:53:06 +0000"  >&lt;p&gt;hachikuji closed pull request #5991: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2334&quot; title=&quot;Prevent HW from going back during leader failover &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2334&quot;&gt;&lt;del&gt;KAFKA-2334&lt;/del&gt;&lt;/a&gt; Guard against non-monotonic offsets in the client&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5991&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5991&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index 265fc99721d..180fbba7d04 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -811,7 +811,9 @@ private void handleListOffsetResponse(Map&amp;lt;TopicPartition, ListOffsetRequest.Part&lt;br/&gt;
                         &quot;is before 0.10.0&quot;, topicPartition);&lt;br/&gt;
             } else if (error == Errors.NOT_LEADER_FOR_PARTITION ||&lt;br/&gt;
                        error == Errors.REPLICA_NOT_AVAILABLE ||&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;error == Errors.KAFKA_STORAGE_ERROR) {&lt;br/&gt;
+                       error == Errors.KAFKA_STORAGE_ERROR ||&lt;br/&gt;
+                       error == Errors.OFFSET_NOT_AVAILABLE ||&lt;br/&gt;
+                       error == Errors.LEADER_NOT_AVAILABLE) {&lt;br/&gt;
                 log.debug(&quot;Attempt to fetch offsets for partition {} failed due to {}, retrying.&quot;,&lt;br/&gt;
                         topicPartition, error);&lt;br/&gt;
                 partitionsToRetry.add(topicPartition);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/errors/OffsetNotAvailableException.java b/clients/src/main/java/org/apache/kafka/common/errors/OffsetNotAvailableException.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..97de3b3d64e
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/errors/OffsetNotAvailableException.java&lt;br/&gt;
@@ -0,0 +1,29 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.common.errors;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates that the leader is not able to guarantee monotonically increasing offsets&lt;br/&gt;
+ * due to the high watermark lagging behind the epoch start offset after a recent leader election&lt;br/&gt;
+ */&lt;br/&gt;
+public class OffsetNotAvailableException extends RetriableException 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    private static final long serialVersionUID = 1L;++    public OffsetNotAvailableException(String message) {
+        super(message);
+    }+}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java b/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java&lt;br/&gt;
index bd0815df538..51a78f578e8 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java&lt;br/&gt;
@@ -66,6 +66,7 @@&lt;br/&gt;
 import org.apache.kafka.common.errors.NotEnoughReplicasException;&lt;br/&gt;
 import org.apache.kafka.common.errors.NotLeaderForPartitionException;&lt;br/&gt;
 import org.apache.kafka.common.errors.OffsetMetadataTooLarge;&lt;br/&gt;
+import org.apache.kafka.common.errors.OffsetNotAvailableException;&lt;br/&gt;
 import org.apache.kafka.common.errors.OffsetOutOfRangeException;&lt;br/&gt;
 import org.apache.kafka.common.errors.OperationNotAttemptedException;&lt;br/&gt;
 import org.apache.kafka.common.errors.OutOfOrderSequenceException;&lt;br/&gt;
@@ -290,7 +291,10 @@&lt;br/&gt;
     UNSUPPORTED_COMPRESSION_TYPE(76, &quot;The requesting client does not support the compression type of given partition.&quot;,&lt;br/&gt;
             UnsupportedCompressionTypeException::new),&lt;br/&gt;
     STALE_BROKER_EPOCH(77, &quot;Broker epoch has changed&quot;,&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;StaleBrokerEpochException::new);&lt;br/&gt;
+            StaleBrokerEpochException::new),&lt;br/&gt;
+    OFFSET_NOT_AVAILABLE(78, &quot;The leader high watermark has not caught up from a recent leader &quot; +&lt;br/&gt;
+            &quot;election so the offsets cannot be guaranteed to be monotonically increasing&quot;,&lt;br/&gt;
+            OffsetNotAvailableException::new);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(Errors.class);&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java&lt;br/&gt;
index 5107c4e9140..e9fe942e858 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java&lt;br/&gt;
@@ -118,9 +118,12 @@&lt;br/&gt;
             ISOLATION_LEVEL,&lt;br/&gt;
             TOPICS_V4);&lt;/p&gt;

&lt;p&gt;+    // V5 bump to include new possible error code (OFFSET_NOT_AVAILABLE)&lt;br/&gt;
+    private static final Schema LIST_OFFSET_REQUEST_V5 = LIST_OFFSET_REQUEST_V4;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{LIST_OFFSET_REQUEST_V0, LIST_OFFSET_REQUEST_V1, LIST_OFFSET_REQUEST_V2,
-            LIST_OFFSET_REQUEST_V3, LIST_OFFSET_REQUEST_V4}
&lt;p&gt;;&lt;br/&gt;
+            LIST_OFFSET_REQUEST_V3, LIST_OFFSET_REQUEST_V4, LIST_OFFSET_REQUEST_V5};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private final int replicaId;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java&lt;br/&gt;
index 188571b7002..769c850e22c 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java&lt;br/&gt;
@@ -52,6 +52,8 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;- 
{@link Errors#UNKNOWN_TOPIC_OR_PARTITION}
&lt;p&gt; If the broker does not have metadata for a topic or partition&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;- 
{@link Errors#KAFKA_STORAGE_ERROR}
&lt;p&gt; If the log directory for one of the requested partitions is offline&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;- 
{@link Errors#UNKNOWN_SERVER_ERROR}
&lt;p&gt; For any unexpected errors&lt;br/&gt;
+ * - &lt;/p&gt;
{@link Errors#LEADER_NOT_AVAILABLE}
&lt;p&gt; The leader&apos;s HW has not caught up after recent election (v4 protocol)&lt;br/&gt;
+ * - &lt;/p&gt;
{@link Errors#OFFSET_NOT_AVAILABLE}
&lt;p&gt; The leader&apos;s HW has not caught up after recent election (v5+ protocol)&lt;br/&gt;
  */&lt;br/&gt;
 public class ListOffsetResponse extends AbstractResponse {&lt;br/&gt;
     public static final long UNKNOWN_TIMESTAMP = -1L;&lt;br/&gt;
@@ -125,9 +127,11 @@&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;br/&gt;
             TOPICS_V4);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private static final Schema LIST_OFFSET_RESPONSE_V5 = LIST_OFFSET_RESPONSE_V4;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{LIST_OFFSET_RESPONSE_V0, LIST_OFFSET_RESPONSE_V1, LIST_OFFSET_RESPONSE_V2,
-            LIST_OFFSET_RESPONSE_V3, LIST_OFFSET_RESPONSE_V4}
&lt;p&gt;;&lt;br/&gt;
+            LIST_OFFSET_RESPONSE_V3, LIST_OFFSET_RESPONSE_V4, LIST_OFFSET_RESPONSE_V5};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static final class PartitionData {&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 52b78e3de62..134cbed6e48 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -1126,6 +1126,45 @@ public void testUpdateFetchPositionResetToLatestOffset() &lt;/p&gt;
{
         assertEquals(5, subscriptions.position(tp0).longValue());
     }

&lt;p&gt;+    /**&lt;br/&gt;
+     * Make sure the client behaves appropriately when receiving an exception for unavailable offsets&lt;br/&gt;
+     */&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void testFetchOffsetErrors() &lt;/p&gt;
{
+        subscriptions.assignFromUser(singleton(tp0));
+        subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);
+
+        // Fail with OFFSET_NOT_AVAILABLE
+        client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP),
+                listOffsetResponse(Errors.OFFSET_NOT_AVAILABLE, 1L, 5L), false);
+        fetcher.resetOffsetsIfNeeded();
+        consumerClient.pollNoWakeup();
+        assertFalse(subscriptions.hasValidPosition(tp0));
+        assertTrue(subscriptions.isOffsetResetNeeded(tp0));
+        assertFalse(subscriptions.isFetchable(tp0));
+
+        // Fail with LEADER_NOT_AVAILABLE
+        time.sleep(retryBackoffMs);
+        client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP),
+                listOffsetResponse(Errors.LEADER_NOT_AVAILABLE, 1L, 5L), false);
+        fetcher.resetOffsetsIfNeeded();
+        consumerClient.pollNoWakeup();
+        assertFalse(subscriptions.hasValidPosition(tp0));
+        assertTrue(subscriptions.isOffsetResetNeeded(tp0));
+        assertFalse(subscriptions.isFetchable(tp0));
+
+        // Back to normal
+        time.sleep(retryBackoffMs);
+        client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP),
+                listOffsetResponse(Errors.NONE, 1L, 5L), false);
+        fetcher.resetOffsetsIfNeeded();
+        consumerClient.pollNoWakeup();
+        assertTrue(subscriptions.hasValidPosition(tp0));
+        assertFalse(subscriptions.isOffsetResetNeeded(tp0));
+        assertTrue(subscriptions.isFetchable(tp0));
+        assertEquals(subscriptions.position(tp0).longValue(), 5L);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testListOffsetsSendsIsolationLevel() {&lt;br/&gt;
         for (final IsolationLevel isolationLevel : IsolationLevel.values()) {&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/api/ApiVersion.scala b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
index cf360926122..a68bcf0c70b 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
@@ -84,7 +84,9 @@ object ApiVersion {&lt;br/&gt;
     KAFKA_2_1_IV2,&lt;br/&gt;
     // Introduced broker generation (KIP-380), and&lt;br/&gt;
     // LeaderAdnIsrRequest V2, UpdateMetadataRequest V5, StopReplicaRequest V1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KAFKA_2_2_IV0&lt;br/&gt;
+    KAFKA_2_2_IV0,&lt;br/&gt;
+    // New error code for ListOffsets when a new leader is lagging behind former HW (KIP-207)&lt;br/&gt;
+    KAFKA_2_2_IV1&lt;br/&gt;
   )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   // Map keys are the union of the short and full versions&lt;br/&gt;
@@ -289,6 +291,13 @@ case object KAFKA_2_2_IV0 extends DefaultApiVersion &lt;/p&gt;
{
   val id: Int = 20
 }

&lt;p&gt;+case object KAFKA_2_2_IV1 extends DefaultApiVersion &lt;/p&gt;
{
+  val shortVersion: String = &quot;2.2&quot;
+  val subVersion = &quot;IV1&quot;
+  val recordVersion = RecordVersion.V2
+  val id: Int = 21
+}
&lt;p&gt;+&lt;br/&gt;
 object ApiVersionValidator extends Validator {&lt;/p&gt;

&lt;p&gt;   override def ensureValid(name: String, value: Any): Unit = &lt;/p&gt;
{
diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala
index 1f52bd769cf..ca3abbbc973 100755
--- a/core/src/main/scala/kafka/cluster/Partition.scala
+++ b/core/src/main/scala/kafka/cluster/Partition.scala
@@ -817,17 +817,36 @@ class Partition(val topicPartition: TopicPartition,
       case None =&amp;gt; localReplica.logEndOffset.messageOffset
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (timestamp == ListOffsetRequest.LATEST_TIMESTAMP) {&lt;/li&gt;
	&lt;li&gt;Some(new TimestampAndOffset(RecordBatch.NO_TIMESTAMP, lastFetchableOffset, Optional.of(leaderEpoch)))&lt;br/&gt;
+    val epochLogString = if(currentLeaderEpoch.isPresent) {&lt;br/&gt;
+      s&quot;epoch ${currentLeaderEpoch.get}&quot;&lt;br/&gt;
     } else 
{
-      def allowed(timestampOffset: TimestampAndOffset): Boolean =
-        timestamp == ListOffsetRequest.EARLIEST_TIMESTAMP || timestampOffset.offset &amp;lt; lastFetchableOffset
+      &quot;unknown epoch&quot;
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val fetchedOffset = logManager.getLog(topicPartition).flatMap 
{ log =&amp;gt;
-        log.fetchOffsetByTimestamp(timestamp)
-      }
&lt;p&gt;+    // Only consider throwing an error if we get a client request (isolationLevel is defined) and the start offset&lt;br/&gt;
+    // is lagging behind the high watermark&lt;br/&gt;
+    val maybeOffsetsError: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ApiException&amp;#93;&lt;/span&gt; = leaderEpochStartOffsetOpt&lt;br/&gt;
+      .filter(epochStart =&amp;gt; isolationLevel.isDefined &amp;amp;&amp;amp; epochStart &amp;gt; localReplica.highWatermark.messageOffset)&lt;br/&gt;
+      .map(epochStart =&amp;gt; Errors.OFFSET_NOT_AVAILABLE.exception(s&quot;Failed to fetch offsets for &quot; +&lt;br/&gt;
+        s&quot;partition $topicPartition with leader $epochLogString as this partition&apos;s &quot; +&lt;br/&gt;
+        s&quot;high watermark (${localReplica.highWatermark.messageOffset}) is lagging behind the &quot; +&lt;br/&gt;
+        s&quot;start offset from the beginning of this epoch ($epochStart).&quot;))&lt;br/&gt;
+&lt;br/&gt;
+    def getOffsetByTimestamp: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;TimestampAndOffset&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
+      logManager.getLog(topicPartition).flatMap(log =&amp;gt; log.fetchOffsetByTimestamp(timestamp))
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetchedOffset.filter(allowed)&lt;br/&gt;
+    // If we&apos;re in the lagging HW state after a leader election, throw OffsetNotAvailable for &quot;latest&quot; offset&lt;br/&gt;
+    // or for a timestamp lookup that is beyond the last fetchable offset.&lt;br/&gt;
+    timestamp match 
{
+      case ListOffsetRequest.LATEST_TIMESTAMP =&amp;gt;
+        maybeOffsetsError.map(e =&amp;gt; throw e)
+          .orElse(Some(new TimestampAndOffset(RecordBatch.NO_TIMESTAMP, lastFetchableOffset, Optional.of(leaderEpoch))))
+      case ListOffsetRequest.EARLIEST_TIMESTAMP =&amp;gt;
+        getOffsetByTimestamp
+      case _ =&amp;gt;
+        getOffsetByTimestamp.filter(timestampAndOffset =&amp;gt; timestampAndOffset.offset &amp;lt; lastFetchableOffset)
+          .orElse(maybeOffsetsError.map(e =&amp;gt; throw e))
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
index 08d03c751fd..389d59cd4f1 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
@@ -827,9 +827,19 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
           ListOffsetResponse.UNKNOWN_OFFSET,&lt;br/&gt;
           Optional.empty()))&lt;br/&gt;
       } else {&lt;br/&gt;
+&lt;br/&gt;
+        def buildErrorResponse(e: Errors): (TopicPartition, ListOffsetResponse.PartitionData) = &lt;/p&gt;
{
+          (topicPartition, new ListOffsetResponse.PartitionData(
+            e,
+            ListOffsetResponse.UNKNOWN_TIMESTAMP,
+            ListOffsetResponse.UNKNOWN_OFFSET,
+            Optional.empty()))
+        }
&lt;p&gt;+&lt;br/&gt;
         try {&lt;br/&gt;
           val fetchOnlyFromLeader = offsetRequest.replicaId != ListOffsetRequest.DEBUGGING_REPLICA_ID&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val isolationLevelOpt = if (offsetRequest.replicaId == ListOffsetRequest.CONSUMER_REPLICA_ID)&lt;br/&gt;
+          val isClientRequest = offsetRequest.replicaId == ListOffsetRequest.CONSUMER_REPLICA_ID&lt;br/&gt;
+          val isolationLevelOpt = if (isClientRequest)&lt;br/&gt;
             Some(offsetRequest.isolationLevel)&lt;br/&gt;
           else&lt;br/&gt;
             None&lt;br/&gt;
@@ -859,16 +869,19 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
                     _ : UnsupportedForMessageFormatException) =&amp;gt;&lt;br/&gt;
             debug(s&quot;Offset request with correlation id $correlationId from client $clientId on &quot; +&lt;br/&gt;
                 s&quot;partition $topicPartition failed due to ${e.getMessage}&quot;)&lt;/li&gt;
	&lt;li&gt;(topicPartition, new ListOffsetResponse.PartitionData(Errors.forException(e),&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_TIMESTAMP,&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_OFFSET,&lt;/li&gt;
	&lt;li&gt;Optional.empty()))&lt;br/&gt;
+            buildErrorResponse(Errors.forException(e))&lt;br/&gt;
+&lt;br/&gt;
+          // Only V5 and newer ListOffset calls should get OFFSET_NOT_AVAILABLE&lt;br/&gt;
+          case e: OffsetNotAvailableException =&amp;gt;&lt;br/&gt;
+            if(request.header.apiVersion &amp;gt;= 5) 
{
+              buildErrorResponse(Errors.forException(e))
+            }
&lt;p&gt; else &lt;/p&gt;
{
+              buildErrorResponse(Errors.LEADER_NOT_AVAILABLE)
+            }
&lt;p&gt;+&lt;br/&gt;
           case e: Throwable =&amp;gt;&lt;br/&gt;
             error(&quot;Error while responding to offset request&quot;, e)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;(topicPartition, new ListOffsetResponse.PartitionData(Errors.forException(e),&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_TIMESTAMP,&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_OFFSET,&lt;/li&gt;
	&lt;li&gt;Optional.empty()))&lt;br/&gt;
+            buildErrorResponse(Errors.forException(e))&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index 4452d89b494..4a09ebeffb9 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -19,6 +19,7 @@ package kafka.server&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.util.Optional&lt;/p&gt;

&lt;p&gt;+import kafka.api&lt;br/&gt;
 import kafka.api._&lt;br/&gt;
 import kafka.cluster.BrokerEndPoint&lt;br/&gt;
 import kafka.log.LogAppendInfo&lt;br/&gt;
@@ -80,7 +81,8 @@ class ReplicaFetcherThread(name: String,&lt;/p&gt;

&lt;p&gt;   // Visible for testing&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; val listOffsetRequestVersion: Short =&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_1_IV1) 4&lt;br/&gt;
+    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_2_IV1) 5&lt;br/&gt;
+    else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_1_IV1) 4&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV1) 3&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_11_0_IV0) 2&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_10_1_IV2) 1&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala b/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
index 1ffa695f48c..3bd86f55483 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
@@ -17,10 +17,13 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; package kafka.api&lt;/p&gt;

&lt;p&gt;+import org.apache.commons.collections.CollectionUtils&lt;br/&gt;
 import org.apache.kafka.common.record.RecordVersion&lt;br/&gt;
 import org.junit.Test&lt;br/&gt;
 import org.junit.Assert._&lt;/p&gt;

&lt;p&gt;+import scala.collection.JavaConverters&lt;br/&gt;
+&lt;br/&gt;
 class ApiVersionTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
@@ -84,6 +87,21 @@ class ApiVersionTest &lt;/p&gt;
{
     assertEquals(KAFKA_2_1_IV0, ApiVersion(&quot;2.1-IV0&quot;))
     assertEquals(KAFKA_2_1_IV1, ApiVersion(&quot;2.1-IV1&quot;))
     assertEquals(KAFKA_2_1_IV2, ApiVersion(&quot;2.1-IV2&quot;))
+
+    assertEquals(KAFKA_2_2_IV1, ApiVersion(&quot;2.2&quot;))
+    assertEquals(KAFKA_2_2_IV0, ApiVersion(&quot;2.2-IV0&quot;))
+    assertEquals(KAFKA_2_2_IV1, ApiVersion(&quot;2.2-IV1&quot;))
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testApiVersionUniqueIds(): Unit = {&lt;br/&gt;
+    val allIds: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = ApiVersion.allVersions.map(apiVersion =&amp;gt; &lt;/p&gt;
{
+      apiVersion.id
+    }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
+    val uniqueIds: Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = allIds.toSet&lt;br/&gt;
+&lt;br/&gt;
+    assertEquals(allIds.size, uniqueIds.size)&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
index cfaa147f407..4b9f656a5a2 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
@@ -29,7 +29,7 @@ import kafka.server._&lt;br/&gt;
 import kafka.utils.&lt;/p&gt;
{CoreUtils, MockScheduler, MockTime, TestUtils}
&lt;p&gt; import kafka.zk.KafkaZkClient&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import org.apache.kafka.common.errors.ReplicaNotAvailableException&lt;br/&gt;
+import org.apache.kafka.common.errors.&lt;/p&gt;
{ApiException, LeaderNotAvailableException, OffsetNotAvailableException, ReplicaNotAvailableException}
&lt;p&gt; import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
 import org.apache.kafka.common.record.FileRecords.TimestampAndOffset&lt;br/&gt;
@@ -382,6 +382,172 @@ class PartitionTest &lt;/p&gt;
{
     assertEquals(Optional.of(leaderEpoch), timestampAndOffset.leaderEpoch)
   }

&lt;p&gt;+  /**&lt;br/&gt;
+    * This test checks that after a new leader election, we don&apos;t answer any ListOffsetsRequest until&lt;br/&gt;
+    * the HW of the new leader has caught up to its startLogOffset for this epoch. From a client&lt;br/&gt;
+    * perspective this helps guarantee monotonic offsets&lt;br/&gt;
+    *&lt;br/&gt;
+    * @see &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-207%3A+Offsets+returned+by+ListOffsetsResponse+should+be+monotonically+increasing+even+during+a+partition+leader+change&quot;&amp;gt;KIP-207&amp;lt;/a&amp;gt;&lt;br/&gt;
+    */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testMonotonicOffsetsAfterLeaderChange(): Unit = {&lt;br/&gt;
+    val controllerEpoch = 3&lt;br/&gt;
+    val leader = brokerId&lt;br/&gt;
+    val follower1 = brokerId + 1&lt;br/&gt;
+    val follower2 = brokerId + 2&lt;br/&gt;
+    val controllerId = brokerId + 3&lt;br/&gt;
+    val replicas = List&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower1, follower2).asJava&lt;br/&gt;
+    val isr = List&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower2).asJava&lt;br/&gt;
+    val leaderEpoch = 8&lt;br/&gt;
+    val batch1 = TestUtils.records(records = List(&lt;br/&gt;
+      new SimpleRecord(10, &quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+      new SimpleRecord(11,&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)))&lt;br/&gt;
+    val batch2 = TestUtils.records(records = List(new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+      new SimpleRecord(20,&quot;k4&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+      new SimpleRecord(21,&quot;k5&quot;.getBytes, &quot;v3&quot;.getBytes)))&lt;br/&gt;
+    val batch3 = TestUtils.records(records = List(&lt;br/&gt;
+      new SimpleRecord(30,&quot;k6&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+      new SimpleRecord(31,&quot;k7&quot;.getBytes, &quot;v2&quot;.getBytes)))&lt;br/&gt;
+&lt;br/&gt;
+    val partition = Partition(topicPartition, time, replicaManager)&lt;br/&gt;
+    assertTrue(&quot;Expected first makeLeader() to return &apos;leader changed&apos;&quot;,&lt;br/&gt;
+      partition.makeLeader(controllerId, new LeaderAndIsrRequest.PartitionState(controllerEpoch, leader, leaderEpoch, isr, 1, replicas, true), 0))&lt;br/&gt;
+    assertEquals(&quot;Current leader epoch&quot;, leaderEpoch, partition.getLeaderEpoch)&lt;br/&gt;
+    assertEquals(&quot;ISR&quot;, Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower2), partition.inSyncReplicas.map(_.brokerId))&lt;br/&gt;
+&lt;br/&gt;
+    // after makeLeader(() call, partition should know about all the replicas&lt;br/&gt;
+    val leaderReplica = partition.getReplica(leader).get&lt;br/&gt;
+    val follower1Replica = partition.getReplica(follower1).get&lt;br/&gt;
+    val follower2Replica = partition.getReplica(follower2).get&lt;br/&gt;
+&lt;br/&gt;
+    // append records with initial leader epoch&lt;br/&gt;
+    val lastOffsetOfFirstBatch = partition.appendRecordsToLeader(batch1, isFromClient = true).lastOffset&lt;br/&gt;
+    partition.appendRecordsToLeader(batch2, isFromClient = true)&lt;br/&gt;
+    assertEquals(&quot;Expected leader&apos;s HW not move&quot;, leaderReplica.logStartOffset, leaderReplica.highWatermark.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // let the follower in ISR move leader&apos;s HW to move further but below LEO&lt;br/&gt;
+    def readResult(fetchInfo: FetchDataInfo, leaderReplica: Replica): LogReadResult = &lt;/p&gt;
{
+      LogReadResult(info = fetchInfo,
+        highWatermark = leaderReplica.highWatermark.messageOffset,
+        leaderLogStartOffset = leaderReplica.logStartOffset,
+        leaderLogEndOffset = leaderReplica.logEndOffset.messageOffset,
+        followerLogStartOffset = 0,
+        fetchTimeMs = time.milliseconds,
+        readSize = 10240,
+        lastStableOffset = None)
+    }
&lt;p&gt;+&lt;br/&gt;
+    def fetchOffsetsForTimestamp(timestamp: Long, isolation: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;IsolationLevel&amp;#93;&lt;/span&gt;): Either[ApiException, Option&lt;span class=&quot;error&quot;&gt;&amp;#91;TimestampAndOffset&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
+      try &lt;/p&gt;
{
+        Right(partition.fetchOffsetForTimestamp(
+          timestamp = timestamp,
+          isolationLevel = isolation,
+          currentLeaderEpoch = Optional.of(partition.getLeaderEpoch),
+          fetchOnlyFromLeader = true
+        ))
+      }
&lt;p&gt; catch &lt;/p&gt;
{
+        case e: ApiException =&amp;gt; Left(e)
+      }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    // Update follower 1&lt;br/&gt;
+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower1Replica, readResult(FetchDataInfo(LogOffsetMetadata(0), batch1), leaderReplica))&lt;br/&gt;
+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower1Replica, readResult(FetchDataInfo(LogOffsetMetadata(2), batch2), leaderReplica))&lt;br/&gt;
+&lt;br/&gt;
+    // Update follower 2&lt;br/&gt;
+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower2Replica, readResult(FetchDataInfo(LogOffsetMetadata(0), batch1), leaderReplica))&lt;br/&gt;
+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower2Replica, readResult(FetchDataInfo(LogOffsetMetadata(2), batch2), leaderReplica))&lt;br/&gt;
+&lt;br/&gt;
+    // At this point, the leader has gotten 5 writes, but followers have only fetched two&lt;br/&gt;
+    assertEquals(2, partition.localReplica.get.highWatermark.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // Get the LEO&lt;br/&gt;
+    fetchOffsetsForTimestamp(ListOffsetRequest.LATEST_TIMESTAMP, None) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt; assertEquals(5, offsetAndTimestamp.offset)
+      case Right(None) =&amp;gt; fail(&quot;Should have seen some offsets&quot;)
+      case Left(e) =&amp;gt; fail(&quot;Should not have seen an error&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Get the HW&lt;br/&gt;
+    fetchOffsetsForTimestamp(ListOffsetRequest.LATEST_TIMESTAMP, Some(IsolationLevel.READ_UNCOMMITTED)) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt; assertEquals(2, offsetAndTimestamp.offset)
+      case Right(None) =&amp;gt; fail(&quot;Should have seen some offsets&quot;)
+      case Left(e) =&amp;gt; fail(&quot;Should not have seen an error&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Get a offset beyond the HW by timestamp, get a None&lt;br/&gt;
+    assertEquals(Right(None), fetchOffsetsForTimestamp(30, Some(IsolationLevel.READ_UNCOMMITTED)))&lt;br/&gt;
+&lt;br/&gt;
+    // Make into a follower&lt;br/&gt;
+    assertTrue(partition.makeFollower(controllerId,&lt;br/&gt;
+      new LeaderAndIsrRequest.PartitionState(controllerEpoch, follower2, leaderEpoch + 1, isr, 1, replicas, false), 1))&lt;br/&gt;
+&lt;br/&gt;
+    // Back to leader, this resets the startLogOffset for this epoch (to 2), we&apos;re now in the fault condition&lt;br/&gt;
+    assertTrue(partition.makeLeader(controllerId,&lt;br/&gt;
+      new LeaderAndIsrRequest.PartitionState(controllerEpoch, leader, leaderEpoch + 2, isr, 1, replicas, false), 2))&lt;br/&gt;
+&lt;br/&gt;
+    // Try to get offsets as a client&lt;br/&gt;
+    fetchOffsetsForTimestamp(ListOffsetRequest.LATEST_TIMESTAMP, Some(IsolationLevel.READ_UNCOMMITTED)) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt; fail(&quot;Should have failed with OffsetNotAvailable&quot;)
+      case Right(None) =&amp;gt; fail(&quot;Should have seen an error&quot;)
+      case Left(e: OffsetNotAvailableException) =&amp;gt; // ok
+      case Left(e: ApiException) =&amp;gt; fail(s&quot;Expected OffsetNotAvailableException, got $e&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // If request is not from a client, we skip the check&lt;br/&gt;
+    fetchOffsetsForTimestamp(ListOffsetRequest.LATEST_TIMESTAMP, None) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt; assertEquals(5, offsetAndTimestamp.offset)
+      case Right(None) =&amp;gt; fail(&quot;Should have seen some offsets&quot;)
+      case Left(e: ApiException) =&amp;gt; fail(s&quot;Got ApiException $e&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // If we request the earliest timestamp, we skip the check&lt;br/&gt;
+    fetchOffsetsForTimestamp(ListOffsetRequest.EARLIEST_TIMESTAMP, Some(IsolationLevel.READ_UNCOMMITTED)) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt; assertEquals(0, offsetAndTimestamp.offset)
+      case Right(None) =&amp;gt; fail(&quot;Should have seen some offsets&quot;)
+      case Left(e: ApiException) =&amp;gt; fail(s&quot;Got ApiException $e&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // If we request an offset by timestamp earlier than the HW, we are ok&lt;br/&gt;
+    fetchOffsetsForTimestamp(11, Some(IsolationLevel.READ_UNCOMMITTED)) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt;
+        assertEquals(1, offsetAndTimestamp.offset)
+        assertEquals(11, offsetAndTimestamp.timestamp)
+      case Right(None) =&amp;gt; fail(&quot;Should have seen some offsets&quot;)
+      case Left(e: ApiException) =&amp;gt; fail(s&quot;Got ApiException $e&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Request an offset by timestamp beyond the HW, get an error now since we&apos;re in a bad state&lt;br/&gt;
+    fetchOffsetsForTimestamp(100, Some(IsolationLevel.READ_UNCOMMITTED)) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt; fail(&quot;Should have failed&quot;)
+      case Right(None) =&amp;gt; fail(&quot;Should have failed&quot;)
+      case Left(e: OffsetNotAvailableException) =&amp;gt; // ok
+      case Left(e: ApiException) =&amp;gt; fail(&quot;Should have seen OffsetNotAvailableException, saw $e&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+    // Next fetch from replicas, HW is moved up to 5 (ahead of the LEO)&lt;br/&gt;
+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower1Replica, readResult(FetchDataInfo(LogOffsetMetadata(5), MemoryRecords.EMPTY), leaderReplica))&lt;br/&gt;
+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower2Replica, readResult(FetchDataInfo(LogOffsetMetadata(5), MemoryRecords.EMPTY), leaderReplica))&lt;br/&gt;
+&lt;br/&gt;
+    // Error goes away&lt;br/&gt;
+    fetchOffsetsForTimestamp(ListOffsetRequest.LATEST_TIMESTAMP, Some(IsolationLevel.READ_UNCOMMITTED)) match &lt;/p&gt;
{
+      case Right(Some(offsetAndTimestamp)) =&amp;gt; assertEquals(5, offsetAndTimestamp.offset)
+      case Right(None) =&amp;gt; fail(&quot;Should have seen some offsets&quot;)
+      case Left(e: ApiException) =&amp;gt; fail(s&quot;Got ApiException $e&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Now we see None instead of an error for out of range timestamp&lt;br/&gt;
+    assertEquals(Right(None), fetchOffsetsForTimestamp(100, Some(IsolationLevel.READ_UNCOMMITTED)))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
   private def setupPartitionWithMocks(leaderEpoch: Int,&lt;br/&gt;
                                       isLeader: Boolean,&lt;br/&gt;
                                       log: Log = logManager.getOrCreateLog(topicPartition, logConfig)): Partition = {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
index f04c70fbf88..9b4210ed59e 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
@@ -373,9 +373,13 @@ class KafkaApisTest {&lt;br/&gt;
     val isolationLevel = IsolationLevel.READ_UNCOMMITTED&lt;br/&gt;
     val currentLeaderEpoch = Optional.of&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(15)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(replicaManager.fetchOffsetForTimestamp(tp, ListOffsetRequest.EARLIEST_TIMESTAMP,&lt;/li&gt;
	&lt;li&gt;Some(isolationLevel), currentLeaderEpoch, fetchOnlyFromLeader = true))&lt;/li&gt;
	&lt;li&gt;.andThrow(error.exception)&lt;br/&gt;
+    EasyMock.expect(replicaManager.fetchOffsetForTimestamp(&lt;br/&gt;
+      EasyMock.eq(tp),&lt;br/&gt;
+      EasyMock.eq(ListOffsetRequest.EARLIEST_TIMESTAMP),&lt;br/&gt;
+      EasyMock.eq(Some(isolationLevel)),&lt;br/&gt;
+      EasyMock.eq(currentLeaderEpoch),&lt;br/&gt;
+      fetchOnlyFromLeader = EasyMock.eq(true))&lt;br/&gt;
+    ).andThrow(error.exception)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val capturedResponse = expectNoThrottling()&lt;br/&gt;
     EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel)&lt;br/&gt;
@@ -462,9 +466,13 @@ class KafkaApisTest {&lt;br/&gt;
     val latestOffset = 15L&lt;br/&gt;
     val currentLeaderEpoch = Optional.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(replicaManager.fetchOffsetForTimestamp(tp, ListOffsetRequest.LATEST_TIMESTAMP,&lt;/li&gt;
	&lt;li&gt;Some(isolationLevel), currentLeaderEpoch, fetchOnlyFromLeader = true))&lt;/li&gt;
	&lt;li&gt;.andReturn(Some(new TimestampAndOffset(ListOffsetResponse.UNKNOWN_TIMESTAMP, latestOffset, currentLeaderEpoch)))&lt;br/&gt;
+    EasyMock.expect(replicaManager.fetchOffsetForTimestamp(&lt;br/&gt;
+      EasyMock.eq(tp),&lt;br/&gt;
+      EasyMock.eq(ListOffsetRequest.LATEST_TIMESTAMP),&lt;br/&gt;
+      EasyMock.eq(Some(isolationLevel)),&lt;br/&gt;
+      EasyMock.eq(currentLeaderEpoch),&lt;br/&gt;
+      fetchOnlyFromLeader = EasyMock.eq(true))&lt;br/&gt;
+    ).andReturn(Some(new TimestampAndOffset(ListOffsetResponse.UNKNOWN_TIMESTAMP, latestOffset, currentLeaderEpoch)))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val capturedResponse = expectNoThrottling()&lt;br/&gt;
     EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel)&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 48 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2h94v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>