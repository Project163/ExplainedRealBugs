<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:06:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6366] StackOverflowError in kafka-coordinator-heartbeat-thread</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6366</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;With Kafka 1.0 our consumer groups fall into a permanent cycle of rebalancing once a StackOverflowError in the heartbeat thread occurred due to connectivity issues of the consumers to the coordinating broker:&lt;/p&gt;

&lt;p&gt;Immediately before the exception there are hundreds, if not thousands of log entries of following type:&lt;/p&gt;

&lt;p&gt;2017-12-12 16:23:12.361 [kafka-coordinator-heartbeat-thread | &lt;br/&gt;
my-consumer-group] INFO  - [Consumer clientId=consumer-4, &lt;br/&gt;
groupId=my-consumer-group] Marking the coordinator &amp;lt;IP&amp;gt;:&amp;lt;Port&amp;gt; (id: &lt;br/&gt;
2147483645 rack: null) dead&lt;/p&gt;

&lt;p&gt;The exceptions always happen somewhere in the DateFormat code, even &lt;br/&gt;
though at different lines.&lt;/p&gt;

&lt;p&gt;2017-12-12 16:23:12.363 [kafka-coordinator-heartbeat-thread | &lt;br/&gt;
my-consumer-group] ERROR - Uncaught exception in thread &lt;br/&gt;
&apos;kafka-coordinator-heartbeat-thread | my-consumer-group&apos;:&lt;br/&gt;
java.lang.StackOverflowError&lt;br/&gt;
         at &lt;br/&gt;
java.text.DateFormatSymbols.getProviderInstance(DateFormatSymbols.java:362)&lt;br/&gt;
         at &lt;br/&gt;
java.text.DateFormatSymbols.getInstance(DateFormatSymbols.java:340)&lt;br/&gt;
         at java.util.Calendar.getDisplayName(Calendar.java:2110)&lt;br/&gt;
         at java.text.SimpleDateFormat.subFormat(SimpleDateFormat.java:1125)&lt;br/&gt;
         at java.text.SimpleDateFormat.format(SimpleDateFormat.java:966)&lt;br/&gt;
         at java.text.SimpleDateFormat.format(SimpleDateFormat.java:936)&lt;br/&gt;
         at java.text.DateFormat.format(DateFormat.java:345)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.log4j.helpers.PatternParser$DatePatternConverter.convert(PatternParser.java:443)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.log4j.helpers.PatternConverter.format(PatternConverter.java:65)&lt;br/&gt;
         at org.apache.log4j.PatternLayout.format(PatternLayout.java:506)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310)&lt;br/&gt;
         at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)&lt;br/&gt;
         at org.apache.log4j.Category.callAppenders(Category.java:206)&lt;br/&gt;
         at org.apache.log4j.Category.forcedLog(Category.java:391)&lt;br/&gt;
         at org.apache.log4j.Category.log(Category.java:856)&lt;br/&gt;
         at &lt;br/&gt;
org.slf4j.impl.Log4jLoggerAdapter.info(Log4jLoggerAdapter.java:324)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.common.utils.LogContext$KafkaLogger.info(LogContext.java:341)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.AbstractCoordinator.coordinatorDead(AbstractCoordinator.java:649)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onFailure(AbstractCoordinator.java:797)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.RequestFuture$1.onFailure(RequestFuture.java:209)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.RequestFuture.fireFailure(RequestFuture.java:177)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.RequestFuture.raise(RequestFuture.java:147)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:496)&lt;br/&gt;
...&lt;br/&gt;
the following 9 lines are repeated around hundred times.&lt;br/&gt;
...&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:496)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:353)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.failUnsentRequests(ConsumerNetworkClient.java:416)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.disconnect(ConsumerNetworkClient.java:388)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.AbstractCoordinator.coordinatorDead(AbstractCoordinator.java:653)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onFailure(AbstractCoordinator.java:797)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.RequestFuture$1.onFailure(RequestFuture.java:209)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.RequestFuture.fireFailure(RequestFuture.java:177)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.RequestFuture.raise(RequestFuture.java:147)&lt;br/&gt;
         at &lt;br/&gt;
org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:496)&lt;/p&gt;</description>
                <environment></environment>
        <key id="13125038">KAFKA-6366</key>
            <summary>StackOverflowError in kafka-coordinator-heartbeat-thread</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="joerg.heinicke">Joerg Heinicke</reporter>
                        <labels>
                    </labels>
                <created>Thu, 14 Dec 2017 20:47:03 +0000</created>
                <updated>Thu, 15 Feb 2018 23:19:17 +0000</updated>
                            <resolved>Sat, 20 Jan 2018 21:48:37 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.0.1</fixVersion>
                                    <component>consumer</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16291582" author="joerg.heinicke" created="Thu, 14 Dec 2017 20:47:48 +0000"  >&lt;p&gt;Ted Yu already suggested a fix on the mailing list: &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/kafka-users/201712.mbox/%3CCALte62w6%3DpJObC%2Bi36BkoqbOLTKsQ%3DNrDDv6dM8abfwB5PspLA%40mail.gmail.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/kafka-users/201712.mbox/%3CCALte62w6%3DpJObC%2Bi36BkoqbOLTKsQ%3DNrDDv6dM8abfwB5PspLA%40mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16291585" author="yuzhihong@gmail.com" created="Thu, 14 Dec 2017 20:51:12 +0000"  >&lt;p&gt;First attempt for fixing the stack overflow&lt;/p&gt;</comment>
                            <comment id="16291699" author="hachikuji" created="Thu, 14 Dec 2017 22:14:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joerg.heinicke&quot; class=&quot;user-hover&quot; rel=&quot;joerg.heinicke&quot;&gt;joerg.heinicke&lt;/a&gt; Thanks for reporting the issue. I&apos;m puzzling a bit over the trace. It looks like &lt;tt&gt;ConsumerNetworkClient.disconnect()&lt;/tt&gt; should be reentrant (I tested locally to be sure). Maybe we&apos;re hitting a situation where the foreground thread is in a loop trying to add a request which the heartbeat thread is stuck cancelling in an infinite recursion? Is there another explanation?&lt;/p&gt;</comment>
                            <comment id="16293932" author="hachikuji" created="Sat, 16 Dec 2017 20:21:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joerg.heinicke&quot; class=&quot;user-hover&quot; rel=&quot;joerg.heinicke&quot;&gt;joerg.heinicke&lt;/a&gt; It would be helpful to see your consumer configuration and, if possible, the full logs. So far I&apos;ve been unable to reproduce the problem. The only thing I can think of is that one thread is in a tight loop sending requests while the other is trying to mark the coordinator dead. As far as I can tell, the retry backoff should make this impossible, but I could be missing something.&lt;/p&gt;</comment>
                            <comment id="16295813" author="joerg.heinicke" created="Mon, 18 Dec 2017 23:02:59 +0000"  >&lt;p&gt;Let me try to provide you all the details required:&lt;br/&gt;
Quite a standard use case I guess: a component which reads from a Kafka topic and writes to another topic.&lt;br/&gt;
A component has 6 threads which use an individual KafkaConsumer each:&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
  &lt;span class=&quot;code-comment&quot;&gt;// subscribe to topic
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (running) {
    &lt;span class=&quot;code-comment&quot;&gt;//poll consumer
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; each record {
      &lt;span class=&quot;code-comment&quot;&gt;// convert
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// send producer record on KafkaProducer
&lt;/span&gt;    }
    &lt;span class=&quot;code-comment&quot;&gt;// commit asynchronously
&lt;/span&gt;  }
&lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
  running = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
  &lt;span class=&quot;code-comment&quot;&gt;// flush and close KafkaProducer
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// commit synchronously and close KafkaConsumer
&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16295850" author="joerg.heinicke" created="Mon, 18 Dec 2017 23:29:10 +0000"  >&lt;p&gt;ConsumerConfig values:&lt;br/&gt;
        auto.offset.reset = earliest&lt;br/&gt;
        bootstrap.servers = &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.default.svc:9092&amp;#93;&lt;/span&gt;&lt;br/&gt;
        enable.auto.commit = false&lt;br/&gt;
        group.id = my-consumer-group&lt;br/&gt;
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer&lt;br/&gt;
        max.poll.records = 50&lt;br/&gt;
        request.timeout.ms = 60000&lt;br/&gt;
        value.deserializer = class my.package.MyProtobufDeserializer&lt;/p&gt;

&lt;p&gt;All other properties should be on their default setting.&lt;/p&gt;

&lt;p&gt;Logs I can&apos;t provide at the moment as apparently all instances have been cleanly deployed today. I can provide the logs after the next occasion which I&apos;m quite confident will be tomorrow or so.&lt;/p&gt;</comment>
                            <comment id="16295970" author="hachikuji" created="Tue, 19 Dec 2017 01:05:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joerg.heinicke&quot; class=&quot;user-hover&quot; rel=&quot;joerg.heinicke&quot;&gt;joerg.heinicke&lt;/a&gt; Thanks. Nothing weird that I can see from the configs or the usage. Guess I&apos;ll wait to see the logs.&lt;/p&gt;</comment>
                            <comment id="16296552" author="huxi_2b" created="Tue, 19 Dec 2017 09:44:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; I&apos;ve been thinking another situation which might happen after the Heartbeat thread threw StackOverflowError. &lt;/p&gt;

&lt;p&gt;Since it&apos;s of type &apos;Error&apos;, so `failed` in HeartbeatThread class will not be updated, therefore the thread will be mistakenly thought as normal instead of failed although it already exited. Do we need to set `failed` when facing an Error thrown?&lt;/p&gt;</comment>
                            <comment id="16297569" author="hachikuji" created="Tue, 19 Dec 2017 23:11:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=huxi_2b&quot; class=&quot;user-hover&quot; rel=&quot;huxi_2b&quot;&gt;huxi_2b&lt;/a&gt; Sure, it seems reasonable to catch &lt;tt&gt;Throwable&lt;/tt&gt; in the heartbeat thread. That can probably be addressed separately.&lt;/p&gt;</comment>
                            <comment id="16297642" author="joerg.heinicke" created="Tue, 19 Dec 2017 23:38:21 +0000"  >&lt;p&gt;I attached the log file &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12902945/12902945_ConverterProcessor.zip&quot; title=&quot;ConverterProcessor.zip attached to KAFKA-6366&quot;&gt;ConverterProcessor.zip&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; from start to until the error occurs at 21:03. As you can see the issues were already going on for hours. What also seems to be apparent is that starting from 17:46 for most of the times only one thread seems to be affected: pool-5-thread-3 resp. clientId=consumer-2 (from the log file both seem to be index 1 based).&lt;/p&gt;</comment>
                            <comment id="16298761" author="hachikuji" created="Wed, 20 Dec 2017 17:17:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joerg.heinicke&quot; class=&quot;user-hover&quot; rel=&quot;joerg.heinicke&quot;&gt;joerg.heinicke&lt;/a&gt; Thanks for sharing the logs. One thing that immediately stands out is the large number of async offset commit failures. I counted 13,359 instances. Considering the &quot;Marking coordinator dead&quot; messages, there are about 10,862 instances. This is just a guess, but do you have any retry logic implemented for when async offset commits fail? That would explain the large number of &quot;Marking coordinator dead&quot; messages as well as the stack overflow.&lt;/p&gt;</comment>
                            <comment id="16298880" author="yuzhihong@gmail.com" created="Wed, 20 Dec 2017 18:31:45 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
                    completedOffsetCommits.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; OffsetCommitCompletion(callback, offsets,
                            RetriableCommitFailedException.withUnderlyingMessage(e.getMessage())));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In Joerg&apos;s case, e.getMessage() was null.&lt;/p&gt;

&lt;p&gt;I wonder if we can provide more information to the user when e.getMessage() is null. e.g. log e.&lt;/p&gt;</comment>
                            <comment id="16298908" author="hachikuji" created="Wed, 20 Dec 2017 18:46:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tedyu&quot; class=&quot;user-hover&quot; rel=&quot;tedyu&quot;&gt;tedyu&lt;/a&gt; Agreed. I think we should just include the cause when we throw.&lt;/p&gt;</comment>
                            <comment id="16298915" author="yuzhihong@gmail.com" created="Wed, 20 Dec 2017 18:50:48 +0000"  >&lt;p&gt;Should I log another JIRA for the above ?&lt;/p&gt;

&lt;p&gt;One aspect we need to pay attention is to avoid flooding the log file, since the stack trace is much longer compared to the single sentence.&lt;br/&gt;
To prevent (repeated) stack traces flooding the log, we can keep Map from stack trace to count (number of times the stack trace occurred).&lt;/p&gt;</comment>
                            <comment id="16299258" author="joerg.heinicke" created="Wed, 20 Dec 2017 23:34:42 +0000"  >&lt;p&gt;This is our commit async code block:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.kafkaConsumer.commitAsync((offsets, exception) -&amp;gt; {
    offsets.forEach((k, v) -&amp;gt; {
        log.debug(k + &lt;span class=&quot;code-quote&quot;&gt;&quot;\t&quot;&lt;/span&gt; + v);
    });
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (exception != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        log.error(KafkaConsumer.&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;getSimpleName() + &lt;span class=&quot;code-quote&quot;&gt;&quot; failed committing offets asynchronously! &quot;&lt;/span&gt;, exception);
    } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        log.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Committing Consumer Offset succeeded!&quot;&lt;/span&gt;);
    }
});
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I don&apos;t see any explicit or can&apos;t even imagine implicit retry logic.&lt;/p&gt;</comment>
                            <comment id="16299278" author="joerg.heinicke" created="Wed, 20 Dec 2017 23:56:32 +0000"  >&lt;p&gt;Simply for the quantity structure: Our system has a throughput of about 100 k messages per minute. The topic has 30 partitions. The consumer group matches those and consists of 5 service instances with 6 KafkaConsumers each. Eventually with a theoretically steady processing (in this particular incident processing seemed steady enough (&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12903133/12903133_Screenshot-2017-12-19+21.35-22.10+processing.png&quot; title=&quot;Screenshot-2017-12-19 21.35-22.10 processing.png attached to KAFKA-6366&quot;&gt;Screenshot-2017-12-19 21.35-22.10 processing.png&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;, Timings here are UTC + 1 while in the log file it&apos;s UTC.) while often it starts fluctuating strongly) this means around 3k messages per minute per thread or 50 per messages per second. The batch size is also rather small with just 50 messages, so 1 batch and thereby one async commit per second. The number of async commit failures is slightly off: e.g. &amp;gt; 5,000 failures/ log entries between 20:38 and 21:03, i.e. within 25 mins or 1,500 s. So the number is still more than 3 times as high than expected in case all commits fail within that time.&lt;/p&gt;

&lt;p&gt;Btw., we are aware of the underlying issue with the infrastructure: heavily over-committed VMs in terms of CPU and rather low storage throughput.&lt;/p&gt;</comment>
                            <comment id="16299311" author="hachikuji" created="Thu, 21 Dec 2017 00:27:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joerg.heinicke&quot; class=&quot;user-hover&quot; rel=&quot;joerg.heinicke&quot;&gt;joerg.heinicke&lt;/a&gt; Ok, we may need to take a look at the more verbose logs (DEBUG is probably good enough). One idea I had is the following. Suppose the consumer has a large number of records buffered. It might be possible to hit a race condition in which the foreground thread is hitting &lt;tt&gt;poll()&lt;/tt&gt; and &lt;tt&gt;commitAsync()&lt;/tt&gt; in a tight loop because &lt;tt&gt;max.poll.records=50&lt;/tt&gt; is immediately satisfied with the buffered records. At some point, the heartbeat thread might see the coordinator disconnect and attempt to mark it dead, but the new offset commit requests are piling up as fast as the background thread can cancel them in &lt;tt&gt;disconnect()&lt;/tt&gt;, which ultimately causes the stack overflow. &lt;/p&gt;

&lt;p&gt;I think ultimately the fix for this issue is going to be setting the coordinator to null in &lt;tt&gt;AbstractCoordinator.coordinatorDead()&lt;/tt&gt; prior to disconnecting. I will go ahead and submit a patch to do this. It would be good to confirm from the logging if the scenario I mentioned above is happening or if it&apos;s something else. If we still can&apos;t figure out the cause, perhaps we can at least test with the patch.&lt;/p&gt;</comment>
                            <comment id="16299408" author="githubbot" created="Thu, 21 Dec 2017 02:05:24 +0000"  >&lt;p&gt;GitHub user hachikuji opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/4349&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4349&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6366&quot; title=&quot;StackOverflowError in kafka-coordinator-heartbeat-thread&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6366&quot;&gt;&lt;del&gt;KAFKA-6366&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;WIP&amp;#93;&lt;/span&gt;: Fix stack overflow in consumer due to fast offset commits during coordinator disconnect&lt;/p&gt;

&lt;p&gt;    When the coordinator is marked unknown, we explicitly disconnect its connection and cancel pending requests. Currently the disconnect happens before the coordinator state is set to null, which means that callbacks which inspect the coordinator state will see it still as active. This can lead to further requests being sent. In pathological cases, the disconnect itself is not able to return because new requests are sent to the coordinator before the disconnect can complete, which leads to the stack overflow error. To fix the problem, I have reordered the disconnect to happen after the coordinator is set to null.&lt;/p&gt;

&lt;p&gt;    I have added a basic test case to verify that callbacks for in-flight or unsent requests see the coordinator as unknown which prevents them from attempting to resend. We may need additional test cases after we determine whether this is in fact was it happening in the reported ticket.&lt;/p&gt;

&lt;p&gt;    Note that I have also included some minor cleanups which I noticed along the way.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/hachikuji/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/hachikuji/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6366&quot; title=&quot;StackOverflowError in kafka-coordinator-heartbeat-thread&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6366&quot;&gt;&lt;del&gt;KAFKA-6366&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/4349.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4349.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4349&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 488de3dca5be6111fd447980c8e79477259dc99a&lt;br/&gt;
Author: Jason Gustafson &amp;lt;jason@...&amp;gt;&lt;br/&gt;
Date:   2017-12-18T18:53:38Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6366&quot; title=&quot;StackOverflowError in kafka-coordinator-heartbeat-thread&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6366&quot;&gt;&lt;del&gt;KAFKA-6366&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;WIP&amp;#93;&lt;/span&gt;: Fix stack overflow in consumer due to fast offset commits during coordinator disconnect&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16300300" author="hachikuji" created="Thu, 21 Dec 2017 17:14:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tedyu&quot; class=&quot;user-hover&quot; rel=&quot;tedyu&quot;&gt;tedyu&lt;/a&gt; I missed your comment. I guess we can just fix it here. I agree it&apos;s worth being a little careful about log verbosity. Maybe I&apos;ll add the additional &lt;tt&gt;catch&lt;/tt&gt; that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=huxi_2b&quot; class=&quot;user-hover&quot; rel=&quot;huxi_2b&quot;&gt;huxi_2b&lt;/a&gt; mentioned above as well.&lt;/p&gt;</comment>
                            <comment id="16300303" author="yuzhihong@gmail.com" created="Thu, 21 Dec 2017 17:17:39 +0000"  >&lt;p&gt;Including the cause in the log can be done in your PR.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="16308360" author="hachikuji" created="Tue, 2 Jan 2018 16:58:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joerg.heinicke&quot; class=&quot;user-hover&quot; rel=&quot;joerg.heinicke&quot;&gt;joerg.heinicke&lt;/a&gt; Any chance you can verify whether the patch I posted fixes the problem?&lt;/p&gt;</comment>
                            <comment id="16317346" author="joerg.heinicke" created="Mon, 8 Jan 2018 23:34:30 +0000"  >&lt;p&gt;Sorry for the delay. Even though I planned to at least try to extract the information from the system over Christmas holidays I haven&apos;t managed. Now back from vacation I have done so. I attached the log file &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12905178/12905178_ConverterProcessor_DEBUG.zip&quot; title=&quot;ConverterProcessor_DEBUG.zip attached to KAFKA-6366&quot;&gt;ConverterProcessor_DEBUG.zip&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; (which is around 700k lines and 150 MB for about 5 mins!). I don&apos;t get additional hints on the issue, not sure whether it helps you to confirm the scenario.&lt;/p&gt;

&lt;p&gt;We don&apos;t have particular test scenarios to test the patch which means we would have to run this directly in production - which I&apos;m not too comfortable with if you could not even confirm the scenario yet. Another question which comes to my mind is how the consumer will behave in case we hit the scenario with the patch applied since apparently all other threads are still able to commit while the failing thread (pool-5-thread-5 in the attached log file) marked the coordinator dead, i.e. what is the expected and probably originally intended behavior. And on the most basic and practical side: How do I get a Kafka distribution with the patch applied? Apparently I will have to build it myself. Can you give me some kick-off hints? Is the documentation at &lt;a href=&quot;https://github.com/apache/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka&lt;/a&gt; all I need?&lt;/p&gt;</comment>
                            <comment id="16324992" author="hachikuji" created="Sat, 13 Jan 2018 06:16:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joerg.heinicke&quot; class=&quot;user-hover&quot; rel=&quot;joerg.heinicke&quot;&gt;joerg.heinicke&lt;/a&gt; Thanks for the debug logs. I think I finally understand the problem. All it takes is a coordinator disconnect with a large number of pending offset commits. In this case, the consumer appears to be sending async commits quite frequently (nearly every message apparently). When the coordinator disconnected, I counted 3,346 pending commits which had to be cancelled. I wrote a simple test case for this scenario and reproduced the overflow. I was also able to confirm that my fix above does indeed solve the problem, so I will remove the WIP tag and try to get this into the upcoming bug fix release.&lt;/p&gt;

&lt;p&gt;For a shorter term solution, I would recommend implementing some logic to dampen the rate of offset commits. Typically people make it periodic or base it off of the number of messages consumed.  &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13137076">KAFKA-6541</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12902149" name="6366.v1.txt" size="1797" author="yuzhihong@gmail.com" created="Thu, 14 Dec 2017 20:50:54 +0000"/>
                            <attachment id="12902945" name="ConverterProcessor.zip" size="71378" author="joerg.heinicke" created="Tue, 19 Dec 2017 23:25:48 +0000"/>
                            <attachment id="12905178" name="ConverterProcessor_DEBUG.zip" size="3676128" author="joerg.heinicke" created="Mon, 8 Jan 2018 23:12:38 +0000"/>
                            <attachment id="12903133" name="Screenshot-2017-12-19 21.35-22.10 processing.png" size="16747" author="joerg.heinicke" created="Thu, 21 Dec 2017 00:08:11 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 44 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3nx3r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>