<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:15:23 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7415] OffsetsForLeaderEpoch may incorrectly respond with undefined epoch causing truncation to HW</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7415</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;If the&#160;follower&apos;s last appended epoch is ahead of the leader&apos;s last appended epoch,&#160;the&#160;OffsetsForLeaderEpoch response will incorrectly send&#160;(UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), and the follower will truncate to HW. This may lead to data loss in some rare cases where 2 back-to-back leader elections happen (failure of one leader, followed by quick re-election of the next leader due to preferred leader election, so that all replicas are still in the ISR, and then failure of the 3rd leader).&lt;/p&gt;

&lt;p&gt;The bug is in&#160;LeaderEpochFileCache.endOffsetFor(), which returns&#160;(UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET) if the requested leader epoch&#160;is ahead of the last leader epoch in the cache. The method should return (last leader epoch in the cache, LEO) in this scenario.&lt;/p&gt;

&lt;p&gt;We don&apos;t create an entry in a leader epoch cache until a message is appended with the new leader epoch. Every append to log calls LeaderEpochFileCache.assign(). However, it would be much cleaner if `makeLeader` created an entry in the cache as soon as replica becomes a leader, which will fix the bug. In case the leader never appends any messages, and the next leader epoch starts with the same offset, we already have clearAndFlushLatest() that clears entries with start offsets greater or equal to the passed offset. LeaderEpochFileCache.assign() could be merged with&#160;clearAndFlushLatest(), so that we clear cache entries with offsets equal or greater than the start offset of the new epoch, so that we do not need to call these methods separately.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Here is an example&#160;of a scenario where the issue leads to the data loss.&lt;/p&gt;

&lt;p&gt;Suppose we have three replicas: r1, r2, and r3. Initially, the ISR consists of (r1, r2, r3) and the leader is r1. The data up to offset 10 has been committed to the ISR. Here is the initial state:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Leader: r1
leader epoch: 0
ISR(r1, r2, r3)
r1:&#160;[hw=10, leo=10]
r2:&#160;[hw=8, leo=10]
r3:&#160;[hw=5, leo=10]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Replica 1 fails and leaves the ISR, which makes Replica 2 the new leader with leader epoch = 1. The leader appends a batch, but it is not replicated yet to the followers.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Leader: r2
leader epoch: 1
ISR(r2, r3)
r1:&#160;[hw=10, leo=10]
r2:&#160;[hw=8, leo=11]
r3:&#160;[hw=5, leo=10]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Replica 3 is elected a leader (due to preferred leader election) before it has a chance to truncate, with leader epoch 2.&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Leader: r3
leader epoch: 2
ISR(r2, r3)
r1:&#160;[hw=10, leo=10]
r2:&#160;[hw=8, leo=11]
r3:&#160;[hw=5, leo=10]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Replica 2 sends OffsetsForLeaderEpoch(leader epoch = 1) to Replica 3. Replica 3 incorrectly replies with&#160;UNDEFINED_EPOCH_OFFSET, and Replica 2 truncates to HW. If Replica 3 fails before Replica 2 re-fetches the data, this may lead to data loss.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13185221">KAFKA-7415</key>
            <summary>OffsetsForLeaderEpoch may incorrectly respond with undefined epoch causing truncation to HW</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="apovzner">Anna Povzner</reporter>
                        <labels>
                    </labels>
                <created>Fri, 14 Sep 2018 18:29:00 +0000</created>
                <updated>Fri, 5 Oct 2018 23:08:21 +0000</updated>
                            <resolved>Thu, 4 Oct 2018 22:54:17 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>replication</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16624312" author="githubbot" created="Fri, 21 Sep 2018 23:31:54 +0000"  >&lt;p&gt;hachikuji opened a new pull request #5678: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7415&quot; title=&quot;OffsetsForLeaderEpoch may incorrectly respond with undefined epoch causing truncation to HW&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7415&quot;&gt;&lt;del&gt;KAFKA-7415&lt;/del&gt;&lt;/a&gt;; Persist leader epoch and start offset on becoming a leader&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5678&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5678&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   This patch ensures that the leader epoch cache is updated when a broker becomes leader with the latest epoch and the log end offset as its starting offset. This guarantees that the leader will be able to provide the right truncation point even if the follower has data from leader epochs which the leader itself does not have. This situation can occur when there are back to back leader elections.&lt;/p&gt;

&lt;p&gt;   Additionally, we have made the following changes:&lt;/p&gt;

&lt;p&gt;   1. The leader epoch cache enforces monotonically increase epochs and starting offsets among its entry. Whenever a new entry is appended which violates requirement, we remove the conflicting entries from the cache.&lt;br/&gt;
   2. Previously we returned an unknown epoch and offset if an epoch is queried which comes before the first entry in the cache. Now we return the smallest . For example, if the earliest entry in the cache is (epoch=5, startOffset=10), then a query for epoch 4 will return (epoch=4, endOffset=10). This ensures that followers (and consumers in KIP-320) can always determine where the correct starting point is for the active log range on the leader.&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16638878" author="githubbot" created="Thu, 4 Oct 2018 21:02:27 +0000"  >&lt;p&gt;hachikuji closed pull request #5678: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7415&quot; title=&quot;OffsetsForLeaderEpoch may incorrectly respond with undefined epoch causing truncation to HW&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7415&quot;&gt;&lt;del&gt;KAFKA-7415&lt;/del&gt;&lt;/a&gt;; Persist leader epoch and start offset on becoming a leader&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5678&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5678&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index 2036bb09da8..307fb81447b 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -301,8 +301,17 @@ class Partition(val topic: String,&lt;br/&gt;
       leaderEpoch = partitionStateInfo.basePartitionState.leaderEpoch&lt;br/&gt;
       leaderEpochStartOffsetOpt = Some(leaderEpochStartOffset)&lt;br/&gt;
       zkVersion = partitionStateInfo.basePartitionState.zkVersion&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val isNewLeader = leaderReplicaIdOpt.map(_ != localBrokerId).getOrElse(true)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+      // In the case of successive leader elections in a short time period, a follower may have&lt;br/&gt;
+      // entries in its log from a later epoch than any entry in the new leader&apos;s log. In order&lt;br/&gt;
+      // to ensure that these followers can truncate to the right offset, we must cache the new&lt;br/&gt;
+      // leader epoch and the start offset since it should be larger than any epoch that a follower&lt;br/&gt;
+      // would try to query.&lt;br/&gt;
+      leaderReplica.epochs.foreach &lt;/p&gt;
{ epochCache =&amp;gt;
+        epochCache.assign(leaderEpoch, leaderEpochStartOffset)
+      }
&lt;p&gt;+&lt;br/&gt;
+      val isNewLeader = !leaderReplicaIdOpt.contains(localBrokerId)&lt;br/&gt;
       val curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset&lt;br/&gt;
       val curTimeMs = time.milliseconds&lt;br/&gt;
       // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/cluster/Replica.scala b/core/src/main/scala/kafka/cluster/Replica.scala&lt;br/&gt;
index d729dadcb48..22860c71475 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Replica.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Replica.scala&lt;br/&gt;
@@ -18,7 +18,7 @@&lt;br/&gt;
 package kafka.cluster&lt;/p&gt;

&lt;p&gt; import kafka.log.Log&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.utils.Logging&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{LogOffsetMetadata, LogReadResult}
&lt;p&gt; import org.apache.kafka.common.&lt;/p&gt;
{KafkaException, TopicPartition}&lt;br/&gt;
@@ -55,7 +55,7 @@ class Replica(val brokerId: Int,&lt;br/&gt;
 &lt;br/&gt;
   def lastCaughtUpTimeMs: Long = _lastCaughtUpTimeMs&lt;br/&gt;
 &lt;br/&gt;
-  val epochs: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = log.map(_.leaderEpochCache)&lt;br/&gt;
+  val epochs: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; = log.map(_.leaderEpochCache)&lt;br/&gt;
 &lt;br/&gt;
   info(s&quot;Replica loaded for partition $topicPartition with initial high watermark $initialHighWatermarkValue&quot;)&lt;br/&gt;
   log.foreach(_.onHighWatermarkIncremented(initialHighWatermarkValue))&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index afe151d69b6..4e335ccc33b 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -32,15 +32,15 @@ import kafka.common.{LogSegmentOffsetOverflowException, LongRef, OffsetsOutOfOrd&lt;br/&gt;
 import kafka.message.{BrokerCompressionCodec, CompressionCodec, NoCompressionCodec}&lt;br/&gt;
 import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.server.checkpoints.{LeaderEpochCheckpointFile, LeaderEpochFile}&lt;br/&gt;
-import kafka.server.epoch.{LeaderEpochCache, LeaderEpochFileCache}&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.{BrokerTopicStats, FetchDataInfo, LogDirFailureChannel, LogOffsetMetadata}&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
-import org.apache.kafka.common.{KafkaException, TopicPartition}
&lt;p&gt;-import org.apache.kafka.common.errors.&lt;/p&gt;
{CorruptRecordException, InvalidOffsetException, KafkaStorageException, OffsetOutOfRangeException, RecordBatchTooLargeException, RecordTooLargeException, UnsupportedForMessageFormatException}
&lt;p&gt;+import org.apache.kafka.common.errors._&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
 import org.apache.kafka.common.requests.&lt;/p&gt;
{IsolationLevel, ListOffsetRequest}
&lt;p&gt; import org.apache.kafka.common.utils.&lt;/p&gt;
{Time, Utils}
&lt;p&gt;+import org.apache.kafka.common.&lt;/p&gt;
{KafkaException, TopicPartition}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.{ArrayBuffer, ListBuffer}&lt;br/&gt;
@@ -229,7 +229,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   /* the actual segments of the log */&lt;br/&gt;
   private val segments: ConcurrentNavigableMap&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Long, LogSegment&amp;#93;&lt;/span&gt; = new ConcurrentSkipListMap&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Long, LogSegment&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
-  @volatile private var _leaderEpochCache: LeaderEpochCache = initializeLeaderEpochCache()&lt;br/&gt;
+  @volatile private var _leaderEpochCache: LeaderEpochFileCache = initializeLeaderEpochCache()&lt;br/&gt;
 &lt;br/&gt;
   locally {&lt;br/&gt;
     val startMs = time.milliseconds&lt;br/&gt;
@@ -239,12 +239,12 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     /* Calculate the offset of the next message */&lt;br/&gt;
     nextOffsetMetadata = new LogOffsetMetadata(nextOffset, activeSegment.baseOffset, activeSegment.size)&lt;br/&gt;
 &lt;br/&gt;
-    _leaderEpochCache.clearAndFlushLatest(nextOffsetMetadata.messageOffset)&lt;br/&gt;
+    _leaderEpochCache.truncateFromEnd(nextOffsetMetadata.messageOffset)&lt;br/&gt;
 &lt;br/&gt;
     logStartOffset = math.max(logStartOffset, segments.firstEntry.getValue.baseOffset)&lt;br/&gt;
 &lt;br/&gt;
     // The earliest leader epoch may not be flushed during a hard failure. Recover it here.&lt;br/&gt;
-    _leaderEpochCache.clearAndFlushEarliest(logStartOffset)&lt;br/&gt;
+    _leaderEpochCache.truncateFromStart(logStartOffset)&lt;br/&gt;
 &lt;br/&gt;
     // Any segment loading or recovery code must not use producerStateManager, so that we can build the full state here&lt;br/&gt;
     // from scratch.&lt;br/&gt;
@@ -296,11 +296,11 @@ class Log(@volatile var dir: File,&lt;br/&gt;
 &lt;br/&gt;
   def leaderEpochCache = _leaderEpochCache&lt;br/&gt;
 &lt;br/&gt;
-  private def initializeLeaderEpochCache(): LeaderEpochCache = {&lt;br/&gt;
+  private def initializeLeaderEpochCache(): LeaderEpochFileCache = {
     // create the log directory if it doesn&apos;t exist
     Files.createDirectories(dir.toPath)
-    new LeaderEpochFileCache(topicPartition, () =&amp;gt; logEndOffsetMetadata,
-      new LeaderEpochCheckpointFile(LeaderEpochFile.newFile(dir), logDirFailureChannel))
+    val checkpointFile = new LeaderEpochCheckpointFile(LeaderEpochFile.newFile(dir), logDirFailureChannel)
+    new LeaderEpochFileCache(topicPartition, logEndOffset _, checkpointFile)
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -422,7 +422,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
    * @throws LogSegmentOffsetOverflowException if the segment contains messages that cause index offset overflow&lt;br/&gt;
    */&lt;br/&gt;
   private def recoverSegment(segment: LogSegment,&lt;br/&gt;
-                             leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = lock synchronized {&lt;br/&gt;
+                             leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; = None): Int = lock synchronized {&lt;br/&gt;
     val producerStateManager = new ProducerStateManager(topicPartition, dir, maxProducerIdExpirationMs)&lt;br/&gt;
     rebuildProducerState(segment.baseOffset, reloadFromCleanShutdown = false, producerStateManager)&lt;br/&gt;
     val bytesTruncated = segment.recover(producerStateManager, leaderEpochCache)&lt;br/&gt;
@@ -941,7 +941,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         if (newLogStartOffset &amp;gt; logStartOffset) {
           info(s&quot;Incrementing log start offset to $newLogStartOffset&quot;)
           logStartOffset = newLogStartOffset
-          _leaderEpochCache.clearAndFlushEarliest(logStartOffset)
+          _leaderEpochCache.truncateFromStart(logStartOffset)
           producerStateManager.truncateHead(logStartOffset)
           updateFirstUnstableOffset()
         }&lt;br/&gt;
@@ -1645,7 +1645,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
             updateLogEndOffset(targetOffset)&lt;br/&gt;
             this.recoveryPoint = math.min(targetOffset, this.recoveryPoint)&lt;br/&gt;
             this.logStartOffset = math.min(targetOffset, this.logStartOffset)&lt;br/&gt;
-            _leaderEpochCache.clearAndFlushLatest(targetOffset)&lt;br/&gt;
+            _leaderEpochCache.truncateFromEnd(targetOffset)&lt;br/&gt;
             loadProducerState(targetOffset, reloadFromCleanShutdown = false)&lt;br/&gt;
           }&lt;br/&gt;
           true&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogSegment.scala b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
index 0c00e5588f2..80763a8d797 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
@@ -23,7 +23,7 @@ import java.util.concurrent.TimeUnit&lt;br/&gt;
 &lt;br/&gt;
 import kafka.common.LogSegmentOffsetOverflowException&lt;br/&gt;
 import kafka.metrics.{KafkaMetricsGroup, KafkaTimer}&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.{FetchDataInfo, LogOffsetMetadata}&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.errors.CorruptRecordException&lt;br/&gt;
@@ -330,7 +330,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
    * @throws LogSegmentOffsetOverflowException if the log segment contains an offset that causes the index offset to overflow&lt;br/&gt;
    */&lt;br/&gt;
   @nonthreadsafe&lt;br/&gt;
-  def recover(producerStateManager: ProducerStateManager, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = {&lt;br/&gt;
+  def recover(producerStateManager: ProducerStateManager, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; = None): Int = {&lt;br/&gt;
     offsetIndex.reset()&lt;br/&gt;
     timeIndex.reset()&lt;br/&gt;
     txnIndex.reset()&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala b/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
index 88f5d6bd8e3..cee6bb66bdf 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
@@ -18,53 +18,69 @@ package kafka.server.epoch&lt;br/&gt;
 &lt;br/&gt;
 import java.util.concurrent.locks.ReentrantReadWriteLock&lt;br/&gt;
 &lt;br/&gt;
-import kafka.server.LogOffsetMetadata&lt;br/&gt;
 import kafka.server.checkpoints.LeaderEpochCheckpoint&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import kafka.utils.CoreUtils._&lt;br/&gt;
 import kafka.utils.Logging&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import scala.collection.mutable.ListBuffer&lt;br/&gt;
 &lt;br/&gt;
-trait LeaderEpochCache {
-  def assign(leaderEpoch: Int, offset: Long)
-  def latestEpoch: Int
-  def endOffsetFor(epoch: Int): (Int, Long)
-  def clearAndFlushLatest(offset: Long)
-  def clearAndFlushEarliest(offset: Long)
-  def clearAndFlush()
-  def clear()
-}&lt;br/&gt;
+import scala.collection.mutable.ListBuffer&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
-  * Represents a cache of (LeaderEpoch =&amp;gt; Offset) mappings for a particular replica.&lt;br/&gt;
-  *&lt;br/&gt;
-  * Leader Epoch = epoch assigned to each leader by the controller.&lt;br/&gt;
-  * Offset = offset of the first message in each epoch.&lt;br/&gt;
-  *&lt;br/&gt;
-  * @param leo a function that determines the log end offset&lt;br/&gt;
-  * @param checkpoint the checkpoint file&lt;br/&gt;
-  */&lt;br/&gt;
-class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetMetadata, checkpoint: LeaderEpochCheckpoint) extends LeaderEpochCache with Logging {&lt;br/&gt;
+ * Represents a cache of (LeaderEpoch =&amp;gt; Offset) mappings for a particular replica.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Leader Epoch = epoch assigned to each leader by the controller.&lt;br/&gt;
+ * Offset = offset of the first message in each epoch.&lt;br/&gt;
+ *&lt;br/&gt;
+ * @param topicPartition the associated topic partition&lt;br/&gt;
+ * @param checkpoint the checkpoint file&lt;br/&gt;
+ * @param logEndOffset function to fetch the current log end offset&lt;br/&gt;
+ */&lt;br/&gt;
+class LeaderEpochFileCache(topicPartition: TopicPartition,&lt;br/&gt;
+                           logEndOffset: () =&amp;gt; Long,&lt;br/&gt;
+                           checkpoint: LeaderEpochCheckpoint) extends Logging {&lt;br/&gt;
+  this.logIdent = s&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache $topicPartition&amp;#93;&lt;/span&gt; &quot;&lt;br/&gt;
+&lt;br/&gt;
   private val lock = new ReentrantReadWriteLock()&lt;br/&gt;
   private var epochs: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = inWriteLock(lock) { ListBuffer(checkpoint.read(): _*) }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
     * Assigns the supplied Leader Epoch to the supplied Offset&lt;br/&gt;
     * Once the epoch is assigned it cannot be reassigned&lt;br/&gt;
-    *&lt;br/&gt;
-    * @param epoch&lt;br/&gt;
-    * @param offset&lt;br/&gt;
     */&lt;br/&gt;
-  override def assign(epoch: Int, offset: Long): Unit = {&lt;br/&gt;
+  def assign(epoch: Int, startOffset: Long): Unit = {&lt;br/&gt;
     inWriteLock(lock) {&lt;br/&gt;
-      if (epoch &amp;gt;= 0 &amp;amp;&amp;amp; epoch &amp;gt; latestEpoch &amp;amp;&amp;amp; offset &amp;gt;= latestOffset) {&lt;br/&gt;
-        info(s&quot;Updated PartitionLeaderEpoch. ${epochChangeMsg(epoch, offset)}. Cache now contains ${epochs.size} entries.&quot;)&lt;br/&gt;
-        epochs += EpochEntry(epoch, offset)&lt;br/&gt;
-        flush()&lt;br/&gt;
+      val updateNeeded = if (epochs.isEmpty) {
+        true
       } else {
-        validateAndMaybeWarn(epoch, offset)
+        val lastEntry = epochs.last
+        lastEntry.epoch != epoch || startOffset &amp;lt; lastEntry.startOffset
       }&lt;br/&gt;
+&lt;br/&gt;
+      if (updateNeeded) {
+        truncateAndAppend(EpochEntry(epoch, startOffset))
+        flush()
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Remove any entries which violate monotonicity following the insertion of an assigned epoch.&lt;br/&gt;
+   */&lt;br/&gt;
+  private def truncateAndAppend(entryToAppend: EpochEntry): Unit = {&lt;br/&gt;
+    validateAndMaybeWarn(entryToAppend)&lt;br/&gt;
+&lt;br/&gt;
+    val (retainedEpochs, removedEpochs) = epochs.partition { entry =&amp;gt;
+      entry.epoch &amp;lt; entryToAppend.epoch &amp;amp;&amp;amp; entry.startOffset &amp;lt; entryToAppend.startOffset
+    }&lt;br/&gt;
+&lt;br/&gt;
+    epochs = retainedEpochs :+ entryToAppend&lt;br/&gt;
+&lt;br/&gt;
+    if (removedEpochs.isEmpty) {&lt;br/&gt;
+      debug(s&quot;Appended new epoch entry $entryToAppend. Cache now contains ${epochs.size} entries.&quot;)&lt;br/&gt;
+    } else {&lt;br/&gt;
+      warn(s&quot;New epoch entry $entryToAppend caused truncation of conflicting entries $removedEpochs. &quot; +&lt;br/&gt;
+        s&quot;Cache now contains ${epochs.size} entries.&quot;)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -74,7 +90,7 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;br/&gt;
     *&lt;br/&gt;
     * @return&lt;br/&gt;
     */&lt;br/&gt;
-  override def latestEpoch(): Int = {&lt;br/&gt;
+  def latestEpoch: Int = {&lt;br/&gt;
     inReadLock(lock) {
       if (epochs.isEmpty) UNDEFINED_EPOCH else epochs.last.epoch
     }&lt;br/&gt;
@@ -93,45 +109,59 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;br/&gt;
     * so that the follower falls back to High Water Mark.&lt;br/&gt;
     *&lt;br/&gt;
     * @param requestedEpoch requested leader epoch&lt;br/&gt;
-    * @return leader epoch and offset&lt;br/&gt;
+    * @return found leader epoch and end offset&lt;br/&gt;
     */&lt;br/&gt;
-  override def endOffsetFor(requestedEpoch: Int): (Int, Long) = {&lt;br/&gt;
+  def endOffsetFor(requestedEpoch: Int): (Int, Long) = {&lt;br/&gt;
     inReadLock(lock) {&lt;br/&gt;
       val epochAndOffset =&lt;br/&gt;
         if (requestedEpoch == UNDEFINED_EPOCH) {
-          // this may happen if a bootstrapping follower sends a request with undefined epoch or
+          // This may happen if a bootstrapping follower sends a request with undefined epoch or
           // a follower is on the older message format where leader epochs are not recorded
           (UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
         } else if (requestedEpoch == latestEpoch) {
-          (requestedEpoch, leo().messageOffset)
+          // For the leader, the latest epoch is always the current leader epoch that is still being written to.
+          // Followers should not have any reason to query for the end offset of the current epoch, but a consumer
+          // might if it is verifying its committed offset following a group rebalance. In this case, we return
+          // the current log end offset which makes the truncation check work as expected.
+          (requestedEpoch, logEndOffset())
         } else {&lt;br/&gt;
           val (subsequentEpochs, previousEpochs) = epochs.partition { e =&amp;gt; e.epoch &amp;gt; requestedEpoch}&lt;br/&gt;
-          if (subsequentEpochs.isEmpty || requestedEpoch &amp;lt; epochs.head.epoch)&lt;br/&gt;
-            // no epochs recorded or requested epoch &amp;lt; the first epoch cached&lt;br/&gt;
+          if (subsequentEpochs.isEmpty) {&lt;br/&gt;
+            // The requested epoch is larger than any known epoch. This case should never be hit because&lt;br/&gt;
+            // the latest cached epoch is always the largest.&lt;br/&gt;
             (UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
-          else {
-            // we must get at least one element in previous epochs list, because if we are here,
-            // it means that requestedEpoch &amp;gt;= epochs.head.epoch -- so at least the first epoch is
+          } else if (previousEpochs.isEmpty) {
+            // The requested epoch is smaller than any known epoch, so we return the start offset of the first
+            // known epoch which is larger than it. This may be inaccurate as there could have been
+            // epochs in between, but the point is that the data has already been removed from the log
+            // and we want to ensure that the follower can replicate correctly beginning from the leader&apos;s
+            // start offset.
+            (requestedEpoch, subsequentEpochs.head.startOffset)
+          } else {
+            // We have at least one previous epoch and one subsequent epoch. The result is the first
+            // prior epoch and the starting offset of the first subsequent epoch.
             (previousEpochs.last.epoch, subsequentEpochs.head.startOffset)
           }&lt;br/&gt;
         }&lt;br/&gt;
-      debug(s&quot;Processed offset for epoch request for partition ${topicPartition} epoch:$requestedEpoch and returning epoch ${epochAndOffset._1} and offset ${epochAndOffset._2} from epoch list of size ${epochs.size}&quot;)&lt;br/&gt;
+      debug(s&quot;Processed end offset request for epoch $requestedEpoch and returning epoch ${epochAndOffset._1} &quot; +&lt;br/&gt;
+        s&quot;with end offset ${epochAndOffset._2} from epoch cache of size ${epochs.size}&quot;)&lt;br/&gt;
       epochAndOffset&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
     * Removes all epoch entries from the store with start offsets greater than or equal to the passed offset.&lt;br/&gt;
-    *&lt;br/&gt;
-    * @param offset&lt;br/&gt;
     */&lt;br/&gt;
-  override def clearAndFlushLatest(offset: Long): Unit = {&lt;br/&gt;
+  def truncateFromEnd(endOffset: Long): Unit = {&lt;br/&gt;
     inWriteLock(lock) {&lt;br/&gt;
-      val before = epochs&lt;br/&gt;
-      if (offset &amp;gt;= 0 &amp;amp;&amp;amp; offset &amp;lt;= latestOffset()) {&lt;br/&gt;
-        epochs = epochs.filter(entry =&amp;gt; entry.startOffset &amp;lt; offset)&lt;br/&gt;
+      if (endOffset &amp;gt;= 0 &amp;amp;&amp;amp; latestEntry.exists(_.startOffset &amp;gt;= endOffset)) {&lt;br/&gt;
+        val (subsequentEntries, previousEntries) = epochs.partition(_.startOffset &amp;gt;= endOffset)&lt;br/&gt;
+        epochs = previousEntries&lt;br/&gt;
+&lt;br/&gt;
         flush()&lt;br/&gt;
-        info(s&quot;Cleared latest ${before.toSet.filterNot(epochs.toSet)} entries from epoch cache based on passed offset $offset leaving ${epochs.size} in EpochFile for partition $topicPartition&quot;)&lt;br/&gt;
+&lt;br/&gt;
+        debug(s&quot;Cleared entries $subsequentEntries from epoch cache after &quot; +&lt;br/&gt;
+          s&quot;truncating to end offset $endOffset, leaving ${epochs.size} entries in the cache.&quot;)&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -142,20 +172,21 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;br/&gt;
     *&lt;br/&gt;
     * This method is exclusive: so clearEarliest(6) will retain an entry at offset 6.&lt;br/&gt;
     *&lt;br/&gt;
-    * @param offset the offset to clear up to&lt;br/&gt;
+    * @param startOffset the offset to clear up to&lt;br/&gt;
     */&lt;br/&gt;
-  override def clearAndFlushEarliest(offset: Long): Unit = {&lt;br/&gt;
+  def truncateFromStart(startOffset: Long): Unit = {&lt;br/&gt;
     inWriteLock(lock) {&lt;br/&gt;
-      val before = epochs&lt;br/&gt;
-      if (offset &amp;gt;= 0 &amp;amp;&amp;amp; earliestOffset() &amp;lt; offset) {&lt;br/&gt;
-        val earliest = epochs.filter(entry =&amp;gt; entry.startOffset &amp;lt; offset)&lt;br/&gt;
-        if (earliest.nonEmpty) {&lt;br/&gt;
-          epochs = epochs --= earliest&lt;br/&gt;
-          //If the offset is less than the earliest offset remaining, add previous epoch back, but with an updated offset&lt;br/&gt;
-          if (offset &amp;lt; earliestOffset() || epochs.isEmpty)&lt;br/&gt;
-            new EpochEntry(earliest.last.epoch, offset) +=: epochs&lt;br/&gt;
+      if (epochs.nonEmpty) {&lt;br/&gt;
+        val (subsequentEntries, previousEntries) = epochs.partition(_.startOffset &amp;gt; startOffset)&lt;br/&gt;
+&lt;br/&gt;
+        previousEntries.lastOption.foreach { firstBeforeStartOffset =&amp;gt;&lt;br/&gt;
+          val updatedFirstEntry = EpochEntry(firstBeforeStartOffset.epoch, startOffset)&lt;br/&gt;
+          epochs = updatedFirstEntry +: subsequentEntries&lt;br/&gt;
+&lt;br/&gt;
           flush()&lt;br/&gt;
-          info(s&quot;Cleared earliest ${before.toSet.filterNot(epochs.toSet).size} entries from epoch cache based on passed offset $offset leaving ${epochs.size} in EpochFile for partition $topicPartition&quot;)&lt;br/&gt;
+&lt;br/&gt;
+          debug(s&quot;Cleared entries $previousEntries and rewrote first entry $updatedFirstEntry after &quot; +&lt;br/&gt;
+            s&quot;truncating to start offset $startOffset, leaving ${epochs.size} in the cache.&quot;)&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
@@ -164,47 +195,55 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;br/&gt;
   /**&lt;br/&gt;
     * Delete all entries.&lt;br/&gt;
     */&lt;br/&gt;
-  override def clearAndFlush() = {&lt;br/&gt;
+  def clearAndFlush() = {&lt;br/&gt;
     inWriteLock(lock) {
       epochs.clear()
       flush()
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  override def clear() = {&lt;br/&gt;
+  def clear() = {&lt;br/&gt;
     inWriteLock(lock) {
       epochs.clear()
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def epochEntries(): ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+  // Visible for testing&lt;br/&gt;
+  def epochEntries: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = {
     epochs
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def earliestOffset(): Long = {
-    if (epochs.isEmpty) -1 else epochs.head.startOffset
-  }&lt;br/&gt;
-&lt;br/&gt;
-  private def latestOffset(): Long = {
-    if (epochs.isEmpty) -1 else epochs.last.startOffset
-  }&lt;br/&gt;
+  private def latestEntry: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = epochs.lastOption&lt;br/&gt;
 &lt;br/&gt;
   private def flush(): Unit = {
     checkpoint.write(epochs)
   }&lt;br/&gt;
 &lt;br/&gt;
-  def epochChangeMsg(epoch: Int, offset: Long) = s&quot;New: {epoch:$epoch, offset:$offset}, Current: {epoch:$latestEpoch, offset:$latestOffset} for Partition: $topicPartition&quot;&lt;br/&gt;
-&lt;br/&gt;
-  def validateAndMaybeWarn(epoch: Int, offset: Long) = {&lt;br/&gt;
-    assert(epoch &amp;gt;= 0, s&quot;Received a PartitionLeaderEpoch assignment for an epoch &amp;lt; 0. This should not happen. ${epochChangeMsg(epoch, offset)}&quot;)&lt;br/&gt;
-    if (epoch &amp;lt; latestEpoch())&lt;br/&gt;
-      warn(s&quot;Received a PartitionLeaderEpoch assignment for an epoch &amp;lt; latestEpoch. &quot; +&lt;br/&gt;
-        s&quot;This implies messages have arrived out of order. ${epochChangeMsg(epoch, offset)}&quot;)&lt;br/&gt;
-    else if (offset &amp;lt; latestOffset())&lt;br/&gt;
-      warn(s&quot;Received a PartitionLeaderEpoch assignment for an offset &amp;lt; latest offset for the most recent, stored PartitionLeaderEpoch. &quot; +&lt;br/&gt;
-        s&quot;This implies messages have arrived out of order. ${epochChangeMsg(epoch, offset)}&quot;)&lt;br/&gt;
+  private def validateAndMaybeWarn(entry: EpochEntry) = {&lt;br/&gt;
+    if (entry.epoch &amp;lt; 0) {
+      throw new IllegalArgumentException(s&quot;Received invalid partition leader epoch entry $entry&quot;)
+    } else {&lt;br/&gt;
+      // If the latest append violates the monotonicity of epochs or starting offsets, our choices&lt;br/&gt;
+      // are either to raise an error, ignore the append, or allow the append and truncate the&lt;br/&gt;
+      // conflicting entries from the cache. Raising an error risks killing the fetcher threads in&lt;br/&gt;
+      // pathological cases (i.e. cases we are not yet aware of). We instead take the final approach&lt;br/&gt;
+      // and assume that the latest append is always accurate.&lt;br/&gt;
+&lt;br/&gt;
+      latestEntry.foreach { latest =&amp;gt;
+        if (entry.epoch &amp;lt; latest.epoch)
+          warn(s&quot;Received leader epoch assignment $entry which has an epoch less than the epoch &quot; +
+            s&quot;of the latest entry $latest. This implies messages have arrived out of order.&quot;)
+        else if (entry.startOffset &amp;lt; latest.startOffset)
+          warn(s&quot;Received leader epoch assignment $entry which has a starting offset which is less than &quot; +
+            s&quot;the starting offset of the latest entry $latest. This implies messages have arrived out of order.&quot;)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 // Mapping of epoch to the first offset of the subsequent epoch&lt;br/&gt;
-case class EpochEntry(epoch: Int, startOffset: Long)&lt;br/&gt;
+case class EpochEntry(epoch: Int, startOffset: Long) {&lt;br/&gt;
+  override def toString: String = {
+    s&quot;EpochEntry(epoch=$epoch, startOffset=$startOffset)&quot;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
index 343693e82c7..7cdc7789963 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
@@ -97,6 +97,39 @@ class PartitionTest {
     replicaManager.shutdown(checkpointHW = false)
   }&lt;br/&gt;
 &lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testMakeLeaderUpdatesEpochCache(): Unit = {
+    val controllerEpoch = 3
+    val leader = brokerId
+    val follower = brokerId + 1
+    val controllerId = brokerId + 3
+    val replicas = List[Integer](leader, follower).asJava
+    val isr = List[Integer](leader, follower).asJava
+    val leaderEpoch = 8
+
+    val log = logManager.getOrCreateLog(topicPartition, logConfig)
+    log.appendAsLeader(MemoryRecords.withRecords(0L, CompressionType.NONE, 0,
+      new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),
+      new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)
+    ), leaderEpoch = 0)
+    log.appendAsLeader(MemoryRecords.withRecords(0L, CompressionType.NONE, 5,
+      new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes),
+      new SimpleRecord(&quot;k4&quot;.getBytes, &quot;v4&quot;.getBytes)
+    ), leaderEpoch = 5)
+    assertEquals(4, log.logEndOffset)
+
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)
+    assertTrue(&quot;Expected makeLeader to succeed&quot;,
+      partition.makeLeader(controllerId, new LeaderAndIsrRequest.PartitionState(controllerEpoch, leader, leaderEpoch,
+        isr, 1, replicas, true), 0))
+
+    assertEquals(Some(4), partition.leaderReplicaIfLocal.map(_.logEndOffset.messageOffset))
+
+    val epochEndOffset = partition.lastOffsetForLeaderEpoch(leaderEpoch)
+    assertEquals(4, epochEndOffset.endOffset)
+    assertEquals(leaderEpoch, epochEndOffset.leaderEpoch)
+  }&lt;br/&gt;
+&lt;br/&gt;
   @Test&lt;br/&gt;
   // Verify that partition.removeFutureLocalReplica() and partition.maybeReplaceCurrentWithFutureReplica() can run concurrently&lt;br/&gt;
   def testMaybeReplaceCurrentWithFutureReplica(): Unit = {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index 9a9bc613585..e584b8cedbf 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -25,7 +25,7 @@ import java.util.Properties&lt;br/&gt;
 import kafka.api.{ApiVersion, KAFKA_0_11_0_IV0}&lt;br/&gt;
 import kafka.common.{OffsetsOutOfOrderException, UnexpectedAppendOffsetException}&lt;br/&gt;
 import kafka.log.Log.DeleteDirSuffix&lt;br/&gt;
-import kafka.server.epoch.{EpochEntry, LeaderEpochCache, LeaderEpochFileCache}&lt;br/&gt;
+import kafka.server.epoch.{EpochEntry, LeaderEpochFileCache}&lt;br/&gt;
 import kafka.server.{BrokerTopicStats, FetchDataInfo, KafkaConfig, LogDirFailureChannel}&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.{KafkaException, TopicPartition}
&lt;p&gt;@@ -287,7 +287,7 @@ class LogTest {&lt;br/&gt;
             }&lt;/p&gt;

&lt;p&gt;             override def recover(producerStateManager: ProducerStateManager,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;): Int = {&lt;br/&gt;
+                                 leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;): Int = 
{
               recoveredSegments += this
               super.recover(producerStateManager, leaderEpochCache)
             }
&lt;p&gt;@@ -2589,8 +2589,8 @@ class LogTest {&lt;br/&gt;
     log.onHighWatermarkIncremented(log.logEndOffset)&lt;br/&gt;
     log.deleteOldSegments()&lt;br/&gt;
     assertEquals(&quot;The deleted segments should be gone.&quot;, 1, log.numberOfSegments)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries().size)&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Epoch entry should be the latest epoch and the leo.&quot;, EpochEntry(1, 100), epochCache(log).epochEntries().head)&lt;br/&gt;
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries.size)&lt;br/&gt;
+    assertEquals(&quot;Epoch entry should be the latest epoch and the leo.&quot;, EpochEntry(1, 100), epochCache(log).epochEntries.head)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 100)&lt;br/&gt;
@@ -2599,7 +2599,7 @@ class LogTest &lt;/p&gt;
{
     log.delete()
     assertEquals(&quot;The number of segments should be 0&quot;, 0, log.numberOfSegments)
     assertEquals(&quot;The number of deleted segments should be zero.&quot;, 0, log.deleteOldSegments())
-    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries().size)
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries.size)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -2612,12 +2612,12 @@ class LogTest &lt;/p&gt;
{
     log.appendAsLeader(createRecords, leaderEpoch = 0)
 
     assertEquals(&quot;The deleted segments should be gone.&quot;, 1, log.numberOfSegments)
-    assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries().size)
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries.size)
 
     log.close()
     log.delete()
     assertEquals(&quot;The number of segments should be 0&quot;, 0, log.numberOfSegments)
-    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries().size)
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries.size)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -2790,7 +2790,7 @@ class LogTest &lt;/p&gt;
{
     for (i &amp;lt;- records.indices)
       log.appendAsFollower(recordsForEpoch(i))
 
-    assertEquals(42, log.leaderEpochCache.asInstanceOf[LeaderEpochFileCache].latestEpoch())
+    assertEquals(42, log.leaderEpochCache.latestEpoch)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -2845,19 +2845,24 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldTruncateLeaderEpochFileWhenTruncatingLog() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;/li&gt;
	&lt;li&gt;val logConfig = LogTest.createLogConfig(segmentBytes = 10 * createRecords.sizeInBytes)&lt;br/&gt;
+    def createRecords(startOffset: Long, epoch: Int): MemoryRecords = 
{
+      TestUtils.records(Seq(new SimpleRecord(&quot;value&quot;.getBytes)),
+        baseOffset = startOffset, partitionLeaderEpoch = epoch)
+    }
&lt;p&gt;+&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 10 * createRecords(0, 0).sizeInBytes)&lt;br/&gt;
     val log = createLog(logDir, logConfig)&lt;br/&gt;
     val cache = epochCache(log)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Given 2 segments, 10 messages per segment&lt;/li&gt;
	&lt;li&gt;for (epoch &amp;lt;- 1 to 20)&lt;/li&gt;
	&lt;li&gt;log.appendAsLeader(createRecords, leaderEpoch = 0)&lt;br/&gt;
+    def append(epoch: Int, startOffset: Long, count: Int): Unit = 
{
+      for (i &amp;lt;- 0 until count)
+        log.appendAsFollower(createRecords(startOffset + i, epoch))
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Simulate some leader changes at specific offsets&lt;/li&gt;
	&lt;li&gt;cache.assign(0, 0)&lt;/li&gt;
	&lt;li&gt;cache.assign(1, 10)&lt;/li&gt;
	&lt;li&gt;cache.assign(2, 16)&lt;br/&gt;
+    //Given 2 segments, 10 messages per segment&lt;br/&gt;
+    append(epoch = 0, startOffset = 0, count = 10)&lt;br/&gt;
+    append(epoch = 1, startOffset = 10, count = 6)&lt;br/&gt;
+    append(epoch = 2, startOffset = 16, count = 4)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(2, log.numberOfSegments)&lt;br/&gt;
     assertEquals(20, log.logEndOffset)&lt;br/&gt;
@@ -2909,7 +2914,7 @@ class LogTest {&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(1, 0), EpochEntry(2, 1), EpochEntry(3, 3)), leaderEpochCache.epochEntries)&lt;/p&gt;

&lt;p&gt;     // deliberately remove some of the epoch entries&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leaderEpochCache.clearAndFlushLatest(2)&lt;br/&gt;
+    leaderEpochCache.truncateFromEnd(2)&lt;br/&gt;
     assertNotEquals(ListBuffer(EpochEntry(1, 0), EpochEntry(2, 1), EpochEntry(3, 3)), leaderEpochCache.epochEntries)&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala b/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala&lt;br/&gt;
index c90a5b97a1a..3dff709bef6 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala&lt;br/&gt;
@@ -22,7 +22,7 @@ import java.util.concurrent.atomic.AtomicBoolean&lt;/p&gt;

&lt;p&gt; import kafka.cluster.&lt;/p&gt;
{Partition, Replica}
&lt;p&gt; import kafka.log.Log&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
@@ -253,7 +253,7 @@ class IsrExpirationTest {&lt;/p&gt;

&lt;p&gt;   private def logMock: Log = {&lt;br/&gt;
     val log = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.Log&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val cache = EasyMock.createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val cache = EasyMock.createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     EasyMock.expect(log.dir).andReturn(TestUtils.tempDir()).anyTimes()&lt;br/&gt;
     EasyMock.expect(log.leaderEpochCache).andReturn(cache).anyTimes()&lt;br/&gt;
     EasyMock.expect(log.onHighWatermarkIncremented(0L))&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
index 8fb5ab6feba..2e28ee13f2d 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
@@ -21,7 +21,7 @@ import kafka.api.Request&lt;br/&gt;
 import kafka.cluster.
{BrokerEndPoint, Partition, Replica}
&lt;p&gt; import kafka.log.LogManager&lt;br/&gt;
 import kafka.server.AbstractFetcherThread.ResultWithPartitions&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.utils.&lt;/p&gt;
{DelayedItem, TestUtils}
&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.errors.&lt;/p&gt;
{KafkaStorageException, ReplicaNotAvailableException}
&lt;p&gt;@@ -46,7 +46,7 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all dependencies&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val futureReplica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val partition = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -87,7 +87,7 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all dependencies&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val partition = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -133,9 +133,9 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;br/&gt;
     val quotaManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochsT1p0 = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochsT1p1 = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochsT1p0 = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochsT1p1 = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaT1p0 = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaT1p1 = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -195,8 +195,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;br/&gt;
     val quotaManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     // one future replica mock because our mocking methods return same values for both future replicas&lt;br/&gt;
@@ -265,8 +265,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val futureReplica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val partition = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val responseCallback: Capture[Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchPartitionData)&amp;#93;&lt;/span&gt; =&amp;gt; Unit]  = EasyMock.newCapture()&lt;br/&gt;
@@ -319,8 +319,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;br/&gt;
     val quotaManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val futureReplica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -401,8 +401,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     //Setup all dependencies&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;br/&gt;
     val quotaManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val futureReplicaLeaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val futureReplica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
index 520801c9bab..9440c29ee7b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
@@ -20,7 +20,7 @@ import kafka.cluster.
{BrokerEndPoint, Replica}
&lt;p&gt; import kafka.log.LogManager&lt;br/&gt;
 import kafka.cluster.Partition&lt;br/&gt;
 import kafka.server.QuotaFactory.UnboundedQuota&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.epoch.util.ReplicaFetcherMockBlockingSend&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.clients.ClientResponse&lt;br/&gt;
@@ -154,7 +154,7 @@ class ReplicaFetcherThreadTest {&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all dependencies&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -278,7 +278,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all dependencies&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -339,7 +339,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -388,7 +388,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -442,7 +442,7 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Setup all dependencies&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -513,7 +513,7 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Setup all dependencies&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -574,7 +574,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -619,7 +619,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -677,7 +677,7 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all stubs&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -728,7 +728,7 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all stubs&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
index 08440528638..90d488dd37a 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
@@ -26,7 +26,7 @@ import kafka.log.
{Log, LogConfig, LogManager, ProducerStateManager}
&lt;p&gt; import kafka.utils.&lt;/p&gt;
{MockScheduler, MockTime, TestUtils}
&lt;p&gt; import TestUtils.createBroker&lt;br/&gt;
 import kafka.cluster.BrokerEndPoint&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.epoch.util.ReplicaFetcherMockBlockingSend&lt;br/&gt;
 import kafka.utils.timer.MockTimer&lt;br/&gt;
 import kafka.zk.KafkaZkClient&lt;br/&gt;
@@ -641,7 +641,7 @@ class ReplicaManagerTest {&lt;br/&gt;
     val mockScheduler = new MockScheduler(time)&lt;br/&gt;
     val mockBrokerTopicStats = new BrokerTopicStats&lt;br/&gt;
     val mockLogDirFailureChannel = new LogDirFailureChannel(config.logDirs.size)&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;val mockLeaderEpochCache = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val mockLeaderEpochCache = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     EasyMock.expect(mockLeaderEpochCache.latestEpoch).andReturn(leaderEpochFromLeader)&lt;br/&gt;
     EasyMock.expect(mockLeaderEpochCache.endOffsetFor(leaderEpochFromLeader))&lt;br/&gt;
       .andReturn((leaderEpochFromLeader, localLogOffset))&lt;br/&gt;
@@ -661,7 +661,7 @@ class ReplicaManagerTest {&lt;br/&gt;
         new File(new File(config.logDirs.head), s&quot;$topic-$topicPartition&quot;), 30000),&lt;br/&gt;
       logDirFailureChannel = mockLogDirFailureChannel) 
{
 
-      override def leaderEpochCache: LeaderEpochCache = mockLeaderEpochCache
+      override def leaderEpochCache: LeaderEpochFileCache = mockLeaderEpochCache
 
       override def logEndOffsetMetadata = LogOffsetMetadata(localLogOffset)
     }
&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala b/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala&lt;br/&gt;
index e7c6a9785bc..0c47f15a09f 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala&lt;br/&gt;
@@ -24,7 +24,6 @@ import org.junit.Assert._&lt;br/&gt;
 import org.junit.Test&lt;br/&gt;
 import org.scalatest.junit.JUnitSuite&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
 class LeaderEpochCheckpointFileTest extends JUnitSuite with Logging&lt;/p&gt;
{
 
   @Test
diff --git a/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala b/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala
index 48590190b57..5c37891c937 100644
--- a/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala
+++ b/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala
@@ -90,23 +90,23 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness
     assertEquals(0, latestRecord(follower).partitionLeaderEpoch())
 
     //Both leader and follower should have recorded Epoch 0 at Offset 0
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries)
 
     //Bounce the follower
     bounce(follower)
     awaitISR(tp)
 
     //Nothing happens yet as we haven&apos;t sent any new messages.
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries)
 
     //Send a message
     producer.send(new ProducerRecord(topic, 0, null, msg)).get
 
     //Epoch1 should now propagate to the follower with the written message
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries)
 
     //The new message should have epoch 1 stamped
     assertEquals(1, latestRecord(leader).partitionLeaderEpoch())
@@ -117,8 +117,8 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness
     awaitISR(tp)
 
     //Epochs 2 should be added to the leader, but not on the follower (yet), as there has been no replication.
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries)
 
     //Send a message
     producer.send(new ProducerRecord(topic, 0, null, msg)).get
@@ -128,8 +128,8 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness
     assertEquals(2, latestRecord(follower).partitionLeaderEpoch())
 
     //The leader epoch files should now match on leader and follower
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(follower).epochEntries)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -377,8 +377,8 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness&lt;/p&gt;

&lt;p&gt;   private def log(leader: KafkaServer, follower: KafkaServer): Unit = {&lt;br/&gt;
     info(s&quot;Bounce complete for follower ${follower.config.brokerId}&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info(s&quot;Leader: leo${leader.config.brokerId}: &quot; + getLog(leader, 0).logEndOffset + &quot; cache: &quot; + epochCache(leader).epochEntries())&lt;/li&gt;
	&lt;li&gt;info(s&quot;Follower: leo${follower.config.brokerId}: &quot; + getLog(follower, 0).logEndOffset + &quot; cache: &quot; + epochCache(follower).epochEntries())&lt;br/&gt;
+    info(s&quot;Leader: leo${leader.config.brokerId}: &quot; + getLog(leader, 0).logEndOffset + &quot; cache: &quot; + epochCache(leader).epochEntries)&lt;br/&gt;
+    info(s&quot;Follower: leo${follower.config.brokerId}: &quot; + getLog(follower, 0).logEndOffset + &quot; cache: &quot; + epochCache(follower).epochEntries)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def waitForLogsToMatch(b1: KafkaServer, b2: KafkaServer, partition: Int = 0): Unit = {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
index d1f93900ccf..7ac606a2dc5 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
@@ -16,6 +16,7 @@&lt;br/&gt;
   */&lt;/p&gt;

&lt;p&gt; package kafka.server.epoch&lt;br/&gt;
+&lt;br/&gt;
 import java.io.File&lt;/p&gt;

&lt;p&gt; import kafka.server.LogOffsetMetadata&lt;br/&gt;
@@ -24,7 +25,7 @@ import org.apache.kafka.common.requests.EpochEndOffset.{UNDEFINED_EPOCH, UNDEFIN&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
-import org.junit.&lt;/p&gt;
{Before, Test}
&lt;p&gt;+import org.junit.Test&lt;/p&gt;

&lt;p&gt; import scala.collection.mutable.ListBuffer&lt;/p&gt;

&lt;p&gt;@@ -33,54 +34,44 @@ import scala.collection.mutable.ListBuffer&lt;br/&gt;
   */&lt;br/&gt;
 class LeaderEpochFileCacheTest {&lt;br/&gt;
   val tp = new TopicPartition(&quot;TestTopic&quot;, 5)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var checkpoint: LeaderEpochCheckpoint = _&lt;br/&gt;
+  private var logEndOffset = 0L&lt;br/&gt;
+  private val checkpoint: LeaderEpochCheckpoint = new LeaderEpochCheckpoint 
{
+    private var epochs: Seq[EpochEntry] = Seq()
+    override def write(epochs: Seq[EpochEntry]): Unit = this.epochs = epochs
+    override def read(): Seq[EpochEntry] = this.epochs
+  }
&lt;p&gt;+  private val cache = new LeaderEpochFileCache(tp, logEndOffset _, checkpoint)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldAddEpochAndMessageOffsetToCache() = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //When
-    cache.assign(epoch = 2, offset = 10)
-    leo = 11
+    cache.assign(epoch = 2, startOffset = 10)
+    logEndOffset = 11
 
     //Then
-    assertEquals(2, cache.latestEpoch())
-    assertEquals(EpochEntry(2, 10), cache.epochEntries()(0))
-    assertEquals((2, leo), cache.endOffsetFor(2)) //should match leo
+    assertEquals(2, cache.latestEpoch)
+    assertEquals(EpochEntry(2, 10), cache.epochEntries(0))
+    assertEquals((2, logEndOffset), cache.endOffsetFor(2)) //should match logEndOffset
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnLogEndOffsetIfLatestEpochRequested() = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //When just one epoch
-    cache.assign(epoch = 2, offset = 11)
-    cache.assign(epoch = 2, offset = 12)
-    leo = 14
+    cache.assign(epoch = 2, startOffset = 11)
+    cache.assign(epoch = 2, startOffset = 12)
+    logEndOffset = 14
 
     //Then
-    assertEquals((2, leo), cache.endOffsetFor(2))
+    assertEquals((2, logEndOffset), cache.endOffsetFor(2))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnUndefinedOffsetIfUndefinedEpochRequested() = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
     val expectedEpochEndOffset = (UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Given cache with some data on leader&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
     // assign couple of epochs&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 11)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 12)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 11)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 12)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When (say a bootstraping follower) sends request for UNDEFINED_EPOCH&lt;br/&gt;
     val epochAndOffsetFor = cache.endOffsetFor(UNDEFINED_EPOCH)&lt;br/&gt;
@@ -92,68 +83,51 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotOverwriteLogEndOffsetForALeaderEpochOnceItHasBeenAssigned() = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
     //Given
-    leo = 9
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
+    logEndOffset = 9
 
-    cache.assign(2, leo)
+    cache.assign(2, logEndOffset)
 
     //When called again later
     cache.assign(2, 10)
 
     //Then the offset should NOT have been updated
-    assertEquals(leo, cache.epochEntries()(0).startOffset)
+    assertEquals(logEndOffset, cache.epochEntries(0).startOffset)
+    assertEquals(ListBuffer(EpochEntry(2, 9)), cache.epochEntries)
   }

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldAllowLeaderEpochToChangeEvenIfOffsetDoesNot() = {&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
+  def shouldEnforceMonotonicallyIncreasingStartOffsets() = 
{
     //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
     cache.assign(2, 9)
 
     //When update epoch new epoch but same offset
     cache.assign(3, 9)
 
     //Then epoch should have been updated
-    assertEquals(ListBuffer(EpochEntry(2, 9), EpochEntry(3, 9)), cache.epochEntries())
+    assertEquals(ListBuffer(EpochEntry(3, 9)), cache.epochEntries)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotOverwriteOffsetForALeaderEpochOnceItHasBeenAssigned() = &lt;/p&gt;
{
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; new LogOffsetMetadata(0), checkpoint)
     cache.assign(2, 6)
 
     //When called again later with a greater offset
     cache.assign(2, 10)
 
     //Then later update should have been ignored
-    assertEquals(6, cache.epochEntries()(0).startOffset)
+    assertEquals(6, cache.epochEntries(0).startOffset)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnUnsupportedIfNoEpochRecorded()&lt;/p&gt;
{
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
     assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), cache.endOffsetFor(0))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnUnsupportedIfNoEpochRecordedAndUndefinedEpochRequested(){&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leo = 73&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
+    logEndOffset = 73&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When (say a follower on older message format version) sends request for UNDEFINED_EPOCH&lt;br/&gt;
     val offsetFor = cache.endOffsetFor(UNDEFINED_EPOCH)&lt;br/&gt;
@@ -164,39 +138,41 @@ class LeaderEpochFileCacheTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldReturnUnsupportedIfRequestedEpochLessThanFirstEpoch(){&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 5, offset = 11)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 6, offset = 12)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 7, offset = 13)&lt;br/&gt;
+  def shouldReturnFirstEpochIfRequestedEpochLessThanFirstEpoch()
{
+    cache.assign(epoch = 5, startOffset = 11)
+    cache.assign(epoch = 6, startOffset = 12)
+    cache.assign(epoch = 7, startOffset = 13)
 
     //When
-    val epochAndOffset = cache.endOffsetFor(5 - 1)
+    val epochAndOffset = cache.endOffsetFor(4)
 
     //Then
-    assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), epochAndOffset)
+    assertEquals((4, 11), epochAndOffset)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldGetFirstOffsetOfSubsequentEpochWhenOffsetRequestedForPreviousEpoch() = {&lt;/li&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
+  def shouldTruncateIfMatchingEpochButEarlierStartingOffset(): Unit = 
{
+    cache.assign(epoch = 5, startOffset = 11)
+    cache.assign(epoch = 6, startOffset = 12)
+    cache.assign(epoch = 7, startOffset = 13)
 
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
+    // epoch 7 starts at an earlier offset
+    cache.assign(epoch = 7, startOffset = 12)
 
+    assertEquals((5, 12), cache.endOffsetFor(5))
+    assertEquals((5, 12), cache.endOffsetFor(6))
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def shouldGetFirstOffsetOfSubsequentEpochWhenOffsetRequestedForPreviousEpoch() = {&lt;br/&gt;
     //When several epochs&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 11)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 12)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 13)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 14)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 15)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 16)&lt;/li&gt;
	&lt;li&gt;leo = 17&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 11)&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 12)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 13)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 14)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 15)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 16)&lt;br/&gt;
+    logEndOffset = 17&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then get the start offset of the next epoch&lt;br/&gt;
     assertEquals((2, 15), cache.endOffsetFor(2))&lt;br/&gt;
@@ -204,15 +180,10 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnNextAvailableEpochIfThereIsNoExactEpochForTheOneRequested(){&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
     //When&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 10)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 13)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 17)&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 10)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 13)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 17)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals((0, 13), cache.endOffsetFor(requestedEpoch = 1))&lt;br/&gt;
@@ -222,14 +193,9 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotUpdateEpochAndStartOffsetIfItDidNotChange() = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
     //When&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 7)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 7)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals(1, cache.epochEntries.size)&lt;br/&gt;
@@ -238,14 +204,10 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnInvalidOffsetIfEpochIsRequestedWhichIsNotCurrentlyTracked(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leo = 100&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
+    logEndOffset = 100&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 100)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 100)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), cache.endOffsetFor(3))&lt;br/&gt;
@@ -253,35 +215,28 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldSupportEpochsThatDoNotStartFromZero(): Unit = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //When
-    cache.assign(epoch = 2, offset = 6)
-    leo = 7
+    cache.assign(epoch = 2, startOffset = 6)
+    logEndOffset = 7
 
     //Then
-    assertEquals((2, leo), cache.endOffsetFor(2))
+    assertEquals((2, logEndOffset), cache.endOffsetFor(2))
     assertEquals(1, cache.epochEntries.size)
-    assertEquals(EpochEntry(2, 6), cache.epochEntries()(0))
+    assertEquals(EpochEntry(2, 6), cache.epochEntries(0))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldPersistEpochsBetweenInstances(){&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
     val checkpointPath = TestUtils.tempFile().getAbsolutePath&lt;/li&gt;
	&lt;li&gt;checkpoint = new LeaderEpochCheckpointFile(new File(checkpointPath))&lt;br/&gt;
+    val checkpoint = new LeaderEpochCheckpointFile(new File(checkpointPath))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Given&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;br/&gt;
+    val cache = new LeaderEpochFileCache(tp, logEndOffset _, checkpoint)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     val checkpoint2 = new LeaderEpochCheckpointFile(new File(checkpointPath))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val cache2 = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint2)&lt;br/&gt;
+    val cache2 = new LeaderEpochFileCache(tp, logEndOffset _, checkpoint2)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals(1, cache2.epochEntries.size)&lt;br/&gt;
@@ -289,81 +244,68 @@ class LeaderEpochFileCacheTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldNotLetEpochGoBackwardsEvenIfMessageEpochsDo(): Unit = {&lt;/li&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
+  def shouldEnforceMonotonicallyIncreasingEpochs(): Unit = 
{
     //Given
-    cache.assign(epoch = 1, offset = 5); leo = 6
-    cache.assign(epoch = 2, offset = 6); leo = 7
-
-    //When we update an epoch in the past with an earlier offset
-    cache.assign(epoch = 1, offset = 7); leo = 8
+    cache.assign(epoch = 1, startOffset = 5); logEndOffset = 6
+    cache.assign(epoch = 2, startOffset = 6); logEndOffset = 7
 
-    //Then epoch should not be changed
-    assertEquals(2, cache.latestEpoch())
+    //When we update an epoch in the past with a different offset, the log has already reached
+    //an inconsistent state. Our options are either to raise an error, ignore the new append,
+    //or truncate the cached epochs to the point of conflict. We take this latter approach in
+    //order to guarantee that epochs and offsets in the cache increase monotonically, which makes
+    //the search logic simpler to reason about.
+    cache.assign(epoch = 1, startOffset = 7); logEndOffset = 8
 
-    //Then end offset for epoch 1 shouldn&apos;t have changed
-    assertEquals((1, 6), cache.endOffsetFor(1))
+    //Then later epochs will be removed
+    assertEquals(1, cache.latestEpoch)
 
-    //Then end offset for epoch 2 has to be the offset of the epoch 1 message (I can&apos;t think of a better option)
-    assertEquals((2, 8), cache.endOffsetFor(2))
+    //Then end offset for epoch 1 will have changed
+    assertEquals((1, 8), cache.endOffsetFor(1))
 
-    //Epoch history shouldn&apos;t have changed
-    assertEquals(EpochEntry(1, 5), cache.epochEntries()(0))
-    assertEquals(EpochEntry(2, 6), cache.epochEntries()(1))
+    //Then end offset for epoch 2 is now undefined
+    assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), cache.endOffsetFor(2))
+    assertEquals(EpochEntry(1, 7), cache.epochEntries(0))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldNotLetOffsetsGoBackwardsEvenIfEpochsProgress() = {&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
+  def shouldEnforceOffsetsIncreaseMonotonically() = 
{
     //When epoch goes forward but offset goes backwards
-    cache.assign(epoch = 2, offset = 6)
-    cache.assign(epoch = 3, offset = 5)
+    cache.assign(epoch = 2, startOffset = 6)
+    cache.assign(epoch = 3, startOffset = 5)
 
-    //Then latter assign should be ignored
-    assertEquals(EpochEntry(2, 6), cache.epochEntries.toList(0))
+    //The last assignment wins and the conflicting one is removed from the log
+    assertEquals(EpochEntry(3, 5), cache.epochEntries.toList(0))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldIncreaseAndTrackEpochsAsLeadersChangeManyTimes(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 0) //leo=0&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 0) //logEndOffset=0&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 0) //leo=0&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 0) //logEndOffset=0&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then epoch should go up&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1, cache.latestEpoch())&lt;br/&gt;
+    assertEquals(1, cache.latestEpoch)&lt;br/&gt;
     //offset for 1 should still be 0&lt;br/&gt;
     assertEquals((1, 0), cache.endOffsetFor(1))&lt;br/&gt;
     //offset for epoch 0 should still be 0&lt;br/&gt;
     assertEquals((0, 0), cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When we write 5 messages as epoch 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leo = 5&lt;br/&gt;
+    logEndOffset = 5&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Then end offset for epoch(1) should be leo =&amp;gt; 5&lt;br/&gt;
+    //Then end offset for epoch(1) should be logEndOffset =&amp;gt; 5&lt;br/&gt;
     assertEquals((1, 5), cache.endOffsetFor(1))&lt;br/&gt;
     //Epoch 0 should still be at offset 0&lt;br/&gt;
     assertEquals((0, 0), cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 5) //leo=5&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 5) //logEndOffset=5&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leo = 10 //write another 5 messages&lt;br/&gt;
+    logEndOffset = 10 //write another 5 messages&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Then end offset for epoch(2) should be leo =&amp;gt; 10&lt;br/&gt;
+    //Then end offset for epoch(2) should be logEndOffset =&amp;gt; 10&lt;br/&gt;
     assertEquals((2, 10), cache.endOffsetFor(2))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //end offset for epoch(1) should be the start offset of epoch(2) =&amp;gt; 5&lt;br/&gt;
@@ -375,36 +317,30 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldIncreaseAndTrackEpochsAsFollowerReceivesManyMessages(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//When new&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
     //When Messages come in&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 0); leo = 1&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 1); leo = 2&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 2); leo = 3&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 0); logEndOffset = 1&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 1); logEndOffset = 2&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 2); logEndOffset = 3&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then epoch should stay, offsets should grow&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(0, cache.latestEpoch())&lt;/li&gt;
	&lt;li&gt;assertEquals((0, leo), cache.endOffsetFor(0))&lt;br/&gt;
+    assertEquals(0, cache.latestEpoch)&lt;br/&gt;
+    assertEquals((0, logEndOffset), cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When messages arrive with greater epoch&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 3); leo = 4&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 4); leo = 5&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 5); leo = 6&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 3); logEndOffset = 4&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 4); logEndOffset = 5&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 5); logEndOffset = 6&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1, cache.latestEpoch())&lt;/li&gt;
	&lt;li&gt;assertEquals((1, leo), cache.endOffsetFor(1))&lt;br/&gt;
+    assertEquals(1, cache.latestEpoch)&lt;br/&gt;
+    assertEquals((1, logEndOffset), cache.endOffsetFor(1))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6); leo = 7&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 7); leo = 8&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 8); leo = 9&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6); logEndOffset = 7&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 7); logEndOffset = 8&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 8); logEndOffset = 9&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(2, cache.latestEpoch())&lt;/li&gt;
	&lt;li&gt;assertEquals((2, leo), cache.endOffsetFor(2))&lt;br/&gt;
+    assertEquals(2, cache.latestEpoch)&lt;br/&gt;
+    assertEquals((2, logEndOffset), cache.endOffsetFor(2))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Older epochs should return the start offset of the first message in the subsequent epoch.&lt;br/&gt;
     assertEquals((0, 3), cache.endOffsetFor(0))&lt;br/&gt;
@@ -413,16 +349,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldDropEntriesOnEpochBoundaryWhenRemovingLatestEntries(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When clear latest on epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushLatest(offset = 8)&lt;br/&gt;
+    cache.truncateFromEnd(endOffset = 8)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should remove two latest epochs (remove is inclusive)&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(2, 6)), cache.epochEntries)&lt;br/&gt;
@@ -430,16 +363,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldPreserveResetOffsetOnClearEarliestIfOneExists(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset ON epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 8)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 8)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should preserve (3, 8)&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(3, 8), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -447,16 +377,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldUpdateSavedOffsetWhenOffsetToClearToIsBetweenEpochs(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset BETWEEN epoch boundaries&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 9)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 9)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then we should retain epoch 3, but update it&apos;s offset to 9 as 8 has been removed&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(3, 9), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -464,16 +391,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotClearAnythingIfOffsetToEarly(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset before first epoch offset&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 1)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 1)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then nothing should change&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(2, 6),EpochEntry(3, 8), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -481,16 +405,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotClearAnythingIfOffsetToFirstOffset(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset on earliest epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 6)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 6)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then nothing should change&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(2, 6),EpochEntry(3, 8), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -498,16 +419,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldRetainLatestEpochOnClearAllEarliest(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 11)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then retain the last&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -515,16 +433,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldUpdateOffsetBetweenEpochBoundariesOnClearEarliest(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When we clear from a postition between offset 8 &amp;amp; offset 11&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 9)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 9)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then we should update the middle epoch entry&apos;s offset&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(3, 9), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -532,16 +447,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldUpdateOffsetBetweenEpochBoundariesOnClearEarliest2(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 0)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 7)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 10)&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 0)&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 7)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 10)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When we clear from a postition between offset 0 &amp;amp; offset 7&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 5)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 5)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then we should keeep epoch 0 but update the offset appropriately&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(0,5), EpochEntry(1, 7), EpochEntry(2, 10)), cache.epochEntries)&lt;br/&gt;
@@ -549,16 +461,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldRetainLatestEpochOnClearAllEarliestAndUpdateItsOffset(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset beyond last epoch&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 15)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 15)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then update the last&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(4, 15)), cache.epochEntries)&lt;br/&gt;
@@ -566,51 +475,42 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldDropEntriesBetweenEpochBoundaryWhenRemovingNewest(): Unit = &lt;/p&gt;
{
-    def leoFinder() = new LogOffsetMetadata(0)
-
     //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-    cache.assign(epoch = 2, offset = 6)
-    cache.assign(epoch = 3, offset = 8)
-    cache.assign(epoch = 4, offset = 11)
+    cache.assign(epoch = 2, startOffset = 6)
+    cache.assign(epoch = 3, startOffset = 8)
+    cache.assign(epoch = 4, startOffset = 11)
 
     //When reset to offset BETWEEN epoch boundaries
-    cache.clearAndFlushLatest(offset = 9)
+    cache.truncateFromEnd(endOffset = 9)
 
     //Then should keep the preceding epochs
-    assertEquals(3, cache.latestEpoch())
+    assertEquals(3, cache.latestEpoch)
     assertEquals(ListBuffer(EpochEntry(2, 6), EpochEntry(3, 8)), cache.epochEntries)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldClearAllEntries(): Unit = &lt;/p&gt;
{
-    def leoFinder() = new LogOffsetMetadata(0)
-
     //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-    cache.assign(epoch = 2, offset = 6)
-    cache.assign(epoch = 3, offset = 8)
-    cache.assign(epoch = 4, offset = 11)
+    cache.assign(epoch = 2, startOffset = 6)
+    cache.assign(epoch = 3, startOffset = 8)
+    cache.assign(epoch = 4, startOffset = 11)
 
-    //When 
+    //When
     cache.clearAndFlush()
 
-    //Then 
+    //Then
     assertEquals(0, cache.epochEntries.size)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotResetEpochHistoryHeadIfUndefinedPassed(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset on epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushLatest(offset = UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+    cache.truncateFromEnd(endOffset = UNDEFINED_EPOCH_OFFSET)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should do nothing&lt;br/&gt;
     assertEquals(3, cache.epochEntries.size)&lt;br/&gt;
@@ -618,16 +518,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotResetEpochHistoryTailIfUndefinedPassed(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset on epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+    cache.truncateFromEnd(endOffset = UNDEFINED_EPOCH_OFFSET)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should do nothing&lt;br/&gt;
     assertEquals(3, cache.epochEntries.size)&lt;br/&gt;
@@ -635,54 +532,26 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldFetchLatestEpochOfEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
     assertEquals(-1, cache.latestEpoch)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldFetchEndOffsetOfEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
     assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), cache.endOffsetFor(7))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldClearEarliestOnEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
-    cache.clearAndFlushEarliest(7)
+    cache.truncateFromStart(7)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldClearLatestOnEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
-    cache.clearAndFlushLatest(7)
+    cache.truncateFromEnd(7)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Before&lt;/li&gt;
	&lt;li&gt;def setUp() 
{
-    checkpoint = new LeaderEpochCheckpointFile(TestUtils.tempFile())
-  }
&lt;p&gt; }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
index 5ad641f11a0..efc07177037 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
@@ -16,7 +16,7 @@&lt;br/&gt;
   */&lt;br/&gt;
 package kafka.server.epoch&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import java.util.&lt;/p&gt;
{Optional, Map =&amp;gt; JMap}
&lt;p&gt;+import java.util.Optional&lt;/p&gt;

&lt;p&gt; import kafka.server.KafkaConfig._&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{BlockingSend, KafkaServer, ReplicaFetcherBlockingSend}
&lt;p&gt;@@ -37,9 +37,10 @@ import org.apache.kafka.common.requests.{EpochEndOffset, OffsetsForLeaderEpochRe&lt;/p&gt;

&lt;p&gt; import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.Map&lt;br/&gt;
+import scala.collection.mutable.ListBuffer&lt;/p&gt;

&lt;p&gt; class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var brokers: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaServer&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
+  var brokers: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaServer&amp;#93;&lt;/span&gt; = ListBuffer()&lt;br/&gt;
   val topic1 = &quot;foo&quot;&lt;br/&gt;
   val topic2 = &quot;bar&quot;&lt;br/&gt;
   val t1p0 = new TopicPartition(topic1, 0)&lt;br/&gt;
@@ -60,7 +61,7 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldAddCurrentLeaderEpochToMessagesAsTheyAreWrittenToLeader() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;brokers = (0 to 1).map 
{ id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }&lt;br/&gt;
+    brokers ++= (0 to 1).map { id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Given two topics with replication of a single partition&lt;br/&gt;
     for (topic &amp;lt;- List(topic1, topic2)) {&lt;br/&gt;
@@ -94,7 +95,7 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
   def shouldSendLeaderEpochRequestAndGetAResponse(): Unit = {&lt;/p&gt;

&lt;p&gt;     //3 brokers, put partition on 100/101 and then pretend to be 102&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;brokers = (100 to 102).map 
{ id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }&lt;br/&gt;
+    brokers ++= (100 to 102).map { id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val assignment1 = Map(0 -&amp;gt; Seq(100), 1 -&amp;gt; Seq(101))&lt;br/&gt;
     TestUtils.createTopic(zkClient, topic1, assignment1, brokers)&lt;br/&gt;
@@ -138,9 +139,12 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldIncreaseLeaderEpochBetweenLeaderRestarts(): Unit = {&lt;br/&gt;
-&lt;br/&gt;
     //Setup: we are only interested in the single partition on broker 101&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;brokers = Seq(100, 101).map 
{ id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }
&lt;p&gt;+    brokers += createServer(fromProps(createBrokerConfig(100, zkConnect)))&lt;br/&gt;
+    assertEquals(100, TestUtils.waitUntilControllerElected(zkClient))&lt;br/&gt;
+&lt;br/&gt;
+    brokers += createServer(fromProps(createBrokerConfig(101, zkConnect)))&lt;br/&gt;
+&lt;br/&gt;
     def leo() = brokers(1).replicaManager.getReplica(tp).get.logEndOffset.messageOffset&lt;br/&gt;
     TestUtils.createTopic(zkClient, tp.topic, Map(tp.partition -&amp;gt; Seq(101)), brokers)&lt;br/&gt;
     producer = createProducer(getBrokerListStrFromServers(brokers), acks = -1)&lt;br/&gt;
@@ -150,10 +154,10 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
     var fetcher = new TestFetcherThread(sender(brokers(0), brokers(1)))&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then epoch should be 0 and leo: 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var offset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp).endOffset()&lt;/li&gt;
	&lt;li&gt;assertEquals(1, offset)&lt;/li&gt;
	&lt;li&gt;assertEquals(leo(), offset)&lt;br/&gt;
-&lt;br/&gt;
+    var epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp)&lt;br/&gt;
+    assertEquals(0, epochEndOffset.leaderEpoch)&lt;br/&gt;
+    assertEquals(1, epochEndOffset.endOffset)&lt;br/&gt;
+    assertEquals(1, leo())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //2. When broker is bounced&lt;br/&gt;
     brokers(1).shutdown()&lt;br/&gt;
@@ -162,15 +166,23 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
     producer.send(new ProducerRecord(tp.topic, tp.partition, null, &quot;IHeartLogs&quot;.getBytes)).get&lt;br/&gt;
     fetcher = new TestFetcherThread(sender(brokers(0), brokers(1)))&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     //Then epoch 0 should still be the start offset of epoch 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;offset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp).endOffset()&lt;/li&gt;
	&lt;li&gt;assertEquals(1, offset)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Then epoch 2 should be the leo (NB: The leader epoch goes up in factors of 2 - This is because we have to first change leader to -1 and then change it again to the live replica)&lt;/li&gt;
	&lt;li&gt;assertEquals(2, fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 2))(tp).endOffset())&lt;/li&gt;
	&lt;li&gt;assertEquals(leo(), fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 2))(tp).endOffset())&lt;br/&gt;
-&lt;br/&gt;
+    epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp)&lt;br/&gt;
+    assertEquals(1, epochEndOffset.endOffset)&lt;br/&gt;
+    assertEquals(0, epochEndOffset.leaderEpoch)&lt;br/&gt;
+&lt;br/&gt;
+    //No data written in epoch 1&lt;br/&gt;
+    epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 1))(tp)&lt;br/&gt;
+    assertEquals(0, epochEndOffset.leaderEpoch)&lt;br/&gt;
+    assertEquals(1, epochEndOffset.endOffset)&lt;br/&gt;
+&lt;br/&gt;
+    //Then epoch 2 should be the leo (NB: The leader epoch goes up in factors of 2 -&lt;br/&gt;
+    //This is because we have to first change leader to -1 and then change it again to the live replica)&lt;br/&gt;
+    //Note that the expected leader changes depend on the controller being on broker 100, which is not restarted&lt;br/&gt;
+    epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 2))(tp)&lt;br/&gt;
+    assertEquals(2, epochEndOffset.leaderEpoch)&lt;br/&gt;
+    assertEquals(2, epochEndOffset.endOffset)&lt;br/&gt;
+    assertEquals(2, leo())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //3. When broker is bounced again&lt;br/&gt;
     brokers(1).shutdown()&lt;br/&gt;
@@ -179,7 +191,6 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
     producer.send(new ProducerRecord(tp.topic, tp.partition, null, &quot;IHeartLogs&quot;.getBytes)).get&lt;br/&gt;
     fetcher = new TestFetcherThread(sender(brokers(0), brokers(1)))&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     //Then Epoch 0 should still map to offset 1&lt;br/&gt;
     assertEquals(1, fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp).endOffset())&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
index 4fdc4d26992..86a087b480d 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
@@ -47,7 +47,7 @@ class OffsetsForLeaderEpochTest {&lt;/p&gt;

&lt;p&gt;     //Stubs&lt;br/&gt;
     val mockLog = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.Log&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val mockCache = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.epoch.LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val mockCache = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.epoch.LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     expect(mockCache.endOffsetFor(epochRequested)).andReturn(epochAndOffset)&lt;br/&gt;
     expect(mockLog.leaderEpochCache).andReturn(mockCache).anyTimes()&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/utils/TestUtils.scala b/core/src/test/scala/unit/kafka/utils/TestUtils.scala&lt;br/&gt;
index 4dc822b52c9..df9902f7003 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/utils/TestUtils.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/utils/TestUtils.scala&lt;br/&gt;
@@ -373,10 +373,11 @@ object TestUtils extends Logging {&lt;br/&gt;
               producerId: Long = RecordBatch.NO_PRODUCER_ID,&lt;br/&gt;
               producerEpoch: Short = RecordBatch.NO_PRODUCER_EPOCH,&lt;br/&gt;
               sequence: Int = RecordBatch.NO_SEQUENCE,&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;baseOffset: Long = 0L): MemoryRecords = {&lt;br/&gt;
+              baseOffset: Long = 0L,&lt;br/&gt;
+              partitionLeaderEpoch: Int = RecordBatch.NO_PARTITION_LEADER_EPOCH): MemoryRecords = 
{
     val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))
     val builder = MemoryRecords.builder(buf, magicValue, codec, TimestampType.CREATE_TIME, baseOffset,
-      System.currentTimeMillis, producerId, producerEpoch, sequence)
+      System.currentTimeMillis, producerId, producerEpoch, sequence, false, partitionLeaderEpoch)
     records.foreach(builder.append)
     builder.build()
   }&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16640178" author="githubbot" created="Fri, 5 Oct 2018 18:19:41 +0000"  >&lt;p&gt;hachikuji opened a new pull request #5749: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7415&quot; title=&quot;OffsetsForLeaderEpoch may incorrectly respond with undefined epoch causing truncation to HW&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7415&quot;&gt;&lt;del&gt;KAFKA-7415&lt;/del&gt;&lt;/a&gt;; Persist leader epoch and start offset on becoming a leader&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5749&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5749&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Note this is a backport of #5678 for 1.1&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16640391" author="githubbot" created="Fri, 5 Oct 2018 22:01:33 +0000"  >&lt;p&gt;hachikuji closed pull request #5749: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7415&quot; title=&quot;OffsetsForLeaderEpoch may incorrectly respond with undefined epoch causing truncation to HW&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7415&quot;&gt;&lt;del&gt;KAFKA-7415&lt;/del&gt;&lt;/a&gt;; Persist leader epoch and start offset on becoming a leader&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5749&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5749&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index e3a8186094f..ac0de9dba1d 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -280,8 +280,17 @@ class Partition(val topic: String,&lt;br/&gt;
       leaderEpoch = partitionStateInfo.basePartitionState.leaderEpoch&lt;br/&gt;
       leaderEpochStartOffsetOpt = Some(leaderEpochStartOffset)&lt;br/&gt;
       zkVersion = partitionStateInfo.basePartitionState.zkVersion&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val isNewLeader = leaderReplicaIdOpt.map(_ != localBrokerId).getOrElse(true)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+      // In the case of successive leader elections in a short time period, a follower may have&lt;br/&gt;
+      // entries in its log from a later epoch than any entry in the new leader&apos;s log. In order&lt;br/&gt;
+      // to ensure that these followers can truncate to the right offset, we must cache the new&lt;br/&gt;
+      // leader epoch and the start offset since it should be larger than any epoch that a follower&lt;br/&gt;
+      // would try to query.&lt;br/&gt;
+      leaderReplica.epochs.foreach &lt;/p&gt;
{ epochCache =&amp;gt;
+        epochCache.assign(leaderEpoch, leaderEpochStartOffset)
+      }
&lt;p&gt;+&lt;br/&gt;
+      val isNewLeader = !leaderReplicaIdOpt.contains(localBrokerId)&lt;br/&gt;
       val curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset&lt;br/&gt;
       val curTimeMs = time.milliseconds&lt;br/&gt;
       // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/cluster/Replica.scala b/core/src/main/scala/kafka/cluster/Replica.scala&lt;br/&gt;
index 4b65e439e2c..462f1f3cc23 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Replica.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Replica.scala&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;br/&gt;
 package kafka.cluster&lt;/p&gt;

&lt;p&gt; import kafka.log.Log&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.utils.Logging&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{LogOffsetMetadata, LogReadResult}
&lt;p&gt; import kafka.common.KafkaException&lt;br/&gt;
@@ -55,7 +56,7 @@ class Replica(val brokerId: Int,&lt;/p&gt;

&lt;p&gt;   def lastCaughtUpTimeMs = _lastCaughtUpTimeMs&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val epochs = log.map(_.leaderEpochCache)&lt;br/&gt;
+  val epochs: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; = log.map(_.leaderEpochCache)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   info(s&quot;Replica loaded for partition $topicPartition with initial high watermark $initialHighWatermarkValue&quot;)&lt;br/&gt;
   log.foreach(_.onHighWatermarkIncremented(initialHighWatermarkValue))&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index 9b423ba5933..eeb569a0771 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -39,7 +39,7 @@ import com.yammer.metrics.core.Gauge&lt;br/&gt;
 import org.apache.kafka.common.utils.&lt;/p&gt;
{Time, Utils}
&lt;p&gt; import kafka.message.&lt;/p&gt;
{BrokerCompressionCodec, CompressionCodec, NoCompressionCodec}
&lt;p&gt; import kafka.server.checkpoints.&lt;/p&gt;
{LeaderEpochCheckpointFile, LeaderEpochFile}
&lt;p&gt;-import kafka.server.epoch.&lt;/p&gt;
{LeaderEpochCache, LeaderEpochFileCache}
&lt;p&gt;+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
 import java.util.Map.&lt;/p&gt;
{Entry =&amp;gt; JEntry}
&lt;p&gt;@@ -208,7 +208,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   /* the actual segments of the log */&lt;br/&gt;
   private val segments: ConcurrentNavigableMap&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Long, LogSegment&amp;#93;&lt;/span&gt; = new ConcurrentSkipListMap&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Long, LogSegment&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@volatile private var _leaderEpochCache: LeaderEpochCache = initializeLeaderEpochCache()&lt;br/&gt;
+  @volatile private var _leaderEpochCache: LeaderEpochFileCache = initializeLeaderEpochCache()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   locally {&lt;br/&gt;
     val startMs = time.milliseconds&lt;br/&gt;
@@ -218,12 +218,12 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     /* Calculate the offset of the next message */&lt;br/&gt;
     nextOffsetMetadata = new LogOffsetMetadata(nextOffset, activeSegment.baseOffset, activeSegment.size)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;_leaderEpochCache.clearAndFlushLatest(nextOffsetMetadata.messageOffset)&lt;br/&gt;
+    _leaderEpochCache.truncateFromEnd(nextOffsetMetadata.messageOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     logStartOffset = math.max(logStartOffset, segments.firstEntry.getValue.baseOffset)&lt;/p&gt;

&lt;p&gt;     // The earliest leader epoch may not be flushed during a hard failure. Recover it here.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;_leaderEpochCache.clearAndFlushEarliest(logStartOffset)&lt;br/&gt;
+    _leaderEpochCache.truncateFromStart(logStartOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)&lt;/p&gt;

&lt;p&gt;@@ -271,11 +271,11 @@ class Log(@volatile var dir: File,&lt;/p&gt;

&lt;p&gt;   def leaderEpochCache = _leaderEpochCache&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def initializeLeaderEpochCache(): LeaderEpochCache = {&lt;br/&gt;
+  private def initializeLeaderEpochCache(): LeaderEpochFileCache = 
{
     // create the log directory if it doesn&apos;t exist
     Files.createDirectories(dir.toPath)
-    new LeaderEpochFileCache(topicPartition, () =&amp;gt; logEndOffsetMetadata,
-      new LeaderEpochCheckpointFile(LeaderEpochFile.newFile(dir), logDirFailureChannel))
+    val checkpointFile = new LeaderEpochCheckpointFile(LeaderEpochFile.newFile(dir), logDirFailureChannel)
+    new LeaderEpochFileCache(topicPartition, logEndOffset _, checkpointFile)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def removeTempFilesAndCollectSwapFiles(): Set&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
@@ -352,7 +352,7 @@ class Log(@volatile var dir: File,
     }
&lt;p&gt;   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def recoverSegment(segment: LogSegment, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = lock synchronized {&lt;br/&gt;
+  private def recoverSegment(segment: LogSegment, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; = None): Int = lock synchronized {&lt;br/&gt;
     val stateManager = new ProducerStateManager(topicPartition, dir, maxProducerIdExpirationMs)&lt;br/&gt;
     stateManager.truncateAndReload(logStartOffset, segment.baseOffset, time.milliseconds)&lt;br/&gt;
     logSegments(stateManager.mapEndOffset, segment.baseOffset).foreach { segment =&amp;gt;&lt;br/&gt;
@@ -830,7 +830,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         if (newLogStartOffset &amp;gt; logStartOffset) 
{
           info(s&quot;Incrementing log start offset to $newLogStartOffset&quot;)
           logStartOffset = newLogStartOffset
-          _leaderEpochCache.clearAndFlushEarliest(logStartOffset)
+          _leaderEpochCache.truncateFromStart(logStartOffset)
           producerStateManager.truncateHead(logStartOffset)
           updateFirstUnstableOffset()
         }
&lt;p&gt;@@ -1513,7 +1513,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
             updateLogEndOffset(targetOffset)&lt;br/&gt;
             this.recoveryPoint = math.min(targetOffset, this.recoveryPoint)&lt;br/&gt;
             this.logStartOffset = math.min(targetOffset, this.logStartOffset)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;_leaderEpochCache.clearAndFlushLatest(targetOffset)&lt;br/&gt;
+            _leaderEpochCache.truncateFromEnd(targetOffset)&lt;br/&gt;
             loadProducerState(targetOffset, reloadFromCleanShutdown = false)&lt;br/&gt;
           }&lt;br/&gt;
           true&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogSegment.scala b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
index 5970f42f6d9..012494694de 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
@@ -22,7 +22,7 @@ import java.nio.file.attribute.FileTime&lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import kafka.metrics.&lt;/p&gt;
{KafkaMetricsGroup, KafkaTimer}
&lt;p&gt;-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{FetchDataInfo, LogOffsetMetadata}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.errors.CorruptRecordException&lt;br/&gt;
@@ -265,7 +265,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return The number of bytes truncated from the log&lt;br/&gt;
    */&lt;br/&gt;
   @nonthreadsafe&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def recover(producerStateManager: ProducerStateManager, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = {&lt;br/&gt;
+  def recover(producerStateManager: ProducerStateManager, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; = None): Int = {&lt;br/&gt;
     offsetIndex.reset()&lt;br/&gt;
     timeIndex.reset()&lt;br/&gt;
     txnIndex.reset()&lt;br/&gt;
@@ -293,7 +293,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (batch.magic &amp;gt;= RecordBatch.MAGIC_VALUE_V2) {&lt;br/&gt;
           leaderEpochCache.foreach &lt;/p&gt;
{ cache =&amp;gt;
-            if (batch.partitionLeaderEpoch &amp;gt; cache.latestEpoch()) // this is to avoid unnecessary warning in cache.assign()
+            if (batch.partitionLeaderEpoch &amp;gt; cache.latestEpoch) // this is to avoid unnecessary warning in cache.assign()
               cache.assign(batch.partitionLeaderEpoch, batch.baseOffset)
           }
&lt;p&gt;           updateProducerState(producerStateManager, batch)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
index 74ef3e848ed..9b5c1bbff7d 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
@@ -29,7 +29,7 @@ import ReplicaAlterLogDirsThread.FetchRequest&lt;br/&gt;
 import ReplicaAlterLogDirsThread.PartitionData&lt;br/&gt;
 import kafka.api.Request&lt;br/&gt;
 import kafka.server.QuotaFactory.UnboundedQuota&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import org.apache.kafka.common.errors.KafkaStorageException&lt;br/&gt;
 import org.apache.kafka.common.protocol.&lt;/p&gt;
{ApiKeys, Errors}
&lt;p&gt; import org.apache.kafka.common.record.&lt;/p&gt;
{FileRecords, MemoryRecords}
&lt;p&gt;@@ -58,7 +58,7 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
   private val maxBytes = brokerConfig.replicaFetchResponseMaxBytes&lt;br/&gt;
   private val fetchSize = brokerConfig.replicaFetchMaxBytes&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def epochCacheOpt(tp: TopicPartition): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; =  replicaMgr.getReplica(tp).map(_.epochs.get)&lt;br/&gt;
+  private def epochCacheOpt(tp: TopicPartition): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; =  replicaMgr.getReplica(tp).map(_.epochs.get)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def fetch(fetchRequest: FetchRequest): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, PartitionData)&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     var partitionData: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchResponse.PartitionData)&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
@@ -148,7 +148,7 @@ class ReplicaAlterLogDirsThread(name: String,&lt;/p&gt;

&lt;p&gt;     val (partitionsWithEpoch, partitionsWithoutEpoch) = partitionEpochOpts.partition &lt;/p&gt;
{ case (tp, epochCacheOpt) =&amp;gt; epochCacheOpt.nonEmpty }&lt;br/&gt;
 &lt;br/&gt;
-    val result = partitionsWithEpoch.map { case (tp, epochCacheOpt) =&amp;gt; tp -&amp;gt; epochCacheOpt.get.latestEpoch() }&lt;br/&gt;
+    val result = partitionsWithEpoch.map { case (tp, epochCacheOpt) =&amp;gt; tp -&amp;gt; epochCacheOpt.get.latestEpoch }&lt;br/&gt;
     ResultWithPartitions(result, partitionsWithoutEpoch.keys.toSet)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index 5e0e9bed3f5..4fceef847e3 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -23,7 +23,7 @@ import AbstractFetcherThread.ResultWithPartitions&lt;br/&gt;
 import kafka.api.{FetchRequest =&amp;gt; _, _}&lt;br/&gt;
 import kafka.cluster.{BrokerEndPoint, Replica}&lt;br/&gt;
 import kafka.server.ReplicaFetcherThread._&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import org.apache.kafka.clients.FetchSessionHandler&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -79,7 +79,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
   private val shouldSendLeaderEpochRequest: Boolean = brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_11_0_IV2&lt;br/&gt;
   private val fetchSessionHandler = new FetchSessionHandler(logContext, sourceBroker.id)&lt;br/&gt;
 &lt;br/&gt;
-  private def epochCacheOpt(tp: TopicPartition): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; =  replicaMgr.getReplica(tp).map(_.epochs.get)&lt;br/&gt;
+  private def epochCacheOpt(tp: TopicPartition): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt; =  replicaMgr.getReplica(tp).map(_.epochs.get)&lt;br/&gt;
 &lt;br/&gt;
   override def initiateShutdown(): Boolean = {&lt;br/&gt;
     val justShutdown = super.initiateShutdown()&lt;br/&gt;
@@ -324,7 +324,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
     val (partitionsWithEpoch, partitionsWithoutEpoch) = partitionEpochOpts.partition { case (tp, epochCacheOpt) =&amp;gt; epochCacheOpt.nonEmpty }

&lt;p&gt;     debug(s&quot;Build leaderEpoch request $partitionsWithEpoch&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val result = partitionsWithEpoch.map 
{ case (tp, epochCacheOpt) =&amp;gt; tp -&amp;gt; epochCacheOpt.get.latestEpoch() }
&lt;p&gt;+    val result = partitionsWithEpoch.map &lt;/p&gt;
{ case (tp, epochCacheOpt) =&amp;gt; tp -&amp;gt; epochCacheOpt.get.latestEpoch }
&lt;p&gt;     ResultWithPartitions(result, partitionsWithoutEpoch.keys.toSet)&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala b/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
index 220432d32c0..10bdb1708c3 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
@@ -18,53 +18,69 @@ package kafka.server.epoch&lt;/p&gt;

&lt;p&gt; import java.util.concurrent.locks.ReentrantReadWriteLock&lt;/p&gt;

&lt;p&gt;-import kafka.server.LogOffsetMetadata&lt;br/&gt;
 import kafka.server.checkpoints.LeaderEpochCheckpoint&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset.&lt;/p&gt;
{UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET}
&lt;p&gt; import kafka.utils.CoreUtils._&lt;br/&gt;
 import kafka.utils.Logging&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import scala.collection.mutable.ListBuffer&lt;/p&gt;

&lt;p&gt;-trait LeaderEpochCache &lt;/p&gt;
{
-  def assign(leaderEpoch: Int, offset: Long)
-  def latestEpoch(): Int
-  def endOffsetFor(epoch: Int): Long
-  def clearAndFlushLatest(offset: Long)
-  def clearAndFlushEarliest(offset: Long)
-  def clearAndFlush()
-  def clear()
-}
&lt;p&gt;+import scala.collection.mutable.ListBuffer&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Represents a cache of (LeaderEpoch =&amp;gt; Offset) mappings for a particular replica.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Leader Epoch = epoch assigned to each leader by the controller.&lt;/li&gt;
	&lt;li&gt;* Offset = offset of the first message in each epoch.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param leo a function that determines the log end offset&lt;/li&gt;
	&lt;li&gt;* @param checkpoint the checkpoint file&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetMetadata, checkpoint: LeaderEpochCheckpoint) extends LeaderEpochCache with Logging {&lt;br/&gt;
+ * Represents a cache of (LeaderEpoch =&amp;gt; Offset) mappings for a particular replica.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Leader Epoch = epoch assigned to each leader by the controller.&lt;br/&gt;
+ * Offset = offset of the first message in each epoch.&lt;br/&gt;
+ *&lt;br/&gt;
+ * @param topicPartition the associated topic partition&lt;br/&gt;
+ * @param checkpoint the checkpoint file&lt;br/&gt;
+ * @param logEndOffset function to fetch the current log end offset&lt;br/&gt;
+ */&lt;br/&gt;
+class LeaderEpochFileCache(topicPartition: TopicPartition,&lt;br/&gt;
+                           logEndOffset: () =&amp;gt; Long,&lt;br/&gt;
+                           checkpoint: LeaderEpochCheckpoint) extends Logging {&lt;br/&gt;
+  this.logIdent = s&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache $topicPartition&amp;#93;&lt;/span&gt; &quot;&lt;br/&gt;
+&lt;br/&gt;
   private val lock = new ReentrantReadWriteLock()&lt;br/&gt;
   private var epochs: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = inWriteLock(lock) 
{ ListBuffer(checkpoint.read(): _*) }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Assigns the supplied Leader Epoch to the supplied Offset&lt;/li&gt;
	&lt;li&gt;Once the epoch is assigned it cannot be reassigned&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param epoch&lt;/li&gt;
	&lt;li&gt;* @param offset&lt;br/&gt;
     */&lt;/li&gt;
	&lt;li&gt;override def assign(epoch: Int, offset: Long): Unit = {&lt;br/&gt;
+  def assign(epoch: Int, startOffset: Long): Unit = {&lt;br/&gt;
     inWriteLock(lock) {&lt;/li&gt;
	&lt;li&gt;if (epoch &amp;gt;= 0 &amp;amp;&amp;amp; epoch &amp;gt; latestEpoch &amp;amp;&amp;amp; offset &amp;gt;= latestOffset) {&lt;/li&gt;
	&lt;li&gt;info(s&quot;Updated PartitionLeaderEpoch. ${epochChangeMsg(epoch, offset)}. Cache now contains ${epochs.size} entries.&quot;)&lt;/li&gt;
	&lt;li&gt;epochs += EpochEntry(epoch, offset)&lt;/li&gt;
	&lt;li&gt;flush()&lt;br/&gt;
+      val updateNeeded = if (epochs.isEmpty) 
{
+        true
       }
&lt;p&gt; else &lt;/p&gt;
{
-        validateAndMaybeWarn(epoch, offset)
+        val lastEntry = epochs.last
+        lastEntry.epoch != epoch || startOffset &amp;lt; lastEntry.startOffset
       }
&lt;p&gt;+&lt;br/&gt;
+      if (updateNeeded) &lt;/p&gt;
{
+        truncateAndAppend(EpochEntry(epoch, startOffset))
+        flush()
+      }
&lt;p&gt;+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Remove any entries which violate monotonicity following the insertion of an assigned epoch.&lt;br/&gt;
+   */&lt;br/&gt;
+  private def truncateAndAppend(entryToAppend: EpochEntry): Unit = {&lt;br/&gt;
+    validateAndMaybeWarn(entryToAppend)&lt;br/&gt;
+&lt;br/&gt;
+    val (retainedEpochs, removedEpochs) = epochs.partition &lt;/p&gt;
{ entry =&amp;gt;
+      entry.epoch &amp;lt; entryToAppend.epoch &amp;amp;&amp;amp; entry.startOffset &amp;lt; entryToAppend.startOffset
+    }
&lt;p&gt;+&lt;br/&gt;
+    epochs = retainedEpochs :+ entryToAppend&lt;br/&gt;
+&lt;br/&gt;
+    if (removedEpochs.isEmpty) {&lt;br/&gt;
+      debug(s&quot;Appended new epoch entry $entryToAppend. Cache now contains ${epochs.size} entries.&quot;)&lt;br/&gt;
+    } else {&lt;br/&gt;
+      warn(s&quot;New epoch entry $entryToAppend caused truncation of conflicting entries $removedEpochs. &quot; +&lt;br/&gt;
+        s&quot;Cache now contains ${epochs.size} entries.&quot;)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -74,7 +90,7 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;br/&gt;
     *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return&lt;br/&gt;
     */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def latestEpoch(): Int = {&lt;br/&gt;
+  def latestEpoch: Int = {&lt;br/&gt;
     inReadLock(lock) 
{
       if (epochs.isEmpty) UNDEFINED_EPOCH else epochs.last.epoch
     }
&lt;p&gt;@@ -90,42 +106,53 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;if requestedEpoch is &amp;lt; the first epoch cached, UNSUPPORTED_EPOCH_OFFSET will be returned&lt;/li&gt;
	&lt;li&gt;so that the follower falls back to High Water Mark.&lt;br/&gt;
     *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @param requestedEpoch&lt;/li&gt;
	&lt;li&gt;* @return offset&lt;br/&gt;
+    * @param requestedEpoch requested leader epoch&lt;br/&gt;
+    * @return found end offset&lt;br/&gt;
     */&lt;/li&gt;
	&lt;li&gt;override def endOffsetFor(requestedEpoch: Int): Long = {&lt;br/&gt;
+  def endOffsetFor(requestedEpoch: Int): Long = {&lt;br/&gt;
     inReadLock(lock) {&lt;/li&gt;
	&lt;li&gt;val offset =&lt;br/&gt;
+      val endOffset =&lt;br/&gt;
         if (requestedEpoch == UNDEFINED_EPOCH) 
{
-          // this may happen if a bootstrapping follower sends a request with undefined epoch or
+          // This may happen if a bootstrapping follower sends a request with undefined epoch or
           // a follower is on the older message format where leader epochs are not recorded
           UNDEFINED_EPOCH_OFFSET
         }
&lt;p&gt; else if (requestedEpoch == latestEpoch) &lt;/p&gt;
{
-          leo().messageOffset
+          // For the leader, the latest epoch is always the current leader epoch that is still being written to.
+          // Followers should not have any reason to query for the end offset of the current epoch, but a consumer
+          // might if it is verifying its committed offset following a group rebalance. In this case, we return
+          // the current log end offset which makes the truncation check work as expected.
+          logEndOffset()
         }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val subsequentEpochs = epochs.filter(e =&amp;gt; e.epoch &amp;gt; requestedEpoch)&lt;/li&gt;
	&lt;li&gt;if (subsequentEpochs.isEmpty || requestedEpoch &amp;lt; epochs.head.epoch)&lt;br/&gt;
+          val subsequentEpochs = epochs.filter 
{ e =&amp;gt; e.epoch &amp;gt; requestedEpoch}
&lt;p&gt;+          if (subsequentEpochs.isEmpty) &lt;/p&gt;
{
+            // The requested epoch is larger than any known epoch. This case should never be hit because
+            // the latest cached epoch is always the largest.
             UNDEFINED_EPOCH_OFFSET
-          else
+          }
&lt;p&gt; else &lt;/p&gt;
{
+            // We have at least one previous epoch and one subsequent epoch. The result is the first
+            // prior epoch and the starting offset of the first subsequent epoch.
             subsequentEpochs.head.startOffset
+          }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;debug(s&quot;Processed offset for epoch request for partition ${topicPartition} epoch:$requestedEpoch and returning offset $offset from epoch list of size ${epochs.size}&quot;)&lt;/li&gt;
	&lt;li&gt;offset&lt;br/&gt;
+      debug(s&quot;Processed end offset request for epoch $requestedEpoch and returning end offset $endOffset &quot; +&lt;br/&gt;
+        s&quot;from epoch cache of size ${epochs.size}&quot;)&lt;br/&gt;
+      endOffset&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Removes all epoch entries from the store with start offsets greater than or equal to the passed offset.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param offset&lt;br/&gt;
     */&lt;/li&gt;
	&lt;li&gt;override def clearAndFlushLatest(offset: Long): Unit = {&lt;br/&gt;
+  def truncateFromEnd(endOffset: Long): Unit = {&lt;br/&gt;
     inWriteLock(lock) {&lt;/li&gt;
	&lt;li&gt;val before = epochs&lt;/li&gt;
	&lt;li&gt;if (offset &amp;gt;= 0 &amp;amp;&amp;amp; offset &amp;lt;= latestOffset()) {&lt;/li&gt;
	&lt;li&gt;epochs = epochs.filter(entry =&amp;gt; entry.startOffset &amp;lt; offset)&lt;br/&gt;
+      if (endOffset &amp;gt;= 0 &amp;amp;&amp;amp; latestEntry.exists(_.startOffset &amp;gt;= endOffset)) {&lt;br/&gt;
+        val (subsequentEntries, previousEntries) = epochs.partition(_.startOffset &amp;gt;= endOffset)&lt;br/&gt;
+        epochs = previousEntries&lt;br/&gt;
+&lt;br/&gt;
         flush()&lt;/li&gt;
	&lt;li&gt;info(s&quot;Cleared latest ${before.toSet.filterNot(epochs.toSet)} entries from epoch cache based on passed offset $offset leaving ${epochs.size} in EpochFile for partition $topicPartition&quot;)&lt;br/&gt;
+&lt;br/&gt;
+        debug(s&quot;Cleared entries $subsequentEntries from epoch cache after &quot; +&lt;br/&gt;
+          s&quot;truncating to end offset $endOffset, leaving ${epochs.size} entries in the cache.&quot;)&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -136,20 +163,21 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;br/&gt;
     *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;This method is exclusive: so clearEarliest(6) will retain an entry at offset 6.&lt;br/&gt;
     *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @param offset the offset to clear up to&lt;br/&gt;
+    * @param startOffset the offset to clear up to&lt;br/&gt;
     */&lt;/li&gt;
	&lt;li&gt;override def clearAndFlushEarliest(offset: Long): Unit = {&lt;br/&gt;
+  def truncateFromStart(startOffset: Long): Unit = {&lt;br/&gt;
     inWriteLock(lock) {&lt;/li&gt;
	&lt;li&gt;val before = epochs&lt;/li&gt;
	&lt;li&gt;if (offset &amp;gt;= 0 &amp;amp;&amp;amp; earliestOffset() &amp;lt; offset) {&lt;/li&gt;
	&lt;li&gt;val earliest = epochs.filter(entry =&amp;gt; entry.startOffset &amp;lt; offset)&lt;/li&gt;
	&lt;li&gt;if (earliest.nonEmpty) {&lt;/li&gt;
	&lt;li&gt;epochs = epochs --= earliest&lt;/li&gt;
	&lt;li&gt;//If the offset is less than the earliest offset remaining, add previous epoch back, but with an updated offset&lt;/li&gt;
	&lt;li&gt;if (offset &amp;lt; earliestOffset() || epochs.isEmpty)&lt;/li&gt;
	&lt;li&gt;new EpochEntry(earliest.last.epoch, offset) +=: epochs&lt;br/&gt;
+      if (epochs.nonEmpty) {&lt;br/&gt;
+        val (subsequentEntries, previousEntries) = epochs.partition(_.startOffset &amp;gt; startOffset)&lt;br/&gt;
+&lt;br/&gt;
+        previousEntries.lastOption.foreach { firstBeforeStartOffset =&amp;gt;&lt;br/&gt;
+          val updatedFirstEntry = EpochEntry(firstBeforeStartOffset.epoch, startOffset)&lt;br/&gt;
+          epochs = updatedFirstEntry +: subsequentEntries&lt;br/&gt;
+&lt;br/&gt;
           flush()&lt;/li&gt;
	&lt;li&gt;info(s&quot;Cleared earliest ${before.toSet.filterNot(epochs.toSet).size} entries from epoch cache based on passed offset $offset leaving ${epochs.size} in EpochFile for partition $topicPartition&quot;)&lt;br/&gt;
+&lt;br/&gt;
+          debug(s&quot;Cleared entries $previousEntries and rewrote first entry $updatedFirstEntry after &quot; +&lt;br/&gt;
+            s&quot;truncating to start offset $startOffset, leaving ${epochs.size} in the cache.&quot;)&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
@@ -158,47 +186,55 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM&lt;br/&gt;
   /**&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Delete all entries.&lt;br/&gt;
     */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def clearAndFlush() = {&lt;br/&gt;
+  def clearAndFlush() = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     inWriteLock(lock) {
       epochs.clear()
       flush()
     }   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def clear() = {&lt;br/&gt;
+  def clear() = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     inWriteLock(lock) {
       epochs.clear()
     }   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def epochEntries(): ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+  // Visible for testing&lt;br/&gt;
+  def epochEntries: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = 
{
     epochs
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def earliestOffset(): Long = 
{
-    if (epochs.isEmpty) -1 else epochs.head.startOffset
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def latestOffset(): Long = 
{
-    if (epochs.isEmpty) -1 else epochs.last.startOffset
-  }
&lt;p&gt;+  private def latestEntry: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;EpochEntry&amp;#93;&lt;/span&gt; = epochs.lastOption&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def flush(): Unit = &lt;/p&gt;
{
     checkpoint.write(epochs)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def epochChangeMsg(epoch: Int, offset: Long) = s&quot;New: 
{epoch:$epoch, offset:$offset}
&lt;p&gt;, Current: &lt;/p&gt;
{epoch:$latestEpoch, offset:$latestOffset}
&lt;p&gt; for Partition: $topicPartition&quot;&lt;br/&gt;
-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;def validateAndMaybeWarn(epoch: Int, offset: Long) = {&lt;/li&gt;
	&lt;li&gt;assert(epoch &amp;gt;= 0, s&quot;Received a PartitionLeaderEpoch assignment for an epoch &amp;lt; 0. This should not happen. ${epochChangeMsg(epoch, offset)}&quot;)&lt;/li&gt;
	&lt;li&gt;if (epoch &amp;lt; latestEpoch())&lt;/li&gt;
	&lt;li&gt;warn(s&quot;Received a PartitionLeaderEpoch assignment for an epoch &amp;lt; latestEpoch. &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;This implies messages have arrived out of order. ${epochChangeMsg(epoch, offset)}&quot;)&lt;/li&gt;
	&lt;li&gt;else if (offset &amp;lt; latestOffset())&lt;/li&gt;
	&lt;li&gt;warn(s&quot;Received a PartitionLeaderEpoch assignment for an offset &amp;lt; latest offset for the most recent, stored PartitionLeaderEpoch. &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;This implies messages have arrived out of order. ${epochChangeMsg(epoch, offset)}&quot;)&lt;br/&gt;
+  private def validateAndMaybeWarn(entry: EpochEntry) = {&lt;br/&gt;
+    if (entry.epoch &amp;lt; 0) 
{
+      throw new IllegalArgumentException(s&quot;Received invalid partition leader epoch entry $entry&quot;)
+    }
&lt;p&gt; else &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      // If the latest append violates the monotonicity of epochs or starting offsets, our choices+      // are either to raise an error, ignore the append, or allow the append and truncate the+      // conflicting entries from the cache. Raising an error risks killing the fetcher threads in+      // pathological cases (i.e. cases we are not yet aware of). We instead take the final approach+      // and assume that the latest append is always accurate.++      latestEntry.foreach { latest =&amp;gt;
+        if (entry.epoch &amp;lt; latest.epoch)
+          warn(s&quot;Received leader epoch assignment $entry which has an epoch less than the epoch &quot; +
+            s&quot;of the latest entry $latest. This implies messages have arrived out of order.&quot;)
+        else if (entry.startOffset &amp;lt; latest.startOffset)
+          warn(s&quot;Received leader epoch assignment $entry which has a starting offset which is less than &quot; +
+            s&quot;the starting offset of the latest entry $latest. This implies messages have arrived out of order.&quot;)
+      }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;br/&gt;
 }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; // Mapping of epoch to the first offset of the subsequent epoch&lt;br/&gt;
-case class EpochEntry(epoch: Int, startOffset: Long)&lt;br/&gt;
+case class EpochEntry(epoch: Int, startOffset: Long) {&lt;br/&gt;
+  override def toString: String = &lt;/p&gt;
{
+    s&quot;EpochEntry(epoch=$epoch, startOffset=$startOffset)&quot;
+  }
&lt;p&gt;+}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index cf679f63513..e0dcdfdbb55 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -29,7 +29,7 @@ import org.junit.Assert._&lt;br/&gt;
 import org.junit.&lt;/p&gt;
{After, Before, Test}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{BrokerTopicStats, FetchDataInfo, KafkaConfig, LogDirFailureChannel}
&lt;p&gt;-import kafka.server.epoch.&lt;/p&gt;
{EpochEntry, LeaderEpochCache, LeaderEpochFileCache}
&lt;p&gt;+import kafka.server.epoch.&lt;/p&gt;
{EpochEntry, LeaderEpochFileCache}
&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter.BatchRetention&lt;br/&gt;
@@ -268,7 +268,7 @@ class LogTest {&lt;br/&gt;
             }&lt;/p&gt;

&lt;p&gt;             override def recover(producerStateManager: ProducerStateManager,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;): Int = {&lt;br/&gt;
+                                 leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;): Int = 
{
               recoveredSegments += this
               super.recover(producerStateManager, leaderEpochCache)
             }
&lt;p&gt;@@ -2246,8 +2246,8 @@ class LogTest {&lt;br/&gt;
     log.onHighWatermarkIncremented(log.logEndOffset)&lt;br/&gt;
     log.deleteOldSegments()&lt;br/&gt;
     assertEquals(&quot;The deleted segments should be gone.&quot;, 1, log.numberOfSegments)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries().size)&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Epoch entry should be the latest epoch and the leo.&quot;, EpochEntry(1, 100), epochCache(log).epochEntries().head)&lt;br/&gt;
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries.size)&lt;br/&gt;
+    assertEquals(&quot;Epoch entry should be the latest epoch and the leo.&quot;, EpochEntry(1, 100), epochCache(log).epochEntries.head)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 100)&lt;br/&gt;
@@ -2256,7 +2256,7 @@ class LogTest &lt;/p&gt;
{
     log.delete()
     assertEquals(&quot;The number of segments should be 0&quot;, 0, log.numberOfSegments)
     assertEquals(&quot;The number of deleted segments should be zero.&quot;, 0, log.deleteOldSegments())
-    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries().size)
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries.size)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -2269,12 +2269,12 @@ class LogTest &lt;/p&gt;
{
     log.appendAsLeader(createRecords, leaderEpoch = 0)
 
     assertEquals(&quot;The deleted segments should be gone.&quot;, 1, log.numberOfSegments)
-    assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries().size)
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 1, epochCache(log).epochEntries.size)
 
     log.close()
     log.delete()
     assertEquals(&quot;The number of segments should be 0&quot;, 0, log.numberOfSegments)
-    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries().size)
+    assertEquals(&quot;Epoch entries should have gone.&quot;, 0, epochCache(log).epochEntries.size)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -2447,7 +2447,7 @@ class LogTest &lt;/p&gt;
{
     for (i &amp;lt;- records.indices)
       log.appendAsFollower(recordsForEpoch(i))
 
-    assertEquals(42, log.leaderEpochCache.asInstanceOf[LeaderEpochFileCache].latestEpoch())
+    assertEquals(42, log.leaderEpochCache.latestEpoch)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -2502,19 +2502,24 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldTruncateLeaderEpochFileWhenTruncatingLog() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 10 * createRecords.sizeInBytes)&lt;br/&gt;
+    def createRecords(startOffset: Long, epoch: Int): MemoryRecords = 
{
+      TestUtils.records(Seq(new SimpleRecord(&quot;value&quot;.getBytes)),
+        baseOffset = startOffset, partitionLeaderEpoch = epoch)
+    }
&lt;p&gt;+&lt;br/&gt;
+    val logConfig = createLogConfig(segmentBytes = 10 * createRecords(0, 0).sizeInBytes)&lt;br/&gt;
     val log = createLog(logDir, logConfig)&lt;br/&gt;
     val cache = epochCache(log)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Given 2 segments, 10 messages per segment&lt;/li&gt;
	&lt;li&gt;for (epoch &amp;lt;- 1 to 20)&lt;/li&gt;
	&lt;li&gt;log.appendAsLeader(createRecords, leaderEpoch = 0)&lt;br/&gt;
+    def append(epoch: Int, startOffset: Long, count: Int): Unit = 
{
+      for (i &amp;lt;- 0 until count)
+        log.appendAsFollower(createRecords(startOffset + i, epoch))
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Simulate some leader changes at specific offsets&lt;/li&gt;
	&lt;li&gt;cache.assign(0, 0)&lt;/li&gt;
	&lt;li&gt;cache.assign(1, 10)&lt;/li&gt;
	&lt;li&gt;cache.assign(2, 16)&lt;br/&gt;
+    //Given 2 segments, 10 messages per segment&lt;br/&gt;
+    append(epoch = 0, startOffset = 0, count = 10)&lt;br/&gt;
+    append(epoch = 1, startOffset = 10, count = 6)&lt;br/&gt;
+    append(epoch = 2, startOffset = 16, count = 4)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(2, log.numberOfSegments)&lt;br/&gt;
     assertEquals(20, log.logEndOffset)&lt;br/&gt;
@@ -2566,7 +2571,7 @@ class LogTest {&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(1, 0), EpochEntry(2, 1), EpochEntry(3, 3)), leaderEpochCache.epochEntries)&lt;/p&gt;

&lt;p&gt;     // deliberately remove some of the epoch entries&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leaderEpochCache.clearAndFlushLatest(2)&lt;br/&gt;
+    leaderEpochCache.truncateFromEnd(2)&lt;br/&gt;
     assertNotEquals(ListBuffer(EpochEntry(1, 0), EpochEntry(2, 1), EpochEntry(3, 3)), leaderEpochCache.epochEntries)&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala b/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala&lt;br/&gt;
index 8212ed680c5..cb914c43344 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala&lt;br/&gt;
@@ -22,7 +22,7 @@ import java.util.concurrent.atomic.AtomicBoolean&lt;/p&gt;

&lt;p&gt; import kafka.cluster.&lt;/p&gt;
{Partition, Replica}
&lt;p&gt; import kafka.log.Log&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
@@ -218,7 +218,7 @@ class IsrExpirationTest {&lt;/p&gt;

&lt;p&gt;   private def logMock: Log = {&lt;br/&gt;
     val log = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.Log&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val cache = EasyMock.createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val cache = EasyMock.createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     EasyMock.expect(log.dir).andReturn(TestUtils.tempDir()).anyTimes()&lt;br/&gt;
     EasyMock.expect(log.leaderEpochCache).andReturn(cache).anyTimes()&lt;br/&gt;
     EasyMock.expect(log.onHighWatermarkIncremented(0L))&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
index 2074044d0e1..1f4d04b7cb4 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
@@ -19,8 +19,8 @@ package kafka.server&lt;br/&gt;
 import kafka.cluster.
{BrokerEndPoint, Replica}
&lt;p&gt; import kafka.log.LogManager&lt;br/&gt;
 import kafka.cluster.Partition&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.epoch.util.ReplicaFetcherMockBlockingSend&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -109,7 +109,7 @@ class ReplicaFetcherThreadTest {&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all dependencies&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -167,7 +167,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -213,7 +213,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -258,7 +258,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     // Setup all the dependencies&lt;br/&gt;
     val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -314,7 +314,7 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all stubs&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -362,7 +362,7 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all stubs&lt;br/&gt;
     val quota = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicationQuotaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val leaderEpochs = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaAlterLogDirsManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replica = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
index 3be33a2738c..01ba4b03694 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
@@ -26,7 +26,7 @@ import kafka.log.
{Log, LogConfig, LogManager, ProducerStateManager}
&lt;p&gt; import kafka.utils.&lt;/p&gt;
{MockScheduler, MockTime, TestUtils}
&lt;p&gt; import TestUtils.createBroker&lt;br/&gt;
 import kafka.cluster.BrokerEndPoint&lt;br/&gt;
-import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.epoch.util.ReplicaFetcherMockBlockingSend&lt;br/&gt;
 import kafka.utils.timer.MockTimer&lt;br/&gt;
 import kafka.zk.KafkaZkClient&lt;br/&gt;
@@ -624,8 +624,8 @@ class ReplicaManagerTest {&lt;br/&gt;
     val mockScheduler = new MockScheduler(time)&lt;br/&gt;
     val mockBrokerTopicStats = new BrokerTopicStats&lt;br/&gt;
     val mockLogDirFailureChannel = new LogDirFailureChannel(config.logDirs.size)&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;val mockLeaderEpochCache = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;EasyMock.expect(mockLeaderEpochCache.latestEpoch()).andReturn(leaderEpochFromLeader)&lt;br/&gt;
+    val mockLeaderEpochCache = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(mockLeaderEpochCache.latestEpoch).andReturn(leaderEpochFromLeader)&lt;br/&gt;
     EasyMock.expect(mockLeaderEpochCache.endOffsetFor(leaderEpochFromLeader))&lt;br/&gt;
       .andReturn(localLogOffset)&lt;br/&gt;
     EasyMock.replay(mockLeaderEpochCache)&lt;br/&gt;
@@ -644,7 +644,7 @@ class ReplicaManagerTest {&lt;br/&gt;
         new File(new File(config.logDirs.head), s&quot;$topic-$topicPartition&quot;), 30000),&lt;br/&gt;
       logDirFailureChannel = mockLogDirFailureChannel) 
{
 
-      override def leaderEpochCache: LeaderEpochCache = mockLeaderEpochCache
+      override def leaderEpochCache: LeaderEpochFileCache = mockLeaderEpochCache
 
       override def logEndOffsetMetadata = LogOffsetMetadata(localLogOffset)
     }
&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala b/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala&lt;br/&gt;
index e7c6a9785bc..0c47f15a09f 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala&lt;br/&gt;
@@ -24,7 +24,6 @@ import org.junit.Assert._&lt;br/&gt;
 import org.junit.Test&lt;br/&gt;
 import org.scalatest.junit.JUnitSuite&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
 class LeaderEpochCheckpointFileTest extends JUnitSuite with Logging&lt;/p&gt;
{
 
   @Test
diff --git a/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala b/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala
index 6288d8faf1d..bd87bc2951f 100644
--- a/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala
+++ b/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala
@@ -89,23 +89,23 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness
     assertEquals(0, latestRecord(follower).partitionLeaderEpoch())
 
     //Both leader and follower should have recorded Epoch 0 at Offset 0
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries)
 
     //Bounce the follower
     bounce(follower)
     awaitISR(tp)
 
     //Nothing happens yet as we haven&apos;t sent any new messages.
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0)), epochCache(follower).epochEntries)
 
     //Send a message
     producer.send(new ProducerRecord(topic, 0, null, msg)).get
 
     //Epoch1 should now propagate to the follower with the written message
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries)
 
     //The new message should have epoch 1 stamped
     assertEquals(1, latestRecord(leader).partitionLeaderEpoch())
@@ -116,8 +116,8 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness
     awaitISR(tp)
 
     //Epochs 2 should be added to the leader, but not on the follower (yet), as there has been no replication.
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1)), epochCache(follower).epochEntries)
 
     //Send a message
     producer.send(new ProducerRecord(topic, 0, null, msg)).get
@@ -127,8 +127,8 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness
     assertEquals(2, latestRecord(follower).partitionLeaderEpoch())
 
     //The leader epoch files should now match on leader and follower
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(leader).epochEntries())
-    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(follower).epochEntries())
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(leader).epochEntries)
+    assertEquals(Buffer(EpochEntry(0, 0), EpochEntry(1, 1), EpochEntry(2, 2)), epochCache(follower).epochEntries)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -300,8 +300,8 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness&lt;/p&gt;

&lt;p&gt;   private def log(leader: KafkaServer, follower: KafkaServer): Unit = {&lt;br/&gt;
     info(s&quot;Bounce complete for follower ${follower.config.brokerId}&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info(s&quot;Leader: leo${leader.config.brokerId}: &quot; + getLog(leader, 0).logEndOffset + &quot; cache: &quot; + epochCache(leader).epochEntries())&lt;/li&gt;
	&lt;li&gt;info(s&quot;Follower: leo${follower.config.brokerId}: &quot; + getLog(follower, 0).logEndOffset + &quot; cache: &quot; + epochCache(follower).epochEntries())&lt;br/&gt;
+    info(s&quot;Leader: leo${leader.config.brokerId}: &quot; + getLog(leader, 0).logEndOffset + &quot; cache: &quot; + epochCache(leader).epochEntries)&lt;br/&gt;
+    info(s&quot;Follower: leo${follower.config.brokerId}: &quot; + getLog(follower, 0).logEndOffset + &quot; cache: &quot; + epochCache(follower).epochEntries)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def waitForLogsToMatch(b1: KafkaServer, b2: KafkaServer, partition: Int = 0): Unit = {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
index 4a8df11f8a3..6cd08c7dd7f 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
@@ -16,6 +16,7 @@&lt;br/&gt;
   */&lt;/p&gt;

&lt;p&gt; package kafka.server.epoch&lt;br/&gt;
+&lt;br/&gt;
 import java.io.File&lt;/p&gt;

&lt;p&gt; import kafka.server.LogOffsetMetadata&lt;br/&gt;
@@ -24,7 +25,7 @@ import org.apache.kafka.common.requests.EpochEndOffset.{UNDEFINED_EPOCH, UNDEFIN&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
-import org.junit.&lt;/p&gt;
{Before, Test}
&lt;p&gt;+import org.junit.Test&lt;/p&gt;

&lt;p&gt; import scala.collection.mutable.ListBuffer&lt;/p&gt;

&lt;p&gt;@@ -33,51 +34,42 @@ import scala.collection.mutable.ListBuffer&lt;br/&gt;
   */&lt;br/&gt;
 class LeaderEpochFileCacheTest {&lt;br/&gt;
   val tp = new TopicPartition(&quot;TestTopic&quot;, 5)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var checkpoint: LeaderEpochCheckpoint = _&lt;br/&gt;
+  private var logEndOffset = 0L&lt;br/&gt;
+  private val checkpoint: LeaderEpochCheckpoint = new LeaderEpochCheckpoint 
{
+    private var epochs: Seq[EpochEntry] = Seq()
+    override def write(epochs: Seq[EpochEntry]): Unit = this.epochs = epochs
+    override def read(): Seq[EpochEntry] = this.epochs
+  }
&lt;p&gt;+  private val cache = new LeaderEpochFileCache(tp, logEndOffset _, checkpoint)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldAddEpochAndMessageOffsetToCache() = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //When
-    cache.assign(epoch = 2, offset = 10)
-    leo = 11
+    cache.assign(epoch = 2, startOffset = 10)
+    logEndOffset = 11
 
     //Then
-    assertEquals(2, cache.latestEpoch())
-    assertEquals(EpochEntry(2, 10), cache.epochEntries()(0))
-    assertEquals(11, cache.endOffsetFor(2)) //should match leo
+    assertEquals(2, cache.latestEpoch)
+    assertEquals(EpochEntry(2, 10), cache.epochEntries(0))
+    assertEquals(logEndOffset, cache.endOffsetFor(2)) //should match logEndOffset
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnLogEndOffsetIfLatestEpochRequested() = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //When just one epoch
-    cache.assign(epoch = 2, offset = 11)
-    cache.assign(epoch = 2, offset = 12)
-    leo = 14
+    cache.assign(epoch = 2, startOffset = 11)
+    cache.assign(epoch = 2, startOffset = 12)
+    logEndOffset = 14
 
     //Then
-    assertEquals(14, cache.endOffsetFor(2))
+    assertEquals(logEndOffset, cache.endOffsetFor(2))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnUndefinedOffsetIfUndefinedEpochRequested() = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given cache with some data on leader&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 11)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 12)&lt;br/&gt;
+    // assign couple of epochs&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 11)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 12)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When (say a bootstraping follower) sends request for UNDEFINED_EPOCH&lt;br/&gt;
     val offsetFor = cache.endOffsetFor(UNDEFINED_EPOCH)&lt;br/&gt;
@@ -88,68 +80,51 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotOverwriteLogEndOffsetForALeaderEpochOnceItHasBeenAssigned() = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
     //Given
-    leo = 9
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
+    logEndOffset = 9
 
-    cache.assign(2, leo)
+    cache.assign(2, logEndOffset)
 
     //When called again later
     cache.assign(2, 10)
 
     //Then the offset should NOT have been updated
-    assertEquals(leo, cache.epochEntries()(0).startOffset)
+    assertEquals(logEndOffset, cache.epochEntries(0).startOffset)
+    assertEquals(ListBuffer(EpochEntry(2, 9)), cache.epochEntries)
   }

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldAllowLeaderEpochToChangeEvenIfOffsetDoesNot() = {&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
+  def shouldEnforceMonotonicallyIncreasingStartOffsets() = 
{
     //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
     cache.assign(2, 9)
 
     //When update epoch new epoch but same offset
     cache.assign(3, 9)
 
     //Then epoch should have been updated
-    assertEquals(ListBuffer(EpochEntry(2, 9), EpochEntry(3, 9)), cache.epochEntries())
+    assertEquals(ListBuffer(EpochEntry(3, 9)), cache.epochEntries)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotOverwriteOffsetForALeaderEpochOnceItHasBeenAssigned() = &lt;/p&gt;
{
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; new LogOffsetMetadata(0), checkpoint)
     cache.assign(2, 6)
 
     //When called again later with a greater offset
     cache.assign(2, 10)
 
     //Then later update should have been ignored
-    assertEquals(6, cache.epochEntries()(0).startOffset)
+    assertEquals(6, cache.epochEntries(0).startOffset)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnUnsupportedIfNoEpochRecorded()&lt;/p&gt;
{
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
     assertEquals(UNDEFINED_EPOCH_OFFSET, cache.endOffsetFor(0))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnUnsupportedIfNoEpochRecordedAndUndefinedEpochRequested(){&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leo = 73&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
+    logEndOffset = 73&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When (say a follower on older message format version) sends request for UNDEFINED_EPOCH&lt;br/&gt;
     val offsetFor = cache.endOffsetFor(UNDEFINED_EPOCH)&lt;br/&gt;
@@ -159,39 +134,41 @@ class LeaderEpochFileCacheTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldReturnUnsupportedIfRequestedEpochLessThanFirstEpoch(){&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 5, offset = 11)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 6, offset = 12)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 7, offset = 13)&lt;br/&gt;
+  def shouldReturnFirstEpochIfRequestedEpochLessThanFirstEpoch()
{
+    cache.assign(epoch = 5, startOffset = 11)
+    cache.assign(epoch = 6, startOffset = 12)
+    cache.assign(epoch = 7, startOffset = 13)
 
     //When
-    val offset = cache.endOffsetFor(5 - 1)
+    val endOffset = cache.endOffsetFor(4)
 
     //Then
-    assertEquals(UNDEFINED_EPOCH_OFFSET, offset)
+    assertEquals(11, endOffset)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldGetFirstOffsetOfSubsequentEpochWhenOffsetRequestedForPreviousEpoch() = {&lt;/li&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
+  def shouldTruncateIfMatchingEpochButEarlierStartingOffset(): Unit = 
{
+    cache.assign(epoch = 5, startOffset = 11)
+    cache.assign(epoch = 6, startOffset = 12)
+    cache.assign(epoch = 7, startOffset = 13)
 
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
+    // epoch 7 starts at an earlier offset
+    cache.assign(epoch = 7, startOffset = 12)
 
+    assertEquals(12, cache.endOffsetFor(5))
+    assertEquals(12, cache.endOffsetFor(6))
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def shouldGetFirstOffsetOfSubsequentEpochWhenOffsetRequestedForPreviousEpoch() = {&lt;br/&gt;
     //When several epochs&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 11)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 12)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 13)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 14)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 15)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 16)&lt;/li&gt;
	&lt;li&gt;leo = 17&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 11)&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 12)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 13)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 14)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 15)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 16)&lt;br/&gt;
+    logEndOffset = 17&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then get the start offset of the next epoch&lt;br/&gt;
     assertEquals(15, cache.endOffsetFor(2))&lt;br/&gt;
@@ -199,15 +176,10 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnNextAvailableEpochIfThereIsNoExactEpochForTheOneRequested(){&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
     //When&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 10)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 13)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 17)&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 10)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 13)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 17)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals(13, cache.endOffsetFor(requestedEpoch = 1))&lt;br/&gt;
@@ -216,14 +188,9 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotUpdateEpochAndStartOffsetIfItDidNotChange() = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
     //When&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 7)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 7)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals(1, cache.epochEntries.size)&lt;br/&gt;
@@ -232,14 +199,10 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnInvalidOffsetIfEpochIsRequestedWhichIsNotCurrentlyTracked(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leo = 100&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
+    logEndOffset = 100&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 100)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 100)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals(UNDEFINED_EPOCH_OFFSET, cache.endOffsetFor(3))&lt;br/&gt;
@@ -247,35 +210,28 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldSupportEpochsThatDoNotStartFromZero(): Unit = &lt;/p&gt;
{
-    var leo = 0
-    def leoFinder() = new LogOffsetMetadata(leo)
-
-    //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //When
-    cache.assign(epoch = 2, offset = 6)
-    leo = 7
+    cache.assign(epoch = 2, startOffset = 6)
+    logEndOffset = 7
 
     //Then
-    assertEquals(leo, cache.endOffsetFor(2))
+    assertEquals(logEndOffset, cache.endOffsetFor(2))
     assertEquals(1, cache.epochEntries.size)
-    assertEquals(EpochEntry(2, 6), cache.epochEntries()(0))
+    assertEquals(EpochEntry(2, 6), cache.epochEntries(0))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldPersistEpochsBetweenInstances(){&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
     val checkpointPath = TestUtils.tempFile().getAbsolutePath&lt;/li&gt;
	&lt;li&gt;checkpoint = new LeaderEpochCheckpointFile(new File(checkpointPath))&lt;br/&gt;
+    val checkpoint = new LeaderEpochCheckpointFile(new File(checkpointPath))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Given&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;br/&gt;
+    val cache = new LeaderEpochFileCache(tp, logEndOffset _, checkpoint)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     val checkpoint2 = new LeaderEpochCheckpointFile(new File(checkpointPath))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val cache2 = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint2)&lt;br/&gt;
+    val cache2 = new LeaderEpochFileCache(tp, logEndOffset _, checkpoint2)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then&lt;br/&gt;
     assertEquals(1, cache2.epochEntries.size)&lt;br/&gt;
@@ -283,81 +239,68 @@ class LeaderEpochFileCacheTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldNotLetEpochGoBackwardsEvenIfMessageEpochsDo(): Unit = {&lt;/li&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
+  def shouldEnforceMonotonicallyIncreasingEpochs(): Unit = 
{
     //Given
-    cache.assign(epoch = 1, offset = 5); leo = 6
-    cache.assign(epoch = 2, offset = 6); leo = 7
-
-    //When we update an epoch in the past with an earlier offset
-    cache.assign(epoch = 1, offset = 7); leo = 8
+    cache.assign(epoch = 1, startOffset = 5); logEndOffset = 6
+    cache.assign(epoch = 2, startOffset = 6); logEndOffset = 7
 
-    //Then epoch should not be changed
-    assertEquals(2, cache.latestEpoch())
+    //When we update an epoch in the past with a different offset, the log has already reached
+    //an inconsistent state. Our options are either to raise an error, ignore the new append,
+    //or truncate the cached epochs to the point of conflict. We take this latter approach in
+    //order to guarantee that epochs and offsets in the cache increase monotonically, which makes
+    //the search logic simpler to reason about.
+    cache.assign(epoch = 1, startOffset = 7); logEndOffset = 8
 
-    //Then end offset for epoch 1 shouldn&apos;t have changed
-    assertEquals(6, cache.endOffsetFor(1))
+    //Then later epochs will be removed
+    assertEquals(1, cache.latestEpoch)
 
-    //Then end offset for epoch 2 has to be the offset of the epoch 1 message (I can&apos;t thing of a better option)
-    assertEquals(8, cache.endOffsetFor(2))
+    //Then end offset for epoch 1 will have changed
+    assertEquals(8, cache.endOffsetFor(1))
 
-    //Epoch history shouldn&apos;t have changed
-    assertEquals(EpochEntry(1, 5), cache.epochEntries()(0))
-    assertEquals(EpochEntry(2, 6), cache.epochEntries()(1))
+    //Then end offset for epoch 2 is now undefined
+    assertEquals(UNDEFINED_EPOCH_OFFSET, cache.endOffsetFor(2))
+    assertEquals(EpochEntry(1, 7), cache.epochEntries(0))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldNotLetOffsetsGoBackwardsEvenIfEpochsProgress() = {&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
+  def shouldEnforceOffsetsIncreaseMonotonically() = 
{
     //When epoch goes forward but offset goes backwards
-    cache.assign(epoch = 2, offset = 6)
-    cache.assign(epoch = 3, offset = 5)
+    cache.assign(epoch = 2, startOffset = 6)
+    cache.assign(epoch = 3, startOffset = 5)
 
-    //Then latter assign should be ignored
-    assertEquals(EpochEntry(2, 6), cache.epochEntries.toList(0))
+    //The last assignment wins and the conflicting one is removed from the log
+    assertEquals(EpochEntry(3, 5), cache.epochEntries.toList(0))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldIncreaseAndTrackEpochsAsLeadersChangeManyTimes(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 0) //leo=0&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 0) //logEndOffset=0&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 0) //leo=0&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 0) //logEndOffset=0&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then epoch should go up&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1, cache.latestEpoch())&lt;br/&gt;
+    assertEquals(1, cache.latestEpoch)&lt;br/&gt;
     //offset for 1 should still be 0&lt;br/&gt;
     assertEquals(0, cache.endOffsetFor(1))&lt;br/&gt;
     //offset for epoch 0 should still be 0&lt;br/&gt;
     assertEquals(0, cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When we write 5 messages as epoch 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leo = 5&lt;br/&gt;
+    logEndOffset = 5&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Then end offset for epoch(1) should be leo =&amp;gt; 5&lt;br/&gt;
+    //Then end offset for epoch(1) should be logEndOffset =&amp;gt; 5&lt;br/&gt;
     assertEquals(5, cache.endOffsetFor(1))&lt;br/&gt;
     //Epoch 0 should still be at offset 0&lt;br/&gt;
     assertEquals(0, cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 5) //leo=5&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 5) //logEndOffset=5&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leo = 10 //write another 5 messages&lt;br/&gt;
+    logEndOffset = 10 //write another 5 messages&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Then end offset for epoch(2) should be leo =&amp;gt; 10&lt;br/&gt;
+    //Then end offset for epoch(2) should be logEndOffset =&amp;gt; 10&lt;br/&gt;
     assertEquals(10, cache.endOffsetFor(2))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //end offset for epoch(1) should be the start offset of epoch(2) =&amp;gt; 5&lt;br/&gt;
@@ -369,36 +312,30 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldIncreaseAndTrackEpochsAsFollowerReceivesManyMessages(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var leo = 0&lt;/li&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(leo)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;//When new&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;br/&gt;
-&lt;br/&gt;
     //When Messages come in&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 0); leo = 1&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 1); leo = 2&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 2); leo = 3&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 0); logEndOffset = 1&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 1); logEndOffset = 2&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 2); logEndOffset = 3&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then epoch should stay, offsets should grow&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(0, cache.latestEpoch())&lt;/li&gt;
	&lt;li&gt;assertEquals(leo, cache.endOffsetFor(0))&lt;br/&gt;
+    assertEquals(0, cache.latestEpoch)&lt;br/&gt;
+    assertEquals(logEndOffset, cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When messages arrive with greater epoch&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 3); leo = 4&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 4); leo = 5&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 5); leo = 6&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 3); logEndOffset = 4&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 4); logEndOffset = 5&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 5); logEndOffset = 6&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1, cache.latestEpoch())&lt;/li&gt;
	&lt;li&gt;assertEquals(leo, cache.endOffsetFor(1))&lt;br/&gt;
+    assertEquals(1, cache.latestEpoch)&lt;br/&gt;
+    assertEquals(logEndOffset, cache.endOffsetFor(1))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6); leo = 7&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 7); leo = 8&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 8); leo = 9&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6); logEndOffset = 7&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 7); logEndOffset = 8&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 8); logEndOffset = 9&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(2, cache.latestEpoch())&lt;/li&gt;
	&lt;li&gt;assertEquals(leo, cache.endOffsetFor(2))&lt;br/&gt;
+    assertEquals(2, cache.latestEpoch)&lt;br/&gt;
+    assertEquals(logEndOffset, cache.endOffsetFor(2))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Older epochs should return the start offset of the first message in the subsequent epoch.&lt;br/&gt;
     assertEquals(3, cache.endOffsetFor(0))&lt;br/&gt;
@@ -407,16 +344,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldDropEntriesOnEpochBoundaryWhenRemovingLatestEntries(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When clear latest on epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushLatest(offset = 8)&lt;br/&gt;
+    cache.truncateFromEnd(endOffset = 8)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should remove two latest epochs (remove is inclusive)&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(2, 6)), cache.epochEntries)&lt;br/&gt;
@@ -424,16 +358,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldPreserveResetOffsetOnClearEarliestIfOneExists(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset ON epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 8)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 8)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should preserve (3, 8)&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(3, 8), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -441,16 +372,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldUpdateSavedOffsetWhenOffsetToClearToIsBetweenEpochs(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset BETWEEN epoch boundaries&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 9)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 9)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then we should retain epoch 3, but update it&apos;s offset to 9 as 8 has been removed&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(3, 9), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -458,16 +386,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotClearAnythingIfOffsetToEarly(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset before first epoch offset&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 1)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 1)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then nothing should change&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(2, 6),EpochEntry(3, 8), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -475,16 +400,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotClearAnythingIfOffsetToFirstOffset(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset on earliest epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 6)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 6)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then nothing should change&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(2, 6),EpochEntry(3, 8), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -492,16 +414,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldRetainLatestEpochOnClearAllEarliest(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 11)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then retain the last&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -509,16 +428,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldUpdateOffsetBetweenEpochBoundariesOnClearEarliest(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When we clear from a postition between offset 8 &amp;amp; offset 11&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 9)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 9)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then we should update the middle epoch entry&apos;s offset&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(3, 9), EpochEntry(4, 11)), cache.epochEntries)&lt;br/&gt;
@@ -526,16 +442,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldUpdateOffsetBetweenEpochBoundariesOnClearEarliest2(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 0, offset = 0)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 1, offset = 7)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 10)&lt;br/&gt;
+    cache.assign(epoch = 0, startOffset = 0)&lt;br/&gt;
+    cache.assign(epoch = 1, startOffset = 7)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 10)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When we clear from a postition between offset 0 &amp;amp; offset 7&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 5)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 5)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then we should keeep epoch 0 but update the offset appropriately&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(0,5), EpochEntry(1, 7), EpochEntry(2, 10)), cache.epochEntries)&lt;br/&gt;
@@ -543,16 +456,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldRetainLatestEpochOnClearAllEarliestAndUpdateItsOffset(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset beyond last epoch&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = 15)&lt;br/&gt;
+    cache.truncateFromStart(startOffset = 15)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then update the last&lt;br/&gt;
     assertEquals(ListBuffer(EpochEntry(4, 15)), cache.epochEntries)&lt;br/&gt;
@@ -560,51 +470,42 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldDropEntriesBetweenEpochBoundaryWhenRemovingNewest(): Unit = &lt;/p&gt;
{
-    def leoFinder() = new LogOffsetMetadata(0)
-
     //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-    cache.assign(epoch = 2, offset = 6)
-    cache.assign(epoch = 3, offset = 8)
-    cache.assign(epoch = 4, offset = 11)
+    cache.assign(epoch = 2, startOffset = 6)
+    cache.assign(epoch = 3, startOffset = 8)
+    cache.assign(epoch = 4, startOffset = 11)
 
     //When reset to offset BETWEEN epoch boundaries
-    cache.clearAndFlushLatest(offset = 9)
+    cache.truncateFromEnd(endOffset = 9)
 
     //Then should keep the preceding epochs
-    assertEquals(3, cache.latestEpoch())
+    assertEquals(3, cache.latestEpoch)
     assertEquals(ListBuffer(EpochEntry(2, 6), EpochEntry(3, 8)), cache.epochEntries)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldClearAllEntries(): Unit = &lt;/p&gt;
{
-    def leoFinder() = new LogOffsetMetadata(0)
-
     //Given
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-    cache.assign(epoch = 2, offset = 6)
-    cache.assign(epoch = 3, offset = 8)
-    cache.assign(epoch = 4, offset = 11)
+    cache.assign(epoch = 2, startOffset = 6)
+    cache.assign(epoch = 3, startOffset = 8)
+    cache.assign(epoch = 4, startOffset = 11)
 
-    //When 
+    //When
     cache.clearAndFlush()
 
-    //Then 
+    //Then
     assertEquals(0, cache.epochEntries.size)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotResetEpochHistoryHeadIfUndefinedPassed(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset on epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushLatest(offset = UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+    cache.truncateFromEnd(endOffset = UNDEFINED_EPOCH_OFFSET)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should do nothing&lt;br/&gt;
     assertEquals(3, cache.epochEntries.size)&lt;br/&gt;
@@ -612,16 +513,13 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldNotResetEpochHistoryTailIfUndefinedPassed(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def leoFinder() = new LogOffsetMetadata(0)&lt;br/&gt;
-&lt;br/&gt;
     //Given&lt;/li&gt;
	&lt;li&gt;val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 2, offset = 6)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 3, offset = 8)&lt;/li&gt;
	&lt;li&gt;cache.assign(epoch = 4, offset = 11)&lt;br/&gt;
+    cache.assign(epoch = 2, startOffset = 6)&lt;br/&gt;
+    cache.assign(epoch = 3, startOffset = 8)&lt;br/&gt;
+    cache.assign(epoch = 4, startOffset = 11)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When reset to offset on epoch boundary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cache.clearAndFlushEarliest(offset = UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+    cache.truncateFromEnd(endOffset = UNDEFINED_EPOCH_OFFSET)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then should do nothing&lt;br/&gt;
     assertEquals(3, cache.epochEntries.size)&lt;br/&gt;
@@ -629,54 +527,26 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldFetchLatestEpochOfEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
     assertEquals(-1, cache.latestEpoch)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldFetchEndOffsetOfEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
     assertEquals(-1, cache.endOffsetFor(7))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldClearEarliestOnEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
-    cache.clearAndFlushEarliest(7)
+    cache.truncateFromStart(7)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldClearLatestOnEmptyCache(): Unit = &lt;/p&gt;
{
-    //Given
-    def leoFinder() = new LogOffsetMetadata(0)
-
-    //When
-    val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
-
     //Then
-    cache.clearAndFlushLatest(7)
+    cache.truncateFromEnd(7)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Before&lt;/li&gt;
	&lt;li&gt;def setUp() 
{
-    checkpoint = new LeaderEpochCheckpointFile(TestUtils.tempFile())
-  }
&lt;p&gt; }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
index dc6ff9edc1a..0d479fffe4a 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
@@ -37,9 +37,10 @@ import org.apache.kafka.common.requests.{EpochEndOffset, OffsetsForLeaderEpochRe&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.Map&lt;br/&gt;
+import scala.collection.mutable.ListBuffer&lt;/p&gt;

&lt;p&gt; class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var brokers: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaServer&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
+  var brokers: ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaServer&amp;#93;&lt;/span&gt; = ListBuffer()&lt;br/&gt;
   val topic1 = &quot;foo&quot;&lt;br/&gt;
   val topic2 = &quot;bar&quot;&lt;br/&gt;
   val t1p0 = new TopicPartition(topic1, 0)&lt;br/&gt;
@@ -60,7 +61,7 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def shouldAddCurrentLeaderEpochToMessagesAsTheyAreWrittenToLeader() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;brokers = (0 to 1).map 
{ id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }&lt;br/&gt;
+    brokers ++= (0 to 1).map { id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Given two topics with replication of a single partition&lt;br/&gt;
     for (topic &amp;lt;- List(topic1, topic2)) {&lt;br/&gt;
@@ -94,14 +95,13 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
   def shouldSendLeaderEpochRequestAndGetAResponse(): Unit = {&lt;/p&gt;

&lt;p&gt;     //3 brokers, put partition on 100/101 and then pretend to be 102&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;brokers = (100 to 102).map 
{ id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }&lt;br/&gt;
-    adminZkClient.createOrUpdateTopicPartitionAssignmentPathInZK(topic1, Map(&lt;br/&gt;
-      0 -&amp;gt; Seq(100),&lt;br/&gt;
-      1 -&amp;gt; Seq(101)&lt;br/&gt;
-    ))&lt;br/&gt;
-    adminZkClient.createOrUpdateTopicPartitionAssignmentPathInZK(topic2, Map(&lt;br/&gt;
-      0 -&amp;gt; Seq(100)&lt;br/&gt;
-    ))&lt;br/&gt;
+    brokers ++= (100 to 102).map { id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }
&lt;p&gt;+&lt;br/&gt;
+    val assignment1 = Map(0 -&amp;gt; Seq(100), 1 -&amp;gt; Seq(101))&lt;br/&gt;
+    TestUtils.createTopic(zkClient, topic1, assignment1, brokers)&lt;br/&gt;
+&lt;br/&gt;
+    val assignment2 = Map(0 -&amp;gt; Seq(100))&lt;br/&gt;
+    TestUtils.createTopic(zkClient, topic2, assignment2, brokers)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Send messages equally to the two partitions, then half as many to a third&lt;br/&gt;
     producer = createNewProducer(getBrokerListStrFromServers(brokers), retries = 5, acks = -1)&lt;br/&gt;
@@ -139,9 +139,12 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldIncreaseLeaderEpochBetweenLeaderRestarts(): Unit = {&lt;br/&gt;
-&lt;br/&gt;
     //Setup: we are only interested in the single partition on broker 101&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;brokers = Seq(100, 101).map 
{ id =&amp;gt; createServer(fromProps(createBrokerConfig(id, zkConnect))) }
&lt;p&gt;+    brokers += createServer(fromProps(createBrokerConfig(100, zkConnect)))&lt;br/&gt;
+    assertEquals(100, TestUtils.waitUntilControllerElected(zkClient))&lt;br/&gt;
+&lt;br/&gt;
+    brokers += createServer(fromProps(createBrokerConfig(101, zkConnect)))&lt;br/&gt;
+&lt;br/&gt;
     def leo() = brokers(1).replicaManager.getReplica(tp).get.logEndOffset.messageOffset&lt;br/&gt;
     adminZkClient.createOrUpdateTopicPartitionAssignmentPathInZK(tp.topic, Map(tp.partition -&amp;gt; Seq(101)))&lt;br/&gt;
     producer = createNewProducer(getBrokerListStrFromServers(brokers), retries = 10, acks = -1)&lt;br/&gt;
@@ -151,10 +154,9 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
     var fetcher = new TestFetcherThread(sender(brokers(0), brokers(1)))&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Then epoch should be 0 and leo: 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var offset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp).endOffset()&lt;/li&gt;
	&lt;li&gt;assertEquals(1, offset)&lt;/li&gt;
	&lt;li&gt;assertEquals(leo(), offset)&lt;br/&gt;
-&lt;br/&gt;
+    var epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp)&lt;br/&gt;
+    assertEquals(1, epochEndOffset.endOffset)&lt;br/&gt;
+    assertEquals(1, leo())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //2. When broker is bounced&lt;br/&gt;
     brokers(1).shutdown()&lt;br/&gt;
@@ -163,15 +165,20 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
     producer.send(new ProducerRecord(tp.topic, tp.partition, null, &quot;IHeartLogs&quot;.getBytes)).get&lt;br/&gt;
     fetcher = new TestFetcherThread(sender(brokers(0), brokers(1)))&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     //Then epoch 0 should still be the start offset of epoch 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;offset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp).endOffset()&lt;/li&gt;
	&lt;li&gt;assertEquals(1, offset)&lt;br/&gt;
+    epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp)&lt;br/&gt;
+    assertEquals(1, epochEndOffset.endOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Then epoch 2 should be the leo (NB: The leader epoch goes up in factors of 2 - This is because we have to first change leader to -1 and then change it again to the live replica)&lt;/li&gt;
	&lt;li&gt;assertEquals(2, fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 2))(tp).endOffset())&lt;/li&gt;
	&lt;li&gt;assertEquals(leo(), fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 2))(tp).endOffset())&lt;br/&gt;
+    //No data written in epoch 1&lt;br/&gt;
+    epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 1))(tp)&lt;br/&gt;
+    assertEquals(1, epochEndOffset.endOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    //Then epoch 2 should be the leo (NB: The leader epoch goes up in factors of 2 -&lt;br/&gt;
+    //This is because we have to first change leader to -1 and then change it again to the live replica)&lt;br/&gt;
+    //Note that the expected leader changes depend on the controller being on broker 100, which is not restarted&lt;br/&gt;
+    epochEndOffset = fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 2))(tp)&lt;br/&gt;
+    assertEquals(2, epochEndOffset.endOffset)&lt;br/&gt;
+    assertEquals(2, leo())&lt;/p&gt;

&lt;p&gt;     //3. When broker is bounced again&lt;br/&gt;
     brokers(1).shutdown()&lt;br/&gt;
@@ -180,7 +187,6 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
     producer.send(new ProducerRecord(tp.topic, tp.partition, null, &quot;IHeartLogs&quot;.getBytes)).get&lt;br/&gt;
     fetcher = new TestFetcherThread(sender(brokers(0), brokers(1)))&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     //Then Epoch 0 should still map to offset 1&lt;br/&gt;
     assertEquals(1, fetcher.leaderOffsetsFor(Map(tp -&amp;gt; 0))(tp).endOffset())&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
index 1c01d622438..da1ebbecc14 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
@@ -47,7 +47,7 @@ class OffsetsForLeaderEpochTest {&lt;/p&gt;

&lt;p&gt;     //Stubs&lt;br/&gt;
     val mockLog = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.Log&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val mockCache = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.epoch.LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val mockCache = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.epoch.LeaderEpochFileCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.LogManager&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     expect(mockCache.endOffsetFor(epochRequested)).andReturn(offset)&lt;br/&gt;
     expect(mockLog.leaderEpochCache).andReturn(mockCache).anyTimes()&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/utils/TestUtils.scala b/core/src/test/scala/unit/kafka/utils/TestUtils.scala&lt;br/&gt;
index 7eb5caf5269..b425df86f5c 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/utils/TestUtils.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/utils/TestUtils.scala&lt;br/&gt;
@@ -377,10 +377,11 @@ object TestUtils extends Logging {&lt;br/&gt;
               producerId: Long = RecordBatch.NO_PRODUCER_ID,&lt;br/&gt;
               producerEpoch: Short = RecordBatch.NO_PRODUCER_EPOCH,&lt;br/&gt;
               sequence: Int = RecordBatch.NO_SEQUENCE,&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;baseOffset: Long = 0L): MemoryRecords = {&lt;br/&gt;
+              baseOffset: Long = 0L,&lt;br/&gt;
+              partitionLeaderEpoch: Int = RecordBatch.NO_PARTITION_LEADER_EPOCH): MemoryRecords = 
{
     val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))
     val builder = MemoryRecords.builder(buf, magicValue, codec, TimestampType.CREATE_TIME, baseOffset,
-      System.currentTimeMillis, producerId, producerEpoch, sequence)
+      System.currentTimeMillis, producerId, producerEpoch, sequence, false, partitionLeaderEpoch)
     records.foreach(builder.append)
     builder.build()
   }&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 6 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3y427:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>