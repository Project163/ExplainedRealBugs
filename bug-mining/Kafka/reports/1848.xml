<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:07:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6624] log segment deletion could cause a disk to be marked offline incorrectly</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6624</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Saw the following log.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-03-06 23:12:20,721&amp;#93;&lt;/span&gt; ERROR Error while flushing log for topic1-0 in dir /data01/kafka-logs with offset 80993 (kafka.server.LogDirFailureChannel)&lt;/p&gt;

&lt;p&gt;java.nio.channels.ClosedChannelException&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:110)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:379)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at org.apache.kafka.common.record.FileRecords.flush(FileRecords.java:163)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.LogSegment$$anonfun$flush$1.apply$mcV$sp(LogSegment.scala:375)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.LogSegment$$anonfun$flush$1.apply(LogSegment.scala:374)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.LogSegment$$anonfun$flush$1.apply(LogSegment.scala:374)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.LogSegment.flush(LogSegment.scala:374)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log$$anonfun$flush$1$$anonfun$apply$mcV$sp$4.apply(Log.scala:1374)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log$$anonfun$flush$1$$anonfun$apply$mcV$sp$4.apply(Log.scala:1373)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at scala.collection.Iterator$class.foreach(Iterator.scala:891)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log$$anonfun$flush$1.apply$mcV$sp(Log.scala:1373)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log$$anonfun$flush$1.apply(Log.scala:1368)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log$$anonfun$flush$1.apply(Log.scala:1368)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log.maybeHandleIOException(Log.scala:1669)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log.flush(Log.scala:1368)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.log.Log$$anonfun$roll$2$$anonfun$apply$1.apply$mcV$sp(Log.scala:1343)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.utils.KafkaScheduler$$anonfun$1.apply$mcV$sp(KafkaScheduler.scala:110)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:61)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; at java.lang.Thread.run(Thread.java:748)&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-03-06 23:12:20,722&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaManager broker=0&amp;#93;&lt;/span&gt; Stopping serving replicas in dir /data01/kafka-logs (kafka.server.ReplicaManager)&lt;/p&gt;

&lt;p&gt;It seems that topic1 was being deleted around the time when flushing was called. Then flushing hit an IOException, which caused the disk to be marked offline incorrectly.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13143413">KAFKA-6624</key>
            <summary>log segment deletion could cause a disk to be marked offline incorrectly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lindong">Dong Lin</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                    </labels>
                <created>Thu, 8 Mar 2018 01:34:02 +0000</created>
                <updated>Tue, 13 Mar 2018 05:22:51 +0000</updated>
                            <resolved>Tue, 13 Mar 2018 05:22:51 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16390572" author="junrao" created="Thu, 8 Mar 2018 01:37:48 +0000"  >&lt;p&gt;One way to fix this is to add a flag for&#160;a log segment to be deleted. Then, if we hit an IOException during flushing, we could check if the deletion flag of the segment is set and if so, just ignore the IOException.&lt;/p&gt;</comment>
                            <comment id="16390727" author="junrao" created="Thu, 8 Mar 2018 04:25:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lindong&quot; class=&quot;user-hover&quot; rel=&quot;lindong&quot;&gt;lindong&lt;/a&gt;, do you think this is an issue? Thanks.&lt;/p&gt;</comment>
                            <comment id="16390748" author="githubbot" created="Thu, 8 Mar 2018 04:50:29 +0000"  >&lt;p&gt;lindong28 opened a new pull request #4663: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6624&quot; title=&quot;log segment deletion could cause a disk to be marked offline incorrectly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6624&quot;&gt;&lt;del&gt;KAFKA-6624&lt;/del&gt;&lt;/a&gt;; Prevent concurrent log flush and log deletion&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4663&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4663&lt;/a&gt;&lt;/p&gt;




&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16390753" author="lindong" created="Thu, 8 Mar 2018 04:53:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; It may be reasonable to grab the lock of the log during log.flush(). This will be consistent with the idea that all write/read operation of log segments will be synchronized using the per-log lock.&lt;/p&gt;

&lt;p&gt;It will incur slightly higher overhead but personally I think it is OK. We will only flush the log segments since the previous flushed offset (i.e. recovery offset). Alternatively we can use the flag to optimize it. Not sure if the performance improvement with that optimization is worth the extra complexity.&lt;/p&gt;

&lt;p&gt;Can you review &lt;a href=&quot;https://github.com/apache/kafka/pull/4663/files&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4663/files?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16396554" author="githubbot" created="Tue, 13 Mar 2018 05:20:47 +0000"  >&lt;p&gt;junrao closed pull request #4663: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6624&quot; title=&quot;log segment deletion could cause a disk to be marked offline incorrectly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6624&quot;&gt;&lt;del&gt;KAFKA-6624&lt;/del&gt;&lt;/a&gt;; Prevent concurrent log flush and log deletion&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4663&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4663&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/LogManager.scala b/core/src/main/scala/kafka/log/LogManager.scala&lt;br/&gt;
index 9ae93aadf06..7aa5bcd88d8 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogManager.scala&lt;br/&gt;
@@ -75,7 +75,8 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
   // from one log directory to another log directory on the same broker. The directory of the future log will be renamed&lt;br/&gt;
   // to replace the current log of the partition after the future log catches up with the current log&lt;br/&gt;
   private val futureLogs = new Pool&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Log&amp;#93;&lt;/span&gt;()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private val logsToBeDeleted = new LinkedBlockingQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;Log&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+  // Each element in the queue contains the log object to be deleted and the time it is scheduled for deletion.&lt;br/&gt;
+  private val logsToBeDeleted = new LinkedBlockingQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;(Log, Long)&amp;#93;&lt;/span&gt;()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private val _liveLogDirs: ConcurrentLinkedQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt; = createAndValidateLogDirs(logDirs, initialOfflineDirs)&lt;br/&gt;
   @volatile var currentDefaultConfig = initialDefaultConfig&lt;br/&gt;
@@ -240,6 +241,10 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  private def addLogToBeDeleted(log: Log): Unit = &lt;/p&gt;
{
+    this.logsToBeDeleted.add((log, time.milliseconds()))
+  }
&lt;p&gt;+&lt;br/&gt;
   private def loadLog(logDir: File, recoveryPoints: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;, logStartOffsets: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
     debug(&quot;Loading log &apos;&quot; + logDir.getName + &quot;&apos;&quot;)&lt;br/&gt;
     val topicPartition = Log.parseTopicPartitionName(logDir)&lt;br/&gt;
@@ -260,7 +265,7 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       logDirFailureChannel = logDirFailureChannel)&lt;/p&gt;

&lt;p&gt;     if (logDir.getName.endsWith(Log.DeleteDirSuffix)) &lt;/p&gt;
{
-      this.logsToBeDeleted.add(log)
+      addLogToBeDeleted(log)
     }
&lt;p&gt; else {&lt;br/&gt;
       val previous = {&lt;br/&gt;
         if (log.isFuture)&lt;br/&gt;
@@ -704,9 +709,12 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
   private def deleteLogs(): Unit = {&lt;br/&gt;
     try {&lt;br/&gt;
       while (!logsToBeDeleted.isEmpty) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val removedLog = logsToBeDeleted.take()&lt;br/&gt;
+        val (removedLog, scheduleTimeMs) = logsToBeDeleted.take()&lt;br/&gt;
         if (removedLog != null) {&lt;br/&gt;
           try {&lt;br/&gt;
+            val waitingTimeMs = scheduleTimeMs + currentDefaultConfig.fileDeleteDelayMs - time.milliseconds()&lt;br/&gt;
+            if (waitingTimeMs &amp;gt; 0)&lt;br/&gt;
+              Thread.sleep(waitingTimeMs)&lt;br/&gt;
             removedLog.delete()&lt;br/&gt;
             info(s&quot;Deleted log for partition ${removedLog.topicPartition} in ${removedLog.dir.getAbsolutePath}.&quot;)&lt;br/&gt;
           } catch 
{
@@ -767,7 +775,7 @@ class LogManager(logDirs: Seq[File],
         sourceLog.close()
         checkpointLogRecoveryOffsetsInDir(sourceLog.dir.getParentFile)
         checkpointLogStartOffsetsInDir(sourceLog.dir.getParentFile)
-        logsToBeDeleted.add(sourceLog)
+        addLogToBeDeleted(sourceLog)
       }
&lt;p&gt; catch {&lt;br/&gt;
         case e: KafkaStorageException =&amp;gt;&lt;br/&gt;
           // If sourceLog&apos;s log directory is offline, we need close its handlers here.&lt;br/&gt;
@@ -805,7 +813,7 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       removedLog.renameDir(Log.logDeleteDirName(topicPartition))&lt;br/&gt;
       checkpointLogRecoveryOffsetsInDir(removedLog.dir.getParentFile)&lt;br/&gt;
       checkpointLogStartOffsetsInDir(removedLog.dir.getParentFile)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;logsToBeDeleted.add(removedLog)&lt;br/&gt;
+      addLogToBeDeleted(removedLog)&lt;br/&gt;
       info(s&quot;Log for partition ${removedLog.topicPartition} is renamed to ${removedLog.dir.getAbsolutePath} and is scheduled for deletion&quot;)&lt;br/&gt;
     } else if (offlineLogDirs.nonEmpty) {&lt;br/&gt;
       throw new KafkaStorageException(&quot;Failed to delete log for &quot; + topicPartition + &quot; because it may be in one of the offline directories &quot; + offlineLogDirs.mkString(&quot;,&quot;))&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 36 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3r0rr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>