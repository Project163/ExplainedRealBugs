<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:04:24 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6042] Kafka Request Handler deadlocks and brings down the cluster.</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6042</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We have been experiencing a deadlock that happens on a consistent server within our cluster. This happens multiple times a week currently. It first started happening when we upgraded to 0.11.0.0. Sadly 0.11.0.1 failed to resolve the issue.&lt;/p&gt;

&lt;p&gt;Sequence of events:&lt;/p&gt;

&lt;p&gt;At a seemingly random time broker 125 goes into a deadlock. As soon as it is deadlocked it will remove all the ISR&apos;s for any partition is its the leader for.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,061&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,24&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,073&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,974&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,079&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,64&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,081&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,21&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,084&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,12&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,085&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,61&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,086&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,53&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,088&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,27&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,090&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,182&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:06:10,091&amp;#93;&lt;/span&gt; INFO Partition &lt;span class=&quot;error&quot;&gt;&amp;#91;XXXXXXXXXX,16&amp;#93;&lt;/span&gt; on broker 125: Shrinking ISR from 117,125 to 125 (kafka.cluster.Partition)&lt;br/&gt;
....&lt;/p&gt;


&lt;p&gt;The other nodes fail to connect to the node 125 &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2017-10-10 00:08:42,318&amp;#93;&lt;/span&gt; WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaFetcherThread-0-125&amp;#93;&lt;/span&gt;: Error in fetch to broker 125, request (type=FetchRequest, replicaId=101, maxWait=500, minBytes=1, maxBytes=10485760, fetchData=&lt;/p&gt;
{XXXXXXXXXX-94=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-22=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-58=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-11=(offset=78932482, logStartOffset=50881481, maxBytes=1048576), XXXXXXXXXX-55=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-19=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-91=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-5=(offset=903857106, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-80=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-88=(offset=0, logStartOffset=0, maxBytes=1048576), XXXXXXXXXX-34=(offset=308, logStartOffset=308, maxBytes=1048576), XXXXXXXXXX-7=(offset=369990, logStartOffset=369990, maxBytes=1048576), XXXXXXXXXX-0=(offset=57965795, logStartOffset=0, maxBytes=1048576)}
&lt;p&gt;) (kafka.server.ReplicaFetcherThread)&lt;br/&gt;
java.io.IOException: Connection to 125 was disconnected before the response was read&lt;br/&gt;
        at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:93)&lt;br/&gt;
        at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:93)&lt;br/&gt;
        at kafka.server.ReplicaFetcherThread.fetch(ReplicaFetcherThread.scala:207)&lt;br/&gt;
        at kafka.server.ReplicaFetcherThread.fetch(ReplicaFetcherThread.scala:42)&lt;br/&gt;
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:151)&lt;br/&gt;
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:112)&lt;br/&gt;
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)&lt;/p&gt;

&lt;p&gt;As node 125 removed all the ISRs as it was locking up, a failover for any partition without an unclean leader election is not possible. This breaks any partition that this node was leader for. As we spread all topics across all servers this essentials brings down the entire cluster.&lt;/p&gt;


&lt;p&gt;Recovery:&lt;/p&gt;

&lt;p&gt;Unforuntately with broker 125 in its deadlocked state a clean shutdown does not work. A kill -9 is necessary. After an unclean shutdown the indexes must be rebuilt and start up time is around 10 minutes. After node 125 finally starts the cluster recovers.&lt;/p&gt;


&lt;p&gt;A thread dump on 125 shows:&lt;/p&gt;

&lt;p&gt;Found one Java-level deadlock:&lt;br/&gt;
=============================&lt;br/&gt;
&quot;executor-Produce&quot;:&lt;br/&gt;
  waiting to lock monitor 0x00007f01e417ac48 (object 0x000000068d10a1a0, a kafka.coordinator.group.GroupMetadata),&lt;br/&gt;
  which is held by &quot;kafka-request-handler-2&quot;&lt;br/&gt;
&quot;kafka-request-handler-2&quot;:&lt;br/&gt;
  waiting to lock monitor 0x00007f0208f5e198 (object 0x000000068d2a45f8, a kafka.coordinator.group.GroupMetadata),&lt;br/&gt;
  which is held by &quot;kafka-request-handler-1&quot;&lt;br/&gt;
&quot;kafka-request-handler-1&quot;:&lt;br/&gt;
  waiting to lock monitor 0x00007f01e417ac48 (object 0x000000068d10a1a0, a kafka.coordinator.group.GroupMetadata),&lt;br/&gt;
  which is held by &quot;kafka-request-handler-2&quot;&lt;/p&gt;

&lt;p&gt;Java stack information for the threads listed above:&lt;br/&gt;
===================================================&lt;br/&gt;
&quot;executor-Produce&quot;:&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager.putCacheCallback$2(GroupMetadataManager.scala:312)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;waiting to lock &amp;lt;0x000000068d10a1a0&amp;gt; (a kafka.coordinator.group.GroupMetadata)&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager.$anonfun$storeOffsets$10(GroupMetadataManager.scala:381)&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager.$anonfun$storeOffsets$10$adapted(GroupMetadataManager.scala:381)&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager$$Lambda$990/350831638.apply(Unknown Source)&lt;br/&gt;
        at kafka.server.DelayedProduce.onComplete(DelayedProduce.scala:131)&lt;br/&gt;
        at kafka.server.DelayedOperation.forceComplete(DelayedOperation.scala:66)&lt;br/&gt;
        at kafka.server.DelayedOperation.run(DelayedOperation.scala:112)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:748)&lt;br/&gt;
&quot;kafka-request-handler-2&quot;:&lt;br/&gt;
        at kafka.server.DelayedProduce.safeTryComplete(DelayedProduce.scala:75)&lt;/li&gt;
	&lt;li&gt;waiting to lock &amp;lt;0x000000068d2a45f8&amp;gt; (a kafka.coordinator.group.GroupMetadata)&lt;br/&gt;
        at kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:338)&lt;br/&gt;
        at kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:244)&lt;br/&gt;
        at kafka.server.ReplicaManager.tryCompleteDelayedProduce(ReplicaManager.scala:250)&lt;br/&gt;
        at kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:418)&lt;br/&gt;
        at kafka.cluster.Partition.appendRecordsToLeader(Partition.scala:500)&lt;br/&gt;
        at kafka.server.ReplicaManager.$anonfun$appendToLocalLog$2(ReplicaManager.scala:546)&lt;br/&gt;
        at kafka.server.ReplicaManager$$Lambda$887/1936159787.apply(Unknown Source)&lt;br/&gt;
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.TraversableLike$$Lambda$14/1859039536.apply(Unknown Source)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:120)&lt;br/&gt;
        at scala.collection.TraversableLike.map(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.TraversableLike.map$(TraversableLike.scala:227)&lt;br/&gt;
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;br/&gt;
        at kafka.server.ReplicaManager.appendToLocalLog(ReplicaManager.scala:532)&lt;br/&gt;
        at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:374)&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager.appendForGroup(GroupMetadataManager.scala:246)&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager.storeOffsets(GroupMetadataManager.scala:381)&lt;br/&gt;
        at kafka.coordinator.group.GroupCoordinator.doCommitOffsets(GroupCoordinator.scala:465)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x000000068d10a1a0&amp;gt; (a kafka.coordinator.group.GroupMetadata)&lt;br/&gt;
        at kafka.coordinator.group.GroupCoordinator.handleCommitOffsets(GroupCoordinator.scala:429)&lt;br/&gt;
        at kafka.server.KafkaApis.handleOffsetCommitRequest(KafkaApis.scala:361)&lt;br/&gt;
        at kafka.server.KafkaApis.handle(KafkaApis.scala:105)&lt;br/&gt;
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:66)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:748)&lt;br/&gt;
&quot;kafka-request-handler-1&quot;:&lt;br/&gt;
        at kafka.server.DelayedProduce.safeTryComplete(DelayedProduce.scala:75)&lt;/li&gt;
	&lt;li&gt;waiting to lock &amp;lt;0x000000068d10a1a0&amp;gt; (a kafka.coordinator.group.GroupMetadata)&lt;br/&gt;
        at kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:338)&lt;br/&gt;
        at kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:244)&lt;br/&gt;
        at kafka.server.ReplicaManager.tryCompleteDelayedProduce(ReplicaManager.scala:250)&lt;br/&gt;
        at kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:418)&lt;br/&gt;
        at kafka.cluster.Partition.appendRecordsToLeader(Partition.scala:500)&lt;br/&gt;
        at kafka.server.ReplicaManager.$anonfun$appendToLocalLog$2(ReplicaManager.scala:546)&lt;br/&gt;
        at kafka.server.ReplicaManager$$Lambda$887/1936159787.apply(Unknown Source)&lt;br/&gt;
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.TraversableLike$$Lambda$14/1859039536.apply(Unknown Source)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:120)&lt;br/&gt;
        at scala.collection.TraversableLike.map(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.TraversableLike.map$(TraversableLike.scala:227)&lt;br/&gt;
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;br/&gt;
        at kafka.server.ReplicaManager.appendToLocalLog(ReplicaManager.scala:532)&lt;br/&gt;
        at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:374)&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager.appendForGroup(GroupMetadataManager.scala:246)&lt;br/&gt;
        at kafka.coordinator.group.GroupMetadataManager.storeOffsets(GroupMetadataManager.scala:381)&lt;br/&gt;
        at kafka.coordinator.group.GroupCoordinator.doCommitOffsets(GroupCoordinator.scala:465)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x000000068d2a45f8&amp;gt; (a kafka.coordinator.group.GroupMetadata)&lt;br/&gt;
        at kafka.coordinator.group.GroupCoordinator.handleCommitOffsets(GroupCoordinator.scala:429)&lt;br/&gt;
        at kafka.server.KafkaApis.handleOffsetCommitRequest(KafkaApis.scala:361)&lt;br/&gt;
        at kafka.server.KafkaApis.handle(KafkaApis.scala:105)&lt;br/&gt;
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:66)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:748)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Full thread dump attached.&lt;/p&gt;</description>
                <environment>kafka version: 0.11.0.1&lt;br/&gt;
client versions: 0.8.2.1-0.10.2.1&lt;br/&gt;
platform: aws (eu-west-1a)&lt;br/&gt;
nodes: 36 x r4.xlarge&lt;br/&gt;
disk storage: 2.5 tb per node (~73% usage per node)&lt;br/&gt;
topics: 250&lt;br/&gt;
number of partitions: 48k (approx)&lt;br/&gt;
os: ubuntu 14.04&lt;br/&gt;
jvm: Java(TM) SE Runtime Environment (build 1.8.0_131-b11), Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)</environment>
        <key id="13108230">KAFKA-6042</key>
            <summary>Kafka Request Handler deadlocks and brings down the cluster.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsivaram">Rajini Sivaram</assignee>
                                    <reporter username="corlettb">Ben Corlett</reporter>
                        <labels>
                    </labels>
                <created>Tue, 10 Oct 2017 09:31:24 +0000</created>
                <updated>Thu, 28 Mar 2019 06:47:12 +0000</updated>
                            <resolved>Sun, 22 Oct 2017 03:18:46 +0000</resolved>
                                    <version>0.11.0.0</version>
                    <version>0.11.0.1</version>
                                    <fixVersion>0.11.0.2</fixVersion>
                    <fixVersion>1.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>17</watches>
                                                                                                                <comments>
                            <comment id="16198453" author="omkreddy" created="Tue, 10 Oct 2017 09:55:17 +0000"  >&lt;p&gt;This may be related &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5970&quot; title=&quot;Deadlock due to locking of DelayedProduce and group&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5970&quot;&gt;&lt;del&gt;KAFKA-5970&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16198483" author="corlettb" created="Tue, 10 Oct 2017 10:25:09 +0000"  >&lt;p&gt;The second stack trace in that issue looks very familiar. You may well be right. I should have searched a bit harder.&lt;/p&gt;

&lt;p&gt;Do you know when 0.11.0.2 will be released. Wondering if I should create a build with that pull request?&lt;/p&gt;</comment>
                            <comment id="16198497" author="omkreddy" created="Tue, 10 Oct 2017 10:34:44 +0000"  >&lt;p&gt;It would we great if you can use &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5970&quot; title=&quot;Deadlock due to locking of DelayedProduce and group&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5970&quot;&gt;&lt;del&gt;KAFKA-5970&lt;/del&gt;&lt;/a&gt; and validate in your cluster.&lt;/p&gt;

&lt;p&gt;Kafka Community has not yet started discussion on 0.11.0.2 release.  &lt;/p&gt;

</comment>
                            <comment id="16198505" author="ijuma" created="Tue, 10 Oct 2017 10:49:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=corlettb&quot; class=&quot;user-hover&quot; rel=&quot;corlettb&quot;&gt;corlettb&lt;/a&gt;, you could create a build from the 0.11.0 branch as you probably don&apos;t want to wait until the new release is out. Also, it would be good to have confirmation that it does indeed fix the issue. It should do, but until we have someone who has confirmed it in their environment, we can&apos;t be 100% sure.&lt;/p&gt;</comment>
                            <comment id="16198640" author="corlettb" created="Tue, 10 Oct 2017 13:05:25 +0000"  >&lt;p&gt;I&apos;ve deployed a build of 0.11.0.1 with the commits of pull request 3956 to node 125. Will let you know how I get on.&lt;/p&gt;</comment>
                            <comment id="16209195" author="corlettb" created="Wed, 18 Oct 2017 11:54:54 +0000"  >&lt;p&gt;So far after a week. No issues.&lt;/p&gt;</comment>
                            <comment id="16212375" author="corlettb" created="Fri, 20 Oct 2017 08:46:22 +0000"  >&lt;p&gt;Unfortunately we&apos;ve had another incident today on broker 125.&lt;/p&gt;

&lt;p&gt;Here is the github commits of the patched build I was using. &lt;a href=&quot;https://github.com/corlettb/kafka/commits/deadlock&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/corlettb/kafka/commits/deadlock&lt;/a&gt;. Basically 0.11.0.1 with a cherry pick of the changes in pull request 3956 (for kafka-5970).&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Found one Java-level deadlock:
=============================
&quot;executor-Heartbeat&quot;:
  waiting to lock monitor 0x00007fbd8c1834c8 (object 0x000000068cccb590, a kafka.coordinator.group.GroupMetadata),
  which is held by &quot;kafka-request-handler-7&quot;
&quot;kafka-request-handler-7&quot;:
  waiting to lock monitor 0x00007fbe1942f698 (object 0x000000068cd2c420, a kafka.coordinator.group.GroupMetadata),
  which is held by &quot;kafka-request-handler-4&quot;
&quot;kafka-request-handler-4&quot;:
  waiting to lock monitor 0x00007fbd8c1834c8 (object 0x000000068cccb590, a kafka.coordinator.group.GroupMetadata),
  which is held by &quot;kafka-request-handler-7&quot;

Java stack information for the threads listed above:
===================================================
&quot;executor-Heartbeat&quot;:
	at kafka.coordinator.group.GroupCoordinator.onExpireHeartbeat(GroupCoordinator.scala:776)
	- waiting to lock &amp;lt;0x000000068cccb590&amp;gt; (a kafka.coordinator.group.GroupMetadata)
	at kafka.coordinator.group.DelayedHeartbeat.onExpiration(DelayedHeartbeat.scala:34)
	at kafka.server.DelayedOperation.run(DelayedOperation.scala:120)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
&quot;kafka-request-handler-7&quot;:
	at kafka.coordinator.group.GroupMetadataManager.putCacheCallback$2(GroupMetadataManager.scala:311)
	- waiting to lock &amp;lt;0x000000068cd2c420&amp;gt; (a kafka.coordinator.group.GroupMetadata)
	at kafka.coordinator.group.GroupMetadataManager.$anonfun$storeOffsets$10(GroupMetadataManager.scala:380)
	at kafka.coordinator.group.GroupMetadataManager.$anonfun$storeOffsets$10$adapted(GroupMetadataManager.scala:380)
	at kafka.coordinator.group.GroupMetadataManager$$Lambda$1045/747223912.apply(Unknown Source)
	at kafka.server.DelayedProduce.onComplete(DelayedProduce.scala:124)
	at kafka.server.DelayedOperation.forceComplete(DelayedOperation.scala:68)
	at kafka.server.DelayedProduce.tryComplete(DelayedProduce.scala:106)
	at kafka.server.DelayedOperation.maybeTryComplete(DelayedOperation.scala:107)
	at kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:347)
	at kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:253)
	at kafka.server.ReplicaManager.tryCompleteDelayedProduce(ReplicaManager.scala:250)
	at kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:418)
	at kafka.cluster.Partition.appendRecordsToLeader(Partition.scala:500)
	at kafka.server.ReplicaManager.$anonfun$appendToLocalLog$2(ReplicaManager.scala:545)
	at kafka.server.ReplicaManager$$Lambda$909/475609331.apply(Unknown Source)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$Lambda$14/1859039536.apply(Unknown Source)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.server.ReplicaManager.appendToLocalLog(ReplicaManager.scala:531)
	at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:373)
	at kafka.coordinator.group.GroupMetadataManager.appendForGroup(GroupMetadataManager.scala:245)
	at kafka.coordinator.group.GroupMetadataManager.storeOffsets(GroupMetadataManager.scala:380)
	at kafka.coordinator.group.GroupCoordinator.doCommitOffsets(GroupCoordinator.scala:465)
	- locked &amp;lt;0x000000068cccb590&amp;gt; (a kafka.coordinator.group.GroupMetadata)
	at kafka.coordinator.group.GroupCoordinator.handleCommitOffsets(GroupCoordinator.scala:429)
	at kafka.server.KafkaApis.handleOffsetCommitRequest(KafkaApis.scala:361)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:105)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:66)
	at java.lang.Thread.run(Thread.java:748)
&quot;kafka-request-handler-4&quot;:
	at kafka.coordinator.group.GroupMetadataManager.putCacheCallback$2(GroupMetadataManager.scala:311)
	- waiting to lock &amp;lt;0x000000068cccb590&amp;gt; (a kafka.coordinator.group.GroupMetadata)
	at kafka.coordinator.group.GroupMetadataManager.$anonfun$storeOffsets$10(GroupMetadataManager.scala:380)
	at kafka.coordinator.group.GroupMetadataManager.$anonfun$storeOffsets$10$adapted(GroupMetadataManager.scala:380)
	at kafka.coordinator.group.GroupMetadataManager$$Lambda$1045/747223912.apply(Unknown Source)
	at kafka.server.DelayedProduce.onComplete(DelayedProduce.scala:124)
	at kafka.server.DelayedOperation.forceComplete(DelayedOperation.scala:68)
	at kafka.server.DelayedProduce.tryComplete(DelayedProduce.scala:106)
	at kafka.server.DelayedOperation.maybeTryComplete(DelayedOperation.scala:107)
	at kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:347)
	at kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:253)
	at kafka.server.ReplicaManager.tryCompleteDelayedProduce(ReplicaManager.scala:250)
	at kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:418)
	at kafka.cluster.Partition.appendRecordsToLeader(Partition.scala:500)
	at kafka.server.ReplicaManager.$anonfun$appendToLocalLog$2(ReplicaManager.scala:545)
	at kafka.server.ReplicaManager$$Lambda$909/475609331.apply(Unknown Source)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$Lambda$14/1859039536.apply(Unknown Source)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.server.ReplicaManager.appendToLocalLog(ReplicaManager.scala:531)
	at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:373)
	at kafka.coordinator.group.GroupMetadataManager.appendForGroup(GroupMetadataManager.scala:245)
	at kafka.coordinator.group.GroupMetadataManager.storeOffsets(GroupMetadataManager.scala:380)
	at kafka.coordinator.group.GroupCoordinator.doCommitOffsets(GroupCoordinator.scala:465)
	- locked &amp;lt;0x000000068cd2c420&amp;gt; (a kafka.coordinator.group.GroupMetadata)
	at kafka.coordinator.group.GroupCoordinator.handleCommitOffsets(GroupCoordinator.scala:429)
	at kafka.server.KafkaApis.handleOffsetCommitRequest(KafkaApis.scala:361)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:105)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:66)
	at java.lang.Thread.run(Thread.java:748)

Found 1 deadlock.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16212596" author="githubbot" created="Fri, 20 Oct 2017 12:43:04 +0000"  >&lt;p&gt;GitHub user rajinisivaram opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/4103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6042&quot; title=&quot;Kafka Request Handler deadlocks and brings down the cluster.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6042&quot;&gt;&lt;del&gt;KAFKA-6042&lt;/del&gt;&lt;/a&gt;: Avoid deadlock between two groups with delayed operations&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/rajinisivaram/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/rajinisivaram/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6042&quot; title=&quot;Kafka Request Handler deadlocks and brings down the cluster.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6042&quot;&gt;&lt;del&gt;KAFKA-6042&lt;/del&gt;&lt;/a&gt;-group-deadlock&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/4103.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4103.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4103&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 80039370b71bc0efd77d1f4cb46b855bfea45362&lt;br/&gt;
Author: Rajini Sivaram &amp;lt;rajinisivaram@googlemail.com&amp;gt;&lt;br/&gt;
Date:   2017-10-20T12:21:46Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6042&quot; title=&quot;Kafka Request Handler deadlocks and brings down the cluster.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6042&quot;&gt;&lt;del&gt;KAFKA-6042&lt;/del&gt;&lt;/a&gt;: Avoid deadlock between two groups with delayed operations&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16213848" author="brettrann" created="Sat, 21 Oct 2017 11:04:35 +0000"  >&lt;p&gt;We are experiencing these symptoms as well after upgrading to 0.11.0.1. 4 occurrences in 2-3 weeks seemingly randomly from about 20 AWS brokers in 2 clusters. FD use steadily climbs on the affected broker because it&apos;s not closing incoming network connection attempts.  Only once was it necessary to -9 though.3 times the restart was &amp;lt; minute, but the last one dragged on for 10m with some repairing.  Let me know if you want logs / additional info.  We haven&apos;t patched in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5970&quot; title=&quot;Deadlock due to locking of DelayedProduce and group&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5970&quot;&gt;&lt;del&gt;KAFKA-5970&lt;/del&gt;&lt;/a&gt; yet.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rsivaram&quot; class=&quot;user-hover&quot; rel=&quot;rsivaram&quot;&gt;rsivaram&lt;/a&gt; how confident are you feeling with that PR from yesterday?&lt;/p&gt;</comment>
                            <comment id="16214080" author="rsivaram" created="Sat, 21 Oct 2017 19:09:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brettrann&quot; class=&quot;user-hover&quot; rel=&quot;brettrann&quot;&gt;brettrann&lt;/a&gt; We are confident that the PR fixes the deadlock reported here. However, we think we should also add more tests to catch any deadlocks early. Several deadlocks in this area were detected only in production, so are really lacking in concurrent tests here. We are hoping to add more tests under &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6096&quot; title=&quot;Add concurrent tests to exercise all paths in group/transaction managers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6096&quot;&gt;&lt;del&gt;KAFKA-6096&lt;/del&gt;&lt;/a&gt; soon.&lt;/p&gt;</comment>
                            <comment id="16214168" author="guozhang" created="Sun, 22 Oct 2017 03:18:46 +0000"  >&lt;p&gt;Issue resolved by pull request 4103&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/4103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4103&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16214170" author="githubbot" created="Sun, 22 Oct 2017 03:20:29 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/4103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4103&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16215143" author="corlettb" created="Mon, 23 Oct 2017 13:35:24 +0000"  >&lt;p&gt;I&apos;ve deployed 1.0.0-SNAPSHOT to broker 25. I&apos;ll let you know how we get on.&lt;/p&gt;</comment>
                            <comment id="16224617" author="corlettb" created="Mon, 30 Oct 2017 10:18:44 +0000"  >&lt;p&gt;We had another issue on broker 125 again. Not the same this time. All other nodes in the cluster are on 0.11.0.1. Broker 125 is running a build from:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/commits/2a321941387c7739f2fbbbe592d017b703223ada&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/commits/2a321941387c7739f2fbbbe592d017b703223ada&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It ran out of heap space. We are currently running a heap of 5GB. This is the first time we&apos;ve seen an out of heap issue with kafka. I don&apos;t know if this is related to this issue. It might be that 1.0 requires more heap space or that running mixed versions uses more heap.&lt;/p&gt;

&lt;p&gt;This issue affected the entire cluster and messages rates didn&apos;t go back to normal until broker 125 was restarted.&lt;/p&gt;

&lt;p&gt;I can increase the heap size.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2017-10-28 16:19:31,061] ERROR [KafkaApi-125] Error when handling request {replica_id=-1,max_wait_time=500,min_bytes=1,max_bytes=52428800,topics=[{topic=XXXXXXXXX,partitions=[{partition=40,fetch_offset=153707886,max_bytes=1048576}]}]} (kafka.server.KafkaApis)
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.&amp;lt;init&amp;gt;(HeapByteBuffer.java:57)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
        at org.apache.kafka.common.record.AbstractRecords.downConvert(AbstractRecords.java:101)
        at org.apache.kafka.common.record.FileRecords.downConvert(FileRecords.java:253)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$4(KafkaApis.scala:520)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$4$adapted(KafkaApis.scala:518)
        at kafka.server.KafkaApis$$Lambda$837/843104331.apply(Unknown Source)
        at scala.Option.map(Option.scala:146)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$3(KafkaApis.scala:518)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$3$adapted(KafkaApis.scala:508)
        at kafka.server.KafkaApis$$Lambda$836/1538921035.apply(Unknown Source)
        at scala.Option.flatMap(Option.scala:171)
        at kafka.server.KafkaApis.convertedPartitionData$1(KafkaApis.scala:508)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$12(KafkaApis.scala:556)
        at kafka.server.KafkaApis$$Lambda$833/1032345356.apply(Unknown Source)
        at scala.collection.Iterator.foreach(Iterator.scala:929)
        at scala.collection.Iterator.foreach$(Iterator.scala:929)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)
        at scala.collection.IterableLike.foreach(IterableLike.scala:71)
        at scala.collection.IterableLike.foreach$(IterableLike.scala:70)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at kafka.server.KafkaApis.createResponse$2(KafkaApis.scala:555)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$14(KafkaApis.scala:569)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$14$adapted(KafkaApis.scala:569)
        at kafka.server.KafkaApis$$Lambda$844/44004770.apply(Unknown Source)
        at kafka.server.KafkaApis.$anonfun$sendResponseMaybeThrottle$1(KafkaApis.scala:2034)
        at kafka.server.KafkaApis$$Lambda$439/940799008.apply$mcVI$sp(Unknown Source)
        at kafka.server.ClientRequestQuotaManager.maybeRecordAndThrottle(ClientRequestQuotaManager.scala:52)
        at kafka.server.KafkaApis.sendResponseMaybeThrottle(KafkaApis.scala:2034)
        at kafka.server.KafkaApis.fetchResponseCallback$1(KafkaApis.scala:569)
        at kafka.server.KafkaApis.$anonfun$handleFetchRequest$15(KafkaApis.scala:588)
        at kafka.server.KafkaApis$$Lambda$843/1757998472.apply$mcVI$sp(Unknown Source)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2017-10-28T16:19:31.207+0100: 439215.093: [GC pause (G1 Evacuation Pause) (young), 0.0025596 secs]
   [Parallel Time: 1.3 ms, GC Workers: 4]
      [GC Worker Start (ms): Min: 439215093.0, Avg: 439215093.0, Max: 439215093.0, Diff: 0.0]
      [Ext Root Scanning (ms): Min: 0.6, Avg: 0.6, Max: 0.7, Diff: 0.1, Sum: 2.5]
      [Update RS (ms): Min: 0.0, Avg: 0.4, Max: 0.5, Diff: 0.5, Sum: 1.6]
         [Processed Buffers: Min: 1, Avg: 1.2, Max: 2, Diff: 1, Sum: 5]
      [Scan RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]
      [Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]
      [&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; Copy (ms): Min: 0.1, Avg: 0.1, Max: 0.1, Diff: 0.0, Sum: 0.3]
      [Termination (ms): Min: 0.0, Avg: 0.1, Max: 0.4, Diff: 0.4, Sum: 0.4]
         [Termination Attempts: Min: 1, Avg: 1.0, Max: 1, Diff: 0, Sum: 4]
      [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]
      [GC Worker Total (ms): Min: 1.2, Avg: 1.2, Max: 1.2, Diff: 0.0, Sum: 4.9]
      [GC Worker End (ms): Min: 439215094.2, Avg: 439215094.2, Max: 439215094.2, Diff: 0.0]
   [Code Root Fixup: 0.1 ms]
   [Code Root Purge: 0.0 ms]
   [Clear CT: 0.1 ms]
   [Other: 1.0 ms]
      [Choose CSet: 0.0 ms]
      [Ref Proc: 0.4 ms]
      [Ref Enq: 0.0 ms]
      [Redirty Cards: 0.1 ms]
      [Humongous Register: 0.4 ms]
      [Humongous Reclaim: 0.0 ms]
      [Free CSet: 0.0 ms]
   [Eden: 0.0B(256.0M)-&amp;gt;0.0B(256.0M) Survivors: 0.0B-&amp;gt;0.0B Heap: 4702.9M(5120.0M)-&amp;gt;4702.9M(5120.0M)]
 [Times: user=0.00 sys=0.00, real=0.00 secs]
2017-10-28T16:19:31.210+0100: 439215.096: [Full GC (Allocation Failure)  4702M-&amp;gt;4362M(5120M), 0.3454285 secs]
   [Eden: 0.0B(256.0M)-&amp;gt;0.0B(256.0M) Survivors: 0.0B-&amp;gt;0.0B Heap: 4702.9M(5120.0M)-&amp;gt;4362.7M(5120.0M)], [Metaspace: 38956K-&amp;gt;38955K(1087488K)]
 [Times: user=0.38 sys=0.13, real=0.35 secs]
2017-10-28T16:19:31.556+0100: 439215.441: [GC concurrent-mark-abort]
2017-10-28T16:19:31.714+0100: 439215.600: [GC pause (GCLocker Initiated GC) (young) (initial-mark), 0.0131186 secs]
   [Parallel Time: 9.7 ms, GC Workers: 4]
      [GC Worker Start (ms): Min: 439215600.8, Avg: 439215600.8, Max: 439215600.8, Diff: 0.0]
      [Ext Root Scanning (ms): Min: 1.1, Avg: 1.2, Max: 1.2, Diff: 0.1, Sum: 4.8]
      [Update RS (ms): Min: 2.4, Avg: 2.6, Max: 2.6, Diff: 0.2, Sum: 10.2]
         [Processed Buffers: Min: 10, Avg: 18.2, Max: 35, Diff: 25, Sum: 73]
      [Scan RS (ms): Min: 0.6, Avg: 0.7, Max: 0.8, Diff: 0.2, Sum: 2.8]
      [Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]
      [&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; Copy (ms): Min: 5.1, Avg: 5.1, Max: 5.1, Diff: 0.0, Sum: 20.3]
      [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]
         [Termination Attempts: Min: 1, Avg: 1.0, Max: 1, Diff: 0, Sum: 4]
      [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]
      [GC Worker Total (ms): Min: 9.5, Avg: 9.5, Max: 9.5, Diff: 0.0, Sum: 38.1]
      [GC Worker End (ms): Min: 439215610.3, Avg: 439215610.3, Max: 439215610.3, Diff: 0.0]
   [Code Root Fixup: 0.1 ms]
   [Code Root Purge: 0.1 ms]
   [Clear CT: 0.1 ms]
   [Other: 3.0 ms]
      [Choose CSet: 0.0 ms]
      [Ref Proc: 1.3 ms]
      [Ref Enq: 0.0 ms]
      [Redirty Cards: 0.1 ms]
      [Humongous Register: 0.8 ms]
      [Humongous Reclaim: 0.1 ms]
      [Free CSet: 0.1 ms]
   [Eden: 64.0M(256.0M)-&amp;gt;0.0B(244.0M) Survivors: 0.0B-&amp;gt;12.0M Heap: 4425.2M(5120.0M)-&amp;gt;4373.6M(5120.0M)]
 [Times: user=0.03 sys=0.01, real=0.01 secs]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16224750" author="ijuma" created="Mon, 30 Oct 2017 11:33:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=corlettb&quot; class=&quot;user-hover&quot; rel=&quot;corlettb&quot;&gt;corlettb&lt;/a&gt;, it&apos;s unrelated to this issue so best to discuss it in the mailing list. From the stacktrace, it can be seen that the message is being down converted (i.e. the broker is using a newer message format than is supported by an older client). This is generally to be avoided if possible and can lead to memory issues like the one you are seeing. The upgrade notes discuss this in detail (search for message.format.version). Hope this helps.&lt;/p&gt;</comment>
                            <comment id="16225144" author="corlettb" created="Mon, 30 Oct 2017 15:31:20 +0000"  >&lt;p&gt;It might be a memory leak. I&apos;ll keep an eye on it over the next few days.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12894769/12894769_heapusage.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="16225159" author="corlettb" created="Mon, 30 Oct 2017 15:35:14 +0000"  >&lt;p&gt;The new build was installed 14:10ish on the 23rd. It was restarted mid-day on the 29th.&lt;/p&gt;</comment>
                            <comment id="16225164" author="ijuma" created="Mon, 30 Oct 2017 15:37:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=corlettb&quot; class=&quot;user-hover&quot; rel=&quot;corlettb&quot;&gt;corlettb&lt;/a&gt;, can you please file a new JIRA ticket since it&apos;s not related to this one? This issue has been resolved and it&apos;s confusing to have two separate problems in the same issue. It also makes it hard to track. In the new JIRA, please mention if you have older consumers communicating with the broker.&lt;/p&gt;</comment>
                            <comment id="16243339" author="brettrann" created="Wed, 8 Nov 2017 03:37:56 +0000"  >&lt;p&gt;I have the same memory issue &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=corlettb&quot; class=&quot;user-hover&quot; rel=&quot;corlettb&quot;&gt;corlettb&lt;/a&gt; has described. I&apos;ve created a new ticket for it: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6185&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-6185&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16803600" author="astubbs" created="Thu, 28 Mar 2019 05:29:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;&#160;this is marked as both affects and fixed in 1.0.0 -&#160;this can&apos;t be accurate?&lt;/p&gt;</comment>
                            <comment id="16803630" author="ijuma" created="Thu, 28 Mar 2019 06:47:12 +0000"  >&lt;p&gt;Fixed.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13112363">KAFKA-6132</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12894769" name="heapusage.png" size="39689" author="corlettb" created="Mon, 30 Oct 2017 15:31:13 +0000"/>
                            <attachment id="12891227" name="thread_dump.txt.gz" size="8494" author="corlettb" created="Tue, 10 Oct 2017 09:18:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 33 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3l2m7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>