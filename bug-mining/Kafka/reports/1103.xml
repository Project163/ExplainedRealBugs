<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:54:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-3810] replication of internal topics should not be limited by replica.fetch.max.bytes</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-3810</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;From the kafka-dev mailing list discussion:&lt;br/&gt;
&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/kafka-dev/201605.mbox/%3CCAMQuQBZDdtAdhcgL6h4SmTgO83UQt4s72gc03B3VFghnME3FTA@mail.gmail.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;[DISCUSS] scalability limits in the coordinator&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There&apos;s a scalability limit on the new consumer / coordinator regarding the amount of group metadata we can fit into one message. This restricts a combination of consumer group size, topic subscription sizes, topic assignment sizes, and any remaining member metadata.&lt;/p&gt;

&lt;p&gt;Under more strenuous use cases like mirroring clusters with thousands of topics, this limitation can be reached even after applying gzip to the __consumer_offsets topic.&lt;/p&gt;

&lt;p&gt;Various options were proposed in the discussion:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Config change: reduce the number of consumers in the group. This isn&apos;t always a realistic answer in more strenuous use cases like MirrorMaker clusters or for auditing.&lt;/li&gt;
	&lt;li&gt;Config change: split the group into smaller groups which together will get full coverage of the topics. This gives each group member a smaller subscription.(ex: g1 has topics starting with a-m while g2 has topics starting with n-z). This would be operationally painful to manage.&lt;/li&gt;
	&lt;li&gt;Config change: split the topics among members of the group. Again this gives each group member a smaller subscription. This would also be operationally painful to manage.&lt;/li&gt;
	&lt;li&gt;Config change: bump up KafkaConfig.messageMaxBytes (a topic-level config) and KafkaConfig.replicaFetchMaxBytes (a broker-level config). Applying messageMaxBytes to just the __consumer_offsets topic seems relatively harmless, but bumping up the broker-level replicaFetchMaxBytes would probably need more attention.&lt;/li&gt;
	&lt;li&gt;Config change: try different compression codecs. Based on 2 minutes of googling, it seems like lz4 and snappy are faster than gzip but have worse compression, so this probably won&apos;t help.&lt;/li&gt;
	&lt;li&gt;Implementation change: support sending the regex over the wire instead of the fully expanded topic subscriptions. I think people said in the past that different languages have subtle differences in regex, so this doesn&apos;t play nicely with cross-language groups.&lt;/li&gt;
	&lt;li&gt;Implementation change: maybe we can reverse the mapping? Instead of mapping from member to subscriptions, we can map a subscription to a list of members.&lt;/li&gt;
	&lt;li&gt;Implementation change: maybe we can try to break apart the subscription and assignments from the same SyncGroupRequest into multiple records? They can still go to the same message set and get appended together. This way the limit become the segment size, which shouldn&apos;t be a problem. This can be tricky to get right because we&apos;re currently keying these messages on the group, so I think records from the same rebalance might accidentally compact one another, but my understanding of compaction isn&apos;t that great.&lt;/li&gt;
	&lt;li&gt;Implementation change: try to apply some tricks on the assignment serialization to make it smaller.&lt;/li&gt;
	&lt;li&gt;Config and Implementation change: bump up the __consumer_offsets topic messageMaxBytes and (from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;) fix how we deal with the case when a message is larger than the fetch size. Today, if the fetch size is smaller than the fetch size, the consumer will get stuck. Instead, we can simply return the full message if it&apos;s larger than the fetch size w/o requiring the consumer to manually adjust the fetch size.&lt;/li&gt;
	&lt;li&gt;Config and Implementation change: same as above but only apply the special fetch logic when fetching from internal topics&lt;/li&gt;
&lt;/ol&gt;
</description>
                <environment></environment>
        <key id="12977171">KAFKA-3810</key>
            <summary>replication of internal topics should not be limited by replica.fetch.max.bytes</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="onurkaraman">Onur Karaman</assignee>
                                    <reporter username="onurkaraman">Onur Karaman</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 Jun 2016 09:42:20 +0000</created>
                <updated>Mon, 8 Aug 2016 12:52:02 +0000</updated>
                            <resolved>Sun, 19 Jun 2016 18:42:28 +0000</resolved>
                                                    <fixVersion>0.10.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15322267" author="githubbot" created="Thu, 9 Jun 2016 09:51:43 +0000"  >&lt;p&gt;GitHub user onurkaraman opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1484&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1484&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3810&quot; title=&quot;replication of internal topics should not be limited by replica.fetch.max.bytes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3810&quot;&gt;&lt;del&gt;KAFKA-3810&lt;/del&gt;&lt;/a&gt;: replication of internal topics should not be limited by replica.fetch.max.bytes&lt;/p&gt;

&lt;p&gt;    From the kafka-dev mailing list discussion: &lt;span class=&quot;error&quot;&gt;&amp;#91;DISCUSS&amp;#93;&lt;/span&gt; scalability limits in the coordinator&lt;/p&gt;

&lt;p&gt;    There&apos;s a scalability limit on the new consumer / coordinator regarding the amount of group metadata we can fit into one message. This restricts a combination of consumer group size, topic subscription sizes, topic assignment sizes, and any remaining member metadata.&lt;/p&gt;

&lt;p&gt;    Under more strenuous use cases like mirroring clusters with thousands of topics, this limitation can be reached even after applying gzip to the __consumer_offsets topic.&lt;/p&gt;

&lt;p&gt;    Various options were proposed in the discussion:&lt;br/&gt;
    1. Config change: reduce the number of consumers in the group. This isn&apos;t always a realistic answer in more strenuous use cases like MirrorMaker clusters or for auditing.&lt;br/&gt;
    2. Config change: split the group into smaller groups which together will get full coverage of the topics. This gives each group member a smaller subscription.(ex: g1 has topics starting with a-m while g2 has topics starting with n-z). This would be operationally painful to manage.&lt;br/&gt;
    3. Config change: split the topics among members of the group. Again this gives each group member a smaller subscription. This would also be operationally painful to manage.&lt;br/&gt;
    4. Config change: bump up KafkaConfig.messageMaxBytes (a topic-level config) and KafkaConfig.replicaFetchMaxBytes (a broker-level config). Applying messageMaxBytes to just the __consumer_offsets topic seems relatively harmless, but bumping up the broker-level replicaFetchMaxBytes would probably need more attention.&lt;br/&gt;
    5. Config change: try different compression codecs. Based on 2 minutes of googling, it seems like lz4 and snappy are faster than gzip but have worse compression, so this probably won&apos;t help.&lt;br/&gt;
    6. Implementation change: support sending the regex over the wire instead of the fully expanded topic subscriptions. I think people said in the past that different languages have subtle differences in regex, so this doesn&apos;t play nicely with cross-language groups.&lt;br/&gt;
    7. Implementation change: maybe we can reverse the mapping? Instead of mapping from member to subscriptions, we can map a subscription to a list of members.&lt;br/&gt;
    8. Implementation change: maybe we can try to break apart the subscription and assignments from the same SyncGroupRequest into multiple records? They can still go to the same message set and get appended together. This way the limit become the segment size, which shouldn&apos;t be a problem. This can be tricky to get right because we&apos;re currently keying these messages on the group, so I think records from the same rebalance might accidentally compact one another, but my understanding of compaction isn&apos;t that great.&lt;br/&gt;
    9. Implementation change: try to apply some tricks on the assignment serialization to make it smaller.&lt;br/&gt;
    10. Config and Implementation change: bump up the __consumer_offsets topic messageMaxBytes and (from Jun Rao) fix how we deal with the case when a message is larger than the fetch size. Today, if the fetch size is smaller than the fetch size, the consumer will get stuck. Instead, we can simply return the full message if it&apos;s larger than the fetch size w/o requiring the consumer to manually adjust the fetch size.&lt;br/&gt;
    11. Config and Implementation change: same as above but only apply the special fetch logic when fetching from internal topics&lt;/p&gt;

&lt;p&gt;    This PR provides an implementation of option 11.&lt;/p&gt;

&lt;p&gt;    That being said, I&apos;m not very happy with this approach as it essentially doesn&apos;t honor the &quot;replica.fetch.max.bytes&quot; config. Better alternatives are definitely welcome!&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/onurkaraman/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/onurkaraman/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3810&quot; title=&quot;replication of internal topics should not be limited by replica.fetch.max.bytes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3810&quot;&gt;&lt;del&gt;KAFKA-3810&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1484.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1484.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1484&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit f8c1bb2d07df13b90ec338ddc4de08d58aab153d&lt;br/&gt;
Author: Onur Karaman &amp;lt;okaraman@linkedin.com&amp;gt;&lt;br/&gt;
Date:   2016-06-09T09:43:25Z&lt;/p&gt;

&lt;p&gt;    replication of internal topics should not be limited by replica.fetch.max.bytes&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15338678" author="ewencp" created="Sun, 19 Jun 2016 18:42:29 +0000"  >&lt;p&gt;Issue resolved by pull request 1484&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/1484&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1484&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15338679" author="githubbot" created="Sun, 19 Jun 2016 18:42:44 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1484&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1484&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15354011" author="junrao" created="Wed, 29 Jun 2016 00:58:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=onurkaraman&quot; class=&quot;user-hover&quot; rel=&quot;onurkaraman&quot;&gt;onurkaraman&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, thanks for the patch. I have a couple of follow up comments.&lt;/p&gt;

&lt;p&gt;1. I actually like &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt;&apos;s suggestion in the PR. Being able to give a full message back to the consumer when the fetch size is too small seems like a useful general feature, not just for internal topics. It&apos;s true that the consumer will get more bytes than requested. However, that&apos;s what&apos;s needed for the consumer to make progress.&lt;/p&gt;

&lt;p&gt;2. The patch always adjusts the fetch size based on Log.MaxMessageSize, regardless of wether they are large messages in the log or not. This increases the memory footprint in the consumer unnecessarily. A potential better implementation is to only return more than bytes than the fetch size when the first message to be fetched is larger than the fetch size. Since large messages should be rare, this improves the memory footprint in the consumer since in the common case, the fetch data is still bounded by the fetch size. We know the size of the first message to be fetched in FileMessageSet.searchFor(). We just need to pass that info back to the caller.&lt;/p&gt;

&lt;p&gt;So, I was thinking we can make the fix more general. If we find a message in the log larger than the fetch size, we just give the full message back. The fetch size in the consumer is than just for performance tuning. We allow the user to set the MaxMessageSize per topic. So, it&apos;s expected that the producer/consumer need at least that amount of memory. Do you think this makes sense? Do you want to do a follow up KIP to make the solution more general?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 20 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2z7on:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>