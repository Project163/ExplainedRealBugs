<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:18:33 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-8315] Historical join issues</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-8315</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The problem we are experiencing is that we cannot reliably perform simple joins over pre-populated kafka topics. This seems more apparent where one topic has records at less frequent record timestamp intervals that the other.&lt;br/&gt;
 An example of the issue is provided in this repository :&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/the4thamigo-uk/join-example&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;br/&gt;
The only way to increase the period of historically joined records is to increase the grace period for the join windows, and this has repercussions when you extend it to a large period e.g. 2 years of minute-by-minute records.&lt;/p&gt;



&lt;p&gt;Related slack conversations :&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1556799561287300&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1556799561287300&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1557733979453900&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1557733979453900&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;br/&gt;
 Research on this issue has gone through a few phases :&lt;/p&gt;

&lt;p&gt;1) This issue was initially thought to be due to the inability to set the retention period for a join window via &lt;tt&gt;Materialized: i.e.&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;The documentation says to use `Materialized` not `JoinWindows.until()` (&lt;a href=&quot;https://kafka.apache.org/22/javadoc/org/apache/kafka/streams/kstream/JoinWindows.html#until-long-&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://kafka.apache.org/22/javadoc/org/apache/kafka/streams/kstream/JoinWindows.html#until-long-&lt;/a&gt;), but there is no where to pass a `Materialized` instance to the join operation, only to the group operation is supported it seems.&lt;/p&gt;

&lt;p&gt;This was considered to be a problem with the documentation not with the API and is addressed in &lt;a href=&quot;https://github.com/apache/kafka/pull/6664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6664&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) We then found an apparent issue in the code which would affect the partition that is selected to deliver the next record to the join. This would only be a problem for data that is out-of-order, and join-example uses data that is in order of timestamp in both topics. So this fix is thought not to affect join-example.&lt;/p&gt;

&lt;p&gt;This was considered to be an issue and is being addressed in &lt;a href=&quot;https://github.com/apache/kafka/pull/6719&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6719&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;3) Further investigation using a crafted unit test seems to show that the partition-selection and ordering (PartitionGroup/RecordQueue) seems to work ok&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/commit/5121851491f2fd0471d8f3c49940175e23a26f1b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/commit/5121851491f2fd0471d8f3c49940175e23a26f1b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;4) the current assumption is that the issue is rooted in the way records are consumed from the topics :&lt;/p&gt;

&lt;p&gt;We have tried to set various options to suppress reads form the source topics but it doesnt seem to make any difference : &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/commit/c674977fd0fdc689152695065d9277abea6bef63&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/commit/c674977fd0fdc689152695065d9277abea6bef63&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13231248">KAFKA-8315</key>
            <summary>Historical join issues</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vvcephei">John Roesler</assignee>
                                    <reporter username="the4thamigo_uk">Andrew</reporter>
                        <labels>
                    </labels>
                <created>Thu, 2 May 2019 12:39:30 +0000</created>
                <updated>Thu, 8 Jul 2021 18:03:51 +0000</updated>
                            <resolved>Thu, 8 Jul 2021 18:03:51 +0000</resolved>
                                                    <fixVersion>3.0.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16831619" author="vvcephei" created="Thu, 2 May 2019 13:29:42 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt;, &lt;/p&gt;

&lt;p&gt;Sorry about that. It looks like the javadoc got messed up in this change: &lt;a href=&quot;https://github.com/apache/kafka/pull/5911/files#diff-35e3523474fa277a63e36a3fe9e22af8&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5911/files#diff-35e3523474fa277a63e36a3fe9e22af8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ll submit a PR to fix it.&lt;/p&gt;

&lt;p&gt;In summary, you should use the JoinWindows &quot;grace period&quot; instead of &quot;until&quot;. &lt;/p&gt;</comment>
                            <comment id="16831624" author="githubbot" created="Thu, 2 May 2019 13:36:02 +0000"  >&lt;p&gt;vvcephei commented on pull request #6664: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8315&quot; title=&quot;Historical join issues&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8315&quot;&gt;&lt;del&gt;KAFKA-8315&lt;/del&gt;&lt;/a&gt;: fix the JoinWindows retention deprecation doc&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/6664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6664&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Fix a javadoc mistake introduced in &lt;a href=&quot;https://github.com/apache/kafka/pull/5911/files#diff-35e3523474fa277a63e36a3fe9e22af8&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5911/files#diff-35e3523474fa277a63e36a3fe9e22af8&lt;/a&gt; .&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16831625" author="vvcephei" created="Thu, 2 May 2019 13:37:47 +0000"  >&lt;p&gt;Ok, submitted the PR &lt;a href=&quot;https://github.com/apache/kafka/pull/6664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6664&lt;/a&gt; . Sorry for the confusion!&lt;/p&gt;</comment>
                            <comment id="16831735" author="the4thamigo_uk" created="Thu, 2 May 2019 16:27:27 +0000"  >&lt;p&gt;Ok, thanks.&lt;/p&gt;

&lt;p&gt;However, I thought retention period was meant to be independent of grace period, like it is for grouped aggregations? I can see from the code though that the retention is explicitly set to the grace period.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &#160;&#160;&#160; @SuppressWarnings(&lt;span class=&quot;code-quote&quot;&gt;&quot;deprecation&quot;&lt;/span&gt;) &lt;span class=&quot;code-comment&quot;&gt;// continuing to support Windows#maintainMs/segmentInterval in fallback mode
&lt;/span&gt;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &amp;lt;K, V&amp;gt; StoreBuilder&amp;lt;WindowStore&amp;lt;K, V&amp;gt;&amp;gt; joinWindowStoreBuilder(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; joinName,
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JoinWindows windows,
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Serde&amp;lt;K&amp;gt; keySerde,
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Serde&amp;lt;V&amp;gt; valueSerde) {
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; Stores.windowStoreBuilder(
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Stores.persistentWindowStore(
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; joinName + &lt;span class=&quot;code-quote&quot;&gt;&quot;-store&quot;&lt;/span&gt;,
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Duration.ofMillis(windows.size() + windows.gracePeriodMs()),
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Duration.ofMillis(windows.size()),
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; ),
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; keySerde,
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; valueSerde
&#160;&#160;&#160;&#160;&#160;&#160;&#160; );
&#160;&#160;&#160; }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;so it looks like the grace period defines the retention period...&lt;/p&gt;</comment>
                            <comment id="16831747" author="the4thamigo_uk" created="Thu, 2 May 2019 16:39:24 +0000"  >&lt;p&gt;I should also mention my use case from the original slack community conversation :&lt;/p&gt;

&lt;p&gt;I am performing a large historical inner join (2 years) of two streams (using event time), followed by an aggregation.&lt;/p&gt;

&lt;p&gt;For the join, I have : 2 days of join window into the past only, with a grace period of 2 days ( I dont want to accept updates to the aggregation beyond this grace period).&lt;br/&gt;
 For the grouped aggregation I have : a tumbling window of 1 second and a grace of 4 days&lt;/p&gt;

&lt;p&gt;For the grouped aggregation if I also set the group retention using Materialized, I can see that this affects the retention period of the underlying KSTREAM-AGGREGATE-STATE-STORE topics. This seems to be independent of the grace period.&lt;/p&gt;

&lt;p&gt;However, using `until()` for the JoinWindows does not do the equvalent for the KSTREAM-JOINTHIS and KSTREAM-JOINOTHER topics, as I would have expected. These topics always have 120 hours retention period set on the topic.&lt;/p&gt;

&lt;p&gt;What I see is that I get no aggregation records other than for the most recent 120 hour period. So the vast majority of my 2 years fails to be joined/aggregated, and outputs nothing.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Andy Smith&#160;&#160; &lt;span class=&quot;error&quot;&gt;&amp;#91;1 minute ago&amp;#93;&lt;/span&gt;&lt;br/&gt;
So, it &lt;em&gt;is&lt;/em&gt; right, that I should be able to set the retention period independently of the grace period?&lt;/p&gt;

&lt;p&gt;e.g. the use case is :&lt;br/&gt;
Perform a windowed aggregation, say, with a window of 1 day, and a grace of 1 day, over the whole of 1 year.&lt;br/&gt;
I dont want to update my windows if data arrives more than 1 day after each window expires as stream-time progresses (hence the grace period is 1 day)&lt;br/&gt;
I want to ensure I output joined and aggregated values for all days over the entire year&lt;/p&gt;

&lt;p&gt;In order to do this I should set :&lt;/p&gt;

&lt;p&gt;join window = 1 day&lt;br/&gt;
join grace = 1 day&lt;br/&gt;
join retention = 1 year&lt;br/&gt;
group window = 1 day&lt;br/&gt;
group grace = 1 day&lt;br/&gt;
group retention = 1 year&lt;/p&gt;

&lt;p&gt;This right? (edited)&lt;/p&gt;

&lt;p&gt;Matthias J Sax&#160;&#160; &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;lt; 1 minute ago&amp;#93;&lt;/span&gt;&lt;br/&gt;
Yes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16831927" author="vvcephei" created="Thu, 2 May 2019 20:19:10 +0000"  >&lt;p&gt;Interesting. Thanks for the context. For some reason, the slack link doesn&apos;t take me to the thread.&lt;/p&gt;

&lt;p&gt;Your understanding is almost spot-on.&lt;/p&gt;

&lt;p&gt;The grace period defines how long after the window ends it will accept late-arriving records and update the results. The retention time defines how long we keep the state of the window in storage. Clearly, the retention time must be at least big enough to support the updates that may happen during the grace period, but it could be much larger, to support Interactive Queries even after the window is closed to updates.&lt;/p&gt;

&lt;p&gt;Join windows are a little bit different, because they are not queriable. Therefore, there is no reason to have any retention beyond the grace period. This is also why there&apos;s no `Materialized` parameter. The state for the join is purely bookeeping, not a &quot;materialized view&quot; in the data processing sense. Since you mention the apparent similarity with grouped aggregations, the fact that JoinWindows shares a class hierarchy with Windows, and the fact that it uses a &quot;normal&quot; WindowStore internally, was mostly for implementation convenience. It&apos;s actually a little semantically abusive if you really get into it, and I&apos;ve heard a few times that people would like to break joins out and clean the whole situation up.&lt;/p&gt;

&lt;p&gt;Things get really complicated when we need to compute defaults, though. The concepts of &quot;grace&quot; and &quot;retention&quot; used to be coupled into just &quot;retention&quot; (aka &quot;until&quot; aka &quot;maintainMs&quot;), and the default was set to 24h. So, if we pick any default grace period shorter than 24h, then some apps may start to drop late data that didn&apos;t before. Also, the &quot;retention&quot; configuration in Windows is only deprecated, not removed, so someone may set a retention time on the deprecated methods, but not a grace period, and we also need to do the &quot;right thing&quot; in that case. This is just in the way of justifying why the code is so complicated. Hopefully, we can drop the deprecated methods soonish and clean the whole thing up.&lt;/p&gt;

&lt;p&gt;So, back to your actual behavior, you &lt;b&gt;should&lt;/b&gt; see that the window stores for join windows use `Duration.ofMillis(windows.size() + windows.gracePeriodMs())`, as you pointed out above. The deprecated `until` should be ignored. It&apos;s possible the topic retention doesn&apos;t get updated when you change your configs, which would be a bug.&lt;/p&gt;

&lt;p&gt;One thing I didn&apos;t understand is the arithmetic from your conversation. I&apos;ll take a shot and maybe you can set me straight...&lt;br/&gt;
You want to join 2 years of historical data.&lt;br/&gt;
For each join candidate, you only want to look back 2 days, so you set the join window to size=2 days.&lt;br/&gt;
You want to emit updated join results in the case of time-disordered records, but not indefinitely. Specifically, you only want to emit updated results up to 2 days after the fact, so you set the grace period to 2 days as well.&lt;br/&gt;
With these configurations, you should see the retention time on the topic set to at least 4 days. 120 hours is 5 days, so this seems about right to me (there might be some fudge factor, I&apos;m not sure).&lt;/p&gt;

&lt;p&gt;I guess the big problem is that your data fails to join. I&apos;d start with identifying some pair of records that you think &lt;b&gt;should&lt;/b&gt; join, and then identify why they don&apos;t (could be a join window too small, or it could be the grace period too small).&lt;/p&gt;</comment>
                            <comment id="16832325" author="the4thamigo_uk" created="Fri, 3 May 2019 07:54:24 +0000"  >&lt;p&gt;The fudge factor is the {windowstore.changelog.additional.retention.ms} which is set to 24h by default, so that seems to add up to 5 days (120 hours).&lt;/p&gt;

&lt;p&gt;You understand my use case well. The choice of limiting the grace was to reduce the performance overhead during the large historical processing.&lt;/p&gt;

&lt;p&gt;I definitely have lots of data beyond this that should join and the joins and aggregation within the latest 120 hours seem to be correct and complete. This seems to be just related to retention. I will experiment with increasing {windowstore.changelog.additional.retention.ms} to see if it brings in more data.&lt;/p&gt;

&lt;p&gt;One oddity of our left stream is that it contains records in batches from different devices. Each batch is about 1000 records and contiguous within the stream. Within a batch the records are in increasing timestamp order. Subsequent batches from different devices will be within 1-2 days of the previous batch (we don&apos;t have, say, a batch for 1/1/2019 followed by a batch for 4/1/2019 or a batch for 24/12/2019 for example. The right stream is single records not batches with similar date ordering (i.e. subsequent records should be within 1-2 days of each other.&lt;/p&gt;

&lt;p&gt;My understanding is that the windows are closed when streamtime - windowstart &amp;gt; windowsize + grace. So as stream time increases as newer batches arrive, joins/aggregations should continue, until a batch arrives after windowsize + grace. However, we are not seeing this.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Correction&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I think I meant : &lt;/p&gt;

&lt;p&gt;streamtime - windowend &amp;gt; grace&lt;/p&gt;</comment>
                            <comment id="16832543" author="the4thamigo_uk" created="Fri, 3 May 2019 14:37:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; Ive been struggling with this today.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I have logging on every join, and I can see that joins do not occur before the 6 day period, except in a few cases (see below)&lt;/li&gt;
	&lt;li&gt;Ive checked the data and I can definitely see that the vast majority of records should have joins throughout the whole period.&lt;/li&gt;
	&lt;li&gt;I have added `windowstore.changelog.additional.retention.ms` so that the auto-generated JOINTHIS/JOINOTHER intermediate topics now have a retention.ms of 840 hours.&lt;/li&gt;
	&lt;li&gt;I have removed my previous call to `until()` and only set the join window size to 2 days, and increased the join grace to 3 days.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;None of the above seems to make any difference.&lt;/p&gt;

&lt;p&gt;What I have observed is that I do get a few joins before the 6 day window for a single partition (13) and this partition is the first to complete by far, as it has the fewest records.&lt;/p&gt;

&lt;p&gt;Both topics are partitioned using murmur2 into 20 partitions (Ive checked we have the same keys in the corresponding partitions of the left and right topics). We are running 4 instances of the streams application, and we do not explicitly set the `num.stream.threads`.&lt;/p&gt;

&lt;p&gt;My understanding is that a task is created for each partition (i.e. 20 tasks) and the work for these tasks is distributed out to the stream threads in our 4 streams application instances (processes). All instances share exactly the same configuration and, in particular, the same application.id. (My assumption is that stream time is per-task (i.e. per partition). Is this correct? Is there &lt;em&gt;any&lt;/em&gt; possibility that the stream time of partition 13 is somehow shared with any of the other tasks, such that windows might be closed before the join-able data is read on the other partitions.&lt;/p&gt;

&lt;p&gt;Id like to understand some more about how stream-time increases. I imagine (probably naively) that within a task, stream time increases as the latest timestamp read from a partition, and that both left and right streams have their own stream time. I also assume that during a join, the left stream is read, up until just after the current right stream-time, then the right stream is read up until the latest left stream-time, so that data is pulled off both streams to minimize the difference in times between the latest records read off the topics. Is this near the mark?&lt;/p&gt;

&lt;p&gt;I will next try the join using a very large grace period to see if it makes a difference. One other thing I might try is to cap my end time using a stream filter to see if I manage to join to earlier records.&lt;/p&gt;

&lt;p&gt;P.S. we are using the following versions&lt;br/&gt;
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &amp;lt;kafka.version&amp;gt;2.2.0&amp;lt;/kafka.version&amp;gt;&lt;br/&gt;
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &amp;lt;confluent.version&amp;gt;5.2.1&amp;lt;/confluent.version&amp;gt;&lt;/p&gt;</comment>
                            <comment id="16832577" author="the4thamigo_uk" created="Fri, 3 May 2019 15:25:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; Update I ran using grace = 20D and I see records from around 23D prior to the latest data in the topics. So it seems that we definitely can join the records, but that the grace period is the critical factor in doing this for a historical ingest.&lt;/p&gt;

&lt;p&gt;So, for a 2 year ingestion do I have to set my grace to 2 years? Seems a bit strange, and maybe sounds a bit inefficient?&lt;br/&gt;
&#160;&lt;/p&gt;</comment>
                            <comment id="16833088" author="the4thamigo_uk" created="Sat, 4 May 2019 15:48:37 +0000"  >&lt;p&gt;Ive attached my code:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I have logging in the ValueJoiner so I can see that joins do not occur before the period mentioned&lt;/li&gt;
	&lt;li&gt;The ValueJoiner is very simply setting the entire left and right GenericRecord objects to properties in a parent GenericRecord e.g.&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; GenericRecord record = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; GenericData.Record(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.schema);
record.put(IS_UPDATED_FIELD, isUpdated);
record.put(LEFT_VALUE_FIELD, leftRecord);
record.put(RIGHT_VALUE_FIELD, rightRecord);
&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; record; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
	&lt;li&gt;I have maybe an unusual transform on line 33, which ensures that the parent GenericRecord emitted from the ValueJoiner always has the timestamp of the left record. This is important for the aggregation phase, but this occurs after the join, so shouldnt affect it.&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
@Override &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; KeyValue transform(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; key, GenericRecord val) {
  &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; ts = tsExtractor.extractTimestamp(joinedRecordFactory.leftValue(val));
  context.forward(key, val, To.all().withTimestamp(ts));
  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
} 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16833161" author="the4thamigo_uk" created="Sat, 4 May 2019 21:25:29 +0000"  >&lt;p&gt;I have run a join by removing the timestamp transform and the aggregation and I still get the same behaviour. i.e. the following topology :&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; Topology joinStreamStream(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JoinerProperties props) {

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JoinedRecordFactory joinedRecordFactory = JoinedRecordFactory.create(props.leftTopic().getSchema(), props.rightTopic().getSchema());
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; FieldMapper leftFieldMapper = FieldMapper.create(props.leftTopic().getFields());
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; FieldMapper rightFieldMapper = FieldMapper.create(props.rightTopic().getFields());
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JoinValueMapper joinValueMapper = JoinValueMapper.create(joinedRecordFactory, leftFieldMapper, rightFieldMapper, props.joinSchema());

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-comment&quot;&gt;// extractors
&lt;/span&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; TimestampExtractor leftTsExtractor = AvroTimestampExtractor.create(props.leftTopic().getTimestampField());
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; TimestampExtractor rightTsExtractor = AvroTimestampExtractor.create(props.rightTopic().getTimestampField());

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; StreamsBuilder builder = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamsBuilder();
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Consumed&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; leftConsumed = Consumed.with(leftTsExtractor);
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; leftStream = AvroMinMaxTimestampTransformer.wrap(
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; builder.stream(props.leftTopic().getName(), leftConsumed),
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; props.minStreamTimestamp(), props.maxStreamTimestamp());

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Consumed&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; rightConsumed = Consumed.with(rightTsExtractor);
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; rightStream = AvroMinMaxTimestampTransformer.wrap(
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; builder.stream(props.rightTopic().getName(), rightConsumed),
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; props.minStreamTimestamp(), props.maxStreamTimestamp());

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-comment&quot;&gt;// setup the join
&lt;/span&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; ValueJoiner&amp;lt;GenericRecord, GenericRecord, GenericRecord&amp;gt; joiner = AvroFieldsValueJoiner.create(joinedRecordFactory);
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JoinWindows joinWindow = JoinWindows.of(Duration.ZERO).after(props.joinWindowAfterSize()).before(props.joinWindowBeforeSize()).grace(props.joinWindowGrace());

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; KStreamKStreamJoinFunction join = props.joinType() == JoinerProperties.JoinType.INNER ? leftStream::join : leftStream::leftJoin;
&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; joinStream = join.execute(rightStream, joiner, joinWindow);

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-comment&quot;&gt;// write the change-log stream to the topic
&lt;/span&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160; joinStream
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; .mapValues(joinValueMapper::apply)
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; .to(props.joinTopic());

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; builder.build();
&#160;&#160;&#160; }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16835784" author="the4thamigo_uk" created="Wed, 8 May 2019 17:53:28 +0000"  >&lt;p&gt;After a lot of investigation, we think this issue is down to the fact that we have a left stream with minute-by-minute data and a right topic with daily data.&lt;/p&gt;

&lt;p&gt;It is not clear what logic controls the rate at which records are read from left and right streams, but we believe that the right topic is being read at a rate such that that it quickly gets too far ahead of the left stream (in terms of event-time), as there are far fewer records, and therefore the right stream windows are being expired before the left stream data has been read.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; What controls the rate that records are read from the left and right streams? Is there any guarantee that the timestamps for the records in the left and right streams are kept more-or-less in line with the records from the right stream?&lt;/p&gt;

&lt;p&gt;If not, is there any way we can somehow delay the right stream?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Thanks for your help above.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16835832" author="vvcephei" created="Wed, 8 May 2019 19:17:03 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt;, Sorry for leaving you hanging a bit. I&apos;m glad your investigation is progressing.&lt;/p&gt;

&lt;p&gt;Last question first: Streams should choose to consume from the left or right based on which one has the lower timestamp in the next record, so I would not expect one side to &quot;run ahead&quot; of the other. There&apos;s one caveat, that when one side is being produced more slowly, Streams won&apos;t just wait indefinitely for the next data, but instead just process the side that does have data. This is controled by the &quot;max idle ms&quot; config, but since you&apos;re processing historically, this shouldn&apos;t be your problem. Still might be worth a look.&lt;/p&gt;

&lt;p&gt;Maybe for debugging purposes, you can print out the key, value, and timestamp for each of the sides as well as in the joiner, so you can identify which side is triggering the join, and evaluate whether or not it&apos;s correctly time-ordered.&lt;/p&gt;

&lt;p&gt;If it is in fact running ahead on one side, despite what it should be doing, this would explain why you see better results with a larger grace period. To confirm, the grace period should only matter up to the maximum time skew in your stream. So, as you said, if you have two producers that each produce a full 24 hours of data, sequentially, then you should see stream time advance when the first producer writes its data, and then &quot;freeze&quot; while the second producer writes its (out-of-order) data. Thus, you&apos;ll want to set the grace period to keep old windows around for at least 24 hours, since you know you have to wait for that second producer&apos;s data.&lt;/p&gt;

&lt;p&gt;Finally, to answer your earlier questions, yes, each task is handling just one partition of both input topics (the same partition on the left and right). Stream Time is independently maintained for each task/partition, and it is computed simply as the highest timestamp yet observed for that partition. If you want to look at it in detail, it&apos;s tracked in org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore . Actually, you can set that class&apos;s logger to DEBUG mode and it&apos;ll print out every time it skips a record that is outside of retention.&lt;/p&gt;

&lt;p&gt;Minor point, you should not need to mess with the retention of the changelog topic. Streams sets this appropriately to preserve the same data as the store, but this is only apparent when restoring the store. The actual results of the join are served out of the state store, so only the state store&apos;s retention matters. This is what you&apos;re setting with the grace period.&lt;/p&gt;

&lt;p&gt;I hope this helps!&lt;br/&gt;
-John&lt;/p&gt;</comment>
                            <comment id="16836278" author="the4thamigo_uk" created="Thu, 9 May 2019 10:43:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; Thanks again for your help and advice. &lt;/p&gt;

&lt;p&gt;Ok, so it should work as I had previously hoped then. So maybe our working hypothesis is incorrect? As you suggest, I am currently re-running a test using the following code to determine the lag between the left and right topics.&lt;/p&gt;

&lt;p&gt;The reason we think it might be due to the right stream getting ahead is that this also helps to explain why we manage to perform some initial joins at the start of the ingestion period for about two months (while the streams are presumed to be in line), then nothing for most of the middle period, then a few days of joins at the end.&lt;/p&gt;

&lt;p&gt;As for the retention period, I think I understand that now from all your explanations. The problem here is not retention, it looks like it is something to do with either the specifics of our dataset or the way in which the streams are read.&lt;/p&gt;

&lt;p&gt;We delved into the code, and found the way that the RocksDB code works and think we understand it now. What I didnt manage to find is where the code is for the logic you describe in your first paragraph (&apos;Streams should choose to ...etc&apos;).&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; Topology joinTestStreamStream(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JoinerProperties props) {

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; StreamsBuilder builder = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamsBuilder();

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; TransformerSupplier streamLogger = () -&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Transformer&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;,GenericRecord, KeyValue&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;,GenericRecord&amp;gt;&amp;gt;() {
 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; ProcessorContext context;

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; @Override
 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void init(ProcessorContext context)

{ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.context = context; &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; @Override
 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; KeyValue&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; transform(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; key, GenericRecord value)

{ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; log.info(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;.format(&lt;span class=&quot;code-quote&quot;&gt;&quot;reading : topic=%s, partition=%d, timestamp=%d, offset=%d, key=%s&quot;&lt;/span&gt;, context.topic(), context.partition(), context.timestamp(), context.offset(), key)); &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; KeyValue&amp;lt;&amp;gt;(key,value); &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; @Override
 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void close()

{ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }

&#160;&#160;&#160;&#160;&#160;&#160;&#160; };

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; leftStream = builder.stream(props.leftTopic().getName()).transform(streamLogger);
 &#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; rightStream = builder.stream(props.rightTopic().getName()).transform(streamLogger);

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-comment&quot;&gt;// setup the join
&lt;/span&gt; &#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JoinWindows joinWindow = JoinWindows.of(Duration.ZERO).before(Duration.parse(&lt;span class=&quot;code-quote&quot;&gt;&quot;P2D&quot;&lt;/span&gt;)).grace(Duration.parse(&lt;span class=&quot;code-quote&quot;&gt;&quot;P7D&quot;&lt;/span&gt;));

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, GenericRecord&amp;gt; joinStream = leftStream.join(rightStream,
 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; (l, r) -&amp;gt;

{ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; log.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;joining: &quot;&lt;/span&gt; + l + &lt;span class=&quot;code-quote&quot;&gt;&quot;, &quot;&lt;/span&gt; + r); &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;; &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }

, joinWindow);

&#160;&#160;&#160;&#160;&#160;&#160;&#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; builder.build();
 &#160;&#160;&#160; }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16836606" author="ableegoldman" created="Thu, 9 May 2019 18:23:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt; The code you&apos;re looking for describing the logic of choosing the next record to process is in org.apache.kafka.streams.processor.internals.PartitionGroup, which contains a priority queue &quot;nonEmptyQueuesByTime&quot; that serves up the partition with the lowest timestamp when polled (unless max.idle.ms has passed as checked in StreamTask#isProcessable)&lt;/p&gt;</comment>
                            <comment id="16836735" author="the4thamigo_uk" created="Thu, 9 May 2019 21:58:15 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have today tried to isolate the issue we are facing into a simple demo app. I have two streams of data x and y, x is high frequency (&lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/data/x.dat&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/data/x.dat&lt;/a&gt;) and y is low frequency &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/data/y.dat&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/data/y.dat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Both topics are preloaded into kafka here &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/create_topics.sh#L3&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/create_topics.sh#L3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I set the join window sizes, and the grace here &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/launch.sh#L9&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/launch.sh#L9&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is logging as records are read off the streams via a transformer here &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/src/main/java/the4thamigouk/kafka/streams/joinexample/JoinerStreamProcessor.java#L54&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/src/main/java/the4thamigouk/kafka/streams/joinexample/JoinerStreamProcessor.java#L54&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is logging on the join here &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/src/main/java/the4thamigouk/kafka/streams/joinexample/JoinerStreamProcessor.java#L93&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/src/main/java/the4thamigouk/kafka/streams/joinexample/JoinerStreamProcessor.java#L93&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What I see are joins only from here &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/example.log#L1553&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/example.log#L1553&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, I notice that all the x records are read first, then all the y records &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/example.log#L451&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/example.log#L451&lt;/a&gt; and &lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/blob/master/example.log#L1452&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/blob/master/example.log#L1452&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Am I doing something wrong here?&lt;/p&gt;</comment>
                            <comment id="16837086" author="the4thamigo_uk" created="Fri, 10 May 2019 09:25:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt;Can I sanity check my understanding here. From what I can tell, the &lt;tt&gt;PartitionGroup.nonEmptyQueuesByTime&lt;/tt&gt; orders the &lt;tt&gt;RecordQueue&lt;/tt&gt; instances by the &lt;tt&gt;RecordQueue.partitionTime&lt;/tt&gt;. The &lt;tt&gt;RecordQueue.partitionTime&lt;/tt&gt; is the most recent timestamp that has been read into the &lt;tt&gt;RecordQueue.fifoQueue&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;If I have a &lt;tt&gt;RecordQueue&lt;/tt&gt; A with two records with timestamps 1 and 4, and another &lt;tt&gt;RecordQueue&lt;/tt&gt; B with two records with timestamps 2 and 3, then A will have &lt;tt&gt;RecordQueue.partitionTime = 4&lt;/tt&gt; and B will have &lt;tt&gt;RecordQueue.partitionTime = 3&lt;/tt&gt;. So B will be selected by &lt;tt&gt;PartitionGroup.nonEmptyQueuesByTime.poll()&lt;/tt&gt; and the next record will be 2, not 1. Is that right?&lt;/p&gt;

&lt;p&gt;If, on the other hand we ordered by the earliest time in each &lt;tt&gt;RecordQueue.fifoQueue&lt;/tt&gt;, then A would be selected, and record 1 would be read first.&lt;/p&gt;

&lt;p&gt;Taking this thought further, in my test data set, if I assume that the &lt;tt&gt;fifoQueue&lt;/tt&gt; are populated in chunks of 10, then initially the left stream would have &lt;tt&gt;partitionTime = 10&lt;/tt&gt; (see +++) and the right stream &lt;tt&gt;partitionTime = 100&lt;/tt&gt;. So, the left stream would be selected first, until all records are consumed up to time 100 in the left stream, then the right stream records would be consumed.&lt;/p&gt;

&lt;p&gt;In this case, wouldnt many of the left join windows expire before the first record is read from the &lt;tt&gt;RecordQueue&lt;/tt&gt; of the right stream?&lt;/p&gt;

&lt;p&gt;+++ for convenience I am quoting these times as second offsets from the start time 1902580000000ms&lt;/p&gt;</comment>
                            <comment id="16837303" author="vvcephei" created="Fri, 10 May 2019 13:47:09 +0000"  >&lt;p&gt;Ah, yes. My apologies. I was mistaken about the PartitionGroup logic. I think you&apos;re right, and actually, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; has filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8347&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8347&lt;/a&gt;, so I guess she agrees, too.&lt;/p&gt;

&lt;p&gt;I think it&apos;s actually a pretty straightforward change, and I actually don&apos;t think it needs a KIP either. Should we try to get this change in for the 15 May feature freeze for 2.3?&lt;/p&gt;

&lt;p&gt;Since you&apos;ve taken the time to get familiar with the code, do you want to send a PR, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt;? Offhand, I think we can just redefine RecordQueue.timestamp() to be the head timestamp instead of the high water-mark time. And, of course write/update the relevant tests.&lt;/p&gt;</comment>
                            <comment id="16837765" author="ableegoldman" created="Sat, 11 May 2019 05:49:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt; After I directed you to check out RecordQueue I went back to look over it and filed the ticket John linked: you&apos;re right, it actually was going by partition time rather than by the timestamp of the head record. I&apos;ve opened a simple PR with the fix [here|&lt;a href=&quot;https://github.com/apache/kafka/pull/6719&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6719&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;That said, I&apos;m not sure this actually affects your use case. If all the data is in order, partition time should be the same as the head record&apos;s timestamp, so this should only come into play when processing out of order data. In your example above, partition A would have streamtime = 1 when first choosing the next record, as it will not have seen the record with timestamp 4 yet.&lt;/p&gt;</comment>
                            <comment id="16838012" author="the4thamigo_uk" created="Sun, 12 May 2019 06:48:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; Right, I see what you mean, this loop only goes until the head is found &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java#L158.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java#L158.&lt;/a&gt; We do have out of order data in our real data streams, however, it looks like you are right that it shouldn&apos;t affect my &lt;tt&gt;join-example&lt;/tt&gt; demo, which reproduces the issue with only ordered data. &lt;/p&gt;

&lt;p&gt;Any further ideas on why the &lt;tt&gt;join-example&lt;/tt&gt; doesnt work? If it is a different bug, shall we open a new ticket as this current one is not really relevant anymore?&lt;/p&gt;</comment>
                            <comment id="16838471" author="the4thamigo_uk" created="Mon, 13 May 2019 11:19:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; I was just looking at the unit tests for PartitionGroup and noticed that the comment refers to timestamps, but the ConsumerGroup constructor that is used passes the value in the offset parameter :&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java#L92&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java#L92&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is this correct?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Update: I see now that this is the MockTimestampExtractor that uses the offset as the timestamp... &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/test/MockTimestampExtractor.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/test/MockTimestampExtractor.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16838550" author="the4thamigo_uk" created="Mon, 13 May 2019 13:46:53 +0000"  >&lt;p&gt;This test appears to work ok &lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/commit/560432e7daae217a2255161787dd55ca56845794&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/commit/560432e7daae217a2255161787dd55ca56845794.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Differences with join-example are :&lt;br/&gt;
1) It is using TopologyTestDriver, which means data is not pre-populated in topics.&lt;br/&gt;
2) Im not using timestamp extractors&lt;/p&gt;</comment>
                            <comment id="16838565" author="the4thamigo_uk" created="Mon, 13 May 2019 14:15:45 +0000"  >&lt;p&gt;This is the test enhanced to use timestamp extraction, and it works&#160; ;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/commit/5121851491f2fd0471d8f3c49940175e23a26f1b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/commit/5121851491f2fd0471d8f3c49940175e23a26f1b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;So, it would seem that the issue is how the data is read when the data is already fully populated in the source topics. Seems like, as we discussed previously, it simply reads all the left records first, then the right records. How can we throttle the ingestion of the records to avoid this?&lt;/p&gt;</comment>
                            <comment id="16838720" author="vvcephei" created="Mon, 13 May 2019 17:21:30 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Unfortunately, the TopologyTestDriver is going to be insufficient for exercising the behavior you want, since it processes events synchronously as soon as you call `pipeInput`, but the problem you&apos;re having appears to be with the logic that chooses records polled from Kafka (which only KafkaStreams does).&lt;/p&gt;

&lt;p&gt;I&apos;d suggest, as the fastest way to try and nail this down, actually to pull the Kafka project down (since we have set up integration tests that actually do use the brokers and run a &quot;real&quot; KafkaStreams) and modify one of the join integration tests to reproduce your use case.&lt;/p&gt;

&lt;p&gt;This still sounds like a bug to me, even though it might not be the one that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; reported.&lt;/p&gt;

&lt;p&gt;Regarding the ticket, it&apos;d be better not to split the history of this investigation, so I recommend just editing the title and description of the ticket, instead of making a new ticket.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
-John&lt;/p&gt;</comment>
                            <comment id="16838783" author="the4thamigo_uk" created="Mon, 13 May 2019 18:25:24 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt;, I am running the unit tests in the kafka project. I confess I havent worked out the integration tests yet, but I saw the python scripts for this. I agree the ordering from the RecordQueue/PartitionGroup looks sound, and that it is something weird with how data is pushed into the RecordQueues from the source topics. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; any clues on this would be greatly appreciated.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Thanks again for the responses. Hopefully, we will get to the bottom of this.&lt;/p&gt;</comment>
                            <comment id="16838878" author="vvcephei" created="Mon, 13 May 2019 21:26:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt;, The python tests are something different.&lt;/p&gt;

&lt;p&gt;I was just talking about the Java classes that are called like &quot;WhateverWhateverIntegrationTest&quot;. You should be able to run those right from the IDE. If you want to run all the streams integration tests, it&apos;s `./gradlew clean :streams:test`. It will take 7 minutes-ish.&lt;/p&gt;</comment>
                            <comment id="16839703" author="the4thamigo_uk" created="Tue, 14 May 2019 18:41:16 +0000"  >&lt;p&gt;I constructed a super-ugly workaround for my &lt;tt&gt;join-example&lt;/tt&gt; demo app. It adds a transformer onto each of the left and right streams, and they refer to each other&apos;s streamTime to decide whether to forward the messages. Not the &apos;right&apos; solution, but I might be able to fix this up to serve as a workaround. Need to ensure that the right transformers pair up depending on their assigned partition, and maybe use a state store. Its a hack but it might just work for now.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/pull/1/files&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/pull/1/files&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16839711" author="the4thamigo_uk" created="Tue, 14 May 2019 19:01:27 +0000"  >&lt;p&gt;I constructed a super-ugly workaround for my &lt;tt&gt;join-example&lt;/tt&gt; demo app. It adds a transformer onto each of the left and right streams, and they refer to each other&apos;s streamTime to decide whether to forward the messages. Not the &apos;right&apos; solution, but I might be able to fix this up to serve as a workaround. Need to ensure that the right transformers pair up depending on their assigned partition, and maybe use a state store. Its a hack but it might just work for now.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/the4thamigo-uk/join-example/pull/1/files&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/join-example/pull/1/files&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16840541" author="the4thamigo_uk" created="Wed, 15 May 2019 16:21:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; I have constructed an integration test that I think reproduces the issue. Altering the grace period to a large value (e.g. 1 hour) makes the test pass :&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/blob/debugging/streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java#L100&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/blob/debugging/streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java#L100&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16841213" author="the4thamigo_uk" created="Thu, 16 May 2019 10:53:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; Using the integration test I think I now understand what is going on. &lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The key bit of code is here : &lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L332&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L332&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;What appears to be a happening is this :&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;1) Since the topics are already full of data, the left topic has sufficient data (1000 records) in order to trigger leaving this loop &lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/blob/debugging/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L485.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/blob/debugging/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L485.&lt;/a&gt; So, no right records are fetched.&lt;/p&gt;

&lt;p&gt;2) All the fetched left stream records are added to PartitionGroup, and PartitionGroup.allBuffered = false, since the right stream RecordQueue is still empty&lt;/p&gt;

&lt;p&gt;3) The code drops into here &lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/blob/debugging/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L330,&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/blob/debugging/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L330,&lt;/a&gt; since maxTaskIdleMs == 0 !&lt;br/&gt;
4) The 1000 left stream records are processed (thereby almost immediately expiring their join windows !&lt;br/&gt;
5) The right stream records are fetched and processed, but there are no left stream join windows to join with until the latest records in the left stream for which the windows have not expired.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;And the workaround/fix, is a change of configuration setting : &lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/commit/189aa764aef06643a8a3c30b2aee3c4a29b82ae6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/commit/189aa764aef06643a8a3c30b2aee3c4a29b82ae6&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perhaps this value should default to non-zero to enable historical joins by default?&lt;/p&gt;</comment>
                            <comment id="16841219" author="mjsax" created="Thu, 16 May 2019 11:06:35 +0000"  >&lt;p&gt;John mentioned the max task idle config earlier in this discussion:&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8315?focusedCommentId=16835832&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16835832&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8315?focusedCommentId=16835832&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16835832&lt;/a&gt;&lt;br/&gt;
&#160;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Perhaps this value should default to non-zero to enable historical joins by default?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;br/&gt;
&#160;It&apos;s not easy to change the default, because of backward compatibility.&lt;/p&gt;</comment>
                            <comment id="16841273" author="the4thamigo_uk" created="Thu, 16 May 2019 12:21:40 +0000"  >&lt;p&gt;Ha!, you are absolutely right. Maybe I didn&apos;t understand the significance of this, or I think probably what happened was I misinterpreted it i..e I thought I needed to set it to zero, not non-zero. Either way we had a lot of other challenges last week which side-tracked me a lot from this investigation. Anyway, the deep-dive into kafka internals was well and truly worth it.&lt;/p&gt;

&lt;p&gt;Two remaining points :&lt;/p&gt;

&lt;p&gt;1) will &lt;a href=&quot;https://github.com/apache/kafka/pull/6719&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6719&lt;/a&gt; make it into the next release do you think, as we have a lot of out-of-order data as well in our production feeds?&lt;br/&gt;
2) It doesn&apos;t seem obvious, that in order to do a historical join that the developer has to set a value called &apos;max task idle milliseconds&apos;. If data is in the topic, why should I have to have an &apos;idle time&apos;? Is there anything that can be done to make this more intuitive?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16841378" author="mjsax" created="Thu, 16 May 2019 14:02:52 +0000"  >&lt;p&gt;1) I think, 6719 should make it into 2.3 release.&lt;/p&gt;

&lt;p&gt;2) It&apos;s actually a known issue:&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7458&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-7458&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16841449" author="the4thamigo_uk" created="Thu, 16 May 2019 15:18:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vvcephei&quot; class=&quot;user-hover&quot; rel=&quot;vvcephei&quot;&gt;vvcephei&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; Thanks for your help on this, I think we are good now... Just doing a final test here, but looks promising&lt;/p&gt;</comment>
                            <comment id="16841571" author="vvcephei" created="Thu, 16 May 2019 17:42:39 +0000"  >&lt;p&gt;Oh, man. I&apos;m sorry that I didn&apos;t convey the significance of that configuration enough. I just assumed you tried it. I feel bad for all the time you spent.&lt;/p&gt;

&lt;p&gt;It&apos;s called &quot;max idle time&quot; and defaults to 0 because Streams actually has to resist processing data that it already has (aka, it has to idle) in order to wait for extra data. Streams would never idle before we added the config, and idling could have a severe impact on throughput for high-volume applications, so we basically can&apos;t default greater than 0.&lt;/p&gt;

&lt;p&gt;Still, it seems like in a replay case like yours, it should at least wait until it polls all inputs at least once before starting, so I agree there&apos;s room for improvement here. I&apos;m wondering if we should just take more control over the consumer and explicitly poll each topic, instead of just assigning them all and letting the consumer/broker decide which ones to give data back from first.&lt;/p&gt;

&lt;p&gt;Since you dug in deep enough to figure this out, do you have any ideas, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=the4thamigo_uk&quot; class=&quot;user-hover&quot; rel=&quot;the4thamigo_uk&quot;&gt;the4thamigo_uk&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="16841611" author="the4thamigo_uk" created="Thu, 16 May 2019 18:15:43 +0000"  >&lt;p&gt;Well, Im not sure that just fixing the start is sufficient. I suspect that, if you run over a long enough period, through sheer bad luck you will encounter an issue where a queue is drained, and yet also fails to fetch any records on the next fetch cycle, in this case you will end up with the same situation I think i.e. an empty queue that will still be processed because of the zero max idle time. This can happen on either side of the join, so if one side travels too far ahead of the other you would lose the join windows.&lt;/p&gt;

&lt;p&gt;As I was rummaging through the code, I also wondered whether a closer relationship with the consumer would make things a bit easier. Then you can control better how many records you grab to push into the queues. For example, I&apos;m not sure, but I kinda have a suspicion that the queues can grow quite large, as the limit is on the fetch, and the queue cant put back-pressure on the fetch. But, if you had control over the consumer, and you wanted to limit the size of the queue, then you would know if one side is full then you don&apos;t need to not fetch more records on that side etc. You would also potentially be able to use streamTime to make such decisions I suppose.&lt;/p&gt;

&lt;p&gt;I also wondered whether you could allow the left(right) stream to process (in the absence of right(left records), provided the left(right) streamTime stays less than the right(left) streamTime -right(left) grace - windowSize. i.e. not to go beyond the start of the first currently active window on the right(left) stream.&lt;/p&gt;

&lt;p&gt;Lastly, also it may be worth at least waiting for a fetch that returned 0 records from a topic, before moving on. I think currently, simple latency or the fetch limit, can trigger this, no?&lt;/p&gt;</comment>
                            <comment id="16841635" author="mjsax" created="Thu, 16 May 2019 18:42:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;as the limit is on the fetch, and the queue cant put back-pressure on the fetch.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Queues have a configurable size and partitions are pause if a queue fills up. Hence, setting `max.task.idle` large enough should be sufficient imho.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I also wondered whether you could allow the left(right) stream to process (in the absence of right(left records), provided the left(right) streamTime stays less than the right(left) streamTime -right(left) grace - windowSize. i.e. not to go beyond the start of the first currently active window on the right(left) stream.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not easily possible. At runtime we don&apos;t know anything about the semantics of the program. We don&apos;t know that a join is executed and also retention time is not a runtime concept.&lt;/p&gt;</comment>
                            <comment id="16841640" author="the4thamigo_uk" created="Thu, 16 May 2019 18:54:22 +0000"  >&lt;p&gt;Also, I had a thought whether you could modify &lt;a href=&quot;https://github.com/the4thamigo-uk/kafka/blob/debugging/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L485.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/the4thamigo-uk/kafka/blob/debugging/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L485.&lt;/a&gt; so that at least some records are fetched for a topic, providing there is a completed fetch for it. I think it drains the first one then the next, but you could imagine round robin, or timestamp based extraction from more than one completed fetch, in order to balance the delivery across streams?&lt;/p&gt;

&lt;p&gt;All, just vague hand-wavey thoughts, I appreciate that there is much more to it than I comprehend... :oD&lt;/p&gt;</comment>
                            <comment id="16841710" author="mjsax" created="Thu, 16 May 2019 20:51:02 +0000"  >&lt;p&gt;Don&apos;t think that should be required? The way Kafka Streams&apos; record queue work, should actually work around this behavior. If a partition is paused, `poll()` won&apos;t return any data for this partition, even if the consumer has buffered data.&lt;/p&gt;</comment>
                            <comment id="16842566" author="vvcephei" created="Fri, 17 May 2019 20:55:59 +0000"  >&lt;p&gt;The existing behavior of the consumer is effectively to round-robin the inputs. If you&apos;re subscribed to A,B, and C, and you request 20 records, and it gets 15 records from A and 5 from B, then the next time around, it should give you maybe 10 more from B and then 10 from C.&lt;/p&gt;

&lt;p&gt;I actually think the problem might just be on startup (but would need to verify), since we have no visibility into which partitions have been polled at all. After startup, the Consumer behavior in addition to the existing pause logic should take care of preferring to poll partitions that are empty. If a partition is actually empty (we are caught up), then this is what the max.idle.time is for. But this ticket seems different, since we never even tried to poll all the inputs before starting work on just one side of the join.... Or maybe I&apos;m not thinking about it clearly.&lt;/p&gt;

&lt;p&gt;The point is, as a functional requirement, it seems like historical joins should function properly even with a zero idle time.&lt;/p&gt;</comment>
                            <comment id="16842568" author="mjsax" created="Fri, 17 May 2019 20:58:32 +0000"  >&lt;p&gt;Agreed. As mentioned above:&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8315?focusedCommentId=16841378&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16841378&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8315?focusedCommentId=16841378&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16841378&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&apos;s a known issue tracked as &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7458&quot; title=&quot;Avoid enforced processing during bootstrap phase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7458&quot;&gt;&lt;del&gt;KAFKA-7458&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16845088" author="githubbot" created="Tue, 21 May 2019 17:54:29 +0000"  >&lt;p&gt;guozhangwang commented on pull request #6664: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8315&quot; title=&quot;Historical join issues&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8315&quot;&gt;&lt;del&gt;KAFKA-8315&lt;/del&gt;&lt;/a&gt;: fix the JoinWindows retention deprecation doc&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/6664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6664&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="17244330" author="vvcephei" created="Fri, 4 Dec 2020 22:44:54 +0000"  >&lt;p&gt;Hello, all, I have just proposed KIP-695, which I think would resolve this issue. Please let me know what you think in the mailing list discussion thread!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/x/JSXZCQ&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/x/JSXZCQ&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17377546" author="vvcephei" created="Thu, 8 Jul 2021 18:03:28 +0000"  >&lt;p&gt;Hello again, all, this issue should be resolved in 3.0, via &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-10091&quot; title=&quot;Improve task idling&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-10091&quot;&gt;&lt;del&gt;KAFKA-10091&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13237523">KAFKA-8478</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13232557">KAFKA-8347</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13309268">KAFKA-10091</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12967836" name="code.java" size="3536" author="the4thamigo_uk" created="Sat, 4 May 2019 15:40:34 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 18 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z02c0g:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>