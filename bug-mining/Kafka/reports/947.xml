<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:52:51 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-3159] Kafka consumer 0.9.0.0  client poll is very CPU intensive under certain conditions</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-3159</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We are using the new kafka consumer with the following config (as logged by kafka)&lt;/p&gt;

&lt;p&gt;metric.reporters = []&lt;/p&gt;

&lt;p&gt;        metadata.max.age.ms = 300000&lt;/p&gt;

&lt;p&gt;        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer&lt;/p&gt;

&lt;p&gt;        group.id = myGroup.id&lt;/p&gt;

&lt;p&gt;        partition.assignment.strategy = &lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.kafka.clients.consumer.RangeAssignor&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;        reconnect.backoff.ms = 50&lt;/p&gt;

&lt;p&gt;        sasl.kerberos.ticket.renew.window.factor = 0.8&lt;/p&gt;

&lt;p&gt;        max.partition.fetch.bytes = 2097152&lt;/p&gt;

&lt;p&gt;        bootstrap.servers = &lt;span class=&quot;error&quot;&gt;&amp;#91;myBrokerList&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;        retry.backoff.ms = 100&lt;/p&gt;

&lt;p&gt;        sasl.kerberos.kinit.cmd = /usr/bin/kinit&lt;/p&gt;

&lt;p&gt;        sasl.kerberos.service.name = null&lt;/p&gt;

&lt;p&gt;        sasl.kerberos.ticket.renew.jitter = 0.05&lt;/p&gt;

&lt;p&gt;        ssl.keystore.type = JKS&lt;/p&gt;

&lt;p&gt;        ssl.trustmanager.algorithm = PKIX&lt;/p&gt;

&lt;p&gt;        enable.auto.commit = false&lt;/p&gt;

&lt;p&gt;        ssl.key.password = null&lt;/p&gt;

&lt;p&gt;        fetch.max.wait.ms = 1000&lt;/p&gt;

&lt;p&gt;        sasl.kerberos.min.time.before.relogin = 60000&lt;/p&gt;

&lt;p&gt;        connections.max.idle.ms = 540000&lt;/p&gt;

&lt;p&gt;        ssl.truststore.password = null&lt;/p&gt;

&lt;p&gt;        session.timeout.ms = 30000&lt;/p&gt;

&lt;p&gt;        metrics.num.samples = 2&lt;/p&gt;

&lt;p&gt;        client.id = &lt;/p&gt;

&lt;p&gt;        ssl.endpoint.identification.algorithm = null&lt;/p&gt;

&lt;p&gt;        key.deserializer = class sf.kafka.VoidDeserializer&lt;/p&gt;

&lt;p&gt;        ssl.protocol = TLS&lt;/p&gt;

&lt;p&gt;        check.crcs = true&lt;/p&gt;

&lt;p&gt;        request.timeout.ms = 40000&lt;/p&gt;

&lt;p&gt;        ssl.provider = null&lt;/p&gt;

&lt;p&gt;        ssl.enabled.protocols = &lt;span class=&quot;error&quot;&gt;&amp;#91;TLSv1.2, TLSv1.1, TLSv1&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;        ssl.keystore.location = null&lt;/p&gt;

&lt;p&gt;        heartbeat.interval.ms = 3000&lt;/p&gt;

&lt;p&gt;        auto.commit.interval.ms = 5000&lt;/p&gt;

&lt;p&gt;        receive.buffer.bytes = 32768&lt;/p&gt;

&lt;p&gt;        ssl.cipher.suites = null&lt;/p&gt;

&lt;p&gt;        ssl.truststore.type = JKS&lt;/p&gt;

&lt;p&gt;        security.protocol = PLAINTEXT&lt;/p&gt;

&lt;p&gt;        ssl.truststore.location = null&lt;/p&gt;

&lt;p&gt;        ssl.keystore.password = null&lt;/p&gt;

&lt;p&gt;        ssl.keymanager.algorithm = SunX509&lt;/p&gt;

&lt;p&gt;        metrics.sample.window.ms = 30000&lt;/p&gt;

&lt;p&gt;        fetch.min.bytes = 512&lt;/p&gt;

&lt;p&gt;        send.buffer.bytes = 131072&lt;/p&gt;

&lt;p&gt;        auto.offset.reset = earliest&lt;/p&gt;


&lt;p&gt;We use the consumer.assign() feature to assign a list of partitions and call poll in a loop.  We have the following setup:&lt;/p&gt;

&lt;p&gt;1. The messages have no key and we use the byte array deserializer to get byte arrays from the config.&lt;/p&gt;

&lt;p&gt;2. The messages themselves are on an average about 75 bytes. We get this number by dividing the Kafka broker bytes-in metric by the messages-in metric.&lt;/p&gt;

&lt;p&gt;3. Each consumer is assigned about 64 partitions of the same topic spread across three brokers.&lt;/p&gt;

&lt;p&gt;4. We get very few messages per second maybe around 1-2 messages across all partitions on a client right now.&lt;/p&gt;

&lt;p&gt;5. We have no compression on the topic.&lt;/p&gt;

&lt;p&gt;Our run loop looks something like this&lt;/p&gt;

&lt;p&gt;while (isRunning()) {&lt;/p&gt;

&lt;p&gt;ConsumerRecords&amp;lt;Void, byte[]&amp;gt; records = null;&lt;br/&gt;
        try &lt;/p&gt;
{
            // Here timeout is about 10 seconds, so it is pretty big.
            records = consumer.poll(timeout);
        }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
           // This never hits for us
            logger.error(&quot;Exception polling Kafka &quot;, e);
            records = null;
        }

&lt;p&gt;        if (records != null) {&lt;br/&gt;
            for (ConsumerRecord&amp;lt;Void, byte[]&amp;gt; record : records) &lt;/p&gt;
{
               // The handler puts the byte array on a very fast ring buffer so it barely takes any time.
                handler.handleMessage(ByteBuffer.wrap(record.value()));
            }
&lt;p&gt;        }&lt;br/&gt;
}&lt;/p&gt;


&lt;p&gt;With this setup our performance has taken a horrendous hit as soon as we started this one thread that just polls Kafka in a loop.&lt;/p&gt;

&lt;p&gt;I profiled the application using Java Mission Control and have a few insights.&lt;/p&gt;

&lt;p&gt;1. There doesn&apos;t seem to be a single hotspot. The consumer just ends up using a lot of CPU for handing such a low number of messages. Our process was using 16% CPU before we added a single consumer and it went to 25% and above after. That&apos;s an increase of over 50% from a single consumer getting a single digit number of small messages per second. Here is an attachment of the cpu usage breakdown in the consumer (the namespace is different because we shade the kafka jar before using it) - &lt;a href=&quot;http://imgur.com/BxWs9Q0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://imgur.com/BxWs9Q0&lt;/a&gt; So 20.54% of our entire process CPU is used on polling these 64 partitions (across 3 brokers) with single digit number of 70-80 byte odd messages.  We&apos;ve used bigger timeouts (100 seconds odd) and that doesn&apos;t seem to make much of a difference either.&lt;/p&gt;

&lt;p&gt;2. It also seems like Kafka throws a ton of EOFExceptions. I am not sure whether this is expected but this seems like it would completely kill performance. Here is the exception tab of Java mission control. &lt;a href=&quot;http://imgur.com/X3KSn37&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://imgur.com/X3KSn37&lt;/a&gt; That is 1.8 mn exceptions over a period of 3 minutes which is about 10 thousand exceptions per second! The exception stack trace shows that it originates from the poll call. I don&apos;t understand how it can throw so many exceptions given I call poll it with a timeout of 10 seconds and get a single digit number of messages per second. The exception seems to be thrown from here: &lt;a href=&quot;https://github.com/apache/kafka/blob/0.9.0/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java#L236&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/0.9.0/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java#L236&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3. The single thread seems to allocate a lot too. The single thread is responsible for 17.87% of our entire JVM allocation rate. During other runs it has gone up to 20% of our entire JVM allocation rate. Most of what it allocates seems to be those same EOFExceptions. Here is a chart showing the single thread&apos;s allocation proportion: &lt;a href=&quot;http://imgur.com/GNUJQsz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://imgur.com/GNUJQsz&lt;/a&gt; Here is a chart that shows a breakdown of the allocations: &lt;a href=&quot;http://imgur.com/YjCXljE&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://imgur.com/YjCXljE&lt;/a&gt; About 20% of the allocations are for the EOFExceptions. But given that the 20% of the allocations (exceptions) is around 10k/second, the thread itself is allocating about 50k objects/second which seems excessive given how we are getting very few messages.&lt;/p&gt;

&lt;p&gt;As a comparison, we also run a wrapper over the old SimpleConsumer that gets a lot more data (30 thousand 70 byte messages/sec on a different topic) and it is able to handle that load without much trouble. At this moment we are completely puzzled by this performance. At least some part of that seems to be due to the crazy volumes of exceptions but the CPU profiling breakdown seems to suggest that there are plenty of other causes including the Fetcher.initFetches() call and the ConsumerNetworkClient.poll() call. Note: Our messages seem to all be making through. We haven&apos;t measured the end to end latency. The exceptions are caught by Kafka&apos;s stack and never bubble up to us.&lt;/p&gt;</description>
                <environment>Linux, Oracle JVM 8.</environment>
        <key id="12934536">KAFKA-3159</key>
            <summary>Kafka consumer 0.9.0.0  client poll is very CPU intensive under certain conditions</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="rajiv@signalfx.com">Rajiv Kurian</reporter>
                        <labels>
                    </labels>
                <created>Wed, 27 Jan 2016 18:17:12 +0000</created>
                <updated>Wed, 24 May 2017 09:27:38 +0000</updated>
                            <resolved>Tue, 9 Feb 2016 22:58:53 +0000</resolved>
                                    <version>0.9.0.0</version>
                                    <fixVersion>0.9.0.1</fixVersion>
                                    <component>clients</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="15122467" author="hachikuji" created="Thu, 28 Jan 2016 22:43:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rajiv%40signalfx.com&quot; class=&quot;user-hover&quot; rel=&quot;rajiv@signalfx.com&quot;&gt;rajiv@signalfx.com&lt;/a&gt; Looks like these EOFExceptions are avoidable by checking whether the underlying buffer has data remaining. However, I&apos;m still a bit puzzled by the number reported. In the current implementation, I would expect to see at most one EOFException for each partition in every fetch response. If there are about 64 partitions and &quot;fetch.max.wait.ms&quot; is 1000, then we should see about 64 exceptions raised each second (when there is not much data to fetch). Perhaps most of the exceptions occurred during a load spike or maybe when it was catching up initially?&lt;/p&gt;</comment>
                            <comment id="15122507" author="hachikuji" created="Thu, 28 Jan 2016 23:19:13 +0000"  >&lt;p&gt;However, I do see significantly more exceptions when the topic has been compressed (I tried snappy locally). Are you sure that the topic is not compressed?&lt;/p&gt;</comment>
                            <comment id="15122594" author="rajiv@signalfx.com" created="Fri, 29 Jan 2016 00:20:09 +0000"  >&lt;p&gt;I don&apos;t enable compression on the topic. However the producer (0.8.2) might just decide to compress all the same. How can I tell?&lt;/p&gt;</comment>
                            <comment id="15122607" author="hachikuji" created="Fri, 29 Jan 2016 00:29:42 +0000"  >&lt;p&gt;You can use the DumpLogSegmentsTool. The output should show you if the messages are compressed or not. Sample usage below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;bin/kafka-run-&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;sh kafka.tools.DumpLogSegments --files /tmp/kafka/foo-0/00000000000000000000.log
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15122614" author="rajiv@signalfx.com" created="Fri, 29 Jan 2016 00:31:56 +0000"  >&lt;p&gt;Actually I managed to dig through the logs and find the producer config logs from the producer:&lt;br/&gt;
2016-01-26T02:53:31.497Z INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;PathChildrenCache-0                &amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;s.o.a.k.c.producer.ProducerConfig   &amp;#93;&lt;/span&gt; {}: ProducerConfig values: &lt;br/&gt;
        compression.type = none&lt;br/&gt;
        metric.reporters = []&lt;br/&gt;
        metadata.max.age.ms = 300000&lt;br/&gt;
        metadata.fetch.timeout.ms = 60000&lt;br/&gt;
        acks = 1&lt;br/&gt;
        batch.size = 10240&lt;br/&gt;
        reconnect.backoff.ms = 10&lt;br/&gt;
        bootstrap.servers = &lt;span class=&quot;error&quot;&gt;&amp;#91;our-kafka-brokers&amp;#93;&lt;/span&gt;&lt;br/&gt;
        receive.buffer.bytes = 32768&lt;br/&gt;
        retry.backoff.ms = 100&lt;br/&gt;
        buffer.memory = 2097152&lt;br/&gt;
        timeout.ms = 30000&lt;br/&gt;
        key.serializer = class sf.disco.kafka.VoidSerializer&lt;br/&gt;
        retries = 0&lt;br/&gt;
        max.request.size = 1048576&lt;br/&gt;
        block.on.buffer.full = false&lt;br/&gt;
        value.serializer = class sf.org.apache.kafka.common.serialization.ByteArraySerializer&lt;br/&gt;
        metrics.sample.window.ms = 30000&lt;br/&gt;
        send.buffer.bytes = 131072&lt;br/&gt;
        max.in.flight.requests.per.connection = 5&lt;br/&gt;
        metrics.num.samples = 2&lt;br/&gt;
        linger.ms = 100&lt;br/&gt;
        client.id = &lt;/p&gt;

&lt;p&gt;I don&apos;t explicitly set compression and it appears from the config that no compression was set.&lt;/p&gt;</comment>
                            <comment id="15122640" author="hachikuji" created="Fri, 29 Jan 2016 00:47:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rajiv%40signalfx.com&quot; class=&quot;user-hover&quot; rel=&quot;rajiv@signalfx.com&quot;&gt;rajiv@signalfx.com&lt;/a&gt; Here&apos;s a patch I&apos;ve been messing with: &lt;a href=&quot;https://github.com/hachikuji/kafka/commit/69485add2119d523a1b3c93373eb20923a98320e&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/hachikuji/kafka/commit/69485add2119d523a1b3c93373eb20923a98320e&lt;/a&gt;. Any chance you could give it a try? You only need to update the client. This should address the cause of the EOFs reported above, but it&apos;s tough to know for sure since I haven&apos;t seen anywhere near the number of exceptions reported.&lt;/p&gt;</comment>
                            <comment id="15122659" author="rajiv@signalfx.com" created="Fri, 29 Jan 2016 00:57:08 +0000"  >&lt;p&gt;Thanks Jason. I can try to do that early next week. Have a lot of deadlines this week so might not get the chance to get on it.&lt;/p&gt;</comment>
                            <comment id="15126831" author="rajiv@signalfx.com" created="Mon, 1 Feb 2016 19:18:06 +0000"  >&lt;p&gt;CPU break down of the patched client. Some notes:&lt;br/&gt;
1. 40.58% of the process&apos; CPU profile is on these poll calls which are done with a timeout of 5 seconds.&lt;br/&gt;
2. A lot of cpu is spent on hash map operations.&lt;br/&gt;
3. The rest of the cpu seems to be spent mostly in NetworkClient.poll().&lt;/p&gt;</comment>
                            <comment id="15126867" author="rajiv@signalfx.com" created="Mon, 1 Feb 2016 19:29:32 +0000"  >&lt;p&gt;Memory profile of the patched client. Notes:&lt;/p&gt;

&lt;p&gt;1.A lot of it is in clients.consumer.internals.Fetcher.createFetchRequests(). Again quite a bit of hash map allocations.&lt;br/&gt;
2. The majority of the rest of allocations seems to be in NetworkClient.poll().&lt;/p&gt;</comment>
                            <comment id="15126877" author="rajiv@signalfx.com" created="Mon, 1 Feb 2016 19:34:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; I tried your patch. The Exceptions are now gone, but the CPU has remained high (25% + from 17% before the new client was added). I have attached the CPU breakdown and the allocation break down screen shots and comments.&lt;br/&gt;
Some notes:&lt;br/&gt;
1. The exceptions seem to be gone completely. The overall CPU has gone down to 25% odd from the 27% before. So it has gotten a bit better. But the percentage of CPU used by the Kafka part of the code has gone up to 40.58% of the total used by my process. Most of the CPU is now spent on hash map code. Again I don&apos;t understand why there is so much CPU being used to get single digit 60 byte messages per second (64 partitions striped across 3 brokers).&lt;/p&gt;

&lt;p&gt;2. The allocations % has believe it or not gone up even more at about 31.26% of my entire processes allocation. Again it is baffling that it allocates so much to get so few messages. The total sum allocations from the TLAB in the 5 minute period has gone up to 14.05 GB from the 6.95 GB done by the client which threw a lot of exceptions. Again that seems to be a staggering amount of allocations for something that does 1 message odd a second.&lt;/p&gt;

&lt;p&gt;My poll timings are done with a 5 second timeout which seems high enough.&lt;/p&gt;

&lt;p&gt;Let me know if I can do more profiling or provide other info.&lt;/p&gt;</comment>
                            <comment id="15129287" author="hachikuji" created="Tue, 2 Feb 2016 23:08:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rajiv%40signalfx.com&quot; class=&quot;user-hover&quot; rel=&quot;rajiv@signalfx.com&quot;&gt;rajiv@signalfx.com&lt;/a&gt; I think we&apos;ll probably need some more information to investigate this further. It appears that the EOFExceptions were only symptoms of some other problem which is causing high CPU utilization. It might be helpful to see some of the logs so we know what the consumer was doing during that time. Can you turn on TRACE level logging and attach a sample to this ticket?&lt;/p&gt;</comment>
                            <comment id="15134562" author="rajiv@signalfx.com" created="Fri, 5 Feb 2016 17:49:11 +0000"  >&lt;p&gt;I think I&apos;ve found the underlying issue (might not be the only one in play). It turns out that when I don&apos;t have any messages in the log, the Kafka broker sends back a reply with no messages immediately instead of respecting the fetch_max_wait_ms or the fetch_min_bytes. The EOFExceptions were probably just raised from parsing empty message sets. I can reproduce this consistently. Steps:&lt;br/&gt;
1. Create a topic with a small retention say 5 minutes or wait for said topic to have all its logs cleaned.&lt;br/&gt;
2. Start consuming on the topic without any messages being sent to the topic.&lt;br/&gt;
3. Observe that Kafka sends back an empty reply to every fetch request almost immediately. This can be observed with tcp-dump, or monitoring the networking-in/out or ngrep etc. I also verified it by writing my own client and observing that my requests get immediate replies when the log is empty.&lt;br/&gt;
4. As soon as you start sending messages to the topic, the problem goes away.&lt;/p&gt;

&lt;p&gt;We&apos;ve actually hit this problem in the past (seeing massive number of network traffic) when we were subscribed to a single topic that gets no messages. We didn&apos;t know the underlying issue then but I am pretty sure it is this problem.&lt;/p&gt;

&lt;p&gt;This is a problem if any consumer is sending fetch requests to at least one broker that is a leader for the partitions being queried but has no messages retained in its log. In real life it can also be a problem. Here are a few use cases:&lt;br/&gt;
i) Metadata like topics that are always consumed but very rarely ever written to. We&apos;ve run into this in the past like I said.&lt;br/&gt;
ii) During feature development one can switch on the consumers, and put the producers behind a feature flag. This was the problem we ran into. The consumer code went ahead before the producer code was integrated/switched on and we had to roll back because of the massive regression.&lt;/p&gt;

&lt;p&gt;Moreover it goes against all intuition that doing fetch requests against an empty topic-partition should not be more expensive than actually getting data.&lt;/p&gt;</comment>
                            <comment id="15134768" author="granthenke" created="Fri, 5 Feb 2016 19:19:48 +0000"  >&lt;p&gt;Could this be related to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3003&quot; title=&quot;The fetch.wait.max.ms is not honored when new log segment rolled for low volume topics.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3003&quot;&gt;&lt;del&gt;KAFKA-3003&lt;/del&gt;&lt;/a&gt;? A fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3003&quot; title=&quot;The fetch.wait.max.ms is not honored when new log segment rolled for low volume topics.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3003&quot;&gt;&lt;del&gt;KAFKA-3003&lt;/del&gt;&lt;/a&gt; just got committed. &lt;/p&gt;</comment>
                            <comment id="15134776" author="rajiv@signalfx.com" created="Fri, 5 Feb 2016 19:24:00 +0000"  >&lt;p&gt;It does seem like it is related if not the same problem.&lt;/p&gt;</comment>
                            <comment id="15134809" author="hachikuji" created="Fri, 5 Feb 2016 19:34:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=granthenke&quot; class=&quot;user-hover&quot; rel=&quot;granthenke&quot;&gt;granthenke&lt;/a&gt; Thanks for the suggestion, but appears to be unrelated. I&apos;ve been able to reproduce the problem off of trunk with that patch included.&lt;/p&gt;</comment>
                            <comment id="15134813" author="rajiv@signalfx.com" created="Fri, 5 Feb 2016 19:36:55 +0000"  >&lt;p&gt;Though I should mention that I&apos;ve seen the same issue in older brokers 0.8.2.x etc too if I remember so it doesn&apos;t seem exclusive to 0.9.x.&lt;/p&gt;</comment>
                            <comment id="15134991" author="hachikuji" created="Fri, 5 Feb 2016 21:05:26 +0000"  >&lt;p&gt;Talked with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; offline about this. It seems the fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3003&quot; title=&quot;The fetch.wait.max.ms is not honored when new log segment rolled for low volume topics.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3003&quot;&gt;&lt;del&gt;KAFKA-3003&lt;/del&gt;&lt;/a&gt; may have been incomplete. It doesn&apos;t appear to handle the case where there is only 1 replica in the ISR set. If the high watermark doesn&apos;t get updated when the segment is rolled, then the fetch would return immediately.&lt;/p&gt;</comment>
                            <comment id="15135007" author="ijuma" created="Fri, 5 Feb 2016 21:13:22 +0000"  >&lt;p&gt;For reference, Jun also added a comment to the original PR:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/pull/688#issuecomment-180551456&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/688#issuecomment-180551456&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It may be worth adding a comment to that PR if we are looking to fix the remaining issue as part of this JIRA (to avoid duplicated work).&lt;/p&gt;</comment>
                            <comment id="15135356" author="hachikuji" created="Sat, 6 Feb 2016 00:45:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rajiv%40signalfx.com&quot; class=&quot;user-hover&quot; rel=&quot;rajiv@signalfx.com&quot;&gt;rajiv@signalfx.com&lt;/a&gt; We&apos;re still discussing the best way to address this issue, but for now, would you mind trying this patch: &lt;a href=&quot;https://github.com/hachikuji/kafka/commit/34158c835668f9780f65ab527ade160d9e19c87c?&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/hachikuji/kafka/commit/34158c835668f9780f65ab527ade160d9e19c87c?&lt;/a&gt; As far as what I&apos;ve been able to reproduce locally, this does fix the problem.&lt;/p&gt;</comment>
                            <comment id="15135586" author="rajiv@signalfx.com" created="Sat, 6 Feb 2016 04:48:15 +0000"  >&lt;p&gt;Thanks Jason. I&apos;ll try to apply this patch early next week. Should I build trunk + patch or 0.9.0 + patch?&lt;/p&gt;</comment>
                            <comment id="15136071" author="hachikuji" created="Sun, 7 Feb 2016 00:20:23 +0000"  >&lt;p&gt;Either should work, but perhaps it would be most useful at the moment to try against 0.9.0.&lt;/p&gt;</comment>
                            <comment id="15137342" author="githubbot" created="Mon, 8 Feb 2016 17:53:51 +0000"  >&lt;p&gt;GitHub user hachikuji opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/884&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/884&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3159&quot; title=&quot;Kafka consumer 0.9.0.0  client poll is very CPU intensive under certain conditions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3159&quot;&gt;&lt;del&gt;KAFKA-3159&lt;/del&gt;&lt;/a&gt;: stale high watermark segment offset causes early fetch return&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/hachikuji/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/hachikuji/kafka&lt;/a&gt; K3159&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/884.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/884.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #884&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit b23447db22aa5eaf6992d37f11ae31627598175b&lt;br/&gt;
Author: Jason Gustafson &amp;lt;jason@confluent.io&amp;gt;&lt;br/&gt;
Date:   2016-02-06T00:38:00Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3159&quot; title=&quot;Kafka consumer 0.9.0.0  client poll is very CPU intensive under certain conditions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3159&quot;&gt;&lt;del&gt;KAFKA-3159&lt;/del&gt;&lt;/a&gt;: stale high watermark segment offset causes early fetch return&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15138031" author="ijuma" created="Mon, 8 Feb 2016 23:56:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rajiv%40signalfx.com&quot; class=&quot;user-hover&quot; rel=&quot;rajiv@signalfx.com&quot;&gt;rajiv@signalfx.com&lt;/a&gt;, when you get a chance to try this, try the following branch: &lt;a href=&quot;https://github.com/hachikuji/kafka/tree/K3159&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/hachikuji/kafka/tree/K3159&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It would be great if you could try it soon as this is the only blocker left before we can release 0.9.0.1.&lt;/p&gt;</comment>
                            <comment id="15138038" author="rajiv@signalfx.com" created="Mon, 8 Feb 2016 23:58:10 +0000"  >&lt;p&gt;I&apos;ll try it tomorrow for sure.&lt;/p&gt;</comment>
                            <comment id="15139950" author="githubbot" created="Tue, 9 Feb 2016 22:58:16 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/884&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/884&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15139956" author="junrao" created="Tue, 9 Feb 2016 22:58:53 +0000"  >&lt;p&gt;Issue resolved by pull request 884&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/884&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/884&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15140049" author="rajiv@signalfx.com" created="Wed, 10 Feb 2016 00:09:58 +0000"  >&lt;p&gt;I am running a patched broker with a consumer consuming partitions that have no messages and it seems to be working fine. So it looks good so far. I&apos;ll run it for longer and then finally run it with real messages to make sure there is no regression. Thanks every one!&lt;/p&gt;</comment>
                            <comment id="15140054" author="ijuma" created="Wed, 10 Feb 2016 00:13:18 +0000"  >&lt;p&gt;Good news &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rajiv%40signalfx.com&quot; class=&quot;user-hover&quot; rel=&quot;rajiv@signalfx.com&quot;&gt;rajiv@signalfx.com&lt;/a&gt;, thanks for reporting back.&lt;/p&gt;</comment>
                            <comment id="15592137" author="kchudinov" created="Thu, 20 Oct 2016 15:30:18 +0000"  >&lt;p&gt;Hi everyone. We use kafka-10.0.0 (in which current bug should be fixed), but we&apos;ve faced with the same problem - high CPU usage (close to 50% for dummy consumer without any code in it), tons of EOFException during polling the topic without any data in it.&lt;br/&gt;
We use AWS m3.medium instance for testing (1 core) .&lt;br/&gt;
Our parameters of consumer: &lt;br/&gt;
metric.reporters = []&lt;br/&gt;
	metadata.max.age.ms = 300000&lt;br/&gt;
	partition.assignment.strategy = &lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.kafka.clients.consumer.RangeAssignor&amp;#93;&lt;/span&gt;&lt;br/&gt;
	reconnect.backoff.ms = 50&lt;br/&gt;
	sasl.kerberos.ticket.renew.window.factor = 0.8&lt;br/&gt;
	max.partition.fetch.bytes = 10485760&lt;br/&gt;
	bootstrap.servers = &lt;span class=&quot;error&quot;&gt;&amp;#91;server1:6667, server2:6667, server3.aws:6667&amp;#93;&lt;/span&gt;&lt;br/&gt;
	ssl.keystore.type = JKS&lt;br/&gt;
	enable.auto.commit = false&lt;br/&gt;
	sasl.mechanism = GSSAPI&lt;br/&gt;
	interceptor.classes = null&lt;br/&gt;
	exclude.internal.topics = true&lt;br/&gt;
	ssl.truststore.password = null&lt;br/&gt;
	client.id = consumer-1&lt;br/&gt;
	ssl.endpoint.identification.algorithm = null&lt;br/&gt;
	max.poll.records = 50&lt;br/&gt;
	check.crcs = true&lt;br/&gt;
	request.timeout.ms = 40000&lt;br/&gt;
	heartbeat.interval.ms = 3000&lt;br/&gt;
	auto.commit.interval.ms = 5000&lt;br/&gt;
	receive.buffer.bytes = 10485760&lt;br/&gt;
	ssl.truststore.type = JKS&lt;br/&gt;
	ssl.truststore.location = null&lt;br/&gt;
	ssl.keystore.password = null&lt;br/&gt;
	fetch.min.bytes = 10485760&lt;br/&gt;
	send.buffer.bytes = 131072&lt;br/&gt;
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer&lt;br/&gt;
	group.id = context-facade-consumer&lt;br/&gt;
	retry.backoff.ms = 100&lt;br/&gt;
	sasl.kerberos.kinit.cmd = /usr/bin/kinit&lt;br/&gt;
	sasl.kerberos.service.name = null&lt;br/&gt;
	sasl.kerberos.ticket.renew.jitter = 0.05&lt;br/&gt;
	ssl.trustmanager.algorithm = PKIX&lt;br/&gt;
	ssl.key.password = null&lt;br/&gt;
	fetch.max.wait.ms = 1&lt;br/&gt;
	sasl.kerberos.min.time.before.relogin = 60000&lt;br/&gt;
	connections.max.idle.ms = 540000&lt;br/&gt;
	session.timeout.ms = 30000&lt;br/&gt;
	metrics.num.samples = 2&lt;br/&gt;
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer&lt;br/&gt;
	ssl.protocol = TLS&lt;br/&gt;
	ssl.provider = null&lt;br/&gt;
	ssl.enabled.protocols = &lt;span class=&quot;error&quot;&gt;&amp;#91;TLSv1.2, TLSv1.1, TLSv1&amp;#93;&lt;/span&gt;&lt;br/&gt;
	ssl.keystore.location = null&lt;br/&gt;
	ssl.cipher.suites = null&lt;br/&gt;
	security.protocol = PLAINTEXT&lt;br/&gt;
	ssl.keymanager.algorithm = SunX509&lt;br/&gt;
	metrics.sample.window.ms = 30000&lt;br/&gt;
	auto.offset.reset = latest&lt;/p&gt;</comment>
                            <comment id="16021140" author="githubbot" created="Tue, 23 May 2017 12:36:05 +0000"  >&lt;p&gt;GitHub user felixgborrego opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/3127&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3127&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Add sleep between empty polls to avoid burning CPU&lt;/p&gt;

&lt;p&gt;    Workaround for  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3159&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3159&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/Nitro/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/Nitro/kafka&lt;/a&gt; 0.9.0.2-NITRO&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/3127.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3127.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #3127&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 88cfff0660bd726ab5cd11ceee79c5cc35ddce18&lt;br/&gt;
Author: Felix &amp;lt;fborrego@gonitro.com&amp;gt;&lt;br/&gt;
Date:   2017-05-23T12:35:14Z&lt;/p&gt;

&lt;p&gt;    Add sleep between empty polls to avoid burning CPU&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16022604" author="githubbot" created="Wed, 24 May 2017 09:27:38 +0000"  >&lt;p&gt;Github user felixgborrego closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/3127&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3127&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12785580" name="Memory-profile-patched-client.png" size="417003" author="rajiv@signalfx.com" created="Mon, 1 Feb 2016 19:29:32 +0000"/>
                            <attachment id="12785572" name="Screen Shot 2016-02-01 at 11.09.32 AM.png" size="418695" author="rajiv@signalfx.com" created="Mon, 1 Feb 2016 19:18:06 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 25 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2s1rb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>junrao</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>