<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:12:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6975] AdminClient.deleteRecords() may cause replicas unable to fetch from beginning</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6975</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;AdminClient.deleteRecords(beforeOffset(offset)) will set log start offset to the requested offset. If the requested offset is in the middle of the batch, the replica will not be able to fetch from that offset (because it is in the middle of the batch).&#160;&lt;/p&gt;

&lt;p&gt;One use-case where this could cause problems is replica re-assignment. Suppose we have a topic partition with 3 initial replicas, and at some point the user issues&#160;&#160;AdminClient.deleteRecords() for the offset that falls in the middle of the batch. It now becomes log start offset for this topic partition. Suppose at some later time, the user starts partition re-assignment to 3 new replicas. The new replicas (followers) will start with HW = 0, will try to fetch from 0, then get &quot;out of order offset&quot; because 0 &amp;lt; log start offset (LSO); the follower will be able to reset offset to LSO of the leader and fetch LSO; the leader will send a batch in response with base offset &amp;lt;LSO, this will cause &quot;out of order offset&quot; on the follower which will stop the fetcher thread. The end result is that the new replicas will not be able to start fetching unless LSO moves to an offset that is not in the middle of the batch, and the re-assignment will be stuck for a possibly a very log time.&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13163239">KAFKA-6975</key>
            <summary>AdminClient.deleteRecords() may cause replicas unable to fetch from beginning</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="apovzner">Anna Povzner</assignee>
                                    <reporter username="apovzner">Anna Povzner</reporter>
                        <labels>
                    </labels>
                <created>Thu, 31 May 2018 18:35:20 +0000</created>
                <updated>Sun, 17 Jun 2018 00:52:28 +0000</updated>
                            <resolved>Thu, 14 Jun 2018 15:38:49 +0000</resolved>
                                    <version>1.0.1</version>
                    <version>1.1.0</version>
                                    <fixVersion>1.0.2</fixVersion>
                    <fixVersion>1.1.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16500796" author="githubbot" created="Mon, 4 Jun 2018 20:13:14 +0000"  >&lt;p&gt;apovzner opened a new pull request #5133: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6975&quot; title=&quot;AdminClient.deleteRecords() may cause replicas unable to fetch from beginning&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6975&quot;&gt;&lt;del&gt;KAFKA-6975&lt;/del&gt;&lt;/a&gt;: Fix fetching from non-batch-aligned log start offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5133&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5133&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW. &lt;/p&gt;

&lt;p&gt;   Added two integration tests:&lt;br/&gt;
   1) Produce and then AdminClient#deleteRecords() while one of the followers is down, and then restart of the follower requires fetching from log start offset;&lt;br/&gt;
   2)  AdminClient#AlterReplicaLogDirs() after AdminClient#deleteRecords()&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16512609" author="githubbot" created="Thu, 14 Jun 2018 15:26:49 +0000"  >&lt;p&gt;hachikuji closed pull request #5133: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6975&quot; title=&quot;AdminClient.deleteRecords() may cause replicas unable to fetch from beginning&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6975&quot;&gt;&lt;del&gt;KAFKA-6975&lt;/del&gt;&lt;/a&gt;: Fix fetching from non-batch-aligned log start offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5133&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5133&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index b9180a45378..55f870e96f7 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -22,6 +22,7 @@ import java.util.concurrent.locks.ReentrantReadWriteLock&lt;br/&gt;
 import com.yammer.metrics.core.Gauge&lt;br/&gt;
 import kafka.api.LeaderAndIsr&lt;br/&gt;
 import kafka.api.Request&lt;br/&gt;
+import kafka.common.UnexpectedAppendOffsetException&lt;br/&gt;
 import kafka.controller.KafkaController&lt;br/&gt;
 import kafka.log.&lt;/p&gt;
{LogAppendInfo, LogConfig}
&lt;p&gt; import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
@@ -30,7 +31,7 @@ import kafka.utils.CoreUtils.&lt;/p&gt;
{inReadLock, inWriteLock}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
 import kafka.zk.AdminZkClient&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import org.apache.kafka.common.errors.&lt;/p&gt;
{NotEnoughReplicasException, NotLeaderForPartitionException, PolicyViolationException}
&lt;p&gt;+import org.apache.kafka.common.errors.&lt;/p&gt;
{ReplicaNotAvailableException, NotEnoughReplicasException, NotLeaderForPartitionException, PolicyViolationException}
&lt;p&gt; import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors._&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords&lt;br/&gt;
@@ -187,6 +188,10 @@ class Partition(val topic: String,&lt;/p&gt;

&lt;p&gt;   def getReplica(replicaId: Int = localBrokerId): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt; = Option(allReplicasMap.get(replicaId))&lt;/p&gt;

&lt;p&gt;+  def getReplicaOrException(replicaId: Int = localBrokerId): Replica =&lt;br/&gt;
+    getReplica(replicaId).getOrElse(&lt;br/&gt;
+      throw new ReplicaNotAvailableException(s&quot;Replica $replicaId is not available for partition $topicPartition&quot;))&lt;br/&gt;
+&lt;br/&gt;
   def leaderReplicaIfLocal: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt; =&lt;br/&gt;
     leaderReplicaIdOpt.filter(_ == localBrokerId).flatMap(getReplica)&lt;/p&gt;

&lt;p&gt;@@ -545,15 +550,41 @@ class Partition(val topic: String,&lt;br/&gt;
     laggingReplicas&lt;br/&gt;
   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def appendRecordsToFutureReplica(records: MemoryRecords) {&lt;/li&gt;
	&lt;li&gt;getReplica(Request.FutureLocalReplicaId).get.log.get.appendAsFollower(records)&lt;br/&gt;
+  private def doAppendRecordsToFollowerOrFutureReplica(records: MemoryRecords, isFuture: Boolean): Unit = {&lt;br/&gt;
+      if (isFuture)&lt;br/&gt;
+        getReplicaOrException(Request.FutureLocalReplicaId).log.get.appendAsFollower(records)&lt;br/&gt;
+      else 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        // The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread+        // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.+        inReadLock(leaderIsrUpdateLock) {
+           getReplicaOrException().log.get.appendAsFollower(records)
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def appendRecordsToFollower(records: MemoryRecords) {&lt;/li&gt;
	&lt;li&gt;// The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread&lt;/li&gt;
	&lt;li&gt;// is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.&lt;/li&gt;
	&lt;li&gt;inReadLock(leaderIsrUpdateLock) {&lt;/li&gt;
	&lt;li&gt;getReplica().get.log.get.appendAsFollower(records)&lt;br/&gt;
+  def appendRecordsToFollowerOrFutureReplica(records: MemoryRecords, isFuture: Boolean) {&lt;br/&gt;
+    try 
{
+      doAppendRecordsToFollowerOrFutureReplica(records, isFuture)
+    }
&lt;p&gt; catch {&lt;br/&gt;
+      case e: UnexpectedAppendOffsetException =&amp;gt;&lt;br/&gt;
+        val replica = if (isFuture) getReplicaOrException(Request.FutureLocalReplicaId) else getReplicaOrException()&lt;br/&gt;
+        val logEndOffset = replica.logEndOffset.messageOffset&lt;br/&gt;
+        if (logEndOffset == replica.logStartOffset &amp;amp;&amp;amp;&lt;br/&gt;
+            e.firstOffset &amp;lt; logEndOffset &amp;amp;&amp;amp; e.lastOffset &amp;gt;= logEndOffset) {&lt;br/&gt;
+          // This may happen if the log start offset on the leader (or current replica) falls in&lt;br/&gt;
+          // the middle of the batch due to delete records request and the follower tries to&lt;br/&gt;
+          // fetch its first offset from the leader.&lt;br/&gt;
+          // We handle this case here instead of Log#append() because we will need to remove the&lt;br/&gt;
+          // segment that start with log start offset and create a new one with earlier offset&lt;br/&gt;
+          // (base offset of the batch), which will move recoveryPoint backwards, so we will need&lt;br/&gt;
+          // to checkpoint the new recovery point before we append&lt;br/&gt;
+          val replicaName = if (isFuture) &quot;future replica&quot; else &quot;follower&quot;&lt;br/&gt;
+          info(s&quot;Unexpected offset in append to $topicPartition. First offset ${e.firstOffset} is less than log start offset ${replica.logStartOffset}.&quot; +&lt;br/&gt;
+               s&quot; Since this is the first record to be appended to the $replicaName&apos;s log, will start the log from offset ${e.firstOffset}.&quot;)&lt;br/&gt;
+          truncateFullyAndStartAt(e.firstOffset, isFuture)&lt;br/&gt;
+          doAppendRecordsToFollowerOrFutureReplica(records, isFuture)&lt;br/&gt;
+        } else&lt;br/&gt;
+          throw e&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala b/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..f8daaa4a181&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala&lt;br/&gt;
@@ -0,0 +1,25 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ * &lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates the follower received records with non-monotonically increasing offsets&lt;br/&gt;
+ */&lt;br/&gt;
+class OffsetsOutOfOrderException(message: String) extends RuntimeException(message) &lt;/p&gt;
{
+}&lt;br/&gt;
+&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala b/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..e719a93006d&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala&lt;br/&gt;
@@ -0,0 +1,29 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates the follower or the future replica received records from the leader (or current&lt;br/&gt;
+ * replica) with first offset less than expected next offset. &lt;br/&gt;
+ * @param firstOffset The first offset of the records to append&lt;br/&gt;
+ * @param lastOffset  The last offset of the records to append&lt;br/&gt;
+ */&lt;br/&gt;
+class UnexpectedAppendOffsetException(val message: String,&lt;br/&gt;
+                                      val firstOffset: Long,&lt;br/&gt;
+                                      val lastOffset: Long) extends RuntimeException(message) {+}
&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index c7d2a6e3b6e..c92beee0f34 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -29,7 +29,7 @@ import java.util.regex.Pattern&lt;/p&gt;

&lt;p&gt; import com.yammer.metrics.core.Gauge&lt;br/&gt;
 import kafka.api.KAFKA_0_10_0_IV0&lt;br/&gt;
-import kafka.common.&lt;/p&gt;
{InvalidOffsetException, KafkaException, LogSegmentOffsetOverflowException, LongRef}
&lt;p&gt;+import kafka.common.&lt;/p&gt;
{InvalidOffsetException, KafkaException, LogSegmentOffsetOverflowException, LongRef, UnexpectedAppendOffsetException, OffsetsOutOfOrderException}
&lt;p&gt; import kafka.message.&lt;/p&gt;
{BrokerCompressionCodec, CompressionCodec, NoCompressionCodec}
&lt;p&gt; import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.server.checkpoints.&lt;/p&gt;
{LeaderEpochCheckpointFile, LeaderEpochFile}
&lt;p&gt;@@ -49,11 +49,11 @@ import scala.collection.&lt;/p&gt;
{Seq, Set, mutable}

&lt;p&gt; object LogAppendInfo &lt;/p&gt;
{
   val UnknownLogAppendInfo = LogAppendInfo(None, -1, RecordBatch.NO_TIMESTAMP, -1L, RecordBatch.NO_TIMESTAMP, -1L,
-    RecordConversionStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false)
+    RecordConversionStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false, -1L)
 
   def unknownLogAppendInfoWithLogStartOffset(logStartOffset: Long): LogAppendInfo =
     LogAppendInfo(None, -1, RecordBatch.NO_TIMESTAMP, -1L, RecordBatch.NO_TIMESTAMP, logStartOffset,
-      RecordConversionStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false)
+      RecordConversionStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false, -1L)
 }

&lt;p&gt; /**&lt;br/&gt;
@@ -72,6 +72,7 @@ object LogAppendInfo {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param shallowCount The number of shallow messages&lt;/li&gt;
	&lt;li&gt;@param validBytes The number of valid bytes&lt;/li&gt;
	&lt;li&gt;@param offsetsMonotonic Are the offsets in this message set monotonically increasing&lt;br/&gt;
+ * @param lastOffsetOfFirstBatch The last offset of the first batch&lt;br/&gt;
  */&lt;br/&gt;
 case class LogAppendInfo(var firstOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
                          var lastOffset: Long,&lt;br/&gt;
@@ -84,12 +85,15 @@ case class LogAppendInfo(var firstOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
                          targetCodec: CompressionCodec,&lt;br/&gt;
                          shallowCount: Int,&lt;br/&gt;
                          validBytes: Int,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;offsetsMonotonic: Boolean) {&lt;br/&gt;
+                         offsetsMonotonic: Boolean,&lt;br/&gt;
+                         lastOffsetOfFirstBatch: Long) {&lt;br/&gt;
   /**&lt;/li&gt;
	&lt;li&gt;* Get the first offset if it exists, else get the last offset.&lt;/li&gt;
	&lt;li&gt;* @return The offset of first message if it exists; else offset of the last message.&lt;br/&gt;
+   * Get the first offset if it exists, else get the last offset of the first batch&lt;br/&gt;
+   * For magic versions 2 and newer, this method will return first offset. For magic versions&lt;br/&gt;
+   * older than 2, we use the last offset of the first batch as an approximation of the first&lt;br/&gt;
+   * offset to avoid decompressing the data.&lt;br/&gt;
    */&lt;/li&gt;
	&lt;li&gt;def firstOrLastOffset: Long = firstOffset.getOrElse(lastOffset)&lt;br/&gt;
+  def firstOrLastOffsetOfFirstBatch: Long = firstOffset.getOrElse(lastOffsetOfFirstBatch)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Get the (maximum) number of messages described by LogAppendInfo&lt;br/&gt;
@@ -736,6 +740,8 @@ class Log(@volatile var dir: File,&lt;/li&gt;
	&lt;li&gt;@param assignOffsets Should the log assign offsets to this message set or blindly apply what it is given&lt;/li&gt;
	&lt;li&gt;@param leaderEpoch The partition&apos;s leader epoch which will be applied to messages when offsets are assigned on the leader&lt;/li&gt;
	&lt;li&gt;@throws KafkaStorageException If the append fails due to an I/O error.&lt;br/&gt;
+   * @throws OffsetsOutOfOrderException If out of order offsets found in &apos;records&apos;&lt;br/&gt;
+   * @throws UnexpectedAppendOffsetException If the first or last offset in append is less than next offset&lt;/li&gt;
	&lt;li&gt;@return Information about the appended messages including the first and last offset.&lt;br/&gt;
    */&lt;br/&gt;
   private def append(records: MemoryRecords, isFromClient: Boolean, assignOffsets: Boolean, leaderEpoch: Int): LogAppendInfo = 
{
@@ -798,9 +804,27 @@ class Log(@volatile var dir: File,
           }
&lt;p&gt;         } else {&lt;br/&gt;
           // we are taking the offsets we are given&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!appendInfo.offsetsMonotonic || appendInfo.firstOrLastOffset &amp;lt; nextOffsetMetadata.messageOffset)&lt;/li&gt;
	&lt;li&gt;throw new IllegalArgumentException(s&quot;Out of order offsets found in append to $topicPartition: &quot; +&lt;/li&gt;
	&lt;li&gt;records.records.asScala.map(_.offset))&lt;br/&gt;
+          if (!appendInfo.offsetsMonotonic)&lt;br/&gt;
+            throw new OffsetsOutOfOrderException(s&quot;Out of order offsets found in append to $topicPartition: &quot; +&lt;br/&gt;
+                                                 records.records.asScala.map(_.offset))&lt;br/&gt;
+&lt;br/&gt;
+          if (appendInfo.firstOrLastOffsetOfFirstBatch &amp;lt; nextOffsetMetadata.messageOffset) {&lt;br/&gt;
+            // we may still be able to recover if the log is empty&lt;br/&gt;
+            // one example: fetching from log start offset on the leader which is not batch aligned,&lt;br/&gt;
+            // which may happen as a result of AdminClient#deleteRecords()&lt;br/&gt;
+            val firstOffset = appendInfo.firstOffset match 
{
+              case Some(offset) =&amp;gt; offset
+              case None =&amp;gt; records.batches.asScala.head.baseOffset()
+            }
&lt;p&gt;+&lt;br/&gt;
+            val firstOrLast = if (appendInfo.firstOffset.isDefined) &quot;First offset&quot; else &quot;Last offset of the first batch&quot;&lt;br/&gt;
+            throw new UnexpectedAppendOffsetException(&lt;br/&gt;
+              s&quot;Unexpected offset in append to $topicPartition. $firstOrLast &quot; +&lt;br/&gt;
+              s&quot;${appendInfo.firstOrLastOffsetOfFirstBatch} is less than the next offset ${nextOffsetMetadata.messageOffset}. &quot; +&lt;br/&gt;
+              s&quot;First 10 offsets in append: ${records.records.asScala.take(10).map(_.offset)}, last offset in&quot; +&lt;br/&gt;
+              s&quot; append: ${appendInfo.lastOffset}. Log start offset = $logStartOffset&quot;,&lt;br/&gt;
+              firstOffset, appendInfo.lastOffset)&lt;br/&gt;
+          }&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // update the epoch cache with the epoch stamped onto the message by the leader&lt;br/&gt;
@@ -830,7 +854,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         val segment = maybeRoll(validRecords.sizeInBytes, appendInfo)&lt;/p&gt;

&lt;p&gt;         val logOffsetMetadata = LogOffsetMetadata(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;messageOffset = appendInfo.firstOrLastOffset,&lt;br/&gt;
+          messageOffset = appendInfo.firstOrLastOffsetOfFirstBatch,&lt;br/&gt;
           segmentBaseOffset = segment.baseOffset,&lt;br/&gt;
           relativePositionInSegment = segment.size)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -970,6 +994,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     var maxTimestamp = RecordBatch.NO_TIMESTAMP&lt;br/&gt;
     var offsetOfMaxTimestamp = -1L&lt;br/&gt;
     var readFirstMessage = false&lt;br/&gt;
+    var lastOffsetOfFirstBatch = -1L&lt;/p&gt;

&lt;p&gt;     for (batch &amp;lt;- records.batches.asScala) {&lt;br/&gt;
       // we only validate V2 and higher to avoid potential compatibility issues with older clients&lt;br/&gt;
@@ -986,6 +1011,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
       if (!readFirstMessage) &lt;/p&gt;
{
         if (batch.magic &amp;gt;= RecordBatch.MAGIC_VALUE_V2)
           firstOffset = Some(batch.baseOffset)
+        lastOffsetOfFirstBatch = batch.lastOffset
         readFirstMessage = true
       }

&lt;p&gt;@@ -1024,7 +1050,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     // Apply broker-side compression if any&lt;br/&gt;
     val targetCodec = BrokerCompressionCodec.getTargetCompressionCodec(config.compressionType, sourceCodec)&lt;br/&gt;
     LogAppendInfo(firstOffset, lastOffset, maxTimestamp, offsetOfMaxTimestamp, RecordBatch.NO_TIMESTAMP, logStartOffset,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;RecordConversionStats.EMPTY, sourceCodec, targetCodec, shallowMessageCount, validBytesCount, monotonic)&lt;br/&gt;
+      RecordConversionStats.EMPTY, sourceCodec, targetCodec, shallowMessageCount, validBytesCount, monotonic, lastOffsetOfFirstBatch)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def updateProducers(batch: RecordBatch,&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
index 5a505c3d377..e46473b69e9 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
@@ -98,8 +98,7 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
       throw new IllegalStateException(&quot;Offset mismatch for the future replica %s: fetched offset = %d, log end offset = %d.&quot;.format(&lt;br/&gt;
         topicPartition, fetchOffset, futureReplica.logEndOffset.messageOffset))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Append the leader&apos;s messages to the log&lt;/li&gt;
	&lt;li&gt;partition.appendRecordsToFutureReplica(records)&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(records, isFuture = true)&lt;br/&gt;
     val futureReplicaHighWatermark = futureReplica.logEndOffset.messageOffset.min(partitionData.highWatermark)&lt;br/&gt;
     futureReplica.highWatermark = new LogOffsetMetadata(futureReplicaHighWatermark)&lt;br/&gt;
     futureReplica.maybeIncrementLogStartOffset(partitionData.logStartOffset)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index cf8d829f850..80940f61470 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -112,7 +112,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
         .format(replica.logEndOffset.messageOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Append the leader&apos;s messages to the log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;partition.appendRecordsToFollower(records)&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(records, isFuture = false)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (isTraceEnabled)&lt;br/&gt;
       trace(&quot;Follower has replica log end offset %d after appending %d bytes of messages for partition %s&quot;&lt;br/&gt;
diff --git a/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala&lt;br/&gt;
index 986fa4a366a..02baf66068b 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala&lt;br/&gt;
@@ -811,6 +811,83 @@ class AdminClientIntegrationTest extends IntegrationTestHarness with Logging &lt;/p&gt;
{
       assertEquals(3, servers(i).replicaManager.getReplica(topicPartition).get.logStartOffset)
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testReplicaCanFetchFromLogStartOffsetAfterDeleteRecords(): Unit = {&lt;br/&gt;
+    val leaders = createTopic(topic, numPartitions = 1, replicationFactor = serverCount)&lt;br/&gt;
+    val followerIndex = if (leaders(0) != servers(0).config.brokerId) 0 else 1&lt;br/&gt;
+&lt;br/&gt;
+    def waitForFollowerLog(expectedStartOffset: Long, expectedEndOffset: Long): Unit = {&lt;br/&gt;
+      TestUtils.waitUntilTrue(() =&amp;gt; servers(followerIndex).replicaManager.getReplica(topicPartition) != None,&lt;br/&gt;
+                              &quot;Expected follower to create replica for partition&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      // wait until the follower discovers that log start offset moved beyond its HW&lt;br/&gt;
+      TestUtils.waitUntilTrue(() =&amp;gt; &lt;/p&gt;
{
+        servers(followerIndex).replicaManager.getReplica(topicPartition).get.logStartOffset == expectedStartOffset
+      }
&lt;p&gt;, s&quot;Expected follower to discover new log start offset $expectedStartOffset&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      TestUtils.waitUntilTrue(() =&amp;gt; &lt;/p&gt;
{
+        servers(followerIndex).replicaManager.getReplica(topicPartition).get.logEndOffset.messageOffset == expectedEndOffset
+      }
&lt;p&gt;, s&quot;Expected follower to catch up to log end offset $expectedEndOffset&quot;)&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // we will produce to topic and delete records while one follower is down&lt;br/&gt;
+    killBroker(followerIndex)&lt;br/&gt;
+&lt;br/&gt;
+    client = AdminClient.create(createConfig)&lt;br/&gt;
+    sendRecords(producers.head, 100, topicPartition)&lt;br/&gt;
+&lt;br/&gt;
+    val result = client.deleteRecords(Map(topicPartition -&amp;gt; RecordsToDelete.beforeOffset(3L)).asJava)&lt;br/&gt;
+    result.all().get()&lt;br/&gt;
+&lt;br/&gt;
+    // start the stopped broker to verify that it will be able to fetch from new log start offset&lt;br/&gt;
+    restartDeadBrokers()&lt;br/&gt;
+&lt;br/&gt;
+    waitForFollowerLog(expectedStartOffset=3L, expectedEndOffset=100L)&lt;br/&gt;
+&lt;br/&gt;
+    // after the new replica caught up, all replicas should have same log start offset&lt;br/&gt;
+    for (i &amp;lt;- 0 until serverCount)&lt;br/&gt;
+      assertEquals(3, servers&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.replicaManager.getReplica(topicPartition).get.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // kill the same follower again, produce more records, and delete records beyond follower&apos;s LOE&lt;br/&gt;
+    killBroker(followerIndex)&lt;br/&gt;
+    sendRecords(producers.head, 100, topicPartition)&lt;br/&gt;
+    val result1 = client.deleteRecords(Map(topicPartition -&amp;gt; RecordsToDelete.beforeOffset(117L)).asJava)&lt;br/&gt;
+    result1.all().get()&lt;br/&gt;
+    restartDeadBrokers()&lt;br/&gt;
+    waitForFollowerLog(expectedStartOffset=117L, expectedEndOffset=200L)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAlterLogDirsAfterDeleteRecords(): Unit = {&lt;br/&gt;
+    client = AdminClient.create(createConfig)&lt;br/&gt;
+    createTopic(topic, numPartitions = 1, replicationFactor = serverCount)&lt;br/&gt;
+    val expectedLEO = 100&lt;br/&gt;
+    sendRecords(producers.head, expectedLEO, topicPartition)&lt;br/&gt;
+&lt;br/&gt;
+    // delete records to move log start offset&lt;br/&gt;
+    val result = client.deleteRecords(Map(topicPartition -&amp;gt; RecordsToDelete.beforeOffset(3L)).asJava)&lt;br/&gt;
+    result.all().get()&lt;br/&gt;
+    // make sure we are in the expected state after delete records&lt;br/&gt;
+    for (i &amp;lt;- 0 until serverCount) &lt;/p&gt;
{
+      assertEquals(3, servers(i).replicaManager.getReplica(topicPartition).get.logStartOffset)
+      assertEquals(expectedLEO, servers(i).replicaManager.getReplica(topicPartition).get.logEndOffset.messageOffset)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // we will create another dir just for one server&lt;br/&gt;
+    val futureLogDir = servers(0).config.logDirs(1)&lt;br/&gt;
+    val futureReplica = new TopicPartitionReplica(topic, 0, servers(0).config.brokerId)&lt;br/&gt;
+&lt;br/&gt;
+    // Verify that replica can be moved to the specified log directory&lt;br/&gt;
+    client.alterReplicaLogDirs(Map(futureReplica -&amp;gt; futureLogDir).asJava).all.get&lt;br/&gt;
+    TestUtils.waitUntilTrue(() =&amp;gt; &lt;/p&gt;
{
+      futureLogDir == servers(0).logManager.getLog(topicPartition).get.dir.getParent
+    }
&lt;p&gt;, &quot;timed out waiting for replica movement&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    // once replica moved, its LSO and LEO should match other replicas&lt;br/&gt;
+    assertEquals(3, servers(0).replicaManager.getReplica(topicPartition).get.logStartOffset)&lt;br/&gt;
+    assertEquals(expectedLEO, servers(0).replicaManager.getReplica(topicPartition).get.logEndOffset.messageOffset)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testOffsetsForTimesAfterDeleteRecords(): Unit = {&lt;br/&gt;
     createTopic(topic, numPartitions = 2, replicationFactor = serverCount)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..fe5d578533b&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
@@ -0,0 +1,174 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *      &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package kafka.cluster&lt;br/&gt;
+&lt;br/&gt;
+import java.io.File&lt;br/&gt;
+import java.nio.ByteBuffer&lt;br/&gt;
+import java.util.Properties&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicBoolean&lt;br/&gt;
+&lt;br/&gt;
+import kafka.common.UnexpectedAppendOffsetException&lt;br/&gt;
+import kafka.log.&lt;/p&gt;
{Log, LogConfig, LogManager, CleanerConfig}
&lt;p&gt;+import kafka.server._&lt;br/&gt;
+import kafka.utils.&lt;/p&gt;
{MockTime, TestUtils, MockScheduler}
&lt;p&gt;+import kafka.utils.timer.MockTimer&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.errors.ReplicaNotAvailableException&lt;br/&gt;
+import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils&lt;br/&gt;
+import org.apache.kafka.common.record._&lt;br/&gt;
+import org.junit.&lt;/p&gt;
{After, Before, Test}&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
+import org.scalatest.Assertions.assertThrows&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class PartitionTest {&lt;br/&gt;
+&lt;br/&gt;
+  val brokerId = 101&lt;br/&gt;
+  val topicPartition = new TopicPartition(&quot;test-topic&quot;, 0)&lt;br/&gt;
+  val time = new MockTime()&lt;br/&gt;
+  val brokerTopicStats = new BrokerTopicStats&lt;br/&gt;
+  val metrics = new Metrics&lt;br/&gt;
+&lt;br/&gt;
+  var tmpDir: File = _&lt;br/&gt;
+  var logDir: File = _&lt;br/&gt;
+  var replicaManager: ReplicaManager = _&lt;br/&gt;
+  var logManager: LogManager = _&lt;br/&gt;
+  var logConfig: LogConfig = _&lt;br/&gt;
+&lt;br/&gt;
+  @Before&lt;br/&gt;
+  def setup(): Unit = {
+    val logProps = new Properties()
+    logProps.put(LogConfig.SegmentBytesProp, 512: java.lang.Integer)
+    logProps.put(LogConfig.SegmentIndexBytesProp, 1000: java.lang.Integer)
+    logProps.put(LogConfig.RetentionMsProp, 999: java.lang.Integer)
+    logConfig = LogConfig(logProps)
+
+    tmpDir = TestUtils.tempDir()
+    logDir = TestUtils.randomPartitionLogDir(tmpDir)
+    logManager = TestUtils.createLogManager(
+      logDirs = Seq(logDir), defaultConfig = logConfig, CleanerConfig(enableCleaner = false), time)
+    logManager.startup()
+
+    val brokerProps = TestUtils.createBrokerConfig(brokerId, TestUtils.MockZkConnect)
+    brokerProps.put(&quot;log.dir&quot;, logDir.getAbsolutePath)
+    val brokerConfig = KafkaConfig.fromProps(brokerProps)
+    replicaManager = new ReplicaManager(
+      config = brokerConfig, metrics, time, zkClient = null, new MockScheduler(time),
+      logManager, new AtomicBoolean(false), QuotaFactory.instantiate(brokerConfig, metrics, time, &quot;&quot;),
+      brokerTopicStats, new MetadataCache(brokerId), new LogDirFailureChannel(brokerConfig.logDirs.size))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @After&lt;br/&gt;
+  def tearDown(): Unit = {
+    brokerTopicStats.close()
+    metrics.close()
+
+    logManager.shutdown()
+    Utils.delete(tmpDir)
+    logManager.liveLogDirs.foreach(Utils.delete)
+    replicaManager.shutdown(checkpointHW = false)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendRecordsAsFollowerBelowLogStartOffset(): Unit = {&lt;br/&gt;
+    val log = logManager.getOrCreateLog(topicPartition, logConfig)&lt;br/&gt;
+    val replica = new Replica(brokerId, topicPartition, time, log = Some(log))&lt;br/&gt;
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+    partition.addReplicaIfNotExists(replica)&lt;br/&gt;
+    assertEquals(Some(replica), partition.getReplica(replica.brokerId))&lt;br/&gt;
+&lt;br/&gt;
+    val initialLogStartOffset = 5L&lt;br/&gt;
+    partition.truncateFullyAndStartAt(initialLogStartOffset, isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after truncate fully and start at $initialLogStartOffset:&quot;,&lt;br/&gt;
+                 initialLogStartOffset, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset after truncate fully and start at $initialLogStartOffset:&quot;,&lt;br/&gt;
+                 initialLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // verify that we cannot append records that do not contain log start offset even if the log is empty&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;UnexpectedAppendOffsetException&amp;#93;&lt;/span&gt; {
+      // append one record with offset = 3
+      partition.appendRecordsToFollowerOrFutureReplica(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 3L), isFuture = false)
+    }&lt;br/&gt;
+    assertEquals(s&quot;Log end offset should not change after failure to append&quot;, initialLogStartOffset, replica.logEndOffset.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // verify that we can append records that contain log start offset, even when first&lt;br/&gt;
+    // offset &amp;lt; log start offset if the log is empty&lt;br/&gt;
+    val newLogStartOffset = 4L&lt;br/&gt;
+    val records = createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                     new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+                                     new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),&lt;br/&gt;
+                                baseOffset = newLogStartOffset)&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(records, isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 3 records with base offset $newLogStartOffset:&quot;, 7L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset after append of 3 records with base offset $newLogStartOffset:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // and we can append more records after that&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 7L), isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 1 record at offset 7:&quot;, 8L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset not expected to change:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // but we cannot append to offset &amp;lt; log start if the log is not empty&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;UnexpectedAppendOffsetException&amp;#93;&lt;/span&gt; {
+      val records2 = createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),
+                                        new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)),
+                                   baseOffset = 3L)
+      partition.appendRecordsToFollowerOrFutureReplica(records2, isFuture = false)
+    }&lt;br/&gt;
+    assertEquals(s&quot;Log end offset should not change after failure to append&quot;, 8L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // we still can append to next offset&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 8L), isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 1 record at offset 8:&quot;, 9L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset not expected to change:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testGetReplica(): Unit = {&lt;br/&gt;
+    val log = logManager.getOrCreateLog(topicPartition, logConfig)&lt;br/&gt;
+    val replica = new Replica(brokerId, topicPartition, time, log = Some(log))&lt;br/&gt;
+    val partition = new&lt;br/&gt;
+        Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    assertEquals(None, partition.getReplica(brokerId))&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaNotAvailableException&amp;#93;&lt;/span&gt; {
+      partition.getReplicaOrException(brokerId)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    partition.addReplicaIfNotExists(replica)&lt;br/&gt;
+    assertEquals(replica, partition.getReplicaOrException(brokerId))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendRecordsToFollowerWithNoReplicaThrowsException(): Unit = {&lt;br/&gt;
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaNotAvailableException&amp;#93;&lt;/span&gt; {
+      partition.appendRecordsToFollowerOrFutureReplica(
+           createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 0L), isFuture = false)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def createRecords(records: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;SimpleRecord&amp;#93;&lt;/span&gt;, baseOffset: Long, partitionLeaderEpoch: Int = 0): MemoryRecords = {
+    val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))
+    val builder = MemoryRecords.builder(
+      buf, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.LOG_APPEND_TIME,
+      baseOffset, time.milliseconds, partitionLeaderEpoch)
+    records.foreach(builder.append)
+    builder.build()
+  }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index 1171e5e00df..6c62e5e7b2e 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -22,7 +22,8 @@ import java.nio.ByteBuffer&lt;br/&gt;
 import java.nio.file.{Files, Paths}&lt;br/&gt;
 import java.util.Properties&lt;br/&gt;
 &lt;br/&gt;
-import kafka.common.KafkaException&lt;br/&gt;
+import org.apache.kafka.common.errors._&lt;br/&gt;
+import kafka.common.{OffsetsOutOfOrderException, UnexpectedAppendOffsetException, KafkaException}&lt;br/&gt;
 import kafka.log.Log.DeleteDirSuffix&lt;br/&gt;
 import kafka.server.epoch.{EpochEntry, LeaderEpochCache, LeaderEpochFileCache}&lt;br/&gt;
 import kafka.server.{BrokerTopicStats, FetchDataInfo, KafkaConfig, LogDirFailureChannel}&lt;br/&gt;
@@ -42,6 +43,7 @@ import org.junit.{After, Before, Test}
&lt;p&gt; import scala.collection.Iterable&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.&lt;/p&gt;
{ArrayBuffer, ListBuffer}
&lt;p&gt;+import org.scalatest.Assertions.&lt;/p&gt;
{assertThrows, intercept, withClue}

&lt;p&gt; class LogTest {&lt;br/&gt;
   var config: KafkaConfig = null&lt;br/&gt;
@@ -1885,13 +1887,72 @@ class LogTest &lt;/p&gt;
{
     assertTrue(&quot;Message payload should be null.&quot;, !head.hasValue)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IllegalArgumentException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+  @Test&lt;br/&gt;
   def testAppendWithOutOfOrderOffsetsThrowsException() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)++    val appendOffsets = Seq(0L, 1L, 3L, 2L, 4L)+    val buffer = ByteBuffer.allocate(512)+    for (offset &amp;lt;- appendOffsets) {
+      val builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, CompressionType.NONE,
+                                          TimestampType.LOG_APPEND_TIME, offset, mockTime.milliseconds(),
+                                          1L, 0, 0, false, 0)
+      builder.append(new SimpleRecord(&quot;key&quot;.getBytes, &quot;value&quot;.getBytes))
+      builder.close()
+    }+    buffer.flip()+    val memoryRecords = MemoryRecords.readableRecords(buffer)++    assertThrows[OffsetsOutOfOrderException] {
+      log.appendAsFollower(memoryRecords)
+    }+  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendBelowExpectedOffsetThrowsException() {&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val records = (0 until 2).map(id =&amp;gt; new SimpleRecord(id.toString.getBytes)).toArray&lt;br/&gt;
     records.foreach(record =&amp;gt; log.appendAsLeader(MemoryRecords.withRecords(CompressionType.NONE, record), leaderEpoch = 0))&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val invalidRecord = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(1.toString.getBytes))&lt;/li&gt;
	&lt;li&gt;log.appendAsFollower(invalidRecord)&lt;br/&gt;
+&lt;br/&gt;
+    val magicVals = Seq(RecordBatch.MAGIC_VALUE_V0, RecordBatch.MAGIC_VALUE_V1, RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
+    val compressionTypes = Seq(CompressionType.NONE, CompressionType.LZ4)&lt;br/&gt;
+    for (magic &amp;lt;- magicVals; compression &amp;lt;- compressionTypes) {&lt;br/&gt;
+      val invalidRecord = MemoryRecords.withRecords(magic, compression, new SimpleRecord(1.toString.getBytes))&lt;br/&gt;
+      withClue(s&quot;Magic=$magic, compressionType=$compression&quot;) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        assertThrows[UnexpectedAppendOffsetException] {
+          log.appendAsFollower(invalidRecord)
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendEmptyLogBelowLogStartOffsetThrowsException() {&lt;br/&gt;
+    createEmptyLogs(logDir, 7)&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
+    assertEquals(7L, log.logStartOffset)&lt;br/&gt;
+    assertEquals(7L, log.logEndOffset)&lt;br/&gt;
+&lt;br/&gt;
+    val firstOffset = 4L&lt;br/&gt;
+    val magicVals = Seq(RecordBatch.MAGIC_VALUE_V0, RecordBatch.MAGIC_VALUE_V1, RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
+    val compressionTypes = Seq(CompressionType.NONE, CompressionType.LZ4)&lt;br/&gt;
+    for (magic &amp;lt;- magicVals; compression &amp;lt;- compressionTypes) {&lt;br/&gt;
+      val batch = TestUtils.records(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                         new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+                                         new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),&lt;br/&gt;
+                                    magicValue = magic, codec = compression,&lt;br/&gt;
+                                    baseOffset = firstOffset)&lt;br/&gt;
+&lt;br/&gt;
+      withClue(s&quot;Magic=$magic, compressionType=$compression&quot;) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        val exception = intercept[UnexpectedAppendOffsetException] {
+          log.appendAsFollower(records = batch)
+        }+        assertEquals(s&amp;quot;Magic=$magic, compressionType=$compression, UnexpectedAppendOffsetException#firstOffset&amp;quot;,+                     firstOffset, exception.firstOffset)+        assertEquals(s&amp;quot;Magic=$magic, compressionType=$compression, UnexpectedAppendOffsetException#lastOffset&amp;quot;,+                     firstOffset + 2, exception.lastOffset)+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16512809" author="githubbot" created="Thu, 14 Jun 2018 17:39:45 +0000"  >&lt;p&gt;apovzner opened a new pull request #5229: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6975&quot; title=&quot;AdminClient.deleteRecords() may cause replicas unable to fetch from beginning&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6975&quot;&gt;&lt;del&gt;KAFKA-6975&lt;/del&gt;&lt;/a&gt;; Fix replica fetching from non-batch-aligned log start offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5229&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5229&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW.&lt;/p&gt;

&lt;p&gt;   Added two integration tests:&lt;br/&gt;
   1) Produce and then AdminClient#deleteRecords() while one of the followers is down, and then restart of the follower requires fetching from log start offset;&lt;br/&gt;
   2)  AdminClient#AlterReplicaLogDirs() after AdminClient#deleteRecords()&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16513030" author="githubbot" created="Thu, 14 Jun 2018 21:40:55 +0000"  >&lt;p&gt;hachikuji closed pull request #5229: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6975&quot; title=&quot;AdminClient.deleteRecords() may cause replicas unable to fetch from beginning&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6975&quot;&gt;&lt;del&gt;KAFKA-6975&lt;/del&gt;&lt;/a&gt;: Fix replica fetching from non-batch-aligned log start offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5229&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5229&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index 68faf00c079..9339d29202f 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -22,6 +22,7 @@ import java.util.concurrent.locks.ReentrantReadWriteLock&lt;br/&gt;
 import com.yammer.metrics.core.Gauge&lt;br/&gt;
 import kafka.api.LeaderAndIsr&lt;br/&gt;
 import kafka.api.Request&lt;br/&gt;
+import kafka.common.UnexpectedAppendOffsetException&lt;br/&gt;
 import kafka.controller.KafkaController&lt;br/&gt;
 import kafka.log.&lt;/p&gt;
{LogAppendInfo, LogConfig}
&lt;p&gt; import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
@@ -30,7 +31,7 @@ import kafka.utils.CoreUtils.&lt;/p&gt;
{inReadLock, inWriteLock}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
 import kafka.zk.AdminZkClient&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import org.apache.kafka.common.errors.&lt;/p&gt;
{NotEnoughReplicasException, NotLeaderForPartitionException, PolicyViolationException}
&lt;p&gt;+import org.apache.kafka.common.errors.&lt;/p&gt;
{ReplicaNotAvailableException, NotEnoughReplicasException, NotLeaderForPartitionException, PolicyViolationException}
&lt;p&gt; import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors._&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords&lt;br/&gt;
@@ -187,6 +188,10 @@ class Partition(val topic: String,&lt;/p&gt;

&lt;p&gt;   def getReplica(replicaId: Int = localBrokerId): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt; = Option(allReplicasMap.get(replicaId))&lt;/p&gt;

&lt;p&gt;+  def getReplicaOrException(replicaId: Int = localBrokerId): Replica =&lt;br/&gt;
+    getReplica(replicaId).getOrElse(&lt;br/&gt;
+      throw new ReplicaNotAvailableException(s&quot;Replica $replicaId is not available for partition $topicPartition&quot;))&lt;br/&gt;
+&lt;br/&gt;
   def leaderReplicaIfLocal: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt; =&lt;br/&gt;
     leaderReplicaIdOpt.filter(_ == localBrokerId).flatMap(getReplica)&lt;/p&gt;

&lt;p&gt;@@ -549,15 +554,41 @@ class Partition(val topic: String,&lt;br/&gt;
     laggingReplicas&lt;br/&gt;
   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def appendRecordsToFutureReplica(records: MemoryRecords) {&lt;/li&gt;
	&lt;li&gt;getReplica(Request.FutureLocalReplicaId).get.log.get.appendAsFollower(records)&lt;br/&gt;
+  private def doAppendRecordsToFollowerOrFutureReplica(records: MemoryRecords, isFuture: Boolean): Unit = {&lt;br/&gt;
+      if (isFuture)&lt;br/&gt;
+        getReplicaOrException(Request.FutureLocalReplicaId).log.get.appendAsFollower(records)&lt;br/&gt;
+      else 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        // The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread+        // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.+        inReadLock(leaderIsrUpdateLock) {
+           getReplicaOrException().log.get.appendAsFollower(records)
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def appendRecordsToFollower(records: MemoryRecords) {&lt;/li&gt;
	&lt;li&gt;// The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread&lt;/li&gt;
	&lt;li&gt;// is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.&lt;/li&gt;
	&lt;li&gt;inReadLock(leaderIsrUpdateLock) {&lt;/li&gt;
	&lt;li&gt;getReplica().get.log.get.appendAsFollower(records)&lt;br/&gt;
+  def appendRecordsToFollowerOrFutureReplica(records: MemoryRecords, isFuture: Boolean) {&lt;br/&gt;
+    try 
{
+      doAppendRecordsToFollowerOrFutureReplica(records, isFuture)
+    }
&lt;p&gt; catch {&lt;br/&gt;
+      case e: UnexpectedAppendOffsetException =&amp;gt;&lt;br/&gt;
+        val replica = if (isFuture) getReplicaOrException(Request.FutureLocalReplicaId) else getReplicaOrException()&lt;br/&gt;
+        val logEndOffset = replica.logEndOffset.messageOffset&lt;br/&gt;
+        if (logEndOffset == replica.logStartOffset &amp;amp;&amp;amp;&lt;br/&gt;
+            e.firstOffset &amp;lt; logEndOffset &amp;amp;&amp;amp; e.lastOffset &amp;gt;= logEndOffset) {&lt;br/&gt;
+          // This may happen if the log start offset on the leader (or current replica) falls in&lt;br/&gt;
+          // the middle of the batch due to delete records request and the follower tries to&lt;br/&gt;
+          // fetch its first offset from the leader.&lt;br/&gt;
+          // We handle this case here instead of Log#append() because we will need to remove the&lt;br/&gt;
+          // segment that start with log start offset and create a new one with earlier offset&lt;br/&gt;
+          // (base offset of the batch), which will move recoveryPoint backwards, so we will need&lt;br/&gt;
+          // to checkpoint the new recovery point before we append&lt;br/&gt;
+          val replicaName = if (isFuture) &quot;future replica&quot; else &quot;follower&quot;&lt;br/&gt;
+          info(s&quot;Unexpected offset in append to $topicPartition. First offset ${e.firstOffset} is less than log start offset ${replica.logStartOffset}.&quot; +&lt;br/&gt;
+               s&quot; Since this is the first record to be appended to the $replicaName&apos;s log, will start the log from offset ${e.firstOffset}.&quot;)&lt;br/&gt;
+          truncateFullyAndStartAt(e.firstOffset, isFuture)&lt;br/&gt;
+          doAppendRecordsToFollowerOrFutureReplica(records, isFuture)&lt;br/&gt;
+        } else&lt;br/&gt;
+          throw e&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala b/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..f8daaa4a181&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala&lt;br/&gt;
@@ -0,0 +1,25 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ * &lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates the follower received records with non-monotonically increasing offsets&lt;br/&gt;
+ */&lt;br/&gt;
+class OffsetsOutOfOrderException(message: String) extends RuntimeException(message) &lt;/p&gt;
{
+}&lt;br/&gt;
+&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala b/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..e719a93006d&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala&lt;br/&gt;
@@ -0,0 +1,29 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates the follower or the future replica received records from the leader (or current&lt;br/&gt;
+ * replica) with first offset less than expected next offset. &lt;br/&gt;
+ * @param firstOffset The first offset of the records to append&lt;br/&gt;
+ * @param lastOffset  The last offset of the records to append&lt;br/&gt;
+ */&lt;br/&gt;
+class UnexpectedAppendOffsetException(val message: String,&lt;br/&gt;
+                                      val firstOffset: Long,&lt;br/&gt;
+                                      val lastOffset: Long) extends RuntimeException(message) {+}
&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index 1e9ae4df404..8b62918bc97 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -24,7 +24,7 @@ import java.util.concurrent.atomic._&lt;br/&gt;
 import java.util.concurrent.&lt;/p&gt;
{ConcurrentNavigableMap, ConcurrentSkipListMap, TimeUnit}

&lt;p&gt; import kafka.api.KAFKA_0_10_0_IV0&lt;br/&gt;
-import kafka.common.&lt;/p&gt;
{InvalidOffsetException, KafkaException, LongRef}
&lt;p&gt;+import kafka.common.&lt;/p&gt;
{InvalidOffsetException, KafkaException, LongRef, UnexpectedAppendOffsetException, OffsetsOutOfOrderException}
&lt;p&gt; import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{BrokerTopicStats, FetchDataInfo, LogDirFailureChannel, LogOffsetMetadata}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
@@ -634,6 +634,8 @@ class Log(@volatile var dir: File,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param assignOffsets Should the log assign offsets to this message set or blindly apply what it is given&lt;/li&gt;
	&lt;li&gt;@param leaderEpoch The partition&apos;s leader epoch which will be applied to messages when offsets are assigned on the leader&lt;/li&gt;
	&lt;li&gt;@throws KafkaStorageException If the append fails due to an I/O error.&lt;br/&gt;
+   * @throws OffsetsOutOfOrderException If out of order offsets found in &apos;records&apos;&lt;br/&gt;
+   * @throws UnexpectedAppendOffsetException If the first or last offset in append is less than next offset&lt;/li&gt;
	&lt;li&gt;@return Information about the appended messages including the first and last offset.&lt;br/&gt;
    */&lt;br/&gt;
   private def append(records: MemoryRecords, isFromClient: Boolean, assignOffsets: Boolean, leaderEpoch: Int): LogAppendInfo = 
{
@@ -695,8 +697,24 @@ class Log(@volatile var dir: File,
           }
&lt;p&gt;         } else {&lt;br/&gt;
           // we are taking the offsets we are given&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!appendInfo.offsetsMonotonic || appendInfo.firstOffset &amp;lt; nextOffsetMetadata.messageOffset)&lt;/li&gt;
	&lt;li&gt;throw new IllegalArgumentException(&quot;Out of order offsets found in &quot; + records.records.asScala.map(_.offset))&lt;br/&gt;
+          if (!appendInfo.offsetsMonotonic)&lt;br/&gt;
+            throw new OffsetsOutOfOrderException(s&quot;Out of order offsets found in append to $topicPartition: &quot; +&lt;br/&gt;
+                                                 records.records.asScala.map(_.offset))&lt;br/&gt;
+&lt;br/&gt;
+          if (appendInfo.firstOffset &amp;lt; nextOffsetMetadata.messageOffset) {&lt;br/&gt;
+            // we may still be able to recover if the log is empty&lt;br/&gt;
+            // one example: fetching from log start offset on the leader which is not batch aligned,&lt;br/&gt;
+            // which may happen as a result of AdminClient#deleteRecords()&lt;br/&gt;
+            // appendInfo.firstOffset maybe either first offset or last offset of the first batch.&lt;br/&gt;
+            // get the actual first offset, which may require decompressing the data&lt;br/&gt;
+            val firstOffset = records.batches.asScala.head.baseOffset()&lt;br/&gt;
+            throw new UnexpectedAppendOffsetException(&lt;br/&gt;
+              s&quot;Unexpected offset in append to $topicPartition. First offset or last offset of the first batch &quot; +&lt;br/&gt;
+              s&quot;${appendInfo.firstOffset} is less than the next offset ${nextOffsetMetadata.messageOffset}. &quot; +&lt;br/&gt;
+              s&quot;First 10 offsets in append: ${records.records.asScala.take(10).map(_.offset)}, last offset in&quot; +&lt;br/&gt;
+              s&quot; append: ${appendInfo.lastOffset}. Log start offset = $logStartOffset&quot;,&lt;br/&gt;
+              firstOffset, appendInfo.lastOffset)&lt;br/&gt;
+          }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // update the epoch cache with the epoch stamped onto the message by the leader&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
index 48c83d43835..74ef3e848ed 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
@@ -103,7 +103,7 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
         topicPartition, fetchOffset, futureReplica.logEndOffset.messageOffset))&lt;/p&gt;

&lt;p&gt;     // Append the leader&apos;s messages to the log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;partition.appendRecordsToFutureReplica(records)&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(records, isFuture = true)&lt;br/&gt;
     futureReplica.highWatermark = new LogOffsetMetadata(partitionData.highWatermark)&lt;br/&gt;
     futureReplica.maybeIncrementLogStartOffset(partitionData.logStartOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -263,4 +263,4 @@ object ReplicaAlterLogDirsThread &lt;/p&gt;
{
 
     override def toString = underlying.toString
   }
&lt;p&gt;-}&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index fa45e7ff9e7..8f4a7569a0e 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -109,7 +109,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
         .format(replica.logEndOffset.messageOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))&lt;/p&gt;

&lt;p&gt;     // Append the leader&apos;s messages to the log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;partition.appendRecordsToFollower(records)&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(records, isFuture = false)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (isTraceEnabled)&lt;br/&gt;
       trace(&quot;Follower has replica log end offset %d after appending %d bytes of messages for partition %s&quot;&lt;br/&gt;
diff --git a/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala&lt;br/&gt;
index 867e03db852..5f7ae57f58b 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala&lt;br/&gt;
@@ -799,6 +799,83 @@ class AdminClientIntegrationTest extends IntegrationTestHarness with Logging &lt;/p&gt;
{
     client.close()
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testReplicaCanFetchFromLogStartOffsetAfterDeleteRecords(): Unit = {&lt;br/&gt;
+    val leaders = createTopic(topic, numPartitions = 1, replicationFactor = serverCount)&lt;br/&gt;
+    val followerIndex = if (leaders(0) != servers(0).config.brokerId) 0 else 1&lt;br/&gt;
+&lt;br/&gt;
+    def waitForFollowerLog(expectedStartOffset: Long, expectedEndOffset: Long): Unit = {&lt;br/&gt;
+      TestUtils.waitUntilTrue(() =&amp;gt; servers(followerIndex).replicaManager.getReplica(topicPartition) != None,&lt;br/&gt;
+                              &quot;Expected follower to create replica for partition&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      // wait until the follower discovers that log start offset moved beyond its HW&lt;br/&gt;
+      TestUtils.waitUntilTrue(() =&amp;gt; &lt;/p&gt;
{
+        servers(followerIndex).replicaManager.getReplica(topicPartition).get.logStartOffset == expectedStartOffset
+      }
&lt;p&gt;, s&quot;Expected follower to discover new log start offset $expectedStartOffset&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      TestUtils.waitUntilTrue(() =&amp;gt; &lt;/p&gt;
{
+        servers(followerIndex).replicaManager.getReplica(topicPartition).get.logEndOffset.messageOffset == expectedEndOffset
+      }
&lt;p&gt;, s&quot;Expected follower to catch up to log end offset $expectedEndOffset&quot;)&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // we will produce to topic and delete records while one follower is down&lt;br/&gt;
+    killBroker(followerIndex)&lt;br/&gt;
+&lt;br/&gt;
+    client = AdminClient.create(createConfig)&lt;br/&gt;
+    sendRecords(producers.head, 100, topicPartition)&lt;br/&gt;
+&lt;br/&gt;
+    val result = client.deleteRecords(Map(topicPartition -&amp;gt; RecordsToDelete.beforeOffset(3L)).asJava)&lt;br/&gt;
+    result.all().get()&lt;br/&gt;
+&lt;br/&gt;
+    // start the stopped broker to verify that it will be able to fetch from new log start offset&lt;br/&gt;
+    restartDeadBrokers()&lt;br/&gt;
+&lt;br/&gt;
+    waitForFollowerLog(expectedStartOffset=3L, expectedEndOffset=100L)&lt;br/&gt;
+&lt;br/&gt;
+    // after the new replica caught up, all replicas should have same log start offset&lt;br/&gt;
+    for (i &amp;lt;- 0 until serverCount)&lt;br/&gt;
+      assertEquals(3, servers&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.replicaManager.getReplica(topicPartition).get.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // kill the same follower again, produce more records, and delete records beyond follower&apos;s LOE&lt;br/&gt;
+    killBroker(followerIndex)&lt;br/&gt;
+    sendRecords(producers.head, 100, topicPartition)&lt;br/&gt;
+    val result1 = client.deleteRecords(Map(topicPartition -&amp;gt; RecordsToDelete.beforeOffset(117L)).asJava)&lt;br/&gt;
+    result1.all().get()&lt;br/&gt;
+    restartDeadBrokers()&lt;br/&gt;
+    waitForFollowerLog(expectedStartOffset=117L, expectedEndOffset=200L)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAlterLogDirsAfterDeleteRecords(): Unit = {&lt;br/&gt;
+    client = AdminClient.create(createConfig)&lt;br/&gt;
+    createTopic(topic, numPartitions = 1, replicationFactor = serverCount)&lt;br/&gt;
+    val expectedLEO = 100&lt;br/&gt;
+    sendRecords(producers.head, expectedLEO, topicPartition)&lt;br/&gt;
+&lt;br/&gt;
+    // delete records to move log start offset&lt;br/&gt;
+    val result = client.deleteRecords(Map(topicPartition -&amp;gt; RecordsToDelete.beforeOffset(3L)).asJava)&lt;br/&gt;
+    result.all().get()&lt;br/&gt;
+    // make sure we are in the expected state after delete records&lt;br/&gt;
+    for (i &amp;lt;- 0 until serverCount) &lt;/p&gt;
{
+      assertEquals(3, servers(i).replicaManager.getReplica(topicPartition).get.logStartOffset)
+      assertEquals(expectedLEO, servers(i).replicaManager.getReplica(topicPartition).get.logEndOffset.messageOffset)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // we will create another dir just for one server&lt;br/&gt;
+    val futureLogDir = servers(0).config.logDirs(1)&lt;br/&gt;
+    val futureReplica = new TopicPartitionReplica(topic, 0, servers(0).config.brokerId)&lt;br/&gt;
+&lt;br/&gt;
+    // Verify that replica can be moved to the specified log directory&lt;br/&gt;
+    client.alterReplicaLogDirs(Map(futureReplica -&amp;gt; futureLogDir).asJava).all.get&lt;br/&gt;
+    TestUtils.waitUntilTrue(() =&amp;gt; &lt;/p&gt;
{
+      futureLogDir == servers(0).logManager.getLog(topicPartition).get.dir.getParent
+    }
&lt;p&gt;, &quot;timed out waiting for replica movement&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    // once replica moved, its LSO and LEO should match other replicas&lt;br/&gt;
+    assertEquals(3, servers(0).replicaManager.getReplica(topicPartition).get.logStartOffset)&lt;br/&gt;
+    assertEquals(expectedLEO, servers(0).replicaManager.getReplica(topicPartition).get.logEndOffset.messageOffset)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testOffsetsForTimesAfterDeleteRecords(): Unit = {&lt;br/&gt;
     createTopic(topic, numPartitions = 2, replicationFactor = serverCount)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..fe5d578533b&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
@@ -0,0 +1,174 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *      &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package kafka.cluster&lt;br/&gt;
+&lt;br/&gt;
+import java.io.File&lt;br/&gt;
+import java.nio.ByteBuffer&lt;br/&gt;
+import java.util.Properties&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicBoolean&lt;br/&gt;
+&lt;br/&gt;
+import kafka.common.UnexpectedAppendOffsetException&lt;br/&gt;
+import kafka.log.&lt;/p&gt;
{Log, LogConfig, LogManager, CleanerConfig}
&lt;p&gt;+import kafka.server._&lt;br/&gt;
+import kafka.utils.&lt;/p&gt;
{MockTime, TestUtils, MockScheduler}
&lt;p&gt;+import kafka.utils.timer.MockTimer&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.errors.ReplicaNotAvailableException&lt;br/&gt;
+import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils&lt;br/&gt;
+import org.apache.kafka.common.record._&lt;br/&gt;
+import org.junit.&lt;/p&gt;
{After, Before, Test}&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
+import org.scalatest.Assertions.assertThrows&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class PartitionTest {&lt;br/&gt;
+&lt;br/&gt;
+  val brokerId = 101&lt;br/&gt;
+  val topicPartition = new TopicPartition(&quot;test-topic&quot;, 0)&lt;br/&gt;
+  val time = new MockTime()&lt;br/&gt;
+  val brokerTopicStats = new BrokerTopicStats&lt;br/&gt;
+  val metrics = new Metrics&lt;br/&gt;
+&lt;br/&gt;
+  var tmpDir: File = _&lt;br/&gt;
+  var logDir: File = _&lt;br/&gt;
+  var replicaManager: ReplicaManager = _&lt;br/&gt;
+  var logManager: LogManager = _&lt;br/&gt;
+  var logConfig: LogConfig = _&lt;br/&gt;
+&lt;br/&gt;
+  @Before&lt;br/&gt;
+  def setup(): Unit = {
+    val logProps = new Properties()
+    logProps.put(LogConfig.SegmentBytesProp, 512: java.lang.Integer)
+    logProps.put(LogConfig.SegmentIndexBytesProp, 1000: java.lang.Integer)
+    logProps.put(LogConfig.RetentionMsProp, 999: java.lang.Integer)
+    logConfig = LogConfig(logProps)
+
+    tmpDir = TestUtils.tempDir()
+    logDir = TestUtils.randomPartitionLogDir(tmpDir)
+    logManager = TestUtils.createLogManager(
+      logDirs = Seq(logDir), defaultConfig = logConfig, CleanerConfig(enableCleaner = false), time)
+    logManager.startup()
+
+    val brokerProps = TestUtils.createBrokerConfig(brokerId, TestUtils.MockZkConnect)
+    brokerProps.put(&quot;log.dir&quot;, logDir.getAbsolutePath)
+    val brokerConfig = KafkaConfig.fromProps(brokerProps)
+    replicaManager = new ReplicaManager(
+      config = brokerConfig, metrics, time, zkClient = null, new MockScheduler(time),
+      logManager, new AtomicBoolean(false), QuotaFactory.instantiate(brokerConfig, metrics, time, &quot;&quot;),
+      brokerTopicStats, new MetadataCache(brokerId), new LogDirFailureChannel(brokerConfig.logDirs.size))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @After&lt;br/&gt;
+  def tearDown(): Unit = {
+    brokerTopicStats.close()
+    metrics.close()
+
+    logManager.shutdown()
+    Utils.delete(tmpDir)
+    logManager.liveLogDirs.foreach(Utils.delete)
+    replicaManager.shutdown(checkpointHW = false)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendRecordsAsFollowerBelowLogStartOffset(): Unit = {&lt;br/&gt;
+    val log = logManager.getOrCreateLog(topicPartition, logConfig)&lt;br/&gt;
+    val replica = new Replica(brokerId, topicPartition, time, log = Some(log))&lt;br/&gt;
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+    partition.addReplicaIfNotExists(replica)&lt;br/&gt;
+    assertEquals(Some(replica), partition.getReplica(replica.brokerId))&lt;br/&gt;
+&lt;br/&gt;
+    val initialLogStartOffset = 5L&lt;br/&gt;
+    partition.truncateFullyAndStartAt(initialLogStartOffset, isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after truncate fully and start at $initialLogStartOffset:&quot;,&lt;br/&gt;
+                 initialLogStartOffset, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset after truncate fully and start at $initialLogStartOffset:&quot;,&lt;br/&gt;
+                 initialLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // verify that we cannot append records that do not contain log start offset even if the log is empty&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;UnexpectedAppendOffsetException&amp;#93;&lt;/span&gt; {
+      // append one record with offset = 3
+      partition.appendRecordsToFollowerOrFutureReplica(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 3L), isFuture = false)
+    }&lt;br/&gt;
+    assertEquals(s&quot;Log end offset should not change after failure to append&quot;, initialLogStartOffset, replica.logEndOffset.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // verify that we can append records that contain log start offset, even when first&lt;br/&gt;
+    // offset &amp;lt; log start offset if the log is empty&lt;br/&gt;
+    val newLogStartOffset = 4L&lt;br/&gt;
+    val records = createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                     new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+                                     new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),&lt;br/&gt;
+                                baseOffset = newLogStartOffset)&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(records, isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 3 records with base offset $newLogStartOffset:&quot;, 7L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset after append of 3 records with base offset $newLogStartOffset:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // and we can append more records after that&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 7L), isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 1 record at offset 7:&quot;, 8L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset not expected to change:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // but we cannot append to offset &amp;lt; log start if the log is not empty&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;UnexpectedAppendOffsetException&amp;#93;&lt;/span&gt; {
+      val records2 = createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),
+                                        new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)),
+                                   baseOffset = 3L)
+      partition.appendRecordsToFollowerOrFutureReplica(records2, isFuture = false)
+    }&lt;br/&gt;
+    assertEquals(s&quot;Log end offset should not change after failure to append&quot;, 8L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // we still can append to next offset&lt;br/&gt;
+    partition.appendRecordsToFollowerOrFutureReplica(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 8L), isFuture = false)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 1 record at offset 8:&quot;, 9L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset not expected to change:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testGetReplica(): Unit = {&lt;br/&gt;
+    val log = logManager.getOrCreateLog(topicPartition, logConfig)&lt;br/&gt;
+    val replica = new Replica(brokerId, topicPartition, time, log = Some(log))&lt;br/&gt;
+    val partition = new&lt;br/&gt;
+        Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    assertEquals(None, partition.getReplica(brokerId))&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaNotAvailableException&amp;#93;&lt;/span&gt; {
+      partition.getReplicaOrException(brokerId)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    partition.addReplicaIfNotExists(replica)&lt;br/&gt;
+    assertEquals(replica, partition.getReplicaOrException(brokerId))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendRecordsToFollowerWithNoReplicaThrowsException(): Unit = {&lt;br/&gt;
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaNotAvailableException&amp;#93;&lt;/span&gt; {
+      partition.appendRecordsToFollowerOrFutureReplica(
+           createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 0L), isFuture = false)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def createRecords(records: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;SimpleRecord&amp;#93;&lt;/span&gt;, baseOffset: Long, partitionLeaderEpoch: Int = 0): MemoryRecords = {
+    val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))
+    val builder = MemoryRecords.builder(
+      buf, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.LOG_APPEND_TIME,
+      baseOffset, time.milliseconds, partitionLeaderEpoch)
+    records.foreach(builder.append)
+    builder.build()
+  }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index 352be918b8d..cf679f63513 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -23,7 +23,7 @@ import java.nio.file.Files&lt;br/&gt;
 import java.util.Properties&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.kafka.common.errors._&lt;br/&gt;
-import kafka.common.KafkaException&lt;br/&gt;
+import kafka.common.{OffsetsOutOfOrderException, UnexpectedAppendOffsetException, KafkaException}&lt;br/&gt;
 import kafka.log.Log.DeleteDirSuffix&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.{After, Before, Test}
&lt;p&gt;@@ -41,6 +41,7 @@ import org.easymock.EasyMock&lt;/p&gt;

&lt;p&gt; import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.&lt;/p&gt;
{ArrayBuffer, ListBuffer}
&lt;p&gt;+import org.scalatest.Assertions.&lt;/p&gt;
{assertThrows, intercept, withClue}

&lt;p&gt; class LogTest {&lt;/p&gt;

&lt;p&gt;@@ -1885,13 +1886,72 @@ class LogTest &lt;/p&gt;
{
     assertTrue(&quot;Message payload should be null.&quot;, !head.hasValue)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IllegalArgumentException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+  @Test&lt;br/&gt;
   def testAppendWithOutOfOrderOffsetsThrowsException() {&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
+&lt;br/&gt;
+    val appendOffsets = Seq(0L, 1L, 3L, 2L, 4L)&lt;br/&gt;
+    val buffer = ByteBuffer.allocate(512)&lt;br/&gt;
+    for (offset &amp;lt;- appendOffsets) 
{
+      val builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, CompressionType.NONE,
+                                          TimestampType.LOG_APPEND_TIME, offset, mockTime.milliseconds(),
+                                          1L, 0, 0, false, 0)
+      builder.append(new SimpleRecord(&quot;key&quot;.getBytes, &quot;value&quot;.getBytes))
+      builder.close()
+    }
&lt;p&gt;+    buffer.flip()&lt;br/&gt;
+    val memoryRecords = MemoryRecords.readableRecords(buffer)&lt;br/&gt;
+&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;OffsetsOutOfOrderException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
+      log.appendAsFollower(memoryRecords)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendBelowExpectedOffsetThrowsException() {&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val records = (0 until 2).map(id =&amp;gt; new SimpleRecord(id.toString.getBytes)).toArray&lt;br/&gt;
     records.foreach(record =&amp;gt; log.appendAsLeader(MemoryRecords.withRecords(CompressionType.NONE, record), leaderEpoch = 0))&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val invalidRecord = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(1.toString.getBytes))&lt;/li&gt;
	&lt;li&gt;log.appendAsFollower(invalidRecord)&lt;br/&gt;
+&lt;br/&gt;
+    val magicVals = Seq(RecordBatch.MAGIC_VALUE_V0, RecordBatch.MAGIC_VALUE_V1, RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
+    val compressionTypes = Seq(CompressionType.NONE, CompressionType.LZ4)&lt;br/&gt;
+    for (magic &amp;lt;- magicVals; compression &amp;lt;- compressionTypes) {&lt;br/&gt;
+      val invalidRecord = MemoryRecords.withRecords(magic, compression, new SimpleRecord(1.toString.getBytes))&lt;br/&gt;
+      withClue(s&quot;Magic=$magic, compressionType=$compression&quot;) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        assertThrows[UnexpectedAppendOffsetException] {
+          log.appendAsFollower(invalidRecord)
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendEmptyLogBelowLogStartOffsetThrowsException() {&lt;br/&gt;
+    createEmptyLogs(logDir, 7)&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
+    assertEquals(7L, log.logStartOffset)&lt;br/&gt;
+    assertEquals(7L, log.logEndOffset)&lt;br/&gt;
+&lt;br/&gt;
+    val firstOffset = 4L&lt;br/&gt;
+    val magicVals = Seq(RecordBatch.MAGIC_VALUE_V0, RecordBatch.MAGIC_VALUE_V1, RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
+    val compressionTypes = Seq(CompressionType.NONE, CompressionType.LZ4)&lt;br/&gt;
+    for (magic &amp;lt;- magicVals; compression &amp;lt;- compressionTypes) {&lt;br/&gt;
+      val batch = TestUtils.records(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                         new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+                                         new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),&lt;br/&gt;
+                                    magicValue = magic, codec = compression,&lt;br/&gt;
+                                    baseOffset = firstOffset)&lt;br/&gt;
+&lt;br/&gt;
+      withClue(s&quot;Magic=$magic, compressionType=$compression&quot;) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        val exception = intercept[UnexpectedAppendOffsetException] {
+          log.appendAsFollower(records = batch)
+        }+        assertEquals(s&amp;quot;Magic=$magic, compressionType=$compression, UnexpectedAppendOffsetException#firstOffset&amp;quot;,+                     firstOffset, exception.firstOffset)+        assertEquals(s&amp;quot;Magic=$magic, compressionType=$compression, UnexpectedAppendOffsetException#lastOffset&amp;quot;,+                     firstOffset + 2, exception.lastOffset)+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16513317" author="githubbot" created="Fri, 15 Jun 2018 04:23:51 +0000"  >&lt;p&gt;apovzner opened a new pull request #5235: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6975&quot; title=&quot;AdminClient.deleteRecords() may cause replicas unable to fetch from beginning&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6975&quot;&gt;&lt;del&gt;KAFKA-6975&lt;/del&gt;&lt;/a&gt;; Fix replica fetching from non-batch-aligned log start offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5235&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5235&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW.&lt;/p&gt;

&lt;p&gt;   Added one integration test:&lt;br/&gt;
   1) Produce and then AdminClient#deleteRecords() while one of the followers is down, and then restart of the follower requires fetching from log start offset;&lt;/p&gt;

&lt;p&gt;   Reviewers: Ismael Juma &amp;lt;ismael@juma.me.uk&amp;gt;, Jun Rao &amp;lt;junrao@gmail.com&amp;gt;, Jason Gustafson &amp;lt;jason@confluent.io&amp;gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16514315" author="githubbot" created="Fri, 15 Jun 2018 20:26:35 +0000"  >&lt;p&gt;hachikuji closed pull request #5235: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6975&quot; title=&quot;AdminClient.deleteRecords() may cause replicas unable to fetch from beginning&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6975&quot;&gt;&lt;del&gt;KAFKA-6975&lt;/del&gt;&lt;/a&gt;; Fix replica fetching from non-batch-aligned log start offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5235&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5235&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index e038b5885b7..55edd10b5a3 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -22,6 +22,7 @@ import java.util.concurrent.locks.ReentrantReadWriteLock&lt;br/&gt;
 import com.yammer.metrics.core.Gauge&lt;br/&gt;
 import kafka.admin.AdminUtils&lt;br/&gt;
 import kafka.api.LeaderAndIsr&lt;br/&gt;
+import kafka.common.UnexpectedAppendOffsetException&lt;br/&gt;
 import kafka.controller.KafkaController&lt;br/&gt;
 import kafka.log.&lt;/p&gt;
{LogAppendInfo, LogConfig}
&lt;p&gt; import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
@@ -29,7 +30,7 @@ import kafka.server._&lt;br/&gt;
 import kafka.utils.CoreUtils.&lt;/p&gt;
{inReadLock, inWriteLock}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import org.apache.kafka.common.errors.&lt;/p&gt;
{NotEnoughReplicasException, NotLeaderForPartitionException, PolicyViolationException}
&lt;p&gt;+import org.apache.kafka.common.errors.&lt;/p&gt;
{ReplicaNotAvailableException, NotEnoughReplicasException, NotLeaderForPartitionException, PolicyViolationException}
&lt;p&gt; import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors._&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords&lt;br/&gt;
@@ -155,6 +156,10 @@ class Partition(val topic: String,&lt;/p&gt;

&lt;p&gt;   def getReplica(replicaId: Int = localBrokerId): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt; = Option(assignedReplicaMap.get(replicaId))&lt;/p&gt;

&lt;p&gt;+  def getReplicaOrException(replicaId: Int = localBrokerId): Replica =&lt;br/&gt;
+    getReplica(replicaId).getOrElse(&lt;br/&gt;
+      throw new ReplicaNotAvailableException(s&quot;Replica $replicaId is not available for partition $topicPartition&quot;))&lt;br/&gt;
+&lt;br/&gt;
   def leaderReplicaIfLocal: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt; =&lt;br/&gt;
     leaderReplicaIdOpt.filter(_ == localBrokerId).flatMap(getReplica)&lt;/p&gt;

&lt;p&gt;@@ -486,6 +491,31 @@ class Partition(val topic: String,&lt;br/&gt;
     laggingReplicas&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  def appendRecordsToFollower(records: MemoryRecords) {&lt;br/&gt;
+    try &lt;/p&gt;
{
+      getReplicaOrException().log.get.appendAsFollower(records)
+    }
&lt;p&gt; catch {&lt;br/&gt;
+      case e: UnexpectedAppendOffsetException =&amp;gt;&lt;br/&gt;
+        val replica = getReplicaOrException()&lt;br/&gt;
+        val logEndOffset = replica.logEndOffset.messageOffset&lt;br/&gt;
+        if (logEndOffset == replica.logStartOffset &amp;amp;&amp;amp;&lt;br/&gt;
+            e.firstOffset &amp;lt; logEndOffset &amp;amp;&amp;amp; e.lastOffset &amp;gt;= logEndOffset) {&lt;br/&gt;
+          // This may happen if the log start offset on the leader (or current replica) falls in&lt;br/&gt;
+          // the middle of the batch due to delete records request and the follower tries to&lt;br/&gt;
+          // fetch its first offset from the leader.&lt;br/&gt;
+          // We handle this case here instead of Log#append() because we will need to remove the&lt;br/&gt;
+          // segment that start with log start offset and create a new one with earlier offset&lt;br/&gt;
+          // (base offset of the batch), which will move recoveryPoint backwards, so we will need&lt;br/&gt;
+          // to checkpoint the new recovery point before we append&lt;br/&gt;
+          info(s&quot;Unexpected offset in append to $topicPartition. First offset ${e.firstOffset} is less than log start offset ${replica.logStartOffset}.&quot; +&lt;br/&gt;
+               s&quot; Since this is the first record to be appended to the follower&apos;s log, will start the log from offset ${e.firstOffset}.&quot;)&lt;br/&gt;
+          logManager.truncateFullyAndStartAt(topicPartition, e.firstOffset)&lt;br/&gt;
+          replica.log.get.appendAsFollower(records)&lt;br/&gt;
+        } else&lt;br/&gt;
+          throw e&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   def appendRecordsToLeader(records: MemoryRecords, isFromClient: Boolean, requiredAcks: Int = 0): LogAppendInfo = {&lt;br/&gt;
     val (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) {&lt;br/&gt;
       leaderReplicaIfLocal match {&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala b/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..f8daaa4a181&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala&lt;br/&gt;
@@ -0,0 +1,25 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ * &lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates the follower received records with non-monotonically increasing offsets&lt;br/&gt;
+ */&lt;br/&gt;
+class OffsetsOutOfOrderException(message: String) extends RuntimeException(message) &lt;/p&gt;
{
+}&lt;br/&gt;
+&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala b/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..e719a93006d&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala&lt;br/&gt;
@@ -0,0 +1,29 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates the follower or the future replica received records from the leader (or current&lt;br/&gt;
+ * replica) with first offset less than expected next offset. &lt;br/&gt;
+ * @param firstOffset The first offset of the records to append&lt;br/&gt;
+ * @param lastOffset  The last offset of the records to append&lt;br/&gt;
+ */&lt;br/&gt;
+class UnexpectedAppendOffsetException(val message: String,&lt;br/&gt;
+                                      val firstOffset: Long,&lt;br/&gt;
+                                      val lastOffset: Long) extends RuntimeException(message) {+}
&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index caa7bf5bfac..9157ee1fe1f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -24,7 +24,7 @@ import java.util.concurrent.atomic._&lt;br/&gt;
 import java.util.concurrent.&lt;/p&gt;
{ConcurrentNavigableMap, ConcurrentSkipListMap, TimeUnit}

&lt;p&gt; import kafka.api.KAFKA_0_10_0_IV0&lt;br/&gt;
-import kafka.common.&lt;/p&gt;
{InvalidOffsetException, KafkaException, LongRef}
&lt;p&gt;+import kafka.common.&lt;/p&gt;
{InvalidOffsetException, KafkaException, LongRef, UnexpectedAppendOffsetException, OffsetsOutOfOrderException}
&lt;p&gt; import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{BrokerTopicStats, FetchDataInfo, LogDirFailureChannel, LogOffsetMetadata}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
@@ -618,6 +618,8 @@ class Log(@volatile var dir: File,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param assignOffsets Should the log assign offsets to this message set or blindly apply what it is given&lt;/li&gt;
	&lt;li&gt;@param leaderEpoch The partition&apos;s leader epoch which will be applied to messages when offsets are assigned on the leader&lt;/li&gt;
	&lt;li&gt;@throws KafkaStorageException If the append fails due to an I/O error.&lt;br/&gt;
+   * @throws OffsetsOutOfOrderException If out of order offsets found in &apos;records&apos;&lt;br/&gt;
+   * @throws UnexpectedAppendOffsetException If the first or last offset in append is less than next offset&lt;/li&gt;
	&lt;li&gt;@return Information about the appended messages including the first and last offset.&lt;br/&gt;
    */&lt;br/&gt;
   private def append(records: MemoryRecords, isFromClient: Boolean, assignOffsets: Boolean, leaderEpoch: Int): LogAppendInfo = 
{
@@ -679,8 +681,24 @@ class Log(@volatile var dir: File,
           }
&lt;p&gt;         } else {&lt;br/&gt;
           // we are taking the offsets we are given&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!appendInfo.offsetsMonotonic || appendInfo.firstOffset &amp;lt; nextOffsetMetadata.messageOffset)&lt;/li&gt;
	&lt;li&gt;throw new IllegalArgumentException(&quot;Out of order offsets found in &quot; + records.records.asScala.map(_.offset))&lt;br/&gt;
+          if (!appendInfo.offsetsMonotonic)&lt;br/&gt;
+            throw new OffsetsOutOfOrderException(s&quot;Out of order offsets found in append to $topicPartition: &quot; +&lt;br/&gt;
+                                                 records.records.asScala.map(_.offset))&lt;br/&gt;
+&lt;br/&gt;
+          if (appendInfo.firstOffset &amp;lt; nextOffsetMetadata.messageOffset) {&lt;br/&gt;
+            // we may still be able to recover if the log is empty&lt;br/&gt;
+            // one example: fetching from log start offset on the leader which is not batch aligned,&lt;br/&gt;
+            // which may happen as a result of AdminClient#deleteRecords()&lt;br/&gt;
+            // appendInfo.firstOffset maybe either first offset or last offset of the first batch.&lt;br/&gt;
+            // get the actual first offset, which may require decompressing the data&lt;br/&gt;
+            val firstOffset = records.batches.asScala.head.baseOffset()&lt;br/&gt;
+            throw new UnexpectedAppendOffsetException(&lt;br/&gt;
+              s&quot;Unexpected offset in append to $topicPartition. First offset or last offset of the first batch &quot; +&lt;br/&gt;
+              s&quot;${appendInfo.firstOffset} is less than the next offset ${nextOffsetMetadata.messageOffset}. &quot; +&lt;br/&gt;
+              s&quot;First 10 offsets in append: ${records.records.asScala.take(10).map(_.offset)}, last offset in&quot; +&lt;br/&gt;
+              s&quot; append: ${appendInfo.lastOffset}. Log start offset = $logStartOffset&quot;,&lt;br/&gt;
+              firstOffset, appendInfo.lastOffset)&lt;br/&gt;
+          }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // update the epoch cache with the epoch stamped onto the message by the leader&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index 2fb04486a05..af5763a73d5 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -87,6 +87,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
   // process fetched data&lt;br/&gt;
   def processPartitionData(topicPartition: TopicPartition, fetchOffset: Long, partitionData: PartitionData) {&lt;br/&gt;
     val replica = replicaMgr.getReplicaOrException(topicPartition)&lt;br/&gt;
+    val partition = replicaMgr.getPartition(topicPartition).get&lt;br/&gt;
     val records = partitionData.toRecords&lt;/p&gt;

&lt;p&gt;     maybeWarnIfOversizedRecords(records, topicPartition)&lt;br/&gt;
@@ -99,7 +100,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
         .format(replica.logEndOffset.messageOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))&lt;/p&gt;

&lt;p&gt;     // Append the leader&apos;s messages to the log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;replica.log.get.appendAsFollower(records)&lt;br/&gt;
+    partition.appendRecordsToFollower(records)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (logger.isTraceEnabled)&lt;br/&gt;
       trace(&quot;Follower has replica log end offset %d after appending %d bytes of messages for partition %s&quot;&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..2798b5abe3f&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
@@ -0,0 +1,173 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *      &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package kafka.cluster&lt;br/&gt;
+&lt;br/&gt;
+import java.io.File&lt;br/&gt;
+import java.nio.ByteBuffer&lt;br/&gt;
+import java.util.Properties&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicBoolean&lt;br/&gt;
+&lt;br/&gt;
+import kafka.common.UnexpectedAppendOffsetException&lt;br/&gt;
+import kafka.log.&lt;/p&gt;
{LogConfig, LogManager, CleanerConfig}
&lt;p&gt;+import kafka.server._&lt;br/&gt;
+import kafka.utils.&lt;/p&gt;
{MockTime, TestUtils, MockScheduler}
&lt;p&gt;+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.errors.ReplicaNotAvailableException&lt;br/&gt;
+import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils&lt;br/&gt;
+import org.apache.kafka.common.record._&lt;br/&gt;
+import org.junit.&lt;/p&gt;
{After, Before, Test}&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
+import org.scalatest.Assertions.assertThrows&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class PartitionTest {&lt;br/&gt;
+&lt;br/&gt;
+  val brokerId = 101&lt;br/&gt;
+  val topicPartition = new TopicPartition(&quot;test-topic&quot;, 0)&lt;br/&gt;
+  val time = new MockTime()&lt;br/&gt;
+  val brokerTopicStats = new BrokerTopicStats&lt;br/&gt;
+  val metrics = new Metrics&lt;br/&gt;
+&lt;br/&gt;
+  var tmpDir: File = _&lt;br/&gt;
+  var logDir: File = _&lt;br/&gt;
+  var replicaManager: ReplicaManager = _&lt;br/&gt;
+  var logManager: LogManager = _&lt;br/&gt;
+  var logConfig: LogConfig = _&lt;br/&gt;
+&lt;br/&gt;
+  @Before&lt;br/&gt;
+  def setup(): Unit = {
+    val logProps = new Properties()
+    logProps.put(LogConfig.SegmentBytesProp, 512: java.lang.Integer)
+    logProps.put(LogConfig.SegmentIndexBytesProp, 1000: java.lang.Integer)
+    logProps.put(LogConfig.RetentionMsProp, 999: java.lang.Integer)
+    logConfig = LogConfig(logProps)
+
+    tmpDir = TestUtils.tempDir()
+    logDir = TestUtils.randomPartitionLogDir(tmpDir)
+    logManager = TestUtils.createLogManager(
+      logDirs = Seq(logDir), defaultConfig = logConfig, CleanerConfig(enableCleaner = false), time)
+    logManager.startup()
+
+    val brokerProps = TestUtils.createBrokerConfig(brokerId, TestUtils.MockZkConnect)
+    brokerProps.put(&quot;log.dir&quot;, logDir.getAbsolutePath)
+    val brokerConfig = KafkaConfig.fromProps(brokerProps)
+    replicaManager = new ReplicaManager(
+      config = brokerConfig, metrics, time, zkUtils = null, new MockScheduler(time),
+      logManager, new AtomicBoolean(false), QuotaFactory.instantiate(brokerConfig, metrics, time, &quot;&quot;).follower,
+      brokerTopicStats, new MetadataCache(brokerId), new LogDirFailureChannel(brokerConfig.logDirs.size))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @After&lt;br/&gt;
+  def tearDown(): Unit = {
+    brokerTopicStats.close()
+    metrics.close()
+
+    logManager.shutdown()
+    Utils.delete(tmpDir)
+    logManager.liveLogDirs.foreach(Utils.delete)
+    replicaManager.shutdown(checkpointHW = false)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendRecordsAsFollowerBelowLogStartOffset(): Unit = {&lt;br/&gt;
+    val log = logManager.getOrCreateLog(topicPartition, logConfig)&lt;br/&gt;
+    val replica = new Replica(brokerId, topicPartition, time, log = Some(log))&lt;br/&gt;
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+    partition.addReplicaIfNotExists(replica)&lt;br/&gt;
+    assertEquals(Some(replica), partition.getReplica(replica.brokerId))&lt;br/&gt;
+&lt;br/&gt;
+    val initialLogStartOffset = 5L&lt;br/&gt;
+    logManager.truncateFullyAndStartAt(topicPartition, initialLogStartOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after truncate fully and start at $initialLogStartOffset:&quot;,&lt;br/&gt;
+                 initialLogStartOffset, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset after truncate fully and start at $initialLogStartOffset:&quot;,&lt;br/&gt;
+                 initialLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // verify that we cannot append records that do not contain log start offset even if the log is empty&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;UnexpectedAppendOffsetException&amp;#93;&lt;/span&gt; {
+      // append one record with offset = 3
+      partition.appendRecordsToFollower(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 3L))
+    }&lt;br/&gt;
+    assertEquals(s&quot;Log end offset should not change after failure to append&quot;, initialLogStartOffset, replica.logEndOffset.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // verify that we can append records that contain log start offset, even when first&lt;br/&gt;
+    // offset &amp;lt; log start offset if the log is empty&lt;br/&gt;
+    val newLogStartOffset = 4L&lt;br/&gt;
+    val records = createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                     new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+                                     new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),&lt;br/&gt;
+                                baseOffset = newLogStartOffset)&lt;br/&gt;
+    partition.appendRecordsToFollower(records)&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 3 records with base offset $newLogStartOffset:&quot;, 7L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset after append of 3 records with base offset $newLogStartOffset:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // and we can append more records after that&lt;br/&gt;
+    partition.appendRecordsToFollower(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 7L))&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 1 record at offset 7:&quot;, 8L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset not expected to change:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // but we cannot append to offset &amp;lt; log start if the log is not empty&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;UnexpectedAppendOffsetException&amp;#93;&lt;/span&gt; {
+      val records2 = createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),
+                                        new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)),
+                                   baseOffset = 3L)
+      partition.appendRecordsToFollower(records2)
+    }&lt;br/&gt;
+    assertEquals(s&quot;Log end offset should not change after failure to append&quot;, 8L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // we still can append to next offset&lt;br/&gt;
+    partition.appendRecordsToFollower(createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 8L))&lt;br/&gt;
+    assertEquals(s&quot;Log end offset after append of 1 record at offset 8:&quot;, 9L, replica.logEndOffset.messageOffset)&lt;br/&gt;
+    assertEquals(s&quot;Log start offset not expected to change:&quot;, newLogStartOffset, replica.logStartOffset)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testGetReplica(): Unit = {&lt;br/&gt;
+    val log = logManager.getOrCreateLog(topicPartition, logConfig)&lt;br/&gt;
+    val replica = new Replica(brokerId, topicPartition, time, log = Some(log))&lt;br/&gt;
+    val partition = new&lt;br/&gt;
+        Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    assertEquals(None, partition.getReplica(brokerId))&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaNotAvailableException&amp;#93;&lt;/span&gt; {
+      partition.getReplicaOrException(brokerId)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    partition.addReplicaIfNotExists(replica)&lt;br/&gt;
+    assertEquals(replica, partition.getReplicaOrException(brokerId))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendRecordsToFollowerWithNoReplicaThrowsException(): Unit = {&lt;br/&gt;
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaNotAvailableException&amp;#93;&lt;/span&gt; {
+      partition.appendRecordsToFollower(
+           createRecords(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)), baseOffset = 0L))
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def createRecords(records: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;SimpleRecord&amp;#93;&lt;/span&gt;, baseOffset: Long, partitionLeaderEpoch: Int = 0): MemoryRecords = {
+    val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))
+    val builder = MemoryRecords.builder(
+      buf, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.LOG_APPEND_TIME,
+      baseOffset, time.milliseconds, partitionLeaderEpoch)
+    records.foreach(builder.append)
+    builder.build()
+  }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index 3c8b01e4473..41f6ab79de5 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -22,7 +22,7 @@ import java.nio.ByteBuffer&lt;br/&gt;
 import java.util.Properties&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.kafka.common.errors._&lt;br/&gt;
-import kafka.common.KafkaException&lt;br/&gt;
+import kafka.common.{OffsetsOutOfOrderException, UnexpectedAppendOffsetException, KafkaException}&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.{After, Before, Test}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
@@ -39,6 +39,7 @@ import org.easymock.EasyMock&lt;/p&gt;

&lt;p&gt; import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.&lt;/p&gt;
{ArrayBuffer, ListBuffer}
&lt;p&gt;+import org.scalatest.Assertions.&lt;/p&gt;
{assertThrows, intercept, withClue}

&lt;p&gt; class LogTest {&lt;/p&gt;

&lt;p&gt;@@ -1862,13 +1863,72 @@ class LogTest &lt;/p&gt;
{
     assertTrue(&quot;Message payload should be null.&quot;, !head.hasValue)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IllegalArgumentException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+  @Test&lt;br/&gt;
   def testAppendWithOutOfOrderOffsetsThrowsException() {&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
+&lt;br/&gt;
+    val appendOffsets = Seq(0L, 1L, 3L, 2L, 4L)&lt;br/&gt;
+    val buffer = ByteBuffer.allocate(512)&lt;br/&gt;
+    for (offset &amp;lt;- appendOffsets) 
{
+      val builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, CompressionType.NONE,
+                                          TimestampType.LOG_APPEND_TIME, offset, mockTime.milliseconds(),
+                                          1L, 0, 0, false, 0)
+      builder.append(new SimpleRecord(&quot;key&quot;.getBytes, &quot;value&quot;.getBytes))
+      builder.close()
+    }
&lt;p&gt;+    buffer.flip()&lt;br/&gt;
+    val memoryRecords = MemoryRecords.readableRecords(buffer)&lt;br/&gt;
+&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;OffsetsOutOfOrderException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
+      log.appendAsFollower(memoryRecords)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendBelowExpectedOffsetThrowsException() {&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val records = (0 until 2).map(id =&amp;gt; new SimpleRecord(id.toString.getBytes)).toArray&lt;br/&gt;
     records.foreach(record =&amp;gt; log.appendAsLeader(MemoryRecords.withRecords(CompressionType.NONE, record), leaderEpoch = 0))&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val invalidRecord = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(1.toString.getBytes))&lt;/li&gt;
	&lt;li&gt;log.appendAsFollower(invalidRecord)&lt;br/&gt;
+&lt;br/&gt;
+    val magicVals = Seq(RecordBatch.MAGIC_VALUE_V0, RecordBatch.MAGIC_VALUE_V1, RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
+    val compressionTypes = Seq(CompressionType.NONE, CompressionType.LZ4)&lt;br/&gt;
+    for (magic &amp;lt;- magicVals; compression &amp;lt;- compressionTypes) {&lt;br/&gt;
+      val invalidRecord = MemoryRecords.withRecords(magic, compression, new SimpleRecord(1.toString.getBytes))&lt;br/&gt;
+      withClue(s&quot;Magic=$magic, compressionType=$compression&quot;) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        assertThrows[UnexpectedAppendOffsetException] {
+          log.appendAsFollower(invalidRecord)
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testAppendEmptyLogBelowLogStartOffsetThrowsException() {&lt;br/&gt;
+    createEmptyLogs(logDir, 7)&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
+    assertEquals(7L, log.logStartOffset)&lt;br/&gt;
+    assertEquals(7L, log.logEndOffset)&lt;br/&gt;
+&lt;br/&gt;
+    val firstOffset = 4L&lt;br/&gt;
+    val magicVals = Seq(RecordBatch.MAGIC_VALUE_V0, RecordBatch.MAGIC_VALUE_V1, RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
+    val compressionTypes = Seq(CompressionType.NONE, CompressionType.LZ4)&lt;br/&gt;
+    for (magic &amp;lt;- magicVals; compression &amp;lt;- compressionTypes) {&lt;br/&gt;
+      val batch = TestUtils.records(List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                         new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+                                         new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),&lt;br/&gt;
+                                    magicValue = magic, codec = compression,&lt;br/&gt;
+                                    baseOffset = firstOffset)&lt;br/&gt;
+&lt;br/&gt;
+      withClue(s&quot;Magic=$magic, compressionType=$compression&quot;) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        val exception = intercept[UnexpectedAppendOffsetException] {
+          log.appendAsFollower(records = batch)
+        }+        assertEquals(s&amp;quot;Magic=$magic, compressionType=$compression, UnexpectedAppendOffsetException#firstOffset&amp;quot;,+                     firstOffset, exception.firstOffset)+        assertEquals(s&amp;quot;Magic=$magic, compressionType=$compression, UnexpectedAppendOffsetException#lastOffset&amp;quot;,+                     firstOffset + 2, exception.lastOffset)+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 22 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3ude7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>