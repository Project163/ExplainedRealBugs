<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:12:45 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-4682] Committed offsets should not be deleted if a consumer is still active (KIP-211)</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-4682</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Kafka will delete committed offsets that are older than offsets.retention.minutes&lt;/p&gt;

&lt;p&gt;If there is an active consumer on a low traffic partition, it is possible that Kafka will delete the committed offset for that consumer. Once the offset is deleted, a restart or a rebalance of that consumer will cause the consumer to not find any committed offset and start consuming from earliest/latest (depending on auto.offset.reset). I&apos;m not sure, but a broker failover might also cause you to start reading from auto.offset.reset (due to broker restart, or coordinator failover).&lt;/p&gt;

&lt;p&gt;I think that Kafka should only delete offsets for inactive consumers. The timer should only start after a consumer group goes inactive. For example, if a consumer group goes inactive, then after 1 week, delete the offsets for that consumer group. This is a solution that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; mentioned in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3806?focusedCommentId=15323521&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15323521&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3806?focusedCommentId=15323521&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15323521&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The current workarounds are to:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Commit an offset on every partition you own on a regular basis, making sure that it is more frequent than offsets.retention.minutes (a broker-side setting that a consumer might not be aware of)&lt;br/&gt;
or&lt;/li&gt;
	&lt;li&gt;Turn the value of offsets.retention.minutes up really really high. You have to make sure it is higher than any valid low-traffic rate that you want to support. For example, if you want to support a topic where someone produces once a month, you would have to set offsetes.retention.mintues to 1 month.&lt;br/&gt;
or&lt;/li&gt;
	&lt;li&gt;Turn on enable.auto.commit (this is essentially #1, but easier to implement).&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;None of these are ideal. &lt;/p&gt;

&lt;p&gt;#1 can be spammy. It requires your consumers know something about how the brokers are configured. Sometimes it is out of your control. Mirrormaker, for example, only commits offsets on partitions where it receives data. And it is duplication that you need to put into all of your consumers.&lt;/p&gt;

&lt;p&gt;#2 has disk-space impact on the broker (in __consumer_offsets) as well as memory-size on the broker (to answer OffsetFetch).&lt;/p&gt;

&lt;p&gt;#3 I think has the potential for message loss (the consumer might commit on messages that are not yet fully processed)&lt;/p&gt;</description>
                <environment></environment>
        <key id="13036747">KAFKA-4682</key>
            <summary>Committed offsets should not be deleted if a consumer is still active (KIP-211)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vahid">Vahid Hashemian</assignee>
                                    <reporter username="wushujames">James Cheng</reporter>
                        <labels>
                            <label>kip</label>
                    </labels>
                <created>Sat, 21 Jan 2017 00:53:03 +0000</created>
                <updated>Mon, 10 Sep 2018 00:16:44 +0000</updated>
                            <resolved>Wed, 27 Jun 2018 23:00:50 +0000</resolved>
                                                    <fixVersion>2.1.0</fixVersion>
                                        <due></due>
                            <votes>12</votes>
                                    <watches>24</watches>
                                                                                                                <comments>
                            <comment id="15832697" author="jeffwidman" created="Sat, 21 Jan 2017 01:06:19 +0000"  >&lt;p&gt;Now that consumers have background heartbeat thread, it should be much easier to identify when consumer dies vs alive. So this makes sense to me. However, this would make &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2000&quot; title=&quot;Delete consumer offsets from kafka once the topic is deleted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2000&quot;&gt;&lt;del&gt;KAFKA-2000&lt;/del&gt;&lt;/a&gt; more important because you can&apos;t count on offsets expiring.&lt;/p&gt;

&lt;p&gt;We also had a production problem where a couple of topics log files were totally cleared, but the offsets weren&apos;t cleared, so we had negative lag where consumer offset was higher than broker highwater. This was with zookeeper offset storage, but regardless I could envision something getting screwed up or someone resetting a cluster w/o understanding what they&apos;re doing and making offsets screwed up. If this was implemented those old offsets would never go away unless manually cleared up also. So I&apos;d want to make sure that&apos;s protected against somehow... like if a broker ever encounters consumer offset that&apos;s higher than highwater mark, either an exception is thrown or those consumer offsets get reset to the broker highwater mark. Probably safest to just throw an exception in case something else funky is going on.&lt;/p&gt;</comment>
                            <comment id="15845607" author="hachikuji" created="Mon, 30 Jan 2017 18:15:41 +0000"  >&lt;p&gt;I think the suggestion makes sense. If we did that, I wonder if it is still necessary to enforce each offset&apos;s retention time individually. It might make more sense to drop the retention_time field from the offset commit request (which we don&apos;t use in the new consumer anyway) and expire all offsets for the group at the same time. I&apos;m not sure if we would then need to provide another avenue to override the collective expiration time though (seems few people need it?). A change to the protocol would require a short KIP, so another way would be to continue allowing offset expiration individually, but don&apos;t start the timeout until the group is empty.&lt;/p&gt;

&lt;p&gt;One minor issue is the handling of simple consumers, but perhaps we can just let the expiration timer reset after every new offset commit.&lt;/p&gt;</comment>
                            <comment id="15846224" author="wushujames" created="Tue, 31 Jan 2017 00:45:30 +0000"  >&lt;p&gt;I think that if we decide to start the timer when the partition no longer has an active consumer, then we no longer need an expiration time on the individual commit.&lt;/p&gt;

&lt;p&gt;You said &quot;so another way would be to continue allowing offset expiration individually, but don&apos;t start the timeout until the group is empty&quot;. I&apos;m not sure that will work. According to the code and protocol doc, there is a commit time in the offset in the API call, as well as an expiration time (calculated upon receipt). And the expiration time says something like &quot;expire at so-and-so&quot; time. And there&apos;s no way to compare that time against the &quot;time the partition was last assigned to an active consumer&quot;.&lt;/p&gt;

&lt;p&gt;I suppose you &lt;b&gt;could&lt;/b&gt; take (expiration_time - commit_time) and use that to calculate a duration, and then start the timer for that duration when the partition loses an active consumer. That would work, but it&apos;d be very roundabout.&lt;/p&gt;

&lt;p&gt;What do you mean by &quot;expire all offsets for the group at the same time&quot;? And &quot;collective expiration time&quot;?&lt;/p&gt;</comment>
                            <comment id="15846479" author="hachikuji" created="Tue, 31 Jan 2017 07:10:57 +0000"  >&lt;p&gt;Yeah, the hack you mentioned is what I had in mind. We could also just change the message format so that instead of storing the commit expiration times, we only store the retention time. Obviously I would prefer the KIP route to take the retention time out of the OffsetCommit API (since it&apos;s not used anyway). Are you interested in taking this on?&lt;/p&gt;

&lt;p&gt;By &quot;collective expiration time,&quot; I was referring to the duration before which all offsets are expired. I&apos;m unsure whether users would want to be able to control this, or whether a single global config on the broker would be sufficient &lt;/p&gt;</comment>
                            <comment id="16113290" author="vahid" created="Thu, 3 Aug 2017 18:52:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; et al.&lt;br/&gt;
I&apos;m interested in taking this on. Just to make sure I understand the proper approach (KIP) before starting to write the draft:&lt;/p&gt;

&lt;p&gt;We don&apos;t want to expire group offsets while the group is active, so we would want to phase out individual offset expirations inside the group (that would mean removing the unused retention time field from the OffsetCommit protocol). On the other hand, we seem to now need some sort of an expiration time per consumer group so we can remove offsets within the group if that expiration time is passed and the group is no longer active. This expiration time is set and takes effect only when the group becomes empty.&lt;/p&gt;

&lt;p&gt;Is this a reasonable summary of what needs to happen?&lt;/p&gt;</comment>
                            <comment id="16120621" author="wushujames" created="Wed, 9 Aug 2017 20:38:46 +0000"  >&lt;p&gt;Yes, that&apos;s a reasonable summary.&lt;/p&gt;

&lt;p&gt;Other details to consider:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; asked whether we have a single broker-level config for the expiration time that applies to all groups, or if we need to allow individual consumer groups to specify their own expiration time. In the discussion for &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-186%3A+Increase+offsets+retention+default+to+7+days&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KIP-186&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; raised the same question. I&apos;m not sure.&lt;/li&gt;
	&lt;li&gt;Offsets are normally saved per (partition, groupId). Do we want to allow offsets to be expired for individual partitions separately from the group? As an example, say I have a groupId=&quot;foo&quot; that commits for (Topic A, partition 0) and (Topic B, partition 0). And then groupId stops subscribing to Topic B, and only subscribes to (Topic A, partition 0). Should the offset for (Topic B, partition 0) stay around as long as the group is active? Or, should it be expired, since it is not part of the group anymore?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16122498" author="vahid" created="Thu, 10 Aug 2017 23:05:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wushujames&quot; class=&quot;user-hover&quot; rel=&quot;wushujames&quot;&gt;wushujames&lt;/a&gt; Thanks for your feedback. Regarding the other details you brought up:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;&apos;s suggestion on &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-186%3A+Increase+offsets+retention+default+to+7+days&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KIP-186&lt;/a&gt; makes sense to me. The &lt;tt&gt;OffsetCommit&lt;/tt&gt; API can be used to override the default broker level property &lt;tt&gt;offset.retention.minutes&lt;/tt&gt; for specific group/topic/partitions. This means we probably wouldn&apos;t need to have a group-level retention config. What a potential KIP for this JIRA would be adding is that the retention timer kicks off at the moment the group becomes empty, and while the group is stable no offset will be removed (as retention timer is not ticking yet).&lt;/li&gt;
	&lt;li&gt;Regarding your second point, I guess we could pick either method. It all would depend on the criteria for triggering the retention timer for a partition. If we trigger it when the group is empty (as in the previous bullet) then we would be expiring the offset for &lt;tt&gt;B-0&lt;/tt&gt; with all other group partitions. If, on the other hand, we decide to trigger the timer when the partition stops being consumed within the group, then &lt;tt&gt;B-0&lt;/tt&gt;&apos;s offset could expire while the group is still active. I&apos;m not sure how common this scenario is in real applications. If it&apos;s not that common perhaps it wouldn&apos;t cost a lot to keep &lt;tt&gt;B-0&lt;/tt&gt;&apos;s offsets around with the rest of the group. In any case, we should be able to pick one approach or the other depending on what you and others believe is more reasonable.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What do you think? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;, what are your thoughts on this?&lt;/p&gt;</comment>
                            <comment id="16122580" author="hachikuji" created="Fri, 11 Aug 2017 00:15:27 +0000"  >&lt;p&gt;Sorry for the late response. There seem to be a few open questions:&lt;/p&gt;

&lt;p&gt;1. Does the consumer need the ability to control the retention timeout or is a broker config sufficient? I am not too sure about this. There is at least one use case (ConsoleConsumer) where we might intentionally set a low value, but I&apos;m not sure how bad it would be to let it stick with the default. It certainly would have been helpful prior to Ewen&apos;s KIP.&lt;/p&gt;

&lt;p&gt;2. Do we still need offset-level expiration or should we move it to the group? Personally, it feels a little odd to expire offsets at different times once a group is empty. It&apos;s a little more intuitive to expire them all at once. Another way to view this would be that we deprecate the offset retention setting and add a group metadata retention setting. Once the group has gone empty, we start its retention timer. If it expires, we clear all of its state including offsets. &lt;/p&gt;

&lt;p&gt;3. Do we need to change the format of the offset metadata messages? Currently the offset metadata that is stored in the log includes an expiration timestamp. This won&apos;t make much sense any more because we won&apos;t know what timestamp to use when the offset is first stored. While we&apos;re at it, we could probably also remove the commit timestamp and use the timestamp from the message itself. This also depends on the answer to the first question.&lt;/p&gt;

&lt;p&gt;4. Should we start the expiration timer for an individual offset if the group is no longer subscribed to the corresponding topic? My inclination is to keep it simple and say no, but I guess there is a risk that this tends to grow the cache more than existing behavior. If we&apos;re concerned about this, then we probably need to keep the individual offset expiration timer. Unfortunately because of the generic group protocol (which is also used in Connect), we don&apos;t currently have the ability to inspect subscriptions to know if a topic is still subscribed.&lt;/p&gt;
</comment>
                            <comment id="16123742" author="vahid" created="Fri, 11 Aug 2017 17:56:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; Thank you for your comments. You seem to be looking at this with an inclination to get rid of the retention time from the OffsetCommit protocol. I think with my comments below I&apos;m considering the alternative:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Ewen&apos;s KIP proposes to increase the default retention from 1 day to 7 days. So, allowing consumers to set a lower timeout (for the console consumer) seems to be helpful after his KIP; the same way allowing them to set a higher timeout (for actual consumer applications) is helpful before his KIP.&lt;/li&gt;
	&lt;li&gt;Even if we have offset-level expiration, all offsets in the group should expire together, because the expiration timer starts ticking for all partitions at the same time (when the group becomes empty). The only exception is when a consumer has set a non-default retention time for particular partitions (e.g. using the OffsetCommit API).&lt;/li&gt;
	&lt;li&gt;Agreed. The expiration timestamp won&apos;t make sense. Perhaps the retention time should be stored and whether to expire or not could be calculated on the fly from the time group becomes empty + retention time (we would need to somehow keep the timestamp of the group becoming empty). This expiration check needs to be performed only if the group is empty; otherwise there is no need to expire at all.&lt;/li&gt;
	&lt;li&gt;I don&apos;t have a strong feeling about this. It&apos;s for sure simpler to let all offsets expire at the same time. And if we keep the individual offset retention it would be easier to change this in case the cache size becomes an issue.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I think there is a risk involved in removing the individual retention from the protocol: could some requirement arise in the future that makes us bring it back to the protocol? One option is to let that field stay for now, and remove it later once we are more certain that it won&apos;t be needed back.&lt;/p&gt;</comment>
                            <comment id="16202763" author="vahid" created="Thu, 12 Oct 2017 22:59:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; I have started drafting a KIP for the changes discussed here. Could you please clarify what you mean by&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;... we could probably also remove the commit timestamp and use the timestamp from the message itself. ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I see that the commit timestamp is set to the time the request is processed (which supposedly is when the offset is committed). So I&apos;m not clear what you mean by &quot;timestamp from the message itself&quot;.&lt;br/&gt;
Thanks.&lt;/p&gt;</comment>
                            <comment id="16210323" author="vahid" created="Wed, 18 Oct 2017 23:47:34 +0000"  >&lt;p&gt;I just started a KIP discussion for this JIRA. The KIP can be found &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-211%3A+Revise+Expiration+Semantics+of+Consumer+Group+Offsets&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16238126" author="drew_kutchar" created="Fri, 3 Nov 2017 18:22:47 +0000"  >&lt;p&gt;This just happened to us and I just stumbled upon this JIRA while trying to figure out the cause. A few questions:&lt;/p&gt;

&lt;p&gt;1. Aren&apos;t consumer offset topics compacted? Shouldn&apos;t at least the last entry stay on disk after cleanup?&lt;/p&gt;

&lt;p&gt;2.  Considering that they are compacted, what is the real concern with workaround 2 in the description: &quot;2. Turn the value of offsets.retention.minutes up really really high&quot;?&lt;/p&gt;

&lt;p&gt;3. As a workaround, would it make sense to set &lt;tt&gt;offsets.retention.ms&lt;/tt&gt; to the same value as &lt;tt&gt;logs.retention.ms&lt;/tt&gt; and &lt;tt&gt;auto.offset.reset&lt;/tt&gt; to &lt;tt&gt;earliest&lt;/tt&gt;? That way consumers and logs would &quot;reset&quot; the same time?&lt;/p&gt;

&lt;p&gt;4. Is there a timeline for the release of KIP-211?&lt;/p&gt;</comment>
                            <comment id="16253655" author="jcrowley" created="Wed, 15 Nov 2017 15:42:58 +0000"  >&lt;p&gt;Just found this entry - had previously commented on &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3806&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3806&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is it possible to allow the offsets.retention.minutes to be set per groupId (in a similar way that retention.ms can be set per topic)?&lt;/p&gt;

&lt;p&gt;This would allow a fairly short default - 1 day as is current - to remove abandoned groupId metadata yet allow the user to indicate that a particular groupId should be handled differently. Example in 3806 was a PubSub using Kafka as a persistent, reliable store supporting multiple subscribers. Some of the source data has very low volatility - e.g. next year&apos;s holiday calendar for a company, which probably only changes once a year. A consumer must still poll in case an error update is posted, but will in the normal case not do a real commit for 12 months!&lt;/p&gt;</comment>
                            <comment id="16443661" author="githubbot" created="Thu, 19 Apr 2018 07:43:03 +0000"  >&lt;p&gt;vahidhashemian opened a new pull request #4896: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-4682&quot; title=&quot;Committed offsets should not be deleted if a consumer is still active (KIP-211)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-4682&quot;&gt;&lt;del&gt;KAFKA-4682&lt;/del&gt;&lt;/a&gt;: Revise expiration semantics of consumer group offsets (KIP-211)&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4896&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4896&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16452781" author="gsbiju" created="Wed, 25 Apr 2018 17:58:56 +0000"  >&lt;p&gt;This change will also help Kafka stream use cases where &lt;tt&gt;enable.auto.commit&lt;/tt&gt; is overridden to set it to &lt;tt&gt;false&lt;/tt&gt;. Currently we are seeing issues with partitions where there are no activities for periods more than the &lt;tt&gt;offsets.retention.minutes&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="16508140" author="rsivaram" created="Mon, 11 Jun 2018 14:31:17 +0000"  >&lt;p&gt;Moving this out to 2.1.0.&lt;/p&gt;</comment>
                            <comment id="16519864" author="githubbot" created="Fri, 22 Jun 2018 00:19:27 +0000"  >&lt;p&gt;hachikuji closed pull request #4896: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-4682&quot; title=&quot;Committed offsets should not be deleted if a consumer is still active (KIP-211)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-4682&quot;&gt;&lt;del&gt;KAFKA-4682&lt;/del&gt;&lt;/a&gt;: Revise expiration semantics of consumer group offsets (KIP-211 - Part 1)&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4896&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4896&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java&lt;br/&gt;
index 9c19af17037..b484e110aee 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java&lt;br/&gt;
@@ -798,8 +798,7 @@ public void onComplete(Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets, Exception&lt;/p&gt;

&lt;p&gt;         OffsetCommitRequest.Builder builder = new OffsetCommitRequest.Builder(this.groupId, offsetData).&lt;br/&gt;
                 setGenerationId(generation.generationId).&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;setMemberId(generation.memberId).&lt;/li&gt;
	&lt;li&gt;setRetentionTime(OffsetCommitRequest.DEFAULT_RETENTION_TIME);&lt;br/&gt;
+                setMemberId(generation.memberId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         log.trace(&quot;Sending OffsetCommit request with {} to coordinator {}&quot;, offsets, coordinator);&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java&lt;br/&gt;
index 570c4d5a66a..8a51e84e76a 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java&lt;br/&gt;
@@ -111,9 +111,15 @@&lt;br/&gt;
      */&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_REQUEST_V4 = OFFSET_COMMIT_REQUEST_V3;&lt;/p&gt;

&lt;p&gt;+    private static final Schema OFFSET_COMMIT_REQUEST_V5 = new Schema(&lt;br/&gt;
+            GROUP_ID,&lt;br/&gt;
+            GENERATION_ID,&lt;br/&gt;
+            MEMBER_ID,&lt;br/&gt;
+            new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_TOPIC_V2), &quot;Topics to commit offsets.&quot;));&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{OFFSET_COMMIT_REQUEST_V0, OFFSET_COMMIT_REQUEST_V1, OFFSET_COMMIT_REQUEST_V2,
-            OFFSET_COMMIT_REQUEST_V3, OFFSET_COMMIT_REQUEST_V4}
&lt;p&gt;;&lt;br/&gt;
+            OFFSET_COMMIT_REQUEST_V3, OFFSET_COMMIT_REQUEST_V4, OFFSET_COMMIT_REQUEST_V5};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     // default values for the current version&lt;br/&gt;
@@ -166,7 +172,6 @@ public String toString() {&lt;br/&gt;
         private final Map&amp;lt;TopicPartition, PartitionData&amp;gt; offsetData;&lt;br/&gt;
         private String memberId = DEFAULT_MEMBER_ID;&lt;br/&gt;
         private int generationId = DEFAULT_GENERATION_ID;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long retentionTime = DEFAULT_RETENTION_TIME;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         public Builder(String groupId, Map&amp;lt;TopicPartition, PartitionData&amp;gt; offsetData) {&lt;br/&gt;
             super(ApiKeys.OFFSET_COMMIT);&lt;br/&gt;
@@ -184,11 +189,6 @@ public Builder setGenerationId(int generationId) &lt;/p&gt;
{
             return this;
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Builder setRetentionTime(long retentionTime) 
{
-            this.retentionTime = retentionTime;
-            return this;
-        }
&lt;p&gt;-&lt;br/&gt;
         @Override&lt;br/&gt;
         public OffsetCommitRequest build(short version) {&lt;br/&gt;
             switch (version) {&lt;br/&gt;
@@ -199,8 +199,8 @@ public OffsetCommitRequest build(short version) &lt;/p&gt;
{
                 case 2:
                 case 3:
                 case 4:
-                    long retentionTime = version == 1 ? DEFAULT_RETENTION_TIME : this.retentionTime;
-                    return new OffsetCommitRequest(groupId, generationId, memberId, retentionTime, offsetData, version);
+                case 5:
+                    return new OffsetCommitRequest(groupId, generationId, memberId, DEFAULT_RETENTION_TIME, offsetData, version);
                 default:
                     throw new UnsupportedVersionException(&quot;Unsupported version &quot; + version);
             }
&lt;p&gt;@@ -213,7 +213,6 @@ public String toString() {&lt;br/&gt;
                 append(&quot;, groupId=&quot;).append(groupId).&lt;br/&gt;
                 append(&quot;, memberId=&quot;).append(memberId).&lt;br/&gt;
                 append(&quot;, generationId=&quot;).append(generationId).&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;append(&quot;, retentionTime=&quot;).append(retentionTime).&lt;br/&gt;
                 append(&quot;, offsetData=&quot;).append(offsetData).&lt;br/&gt;
                 append(&quot;)&quot;);&lt;br/&gt;
             return bld.toString();&lt;br/&gt;
@@ -316,6 +315,7 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {&lt;br/&gt;
                 return new OffsetCommitResponse(responseData);&lt;br/&gt;
             case 3:&lt;br/&gt;
             case 4:&lt;br/&gt;
+            case 5:&lt;br/&gt;
                 return new OffsetCommitResponse(throttleTimeMs, responseData);&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalArgumentException(String.format(&quot;Version %d is not valid. Valid versions for %s are 0 to %d&quot;,&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java&lt;br/&gt;
index 0b0b2283818..c79bc57885b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java&lt;br/&gt;
@@ -85,9 +85,11 @@&lt;br/&gt;
      */&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_RESPONSE_V4 = OFFSET_COMMIT_RESPONSE_V3;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private static final Schema OFFSET_COMMIT_RESPONSE_V5 = OFFSET_COMMIT_RESPONSE_V4;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{OFFSET_COMMIT_RESPONSE_V0, OFFSET_COMMIT_RESPONSE_V1, OFFSET_COMMIT_RESPONSE_V2,
-            OFFSET_COMMIT_RESPONSE_V3, OFFSET_COMMIT_RESPONSE_V4}
&lt;p&gt;;&lt;br/&gt;
+            OFFSET_COMMIT_RESPONSE_V3, OFFSET_COMMIT_RESPONSE_V4, OFFSET_COMMIT_RESPONSE_V5};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private final Map&amp;lt;TopicPartition, Errors&amp;gt; responseData;&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java b/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
index 6e705d22210..da613982816 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
@@ -141,9 +141,6 @@ public void testSerialization() throws Exception {&lt;br/&gt;
         checkErrorResponse(createMetadataRequest(3, singletonList(&quot;topic1&quot;)), new UnknownServerException());&lt;br/&gt;
         checkResponse(createMetadataResponse(), 4);&lt;br/&gt;
         checkErrorResponse(createMetadataRequest(4, singletonList(&quot;topic1&quot;)), new UnknownServerException());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;checkRequest(createOffsetCommitRequest(2));&lt;/li&gt;
	&lt;li&gt;checkErrorResponse(createOffsetCommitRequest(2), new UnknownServerException());&lt;/li&gt;
	&lt;li&gt;checkResponse(createOffsetCommitResponse(), 0);&lt;br/&gt;
         checkRequest(OffsetFetchRequest.forAllPartitions(&quot;group1&quot;));&lt;br/&gt;
         checkErrorResponse(OffsetFetchRequest.forAllPartitions(&quot;group1&quot;), new NotCoordinatorException(&quot;Not Coordinator&quot;));&lt;br/&gt;
         checkRequest(createOffsetFetchRequest(0));&lt;br/&gt;
@@ -210,6 +207,16 @@ public void testSerialization() throws Exception {&lt;br/&gt;
         checkErrorResponse(createOffsetCommitRequest(0), new UnknownServerException());&lt;br/&gt;
         checkRequest(createOffsetCommitRequest(1));&lt;br/&gt;
         checkErrorResponse(createOffsetCommitRequest(1), new UnknownServerException());&lt;br/&gt;
+        checkRequest(createOffsetCommitRequest(2));&lt;br/&gt;
+        checkErrorResponse(createOffsetCommitRequest(2), new UnknownServerException());&lt;br/&gt;
+        checkRequest(createOffsetCommitRequest(3));&lt;br/&gt;
+        checkErrorResponse(createOffsetCommitRequest(3), new UnknownServerException());&lt;br/&gt;
+        checkRequest(createOffsetCommitRequest(4));&lt;br/&gt;
+        checkErrorResponse(createOffsetCommitRequest(4), new UnknownServerException());&lt;br/&gt;
+        checkResponse(createOffsetCommitResponse(), 4);&lt;br/&gt;
+        checkRequest(createOffsetCommitRequest(5));&lt;br/&gt;
+        checkErrorResponse(createOffsetCommitRequest(5), new UnknownServerException());&lt;br/&gt;
+        checkResponse(createOffsetCommitResponse(), 5);&lt;br/&gt;
         checkRequest(createJoinGroupRequest(0));&lt;br/&gt;
         checkRequest(createUpdateMetadataRequest(0, null));&lt;br/&gt;
         checkErrorResponse(createUpdateMetadataRequest(0, null), new UnknownServerException());&lt;br/&gt;
@@ -817,7 +824,6 @@ private OffsetCommitRequest createOffsetCommitRequest(int version) 
{
         return new OffsetCommitRequest.Builder(&quot;group1&quot;, commitData)
                 .setGenerationId(100)
                 .setMemberId(&quot;consumer1&quot;)
-                .setRetentionTime(1000000)
                 .build((short) version);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/api/ApiVersion.scala b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
index 9ed6432cbfd..485f2bdce31 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
@@ -73,7 +73,9 @@ object ApiVersion {&lt;br/&gt;
     // Introduced OffsetsForLeaderEpochRequest V1 via KIP-279&lt;br/&gt;
     KAFKA_2_0_IV0,&lt;br/&gt;
     // Introduced ApiVersionsRequest V2 via KIP-219&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KAFKA_2_0_IV1&lt;br/&gt;
+    KAFKA_2_0_IV1,&lt;br/&gt;
+    // Introduced new schemas for group offset (v2) and group metadata (v2) (KIP-211)&lt;br/&gt;
+    KAFKA_2_1_IV0&lt;br/&gt;
   )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   // Map keys are the union of the short and full versions&lt;br/&gt;
@@ -248,4 +250,11 @@ case object KAFKA_2_0_IV1 extends DefaultApiVersion &lt;/p&gt;
{
   val subVersion = &quot;IV1&quot;
   val recordVersion = RecordVersion.V2
   val id: Int = 16
+}
&lt;p&gt;+&lt;br/&gt;
+case object KAFKA_2_1_IV0 extends DefaultApiVersion &lt;/p&gt;
{
+  val shortVersion: String = &quot;2.1&quot;
+  val subVersion = &quot;IV0&quot;
+  val recordVersion = RecordVersion.V2
+  val id: Int = 18
 }
&lt;p&gt;\ No newline at end of file&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/common/OffsetMetadataAndError.scala b/core/src/main/scala/kafka/common/OffsetMetadataAndError.scala&lt;br/&gt;
index 2cf9bb40eec..afe542c29c3 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/common/OffsetMetadataAndError.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/OffsetMetadataAndError.scala&lt;br/&gt;
@@ -34,17 +34,17 @@ object OffsetMetadata {&lt;/p&gt;

&lt;p&gt; case class OffsetAndMetadata(offsetMetadata: OffsetMetadata,&lt;br/&gt;
                              commitTimestamp: Long = org.apache.kafka.common.requests.OffsetCommitRequest.DEFAULT_TIMESTAMP,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expireTimestamp: Long = org.apache.kafka.common.requests.OffsetCommitRequest.DEFAULT_TIMESTAMP) {&lt;br/&gt;
+                             expireTimestamp: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def offset = offsetMetadata.offset&lt;/p&gt;

&lt;p&gt;   def metadata = offsetMetadata.metadata&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def toString = &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;%s,CommitTime %d,ExpirationTime %d&amp;#93;&lt;/span&gt;&quot;.format(offsetMetadata, commitTimestamp, expireTimestamp)&lt;br/&gt;
+  override def toString = s&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;$offsetMetadata,CommitTime $commitTimestamp,ExpirationTime ${expireTimestamp.getOrElse(&amp;quot;_&amp;quot;)}&amp;#93;&lt;/span&gt;&quot;&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; object OffsetAndMetadata {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def apply(offset: Long, metadata: String, commitTimestamp: Long, expireTimestamp: Long) = new OffsetAndMetadata(OffsetMetadata(offset, metadata), commitTimestamp, expireTimestamp)&lt;br/&gt;
+  def apply(offset: Long, metadata: String, commitTimestamp: Long, expireTimestamp: Long) = new OffsetAndMetadata(OffsetMetadata(offset, metadata), commitTimestamp, Some(expireTimestamp))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def apply(offset: Long, metadata: String, timestamp: Long) = new OffsetAndMetadata(OffsetMetadata(offset, metadata), timestamp)&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala b/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala&lt;br/&gt;
index 9748e174c78..2cedacd7200 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala&lt;br/&gt;
@@ -125,7 +125,7 @@ class GroupCoordinator(val brokerId: Int,&lt;br/&gt;
           if (memberId != JoinGroupRequest.UNKNOWN_MEMBER_ID) &lt;/p&gt;
{
             responseCallback(joinError(memberId, Errors.UNKNOWN_MEMBER_ID))
           }
&lt;p&gt; else &lt;/p&gt;
{
-            val group = groupManager.addGroup(new GroupMetadata(groupId, initialState = Empty))
+            val group = groupManager.addGroup(new GroupMetadata(groupId, Empty, time))
             doJoinGroup(group, memberId, clientId, clientHost, rebalanceTimeoutMs, sessionTimeoutMs, protocolType, protocols, responseCallback)
           }

&lt;p&gt;@@ -451,7 +451,7 @@ class GroupCoordinator(val brokerId: Int,&lt;br/&gt;
       case Some(error) =&amp;gt; responseCallback(offsetMetadata.mapValues(_ =&amp;gt; error))&lt;br/&gt;
       case None =&amp;gt;&lt;br/&gt;
         val group = groupManager.getGroup(groupId).getOrElse &lt;/p&gt;
{
-          groupManager.addGroup(new GroupMetadata(groupId, initialState = Empty))
+          groupManager.addGroup(new GroupMetadata(groupId, Empty, time))
         }
&lt;p&gt;         doCommitOffsets(group, NoMemberId, NoGeneration, producerId, producerEpoch, offsetMetadata, responseCallback)&lt;br/&gt;
     }&lt;br/&gt;
@@ -469,7 +469,7 @@ class GroupCoordinator(val brokerId: Int,&lt;br/&gt;
           case None =&amp;gt;&lt;br/&gt;
             if (generationId &amp;lt; 0) &lt;/p&gt;
{
               // the group is not relying on Kafka for group management, so allow the commit
-              val group = groupManager.addGroup(new GroupMetadata(groupId, initialState = Empty))
+              val group = groupManager.addGroup(new GroupMetadata(groupId, Empty, time))
               doCommitOffsets(group, memberId, generationId, NO_PRODUCER_ID, NO_PRODUCER_EPOCH,
                 offsetMetadata, responseCallback)
             }
&lt;p&gt; else {&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala b/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala&lt;br/&gt;
index 2b9c91f61b7..d729449af4e 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala&lt;br/&gt;
@@ -22,6 +22,7 @@ import java.util.concurrent.locks.ReentrantLock&lt;br/&gt;
 import kafka.common.OffsetAndMetadata&lt;br/&gt;
 import kafka.utils.&lt;/p&gt;
{CoreUtils, Logging, nonthreadsafe}
&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.utils.Time&lt;/p&gt;

&lt;p&gt; import scala.collection.&lt;/p&gt;
{Seq, immutable, mutable}

&lt;p&gt;@@ -118,12 +119,15 @@ private object GroupMetadata {&lt;br/&gt;
                 protocolType: String,&lt;br/&gt;
                 protocol: String,&lt;br/&gt;
                 leaderId: String,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;members: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;MemberMetadata&amp;#93;&lt;/span&gt;): GroupMetadata = {&lt;/li&gt;
	&lt;li&gt;val group = new GroupMetadata(groupId, initialState)&lt;br/&gt;
+                currentStateTimestamp: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                members: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;MemberMetadata&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                time: Time): GroupMetadata = 
{
+    val group = new GroupMetadata(groupId, initialState, time)
     group.generationId = generationId
     group.protocolType = if (protocolType == null || protocolType.isEmpty) None else Some(protocolType)
     group.protocol = Option(protocol)
     group.leaderId = Option(leaderId)
+    group.currentStateTimestamp = currentStateTimestamp
     members.foreach(group.add)
     group
   }
&lt;p&gt;@@ -167,10 +171,11 @@ case class CommitRecordMetadataAndOffset(appendedBatchOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;, offs&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;3. leader id&lt;br/&gt;
  */&lt;br/&gt;
 @nonthreadsafe&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; class GroupMetadata(val groupId: String, initialState: GroupState) extends Logging {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; class GroupMetadata(val groupId: String, initialState: GroupState, time: Time) extends Logging {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; val lock = new ReentrantLock&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private var state: GroupState = initialState&lt;br/&gt;
+  var currentStateTimestamp: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = Some(time.milliseconds())&lt;br/&gt;
   var protocolType: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
   var generationId = 0&lt;br/&gt;
   private var leaderId: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
@@ -195,6 +200,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; class GroupMetadata(val groupId: String, initialState: GroupState&lt;br/&gt;
   def isLeader(memberId: String): Boolean = leaderId.contains(memberId)&lt;br/&gt;
   def leaderOrNull: String = leaderId.orNull&lt;br/&gt;
   def protocolOrNull: String = protocol.orNull&lt;br/&gt;
+  def currentStateTimestampOrDefault: Long = currentStateTimestamp.getOrElse(-1)&lt;/p&gt;

&lt;p&gt;   def add(member: MemberMetadata) {&lt;br/&gt;
     if (members.isEmpty)&lt;br/&gt;
@@ -240,6 +246,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; class GroupMetadata(val groupId: String, initialState: GroupState&lt;br/&gt;
   def transitionTo(groupState: GroupState) &lt;/p&gt;
{
     assertValidTransition(groupState)
     state = groupState
+    currentStateTimestamp = Some(time.milliseconds())
   }

&lt;p&gt;   def selectProtocol: String = &lt;/p&gt;
{
@@ -434,18 +441,51 @@ private[group] class GroupMetadata(val groupId: String, initialState: GroupState
     }
&lt;p&gt;.toMap&lt;br/&gt;
   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def removeExpiredOffsets(startMs: Long) : Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetAndMetadata&amp;#93;&lt;/span&gt; = {&lt;/li&gt;
	&lt;li&gt;val expiredOffsets = offsets&lt;/li&gt;
	&lt;li&gt;.filter {&lt;br/&gt;
+  def removeExpiredOffsets(currentTimestamp: Long, offsetRetentionMs: Long) : Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetAndMetadata&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+&lt;br/&gt;
+    def getExpiredOffsets(baseTimestamp: CommitRecordMetadataAndOffset =&amp;gt; Long): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetAndMetadata&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+      offsets.filter 
{
         case (topicPartition, commitRecordMetadataAndOffset) =&amp;gt;
-          commitRecordMetadataAndOffset.offsetAndMetadata.expireTimestamp &amp;lt; startMs &amp;amp;&amp;amp; !pendingOffsetCommits.contains(topicPartition)
-      }&lt;/li&gt;
	&lt;li&gt;.map {&lt;br/&gt;
+          !pendingOffsetCommits.contains(topicPartition) &amp;amp;&amp;amp; 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            commitRecordMetadataAndOffset.offsetAndMetadata.expireTimestamp match {
+              case None =&amp;gt;
+                // current version with no per partition retention
+                currentTimestamp - baseTimestamp(commitRecordMetadataAndOffset) &amp;gt;= offsetRetentionMs
+              case Some(expireTimestamp) =&amp;gt;
+                // older versions with explicit expire_timestamp field =&amp;gt; old expiration semantics is used
+                currentTimestamp &amp;gt;= expireTimestamp
+            }+          }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+      }.map &lt;/p&gt;
{
         case (topicPartition, commitRecordOffsetAndMetadata) =&amp;gt;
           (topicPartition, commitRecordOffsetAndMetadata.offsetAndMetadata)
-      }
&lt;p&gt;+      }.toMap&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    val expiredOffsets: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetAndMetadata&amp;#93;&lt;/span&gt; = protocolType match &lt;/p&gt;
{
+      case Some(_) if is(Empty) =&amp;gt;
+        // no consumer exists in the group =&amp;gt;
+        // - if current state timestamp exists and retention period has passed since group became Empty,
+        //   expire all offsets with no pending offset commit;
+        // - if there is no current state timestamp (old group metadata schema) and retention period has passed
+        //   since the last commit timestamp, expire the offset
+        getExpiredOffsets(commitRecordMetadataAndOffset =&amp;gt;
+          currentStateTimestamp.getOrElse(commitRecordMetadataAndOffset.offsetAndMetadata.commitTimestamp))
+
+      case None =&amp;gt;
+        // protocolType is None =&amp;gt; standalone (simple) consumer, that uses Kafka for offset storage only
+        // expire offsets with no pending offset commit that retention period has passed since their last commit
+        getExpiredOffsets(_.offsetAndMetadata.commitTimestamp)
+
+      case _ =&amp;gt;
+        Map()
+    }
&lt;p&gt;+&lt;br/&gt;
+    if (expiredOffsets.nonEmpty)&lt;br/&gt;
+      debug(s&quot;Expired offsets from group &apos;$groupId&apos;: ${expiredOffsets.keySet}&quot;)&lt;br/&gt;
+&lt;br/&gt;
     offsets --= expiredOffsets.keySet&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;expiredOffsets.toMap&lt;br/&gt;
+    expiredOffsets&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def allOffsets = offsets.map { case (topicPartition, commitRecordMetadataAndOffset) =&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
index 02ba13a72b6..6bd0a5a0d52 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
@@ -25,7 +25,7 @@ import java.util.concurrent.atomic.AtomicBoolean&lt;br/&gt;
 import java.util.concurrent.locks.ReentrantLock&lt;/p&gt;

&lt;p&gt; import com.yammer.metrics.core.Gauge&lt;br/&gt;
-import kafka.api.&lt;/p&gt;
{ApiVersion, KAFKA_0_10_1_IV0}
&lt;p&gt;+import kafka.api.&lt;/p&gt;
{ApiVersion, KAFKA_0_10_1_IV0, KAFKA_2_1_IV0}
&lt;p&gt; import kafka.common.&lt;/p&gt;
{MessageFormatter, OffsetAndMetadata}
&lt;p&gt; import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.server.ReplicaManager&lt;br/&gt;
@@ -197,18 +197,11 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
                  responseCallback: Errors =&amp;gt; Unit): Unit = {&lt;br/&gt;
     getMagic(partitionFor(group.groupId)) match {&lt;br/&gt;
       case Some(magicValue) =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val groupMetadataValueVersion = 
{
-          if (interBrokerProtocolVersion &amp;lt; KAFKA_0_10_1_IV0)
-            0.toShort
-          else
-            GroupMetadataManager.CURRENT_GROUP_VALUE_SCHEMA_VERSION
-        }
&lt;p&gt;-&lt;br/&gt;
         // We always use CREATE_TIME, like the producer. The conversion to LOG_APPEND_TIME (if necessary) happens automatically.&lt;br/&gt;
         val timestampType = TimestampType.CREATE_TIME&lt;br/&gt;
         val timestamp = time.milliseconds()&lt;br/&gt;
         val key = GroupMetadataManager.groupMetadataKey(group.groupId)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val value = GroupMetadataManager.groupMetadataValue(group, groupAssignment, version = groupMetadataValueVersion)&lt;br/&gt;
+        val value = GroupMetadataManager.groupMetadataValue(group, groupAssignment, interBrokerProtocolVersion)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         val records = {&lt;br/&gt;
           val buffer = ByteBuffer.allocate(AbstractRecords.estimateSizeInBytes(magicValue, compressionType,&lt;br/&gt;
@@ -330,7 +323,7 @@ class GroupMetadataManager(brokerId: Int,&lt;/p&gt;

&lt;p&gt;           val records = filteredOffsetMetadata.map &lt;/p&gt;
{ case (topicPartition, offsetAndMetadata) =&amp;gt;
             val key = GroupMetadataManager.offsetCommitKey(group.groupId, topicPartition)
-            val value = GroupMetadataManager.offsetCommitValue(offsetAndMetadata)
+            val value = GroupMetadataManager.offsetCommitValue(offsetAndMetadata, interBrokerProtocolVersion)
             new SimpleRecord(timestamp, key, value)
           }
&lt;p&gt;           val offsetTopicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, partitionFor(group.groupId))&lt;br/&gt;
@@ -580,7 +573,7 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
                   case groupMetadataKey: GroupMetadataKey =&amp;gt;&lt;br/&gt;
                     // load group metadata&lt;br/&gt;
                     val groupId = groupMetadataKey.key&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val groupMetadata = GroupMetadataManager.readGroupMessageValue(groupId, record.value)&lt;br/&gt;
+                    val groupMetadata = GroupMetadataManager.readGroupMessageValue(groupId, record.value, time)&lt;br/&gt;
                     if (groupMetadata != null) {&lt;br/&gt;
                       removedGroups.remove(groupId)&lt;br/&gt;
                       loadedGroups.put(groupId, groupMetadata)&lt;br/&gt;
@@ -630,7 +623,7 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
         // load groups which store offsets in kafka, but which have no active members and thus no group&lt;br/&gt;
         // metadata stored in the log&lt;br/&gt;
         (emptyGroupOffsets.keySet ++ pendingEmptyGroupOffsets.keySet).foreach { groupId =&amp;gt;&lt;/li&gt;
	&lt;li&gt;val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+          val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
           val offsets = emptyGroupOffsets.getOrElse(groupId, Map.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, CommitRecordMetadataAndOffset&amp;#93;&lt;/span&gt;)&lt;br/&gt;
           val pendingOffsets = pendingEmptyGroupOffsets.getOrElse(groupId, Map.empty[Long, mutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, CommitRecordMetadataAndOffset&amp;#93;&lt;/span&gt;])&lt;br/&gt;
           debug(s&quot;Loaded group metadata $group with offsets $offsets and pending offsets $pendingOffsets&quot;)&lt;br/&gt;
@@ -653,18 +646,8 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
                         pendingTransactionalOffsets: Map[Long, mutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, CommitRecordMetadataAndOffset&amp;#93;&lt;/span&gt;]): Unit = {&lt;br/&gt;
     // offsets are initialized prior to loading the group into the cache to ensure that clients see a consistent&lt;br/&gt;
     // view of the group&apos;s offsets&lt;/li&gt;
	&lt;li&gt;val loadedOffsets = offsets.mapValues 
{ case CommitRecordMetadataAndOffset(commitRecordOffset, offsetAndMetadata) =&amp;gt;
-      // special handling for version 0:
-      // set the expiration time stamp as commit time stamp + server default retention time
-      val updatedOffsetAndMetadata =
-        if (offsetAndMetadata.expireTimestamp == org.apache.kafka.common.requests.OffsetCommitRequest.DEFAULT_TIMESTAMP)
-        offsetAndMetadata.copy(expireTimestamp = offsetAndMetadata.commitTimestamp + config.offsetsRetentionMs)
-      else
-        offsetAndMetadata
-      CommitRecordMetadataAndOffset(commitRecordOffset, updatedOffsetAndMetadata)
-    }&lt;/li&gt;
	&lt;li&gt;trace(s&quot;Initialized offsets $loadedOffsets for group ${group.groupId}&quot;)&lt;/li&gt;
	&lt;li&gt;group.initializeOffsets(loadedOffsets, pendingTransactionalOffsets.toMap)&lt;br/&gt;
+    trace(s&quot;Initialized offsets $offsets for group ${group.groupId}&quot;)&lt;br/&gt;
+    group.initializeOffsets(offsets, pendingTransactionalOffsets.toMap)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val currentGroup = addGroup(group)&lt;br/&gt;
     if (group != currentGroup)&lt;br/&gt;
@@ -711,11 +694,11 @@ class GroupMetadataManager(brokerId: Int,&lt;/p&gt;

&lt;p&gt;   // visible for testing&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; def cleanupGroupMetadata(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val startMs = time.milliseconds()&lt;/li&gt;
	&lt;li&gt;val offsetsRemoved = cleanupGroupMetadata(groupMetadataCache.values, group =&amp;gt; {&lt;/li&gt;
	&lt;li&gt;group.removeExpiredOffsets(time.milliseconds())&lt;br/&gt;
+    val currentTimestamp = time.milliseconds()&lt;br/&gt;
+    val numOffsetsRemoved = cleanupGroupMetadata(groupMetadataCache.values, group =&amp;gt; 
{
+      group.removeExpiredOffsets(currentTimestamp, config.offsetsRetentionMs)
     }
&lt;p&gt;)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;info(s&quot;Removed $offsetsRemoved expired offsets in ${time.milliseconds() - startMs} milliseconds.&quot;)&lt;br/&gt;
+    info(s&quot;Removed $numOffsetsRemoved expired offsets in ${time.milliseconds() - currentTimestamp} milliseconds.&quot;)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -917,7 +900,7 @@ class GroupMetadataManager(brokerId: Int,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;-&amp;gt; value version 1:       &lt;span class=&quot;error&quot;&gt;&amp;#91;offset, metadata, commit_timestamp, expire_timestamp&amp;#93;&lt;/span&gt;&lt;br/&gt;
  *&lt;/li&gt;
	&lt;li&gt;key version 2:       group metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*     -&amp;gt; value version 0:       &lt;span class=&quot;error&quot;&gt;&amp;#91;protocol_type, generation, protocol, leader, members&amp;#93;&lt;/span&gt;&lt;br/&gt;
+ *    -&amp;gt; value version 0:       &lt;span class=&quot;error&quot;&gt;&amp;#91;protocol_type, generation, protocol, leader, members&amp;#93;&lt;/span&gt;&lt;br/&gt;
  */&lt;br/&gt;
 object GroupMetadataManager {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -947,6 +930,13 @@ object GroupMetadataManager {&lt;br/&gt;
   private val OFFSET_VALUE_COMMIT_TIMESTAMP_FIELD_V1 = OFFSET_COMMIT_VALUE_SCHEMA_V1.get(&quot;commit_timestamp&quot;)&lt;br/&gt;
   private val OFFSET_VALUE_EXPIRE_TIMESTAMP_FIELD_V1 = OFFSET_COMMIT_VALUE_SCHEMA_V1.get(&quot;expire_timestamp&quot;)&lt;/p&gt;

&lt;p&gt;+  private val OFFSET_COMMIT_VALUE_SCHEMA_V2 = new Schema(new Field(&quot;offset&quot;, INT64),&lt;br/&gt;
+    new Field(&quot;metadata&quot;, STRING, &quot;Associated metadata.&quot;, &quot;&quot;),&lt;br/&gt;
+    new Field(&quot;commit_timestamp&quot;, INT64))&lt;br/&gt;
+  private val OFFSET_VALUE_OFFSET_FIELD_V2 = OFFSET_COMMIT_VALUE_SCHEMA_V2.get(&quot;offset&quot;)&lt;br/&gt;
+  private val OFFSET_VALUE_METADATA_FIELD_V2 = OFFSET_COMMIT_VALUE_SCHEMA_V2.get(&quot;metadata&quot;)&lt;br/&gt;
+  private val OFFSET_VALUE_COMMIT_TIMESTAMP_FIELD_V2 = OFFSET_COMMIT_VALUE_SCHEMA_V2.get(&quot;commit_timestamp&quot;)&lt;br/&gt;
+&lt;br/&gt;
   private val GROUP_METADATA_KEY_SCHEMA = new Schema(new Field(&quot;group&quot;, STRING))&lt;br/&gt;
   private val GROUP_KEY_GROUP_FIELD = GROUP_METADATA_KEY_SCHEMA.get(&quot;group&quot;)&lt;/p&gt;

&lt;p&gt;@@ -975,10 +965,13 @@ object GroupMetadataManager {&lt;br/&gt;
     new Field(SUBSCRIPTION_KEY, BYTES),&lt;br/&gt;
     new Field(ASSIGNMENT_KEY, BYTES))&lt;/p&gt;

&lt;p&gt;+  private val MEMBER_METADATA_V2 = MEMBER_METADATA_V1&lt;br/&gt;
+&lt;br/&gt;
   private val PROTOCOL_TYPE_KEY = &quot;protocol_type&quot;&lt;br/&gt;
   private val GENERATION_KEY = &quot;generation&quot;&lt;br/&gt;
   private val PROTOCOL_KEY = &quot;protocol&quot;&lt;br/&gt;
   private val LEADER_KEY = &quot;leader&quot;&lt;br/&gt;
+  private val CURRENT_STATE_TIMESTAMP_KEY = &quot;current_state_timestamp&quot;&lt;br/&gt;
   private val MEMBERS_KEY = &quot;members&quot;&lt;/p&gt;

&lt;p&gt;   private val GROUP_METADATA_VALUE_SCHEMA_V0 = new Schema(&lt;br/&gt;
@@ -995,6 +988,14 @@ object GroupMetadataManager {&lt;br/&gt;
     new Field(LEADER_KEY, NULLABLE_STRING),&lt;br/&gt;
     new Field(MEMBERS_KEY, new ArrayOf(MEMBER_METADATA_V1)))&lt;/p&gt;

&lt;p&gt;+  private val GROUP_METADATA_VALUE_SCHEMA_V2 = new Schema(&lt;br/&gt;
+    new Field(PROTOCOL_TYPE_KEY, STRING),&lt;br/&gt;
+    new Field(GENERATION_KEY, INT32),&lt;br/&gt;
+    new Field(PROTOCOL_KEY, NULLABLE_STRING),&lt;br/&gt;
+    new Field(LEADER_KEY, NULLABLE_STRING),&lt;br/&gt;
+    new Field(CURRENT_STATE_TIMESTAMP_KEY, INT64),&lt;br/&gt;
+    new Field(MEMBERS_KEY, new ArrayOf(MEMBER_METADATA_V2)))&lt;br/&gt;
+&lt;/p&gt;

&lt;p&gt;   // map of versions to key schemas as data types&lt;br/&gt;
   private val MESSAGE_TYPE_SCHEMAS = Map(&lt;br/&gt;
@@ -1005,19 +1006,20 @@ object GroupMetadataManager {&lt;br/&gt;
   // map of version of offset value schemas&lt;br/&gt;
   private val OFFSET_VALUE_SCHEMAS = Map(&lt;br/&gt;
     0 -&amp;gt; OFFSET_COMMIT_VALUE_SCHEMA_V0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;1 -&amp;gt; OFFSET_COMMIT_VALUE_SCHEMA_V1)&lt;/li&gt;
	&lt;li&gt;private val CURRENT_OFFSET_VALUE_SCHEMA_VERSION = 1.toShort&lt;br/&gt;
+    1 -&amp;gt; OFFSET_COMMIT_VALUE_SCHEMA_V1,&lt;br/&gt;
+    2 -&amp;gt; OFFSET_COMMIT_VALUE_SCHEMA_V2)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   // map of version of group metadata value schemas&lt;br/&gt;
   private val GROUP_VALUE_SCHEMAS = Map(&lt;br/&gt;
     0 -&amp;gt; GROUP_METADATA_VALUE_SCHEMA_V0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;1 -&amp;gt; GROUP_METADATA_VALUE_SCHEMA_V1)&lt;/li&gt;
	&lt;li&gt;private val CURRENT_GROUP_VALUE_SCHEMA_VERSION = 1.toShort&lt;br/&gt;
+    1 -&amp;gt; GROUP_METADATA_VALUE_SCHEMA_V1,&lt;br/&gt;
+    2 -&amp;gt; GROUP_METADATA_VALUE_SCHEMA_V2)&lt;br/&gt;
+  private val CURRENT_GROUP_VALUE_SCHEMA_VERSION = 2.toShort&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private val CURRENT_OFFSET_KEY_SCHEMA = schemaForKey(CURRENT_OFFSET_KEY_SCHEMA_VERSION)&lt;br/&gt;
   private val CURRENT_GROUP_KEY_SCHEMA = schemaForKey(CURRENT_GROUP_KEY_SCHEMA_VERSION)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private val CURRENT_OFFSET_VALUE_SCHEMA = schemaForOffset(CURRENT_OFFSET_VALUE_SCHEMA_VERSION)&lt;br/&gt;
+  private val CURRENT_OFFSET_VALUE_SCHEMA = schemaForOffset(2)&lt;br/&gt;
   private val CURRENT_GROUP_VALUE_SCHEMA = schemaForGroup(CURRENT_GROUP_VALUE_SCHEMA_VERSION)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def schemaForKey(version: Int) = {&lt;br/&gt;
@@ -1081,17 +1083,34 @@ object GroupMetadataManager {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Generates the payload for offset commit message from given offset and metadata&lt;br/&gt;
    *&lt;/li&gt;
	&lt;li&gt;@param offsetAndMetadata consumer&apos;s current offset and metadata&lt;br/&gt;
+   * @param apiVersion the api version&lt;/li&gt;
	&lt;li&gt;@return payload for offset commit message&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; def offsetCommitValue(offsetAndMetadata: OffsetAndMetadata): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = {&lt;/li&gt;
	&lt;li&gt;// generate commit value with schema version 1&lt;/li&gt;
	&lt;li&gt;val value = new Struct(CURRENT_OFFSET_VALUE_SCHEMA)&lt;/li&gt;
	&lt;li&gt;value.set(OFFSET_VALUE_OFFSET_FIELD_V1, offsetAndMetadata.offset)&lt;/li&gt;
	&lt;li&gt;value.set(OFFSET_VALUE_METADATA_FIELD_V1, offsetAndMetadata.metadata)&lt;/li&gt;
	&lt;li&gt;value.set(OFFSET_VALUE_COMMIT_TIMESTAMP_FIELD_V1, offsetAndMetadata.commitTimestamp)&lt;/li&gt;
	&lt;li&gt;value.set(OFFSET_VALUE_EXPIRE_TIMESTAMP_FIELD_V1, offsetAndMetadata.expireTimestamp)&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; def offsetCommitValue(offsetAndMetadata: OffsetAndMetadata,&lt;br/&gt;
+                                       apiVersion: ApiVersion): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    // generate commit value according to schema version&lt;br/&gt;
+    val (version, value) = 
{
+      if (apiVersion &amp;lt; KAFKA_2_1_IV0 || offsetAndMetadata.expireTimestamp.nonEmpty)
+        // if an older version of the API is used, or if an explicit expiration is provided, use the older schema
+        (1.toShort, new Struct(OFFSET_COMMIT_VALUE_SCHEMA_V1))
+      else
+        (2.toShort, new Struct(OFFSET_COMMIT_VALUE_SCHEMA_V2))
+    }
&lt;p&gt;+&lt;br/&gt;
+    if (version == 2) &lt;/p&gt;
{
+      value.set(OFFSET_VALUE_OFFSET_FIELD_V2, offsetAndMetadata.offset)
+      value.set(OFFSET_VALUE_METADATA_FIELD_V2, offsetAndMetadata.metadata)
+      value.set(OFFSET_VALUE_COMMIT_TIMESTAMP_FIELD_V2, offsetAndMetadata.commitTimestamp)
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      value.set(OFFSET_VALUE_OFFSET_FIELD_V1, offsetAndMetadata.offset)
+      value.set(OFFSET_VALUE_METADATA_FIELD_V1, offsetAndMetadata.metadata)
+      value.set(OFFSET_VALUE_COMMIT_TIMESTAMP_FIELD_V1, offsetAndMetadata.commitTimestamp)
+      // version 1 has a non empty expireTimestamp field
+      value.set(OFFSET_VALUE_EXPIRE_TIMESTAMP_FIELD_V1, offsetAndMetadata.expireTimestamp.get)
+    }
&lt;p&gt;+&lt;br/&gt;
     val byteBuffer = ByteBuffer.allocate(2 /* version */ + value.sizeOf)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;byteBuffer.putShort(CURRENT_OFFSET_VALUE_SCHEMA_VERSION)&lt;br/&gt;
+    byteBuffer.putShort(version)&lt;br/&gt;
     value.writeTo(byteBuffer)&lt;br/&gt;
     byteBuffer.array()&lt;br/&gt;
   }&lt;br/&gt;
@@ -1102,19 +1121,30 @@ object GroupMetadataManager {&lt;br/&gt;
    *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param groupMetadata current group metadata&lt;/li&gt;
	&lt;li&gt;@param assignment the assignment for the rebalancing generation&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @param version the version of the value message to use&lt;br/&gt;
+   * @param apiVersion the api version&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return payload for offset commit message&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;group&amp;#93;&lt;/span&gt; def groupMetadataValue(groupMetadata: GroupMetadata,&lt;br/&gt;
                                         assignment: Map[String, Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;],&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;version: Short = 0): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = {&lt;/li&gt;
	&lt;li&gt;val value = if (version == 0) new Struct(GROUP_METADATA_VALUE_SCHEMA_V0) else new Struct(CURRENT_GROUP_VALUE_SCHEMA)&lt;br/&gt;
+                                        apiVersion: ApiVersion): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+&lt;br/&gt;
+    val (version, value) = 
{
+      if (apiVersion &amp;lt; KAFKA_0_10_1_IV0)
+        (0.toShort, new Struct(GROUP_METADATA_VALUE_SCHEMA_V0))
+      else if (apiVersion &amp;lt; KAFKA_2_1_IV0)
+        (1.toShort, new Struct(GROUP_METADATA_VALUE_SCHEMA_V1))
+      else
+        (2.toShort, new Struct(CURRENT_GROUP_VALUE_SCHEMA))
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     value.set(PROTOCOL_TYPE_KEY, groupMetadata.protocolType.getOrElse(&quot;&quot;))&lt;br/&gt;
     value.set(GENERATION_KEY, groupMetadata.generationId)&lt;br/&gt;
     value.set(PROTOCOL_KEY, groupMetadata.protocolOrNull)&lt;br/&gt;
     value.set(LEADER_KEY, groupMetadata.leaderOrNull)&lt;/p&gt;

&lt;p&gt;+    if (version &amp;gt;= 2)&lt;br/&gt;
+      value.set(CURRENT_STATE_TIMESTAMP_KEY, groupMetadata.currentStateTimestampOrDefault)&lt;br/&gt;
+&lt;br/&gt;
     val memberArray = groupMetadata.allMemberMetadata.map { memberMetadata =&amp;gt;&lt;br/&gt;
       val memberStruct = value.instance(MEMBERS_KEY)&lt;br/&gt;
       memberStruct.set(MEMBER_ID_KEY, memberMetadata.memberId)&lt;br/&gt;
@@ -1174,7 +1204,7 @@ object GroupMetadataManager &lt;/p&gt;
{
 
       GroupMetadataKey(version, group)
     }
&lt;p&gt; else &lt;/p&gt;
{
-      throw new IllegalStateException(&quot;Unknown version &quot; + version + &quot; for group metadata message&quot;)
+      throw new IllegalStateException(s&quot;Unknown group metadata message version: $version&quot;)
     }
&lt;p&gt;   }&lt;/p&gt;

&lt;p&gt;@@ -1205,8 +1235,14 @@ object GroupMetadataManager &lt;/p&gt;
{
         val expireTimestamp = value.get(OFFSET_VALUE_EXPIRE_TIMESTAMP_FIELD_V1).asInstanceOf[Long]
 
         OffsetAndMetadata(offset, metadata, commitTimestamp, expireTimestamp)
+      }
&lt;p&gt; else if (version == 2) &lt;/p&gt;
{
+        val offset = value.get(OFFSET_VALUE_OFFSET_FIELD_V2).asInstanceOf[Long]
+        val metadata = value.get(OFFSET_VALUE_METADATA_FIELD_V2).asInstanceOf[String]
+        val commitTimestamp = value.get(OFFSET_VALUE_COMMIT_TIMESTAMP_FIELD_V2).asInstanceOf[Long]
+
+        OffsetAndMetadata(offset, metadata, commitTimestamp)
       }
&lt;p&gt; else &lt;/p&gt;
{
-        throw new IllegalStateException(&quot;Unknown offset message version&quot;)
+        throw new IllegalStateException(s&quot;Unknown offset message version: $version&quot;)
       }
&lt;p&gt;     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -1215,9 +1251,10 @@ object GroupMetadataManager {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Decodes the group metadata messages&apos; payload and retrieves its member metadata from it&lt;br/&gt;
    *&lt;/li&gt;
	&lt;li&gt;@param buffer input byte-buffer&lt;br/&gt;
+   * @param time the time instance to use&lt;/li&gt;
	&lt;li&gt;@return a group metadata object from the message&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def readGroupMessageValue(groupId: String, buffer: ByteBuffer): GroupMetadata = {&lt;br/&gt;
+  def readGroupMessageValue(groupId: String, buffer: ByteBuffer, time: Time): GroupMetadata = {&lt;br/&gt;
     if (buffer == null) 
{ // tombstone
       null
     }
&lt;p&gt; else {&lt;br/&gt;
@@ -1225,13 +1262,23 @@ object GroupMetadataManager {&lt;br/&gt;
       val valueSchema = schemaForGroup(version)&lt;br/&gt;
       val value = valueSchema.read(buffer)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (version == 0 || version == 1) {&lt;br/&gt;
+      if (version &amp;gt;= 0 &amp;amp;&amp;amp; version &amp;lt;= 2) {&lt;br/&gt;
         val generationId = value.get(GENERATION_KEY).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
         val protocolType = value.get(PROTOCOL_TYPE_KEY).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;&lt;br/&gt;
         val protocol = value.get(PROTOCOL_KEY).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;&lt;br/&gt;
         val leaderId = value.get(LEADER_KEY).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;&lt;br/&gt;
         val memberMetadataArray = value.getArray(MEMBERS_KEY)&lt;br/&gt;
         val initialState = if (memberMetadataArray.isEmpty) Empty else Stable&lt;br/&gt;
+        val currentStateTimestamp: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = version match 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+          case version if version == 2 =&amp;gt;+            if (value.hasField(CURRENT_STATE_TIMESTAMP_KEY)) {
+              val timestamp = value.getLong(CURRENT_STATE_TIMESTAMP_KEY)
+              if (timestamp == -1) None else Some(timestamp)
+            } else+              None+          case _ =&amp;gt;+            None+        }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         val members = memberMetadataArray.map { memberMetadataObj =&amp;gt;&lt;br/&gt;
           val memberMetadata = memberMetadataObj.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Struct&amp;#93;&lt;/span&gt;&lt;br/&gt;
@@ -1247,9 +1294,9 @@ object GroupMetadataManager &lt;/p&gt;
{
           member.assignment = Utils.toArray(memberMetadata.get(ASSIGNMENT_KEY).asInstanceOf[ByteBuffer])
           member
         }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;GroupMetadata.loadGroup(groupId, initialState, generationId, protocolType, protocol, leaderId, members)&lt;br/&gt;
+        GroupMetadata.loadGroup(groupId, initialState, generationId, protocolType, protocol, leaderId, currentStateTimestamp, members, time)&lt;br/&gt;
       } else 
{
-        throw new IllegalStateException(&quot;Unknown group metadata message version&quot;)
+        throw new IllegalStateException(s&quot;Unknown group metadata message version: $version&quot;)
       }
&lt;p&gt;     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -1287,7 +1334,7 @@ object GroupMetadataManager &lt;/p&gt;
{
           val value = consumerRecord.value
           val formattedValue =
             if (value == null) &quot;NULL&quot;
-            else GroupMetadataManager.readGroupMessageValue(groupId, ByteBuffer.wrap(value)).toString
+            else GroupMetadataManager.readGroupMessageValue(groupId, ByteBuffer.wrap(value), Time.SYSTEM).toString
           output.write(groupId.getBytes(StandardCharsets.UTF_8))
           output.write(&quot;::&quot;.getBytes(StandardCharsets.UTF_8))
           output.write(formattedValue.getBytes(StandardCharsets.UTF_8))
diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala
index 0c88be9bb5c..de9e0bf66ef 100644
--- a/core/src/main/scala/kafka/server/KafkaApis.scala
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala
@@ -332,33 +332,25 @@ class KafkaApis(val requestChannel: RequestChannel,
       }
&lt;p&gt; else {&lt;br/&gt;
         // for version 1 and beyond store offsets in offset manager&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// compute the retention time based on the request version:&lt;/li&gt;
	&lt;li&gt;// if it is v1 or not specified by user, we can use the default retention&lt;/li&gt;
	&lt;li&gt;val offsetRetention =&lt;/li&gt;
	&lt;li&gt;if (header.apiVersion &amp;lt;= 1 ||&lt;/li&gt;
	&lt;li&gt;offsetCommitRequest.retentionTime == OffsetCommitRequest.DEFAULT_RETENTION_TIME)&lt;/li&gt;
	&lt;li&gt;groupCoordinator.offsetConfig.offsetsRetentionMs&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;offsetCommitRequest.retentionTime&lt;br/&gt;
-&lt;br/&gt;
         // commit timestamp is always set to now.&lt;br/&gt;
         // &quot;default&quot; expiration timestamp is now + retention (and retention may be overridden if v2)&lt;br/&gt;
         // expire timestamp is computed differently for v1 and v2.&lt;/li&gt;
	&lt;li&gt;//   - If v1 and no explicit commit timestamp is provided we use default expiration timestamp.&lt;br/&gt;
+        //   - If v1 and no explicit commit timestamp is provided we treat it the same as v5.&lt;br/&gt;
         //   - If v1 and explicit commit timestamp is provided we calculate retention from that explicit commit timestamp&lt;/li&gt;
	&lt;li&gt;//   - If v2 we use the default expiration timestamp&lt;br/&gt;
+        //   - If v2/v3/v4 (no explicit commit timestamp) we treat it the same as v5.&lt;br/&gt;
+        //   - For v5 and beyond there is no per partition expiration timestamp, so this field is no longer in effect&lt;br/&gt;
         val currentTimestamp = time.milliseconds&lt;/li&gt;
	&lt;li&gt;val defaultExpireTimestamp = offsetRetention + currentTimestamp&lt;br/&gt;
         val partitionData = authorizedTopicRequestInfo.mapValues { partitionData =&amp;gt;&lt;br/&gt;
           val metadata = if (partitionData.metadata == null) OffsetMetadata.NoMetadata else partitionData.metadata&lt;br/&gt;
           new OffsetAndMetadata(&lt;br/&gt;
             offsetMetadata = OffsetMetadata(partitionData.offset, metadata),&lt;/li&gt;
	&lt;li&gt;commitTimestamp = currentTimestamp,&lt;/li&gt;
	&lt;li&gt;expireTimestamp = {&lt;/li&gt;
	&lt;li&gt;if (partitionData.timestamp == OffsetCommitRequest.DEFAULT_TIMESTAMP)&lt;/li&gt;
	&lt;li&gt;defaultExpireTimestamp&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;offsetRetention + partitionData.timestamp&lt;br/&gt;
+            commitTimestamp = partitionData.timestamp match 
{
+              case OffsetCommitRequest.DEFAULT_TIMESTAMP =&amp;gt; currentTimestamp
+              case customTimestamp =&amp;gt; customTimestamp
+            }
&lt;p&gt;,&lt;br/&gt;
+            expireTimestamp = offsetCommitRequest.retentionTime match &lt;/p&gt;
{
+              case OffsetCommitRequest.DEFAULT_RETENTION_TIME =&amp;gt; None
+              case retentionTime =&amp;gt; Some(currentTimestamp + retentionTime)
             }
&lt;p&gt;           )&lt;br/&gt;
         }&lt;br/&gt;
@@ -1912,7 +1904,7 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
       topicPartition -&amp;gt; new OffsetAndMetadata(&lt;br/&gt;
         offsetMetadata = OffsetMetadata(partitionData.offset, metadata),&lt;br/&gt;
         commitTimestamp = currentTimestamp,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;expireTimestamp = defaultExpireTimestamp)&lt;br/&gt;
+        expireTimestamp = Some(defaultExpireTimestamp))&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/tools/ConsoleConsumer.scala b/core/src/main/scala/kafka/tools/ConsoleConsumer.scala&lt;br/&gt;
index c55f6c484c7..7e2c5644a3b 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/tools/ConsoleConsumer.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/tools/ConsoleConsumer.scala&lt;br/&gt;
@@ -368,6 +368,10 @@ object ConsoleConsumer extends Logging {&lt;br/&gt;
         consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, group)&lt;br/&gt;
       case None =&amp;gt;&lt;br/&gt;
         consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, s&quot;console-consumer-${new Random().nextInt(100000)}&quot;)&lt;br/&gt;
+        // By default, avoid unnecessary expansion of the coordinator cache since&lt;br/&gt;
+        // the auto-generated group and its offsets is not intended to be used again&lt;br/&gt;
+        if (!consumerProps.containsKey(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG))&lt;br/&gt;
+          consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;false&quot;)&lt;br/&gt;
         groupIdPassed = false&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/tools/DumpLogSegments.scala b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
index 17fbd8f0f14..1792c7bff81 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
@@ -29,7 +29,7 @@ import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.clients.consumer.internals.ConsumerProtocol&lt;br/&gt;
 import org.apache.kafka.common.KafkaException&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
-import org.apache.kafka.common.utils.Utils&lt;br/&gt;
+import org.apache.kafka.common.utils.&lt;/p&gt;
{Time, Utils}

&lt;p&gt; import scala.collection.&lt;/p&gt;
{Map, mutable}
&lt;p&gt; import scala.collection.mutable.ArrayBuffer&lt;br/&gt;
@@ -321,7 +321,7 @@ object DumpLogSegments {&lt;/p&gt;

&lt;p&gt;     private def parseGroupMetadata(groupMetadataKey: GroupMetadataKey, payload: ByteBuffer) = {&lt;br/&gt;
       val groupId = groupMetadataKey.key&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val group = GroupMetadataManager.readGroupMessageValue(groupId, payload)&lt;br/&gt;
+      val group = GroupMetadataManager.readGroupMessageValue(groupId, payload, Time.SYSTEM)&lt;br/&gt;
       val protocolType = group.protocolType.getOrElse(&quot;&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       val assignment = group.allMemberMetadata.map { member =&amp;gt;&lt;br/&gt;
diff --git a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
index f4ff8e17d24..69b65252b02 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
@@ -334,7 +334,7 @@ class AuthorizerIntegrationTest extends BaseRequestTest {&lt;br/&gt;
   private def createOffsetCommitRequest = &lt;/p&gt;
{
     new requests.OffsetCommitRequest.Builder(
       group, Map(tp -&amp;gt; new requests.OffsetCommitRequest.PartitionData(0, &quot;metadata&quot;)).asJava).
-      setMemberId(&quot;&quot;).setGenerationId(1).setRetentionTime(1000).
+      setMemberId(&quot;&quot;).setGenerationId(1).
       build()
   }

&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala b/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
index 32e40850fcf..2befc8fdf3b 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
@@ -79,6 +79,9 @@ class ApiVersionTest &lt;/p&gt;
{
     assertEquals(KAFKA_2_0_IV1, ApiVersion(&quot;2.0&quot;))
     assertEquals(KAFKA_2_0_IV0, ApiVersion(&quot;2.0-IV0&quot;))
     assertEquals(KAFKA_2_0_IV1, ApiVersion(&quot;2.0-IV1&quot;))
+
+    assertEquals(KAFKA_2_1_IV0, ApiVersion(&quot;2.1&quot;))
+    assertEquals(KAFKA_2_1_IV0, ApiVersion(&quot;2.1-IV0&quot;))
   }

&lt;p&gt;   @Test&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
index 3bfacabc843..77e6fdc3683 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
@@ -17,20 +17,22 @@&lt;/p&gt;

&lt;p&gt; package kafka.coordinator.group&lt;/p&gt;

&lt;p&gt;-import kafka.api.ApiVersion&lt;br/&gt;
+import kafka.api.&lt;/p&gt;
{ApiVersion, KAFKA_1_1_IV0, KAFKA_2_1_IV0}
&lt;p&gt; import kafka.cluster.Partition&lt;br/&gt;
 import kafka.common.OffsetAndMetadata&lt;br/&gt;
 import kafka.log.&lt;/p&gt;
{Log, LogAppendInfo}
&lt;p&gt; import kafka.server.&lt;/p&gt;
{FetchDataInfo, KafkaConfig, LogOffsetMetadata, ReplicaManager}
&lt;p&gt; import kafka.utils.TestUtils.fail&lt;br/&gt;
 import kafka.utils.&lt;/p&gt;
{KafkaScheduler, MockTime, TestUtils}
&lt;p&gt;+import org.apache.kafka.clients.consumer.internals.ConsumerProtocol&lt;br/&gt;
+import org.apache.kafka.clients.consumer.internals.PartitionAssignor.Subscription&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
 import org.apache.kafka.common.requests.&lt;/p&gt;
{IsolationLevel, OffsetFetchResponse}
&lt;p&gt; import org.apache.kafka.common.requests.ProduceResponse.PartitionResponse&lt;br/&gt;
 import org.easymock.&lt;/p&gt;
{Capture, EasyMock, IAnswer}
&lt;p&gt;-import org.junit.Assert.&lt;/p&gt;
{assertEquals, assertFalse, assertTrue, assertNull}
&lt;p&gt;+import org.junit.Assert.&lt;/p&gt;
{assertEquals, assertFalse, assertNull, assertTrue}
&lt;p&gt; import org.junit.&lt;/p&gt;
{Before, Test}&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
 &lt;br/&gt;
@@ -52,6 +54,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
   var scheduler: KafkaScheduler = null&lt;br/&gt;
   var zkClient: KafkaZkClient = null&lt;br/&gt;
   var partition: Partition = null&lt;br/&gt;
+  var defaultOffsetRetentionMs = Long.MaxValue&lt;br/&gt;
 &lt;br/&gt;
   val groupId = &quot;foo&quot;&lt;br/&gt;
   val groupPartitionId = 0&lt;br/&gt;
@@ -75,6 +78,8 @@ class GroupMetadataManagerTest {&lt;br/&gt;
       offsetCommitTimeoutMs = config.offsetCommitTimeoutMs,&lt;br/&gt;
       offsetCommitRequiredAcks = config.offsetCommitRequiredAcks)&lt;br/&gt;
 &lt;br/&gt;
+    defaultOffsetRetentionMs = offsetConfig.offsetsRetentionMs&lt;br/&gt;
+&lt;br/&gt;
     // make two partitions of the group topic to make sure some partitions are not owned by the coordinator&lt;br/&gt;
     zkClient = EasyMock.createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaZkClient&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     EasyMock.expect(zkClient.getTopicPartitionCount(Topic.GROUP_METADATA_TOPIC_NAME)).andReturn(Some(2))&lt;br/&gt;
@@ -506,7 +511,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     // group is owned but does not exist yet&lt;br/&gt;
     assertTrue(groupMetadataManager.groupNotExists(groupId))&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     // group is owned but not Dead&lt;br/&gt;
@@ -616,6 +621,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     assertEquals(committedOffsets.size, group.allOffsets.size)&lt;br/&gt;
     committedOffsets.foreach { case (topicPartition, offset) =&amp;gt;
       assertEquals(Some(offset), group.offset(topicPartition).map(_.offset))
+      assertTrue(group.offset(topicPartition).map(_.expireTimestamp).contains(None))
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -729,9 +735,9 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
   def testAddGroup() {
-    val group = new GroupMetadata(&quot;foo&quot;, initialState = Empty)
+    val group = new GroupMetadata(&quot;foo&quot;, Empty, time)
     assertEquals(group, groupMetadataManager.addGroup(group))
-    assertEquals(group, groupMetadataManager.addGroup(new GroupMetadata(&quot;foo&quot;, initialState = Empty)))
+    assertEquals(group, groupMetadataManager.addGroup(new GroupMetadata(&quot;foo&quot;, Empty, time)))
   }&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
@@ -739,7 +745,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     val generation = 27&lt;br/&gt;
     val protocolType = &quot;consumer&quot;&lt;br/&gt;
 &lt;br/&gt;
-    val group = GroupMetadata.loadGroup(groupId, Empty, generation, protocolType, null, null, Seq.empty)&lt;br/&gt;
+    val group = GroupMetadata.loadGroup(groupId, Empty, generation, protocolType, null, null, None, Seq.empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val capturedRecords = expectAppendMessage(Errors.NONE)&lt;br/&gt;
@@ -758,7 +764,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     assertEquals(1, records.size)&lt;br/&gt;
 &lt;br/&gt;
     val record = records.head&lt;br/&gt;
-    val groupMetadata = GroupMetadataManager.readGroupMessageValue(groupId, record.value)&lt;br/&gt;
+    val groupMetadata = GroupMetadataManager.readGroupMessageValue(groupId, record.value, time)&lt;br/&gt;
     assertTrue(groupMetadata.is(Empty))&lt;br/&gt;
     assertEquals(generation, groupMetadata.generationId)&lt;br/&gt;
     assertEquals(Some(protocolType), groupMetadata.protocolType)&lt;br/&gt;
@@ -766,7 +772,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
   def testStoreEmptySimpleGroup() {&lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val capturedRecords = expectAppendMessage(Errors.NONE)&lt;br/&gt;
@@ -787,7 +793,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     assertEquals(1, records.size)&lt;br/&gt;
 &lt;br/&gt;
     val record = records.head&lt;br/&gt;
-    val groupMetadata = GroupMetadataManager.readGroupMessageValue(groupId, record.value)&lt;br/&gt;
+    val groupMetadata = GroupMetadataManager.readGroupMessageValue(groupId, record.value, time)&lt;br/&gt;
     assertTrue(groupMetadata.is(Empty))&lt;br/&gt;
     assertEquals(0, groupMetadata.generationId)&lt;br/&gt;
     assertEquals(None, groupMetadata.protocolType)&lt;br/&gt;
@@ -809,7 +815,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
   private def assertStoreGroupErrorMapping(appendError: Errors, expectedError: Errors) {&lt;br/&gt;
     EasyMock.reset(replicaManager)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     expectAppendMessage(appendError)&lt;br/&gt;
@@ -832,7 +838,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     val clientId = &quot;clientId&quot;&lt;br/&gt;
     val clientHost = &quot;localhost&quot;&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val member = new MemberMetadata(memberId, groupId, clientId, clientHost, rebalanceTimeout, sessionTimeout,&lt;br/&gt;
@@ -863,7 +869,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     val clientId = &quot;clientId&quot;&lt;br/&gt;
     val clientHost = &quot;localhost&quot;&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
 &lt;br/&gt;
     val member = new MemberMetadata(memberId, groupId, clientId, clientHost, rebalanceTimeout, sessionTimeout,&lt;br/&gt;
       protocolType, List((&quot;protocol&quot;, Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;())))&lt;br/&gt;
@@ -893,7 +899,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val offsets = immutable.Map(topicPartition -&amp;gt; OffsetAndMetadata(offset))&lt;br/&gt;
@@ -935,7 +941,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val offsets = immutable.Map(topicPartition -&amp;gt; OffsetAndMetadata(offset))&lt;br/&gt;
@@ -975,7 +981,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val offsets = immutable.Map(topicPartition -&amp;gt; OffsetAndMetadata(offset))&lt;br/&gt;
@@ -1014,7 +1020,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val offsets = immutable.Map(topicPartition -&amp;gt; OffsetAndMetadata(offset))&lt;br/&gt;
@@ -1052,7 +1058,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val offsets = immutable.Map(topicPartition -&amp;gt; OffsetAndMetadata(offset))&lt;br/&gt;
@@ -1094,7 +1100,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     val offsets = immutable.Map(topicPartition -&amp;gt; OffsetAndMetadata(offset))&lt;br/&gt;
@@ -1132,7 +1138,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     // expire the offset after 1 millisecond&lt;br/&gt;
@@ -1185,7 +1191,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
     group.generationId = 5&lt;br/&gt;
 &lt;br/&gt;
@@ -1233,7 +1239,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
     group.generationId = 5&lt;br/&gt;
 &lt;br/&gt;
@@ -1287,7 +1293,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
     // expire the offset after 1 millisecond&lt;br/&gt;
@@ -1348,31 +1354,39 @@ class GroupMetadataManagerTest {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
-  def testExpireOffsetsWithActiveGroup() {&lt;br/&gt;
+  def testOffsetExpirationSemantics() {&lt;br/&gt;
     val memberId = &quot;memberId&quot;&lt;br/&gt;
     val clientId = &quot;clientId&quot;&lt;br/&gt;
     val clientHost = &quot;localhost&quot;&lt;br/&gt;
-    val topicPartition1 = new TopicPartition(&quot;foo&quot;, 0)&lt;br/&gt;
-    val topicPartition2 = new TopicPartition(&quot;foo&quot;, 1)&lt;br/&gt;
+    val topic = &quot;foo&quot;&lt;br/&gt;
+    val topicPartition1 = new TopicPartition(topic, 0)&lt;br/&gt;
+    val topicPartition2 = new TopicPartition(topic, 1)&lt;br/&gt;
+    val topicPartition3 = new TopicPartition(topic, 2)&lt;br/&gt;
     val offset = 37&lt;br/&gt;
 &lt;br/&gt;
     groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
 &lt;br/&gt;
-    val group = new GroupMetadata(groupId, initialState = Empty)&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
 &lt;br/&gt;
+    val subscription = new Subscription(List(topic).asJava)&lt;br/&gt;
     val member = new MemberMetadata(memberId, groupId, clientId, clientHost, rebalanceTimeout, sessionTimeout,&lt;br/&gt;
-      protocolType, List((&quot;protocol&quot;, Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;())))&lt;br/&gt;
+      protocolType, List((&quot;protocol&quot;, ConsumerProtocol.serializeSubscription(subscription).array())))&lt;br/&gt;
     member.awaitingJoinCallback = _ =&amp;gt; ()&lt;br/&gt;
     group.add(member)&lt;br/&gt;
     group.transitionTo(PreparingRebalance)&lt;br/&gt;
     group.initNextGeneration()&lt;br/&gt;
 &lt;br/&gt;
-    // expire the offset after 1 millisecond&lt;br/&gt;
     val startMs = time.milliseconds&lt;br/&gt;
+    // old clients, expiry timestamp is explicitly set&lt;br/&gt;
+    val tp1OffsetAndMetadata = OffsetAndMetadata(offset, &quot;&quot;, startMs, startMs + 1)&lt;br/&gt;
+    val tp2OffsetAndMetadata = OffsetAndMetadata(offset, &quot;&quot;, startMs, startMs + 3)&lt;br/&gt;
+    // new clients, no per-partition expiry timestamp, offsets of group expire together&lt;br/&gt;
+    val tp3OffsetAndMetadata = OffsetAndMetadata(offset, &quot;&quot;, startMs)&lt;br/&gt;
     val offsets = immutable.Map(&lt;br/&gt;
-      topicPartition1 -&amp;gt; OffsetAndMetadata(offset, &quot;&quot;, startMs, startMs + 1),&lt;br/&gt;
-      topicPartition2 -&amp;gt; OffsetAndMetadata(offset, &quot;&quot;, startMs, startMs + 3))&lt;br/&gt;
+      topicPartition1 -&amp;gt; tp1OffsetAndMetadata,&lt;br/&gt;
+      topicPartition2 -&amp;gt; tp2OffsetAndMetadata,&lt;br/&gt;
+      topicPartition3 -&amp;gt; tp3OffsetAndMetadata)&lt;br/&gt;
 &lt;br/&gt;
     mockGetPartition()&lt;br/&gt;
     expectAppendMessage(Errors.NONE)&lt;br/&gt;
@@ -1389,8 +1403,26 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     assertFalse(commitErrors.isEmpty)&lt;br/&gt;
     assertEquals(Some(Errors.NONE), commitErrors.get.get(topicPartition1))&lt;br/&gt;
 &lt;br/&gt;
-    // expire all of the offsets&lt;br/&gt;
-    time.sleep(4)&lt;br/&gt;
+    // do not expire any offset even though expiration timestamp is reached for one (due to group still being active)&lt;br/&gt;
+    time.sleep(2)&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.cleanupGroupMetadata()&lt;br/&gt;
+&lt;br/&gt;
+    // group and offsets should still be there&lt;br/&gt;
+    assertEquals(Some(group), groupMetadataManager.getGroup(groupId))&lt;br/&gt;
+    assertEquals(Some(tp1OffsetAndMetadata), group.offset(topicPartition1))&lt;br/&gt;
+    assertEquals(Some(tp2OffsetAndMetadata), group.offset(topicPartition2))&lt;br/&gt;
+    assertEquals(Some(tp3OffsetAndMetadata), group.offset(topicPartition3))&lt;br/&gt;
+&lt;br/&gt;
+    var cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1, topicPartition2, topicPartition3)))&lt;br/&gt;
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition1).map(_.offset))&lt;br/&gt;
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition2).map(_.offset))&lt;br/&gt;
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition3).map(_.offset))&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.verify(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    group.transitionTo(PreparingRebalance)&lt;br/&gt;
+    group.transitionTo(Empty)&lt;br/&gt;
 &lt;br/&gt;
     // expect the offset tombstone&lt;br/&gt;
     EasyMock.reset(partition)&lt;br/&gt;
@@ -1401,16 +1433,245 @@ class GroupMetadataManagerTest {
 
     groupMetadataManager.cleanupGroupMetadata()
 
-    // group should still be there, but the offsets should be gone
+    // group is empty now, only one offset should expire
+    assertEquals(Some(group), groupMetadataManager.getGroup(groupId))
+    assertEquals(None, group.offset(topicPartition1))
+    assertEquals(Some(tp2OffsetAndMetadata), group.offset(topicPartition2))
+    assertEquals(Some(tp3OffsetAndMetadata), group.offset(topicPartition3))
+
+    cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1, topicPartition2, topicPartition3)))
+    assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition1).map(_.offset))
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition2).map(_.offset))
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition3).map(_.offset))
+
+    EasyMock.verify(replicaManager)
+
+    time.sleep(2)
+
+    // expect the offset tombstone
+    EasyMock.reset(partition)
+    EasyMock.expect(partition.appendRecordsToLeader(EasyMock.anyObject(classOf[MemoryRecords]),
+      isFromClient = EasyMock.eq(false), requiredAcks = EasyMock.anyInt()))
+      .andReturn(LogAppendInfo.UnknownLogAppendInfo)
+    EasyMock.replay(partition)
+
+    groupMetadataManager.cleanupGroupMetadata()
+
+    // one more offset should expire
     assertEquals(Some(group), groupMetadataManager.getGroup(groupId))
     assertEquals(None, group.offset(topicPartition1))
     assertEquals(None, group.offset(topicPartition2))
+    assertEquals(Some(tp3OffsetAndMetadata), group.offset(topicPartition3))
 
-    val cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1, topicPartition2)))
+    cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1, topicPartition2, topicPartition3)))
+    assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition1).map(_.offset))
+    assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition2).map(_.offset))
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition3).map(_.offset))
+
+    EasyMock.verify(replicaManager)
+
+    // advance time to just before the offset of last partition is to be expired, no offset should expire
+    time.sleep(group.currentStateTimestamp.get + defaultOffsetRetentionMs - time.milliseconds() - 1)
+
+    groupMetadataManager.cleanupGroupMetadata()
+
+    // one more offset should expire
+    assertEquals(Some(group), groupMetadataManager.getGroup(groupId))
+    assertEquals(None, group.offset(topicPartition1))
+    assertEquals(None, group.offset(topicPartition2))
+    assertEquals(Some(tp3OffsetAndMetadata), group.offset(topicPartition3))
+
+    cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1, topicPartition2, topicPartition3)))
+    assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition1).map(_.offset))
+    assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition2).map(_.offset))
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition3).map(_.offset))
+
+    EasyMock.verify(replicaManager)
+
+    // advance time enough for that last offset to expire
+    time.sleep(2)
+
+    // expect the offset tombstone
+    EasyMock.reset(partition)
+    EasyMock.expect(partition.appendRecordsToLeader(EasyMock.anyObject(classOf[MemoryRecords]),
+      isFromClient = EasyMock.eq(false), requiredAcks = EasyMock.anyInt()))
+      .andReturn(LogAppendInfo.UnknownLogAppendInfo)
+    EasyMock.replay(partition)
+
+    groupMetadataManager.cleanupGroupMetadata()
+
+    // group and all its offsets should be gone now
+    assertEquals(None, groupMetadataManager.getGroup(groupId))
+    assertEquals(None, group.offset(topicPartition1))
+    assertEquals(None, group.offset(topicPartition2))
+    assertEquals(None, group.offset(topicPartition3))
+
+    cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1, topicPartition2, topicPartition3)))
     assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition1).map(_.offset))
     assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition2).map(_.offset))
+    assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition3).map(_.offset))
 
     EasyMock.verify(replicaManager)
+
+    assert(group.is(Dead))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testOffsetExpirationOfSimpleConsumer() {&lt;br/&gt;
+    val memberId = &quot;memberId&quot;&lt;br/&gt;
+    val clientId = &quot;clientId&quot;&lt;br/&gt;
+    val clientHost = &quot;localhost&quot;&lt;br/&gt;
+    val topic = &quot;foo&quot;&lt;br/&gt;
+    val topicPartition1 = new TopicPartition(topic, 0)&lt;br/&gt;
+    val offset = 37&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.addPartitionOwnership(groupPartitionId)&lt;br/&gt;
+&lt;br/&gt;
+    val group = new GroupMetadata(groupId, Empty, time)&lt;br/&gt;
+    groupMetadataManager.addGroup(group)&lt;br/&gt;
+&lt;br/&gt;
+    // expire the offset after 1 and 3 milliseconds (old clients) and after default retention (new clients)&lt;br/&gt;
+    val startMs = time.milliseconds&lt;br/&gt;
+    // old clients, expiry timestamp is explicitly set&lt;br/&gt;
+    val tp1OffsetAndMetadata = OffsetAndMetadata(offset, &quot;&quot;, startMs)&lt;br/&gt;
+    val tp2OffsetAndMetadata = OffsetAndMetadata(offset, &quot;&quot;, startMs)&lt;br/&gt;
+    // new clients, no per-partition expiry timestamp, offsets of group expire together&lt;br/&gt;
+    val offsets = immutable.Map(&lt;br/&gt;
+      topicPartition1 -&amp;gt; tp1OffsetAndMetadata)&lt;br/&gt;
+&lt;br/&gt;
+    mockGetPartition()&lt;br/&gt;
+    expectAppendMessage(Errors.NONE)&lt;br/&gt;
+    EasyMock.replay(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    var commitErrors: Option[immutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Errors&amp;#93;&lt;/span&gt;] = None&lt;br/&gt;
+    def callback(errors: immutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Errors&amp;#93;&lt;/span&gt;) {
+      commitErrors = Some(errors)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.storeOffsets(group, memberId, offsets, callback)&lt;br/&gt;
+    assertTrue(group.hasOffsets)&lt;br/&gt;
+&lt;br/&gt;
+    assertFalse(commitErrors.isEmpty)&lt;br/&gt;
+    assertEquals(Some(Errors.NONE), commitErrors.get.get(topicPartition1))&lt;br/&gt;
+&lt;br/&gt;
+    // do not expire offsets while within retention period since commit timestamp&lt;br/&gt;
+    val expiryTimestamp = offsets.get(topicPartition1).get.commitTimestamp + defaultOffsetRetentionMs&lt;br/&gt;
+    time.sleep(expiryTimestamp - time.milliseconds() - 1)&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.cleanupGroupMetadata()&lt;br/&gt;
+&lt;br/&gt;
+    // group and offsets should still be there&lt;br/&gt;
+    assertEquals(Some(group), groupMetadataManager.getGroup(groupId))&lt;br/&gt;
+    assertEquals(Some(tp1OffsetAndMetadata), group.offset(topicPartition1))&lt;br/&gt;
+&lt;br/&gt;
+    var cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1)))&lt;br/&gt;
+    assertEquals(Some(offset), cachedOffsets.get(topicPartition1).map(_.offset))&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.verify(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    // advance time to enough for offsets to expire&lt;br/&gt;
+    time.sleep(2)&lt;br/&gt;
+&lt;br/&gt;
+    // expect the offset tombstone&lt;br/&gt;
+    EasyMock.reset(partition)&lt;br/&gt;
+    EasyMock.expect(partition.appendRecordsToLeader(EasyMock.anyObject(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryRecords&amp;#93;&lt;/span&gt;),&lt;br/&gt;
+      isFromClient = EasyMock.eq(false), requiredAcks = EasyMock.anyInt()))&lt;br/&gt;
+      .andReturn(LogAppendInfo.UnknownLogAppendInfo)&lt;br/&gt;
+    EasyMock.replay(partition)&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.cleanupGroupMetadata()&lt;br/&gt;
+&lt;br/&gt;
+    // group and all its offsets should be gone now&lt;br/&gt;
+    assertEquals(None, groupMetadataManager.getGroup(groupId))&lt;br/&gt;
+    assertEquals(None, group.offset(topicPartition1))&lt;br/&gt;
+&lt;br/&gt;
+    cachedOffsets = groupMetadataManager.getOffsets(groupId, Some(Seq(topicPartition1)))&lt;br/&gt;
+    assertEquals(Some(OffsetFetchResponse.INVALID_OFFSET), cachedOffsets.get(topicPartition1).map(_.offset))&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.verify(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    assert(group.is(Dead))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testLoadOffsetFromOldCommit() = {&lt;br/&gt;
+    val groupMetadataTopicPartition = groupTopicPartition&lt;br/&gt;
+    val generation = 935&lt;br/&gt;
+    val protocolType = &quot;consumer&quot;&lt;br/&gt;
+    val protocol = &quot;range&quot;&lt;br/&gt;
+    val startOffset = 15L&lt;br/&gt;
+    val committedOffsets = Map(&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 0) -&amp;gt; 23L,&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 1) -&amp;gt; 455L,&lt;br/&gt;
+      new TopicPartition(&quot;bar&quot;, 0) -&amp;gt; 8992L&lt;br/&gt;
+    )&lt;br/&gt;
+&lt;br/&gt;
+    val apiVersion = KAFKA_1_1_IV0&lt;br/&gt;
+    val offsetCommitRecords = createCommittedOffsetRecords(committedOffsets, apiVersion = apiVersion, retentionTime = Some(100))&lt;br/&gt;
+    val memberId = &quot;98098230493&quot;&lt;br/&gt;
+    val groupMetadataRecord = buildStableGroupRecordWithMember(generation, protocolType, protocol, memberId, apiVersion)&lt;br/&gt;
+    val records = MemoryRecords.withRecords(startOffset, CompressionType.NONE,&lt;br/&gt;
+      offsetCommitRecords ++ Seq(groupMetadataRecord): _*)&lt;br/&gt;
+&lt;br/&gt;
+    expectGroupMetadataLoad(groupMetadataTopicPartition, startOffset, records)&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.replay(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.loadGroupsAndOffsets(groupMetadataTopicPartition, _ =&amp;gt; ())&lt;br/&gt;
+&lt;br/&gt;
+    val group = groupMetadataManager.getGroup(groupId).getOrElse(fail(&quot;Group was not loaded into the cache&quot;))&lt;br/&gt;
+    assertEquals(groupId, group.groupId)&lt;br/&gt;
+    assertEquals(Stable, group.currentState)&lt;br/&gt;
+    assertEquals(memberId, group.leaderOrNull)&lt;br/&gt;
+    assertEquals(generation, group.generationId)&lt;br/&gt;
+    assertEquals(Some(protocolType), group.protocolType)&lt;br/&gt;
+    assertEquals(protocol, group.protocolOrNull)&lt;br/&gt;
+    assertEquals(Set(memberId), group.allMembers)&lt;br/&gt;
+    assertEquals(committedOffsets.size, group.allOffsets.size)&lt;br/&gt;
+    committedOffsets.foreach { case (topicPartition, offset) =&amp;gt;
+      assertEquals(Some(offset), group.offset(topicPartition).map(_.offset))
+      assertTrue(group.offset(topicPartition).map(_.expireTimestamp).get.nonEmpty)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testLoadOffsetWithExplicitRetention() = {&lt;br/&gt;
+    val groupMetadataTopicPartition = groupTopicPartition&lt;br/&gt;
+    val generation = 935&lt;br/&gt;
+    val protocolType = &quot;consumer&quot;&lt;br/&gt;
+    val protocol = &quot;range&quot;&lt;br/&gt;
+    val startOffset = 15L&lt;br/&gt;
+    val committedOffsets = Map(&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 0) -&amp;gt; 23L,&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 1) -&amp;gt; 455L,&lt;br/&gt;
+      new TopicPartition(&quot;bar&quot;, 0) -&amp;gt; 8992L&lt;br/&gt;
+    )&lt;br/&gt;
+&lt;br/&gt;
+    val offsetCommitRecords = createCommittedOffsetRecords(committedOffsets, retentionTime = Some(100))&lt;br/&gt;
+    val memberId = &quot;98098230493&quot;&lt;br/&gt;
+    val groupMetadataRecord = buildStableGroupRecordWithMember(generation, protocolType, protocol, memberId)&lt;br/&gt;
+    val records = MemoryRecords.withRecords(startOffset, CompressionType.NONE,&lt;br/&gt;
+      offsetCommitRecords ++ Seq(groupMetadataRecord): _*)&lt;br/&gt;
+&lt;br/&gt;
+    expectGroupMetadataLoad(groupMetadataTopicPartition, startOffset, records)&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.replay(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.loadGroupsAndOffsets(groupMetadataTopicPartition, _ =&amp;gt; ())&lt;br/&gt;
+&lt;br/&gt;
+    val group = groupMetadataManager.getGroup(groupId).getOrElse(fail(&quot;Group was not loaded into the cache&quot;))&lt;br/&gt;
+    assertEquals(groupId, group.groupId)&lt;br/&gt;
+    assertEquals(Stable, group.currentState)&lt;br/&gt;
+    assertEquals(memberId, group.leaderOrNull)&lt;br/&gt;
+    assertEquals(generation, group.generationId)&lt;br/&gt;
+    assertEquals(Some(protocolType), group.protocolType)&lt;br/&gt;
+    assertEquals(protocol, group.protocolOrNull)&lt;br/&gt;
+    assertEquals(Set(memberId), group.allMembers)&lt;br/&gt;
+    assertEquals(committedOffsets.size, group.allOffsets.size)&lt;br/&gt;
+    committedOffsets.foreach { case (topicPartition, offset) =&amp;gt;+      assertEquals(Some(offset), group.offset(topicPartition).map(_.offset))+      assertTrue(group.offset(topicPartition).map(_.expireTimestamp).get.nonEmpty)+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def appendAndCaptureCallback(): Capture[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionResponse&amp;#93;&lt;/span&gt; =&amp;gt; Unit] = {&lt;br/&gt;
@@ -1452,20 +1713,21 @@ class GroupMetadataManagerTest {&lt;br/&gt;
   private def buildStableGroupRecordWithMember(generation: Int,&lt;br/&gt;
                                                protocolType: String,&lt;br/&gt;
                                                protocol: String,&lt;br/&gt;
-                                               memberId: String): SimpleRecord = {&lt;br/&gt;
+                                               memberId: String,&lt;br/&gt;
+                                               apiVersion: ApiVersion = ApiVersion.latestVersion): SimpleRecord = {
     val memberProtocols = List((protocol, Array.emptyByteArray))
     val member = new MemberMetadata(memberId, groupId, &quot;clientId&quot;, &quot;clientHost&quot;, 30000, 10000, protocolType, memberProtocols)
-    val group = GroupMetadata.loadGroup(groupId, Stable, generation, protocolType, protocol,
-      leaderId = memberId, Seq(member))
+    val group = GroupMetadata.loadGroup(groupId, Stable, generation, protocolType, protocol, memberId,
+      if (apiVersion &amp;gt;= KAFKA_2_1_IV0) Some(time.milliseconds()) else None, Seq(member), time)
     val groupMetadataKey = GroupMetadataManager.groupMetadataKey(groupId)
-    val groupMetadataValue = GroupMetadataManager.groupMetadataValue(group, Map(memberId -&amp;gt; Array.empty[Byte]))
+    val groupMetadataValue = GroupMetadataManager.groupMetadataValue(group, Map(memberId -&amp;gt; Array.empty[Byte]), apiVersion)
     new SimpleRecord(groupMetadataKey, groupMetadataValue)
   }&lt;br/&gt;
 &lt;br/&gt;
   private def buildEmptyGroupRecord(generation: Int, protocolType: String): SimpleRecord = {
-    val group = GroupMetadata.loadGroup(groupId, Empty, generation, protocolType, null, null, Seq.empty)
+    val group = GroupMetadata.loadGroup(groupId, Empty, generation, protocolType, null, null, None, Seq.empty, time)
     val groupMetadataKey = GroupMetadataManager.groupMetadataKey(groupId)
-    val groupMetadataValue = GroupMetadataManager.groupMetadataValue(group, Map.empty)
+    val groupMetadataValue = GroupMetadataManager.groupMetadataValue(group, Map.empty, ApiVersion.latestVersion)
     new SimpleRecord(groupMetadataKey, groupMetadataValue)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -1511,11 +1773,19 @@ class GroupMetadataManagerTest {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def createCommittedOffsetRecords(committedOffsets: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-                                           groupId: String = groupId): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;SimpleRecord&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+                                           groupId: String = groupId,&lt;br/&gt;
+                                           apiVersion: ApiVersion = ApiVersion.latestVersion,&lt;br/&gt;
+                                           retentionTime: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;SimpleRecord&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     committedOffsets.map { case (topicPartition, offset) =&amp;gt;&lt;br/&gt;
-      val offsetAndMetadata = OffsetAndMetadata(offset)&lt;br/&gt;
+      val offsetAndMetadata = retentionTime match {
+        case Some(timestamp) =&amp;gt;
+          val commitTimestamp = time.milliseconds()
+          OffsetAndMetadata(offset, &quot;&quot;, commitTimestamp, commitTimestamp + timestamp)
+        case None =&amp;gt;
+          OffsetAndMetadata(offset)
+      }&lt;br/&gt;
       val offsetCommitKey = GroupMetadataManager.offsetCommitKey(groupId, topicPartition)&lt;br/&gt;
-      val offsetCommitValue = GroupMetadataManager.offsetCommitValue(offsetAndMetadata)&lt;br/&gt;
+      val offsetCommitValue = GroupMetadataManager.offsetCommitValue(offsetAndMetadata, apiVersion)&lt;br/&gt;
       new SimpleRecord(offsetCommitKey, offsetCommitValue)&lt;br/&gt;
     }.toSeq&lt;br/&gt;
   }&lt;br/&gt;
@@ -1542,7 +1812,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
   def testMetrics() {&lt;br/&gt;
     groupMetadataManager.cleanupGroupMetadata()&lt;br/&gt;
     expectMetrics(groupMetadataManager, 0, 0, 0)&lt;br/&gt;
-    val group = new GroupMetadata(&quot;foo2&quot;, Stable)&lt;br/&gt;
+    val group = new GroupMetadata(&quot;foo2&quot;, Stable, time)&lt;br/&gt;
     groupMetadataManager.addGroup(group)&lt;br/&gt;
     expectMetrics(groupMetadataManager, 1, 0, 0)&lt;br/&gt;
     group.transitionTo(PreparingRebalance)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataTest.scala b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataTest.scala&lt;br/&gt;
index 183860f2155..90545339ca5 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataTest.scala&lt;br/&gt;
@@ -18,9 +18,8 @@&lt;br/&gt;
 package kafka.coordinator.group&lt;br/&gt;
 &lt;br/&gt;
 import kafka.common.OffsetAndMetadata&lt;br/&gt;
-&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-&lt;br/&gt;
+import org.apache.kafka.common.utils.Time&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.{Before, Test}
&lt;p&gt; import org.scalatest.junit.JUnitSuite&lt;br/&gt;
@@ -40,7 +39,7 @@ class GroupMetadataTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;   @Before&lt;br/&gt;
   def setUp() &lt;/p&gt;
{
-    group = new GroupMetadata(&quot;groupId&quot;, initialState = Empty)
+    group = new GroupMetadata(&quot;groupId&quot;, Empty, Time.SYSTEM)
   }

&lt;p&gt;   @Test&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
index 0205bcf2141..d91e008fe3b 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
@@ -240,7 +240,7 @@ class RequestQuotaTest extends BaseRequestTest {&lt;br/&gt;
         case ApiKeys.OFFSET_COMMIT =&amp;gt;&lt;br/&gt;
           new OffsetCommitRequest.Builder(&quot;test-group&quot;,&lt;br/&gt;
             Map(tp -&amp;gt; new OffsetCommitRequest.PartitionData(0, &quot;metadata&quot;)).asJava).&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;setMemberId(&quot;&quot;).setGenerationId(1).setRetentionTime(1000)&lt;br/&gt;
+            setMemberId(&quot;&quot;).setGenerationId(1)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case ApiKeys.OFFSET_FETCH =&amp;gt;&lt;br/&gt;
           new OffsetFetchRequest.Builder(&quot;test-group&quot;, List(tp).asJava)&lt;br/&gt;
diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index cb246f60b15..26c0d15e978 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -19,6 +19,18 @@&lt;/p&gt;

&lt;p&gt; &amp;lt;script id=&quot;upgrade-template&quot; type=&quot;text/x-handlebars-template&quot;&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_2_1_0&quot; href=&quot;#upgrade_2_1_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, or 2.0.0 to 2.1.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
+&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Additional Upgrade Notes:&amp;lt;/b&amp;gt;&amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;ol&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt;Offset expiration semantics has slightly changed in this version. According to the new semantics, offsets of partitions in a group will&lt;br/&gt;
+        not be removed while the group is subscribed to the corresponding topic and is still active (has active consumers). If group becomes&lt;br/&gt;
+        empty all its offsets will be removed after default offset retention period (or the one set by broker) has passed (unless the group becomes&lt;br/&gt;
+        active again). Offsets associated with standalone (simple) consumers, that do not use Kafka group management, will be removed after default&lt;br/&gt;
+        offset retention period (or the one set by broker) has passed since their last commit.&amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ol&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_2_0_0&quot; href=&quot;#upgrade_2_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, or 1.1.x to 2.0.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
 &amp;lt;p&amp;gt;Kafka 2.0.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below,&lt;br/&gt;
     you guarantee no downtime during the upgrade. However, please review the &amp;lt;a href=&quot;#upgrade_200_notable&quot;&amp;gt;notable changes in 2.0.0&amp;lt;/a&amp;gt; before upgrading.&lt;br/&gt;
@@ -66,6 +78,9 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_2_0_0&quot; href=&quot;#upgrade_2_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.1&lt;br/&gt;
         until all brokers in the cluster have been updated.&lt;br/&gt;
         &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;NOTE:&amp;lt;/b&amp;gt; any prefixed ACLs added to a cluster, even after the cluster is fully upgraded, will be ignored should the cluster be downgraded again.&lt;br/&gt;
     &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt;The default for console consumer&apos;s &amp;lt;code&amp;gt;enable.auto.commit&amp;lt;/code&amp;gt; property when no &amp;lt;code&amp;gt;group.id&amp;lt;/code&amp;gt; is provided is now set to &amp;lt;code&amp;gt;false&amp;lt;/code&amp;gt;.&lt;br/&gt;
+        This is to avoid polluting the consumer coordinator cache as the auto-generated group is not likely to be used by other consumers.&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ol&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_200_notable&quot; href=&quot;#upgrade_200_notable&quot;&amp;gt;Notable changes in 2.0.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
@@ -112,9 +127,9 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_200_notable&quot; href=&quot;#upgrade_200_notable&quot;&amp;gt;Notable changes in 2&lt;br/&gt;
         &amp;lt;code&amp;gt;beginningOffsets&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;endOffsets&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;close&amp;lt;/code&amp;gt; that take in a &amp;lt;code&amp;gt;Duration&amp;lt;/code&amp;gt;.&amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;Also as part of KIP-266, the default value of &amp;lt;code&amp;gt;request.timeout.ms&amp;lt;/code&amp;gt; has been changed to 30 seconds.&lt;br/&gt;
         The previous value was a little higher than 5 minutes to account for maximum time that a rebalance would take.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Now we treat the JoinGroup request in the rebalance as a special case and use a value derived from&lt;br/&gt;
+        Now we treat the JoinGroup request in the rebalance as a special case and use a value derived from&lt;br/&gt;
         &amp;lt;code&amp;gt;max.poll.interval.ms&amp;lt;/code&amp;gt; for the request timeout. All other request types use the timeout defined&lt;/li&gt;
	&lt;li&gt;by &amp;lt;code&amp;gt;request.timeout.ms&amp;lt;/code&amp;gt;&amp;lt;/li&amp;gt;&lt;br/&gt;
+        by &amp;lt;code&amp;gt;request.timeout.ms&amp;lt;/code&amp;gt;&amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;The internal method &amp;lt;code&amp;gt;kafka.admin.AdminClient.deleteRecordsBefore&amp;lt;/code&amp;gt; has been removed. Users are encouraged to migrate to &amp;lt;code&amp;gt;org.apache.kafka.clients.admin.AdminClient.deleteRecords&amp;lt;/code&amp;gt;.&amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;The AclCommand tool &amp;lt;code&amp;gt;-&lt;del&gt;producer&amp;lt;/code&amp;gt; convenience option uses the &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-277+&lt;/del&gt;+Fine+Grained+ACL+for+CreateTopics+API&quot;&amp;gt;KIP-277&amp;lt;/a&amp;gt; finer grained ACL on the given topic. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;&amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-176%3A+Remove+deprecated+new-consumer+option+for+tools&quot;&amp;gt;KIP-176&amp;lt;/a&amp;gt; removes&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310060">
                    <name>Container</name>
                                            <outwardlinks description="contains">
                                        <issuelink>
            <issuekey id="13082326">KAFKA-5510</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13090362">KAFKA-5664</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13162173">KAFKA-6951</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 21 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i390vz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>