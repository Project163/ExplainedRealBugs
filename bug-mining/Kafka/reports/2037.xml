<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:14:14 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7286] Loading offsets and group metadata hangs with large group metadata records</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7286</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;When a (Kafka-based) consumer group contains many members, group metadata records (in the &lt;tt&gt;__consumer-offsets&lt;/tt&gt; topic) may happen to be quite large.&lt;/p&gt;

&lt;p&gt;Increasing the &lt;tt&gt;message.max.bytes&lt;/tt&gt; makes storing these records possible.&lt;br/&gt;
 Loading them when a broker restart is done via &lt;a href=&quot;https://github.com/apache/kafka/blob/418a91b5d4e3a0579b91d286f61c2b63c5b4a9b6/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L504&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;doLoadGroupsAndOffsets&lt;/a&gt;. However, this method relies on the &lt;tt&gt;offsets.load.buffer.size&lt;/tt&gt; configuration to create a &lt;a href=&quot;https://github.com/apache/kafka/blob/418a91b5d4e3a0579b91d286f61c2b63c5b4a9b6/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L513&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;buffer&lt;/a&gt; that will contain the records being loaded.&lt;/p&gt;

&lt;p&gt;If a group metadata record is too large for this buffer, the loading method will get stuck trying to load records (in a tight loop) into a buffer that cannot accommodate a single record.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;For example, if the &lt;tt&gt;__consumer-offsets-9&lt;/tt&gt; partition contains a record smaller than &lt;tt&gt;message.max.bytes&lt;/tt&gt; but larger than &lt;tt&gt;offsets.load.buffer.size&lt;/tt&gt;, logs would indicate the following:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;...
[2018-08-13 21:00:21,073] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;But logs will never contain the expected &lt;tt&gt;Finished loading offsets and group metadata from ...&lt;/tt&gt; line.&lt;/p&gt;

&lt;p&gt;Consumers whose group are assigned to this partition will see &lt;tt&gt;Marking the coordinator dead&lt;/tt&gt; and will never be able to stabilize and make progress.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;From what I could gather in the code, it seems that:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/418a91b5d4e3a0579b91d286f61c2b63c5b4a9b6/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L522&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;fetchDataInfo&lt;/a&gt; returns at least one record (even if larger than &lt;tt&gt;offsets.load.buffer.size&lt;/tt&gt;, thanks to &lt;tt&gt;minOneMessage = true&lt;/tt&gt;)&lt;/li&gt;
	&lt;li&gt;No fully-readable record is stored in the buffer with &lt;a href=&quot;https://github.com/apache/kafka/blob/418a91b5d4e3a0579b91d286f61c2b63c5b4a9b6/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L528&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;fileRecords.readInto(buffer, 0)&lt;/a&gt; (too large to fit in the buffer)&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/418a91b5d4e3a0579b91d286f61c2b63c5b4a9b6/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L532&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;memRecords.batches&lt;/a&gt; returns an empty iterator&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/418a91b5d4e3a0579b91d286f61c2b63c5b4a9b6/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L590&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;currOffset&lt;/a&gt; never advances, hence loading the partition hangs forever.&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;
&lt;p&gt;It would be great to let the partition load even if a record is larger than the configured &lt;tt&gt;offsets.load.buffer.size&lt;/tt&gt; limit. The fact that &lt;tt&gt;minOneMessage = true&lt;/tt&gt; when reading records seems to indicate it might be a good idea for the buffer to accommodate at least one record.&lt;/p&gt;

&lt;p&gt;If you think the limit should stay a hard limit, then at least adding a log line indicating &lt;tt&gt;offsets.load.buffer.size&lt;/tt&gt; is not large enough and should be increased. Otherwise, one can only guess and dig through the code to figure out what is happening &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I will try to open a PR with the first idea (allowing large records to be read when needed) soon, but any feedback from anyone who also had the same issue in the past would be appreciated &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13178728">KAFKA-7286</key>
            <summary>Loading offsets and group metadata hangs with large group metadata records</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="flavr">Flavien Raynaud</assignee>
                                    <reporter username="flavr">Flavien Raynaud</reporter>
                        <labels>
                    </labels>
                <created>Mon, 13 Aug 2018 23:13:29 +0000</created>
                <updated>Fri, 9 Nov 2018 10:42:27 +0000</updated>
                            <resolved>Sun, 9 Sep 2018 07:19:48 +0000</resolved>
                                                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16579029" author="githubbot" created="Mon, 13 Aug 2018 23:18:34 +0000"  >&lt;p&gt;flavray opened a new pull request #5500: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7286&quot; title=&quot;Loading offsets and group metadata hangs with large group metadata records&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7286&quot;&gt;&lt;del&gt;KAFKA-7286&lt;/del&gt;&lt;/a&gt;: Avoid getting stuck loading large metadata records&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5500&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5500&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   If a group metadata record size is higher than&lt;br/&gt;
   offsets.load.buffer.size, loading offsets and group metadata from&lt;br/&gt;
   __consumer_offsets would hang forever.&lt;/p&gt;

&lt;p&gt;   This was due to a buffer being too small to fit any message bigger than&lt;br/&gt;
   the maximum configuration. If this happens, temporarily use a buffer&lt;br/&gt;
   that can fit the large records and move on.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16589263" author="flavr" created="Wed, 22 Aug 2018 19:05:15 +0000"  >&lt;p&gt;Any chance anyone could have a look at it? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16608362" author="githubbot" created="Sun, 9 Sep 2018 07:05:51 +0000"  >&lt;p&gt;hachikuji closed pull request #5500: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7286&quot; title=&quot;Loading offsets and group metadata hangs with large group metadata records&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7286&quot;&gt;&lt;del&gt;KAFKA-7286&lt;/del&gt;&lt;/a&gt;: Avoid getting stuck loading large metadata records&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5500&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5500&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
index 6bd0a5a0d52..940ec716e36 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
@@ -510,7 +510,9 @@ class GroupMetadataManager(brokerId: Int,&lt;/p&gt;

&lt;p&gt;       case Some(log) =&amp;gt;&lt;br/&gt;
         var currOffset = log.logStartOffset&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;lazy val buffer = ByteBuffer.allocate(config.loadBufferSize)&lt;br/&gt;
+&lt;br/&gt;
+        // buffer may not be needed if records are read from memory&lt;br/&gt;
+        var buffer = ByteBuffer.allocate(0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // loop breaks if leader changes at any time during the load, since getHighWatermark is -1&lt;br/&gt;
         val loadedOffsets = mutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;GroupTopicPartition, CommitRecordMetadataAndOffset&amp;#93;&lt;/span&gt;()&lt;br/&gt;
@@ -524,7 +526,20 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
           val memRecords = fetchDataInfo.records match {&lt;br/&gt;
             case records: MemoryRecords =&amp;gt; records&lt;br/&gt;
             case fileRecords: FileRecords =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;buffer.clear()&lt;br/&gt;
+              val sizeInBytes = fileRecords.sizeInBytes&lt;br/&gt;
+              val bytesNeeded = Math.max(config.loadBufferSize, sizeInBytes)&lt;br/&gt;
+&lt;br/&gt;
+              // minOneMessage = true in the above log.read means that the buffer may need to be grown to ensure progress can be made&lt;br/&gt;
+              if (buffer.capacity &amp;lt; bytesNeeded) {&lt;br/&gt;
+                if (config.loadBufferSize &amp;lt; bytesNeeded)&lt;br/&gt;
+                  warn(s&quot;Loaded offsets and group metadata from $topicPartition with buffer larger ($bytesNeeded bytes) than &quot; +&lt;br/&gt;
+                    s&quot;configured offsets.load.buffer.size (${config.loadBufferSize} bytes)&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                buffer = ByteBuffer.allocate(bytesNeeded)&lt;br/&gt;
+              } else 
{
+                buffer.clear()
+              }
&lt;p&gt;+&lt;br/&gt;
               fileRecords.readInto(buffer, 0)&lt;br/&gt;
               MemoryRecords.readableRecords(buffer)&lt;br/&gt;
           }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala b/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala&lt;br/&gt;
index a3585154451..50d96c30734 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala&lt;br/&gt;
@@ -296,7 +296,8 @@ class TransactionStateManager(brokerId: Int,&lt;br/&gt;
         warn(s&quot;Attempted to load offsets and group metadata from $topicPartition, but found no log&quot;)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       case Some(log) =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;lazy val buffer = ByteBuffer.allocate(config.transactionLogLoadBufferSize)&lt;br/&gt;
+        // buffer may not be needed if records are read from memory&lt;br/&gt;
+        var buffer = ByteBuffer.allocate(0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // loop breaks if leader changes at any time during the load, since getHighWatermark is -1&lt;br/&gt;
         var currOffset = log.logStartOffset&lt;br/&gt;
@@ -311,6 +312,19 @@ class TransactionStateManager(brokerId: Int,&lt;br/&gt;
             val memRecords = fetchDataInfo.records match {&lt;br/&gt;
               case records: MemoryRecords =&amp;gt; records&lt;br/&gt;
               case fileRecords: FileRecords =&amp;gt;&lt;br/&gt;
+                val sizeInBytes = fileRecords.sizeInBytes&lt;br/&gt;
+                val bytesNeeded = Math.max(config.transactionLogLoadBufferSize, sizeInBytes)&lt;br/&gt;
+&lt;br/&gt;
+                // minOneMessage = true in the above log.read means that the buffer may need to be grown to ensure progress can be made&lt;br/&gt;
+                if (buffer.capacity &amp;lt; bytesNeeded) {&lt;br/&gt;
+                  if (config.transactionLogLoadBufferSize &amp;lt; bytesNeeded)&lt;br/&gt;
+                    warn(s&quot;Loaded offsets and group metadata from $topicPartition with buffer larger ($bytesNeeded bytes) than &quot; +&lt;br/&gt;
+                      s&quot;configured transaction.state.log.load.buffer.size (${config.transactionLogLoadBufferSize} bytes)&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                  buffer = ByteBuffer.allocate(bytesNeeded)&lt;br/&gt;
+                } else &lt;/p&gt;
{
+                  buffer.clear()
+                }
&lt;p&gt;                 buffer.clear()&lt;br/&gt;
                 fileRecords.readInto(buffer, 0)&lt;br/&gt;
                 MemoryRecords.readableRecords(buffer)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/KafkaConfig.scala b/core/src/main/scala/kafka/server/KafkaConfig.scala&lt;br/&gt;
index b651549f1e8..9aada23e868 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/KafkaConfig.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/KafkaConfig.scala&lt;br/&gt;
@@ -656,7 +656,7 @@ object KafkaConfig {&lt;br/&gt;
   val GroupInitialRebalanceDelayMsDoc = &quot;The amount of time the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins.&quot;&lt;br/&gt;
   /** ********* Offset management configuration ***********/&lt;br/&gt;
   val OffsetMetadataMaxSizeDoc = &quot;The maximum size for a metadata entry associated with an offset commit&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val OffsetsLoadBufferSizeDoc = &quot;Batch size for reading from the offsets segments when loading offsets into the cache.&quot;&lt;br/&gt;
+  val OffsetsLoadBufferSizeDoc = &quot;Batch size for reading from the offsets segments when loading offsets into the cache (soft-limit, overridden if records are too large).&quot;&lt;br/&gt;
   val OffsetsTopicReplicationFactorDoc = &quot;The replication factor for the offsets topic (set higher to ensure availability). &quot; +&lt;br/&gt;
   &quot;Internal topic creation will fail until the cluster size meets this replication factor requirement.&quot;&lt;br/&gt;
   val OffsetsTopicPartitionsDoc = &quot;The number of partitions for the offset commit topic (should not change after deployment)&quot;&lt;br/&gt;
@@ -673,7 +673,7 @@ object KafkaConfig {&lt;br/&gt;
   val TransactionsMaxTimeoutMsDoc = &quot;The maximum allowed timeout for transactions. &quot; +&lt;br/&gt;
     &quot;If a client&#8217;s requested transaction time exceed this, then the broker will return an error in InitProducerIdRequest. This prevents a client from too large of a timeout, which can stall consumers reading from topics included in the transaction.&quot;&lt;br/&gt;
   val TransactionsTopicMinISRDoc = &quot;Overridden &quot; + MinInSyncReplicasProp + &quot; config for the transaction topic.&quot;&lt;/li&gt;
	&lt;li&gt;val TransactionsLoadBufferSizeDoc = &quot;Batch size for reading from the transaction log segments when loading producer ids and transactions into the cache.&quot;&lt;br/&gt;
+  val TransactionsLoadBufferSizeDoc = &quot;Batch size for reading from the transaction log segments when loading producer ids and transactions into the cache (soft-limit, overridden if records are too large).&quot;&lt;br/&gt;
   val TransactionsTopicReplicationFactorDoc = &quot;The replication factor for the transaction topic (set higher to ensure availability). &quot; +&lt;br/&gt;
     &quot;Internal topic creation will fail until the cluster size meets this replication factor requirement.&quot;&lt;br/&gt;
   val TransactionsTopicPartitionsDoc = &quot;The number of partitions for the transaction topic (should not change after deployment).&quot;&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
index 21c13658e79..bcc18a41620 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
@@ -645,6 +645,38 @@ class GroupMetadataManagerTest 
{
     assertEquals(None, groupMetadataManager.getGroup(groupId))
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def testLoadGroupWithLargeGroupMetadataRecord() {&lt;br/&gt;
+    val groupMetadataTopicPartition = groupTopicPartition&lt;br/&gt;
+    val startOffset = 15L&lt;br/&gt;
+    val committedOffsets = Map(&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 0) -&amp;gt; 23L,&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 1) -&amp;gt; 455L,&lt;br/&gt;
+      new TopicPartition(&quot;bar&quot;, 0) -&amp;gt; 8992L&lt;br/&gt;
+    )&lt;br/&gt;
+&lt;br/&gt;
+    // create a GroupMetadata record larger then offsets.load.buffer.size (here at least 16 bytes larger)&lt;br/&gt;
+    val assignmentSize = OffsetConfig.DefaultLoadBufferSize + 16&lt;br/&gt;
+    val memberId = &quot;98098230493&quot;&lt;br/&gt;
+&lt;br/&gt;
+    val offsetCommitRecords = createCommittedOffsetRecords(committedOffsets)&lt;br/&gt;
+    val groupMetadataRecord = buildStableGroupRecordWithMember(generation = 15,&lt;br/&gt;
+      protocolType = &quot;consumer&quot;, protocol = &quot;range&quot;, memberId, assignmentSize)&lt;br/&gt;
+    val records = MemoryRecords.withRecords(startOffset, CompressionType.NONE,&lt;br/&gt;
+      offsetCommitRecords ++ Seq(groupMetadataRecord): _*)&lt;br/&gt;
+&lt;br/&gt;
+    expectGroupMetadataLoad(groupMetadataTopicPartition, startOffset, records)&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.replay(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.loadGroupsAndOffsets(groupMetadataTopicPartition, _ =&amp;gt; ())&lt;br/&gt;
+&lt;br/&gt;
+    val group = groupMetadataManager.getGroup(groupId).getOrElse(fail(&quot;Group was not loaded into the cache&quot;))&lt;br/&gt;
+    committedOffsets.foreach &lt;/p&gt;
{ case (topicPartition, offset) =&amp;gt;
+      assertEquals(Some(offset), group.offset(topicPartition).map(_.offset))
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testOffsetWriteAfterGroupRemoved(): Unit = {&lt;br/&gt;
     // this test case checks the following scenario:&lt;br/&gt;
@@ -1606,7 +1638,7 @@ class GroupMetadataManagerTest {&lt;br/&gt;
     val apiVersion = KAFKA_1_1_IV0&lt;br/&gt;
     val offsetCommitRecords = createCommittedOffsetRecords(committedOffsets, apiVersion = apiVersion, retentionTime = Some(100))&lt;br/&gt;
     val memberId = &quot;98098230493&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val groupMetadataRecord = buildStableGroupRecordWithMember(generation, protocolType, protocol, memberId, apiVersion)&lt;br/&gt;
+    val groupMetadataRecord = buildStableGroupRecordWithMember(generation, protocolType, protocol, memberId, apiVersion = apiVersion)&lt;br/&gt;
     val records = MemoryRecords.withRecords(startOffset, CompressionType.NONE,&lt;br/&gt;
       offsetCommitRecords ++ Seq(groupMetadataRecord): _*)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1711,13 +1743,14 @@ class GroupMetadataManagerTest {&lt;br/&gt;
                                                protocolType: String,&lt;br/&gt;
                                                protocol: String,&lt;br/&gt;
                                                memberId: String,&lt;br/&gt;
+                                               assignmentSize: Int = 0,&lt;br/&gt;
                                                apiVersion: ApiVersion = ApiVersion.latestVersion): SimpleRecord = &lt;/p&gt;
{
     val memberProtocols = List((protocol, Array.emptyByteArray))
     val member = new MemberMetadata(memberId, groupId, &quot;clientId&quot;, &quot;clientHost&quot;, 30000, 10000, protocolType, memberProtocols)
     val group = GroupMetadata.loadGroup(groupId, Stable, generation, protocolType, protocol, memberId,
       if (apiVersion &amp;gt;= KAFKA_2_1_IV0) Some(time.milliseconds()) else None, Seq(member), time)
     val groupMetadataKey = GroupMetadataManager.groupMetadataKey(groupId)
-    val groupMetadataValue = GroupMetadataManager.groupMetadataValue(group, Map(memberId -&amp;gt; Array.empty[Byte]), apiVersion)
+    val groupMetadataValue = GroupMetadataManager.groupMetadataValue(group, Map(memberId -&amp;gt; new Array[Byte](assignmentSize)), apiVersion)
     new SimpleRecord(groupMetadataKey, groupMetadataValue)
   }

&lt;p&gt;@@ -1754,6 +1787,8 @@ class GroupMetadataManagerTest {&lt;br/&gt;
       EasyMock.eq(true), EasyMock.eq(IsolationLevel.READ_UNCOMMITTED)))&lt;br/&gt;
       .andReturn(FetchDataInfo(LogOffsetMetadata(startOffset), fileRecordsMock))&lt;/p&gt;

&lt;p&gt;+    EasyMock.expect(fileRecordsMock.sizeInBytes()).andStubReturn(records.sizeInBytes)&lt;br/&gt;
+&lt;br/&gt;
     val bufferCapture = EasyMock.newCapture&lt;span class=&quot;error&quot;&gt;&amp;#91;ByteBuffer&amp;#93;&lt;/span&gt;&lt;br/&gt;
     fileRecordsMock.readInto(EasyMock.capture(bufferCapture), EasyMock.anyInt())&lt;br/&gt;
     EasyMock.expectLastCall().andAnswer(new IAnswer&lt;span class=&quot;error&quot;&gt;&amp;#91;Unit&amp;#93;&lt;/span&gt; {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala b/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala&lt;br/&gt;
index 873b88d1104..060e07e7327 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala&lt;br/&gt;
@@ -259,6 +259,8 @@ class TransactionCoordinatorConcurrencyTest extends AbstractCoordinatorConcurren&lt;br/&gt;
       EasyMock.eq(true), EasyMock.eq(IsolationLevel.READ_UNCOMMITTED)))&lt;br/&gt;
       .andReturn(FetchDataInfo(LogOffsetMetadata(startOffset), fileRecordsMock))&lt;/p&gt;

&lt;p&gt;+    EasyMock.expect(fileRecordsMock.sizeInBytes()).andStubReturn(records.sizeInBytes)&lt;br/&gt;
+&lt;br/&gt;
     val bufferCapture = EasyMock.newCapture&lt;span class=&quot;error&quot;&gt;&amp;#91;ByteBuffer&amp;#93;&lt;/span&gt;&lt;br/&gt;
     fileRecordsMock.readInto(EasyMock.capture(bufferCapture), EasyMock.anyInt())&lt;br/&gt;
     EasyMock.expectLastCall().andAnswer(new IAnswer&lt;span class=&quot;error&quot;&gt;&amp;#91;Unit&amp;#93;&lt;/span&gt; {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala b/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala&lt;br/&gt;
index 34b82d9ea55..74bbe336b3c 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala&lt;br/&gt;
@@ -586,6 +586,8 @@ class TransactionStateManagerTest {&lt;br/&gt;
       EasyMock.eq(true), EasyMock.eq(IsolationLevel.READ_UNCOMMITTED)))&lt;br/&gt;
       .andReturn(FetchDataInfo(LogOffsetMetadata(startOffset), fileRecordsMock))&lt;/p&gt;

&lt;p&gt;+    EasyMock.expect(fileRecordsMock.sizeInBytes()).andStubReturn(records.sizeInBytes)&lt;br/&gt;
+&lt;br/&gt;
     val bufferCapture = EasyMock.newCapture&lt;span class=&quot;error&quot;&gt;&amp;#91;ByteBuffer&amp;#93;&lt;/span&gt;&lt;br/&gt;
     fileRecordsMock.readInto(EasyMock.capture(bufferCapture), EasyMock.anyInt())&lt;br/&gt;
     EasyMock.expectLastCall().andAnswer(new IAnswer&lt;span class=&quot;error&quot;&gt;&amp;#91;Unit&amp;#93;&lt;/span&gt; {&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16608792" author="wushujames" created="Mon, 10 Sep 2018 06:50:01 +0000"  >&lt;p&gt;How many members would be needed in a group to trigger this scenario?&lt;/p&gt;</comment>
                            <comment id="16608869" author="flavr" created="Mon, 10 Sep 2018 08:32:11 +0000"  >&lt;p&gt;In our case, we did not have that many members (something around 30 members). However each member was responsible for a lot of topic-partitions (&amp;gt;500), which ended up causing these large metadata records.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 10 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3x0cf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>