<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:38:09 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-901] Kafka server can become unavailable if clients send several metadata requests</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-901</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Currently, if a broker is bounced without controlled shutdown and there are several clients talking to the Kafka cluster, each of the clients realize the unavailability of leaders for some partitions. This leads to several metadata requests sent to the Kafka brokers. Since metadata requests are pretty slow, all the I/O threads quickly become busy serving the metadata requests. This leads to a full request queue, that stalls handling of finished responses since the same network thread handles requests as well as responses. In this situation, clients timeout on metadata requests and send more metadata requests. This quickly makes the Kafka cluster unavailable. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12646927">KAFKA-901</key>
            <summary>Kafka server can become unavailable if clients send several metadata requests</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nehanarkhede">Neha Narkhede</assignee>
                                    <reporter username="nehanarkhede">Neha Narkhede</reporter>
                        <labels>
                    </labels>
                <created>Fri, 10 May 2013 00:34:58 +0000</created>
                <updated>Tue, 21 May 2013 17:07:46 +0000</updated>
                            <resolved>Sat, 18 May 2013 00:00:17 +0000</resolved>
                                    <version>0.8.0</version>
                                                    <component>replication</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="13653451" author="nehanarkhede" created="Fri, 10 May 2013 00:58:50 +0000"  >&lt;p&gt;This patch changes the way a Kafka server handles metadata requests. Metadata requests are currently a bottleneck in the system since the server reads several paths from zookeeper to serve one metadata request. The latency is also proportional to the number of topics in a metadata request. A faster way to serve metadata requests is through the controller. The reason is the controller makes all state change decisions for the cluster, so it has a cache with the latest leadership information. The patch uses the following algorithm to serve metadata requests -&lt;/p&gt;

&lt;p&gt;1. If broker is controller, read leadership information from cache and send the response&lt;br/&gt;
2. If broker is not controller, instantiate a sync producer to forward metadata request to the controller&lt;br/&gt;
3. If broker is not controller and a controller is unavailable, send ControllerNotAvailable error code back to the client&lt;/p&gt;

&lt;p&gt;There are some things to note here -&lt;/p&gt;

&lt;p&gt;1. How does a broker know who the current controller and its host/port ? If we read from zookeeper, then that makes metadata requests slow, although less slower than before. However, upon controller failover, the controller sends a LeaderAndIsr request to all* brokers. So when a broker receives any state change request from the controller, it stores the controller&apos;s host/port in cache from zookeeper. Since state change requests are rare, this is ok&lt;/p&gt;

&lt;p&gt;Now there is a corner case we need to take care of. If a new broker is brought up and it doesn&apos;t have any partitions assigned to it, it won&apos;t receive any LeaderAndIsr request in the current code base. This patch takes care of this by changing controller to always send leader and isr request to newly restarted brokers, even if there are no partitions in the request.&lt;/p&gt;

&lt;p&gt;2. What timeout should be used when a broker wants to forward a metadata request to the controller ? &lt;br/&gt;
Since there isn&apos;t a way to know the timeout specified by the original metadata request, we can potentially set the socket timeout for the forwarded request to be Integer.MAX. This will ensure that the forwarded request will not prematurely time out. However, we need to ensure that the controller always sends a response back to the broker OR closes the socket. This will prevent the broker&apos;s I/O thread from waiting indefinitely for a response from the controller. This also requires the controller to not block when serving metadata from its cache OR it will block some other broker&apos;s I/O thread, which is bad.&lt;/p&gt;

&lt;p&gt;3. On the controller, should it acquire a lock when reading its cache to serve a metadata request ?&lt;/p&gt;

&lt;p&gt;This is not required and is potentially dangerous. It is not required since even if the controller&apos;s cache information is undergoing change and we send stale information to the client, it will get an error and retry. Eventually, the cache will be consistent and the accurate metadata will be sent to the client. This is also ok since changes to the controller&apos;s cache are relatively rare so chances of stale metadata are low.&lt;/p&gt;

&lt;p&gt;4. Do we need to make the timeout for the forwarded metadata request configurable ?&lt;/p&gt;

&lt;p&gt;Ideally no, except for unit tests. The reason is unit tests involve shutting down kafka brokers. However, when a broker is shut down in a unit test, it does not close the socket connections associated with that broker. The impact of that is if a controller broker is shut down, it will not respond to few forwarded metadata requests and it will not close the socket. That leads to some other broker indefinitely wait on receiving a response from the controller. This doesn&apos;t happen in non unit test environments, since if the broker is shutdown or fails, the processes releases all socket connections associated with it. So the other broker gets a broken pipe exception and doesn&apos;t end up waiting forever&lt;/p&gt;</comment>
                            <comment id="13654667" author="jjkoshy" created="Fri, 10 May 2013 17:56:40 +0000"  >&lt;p&gt;Haven&apos;t looked at the patch yet, but went through the overview. An alternate approach that we may want to consider is to maintain a metadata cache at every broker. The cache can be kept consistent by having the controller send a (new) update-metadata request to all brokers whenever it sends out a leaderAndIsr request. A new request type would avoid needing to &quot;overload&quot; the leader and isr request.&lt;/p&gt;

&lt;p&gt;This would help avoid the herd effect of multiple clients flooding the controller with metadata requests (although these requests should return quickly with your patch).&lt;/p&gt;</comment>
                            <comment id="13657713" author="nehanarkhede" created="Tue, 14 May 2013 23:58:56 +0000"  >&lt;p&gt;I like Joel&apos;s suggestion for 2 reasons -&lt;/p&gt;

&lt;p&gt;1. Like he mentioned, if the controller pushes metadata updates to brokers, it will avoid the herd effect when multiple clients need to update metadata.&lt;br/&gt;
2. It is not a good idea to overload LeaderAndIsrRequest with UpdateMetadata since those 2 requests are fired under different circumstances. Part of this complication is also due to the fact that LeaderAndIsrRequest is also overloaded by start replica state change.&lt;/p&gt;

&lt;p&gt;The latest patch includes Joel&apos;s suggestion. It includes the following changes -&lt;/p&gt;

&lt;p&gt;1. A new controller state change request and response is defined - UpdateMetadataRequest and UpdateMetadataResponse. UpdateMetadataRequest has the partition state information like leader, isr, replicas for a list of partitions. In addition to this, it also has a list of live brokers and the broker id -&amp;gt; host:port mapping for all brokers in the cluster. The live brokers information is used when the broker handles metadata request to figure out if the leader is alive or not. UpdateMetadataResponse is similar to LeaderAndIsrResponse, in the sense that it has an error code per partition and a top level error code just like any other state change request&lt;/p&gt;

&lt;p&gt;2. Every kafka broker maintains 3 data structures - leader cache, the list of alive brokers, the broker id-&amp;gt; host:port mapping for all brokers. These data structures are updated by the UpdateMetadataRequest and queried by the TopicMetadataRequest. So those accesses need to be synchronized&lt;/p&gt;

&lt;p&gt;3. The controller fires the update metadata request -&lt;br/&gt;
3.1 When a new broker is started up. The newly restarted brokers are sent the partition state info for all partitions in the cluster.&lt;br/&gt;
3.2 When a broker fails, since leaders for some partitions would&apos;ve changed&lt;br/&gt;
3.3 On a controller failover, since there could&apos;ve been leader changes during the failover&lt;br/&gt;
3.4 On preferred replica election, since leaders for many partitions could&apos;ve changed&lt;br/&gt;
3.5 On partition reassignment, since leader could&apos;ve changed for the reassigned partitions&lt;br/&gt;
3.6 On controlled shutdown, since leaders move for the partitions hosted on the broker being shut down&lt;/p&gt;

&lt;p&gt;4. Unit tests have changed to wait until the update metadata request has trickled to all servers. The best way I could think of is to make the KafkaApis object and the leaderCache accessible from KafkaServer.&lt;/p&gt;</comment>
                            <comment id="13658505" author="junrao" created="Wed, 15 May 2013 16:22:28 +0000"  >&lt;p&gt;Thanks for the patch. Overall, this is a good patch. It adds the new functionality in a less intrusive way to the controller. Let&apos;s spend a bit more time on the wire protocol change since it may not be trivial to change it later. Some comments:&lt;/p&gt;

&lt;p&gt;1. KafkaController:&lt;br/&gt;
1.1 It seems that everytime that we try to send a LeaderAndIsrRequest, we follow that with an UpdateMetadataRequest. Would it be simpler to consolidate the logic in ControllerBrokerRequestBatch.sendRequestsToBrokers() such that for every LeaderAndIsrRequest that we send, we also send an UpdateMetadataRequest (with partitions in LeaderAndIsrReqeust) to all live brokers?&lt;br/&gt;
1.2 ControllerContext: Not sure why we need allBrokers. Could we just return liveBrokersUnderlying as all brokers?&lt;br/&gt;
1.3 The commet under sendUpdateMetadataRequest is not accurate since it doesn&apos;t always just send to new brokers.&lt;br/&gt;
1.4 removeReplicaFromIsr():&lt;br/&gt;
1.4.1 The logging in the following statement says ISR, but actually prints both leaderAndISr.&lt;br/&gt;
debug(&quot;Removing replica %d from ISR %s for partition %s.&quot;&lt;br/&gt;
1.4.2 Two new statements are added to change partitionLeadershipInfo. Is that fixing an existing issue not related to metadata?&lt;/p&gt;

&lt;p&gt;2. KafkaApis: Not sure why we need to maintain both aliveBrokers and allBrokers. It seems just storing aliveBrokers (with the broker info) is enough for the purpose of answering metadata request.&lt;/p&gt;

&lt;p&gt;3. ControllerBrokerRequestBatch: If we just need to send live brokers in the updateMetadata request, there is no need to maintain aliveBrokers and allBrokers here.&lt;/p&gt;

&lt;p&gt;4. PartitionStateInfo:&lt;br/&gt;
4.1 Now that we are sending allReplicas, there is no need to explicitly send replicationFactor.&lt;br/&gt;
4.2 We encode ISR as strings, but all replicas as ints. We should make them consistent. It&apos;s better to encode ISR as ints too.&lt;/p&gt;

&lt;p&gt;5. UpdateMetadataRequest: Not sure why we need ackTimeoutMs.&lt;/p&gt;

&lt;p&gt;6. BrokerPartitionInfo: Are the changes in getBrokerPartitionInfo() necessary? If partitionMetadata is empty, the caller in DefaultEventHandler already throws NoBrokersForPartitionException.&lt;/p&gt;

&lt;p&gt;7. ConsumerFetcherManager.LeaderFinderThread: The code change here is just for logging. Would it be simpler to just log the metadata response in debug mode? If we want to see the exception type associated with the error coder, we can fix the toString() method in metadata response.&lt;/p&gt;

&lt;p&gt;8. AdminUtils:&lt;br/&gt;
8.1 Could you explain why the test testGetTopicMetadata() is deleted?&lt;br/&gt;
8.2 To ensure that the metadata is propagated to all brokers, could we add a utility function waitUntilMetadataPropagated() that takes in a list of brokers, a list of topics and a timeout? We can reuse this function in all relevant tests.&lt;/p&gt;

&lt;p&gt;9. AsyncProducerTest.testInvalidPartition(): Not sure about the change. If we hit any exception (including UnknowTopicOrPartitionException) during partitionAndCollate(), the event handler will retry. &lt;/p&gt;</comment>
                            <comment id="13658959" author="swapnilghike" created="Wed, 15 May 2013 22:55:42 +0000"  >&lt;p&gt;Just to be sure, the two attached patches are independent of each other, right?&lt;/p&gt;</comment>
                            <comment id="13659005" author="nehanarkhede" created="Wed, 15 May 2013 23:23:05 +0000"  >&lt;p&gt;That&apos;s correct.&lt;/p&gt;</comment>
                            <comment id="13659674" author="nehanarkhede" created="Thu, 16 May 2013 16:22:03 +0000"  >&lt;p&gt;Thanks for the great review!&lt;/p&gt;

&lt;p&gt;1. KafkaController:&lt;br/&gt;
1.1 This is a good suggestion, however it wouldn&apos;t suffice in the following cases -&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new broker startup - Here we have to send the metadata for all partitions to the new brokers. The leader and isr request only sends the relevant partitions&lt;/li&gt;
	&lt;li&gt;controller failover - Here we have to send metadata for all partitions to all brokers&lt;/li&gt;
	&lt;li&gt;partition reassignment - Here we have to send another metadata request just to communicate the change in isr and other replicas.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For now, I&apos;ve left the old calls to sendUpdateMetadataRequest commented out to show what has changed. I will remove those comments before check in. I still think that the send update metadata request handling can be optimized to make it reach the brokers sooner, but every optimization will come with a risk. So I suggest, we first focus on correctness and then optimize if it works on large deployments. &lt;/p&gt;

&lt;p&gt;1.2 ControllerContext: I thought that it is unintuitive to not send broker information and only send broker id for brokers that are offline. There was a bug filed for this where users complained it was unintuitive. However, this change will need more thought to do it correctly. So I might include another patch to fix it properly. This patch doesn&apos;t have this change&lt;/p&gt;

&lt;p&gt;1.3 Fixed&lt;br/&gt;
1.4.1 Fixed&lt;br/&gt;
1.4.2 Correct, it is to fix updating the partition leadership info while shrinking isr since the leader can also change in those cases and we use partition leadership info while sending update metadata request, so it should always be kept current&lt;/p&gt;

&lt;p&gt;2,3. Same concern as 1.2&lt;/p&gt;

&lt;p&gt;4. PartitionStateInfo:&lt;br/&gt;
4.1 We still need to send the number of all replicas to be able to deserialize the replica list correctly, which is the replication factor.&lt;br/&gt;
4.2 Good point, changed that&lt;/p&gt;

&lt;p&gt;5. Good observation. This was somehow leftover in all controller state change requests. Didn&apos;t make sense, so removed it from LeaderAndIsrRequest, StopReplicaRequest and UpdateMetadataRequest&lt;/p&gt;

&lt;p&gt;6. It really didn&apos;t make sense to me that the producer throw NoBrokersForPartitionException when in reality it had failed to fetch metadata. This will help us read errors better&lt;/p&gt;

&lt;p&gt;7. Good point, moved it to the toString() API of TopicMetadata&lt;/p&gt;

&lt;p&gt;8. AdminUtils:&lt;br/&gt;
8.1 Because it is a duplicate of the test in TopicMetadataTest.&lt;br/&gt;
8.2 Good point, added that and changed all tests to use it&lt;/p&gt;

&lt;p&gt;9. Ideally yes. But the old code was not retrying for any exception, including UnknownTopicOrPartitionException. I&apos;ve changed DefaultEventHandler to retry no matter what exception it hits. So the test is changed to reflect that it shouldn&apos;t give up with UnknownTopicOrPartitionException but instead should retry sending the message and succeed.&lt;br/&gt;
~                                                                                                           &lt;/p&gt;</comment>
                            <comment id="13660269" author="nehanarkhede" created="Fri, 17 May 2013 02:13:41 +0000"  >&lt;p&gt;Changes in the latest patch include -&lt;/p&gt;

&lt;p&gt;1. Removed the all brokers and just included alive brokers in the update metadata request. So the topic metadata will not include broker information for dead brokers.&lt;/p&gt;

&lt;p&gt;2. My guess about there being a bug that with the update metadata request processing was right. The bug doesn&apos;t affect the correctness of update metadata, but it just delays the communication of new leaders to all brokers. The bug was that we were removing the broker going through controlled shutdown from the alive brokers list before it is really shutdown. So from a client&apos;s perspective, it takes much longer for a new leader to be available. Fixed it to include shutting down brokers in the list of alive brokers&lt;/p&gt;

&lt;p&gt;3. Fixed another bug related to new topic creation. This bug caused the controller to not communicate the leaders of newly created topics to all brokers causing metadata requests to fail.&lt;/p&gt;

&lt;p&gt;4. Tested this fix with 100s of migration tools sending data to ~400 topics to a 7 node cluster. There are ~500 consumers consuming data from this cluster. The test continuously bounces the brokers in a rolling restart fashion. The clients notice the new leaders within few 10s of ms in most cases.&lt;/p&gt;

&lt;p&gt;5. Also the queue time for all requests is mostly &amp;lt; 10 ms since metadata requests are not a bottleneck in the system anymore. The latency of a metadata request for ~300 topics itself has dropped from 10s of seconds to 10s of ms.&lt;/p&gt;</comment>
                            <comment id="13660835" author="junrao" created="Fri, 17 May 2013 16:28:00 +0000"  >&lt;p&gt;Thanks for patch v4. A few more comments:&lt;/p&gt;

&lt;p&gt;40. KafkaController.ControllerContext: This is not introduced in this patch, but serveOrShuttingDownBrokerIds should just be liveBrokerIdUnderlying.&lt;/p&gt;

&lt;p&gt;41.ControllerBrokerRequestBatch: Instead of maintaining aliveBrokers, could we just get it from controllerContext?&lt;/p&gt;

&lt;p&gt;42. KafkaApis.handleTopicMetadataRequest:&lt;br/&gt;
42.1 We can rewrite the following statement&lt;br/&gt;
      val partitionMetadata = sortedPartitions.map { partitionReplicaMap =&amp;gt; &lt;br/&gt;
   to &lt;br/&gt;
      val partitionMetadata = sortedPartitions.map { case(topicAndPartition, partitionState) =&amp;gt;&lt;br/&gt;
 Then, we don&apos;t have to redefine topicAndPartition and partitionState.&lt;br/&gt;
42.2 The val partitionReplicaAssignment seems unintuitive. Should we rename it to partitionStateInfo?&lt;br/&gt;
42.3 The outermost try/catch is unnecessary since it should be handled by the caller handle().&lt;br/&gt;
42.4 Not sure if we need to log the following error since it&apos;s either due to LeaderNotAvailable or ReplicaNotAvailable, both are expected.&lt;br/&gt;
  error(&quot;Error while fetching topic metadata for topic %s due to %s &quot;.format(topicMetadata.topic,&lt;/p&gt;

&lt;p&gt;43. UpdateMetadataRequest: This class needs to define handleError(). This method is actually required to be defined in every request. So we should remove the empty body of handleError() in RequestOrResponse.&lt;/p&gt;

&lt;p&gt;44. UpdateMetadataResponse: Do we really need the per partition level error code? It seems that a global error code is enough.&lt;/p&gt;

&lt;p&gt;45. ConsumerFetcherManager: We should put the following statement under logger.isDebugEnabled().&lt;br/&gt;
          topicsMetadata.foreach(topicMetadata =&amp;gt; debug(topicMetadata.toString()))&lt;/p&gt;

&lt;p&gt;46. TopicMetadata.toString(): It only prints the leader. We need to print other fields in PartitionMetadata too.&lt;/p&gt;

&lt;p&gt;47. BrokerPartitionInfo: If the metadata response has no error, it seems that we show throw an UnknownTopicOrPartitionException, instead of a KafkaException. Alternatively, should we not throw exception at all in this case since the caller already has to deal with the case when there is no metadata?&lt;/p&gt;

&lt;p&gt;48. AsyncProducerTest.testInvalidPartition(): The message in the following statement is a bit missing leading. It&apos;s probably better to say sth like &quot;Should not thrown any exception&quot;. Actually, instead of catching just UnknownTopicOrPartitionException, we should catch and fail any exception.&lt;br/&gt;
        case e: UnknownTopicOrPartitionException =&amp;gt; fail(&quot;Should fail with UnknownTopicOrPartitionException&quot;)&lt;/p&gt;</comment>
                            <comment id="13660866" author="nehanarkhede" created="Fri, 17 May 2013 17:11:23 +0000"  >&lt;p&gt;I&apos;m not sure I understood your review comment #47. Other review comments are addressed.&lt;/p&gt;</comment>
                            <comment id="13660988" author="nehanarkhede" created="Fri, 17 May 2013 19:43:47 +0000"  >&lt;p&gt;All review comments are addressed. Also made another change to update metadata request handling. Basically, if an update metadata request from a stale controller epoch arrives at the broker, it should reject that request&lt;/p&gt;</comment>
                            <comment id="13661104" author="junrao" created="Fri, 17 May 2013 22:32:27 +0000"  >&lt;p&gt;Thanks for patch v5. Looks good. +1. I have some minor follow up comments that we can address in a separate patch.&lt;/p&gt;</comment>
                            <comment id="13661113" author="junrao" created="Fri, 17 May 2013 22:43:37 +0000"  >&lt;p&gt;Some minor comments:&lt;/p&gt;

&lt;p&gt;50. KafkaApis.handleTopicMetadataRequest: partitionStateOpt is not necessary since inside sortedPartitions.map { }, it can&apos;t be undefined.&lt;/p&gt;

&lt;p&gt;51. AsyncProducerTest.testInvalidPartition(): In the case statement inside catch, let&apos;s catch all exceptions and fail if it happens.&lt;/p&gt;</comment>
                            <comment id="13661165" author="nehanarkhede" created="Sat, 18 May 2013 00:00:17 +0000"  >&lt;p&gt;Thanks for your careful reviews. Checked in patch v5 and the follow up comments.&lt;/p&gt;</comment>
                            <comment id="13662177" author="nehanarkhede" created="Mon, 20 May 2013 17:29:42 +0000"  >&lt;p&gt;Attaching a follow up patch that fixes 2 minor things -&lt;/p&gt;

&lt;p&gt;1. Added update metadata request handling to the state change log. This makes is much easier to troubleshoot any issue with metadata cache refreshing&lt;br/&gt;
2. A minor bug in the controller channel manager that fixes it to read an UpdateMetadataResponse properly. &lt;/p&gt;</comment>
                            <comment id="13663063" author="junrao" created="Tue, 21 May 2013 15:46:35 +0000"  >&lt;p&gt;Thanks for the followup patch. Some comments:&lt;/p&gt;

&lt;p&gt;60. KafkaApis: The following logging logs the whole request for each partition. This will probably pollute the log. Is it enough just to log the whole request once?&lt;br/&gt;
    if(stateChangeLogger.isTraceEnabled)&lt;br/&gt;
      updateMetadataRequest.partitionStateInfos.foreach(p =&amp;gt; stateChangeLogger.trace((&quot;Broker %d handling &quot; +&lt;br/&gt;
        &quot;UpdateMetadata request %s correlation id %d received from controller %d epoch %d for partition %s&quot;)&lt;br/&gt;
        .format(brokerId, p._2, updateMetadataRequest.correlationId, updateMetadataRequest.controllerId,&lt;br/&gt;
        updateMetadataRequest.controllerEpoch, p._1)))&lt;br/&gt;
Is the following logging necessary? If we know a request, we already know what should be in the cache after processing the request.&lt;br/&gt;
        if(stateChangeLogger.isTraceEnabled)&lt;br/&gt;
          stateChangeLogger.trace((&quot;Broker %d caching leader info %s for partition %s in response to UpdateMetadata request sent by controller %d&quot; +&lt;br/&gt;
            &quot; epoch %d with correlation id %d&quot;).format(brokerId, partitionState._2, partitionState._1,&lt;br/&gt;
            updateMetadataRequest.controllerId, updateMetadataRequest.controllerEpoch, updateMetadataRequest.correlationId))&lt;br/&gt;
      }&lt;/p&gt;

</comment>
                            <comment id="13663084" author="nehanarkhede" created="Tue, 21 May 2013 16:18:26 +0000"  >&lt;p&gt;I agree. Kept only the 2nd logging message about caching the leader info.&lt;/p&gt;

&lt;p&gt;Also, both log messages logged only the partition leader info, not the whole request. &lt;/p&gt;

&lt;p&gt;Fixed another exception in ControllerChannelManager &lt;/p&gt;</comment>
                            <comment id="13663118" author="junrao" created="Tue, 21 May 2013 16:53:13 +0000"  >&lt;p&gt;Thanks for the second followup patch. +1.&lt;/p&gt;</comment>
                            <comment id="13663131" author="nehanarkhede" created="Tue, 21 May 2013 17:07:46 +0000"  >&lt;p&gt;Thanks for the quick review, committed it&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12583861" name="kafka-901-followup.patch" size="3667" author="nehanarkhede" created="Mon, 20 May 2013 17:29:42 +0000"/>
                            <attachment id="12584030" name="kafka-901-followup2.patch" size="4782" author="nehanarkhede" created="Tue, 21 May 2013 16:18:26 +0000"/>
                            <attachment id="12583495" name="kafka-901-v2.patch" size="141223" author="nehanarkhede" created="Thu, 16 May 2013 16:22:03 +0000"/>
                            <attachment id="12583603" name="kafka-901-v4.patch" size="136346" author="nehanarkhede" created="Fri, 17 May 2013 02:13:41 +0000"/>
                            <attachment id="12583681" name="kafka-901-v5.patch" size="136854" author="nehanarkhede" created="Fri, 17 May 2013 19:43:47 +0000"/>
                            <attachment id="12583245" name="kafka-901.patch" size="119532" author="nehanarkhede" created="Tue, 14 May 2013 23:58:56 +0000"/>
                            <attachment id="12582557" name="metadata-request-improvement.patch" size="93090" author="nehanarkhede" created="Fri, 10 May 2013 00:58:50 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>327284</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 26 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1kgwv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>327628</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>