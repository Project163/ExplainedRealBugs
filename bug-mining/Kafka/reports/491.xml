<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:41:02 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-1539] Due to OS caching Kafka might loose offset files which causes full reset of data</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-1539</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Seen this while testing power failure and disk failures. Due to chaching on OS level (eg. XFS can cache data for 30 seconds) after failure we got offset files of zero length. This dramatically slows down broker startup (it have to re-check all segments) and if high watermark offsets lost it simply erases all data and start recovering from other brokers (looks funny - first spending 2-3 hours re-checking logs and then deleting them all due to missing high watermark).&lt;/p&gt;

&lt;p&gt;Proposal: introduce offset files rotation. Keep two version of offset file, write to oldest, read from the newest valid. In this case we would be able to configure offset checkpoint time in a way that at least one file is alway flushed and valid.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12727045">KAFKA-1539</key>
            <summary>Due to OS caching Kafka might loose offset files which causes full reset of data</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jkreps">Jay Kreps</assignee>
                                    <reporter username="dmitrybugaychenko">Dmitry Bugaychenko</reporter>
                        <labels>
                    </labels>
                <created>Mon, 14 Jul 2014 07:38:50 +0000</created>
                <updated>Tue, 22 Jul 2014 14:21:18 +0000</updated>
                            <resolved>Mon, 21 Jul 2014 23:57:52 +0000</resolved>
                                    <version>0.8.1.1</version>
                                    <fixVersion>0.8.2.0</fixVersion>
                                    <component>log</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14060413" author="joestein" created="Mon, 14 Jul 2014 07:49:25 +0000"  >&lt;p&gt;Did you have the log.flush.interval.messages == 1 when doing this?  If not then you can either do something like that and sacrifice performance and futz with a single broker flush or have (instead) replicas/brokers outside of zones that are sharing power grids for a partition you are working with.  Use replication to achieve your durability with less sacrifice to performance using more than one broker. If you need/want something within a single broker there are lots of toggle to use in the broker configuration &lt;a href=&quot;https://kafka.apache.org/documentation.html#brokerconfigs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://kafka.apache.org/documentation.html#brokerconfigs&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="14060456" author="dmitrybugaychenko" created="Mon, 14 Jul 2014 08:58:16 +0000"  >&lt;p&gt;This is not about log files themselves, but about chekpoint offset files &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;-rw-r--r--  1 root root   158 Jul 14 12:11 recovery-point-offset-checkpoint
-rw-r--r--  1 root root   163 Jul 14 12:11 replication-offset-checkpoint
-rw-r--r--  1 root root     0 May 28 13:09 cleaner-offset-checkpoint
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If recovery-point-offset-checkpoint got corrupted, broker startup slows down dramatically (to hours), if replication-offset-checkpoint got corrupted, then broker removes all the data it has and starts recovering from other replicas. If both got corrupted then you get both - broker spending hours checking log segment files and then removeing them all.&lt;/p&gt;</comment>
                            <comment id="14061613" author="junrao" created="Tue, 15 Jul 2014 03:23:03 +0000"  >&lt;p&gt;Hmm, interesting. The way that we checkpoint those offset files is to first write to a temp file, force a flush, and then do a rename. This is very close to the two versions of the offset file that you are proposing. Not sure how a zero length file is introduced. Is that easily reproducible?&lt;/p&gt;</comment>
                            <comment id="14061724" author="dmitrybugaychenko" created="Tue, 15 Jul 2014 06:05:08 +0000"  >&lt;p&gt;It looks like even after flush data are not necesary written to HDD. In XFS by default it could be cached up to 30 secodns, it also can be cached by a disk controller and etc. Wrtiting to temp file is a good idea, but it is better to keep the previous file untouched (do not replace it with the temp one).&lt;/p&gt;

&lt;p&gt;On a 20 HDD server with XFS it is pretty easy to reproduce - after power failure we got corrupted offset files on 4-5 disks.&lt;/p&gt;</comment>
                            <comment id="14062973" author="junrao" created="Wed, 16 Jul 2014 01:36:02 +0000"  >&lt;p&gt;If flush is not guaranteed, will keeping two versions of the file help? At some point, we will have flushed both versions and neither one is guaranteed to persist.&lt;/p&gt;</comment>
                            <comment id="14068010" author="dmitrybugaychenko" created="Sun, 20 Jul 2014 18:29:52 +0000"  >&lt;p&gt;Digged the problem a bit more. It looks like calling flush on new BufferedWriter(new FileWriter(temp)) only forces buffered writer to dump everything into a FileOutputStream under the FileWriter and call flush on it. However, according to &lt;a href=&quot;http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7u40-b43/java/io/FileOutputStream.java#FileOutputStream&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7u40-b43/java/io/FileOutputStream.java#FileOutputStream&lt;/a&gt; it does nothing. In order to really force data to be written to disk you need to call fos.getFD().sync(). According to that the patch could be like that:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  def write(offsets: Map[TopicAndPartition, &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;]) {
    lock &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; {
      &lt;span class=&quot;code-comment&quot;&gt;// write to temp file and then swap with the existing file
&lt;/span&gt;      val temp = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; File(file.getAbsolutePath + &lt;span class=&quot;code-quote&quot;&gt;&quot;.tmp&quot;&lt;/span&gt;)

      val fileOutputStream = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FileOutputStream(temp)
      val writer = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BufferedWriter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FileWriter(fileOutputStream))
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        &lt;span class=&quot;code-comment&quot;&gt;// write the current version
&lt;/span&gt;        writer.write(0.toString)
        writer.newLine()
      
        &lt;span class=&quot;code-comment&quot;&gt;// write the number of entries
&lt;/span&gt;        writer.write(offsets.size.toString)
        writer.newLine()

        &lt;span class=&quot;code-comment&quot;&gt;// write the entries
&lt;/span&gt;        offsets.foreach { &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (topicPart, offset) =&amp;gt;
          writer.write(&lt;span class=&quot;code-quote&quot;&gt;&quot;%s %d %d&quot;&lt;/span&gt;.format(topicPart.topic, topicPart.partition, offset))
          writer.newLine()
        }
      
        &lt;span class=&quot;code-comment&quot;&gt;// flush and overwrite old file
&lt;/span&gt;        writer.flush()
        
        &lt;span class=&quot;code-comment&quot;&gt;// Force fsync to disk
&lt;/span&gt;        fileOutputStream.getFD.sync()
      } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
        writer.close()
      }
      
      &lt;span class=&quot;code-comment&quot;&gt;// swap &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; offset checkpoint file with previous one
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(!temp.renameTo(file)) {
        &lt;span class=&quot;code-comment&quot;&gt;// renameTo() fails on Windows &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the destination file exists.
&lt;/span&gt;        file.delete()
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(!temp.renameTo(file))
          &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;File rename from %s to %s failed.&quot;&lt;/span&gt;.format(temp.getAbsolutePath, file.getAbsolutePath))
      }
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the problem is easily reproducable only on XFS, ext3/ext4 seems to handle this case much better. Hope we will be able to try the patch later this week and check if it helps.&lt;/p&gt;</comment>
                            <comment id="14068831" author="jkreps" created="Mon, 21 Jul 2014 17:24:24 +0000"  >&lt;p&gt;Created reviewboard &lt;a href=&quot;https://reviews.apache.org/r/23743/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/23743/&lt;/a&gt;&lt;br/&gt;
 against branch trunk&lt;/p&gt;</comment>
                            <comment id="14068837" author="jkreps" created="Mon, 21 Jul 2014 17:25:34 +0000"  >&lt;p&gt;This is a really good catch, were clearly thinking flush() meant fsync, which is totally wrong. I uploaded a patch with your fix. If you are doing testing with this let me know that this actually fixes the issue you saw.&lt;/p&gt;</comment>
                            <comment id="14068848" author="sriramsub" created="Mon, 21 Jul 2014 17:29:46 +0000"  >&lt;p&gt;I had encountered the same issue in another project and had to explicitly use fsync to fix it.&lt;/p&gt;</comment>
                            <comment id="14069547" author="jkreps" created="Mon, 21 Jul 2014 23:57:52 +0000"  >&lt;p&gt;I&apos;m checking this in since it seems to fix a clear problem, but &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=arhagnel&quot; class=&quot;user-hover&quot; rel=&quot;arhagnel&quot;&gt;arhagnel&lt;/a&gt; it would still be good to get confirmation that the problem you were producing is fixed by this.&lt;/p&gt;</comment>
                            <comment id="14069899" author="dmitrybugaychenko" created="Tue, 22 Jul 2014 06:32:53 +0000"  >&lt;p&gt;Going to test power failure again later today, I&apos;ll get back with results as soon as we get them.&lt;/p&gt;</comment>
                            <comment id="14070274" author="dmitrybugaychenko" created="Tue, 22 Jul 2014 13:58:16 +0000"  >&lt;p&gt;With fileOutputStream.getFD.sync() patch we passed the power failure tests without loosing offset files. So, it seems to work.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12656897" name="KAFKA-1539.patch" size="1110" author="jkreps" created="Mon, 21 Jul 2014 17:24:17 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>405152</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 18 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1xq53:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>405187</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>