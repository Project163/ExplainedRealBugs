<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:39:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-1112] broker can not start itself after kafka is killed with -9</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-1112</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;When I kill kafka with -9, broker cannot start itself because of corrupted index logs. I think kafka should try to delete/rebuild indexes itself without manual intervention. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12677081">KAFKA-1112</key>
            <summary>broker can not start itself after kafka is killed with -9</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jkreps">Jay Kreps</assignee>
                                    <reporter username="KaneK">Kane Kim</reporter>
                        <labels>
                    </labels>
                <created>Fri, 1 Nov 2013 15:03:52 +0000</created>
                <updated>Wed, 14 Jan 2015 05:22:10 +0000</updated>
                            <resolved>Tue, 19 Nov 2013 02:33:03 +0000</resolved>
                                    <version>0.8.0</version>
                    <version>0.8.1</version>
                                    <fixVersion>0.8.1</fixVersion>
                                    <component>log</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="13811473" author="dmaverick" created="Fri, 1 Nov 2013 17:22:51 +0000"  >&lt;p&gt;We&apos;ve also faced with such behavior. Moreover, it doesn&apos;t fail right after startup, it start listening for requests(at least open the port) before checking  the index and syncing internal state. Thus it kind of difficult to figure out from startup script whether Kafka actually started without adding some ugly sleeps.&lt;/p&gt;</comment>
                            <comment id="13814110" author="nehanarkhede" created="Tue, 5 Nov 2013 18:43:23 +0000"  >&lt;p&gt;Stack trace -&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-11-01 17:46:02,685&amp;#93;&lt;/span&gt; INFO Loading log &apos;foo-4&apos; (kafka.log.LogManager)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-11-01 17:46:04,898&amp;#93;&lt;/span&gt; FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)&lt;br/&gt;
java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/mnt/u001/temp/kafka-logs/foo-4/00000000000000000000.index) has non-zero size but the last offset is 0 and the base offset is 0&lt;br/&gt;
at scala.Predef$.require(Predef.scala:145)&lt;br/&gt;
at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:161)&lt;br/&gt;
at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:160)&lt;br/&gt;
at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)&lt;br/&gt;
at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)&lt;br/&gt;
at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaCo&lt;/p&gt;</comment>
                            <comment id="13814134" author="guozhang" created="Tue, 5 Nov 2013 18:57:03 +0000"  >&lt;p&gt;Attached another stack trace with more debugging turned on. The root cause is that when we load the index file, the initial size is set to the limit of the file, and hence the position is pointing to the last entry. In most cases the last entry will be 0, and the recoveryLog process will skip since it shows the latest offset is smaller than the checkpoint. But when doing the sanity check it finds the number of entries is non-zero (actually the max number of entries) while the last offset is equal to the base offset since reading the last entry gives you 0 relative offset.&lt;/p&gt;</comment>
                            <comment id="13817265" author="dmaverick" created="Fri, 8 Nov 2013 13:21:14 +0000"  >&lt;p&gt;Could be related to this issue &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-757&quot; title=&quot;System Test Hard Failure cases : &amp;quot;Fatal error during KafkaServerStable startup&amp;quot; when hard-failed broker is re-started&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-757&quot;&gt;&lt;del&gt;KAFKA-757&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13817628" author="guozhang" created="Fri, 8 Nov 2013 19:53:33 +0000"  >&lt;p&gt;Yes. We have realized this bug before and tried to fix it, but it seems we still missed some cases.&lt;/p&gt;</comment>
                            <comment id="13820948" author="jkreps" created="Wed, 13 Nov 2013 05:04:25 +0000"  >&lt;p&gt;The way the check was supposed to work was this: if the last offset in the file is the recoveryPoint-1 then skip the recovery (because the whole file is flushed). The way this was implemented was by using the last entry in the index to find the final message.&lt;/p&gt;

&lt;p&gt;Overall I feel this is a bit of a hack, but we wanted to separate out the &quot;fsync is async&quot; feature from a full incremental recovery implementation that only recovers unflushed data.&lt;/p&gt;

&lt;p&gt;The immediate problem was that we broke the short circuit by adding code to try to handle a corner case: what if log is truncated after to a flush and hence the end of the log is &amp;lt; recovery point. This was just totally broken and we were short circuiting out of the check in virtually all cases including corrupt index.&lt;/p&gt;

&lt;p&gt;This issue wasn&apos;t caught because there was a bug in the log corruption unit test that gave a false pass on all index corruptions. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The fix is the following:&lt;br/&gt;
1. Fix the logical bug&lt;br/&gt;
2. Add LogSegment.needsRecovery() which is a more paranoid version of what we were doing before that attempts to be safe regardless of any index or log corruption that may have occurred. Having this method here is a little hacky but probably okay until we get a full incremental recovery impl.&lt;br/&gt;
3. Fix the unit test that covers this.&lt;/p&gt;</comment>
                            <comment id="13821554" author="junrao" created="Wed, 13 Nov 2013 17:25:15 +0000"  >&lt;p&gt;Thanks for the patch. A few comments.&lt;/p&gt;

&lt;p&gt;1. I am a bit concerned of depending on a potentially corrupted index to look for recoveryPoint - 1 in LogSegment.needsRecovery(). If the index points to an arbitrary position in FileMessageSet, the offset value that FileMessageSet.searchFor() finds  is garbage. If that value happens to be larger than targetOffset, we will assume that we find targetOffset, but in fact we haven&apos;t.&lt;/p&gt;

&lt;p&gt;2. LogTest.testCorruptLog(): Is the println statement needed?&lt;/p&gt;

&lt;p&gt;3. Could you rebase?&lt;/p&gt;</comment>
                            <comment id="13821591" author="nehanarkhede" created="Wed, 13 Nov 2013 17:56:59 +0000"  >&lt;p&gt;Thanks for the patch. Few comments -&lt;/p&gt;

&lt;p&gt;1. In the most common case of needsRecovery, the position of the last entry will be zero. In this case, we will search the entire log segment up until the recovery point. This will slow down server startup but probably only when we really need recovery.&lt;br/&gt;
2. LogSegment: We have to be carefully =&amp;gt; We have to be careful&lt;br/&gt;
3. Log: If sanityCheck throws an exception, can we automatically invoke index rebuild instead of bailing out?&lt;br/&gt;
4. Could you rebase?&lt;br/&gt;
5. Could you give the patch review tool a spin? The setup is minimal and we can save time for this and future reviews - &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Kafka+patch+review+tool#Kafkapatchreviewtool-1.Setup&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Kafka+patch+review+tool#Kafkapatchreviewtool-1.Setup&lt;/a&gt;&lt;br/&gt;
Usage: &lt;br/&gt;
python kafka-patch-review.py -j &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1112&quot; title=&quot;broker can not start itself after kafka is killed with -9&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1112&quot;&gt;&lt;del&gt;KAFKA-1112&lt;/del&gt;&lt;/a&gt; -b trunk&lt;/p&gt;</comment>
                            <comment id="13821594" author="jkreps" created="Wed, 13 Nov 2013 17:58:53 +0000"  >&lt;p&gt;Actually I am attempting to cover every possible case here, so the only case that should go through is the one where the offset of the final message is recoveryPoint-1 exactly. Notice that the unit test actually runs through 50 cases of random garbage appended to the index so assuming that test is write I think this does work.&lt;/p&gt;</comment>
                            <comment id="13821638" author="guozhang" created="Wed, 13 Nov 2013 18:23:43 +0000"  >&lt;p&gt;Regarding Jun&apos;s comment #1, I am more concerned about using searchFor function on a FileMessageSet that might be corrupted. From the code it seems if FileMessageSet is corrupted the searchFor function may actually not return due to variable position not monotonically increasing?&lt;/p&gt;</comment>
                            <comment id="13822188" author="davidlao" created="Thu, 14 Nov 2013 05:54:28 +0000"  >&lt;p&gt;I hitting this exact issue. For what it&apos;s worth the content of the corrupt index file is consist of 00&apos;s for the entire file.&lt;/p&gt;</comment>
                            <comment id="13822226" author="davidlao" created="Thu, 14 Nov 2013 07:14:09 +0000"  >&lt;p&gt;Jay , can you provide a patch for the 0.8 branch as well? Thanks.&lt;/p&gt;</comment>
                            <comment id="13822900" author="jkreps" created="Thu, 14 Nov 2013 20:57:56 +0000"  >&lt;p&gt;Added a new patch that addresses issues raised.&lt;/p&gt;

&lt;p&gt;Jun&lt;br/&gt;
1. I don&apos;t think this is true. The check is for exact match.&lt;br/&gt;
2. Removed.&lt;br/&gt;
3. Done&lt;/p&gt;

&lt;p&gt;Neha&lt;br/&gt;
1. I think I am handling this--in the case of zero we don&apos;t do a full scan.&lt;br/&gt;
2. Ack, fixed.&lt;br/&gt;
3. Well the sanity check is POST recovery. So if we have a corrupt index after recovery we have a bug, I don&apos;t think we should automatically try to recovery from this (that would be another recovery).&lt;br/&gt;
4. done&lt;br/&gt;
5. Yeah, haven&apos;t had time yet.&lt;/p&gt;</comment>
                            <comment id="13823291" author="junrao" created="Fri, 15 Nov 2013 05:14:40 +0000"  >&lt;p&gt;The following is my confusion.&lt;/p&gt;

&lt;p&gt;The patch relies on a potentially corrupted index to find the right starting position in the segment file. What if the starting position given by the last index entry is corrupted? Then, the position could point to the middle of a message in the segment file. Then, the offset value we read from the segment file could be anything. If that value happens to match recoverPoint - 1, we could think no recovery is needed, but the segment file is actually corrupted. &lt;/p&gt;

&lt;p&gt;Similarly, even if the index file is not corrupted, the segment file could still be corrupted before recoverPoint - 1 (since the unit of flushing is a page). It&apos;s also possible that we read a corrupted piece of data as the offset that happens to match recoverPoint - 1, and therefore incorrectly think that recovery is not needed.&lt;/p&gt;</comment>
                            <comment id="13823822" author="jkreps" created="Fri, 15 Nov 2013 17:14:24 +0000"  >&lt;p&gt;David, this should not be happening in 0.8. If it is I suspect it is a different problem that causes the same bad outcome. Are you seeing this on 0.8? If so how reproducable is it?&lt;/p&gt;</comment>
                            <comment id="13823829" author="jkreps" created="Fri, 15 Nov 2013 17:20:15 +0000"  >&lt;p&gt;Jun, this is true.&lt;/p&gt;

&lt;p&gt;However, if you think about it recovery of the log has the same problem. We read a message and then compare it to its CRC. The CRC is a 32 bit number. The crc could certainly match the message by chance.&lt;/p&gt;

&lt;p&gt;In this case we compare to a 64 bit number so this should be less likely. But in reality there are many rare events here: (1) we hard crash, (2) hard crash leads to corruption, (3) corruption of index points to a location that exactly matches the recovery offset.&lt;/p&gt;

&lt;p&gt;In general I think peoples concern with this approach is that it is just kind of hacky. I agree with this complaint and am sort of disappointed with this set of changes overall.&lt;/p&gt;

&lt;p&gt;I will post a slightly more paranoid version of the check, and then let&apos;s discuss that.&lt;/p&gt;</comment>
                            <comment id="13823879" author="jkreps" created="Fri, 15 Nov 2013 18:04:04 +0000"  >&lt;p&gt;Okay here is a maximally paranoid patch.&lt;/p&gt;</comment>
                            <comment id="13823883" author="guozhang" created="Fri, 15 Nov 2013 18:07:24 +0000"  >&lt;p&gt;How about we resort back to the clean shutdown file for recovery checking, and if recovery is needed, we can use the recovery point to optimize recovery overhead.&lt;/p&gt;</comment>
                            <comment id="13824068" author="jkreps" created="Fri, 15 Nov 2013 20:43:36 +0000"  >&lt;p&gt;Yeah I would not be opposed to that as an alternative. Both are really a hack.&lt;/p&gt;

&lt;p&gt;I guess the questions is what should the end state be?&lt;/p&gt;</comment>
                            <comment id="13825487" author="junrao" created="Mon, 18 Nov 2013 17:13:19 +0000"  >&lt;p&gt;Thinking about this a bit more. The end state is that we want to only recover the portion of the log segment from the recovery point, instead of recovering the whole log segment. The dilemma is that we are not sure what portion of the index is valid. Scanning from the beginning of the log segment defeats the purpose of incremental recovery. One possible solution is to checkpoint an index recovery point, in addition to the recovery offset per log. The index recovery point is the # of valid index entries in the segment to which the recovery offset belongs. This way, on startup, we will be sure that the data in the last valid index entry is not corrupted and we can use it to quickly locate the recovery offset in the log file.&lt;/p&gt;
</comment>
                            <comment id="13825588" author="guozhang" created="Mon, 18 Nov 2013 18:36:06 +0000"  >&lt;p&gt;Did some research on network about &quot;fsync&quot;, and it seems fsync can be reliable even with disk&apos;s block-write behavior since it is sequential, which means even file system crashed during fsync we will not expect random behavior.&lt;/p&gt;
</comment>
                            <comment id="13825620" author="nehanarkhede" created="Mon, 18 Nov 2013 18:57:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; This approach seems reasonable unless I&apos;m missing any caveats in Log. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt; what do you think?&lt;/p&gt;</comment>
                            <comment id="13825708" author="jkreps" created="Mon, 18 Nov 2013 20:02:40 +0000"  >&lt;p&gt;Yeah at a high-level there are a couple of things we could do:&lt;br/&gt;
1. Non-incremental &lt;br/&gt;
    a. Harden the current approach (what the attached patches do)&lt;br/&gt;
    b. Use the clean shutdown file &lt;br/&gt;
2. Implement incremental recovery (what Jun is proposing)&lt;/p&gt;

&lt;p&gt;All of these are good. 1a is implemented, but is arguably gross. I am open to 1b or 2 or a short-term/long-term thing.&lt;/p&gt;

&lt;p&gt;For 2 I think the details to figure out would be &lt;br/&gt;
1. OffsetCheckpoint is shared so adding the position to that file will impact other use cases how will that be handled?&lt;br/&gt;
2. I suspect that if we want to move to positions we should do something like (file, log_position, index_position) rather than a mixture of logical and physical.&lt;br/&gt;
3. We need to ensure that log compaction is thought through. This could cause the physical position to change. That could be fine but we need to reason through it.&lt;br/&gt;
4. We need to ensure that we handle truncation which implies that a position X could be stable, then deleted, then rewritten differently without flush. This may be fine we just have to think it through.&lt;/p&gt;</comment>
                            <comment id="13825831" author="junrao" created="Mon, 18 Nov 2013 22:10:10 +0000"  >&lt;p&gt;Ok, so it seems that the end state is not that simple and may need some more thoughts. I took patch v3 , removed the recovery part in LogSegment and replaced it with the simpler approach using the clean shutdown file.&lt;/p&gt;</comment>
                            <comment id="13825893" author="nehanarkhede" created="Mon, 18 Nov 2013 22:53:08 +0000"  >&lt;p&gt;Thanks for the patch, Jun! Overall, looks good (+1). Few minor comments that you can address on checkin -&lt;/p&gt;

&lt;p&gt;1. Log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;okay we need to actually recovery this log =&amp;gt; okay we need to actually recover this log&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;2. OffsetIndex&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;In sanityCheck(), in one error message, we print the index file&apos;s absolute path and in another, we print only the name. Can we standardize on one? It is better to print the entire path since we can have more than one data directories.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13825922" author="jkreps" created="Mon, 18 Nov 2013 23:07:14 +0000"  >&lt;p&gt;+1 lgtm.&lt;/p&gt;</comment>
                            <comment id="13826087" author="junrao" created="Tue, 19 Nov 2013 02:33:03 +0000"  >&lt;p&gt;Thanks for the reviews. Committed to trunk after addressing Neha&apos;s comments.&lt;/p&gt;</comment>
                            <comment id="13856647" author="dgoya" created="Wed, 25 Dec 2013 19:25:30 +0000"  >&lt;p&gt;Commenting here as requested.&lt;/p&gt;

&lt;p&gt;After migrating a cluster from 0.8.0 to 0.8.1 (trunk/87efda7) I had a few brokers that wouldn&apos;t come up.&lt;/p&gt;

&lt;p&gt;This is the exception I ran into, I was able to fix it by deleting the /data/kafka/logs/Events2-124/ directory.  That directory contained a non zero size index file and a zero size log file.  I had a bunch of these directories scattered around the cluster.  I suspect they were there from partition reassignment failures which happened when the cluster was at 0.8.0.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-12-18 02:40:37,163&amp;#93;&lt;/span&gt; FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)&lt;br/&gt;
java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/data/kafka/logs/Events2-124/00000000000000000000.index) has non-zero size but the last offset is 0 and the base offset is 0&lt;br/&gt;
	at scala.Predef$.require(Predef.scala:145)&lt;br/&gt;
	at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:160)&lt;br/&gt;
	at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:159)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
	at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)&lt;br/&gt;
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)&lt;br/&gt;
	at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)&lt;br/&gt;
	at kafka.log.Log.loadSegments(Log.scala:159)&lt;br/&gt;
	at kafka.log.Log.&amp;lt;init&amp;gt;(Log.scala:64)&lt;br/&gt;
	at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:120)&lt;br/&gt;
	at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:115)&lt;br/&gt;
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)&lt;br/&gt;
	at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)&lt;br/&gt;
	at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:115)&lt;br/&gt;
	at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:107)&lt;br/&gt;
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)&lt;br/&gt;
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)&lt;br/&gt;
	at kafka.log.LogManager.loadLogs(LogManager.scala:107)&lt;br/&gt;
	at kafka.log.LogManager.&amp;lt;init&amp;gt;(LogManager.scala:59)&lt;/p&gt;</comment>
                            <comment id="14059533" author="alexismidon" created="Sat, 12 Jul 2014 00:09:57 +0000"  >&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I suffered from the same error using Kafka 0.8.1. Should I reopen this issue or create a new one?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,696 INFO main kafka.server.KafkaServer.info - [Kafka Server 847605514], starting
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,698 INFO main kafka.server.KafkaServer.info - [Kafka Server 847605514], Connecting to zookeeper on zk-main0.XXX:2181,zk-main1.XXX:2181,zk-main2.XXXX:2181/production/kafka/main
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,708 INFO ZkClient-EventThread-14-zk-main0.XXX.com:2181,zk-main1.XXX.com:2181,zk-main2.XXX.com:2181,zk-main3.XXX.com:2181,zk-main4.XXX.com:2181/production/kafka/main org.I0Itec.zkclient.ZkEventThread.run - Starting ZkClient event thread.
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,714 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:zookeeper.version=3.3.3-1203054, built on 11/17/2011 05:47 GMT
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,714 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:host.name=i-6b948138.inst.aws.airbnb.com
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,714 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.version=1.7.0_55
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.vendor=Oracle Corporation
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.home=/usr/lib/jvm/jre-7-oracle-x64/jre
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;path=libs/snappy-java-1.0.5.jar:libs/scala-library-2.10.1.jar:libs/slf4j-api-1.7.2.jar:libs/jopt-simple-3.2.jar:libs/metrics-annotation-2.2.0.jar:libs/log4j-1.2.15.jar:libs/kafka_2.10-0.8.1.jar:libs/zkclient-0.3.jar:libs/zookeeper-3.3.4.jar:libs/metrics-core-2.2.0.jar
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.io.tmpdir=/tmp
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.compiler=&amp;lt;NA&amp;gt;
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:os.name=Linux
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:os.arch=amd64
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:os.version=3.2.0-61-virtual
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:user.name=kafka
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:user.home=/srv/kafka
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:user.dir=/srv/kafka/kafka_2.10-0.8.1
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,718 INFO main org.apache.zookeeper.ZooKeeper.&amp;lt;init&amp;gt; - Initiating client connection, connectString=zk-main0.XXX.com:2181,zk-main1.XXX.com:2181,zk-main2.XXX.com:2181,zk-main3.XXX.com:2181,zk-main4.XXX.com:2181/production/kafka/main sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4758af63
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,733 INFO main-SendThread() org.apache.zookeeper.ClientCnxn.startConnect - Opening socket connection to server zk-main1.XXX.com/10.12.135.61:2181
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,738 INFO main-SendThread(zk-main1.XXX.com:2181) org.apache.zookeeper.ClientCnxn.primeConnection - Socket connection established to zk-main1.XXX.com/10.12.135.61:2181, initiating session
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,745 INFO main-SendThread(zk-main1.XXX.com:2181) org.apache.zookeeper.ClientCnxn.readConnectResult - Session establishment complete on server zk-main1.XXX.com/10.12.135.61:2181, sessionid = 0x646838f07761601, negotiated timeout = 6000
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,747 INFO main-EventThread org.I0Itec.zkclient.ZkClient.processStateChanged - zookeeper state changed (SyncConnected)
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,961 INFO main kafka.log.LogManager.info - Found clean shutdown file. Skipping recovery &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; all logs in data directory &lt;span class=&quot;code-quote&quot;&gt;&apos;/mnt/kafka_logs&apos;&lt;/span&gt;
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,962 INFO main kafka.log.LogManager.info - Loading log &lt;span class=&quot;code-quote&quot;&gt;&apos;flog-30&apos;&lt;/span&gt;
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg 2014-07-11 - 00:53:18,349 FATAL main kafka.server.KafkaServerStartable.fatal - Fatal error during KafkaServerStable startup. Prepare to shutdown
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg java.lang.IllegalArgumentException: - requirement failed: Corrupt index found, index file (/mnt/kafka_logs/flog-30/00000000000121158146.index) has non-zero size but the last offset is 121158146 and the base offset is 121158146
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.Predef$.require(Predef.scala:233)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.OffsetIndex.sanityCheck(OffsetIndex.scala:352)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:159)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:158)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.IterableLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IterableLike.scala:72)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log.loadSegments(Log.scala:158)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log.&amp;lt;init&amp;gt;(Log.scala:64)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$4.apply(LogManager.scala:118)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$4.apply(LogManager.scala:113)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IndexedSeqOptimized.scala:33)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:105)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:113)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:105)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IndexedSeqOptimized.scala:33)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager.loadLogs(LogManager.scala:105)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager.&amp;lt;init&amp;gt;(LogManager.scala:57)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.server.KafkaServer.createLogManager(KafkaServer.scala:275)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.server.KafkaServer.startup(KafkaServer.scala:72)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.Kafka$.main(Kafka.scala:46)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.Kafka.main(Kafka.scala)
2014-07-11T00:53:18+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:18,351 INFO main kafka.server.KafkaServer.info - [Kafka Server 847605514], shutting down
2014-07-11T00:53:18+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:18,353 INFO ZkClient-EventThread-14-zk-main0.XXX.com:2181,zk-main1.XXX.com:2181,zk-main2.XXX.com:2181,zk-main3.XXX.com:2181,zk-main4.XXX.com:2181/production/kafka/main org.I0Itec.zkclient.ZkEventThread.run - Terminate ZkClient event thread.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14063604" author="junrao" created="Wed, 16 Jul 2014 15:22:14 +0000"  >&lt;p&gt;This seems to happen on a clean restart (the following log entry indicates there is no log recovery). So, this could be a different issue. Could you open a separate jira?&lt;/p&gt;

&lt;p&gt;2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,961 INFO main kafka.log.LogManager.info - Found clean shutdown file. Skipping recovery for all logs in data directory &apos;/mnt/kafka_logs&apos;&lt;/p&gt;</comment>
                            <comment id="14068888" author="alexismidon" created="Mon, 21 Jul 2014 17:49:46 +0000"  >&lt;p&gt;I created &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1554&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-1554&lt;/a&gt;.&lt;br/&gt;
thanks&lt;/p&gt;</comment>
                            <comment id="14276497" author="qdutj" created="Wed, 14 Jan 2015 05:22:10 +0000"  >&lt;p&gt;I also met similar issues in Kafka-2.9.2-0.8.1 ( after an uncleanShutDown &amp;#8212; kill -9. ). &lt;br/&gt;
I am afraid that I cannot provide more logs since it was months ago: &lt;br/&gt;
******************************************************************************************************************************&lt;br/&gt;
FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)&lt;br/&gt;
java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/home/storm/kafka_2.9.2-0.8.1/logs/tracker-1/00000000000006911940.index) has non-zero size but the last offset is 6911940 and the base offset is 6911940&lt;br/&gt;
******************************************************************************************************************************&lt;br/&gt;
Here is my analysis:&lt;br/&gt;
Once unclean shutdown, Kafka needs to rebuild index files for her logsegments when restarted.&lt;br/&gt;
If InvalidOffsetException is thrown when appending entries to an index file,    recoverLog()  function will handle it by truncating the log segment to baseOffset (==&amp;gt; lastIndexEntry == baseOffset ). &lt;br/&gt;
However, since the index files are written by mmap, OS will flush the update to disks in time. It means the index file already has some entries now.&lt;br/&gt;
I am a beginner for scala, just guessing the logSegment with InvalidOffsetException thrown has been passed by the iterator, so it was not deleted in fact? (I mean all log segments after it will be deleted in recoverLog() in log.scala. )&lt;br/&gt;
This log segment missed to delete could not pass sanityCheck(), since it has index file with non-zero size but lastIndexEntry == baseOffset ( it was truncated to baseOffset when handling the InvalidOffsetException. ).&lt;br/&gt;
Quite sorry if any mistake above.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12632021">KAFKA-757</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12613527" name="KAFKA-1112-v1.patch" size="10348" author="jkreps" created="Wed, 13 Nov 2013 05:04:25 +0000"/>
                            <attachment id="12613931" name="KAFKA-1112-v2.patch" size="11273" author="jkreps" created="Thu, 14 Nov 2013 20:57:56 +0000"/>
                            <attachment id="12614094" name="KAFKA-1112-v3.patch" size="14175" author="jkreps" created="Fri, 15 Nov 2013 18:04:04 +0000"/>
                            <attachment id="12614489" name="KAFKA-1112-v4.patch" size="11460" author="junrao" created="Mon, 18 Nov 2013 22:10:10 +0000"/>
                            <attachment id="12612219" name="KAFKA-1112.out" size="3009" author="guozhang" created="Tue, 5 Nov 2013 18:57:23 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>356457</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 44 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1pg8n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>356745</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>