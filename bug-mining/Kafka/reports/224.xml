<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:37:34 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-772] System Test Transient Failure on testcase_0122</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-772</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;ul&gt;
	&lt;li&gt;This test case is failing randomly in the past few weeks. Please note there is a small % data loss allowance for the test case with Ack = 1. But the failure in this case is the mismatch of log segment checksum across the replicas.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Test description:&lt;br/&gt;
3 brokers cluster&lt;br/&gt;
Replication factor = 3&lt;br/&gt;
No. topic = 2&lt;br/&gt;
No. partitions = 3&lt;br/&gt;
Controlled failure (kill -15)&lt;br/&gt;
Ack = 1&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Test case output&lt;br/&gt;
_test_case_name  :  testcase_0122&lt;br/&gt;
_test_class_name  :  ReplicaBasicTest&lt;br/&gt;
arg : auto_create_topic  :  true&lt;br/&gt;
arg : bounce_broker  :  true&lt;br/&gt;
arg : broker_type  :  leader&lt;br/&gt;
arg : message_producing_free_time_sec  :  15&lt;br/&gt;
arg : num_iteration  :  3&lt;br/&gt;
arg : num_partition  :  3&lt;br/&gt;
arg : replica_factor  :  3&lt;br/&gt;
arg : sleep_seconds_between_producer_calls  :  1&lt;br/&gt;
validation_status  : &lt;br/&gt;
     Leader Election Latency - iter 1 brokerid 3  :  377.00 ms&lt;br/&gt;
     Leader Election Latency - iter 2 brokerid 1  :  374.00 ms&lt;br/&gt;
     Leader Election Latency - iter 3 brokerid 2  :  384.00 ms&lt;br/&gt;
     Leader Election Latency MAX  :  384.00&lt;br/&gt;
     Leader Election Latency MIN  :  374.00&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-0_r1.log  :  1750&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-0_r2.log  :  1750&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-0_r3.log  :  1750&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-1_r1.log  :  1750&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-1_r2.log  :  1750&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-1_r3.log  :  1750&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-2_r1.log  :  1500&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-2_r2.log  :  1500&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-2_r3.log  :  1500&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt;  :  5000&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-0_r1.log  :  1714&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-0_r2.log  :  1714&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-0_r3.log  :  1680&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-1_r1.log  :  1708&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-1_r2.log  :  1708&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-1_r3.log  :  1708&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-2_r1.log  :  1469&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-2_r2.log  :  1469&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-2_r3.log  :  1469&lt;br/&gt;
     Unique messages from producer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt;  :  4900&lt;br/&gt;
     Validate for data matched on topic &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; across replicas  :  PASSED&lt;br/&gt;
     Validate for data matched on topic &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt;  :  FAILED&lt;br/&gt;
     Validate for data matched on topic &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; across replicas  :  FAILED&lt;br/&gt;
     Validate for merged log segment checksum in cluster &lt;span class=&quot;error&quot;&gt;&amp;#91;source&amp;#93;&lt;/span&gt;  :  FAILED&lt;br/&gt;
     Validate leader election successful  :  PASSED&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12633953">KAFKA-772</key>
            <summary>System Test Transient Failure on testcase_0122</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sriramsub">Sriram</assignee>
                                    <reporter username="jfung">John Fung</reporter>
                        <labels>
                            <label>kafka-0.8</label>
                            <label>p1</label>
                    </labels>
                <created>Mon, 25 Feb 2013 17:34:35 +0000</created>
                <updated>Tue, 5 Mar 2013 16:26:48 +0000</updated>
                            <resolved>Tue, 5 Mar 2013 06:41:05 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13586030" author="jfung" created="Mon, 25 Feb 2013 17:35:49 +0000"  >&lt;p&gt;Attached a tar file for all log4j messages and data log files&lt;/p&gt;</comment>
                            <comment id="13589971" author="sriramsub" created="Thu, 28 Feb 2013 21:59:57 +0000"  >&lt;p&gt;There are two issues with the given logs. Both the issues are for topic 2 - partition 0 on broker 3.&lt;/p&gt;

&lt;p&gt;1. Segment 1 starting with logical offset 0 on broker 3 does not have continuous logical offsets. Logical offset 699 is followed by 734. &lt;br/&gt;
2. Segment 2 starting with logical offset 974 on broker 3 is 0 bytes while that in broker 2 has values from 974 to 1713. Broker 3 has segment 3 starting with logical offset 1012 to 1713. Broker 2 does not have any third segment.&lt;/p&gt;

&lt;p&gt;We have run the test in a loop multiple times for a day but have not been able to repro this on the local box. I am still investigating how the logs could end up in this state during continuous restarts with ack = 0 and replication factor = 3 &lt;/p&gt;</comment>
                            <comment id="13590771" author="jfung" created="Fri, 1 Mar 2013 17:54:43 +0000"  >&lt;p&gt;There is a similar failure in testcase_0125 yesterday in our distributed environment. Attached the log4j messages and data log segment files for reference.&lt;/p&gt;

&lt;p&gt;The failure is as follows (similar to testcase_0122):&lt;/p&gt;

&lt;p&gt;     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-0_r1.log  :  1715&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-0_r2.log  :  1715&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-0_r3.log  :  1715&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-1_r1.log  :  1711&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-1_r2.log  :  1711&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-1_r3.log  :  1711&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-2_r1.log  :  1469&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-2_r2.log  :  1469&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; at simple_consumer_test_1-2_r3.log  :  1469&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt;  :  4895&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-0_r1.log  :  1715&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-0_r2.log  :  1715&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-0_r3.log  :  1682&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-1_r1.log  :  1708&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-1_r2.log  :  1708&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-1_r3.log  :  1708&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-2_r1.log  :  1467&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-2_r2.log  :  1467&lt;br/&gt;
     Unique messages from consumer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; at simple_consumer_test_2-2_r3.log  :  1467&lt;br/&gt;
     Unique messages from producer on &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt;  :  4900&lt;br/&gt;
     Validate for data matched on topic &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1&amp;#93;&lt;/span&gt; across replicas  :  PASSED&lt;br/&gt;
     Validate for data matched on topic &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt;  :  PASSED&lt;br/&gt;
     Validate for data matched on topic &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2&amp;#93;&lt;/span&gt; across replicas  :  FAILED&lt;br/&gt;
     Validate for merged log segment checksum in cluster &lt;span class=&quot;error&quot;&gt;&amp;#91;source&amp;#93;&lt;/span&gt;  :  FAILED&lt;br/&gt;
     Validate leader election successful  :  PASSED&lt;/p&gt;</comment>
                            <comment id="13592826" author="nehanarkhede" created="Tue, 5 Mar 2013 00:03:44 +0000"  >&lt;p&gt;It would be useful to maybe add a WARN message and log the topic, partition, replica id, current offset, fetch offset when this happens. Other than that, this fix looks good.&lt;/p&gt;</comment>
                            <comment id="13592839" author="sriramsub" created="Tue, 5 Mar 2013 00:14:43 +0000"  >&lt;p&gt;I would like WARN to be actionable. Do you think it would be useful in this case? I am thinking what we would do if we saw this message in the log now that we know this is a valid case.&lt;/p&gt;</comment>
                            <comment id="13592850" author="sriramsub" created="Tue, 5 Mar 2013 00:29:44 +0000"  >&lt;p&gt;The test failed on Monday and then again failed on Friday. It was clear that the issue was timing related. We tried to reproduce the failure on the local box (repeatedly running the test) but could not reproduce it. I did some code browsing but did not have much luck. So I decided to setup tracing and run the test repeatedly in a distributed environment over the weekend and was hoping that it would fail. Luckily, it did and the trace logs proved to be useful in identifying the issue. Thanks to John for setting this up.&lt;/p&gt;

&lt;p&gt;What you see below are excerpts from the trace log which pertain to this failure at different points in time. In this particular failure, topic_2 / partitions 2 had missing logical offsets from 570 to 582 on broker 3 (3 brokers in total).&lt;/p&gt;

&lt;p&gt;current fetch offset = 582 &lt;br/&gt;
current HW = 570&lt;br/&gt;
Leader for topic_2/partition 2 = broker 2&lt;/p&gt;

&lt;p&gt;1. The lines below show the Fetch request that was issued by broker 3 to broker 2 just before broker 1 was shutdown. The requested offset is 582 for &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2,2&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-03-02 12:37:56,034&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaFetcherThread-0-2&amp;#93;&lt;/span&gt;, issuing to broker 2 of fetch request Name: FetchRequest; Version: 0; CorrelationId: 121; ClientId: ReplicaFetcherThread-0-2; ReplicaId: 3; MaxWait: 500 ms; MinBytes: 4096 bytes; RequestInfo: &lt;span class=&quot;error&quot;&gt;&amp;#91;test_1,0&amp;#93;&lt;/span&gt; -&amp;gt; PartitionFetchInfo(700,1048576),&lt;span class=&quot;error&quot;&gt;&amp;#91;test_2,1&amp;#93;&lt;/span&gt; -&amp;gt; PartitionFetchInfo(677,1048576),&lt;span class=&quot;error&quot;&gt;&amp;#91;test_2,2&amp;#93;&lt;/span&gt; -&amp;gt; PartitionFetchInfo(582,1048576),&lt;span class=&quot;error&quot;&gt;&amp;#91;test_2,0&amp;#93;&lt;/span&gt; -&amp;gt; PartitionFetchInfo(679,1048576),&lt;span class=&quot;error&quot;&gt;&amp;#91;test_1,2&amp;#93;&lt;/span&gt; -&amp;gt; PartitionFetchInfo(600,1048576),&lt;span class=&quot;error&quot;&gt;&amp;#91;test_1,1&amp;#93;&lt;/span&gt; -&amp;gt; PartitionFetchInfo(699,1048576) (kafka.server.ReplicaFetcherThread)&lt;/p&gt;

&lt;p&gt;2. Broker 1 is shutdown and broker 3 handles leader and isr request. Note that &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2,2&amp;#93;&lt;/span&gt; still follows broker 2 but we still issue a makefollower call for it.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-03-02 12:37:56,086&amp;#93;&lt;/span&gt; INFO Replica Manager on Broker 3: Handling leader and isr request Name: LeaderAndIsrRequest; Version: 0; CorrelationId: 2; ClientId: ; AckTimeoutMs: 1000 ms; ControllerEpoch: 2; PartitionStateInfo: (test_1,0) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,1,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;1&quot; }
&lt;p&gt;,1),3),(test_2,1) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;2&quot; }
&lt;p&gt;,2),3),(test_2,2) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,1,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;1&quot; }
&lt;p&gt;,1),3),(test_2,0) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;2&quot; }
&lt;p&gt;,2),3),(test_1,2) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;2&quot; }
&lt;p&gt;,2),3),(test_1,1) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,1,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;1&quot; }
&lt;p&gt;,1),3); Leaders: id:2,host:xxxx(kafka.server.ReplicaManager)&lt;/p&gt;

&lt;p&gt;3. The leader and isr request results in removing the fetcher to broker 2 for &lt;span class=&quot;error&quot;&gt;&amp;#91;test_2,2&amp;#93;&lt;/span&gt;, truncating the log to high watermark (570) and then adding back the fetcher to the same broker.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-03-02 12:37:56,088&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaFetcherManager on broker 3&amp;#93;&lt;/span&gt; removing fetcher on topic test_2, partition 2 (kafka.server.ReplicaFetcherManager)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-03-02 12:37:56,088&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka Log on Broker 3&amp;#93;&lt;/span&gt;, Truncated log segment /tmp/kafka_server_3_logs/test_2-2/00000000000000000000.log to target offset 570 (kafka.log.Log)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-03-02 12:37:56,088&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaFetcherManager on broker 3&amp;#93;&lt;/span&gt; adding fetcher on topic test_2, partion 2, initOffset 570 to broker 2 with fetcherId 0 (kafka.server.ReplicaFetcherManager)&lt;/p&gt;

&lt;p&gt;4. The leader and isr request is completed at this point of time.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-03-02 12:37:56,090&amp;#93;&lt;/span&gt; INFO Replica Manager on Broker 3: Completed leader and isr request Name: LeaderAndIsrRequest; Version: 0; CorrelationId: 2; ClientId: ; AckTimeoutMs: 1000 ms; ControllerEpoch: 2; PartitionStateInfo: (test_1,0) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,1,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;1&quot; }
&lt;p&gt;,1),3),(test_2,1) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;2&quot; }
&lt;p&gt;,2),3),(test_2,2) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,1,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;1&quot; }
&lt;p&gt;,1),3),(test_2,0) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;2&quot; }
&lt;p&gt;,2),3),(test_1,2) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;2&quot; }
&lt;p&gt;,2),3),(test_1,1) -&amp;gt; PartitionStateInfo(LeaderIsrAndControllerEpoch(&lt;/p&gt;
{ &quot;ISR&quot;:&quot;2,1,3&quot;, &quot;leader&quot;:&quot;2&quot;, &quot;leaderEpoch&quot;:&quot;1&quot; }
&lt;p&gt;,1),3); Leaders: id:2,host:xxxx (kafka.server.ReplicaManager)&lt;/p&gt;


&lt;p&gt;5.  A log append happens at offset 582 though the nextOffset for the log is at 570. This append actually pertains to the fetch request at step 1. This explains the gap in the log.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-03-02 12:37:56,098&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka Log on Broker 3&amp;#93;&lt;/span&gt;, Appending message set to test_2-2 offset: 582 nextOffset: 570 messageSet: ByteBufferMessageSet(MessageAndOffset(Message(magic = 0, attributes = 0, crc = 1408289663, key = null, payload = java.nio.HeapByteBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;pos=0 lim=500 cap=500&amp;#93;&lt;/span&gt;),582), MessageAndOffset(Message(magic = 0, attributes = 0, crc = 3696400058, key = null, payload = java.nio.HeapByteBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;pos=0 lim=500 cap=500&amp;#93;&lt;/span&gt;),583), MessageAndOffset(Message(magic = 0, attributes = 0, crc = 2403920749, key = null, payload = java.nio.HeapByteBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;pos=0 lim=500 cap=500&amp;#93;&lt;/span&gt;),584), ) (kafka.log.Log)&lt;/p&gt;

&lt;p&gt;From the set of steps above, it is clear that some thing is causing the fetch request at step 1 to complete even though step 2 and 3 removed the fetcher for that topic,partition.&lt;/p&gt;

&lt;p&gt;Looking at the code now it becomes obvious. The race condition is between the thread that removes the fetcher, truncates the log and adds the fetcher back and the thread that fetches bytes from the leader. Follow the steps below to understand what is happening.&lt;/p&gt;

&lt;p&gt;Partition.Scala&lt;/p&gt;

&lt;p&gt;          replicaFetcherManager.removeFetcher(topic, partitionId)           --&amp;gt; step 2 : Removes the topic,partition &#8211; offset mapping from partitionMap in AbstractFetcherThread&lt;br/&gt;
          // make sure local replica exists&lt;br/&gt;
          val localReplica = getOrCreateReplica()&lt;br/&gt;
          localReplica.log.get.truncateTo(localReplica.highWatermark)    --&amp;gt; step 3 : Truncates to offset 570&lt;br/&gt;
          inSyncReplicas = Set.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;&lt;br/&gt;
          leaderEpoch = leaderAndIsr.leaderEpoch&lt;br/&gt;
          zkVersion = leaderAndIsr.zkVersion&lt;br/&gt;
          leaderReplicaIdOpt = Some(newLeaderBrokerId)&lt;br/&gt;
          // start fetcher thread to current leader&lt;br/&gt;
          replicaFetcherManager.addFetcher(topic, partitionId, localReplica.logEndOffset, leaderBroker)    --&amp;gt; step 4: Sets the new fetcher to fetch from the log end offset which is at 570 at this point&lt;/p&gt;

&lt;p&gt;AbstractFetcherThread.Scala&lt;/p&gt;

&lt;p&gt;private def processFetchRequest(fetchRequest: FetchRequest) {&lt;br/&gt;
    val partitionsWithError = new mutable.HashSet&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicAndPartition&amp;#93;&lt;/span&gt;&lt;br/&gt;
    var response: FetchResponse = null&lt;br/&gt;
    try &lt;/p&gt;
{
      trace(&quot;issuing to broker %d of fetch request %s&quot;.format(sourceBroker.id, fetchRequest))
      response = simpleConsumer.fetch(fetchRequest)
    }
&lt;p&gt; catch {&lt;br/&gt;
      case t =&amp;gt;&lt;br/&gt;
        debug(&quot;error in fetch %s&quot;.format(fetchRequest), t)&lt;br/&gt;
        if (isRunning.get) {&lt;br/&gt;
          partitionMapLock synchronized &lt;/p&gt;
{
            partitionsWithError ++= partitionMap.keys
          }
&lt;p&gt;        }&lt;br/&gt;
    }&lt;br/&gt;
    fetcherStats.requestRate.mark()   --&amp;gt;  step 1 : Fetch completes. Fetch request is from offset 582.&lt;/p&gt;

&lt;p&gt;    if (response != null) {&lt;br/&gt;
      // process fetched data &lt;br/&gt;
      partitionMapLock.lock()     ---&amp;gt; step 5: This is where the fetch request is waiting when the addFetcher in Partition.Scala is executing above&lt;br/&gt;
      try {&lt;br/&gt;
        response.data.foreach {&lt;br/&gt;
          case(topicAndPartition, partitionData) =&amp;gt;&lt;br/&gt;
            val (topic, partitionId) = topicAndPartition.asTuple&lt;br/&gt;
            val currentOffset = partitionMap.get(topicAndPartition)&lt;br/&gt;
            if (currentOffset.isDefined) {&lt;br/&gt;
              partitionData.error match {&lt;br/&gt;
                case ErrorMapping.NoError =&amp;gt;&lt;br/&gt;
                  val messages = partitionData.messages.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ByteBufferMessageSet&amp;#93;&lt;/span&gt;&lt;br/&gt;
                  val validBytes = messages.validBytes&lt;br/&gt;
                  val newOffset = messages.lastOption match &lt;/p&gt;
{          --&amp;gt;  step 6: The newOffset is set to 587 and partitionMap is updated
                    case Some(m: MessageAndOffset) =&amp;gt; m.nextOffset
                    case None =&amp;gt; currentOffset.get
                  }
&lt;p&gt;                  partitionMap.put(topicAndPartition, newOffset)&lt;br/&gt;
                  fetcherLagStats.getFetcherLagStats(topic, partitionId).lag = partitionData.hw - newOffset&lt;br/&gt;
                  fetcherStats.byteRate.mark(validBytes)&lt;br/&gt;
                  // Once we hand off the partition data to the subclass, we can&apos;t mess with it any more in this thread&lt;br/&gt;
                  processPartitionData(topicAndPartition, currentOffset.get, partitionData)    --&amp;gt; step 7: This appends data to the log with logical offsets from 582 &#8211; 587. Note that the offset passed to this method is 570 (currentOffset). Hence all offset validation checks in processPartitionData passes.&lt;br/&gt;
                case ErrorMapping.OffsetOutOfRangeCode =&amp;gt;&lt;br/&gt;
                  try &lt;/p&gt;
{
                    val newOffset = handleOffsetOutOfRange(topicAndPartition)
                    partitionMap.put(topicAndPartition, newOffset)
                    warn(&quot;current offset %d for topic %s partition %d out of range; reset offset to %d&quot;
                      .format(currentOffset.get, topic, partitionId, newOffset))
                  }
&lt;p&gt; catch &lt;/p&gt;
{
                    case e =&amp;gt;
                      warn(&quot;error getting offset for %s %d to broker %d&quot;.format(topic, partitionId, sourceBroker.id), e)
                      partitionsWithError += topicAndPartition
                  }
&lt;p&gt;                case _ =&amp;gt;&lt;br/&gt;
                  warn(&quot;error for %s %d to broker %d&quot;.format(topic, partitionId, sourceBroker.id),&lt;br/&gt;
                    ErrorMapping.exceptionFor(partitionData.error))&lt;br/&gt;
                  partitionsWithError += topicAndPartition&lt;br/&gt;
              }&lt;br/&gt;
            }&lt;br/&gt;
        }&lt;br/&gt;
      } finally &lt;/p&gt;
{
        partitionMapLock.unlock()
      }
&lt;p&gt;    }&lt;/p&gt;</comment>
                            <comment id="13593128" author="junrao" created="Tue, 5 Mar 2013 06:41:05 +0000"  >&lt;p&gt;Thanks for the patch. Committed to 0.8.&lt;/p&gt;</comment>
                            <comment id="13593540" author="nehanarkhede" created="Tue, 5 Mar 2013 16:26:48 +0000"  >&lt;p&gt;Yeah, probably ok to skip the message&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12571974" name="KAFKA-772.patch" size="1417" author="sriramsub" created="Mon, 4 Mar 2013 22:44:03 +0000"/>
                            <attachment id="12570815" name="testcase_0122.tar.gz" size="1067144" author="jfung" created="Mon, 25 Feb 2013 17:35:49 +0000"/>
                            <attachment id="12571621" name="testcase_0125.tar.gz" size="1414769" author="jfung" created="Fri, 1 Mar 2013 18:03:25 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>314447</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 38 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1i9gf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>314791</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>