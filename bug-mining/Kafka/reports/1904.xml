<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:10:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6877] Remove completedFetch upon a failed parse if it contains no records.</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6877</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;This patch removed a completedFetch from the completedFetches queue&#160;upon a failed parse if it contains no records. The following scenario explains why this is&#160;needed&#160;for an instance of this case &#8211; i.e. in&#160;TopicAuthorizationException.&lt;/p&gt;

&lt;p&gt;0. Let&apos;s assume a scenario, in which the consumer is attempting to read from a topic without the necessary read permission.&lt;br/&gt;
1. In Fetcher#fetchedRecords(), after peeking the completedFetches, the Fetcher#parseCompletedFetch(CompletedFetch) throws a TopicAuthorizationException (as expected).&lt;br/&gt;
2. Fetcher#fetchedRecords() passes the TopicAuthorizationException up without having a chance to poll completedFetches. So, the same completedFetch remains at the completedFetches queue.&lt;br/&gt;
3. Upon following calls to Fetcher#fetchedRecords(), peeking the completedFetches will always return the same completedFetch independent of any updates to the ACL that the topic is trying to read from.&lt;br/&gt;
4. Hence, despite the creation of an ACL with correct permissions, once the consumer sees the TopicAuthorizationException, it will be unable to recover without a bounce.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13157742">KAFKA-6877</key>
            <summary>Remove completedFetch upon a failed parse if it contains no records.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="agencer">Adem Efe Gencer</assignee>
                                    <reporter username="agencer">Adem Efe Gencer</reporter>
                        <labels>
                    </labels>
                <created>Mon, 7 May 2018 23:36:16 +0000</created>
                <updated>Wed, 6 Jun 2018 12:16:22 +0000</updated>
                            <resolved>Wed, 6 Jun 2018 12:16:22 +0000</resolved>
                                    <version>0.11.0.2</version>
                    <version>1.0.0</version>
                    <version>1.0.1</version>
                    <version>1.1.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>clients</component>
                    <component>consumer</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16466620" author="githubbot" created="Mon, 7 May 2018 23:38:58 +0000"  >&lt;p&gt;efeg opened a new pull request #4974: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6877&quot; title=&quot;Remove completedFetch upon a failed parse if it contains no records.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6877&quot;&gt;&lt;del&gt;KAFKA-6877&lt;/del&gt;&lt;/a&gt;; Remove completedFetch upon a failed parse if it contains no records.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4974&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4974&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   This patch removed a completedFetch from the completedFetches queue upon a failed parse if it contains no records. The following scenario explains why this is needed for an instance of this case &#8211; i.e. in TopicAuthorizationException.&lt;/p&gt;

&lt;p&gt;   0. Let&apos;s assume a scenario, in which the consumer is attempting to read from a topic without the necessary read permission.&lt;br/&gt;
   1. In Fetcher#fetchedRecords(), after peeking the completedFetches, the Fetcher#parseCompletedFetch(CompletedFetch) throws a TopicAuthorizationException (as expected).&lt;br/&gt;
   2. Fetcher#fetchedRecords() passes the TopicAuthorizationException up without having a chance to poll completedFetches. So, the same completedFetch remains at the completedFetches queue.&lt;br/&gt;
   3. Upon following calls to Fetcher#fetchedRecords(), peeking the completedFetches will always return the same completedFetch independent of any updates to the ACL that the topic is trying to read from.&lt;br/&gt;
   4. Hence, despite the creation of an ACL with correct permissions, once the consumer sees the TopicAuthorizationException, it will be unable to recover without a bounce.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16468221" author="githubbot" created="Wed, 9 May 2018 01:59:43 +0000"  >&lt;p&gt;becketqin closed pull request #4974: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6877&quot; title=&quot;Remove completedFetch upon a failed parse if it contains no records.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6877&quot;&gt;&lt;del&gt;KAFKA-6877&lt;/del&gt;&lt;/a&gt;; Remove completedFetch upon a failed parse if it contains no records.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4974&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4974&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index 6d8fb6c2466..b3791ffb514 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -472,6 +472,7 @@ private ListOffsetResult fetchOffsetsByTimes(Map&amp;lt;TopicPartition, Long&amp;gt; timestamp&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return The fetched records per partition&lt;/li&gt;
	&lt;li&gt;@throws OffsetOutOfRangeException If there is OffsetOutOfRange error in fetchResponse and&lt;/li&gt;
	&lt;li&gt;the defaultResetPolicy is NONE&lt;br/&gt;
+     * @throws TopicAuthorizationException If there is TopicAuthorization error in fetchResponse.&lt;br/&gt;
      */&lt;br/&gt;
     public Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt;&amp;gt; fetchedRecords() {&lt;br/&gt;
         Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt;&amp;gt; fetched = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -483,7 +484,20 @@ private ListOffsetResult fetchOffsetsByTimes(Map&amp;lt;TopicPartition, Long&amp;gt; timestamp&lt;br/&gt;
                     CompletedFetch completedFetch = completedFetches.peek();&lt;br/&gt;
                     if (completedFetch == null) break;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;nextInLineRecords = parseCompletedFetch(completedFetch);&lt;br/&gt;
+                    try 
{
+                        nextInLineRecords = parseCompletedFetch(completedFetch);
+                    }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        // Remove a completedFetch upon a parse with exception if (1) it contains no records, and+                        // (2) there are no fetched records with actual content preceding this exception.+                        // The first condition ensures that the completedFetches is not stuck with the same completedFetch+                        // in cases such as the TopicAuthorizationException, and the second condition ensures that no+                        // potential data loss due to an exception in a following record.+                        FetchResponse.PartitionData partition = completedFetch.partitionData;+                        if (fetched.isEmpty() &amp;amp;&amp;amp; (partition.records == null || partition.records.sizeInBytes() == 0)) {
+                            completedFetches.poll();
+                        }+                        throw e;+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;                     completedFetches.poll();&lt;br/&gt;
                 } else {&lt;br/&gt;
                     List&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; records = fetchRecords(nextInLineRecords, recordsRemaining);&lt;br/&gt;
@@ -945,7 +959,7 @@ private PartitionRecords parseCompletedFetch(CompletedFetch completedFetch) &lt;/p&gt;
{
                 this.metadata.requestUpdate();
             }
&lt;p&gt; else if (error == Errors.OFFSET_OUT_OF_RANGE) {&lt;br/&gt;
                 if (fetchOffset != subscriptions.position(tp)) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;Discarding stale fetch response for partition {} since the fetched offset {}&quot; +&lt;br/&gt;
+                    log.debug(&quot;Discarding stale fetch response for partition {} since the fetched offset {} &quot; +&lt;br/&gt;
                             &quot;does not match the current offset {}&quot;, tp, fetchOffset, subscriptions.position(tp));&lt;br/&gt;
                 } else if (subscriptions.hasDefaultOffsetResetPolicy()) {&lt;br/&gt;
                     log.info(&quot;Fetch offset {} is out of range for partition {}, resetting offset&quot;, fetchOffset, tp);&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 6de11866ec4..7157470b760 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -59,6 +59,7 @@&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecordsBuilder;&lt;br/&gt;
 import org.apache.kafka.common.record.Record;&lt;br/&gt;
 import org.apache.kafka.common.record.RecordBatch;&lt;br/&gt;
+import org.apache.kafka.common.record.Records;&lt;br/&gt;
 import org.apache.kafka.common.record.SimpleRecord;&lt;br/&gt;
 import org.apache.kafka.common.record.TimestampType;&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest;&lt;br/&gt;
@@ -117,6 +118,8 @@&lt;br/&gt;
     private final String metricGroup = &quot;consumer&quot; + groupId + &quot;-fetch-manager-metrics&quot;;&lt;br/&gt;
     private TopicPartition tp0 = new TopicPartition(topicName, 0);&lt;br/&gt;
     private TopicPartition tp1 = new TopicPartition(topicName, 1);&lt;br/&gt;
+    private TopicPartition tp2 = new TopicPartition(topicName, 2);&lt;br/&gt;
+    private TopicPartition tp3 = new TopicPartition(topicName, 3);&lt;br/&gt;
     private int minBytes = 1;&lt;br/&gt;
     private int maxBytes = Integer.MAX_VALUE;&lt;br/&gt;
     private int maxWaitMs = 0;&lt;br/&gt;
@@ -126,7 +129,7 @@&lt;br/&gt;
     private MockTime time = new MockTime(1);&lt;br/&gt;
     private Metadata metadata = new Metadata(0, Long.MAX_VALUE, true);&lt;br/&gt;
     private MockClient client = new MockClient(time, metadata);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;private Cluster cluster = TestUtils.singletonCluster(topicName, 2);&lt;br/&gt;
+    private Cluster cluster = TestUtils.singletonCluster(topicName, 4);&lt;br/&gt;
     private Node node = cluster.nodes().get(0);&lt;br/&gt;
     private Metrics metrics = new Metrics(time);&lt;br/&gt;
     FetcherMetricsRegistry metricsRegistry = new FetcherMetricsRegistry(&quot;consumer&quot; + groupId);&lt;br/&gt;
@@ -140,6 +143,7 @@&lt;br/&gt;
     private MemoryRecords records;&lt;br/&gt;
     private MemoryRecords nextRecords;&lt;br/&gt;
     private MemoryRecords emptyRecords;&lt;br/&gt;
+    private MemoryRecords partialRecords;&lt;br/&gt;
     private Fetcher&amp;lt;byte[], byte[]&amp;gt; fetcher = createFetcher(subscriptions, metrics);&lt;br/&gt;
     private Metrics fetcherMetrics = new Metrics(time);&lt;br/&gt;
     private Fetcher&amp;lt;byte[], byte[]&amp;gt; fetcherNoAutoReset = createFetcher(subscriptionsNoAutoReset, fetcherMetrics);&lt;br/&gt;
@@ -162,6 +166,11 @@ public void setup() throws Exception 
{
 
         builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);
         emptyRecords = builder.build();
+
+        builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 4L);
+        builder.append(0L, &quot;key&quot;.getBytes(), &quot;value-0&quot;.getBytes());
+        partialRecords = builder.build();
+        partialRecords.buffer().putInt(Records.SIZE_OFFSET, 10000);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @After&lt;br/&gt;
@@ -833,7 +842,7 @@ public void testFetchPositionAfterException() {&lt;br/&gt;
         partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,&lt;br/&gt;
             FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, records));&lt;br/&gt;
         partitions.put(tp0, new FetchResponse.PartitionData(Errors.OFFSET_OUT_OF_RANGE, 100,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));&lt;br/&gt;
+            FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));&lt;br/&gt;
         client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions),&lt;br/&gt;
             0, INVALID_SESSION_ID));&lt;br/&gt;
         consumerClient.poll(0);&lt;br/&gt;
@@ -863,6 +872,73 @@ public void testFetchPositionAfterException() 
{
         assertEquals(e.offsetOutOfRangePartitions().size(), 1);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testCompletedFetchRemoval() {&lt;br/&gt;
+        // Ensure the removal of completed fetches that cause an Exception if and only if they contain empty records.&lt;br/&gt;
+        subscriptionsNoAutoReset.assignFromUser(Utils.mkSet(tp0, tp1, tp2, tp3));&lt;br/&gt;
+        subscriptionsNoAutoReset.seek(tp0, 1);&lt;br/&gt;
+        subscriptionsNoAutoReset.seek(tp1, 1);&lt;br/&gt;
+        subscriptionsNoAutoReset.seek(tp2, 1);&lt;br/&gt;
+        subscriptionsNoAutoReset.seek(tp3, 1);&lt;br/&gt;
+&lt;br/&gt;
+        assertEquals(1, fetcherNoAutoReset.sendFetches());&lt;br/&gt;
+&lt;br/&gt;
+        Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100, FetchResponse.INVALID_LAST_STABLE_OFFSET,&lt;br/&gt;
+            FetchResponse.INVALID_LOG_START_OFFSET, null, records));&lt;br/&gt;
+        partitions.put(tp0, new FetchResponse.PartitionData(Errors.OFFSET_OUT_OF_RANGE, 100,&lt;br/&gt;
+            FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));&lt;br/&gt;
+        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100L, 4,&lt;br/&gt;
+            0L, null, nextRecords));&lt;br/&gt;
+        partitions.put(tp3, new FetchResponse.PartitionData(Errors.NONE, 100L, 4,&lt;br/&gt;
+            0L, null, partialRecords));&lt;br/&gt;
+        client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions),&lt;br/&gt;
+                                                 0, INVALID_SESSION_ID));&lt;br/&gt;
+        consumerClient.poll(0);&lt;br/&gt;
+&lt;br/&gt;
+        List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; fetchedRecords = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records: fetcherNoAutoReset.fetchedRecords().values())&lt;br/&gt;
+            fetchedRecords.addAll(records);&lt;br/&gt;
+&lt;br/&gt;
+        assertEquals(fetchedRecords.size(), subscriptionsNoAutoReset.position(tp1) - 1);&lt;br/&gt;
+        assertEquals(4, subscriptionsNoAutoReset.position(tp1).longValue());&lt;br/&gt;
+        assertEquals(3, fetchedRecords.size());&lt;br/&gt;
+&lt;br/&gt;
+        List&amp;lt;OffsetOutOfRangeException&amp;gt; oorExceptions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        try &lt;/p&gt;
{
+            for (List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records: fetcherNoAutoReset.fetchedRecords().values())
+                fetchedRecords.addAll(records);
+        }
&lt;p&gt; catch (OffsetOutOfRangeException oor) &lt;/p&gt;
{
+            oorExceptions.add(oor);
+        }
&lt;p&gt;+&lt;br/&gt;
+        // Should have received one OffsetOutOfRangeException for partition tp1&lt;br/&gt;
+        assertEquals(1, oorExceptions.size());&lt;br/&gt;
+        OffsetOutOfRangeException oor = oorExceptions.get(0);&lt;br/&gt;
+        assertTrue(oor.offsetOutOfRangePartitions().containsKey(tp0));&lt;br/&gt;
+        assertEquals(oor.offsetOutOfRangePartitions().size(), 1);&lt;br/&gt;
+&lt;br/&gt;
+        for (List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records: fetcherNoAutoReset.fetchedRecords().values())&lt;br/&gt;
+            fetchedRecords.addAll(records);&lt;br/&gt;
+&lt;br/&gt;
+        // Should not have received an Exception for tp2.&lt;br/&gt;
+        assertEquals(6, subscriptionsNoAutoReset.position(tp2).longValue());&lt;br/&gt;
+        assertEquals(5, fetchedRecords.size());&lt;br/&gt;
+&lt;br/&gt;
+        int numExceptionsExpected = 3;&lt;br/&gt;
+        List&amp;lt;KafkaException&amp;gt; kafkaExceptions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (int i = 1; i &amp;lt;= numExceptionsExpected; i++) {&lt;br/&gt;
+            try &lt;/p&gt;
{
+                for (List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records: fetcherNoAutoReset.fetchedRecords().values())
+                    fetchedRecords.addAll(records);
+            }
&lt;p&gt; catch (KafkaException e) &lt;/p&gt;
{
+                kafkaExceptions.add(e);
+            }
&lt;p&gt;+        }&lt;br/&gt;
+        // Should have received as much as numExceptionsExpected Kafka exceptions for tp3.&lt;br/&gt;
+        assertEquals(numExceptionsExpected, kafkaExceptions.size());&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testSeekBeforeException() {&lt;br/&gt;
         Fetcher&amp;lt;byte[], byte[]&amp;gt; fetcher = createFetcher(subscriptionsNoAutoReset, new Metrics(time), 2);&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 27 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3tfzj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>becket_qin</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>