<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:40:53 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-17635] Lost events on internal repartition topic when excatly_once_v2 is set and producer is fenced</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-17635</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;In some of the Kafka streams applications we observed that some events are missed during processing, when the processing guarantee was set to exactly_once_v2.&lt;br/&gt;
&#160;&lt;br/&gt;
It happened in different kafka stream applications at different places. The common pattern is that there was always an internal repartition topic involved (e.g. FK joins and aggregations on new key)&lt;/p&gt;

&lt;p&gt;With the following simplified example we could reproduce the problem:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
inputStream
  .groupBy((k, v) -&amp;gt; v, Grouped.with(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;(), &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;()).withName(&lt;span class=&quot;code-quote&quot;&gt;&quot;group&quot;&lt;/span&gt;))
  .count(Materialized.as(&lt;span class=&quot;code-quote&quot;&gt;&quot;count&quot;&lt;/span&gt;).withKeySerde(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;()).withValueSerde(&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;()));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The analysis showed the following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;the event exists in the input topic&lt;/li&gt;
	&lt;li&gt;after repartition the changelog topic does not have always all events aggregated.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It happens only occasional on production environment while processing millions of events on the initial load.&lt;/p&gt;

&lt;p&gt;We were able to seldom reproduce the problem in local environment in debugging mode.&lt;/p&gt;

&lt;p&gt;Our assumption is that there is a problem with the purging of events for the repartition topic.&lt;br/&gt;
The StreamTask holds a list of consumedOffsets (used for purging internal repartition topics).&lt;br/&gt;
After we got a TaskMigratedException (e.g. transaction timeout or similar), the stream task will be migrated and closed dirty.&lt;br/&gt;
When the task is restored, then the consumedOffset list is not cleared.&lt;br/&gt;
The consumedOffset list may contain offsets from aborted transactions.&lt;br/&gt;
On the next purge cycle some not yet committed offset might get deleted from the repartition topic.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2024-09-27T11:35:10.021+02:00  WARN 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.p.internals.StreamThread         : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] Detected that the thread is being fenced. This implies that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; thread missed a rebalance and dropped out of the consumer group. Will close out all assigned tasks and rejoin the consumer group.

org.apache.kafka.streams.errors.TaskMigratedException: Producer got fenced trying to commit a transaction [stream-thread [main]]; it means all tasks belonging to &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; thread should be migrated.
	at org.apache.kafka.streams.processor.internals.StreamsProducer.commitTransaction(StreamsProducer.java:304) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.TaskExecutor.commitOffsetsOrTransaction(TaskExecutor.java:203) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.TaskExecutor.commitTasksAndMaybeUpdateCommittableOffsets(TaskExecutor.java:154) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.TaskManager.commitTasksAndMaybeUpdateCommittableOffsets(TaskManager.java:1875) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:1842) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:1337) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:986) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:686) ~[kafka-streams-3.7.1.jar:na]
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:645) ~[kafka-streams-3.7.1.jar:na]
Caused by: org.apache.kafka.clients.consumer.CommitFailedException: Transaction offset Commit failed due to consumer group metadata mismatch: The coordinator is not aware of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; member.
	at org.apache.kafka.clients.producer.internals.TransactionManager$TxnOffsetCommitHandler.handleResponse(TransactionManager.java:1689) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1236) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:154) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:608) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:600) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:463) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:339) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:253) ~[kafka-clients-3.7.1.jar:na]
	at java.base/java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:1583) ~[na:na]

2024-09-27T11:35:10.021+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.processor.internals.StreamTask   : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] task [1_3] Suspended from RUNNING
2024-09-27T11:35:11.420+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.processor.internals.StreamTask   : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] task [1_3] Closed dirty
2024-09-27T11:37:06.782+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] stream-task [1_3] State store count did not find checkpoint offset, hence would &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; to the starting offset at changelog processTest-1-count-changelog-3
2024-09-27T11:37:06.783+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.processor.internals.StreamTask   : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] task [1_3] Initialized
2024-09-27T11:37:06.787+02:00  INFO 38644 --- [sandbox] [-StreamThread-1] o.a.k.s.s.i.RocksDBTimestampedStore      : Opening store count in regular mode
2024-09-27T11:37:06.843+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] End offset &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; changelog processTest-1-count-changelog-3 initialized as 916.
2024-09-27T11:37:06.843+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4-restore-consumer, groupId=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;] Assigned to partition(s): processTest-1-count-changelog-3, processTest-1-count-changelog-1
2024-09-27T11:37:06.843+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4-restore-consumer, groupId=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;] Seeking to earliest offset of partition processTest-1-count-changelog-1
2024-09-27T11:37:06.844+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4-restore-consumer, groupId=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;] Resetting offset &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition processTest-1-count-changelog-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9093 (id: 2 rack: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;)], epoch=0}}.
2024-09-27T11:37:06.850+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] Finished restoring changelog processTest-1-count-changelog-3 to store count with a total number of 456 records
2024-09-27T11:37:06.851+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.processor.internals.StreamTask   : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] task [1_3] Restored and ready to run
2024-09-27T11:37:06.854+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.p.internals.StreamThread         : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] Restoration took 334 ms &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; all active tasks [0_3, 1_3, 0_1, 1_1]
2024-09-27T11:37:06.854+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] o.a.k.s.p.internals.StreamThread         : stream-thread [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7-StreamThread-4] State transition from PARTITIONS_ASSIGNED to RUNNING
2024-09-27T11:37:06.854+02:00  INFO 38644 --- [sandbox] [-StreamThread-4] org.apache.kafka.streams.KafkaStreams    : stream-client [processTest-1-c6756e6e-e134-481c-8875-e26a77d684e7] State transition from REBALANCING to RUNNING
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In our test we produced the same amount of events to each partition (4)&lt;/p&gt;

&lt;p&gt;In the sample test we just count the events, therefore all 4 partition ahould have the same count eventually.&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13071837/13071837_screenshot-1.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;


&lt;p&gt;Our current workaround would be to temporary increase the transaction.timeout.ms to a very high value.&lt;br/&gt;
this should reduce the probability to have the tasks migrated.&lt;br/&gt;
However, this is not really a solution.&lt;br/&gt;
Another option would be to increase the repartition.purge.interval.ms to a very high value in order to disable the purging of repartition topics during initial load.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13593508">KAFKA-17635</key>
            <summary>Lost events on internal repartition topic when excatly_once_v2 is set and producer is fenced</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bbejeck">Bill Bejeck</assignee>
                                    <reporter username="herbert.wespi">Herbert Wespi</reporter>
                        <labels>
                            <label>exactly-once</label>
                            <label>streams</label>
                    </labels>
                <created>Fri, 27 Sep 2024 12:17:34 +0000</created>
                <updated>Thu, 9 Jan 2025 11:07:16 +0000</updated>
                            <resolved>Wed, 6 Nov 2024 22:47:55 +0000</resolved>
                                    <version>3.7.1</version>
                                    <fixVersion>3.7.2</fixVersion>
                    <fixVersion>3.8.2</fixVersion>
                    <fixVersion>3.9.1</fixVersion>
                    <fixVersion>4.0.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="17885426" author="bbejeck" created="Fri, 27 Sep 2024 16:59:53 +0000"  >&lt;p&gt;Thanks for reporting this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=herbert.wespi&quot; class=&quot;user-hover&quot; rel=&quot;herbert.wespi&quot;&gt;herbert.wespi&lt;/a&gt; - I&apos;ll be taking a look at this ticket soon.&lt;/p&gt;</comment>
                            <comment id="17892986" author="bbejeck" created="Fri, 25 Oct 2024 20:31:04 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=herbert.wespi&quot; class=&quot;user-hover&quot; rel=&quot;herbert.wespi&quot;&gt;herbert.wespi&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;I looked into the issue, and I&apos;ve identified a couple of potential problem spots along with ideas to fix them.&#160; But I need to reproduce the error first to confirm my theory and then apply the fixes.&#160;&#160;&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Bill&lt;/p&gt;</comment>
                            <comment id="17894870" author="bbejeck" created="Fri, 1 Nov 2024 15:58:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=herbert.wespi&quot; class=&quot;user-hover&quot; rel=&quot;herbert.wespi&quot;&gt;herbert.wespi&lt;/a&gt; - I&apos;ve been able to reproduce the problem.&lt;/p&gt;

&lt;p&gt;The steps I took to replicate the issue:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;A Kafka Streams word-count application that uses the word value as a key - forces a repartition&lt;/li&gt;
	&lt;li&gt;Create a 4 partition input topic&lt;/li&gt;
	&lt;li&gt;Produce 1K records per second to the input topic&lt;/li&gt;
	&lt;li&gt;Set the KS threading level to three&lt;/li&gt;
	&lt;li&gt;reduce the max.poll.interval and repartition.purge.interval to 5 seconds each&lt;/li&gt;
	&lt;li&gt;Add a `process()` operator that every 30 seconds or so will pause for a random time between 7 and 20 seconds, forcing the streams consumer to get kicked out of the group due to a failure to call `poll()` within the timeout period.&lt;/li&gt;
	&lt;li&gt;Update the `StreamTask.purgableOffsets` method to check if the offset eligible for purging has been committed and set a break point if not.&lt;/li&gt;
	&lt;li&gt;Let the application run for 16 hours.&#160;&lt;/li&gt;
	&lt;li&gt;At that point, the code hit the breakpoint and this is the observed state&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;consumedOffsets = {HashMap@146436} &#160;size = 1
&#160; {TopicPartition@116508} &quot;bad-repartition-purge-repartition-repartition-3&quot; -&amp;gt; {Long@146438} 14439811
&#160; &#160;key = {TopicPartition@116508} &quot;bad-repartition-purge-repartition-repartition-3&quot;
&#160; &#160;value = {Long@146438} 14439811
&#160;committedOffsets = {HashMap@146437} &#160;size = 1
&#160; {TopicPartition@116508} &quot;bad-repartition-purge-repartition-repartition-3&quot; -&amp;gt; {Long@146410} 14439780
&#160; &#160;key = {TopicPartition@116508} &quot;bad-repartition-purge-repartition-repartition-3&quot;
&#160; &#160;value = {Long@146410} 14439780&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note that the consumed offset (14439811) was ahead of the committed offset (14439780).&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I plan to have a PR for this soon.&lt;/p&gt;</comment>
                            <comment id="17894905" author="bbejeck" created="Fri, 1 Nov 2024 19:34:08 +0000"  >&lt;p&gt;I made another pass over the log file and I&apos;m not convinced the first PR is the correct fix, so I closed it in favor of another approach.&lt;/p&gt;</comment>
                            <comment id="17894906" author="bbejeck" created="Fri, 1 Nov 2024 19:34:33 +0000"  >&lt;p&gt;Cancelled PR going back for another fix&lt;/p&gt;</comment>
                            <comment id="17895232" author="herbert.wespi" created="Mon, 4 Nov 2024 09:04:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; That is really great news that you were able to reproduce the issue as well.&lt;br/&gt;
Thanks a lot for your effort. I&apos;m looking forward for a fix of this issue.&lt;/p&gt;</comment>
                            <comment id="17895432" author="bbejeck" created="Mon, 4 Nov 2024 21:33:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=herbert.wespi&quot; class=&quot;user-hover&quot; rel=&quot;herbert.wespi&quot;&gt;herbert.wespi&lt;/a&gt; - I&apos;ve pushed a new PR&lt;/p&gt;</comment>
                            <comment id="17895529" author="herbert.wespi" created="Tue, 5 Nov 2024 08:50:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbejeck&quot; class=&quot;user-hover&quot; rel=&quot;bbejeck&quot;&gt;bbejeck&lt;/a&gt; The new PR looks in my opinion very promising. I&apos;m sure this will fix our problem as the commitedOffsets are set only after successful commit.&lt;/p&gt;</comment>
                            <comment id="17911275" author="ableegoldman" created="Thu, 9 Jan 2025 00:59:12 +0000"  >&lt;p&gt;Nice find! Seems like a pretty bad bug D:&#160;&lt;/p&gt;

&lt;p&gt;I know the Affect Version is set to 3.7.1 but I&apos;m assuming that&apos;s just he version you saw this on, and not necessarily the minimum affected version? I&apos;m trying to help some users figure out when/if they might hit this.&lt;/p&gt;

&lt;p&gt;First, my understanding is that there are two ways to trigger this bug:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Hitting a TimeoutException during commit, since we will swallow this (unless/until task.timeout.ms is hit) and proceed on to purging the repartition topic events&lt;/li&gt;
	&lt;li&gt;Hitting any kind of non-fatal exception (eg TaskMigratedException) after processing some N records in one poll loop, dropping out &amp;amp; rejoining the group, and then resuming but only processing some M &amp;lt; N records before getting to the next commit/purge.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I assume the first case was introduced as part of KIP-572 and therefore affects 2.8+, but the second case probably impact all versions (prior to this fix), right? Should we change the &quot;Affects Version&quot; of the ticket to reflect this so users know this may be present in earlier versions as well (and hopefully encourage them to upgrade for the fix!)&lt;/p&gt;

&lt;p&gt;In fact I&apos;m guessing this bug was actually even worse for EOS apps in older versions, specifically those prior to 3.2, since we used to purge on every commit. At least the new repartition purge interval config from &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-13549&quot; title=&quot;Add &amp;quot;delete interval&amp;quot; config&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-13549&quot;&gt;&lt;del&gt;KAFKA-13549&lt;/del&gt;&lt;/a&gt; makes it so that EOS apps aren&apos;t purging every 100ms by default, decreasing the chance of hitting this&lt;/p&gt;

&lt;p&gt;Lastly, I guess this primarily affects EOS since uncommitted records in the repartition topic can&apos;t be processed. Whereas under ALOS it&apos;s possible the downstream task managed to consume/process the repartition records before they were prematurely deleted. So it&apos;s probably relatively more rare to hit this with ALOS than EOS, though both are affected by the bug in theory, yes?&lt;/p&gt;</comment>
                            <comment id="17911430" author="herbert.wespi" created="Thu, 9 Jan 2025 11:07:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; You are right, I did set the affect version to 3.7.1 because this was the version we used when we discovered this problem. The bug exist potentially since the purging was introduced.&lt;br/&gt;
Not every TimeoutException or TaskMigratedException was actually causing this bug, because normally the corrupted stream task got closed and removed from task manager.&lt;br/&gt;
However, in some rare situation the stream task got recycled and this was the problem.&lt;/p&gt;

&lt;p&gt;We had this problem only when EOS was enabled. I guess because EOS is enabling transactions.&lt;br/&gt;
Therefore, I think with ALOS and without transaction it is not a problem.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="13071837" name="screenshot-1.png" size="109329" author="herbert.wespi" created="Fri, 27 Sep 2024 12:18:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            43 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1rnq8:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>