<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:41:21 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-1510] Force offset commits when migrating consumer offsets from zookeeper to kafka</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-1510</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;When migrating consumer offsets from ZooKeeper to kafka, we have to turn on dual-commit (i.e., the consumers will commit offsets to both zookeeper and kafka) in addition to setting offsets.storage to kafka. However, when we commit offsets we only commit offsets if they have changed (since the last commit). For low-volume topics or for topics that receive data in bursts offsets may not move for a long period of time. Therefore we may want to force the commit (even if offsets have not changed) when migrating (i.e., when dual-commit is enabled) - we can add a minimum interval threshold (say force commit after every 10 auto-commits) as well as on rebalance and shutdown.&lt;/p&gt;

&lt;p&gt;Also, I think it is safe to switch the default for offsets.storage from zookeeper to kafka and set the default to dual-commit (for people who have not migrated yet). We have deployed this to the largest consumers at linkedin and have not seen any issues so far (except for the migration caveat that this jira will resolve).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12724186">KAFKA-1510</key>
            <summary>Force offset commits when migrating consumer offsets from zookeeper to kafka</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jjkoshy">Joel Jacob Koshy</assignee>
                                    <reporter username="jjkoshy">Joel Jacob Koshy</reporter>
                        <labels>
                            <label>newbie</label>
                    </labels>
                <created>Fri, 27 Jun 2014 18:27:41 +0000</created>
                <updated>Fri, 5 Sep 2014 19:21:15 +0000</updated>
                            <resolved>Fri, 5 Sep 2014 19:21:15 +0000</resolved>
                                    <version>0.8.2.0</version>
                                    <fixVersion>0.8.2.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14066294" author="nmarasoiu" created="Fri, 18 Jul 2014 12:02:32 +0000"  >&lt;p&gt;Hi, it sounds clear and simple enough, I am going to try this.&lt;br/&gt;
I will probably come back with some questions for the low level detail.&lt;/p&gt;

&lt;p&gt;Why is the offset management moving from zookeeper to kafka? To ease the consumer and favor language proliferation of consumers ? Is kafka managing them through zookeeper as well, behind the scenes, or is it using its own / other cluster / consensus mechanism to store the offsets in a HA manner?&lt;/p&gt;</comment>
                            <comment id="14066532" author="guozhang" created="Fri, 18 Jul 2014 17:06:22 +0000"  >&lt;p&gt;Hi Nicolae,&lt;/p&gt;

&lt;p&gt;Thanks for taking this ticket. You can take a look at the offset management design proposal for the motivations of moving it away from ZK.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset+Management&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset+Management&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14066792" author="jjkoshy" created="Fri, 18 Jul 2014 19:50:44 +0000"  >&lt;p&gt;I think it should be sufficient to force commit only on shutdown while dual-commit is enabled. i.e., no need to force commit at intervals.&lt;/p&gt;</comment>
                            <comment id="14072437" author="jjkoshy" created="Wed, 23 Jul 2014 21:58:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoiu&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoiu&quot;&gt;nmarasoiu&lt;/a&gt; do you think you will be able to take this on in the next couple days?&lt;/p&gt;</comment>
                            <comment id="14072913" author="nmarasoiu" created="Thu, 24 Jul 2014 07:03:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjkoshy&quot; class=&quot;user-hover&quot; rel=&quot;jjkoshy&quot;&gt;jjkoshy&lt;/a&gt; Yes I am going to tackle this these days, have a first patch proposal in the weekend or sooner.&lt;/p&gt;</comment>
                            <comment id="14075447" author="nmarasoi" created="Sat, 26 Jul 2014 18:03:57 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
isAutoCommit argument works exactly the other way around, apparently it is &quot;false&quot; from the scheduled auto commit and to &quot;true&quot; from zkConsConn.commitOffsets()?&lt;/p&gt;

&lt;p&gt;So the migration of offsets from zk to kafka is to : set dual commit and kafka storage, restart consumers, wait for kafka to be copied on the offset commits, and take out dual commit.&lt;/p&gt;

&lt;p&gt;So currently kafka is copied with the offsets only when data flows, and for the purpose of this task, we need to add one or 2 more cases when it is getting the offset: when shutting down, or perhaps periodically.&lt;/p&gt;

&lt;p&gt;So this task applies only when storage==kafka and dualCommit ==true, right?&lt;/p&gt;

&lt;p&gt;I would first ask why the write to zookeeper the new offsets, only if the write to kafka was ok? My assumption is To make sure only one write to zookeeper, even though the process of writing to kafka may involve retries. &lt;/p&gt;

&lt;p&gt;I would write both directions at all time, and perhaps keep 2 checkpoint structures, one kafka one zookeeper.&lt;/p&gt;


&lt;p&gt;I create a patch now with: a forceCommit that will make that all offsets are commited to both kafka and zookeeper when shutting down in dual commit mode.&lt;/p&gt;

&lt;p&gt;The usefulness of committing all offsets not only to kafka but to zookeeper as well comes at least from one reason: the one I mentioned above, that if kafka offset write fails completely, zookeeper is never copied on that.&lt;/p&gt;

&lt;p&gt;Forcing all offsets to zk on shutdown too does indeed have the drawback that it will typically copy the same offsets again, and not only once but potentially several times (if kafka is retried).&lt;br/&gt;
However the alternative is to commit to both kafka and zookeeper unconditionally in the normal flow (right now, the commit to zk happens only after a successful commit to kafka if any). That too poses the same risk of committing multiple times to a system (zk) if the other (kafka) needs retries. So a clean way here would be a completely different OffsetDAO implementation, one on kafka , one on zookeeper, and one on dual mode, and read, as now max(both), while write goes to the 2 implementations, each of them doing retries without affecting the other!&lt;/p&gt;</comment>
                            <comment id="14075570" author="nmarasoi" created="Sun, 27 Jul 2014 06:40:23 +0000"  >&lt;p&gt;attached patch as per my interpretation and tradeoffs detailed in my comments&lt;/p&gt;</comment>
                            <comment id="14078904" author="nmarasoi" created="Wed, 30 Jul 2014 05:12:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjkoshy&quot; class=&quot;user-hover&quot; rel=&quot;jjkoshy&quot;&gt;jjkoshy&lt;/a&gt; Can you please take a look at my comments+code, it will probably take one more iteration at least to make it.&lt;/p&gt;</comment>
                            <comment id="14079219" author="nmarasoi" created="Wed, 30 Jul 2014 11:58:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt; Hi, can you please help me with feedback on my comment + code, or who can I ask, so that I can go in the right direction?&lt;/p&gt;</comment>
                            <comment id="14080164" author="jjkoshy" created="Wed, 30 Jul 2014 23:17:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt; - sure thing. Will get back to you on this.&lt;/p&gt;</comment>
                            <comment id="14081781" author="jjkoshy" created="Fri, 1 Aug 2014 01:12:23 +0000"  >&lt;p&gt;Re: isAutoCommit:&lt;br/&gt;
Yes you are right. Can you fix that?&lt;/p&gt;

&lt;p&gt;Re: migration steps:&lt;br/&gt;
Yes that is correct. &quot;Wait for kafka to be copied on commits&quot; would essentially mean do one rolling bounce after your patch (since the shutdown guarantees that the offsets would have moved over).&lt;/p&gt;

&lt;p&gt;Re: applies only when storage==kafka and dual-commit==true.&lt;br/&gt;
Yes that is correct.&lt;/p&gt;

&lt;p&gt;Re:  why the write to zookeeper the new offsets, only if the write to kafka was ok?&lt;br/&gt;
As you observed, avoid redundant writes to ZooKeeper, but also for consistency between ZK and Kafka (although this does not matter too much since while transitioning they are inconsistent to start with).&lt;/p&gt;

&lt;p&gt;Re: separate DAOs&lt;br/&gt;
We probably don&apos;t need that since we plan to eventually remove support for ZooKeeper-based offsets from the server-side - i.e., a consumer can choose to store offsets elsewhere (including ZooKeeper) but support for doing that via the OffsetCommitRequest channel will be removed.&lt;/p&gt;

&lt;p&gt;Your patch looks good, but I&apos;m unclear on why you need the &quot;|| forceCommit&quot; on line 318&lt;/p&gt;</comment>
                            <comment id="14082166" author="nmarasoi" created="Fri, 1 Aug 2014 11:52:51 +0000"  >&lt;p&gt;Hi, the &quot;|| forceCommit&quot; on line 318 is meant to ensure write to zookeeper on shutdown even in the situation when kafka commits do not work.&lt;/p&gt;</comment>
                            <comment id="14083237" author="jjkoshy" created="Sat, 2 Aug 2014 00:21:22 +0000"  >&lt;p&gt;I see - my thinking was since we retry indefinitely until a successful commit it only needs to be done at the end (once) after a successful commit to Kafka.&lt;/p&gt;

&lt;p&gt;So to summarize - let me know if you have any comments/questions:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Can you fix the issue you caught with the isAutoCommit flag?&lt;/li&gt;
	&lt;li&gt;Probably unnecessary to have the &quot;|| forceCommit&quot;&lt;/li&gt;
	&lt;li&gt;Also, as mentioned in the summary I think it is reasonable to switch the default offsets.storage to Kafka and set dual.commit to true.&lt;/li&gt;
	&lt;li&gt;Can you also run the unit tests and verify? It will be useful to also run the system tests (at least the mirror maker test suite). See &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Kafka+System+Tests#KafkaSystemTests-RunningSystemTest&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Kafka+System+Tests#KafkaSystemTests-RunningSystemTest&lt;/a&gt; for more information on this - it should be sufficient to just run the mirror maker tests.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14083541" author="nmarasoiu" created="Sat, 2 Aug 2014 12:30:12 +0000"  >&lt;p&gt;Where is the indefinite retry you mention? I don&apos;t think it is indefinite..&lt;/p&gt;


</comment>
                            <comment id="14083543" author="nmarasoiu" created="Sat, 2 Aug 2014 12:34:12 +0000"  >&lt;p&gt;It is a limited retry , and I found no easy way to determine if this is the&lt;br/&gt;
last attempt, so that I write to zookeeper as well. This can be taken out&lt;br/&gt;
to a different jira task too, but I would let the zookeeper ensure-commit&lt;br/&gt;
here too, and regard this task as a &quot;make all efforts to commit all offsets&lt;br/&gt;
to both storages at shutdown&quot; and rename as such.&lt;/p&gt;


&lt;p&gt;On Sat, Aug 2, 2014 at 3:28 PM, Nicolae Marasoiu &amp;lt;nicolae.marasoiu@gmail.com&lt;/p&gt;
</comment>
                            <comment id="14083551" author="nmarasoi" created="Sat, 2 Aug 2014 12:57:29 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have given it more consideration, and indeed to &quot;||force&quot; on 318 it is a different concern, which can be taken to another task.&lt;br/&gt;
The risk which it would solve, is that when kafka is out for the limited retry count during shutdown, at least zookeeper would get the offsets, and the consumer will not rewind. However it is low probability that both systems are down, so zookeeper would likely be up to date when kafka is down, for instance. The probability that zookeeper will get flooded with all offsets multiple times kafka is retried is comparable to that low probability.&lt;/p&gt;

&lt;p&gt;So, for this task, I take out that line 318 part of the patch, test went fine.&lt;/p&gt;

&lt;p&gt;I will create another task for isAutoCommit issue and analyze if the meaning is truly reversed, cause I feel it is only partially and perhaps used correctly with the reversed name, and it is mostly diffent thing.&lt;/p&gt;

&lt;p&gt;I will do the config changes, no prob - switch the default offsets.storage to Kafka and set dual.commit to true.&lt;/p&gt;</comment>
                            <comment id="14083878" author="nmarasoi" created="Sun, 3 Aug 2014 05:54:55 +0000"  >&lt;p&gt;re-uploaded the patch &lt;/p&gt;</comment>
                            <comment id="14091100" author="jjkoshy" created="Fri, 8 Aug 2014 18:26:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt; I realized later there is actually a flaw in how we get rid of offset commits from old (non-existent) consumers.&lt;/p&gt;

&lt;p&gt;As of now, the offset manager does the following: it periodically goes through its entire cache (i.e., hashtable of offsets) and extracts those entries that have a timestamp earlier than some staleness threshold. It then proceeds to add tombstones for those entries in the offsets commit log.&lt;/p&gt;

&lt;p&gt;The problem with this approach as it stands is similar to the original issue that this jira intended to address. A live consumer may be consuming a low volume topic and its offset may change infrequently. i.e., its offset may not move within the staleness threshold. If we delete the offset and a consumer rebalance occurs and fetches that offset, then depending on the auto.offset.reset configuration, it will pick up the new latest offset of the topic (in which case the consumer could lose some messages) or the earliest offset (in which case the consumer will see duplicates).&lt;/p&gt;

&lt;p&gt;I think the fix for this is the following and I&apos;m backtracking to what I earlier wrote and later (incorrectly) thought was unnecessary:&lt;/p&gt;

&lt;p&gt;A consumer implementation can optionally choose to selectively commit only offsets that have changed since the last commit. HOWEVER, there should be a configurable interval at which the consumer should always commit ALL its offsets regardless of whether it has changed or not.&lt;/p&gt;</comment>
                            <comment id="14091103" author="jjkoshy" created="Fri, 8 Aug 2014 18:28:36 +0000"  >&lt;p&gt;Would you be able to update your patch to take into account the above?&lt;/p&gt;</comment>
                            <comment id="14092205" author="junrao" created="Sun, 10 Aug 2014 21:15:26 +0000"  >&lt;p&gt;Thinking about this a bit more, would it be more reliable to do the expiration of an offset based on the last connect time from the client, instead of the last time the offset is modified? In the new consumer, we will be tracking the set of consumers per consumer group on the broker. We can expire an offset if the time since the last time the partition was actively owned by a consumer exceeds the threshold. Handling consumer coordinator failover can be a bit tricky. We can probably just start doing the expiration countdown from the beginning during the failover. This means that the removal of some of the offsets may be delayed. This maybe ok since the consumer coordinator failover should be rare.&lt;/p&gt;</comment>
                            <comment id="14094347" author="nmarasoi" created="Tue, 12 Aug 2014 17:25:54 +0000"  >&lt;p&gt;ok, so I have not fully understood, but what I think I did, is that for the moment there is no clear decision on any modifications to the patch as it is now, the way that i understand it&lt;/p&gt;</comment>
                            <comment id="14094391" author="jjkoshy" created="Tue, 12 Aug 2014 17:51:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; yes that would make sense. However, we don&apos;t have this tracking implemented in the existing consumer and I would rather not add that feature now just to fix an existing bug. i.e., I think we should just fix the current issue by forcing commits (either periodically or always) when using Kafka-based offset storage.&lt;/p&gt;</comment>
                            <comment id="14095940" author="jjkoshy" created="Wed, 13 Aug 2014 19:00:45 +0000"  >&lt;p&gt;After some discussion with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; here are some additional comments to help clarify my earlier reasoning:&lt;/p&gt;

&lt;p&gt;In order to migrate offsets from ZooKeeper to Kafka, at minimum we need to force an unfiltered commit (regardless of whether offsets have changed or not) at some point - e.g., shut down of the consumer.&lt;/p&gt;

&lt;p&gt;An orthogonal issue is that of a consumer that consumes a low-volume topic. i.e., if the offsets don&apos;t change within the offset retention threshold on the offset manager (defaults to one day) then those offsets will be deleted. If the consumer fails for any reason and does an offset fetch, it will reset to earliest or latest. We have a couple of options:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;One possible approach to address this is to configure the broker-side offset retention period to a large value - i.e., larger than the maximum retention period of any topic. This is not ideal because: (a) if there are short-lived (say, console-) consumers that come and go often then those offsets can sit around for a long time; (b) in general, you cannot really come up with a retention period for a compacted topics. So I would not want to do this, but I wrote this here for completeness.&lt;/li&gt;
	&lt;li&gt;Another approach is to do UN-filtered commits if offsets.storage is set to Kafka. i.e., commit everything always.&lt;/li&gt;
	&lt;li&gt;Yet another approach is to do unfiltered commits at a configurable interval.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thoughts?&lt;/p&gt;

&lt;p&gt;My preference after thinking about it is to go with the second approach.&lt;/p&gt;</comment>
                            <comment id="14099423" author="junrao" created="Sat, 16 Aug 2014 01:11:48 +0000"  >&lt;p&gt;Yes, I agree that always do unfiltered commits when offset.storage is used is the simplest and is good enough. &lt;/p&gt;</comment>
                            <comment id="14101929" author="jjkoshy" created="Tue, 19 Aug 2014 06:42:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt; can you update your patch to do unfiltered commits if offsets.storage is kafka? Or if you have any other preference in approach, feel free to comment.&lt;/p&gt;</comment>
                            <comment id="14108852" author="jjkoshy" created="Mon, 25 Aug 2014 07:39:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt; do you think you will have time to wrap this up? If not, let me know or if you need any clarification.&lt;/p&gt;</comment>
                            <comment id="14108857" author="nmarasoiu" created="Mon, 25 Aug 2014 07:48:00 +0000"  >&lt;p&gt;Hi, I will attach a patch with unfiltered commits today, or latest tomorrow;&lt;/p&gt;

&lt;p&gt;So to clarify the requirement, my understanding is that, when storage =&lt;br/&gt;
kafka and dualcommit = enabled (or just one of those?), we are going to&lt;br/&gt;
make each and every offsets commit to kafka an unfiltered one i.e. commit&lt;br/&gt;
all offsets regardless of change or not; do we do this unfiltered commit&lt;br/&gt;
also on zookeeper?&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Nicu&lt;/p&gt;


</comment>
                            <comment id="14109432" author="jjkoshy" created="Mon, 25 Aug 2014 18:10:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt; that&apos;s right - whenever storage=kafka, we should to unfiltered commits to kafka. In other words, ALL offset commits sent to kafka should be unfiltered.&lt;/p&gt;

&lt;p&gt;As for commits to zookeeper: if storage=zookeeper then we can do filtered commits. If storage=kafka and dual.commit=enabled then if the code doesn&apos;t get too complicated we should continue to do filtered commits to zookeeper (but unfiltered to kafka).&lt;/p&gt;</comment>
                            <comment id="14114244" author="nmarasoi" created="Thu, 28 Aug 2014 19:55:31 +0000"  >&lt;p&gt;Hi, I will do this tomorrow, Friday, so I hope you will have a patch Friday morning your time.&lt;/p&gt;

&lt;p&gt;So you say that the only condition to do unfiltered is kafka storage, regardless of dual commit mode or single commit mode, yes?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Nicu&lt;/p&gt;</comment>
                            <comment id="14115305" author="nmarasoiu" created="Fri, 29 Aug 2014 14:57:04 +0000"  >&lt;p&gt;Patch to push unfiltered offsets to both Kafka and potentially Zookeeper when Kafka is configured to be the offset storage&lt;/p&gt;</comment>
                            <comment id="14115603" author="nmarasoi" created="Fri, 29 Aug 2014 18:11:09 +0000"  >&lt;p&gt;Attached a patch - I am doing unfiltered commits to kafka and using offsets checkpoint Map for zookeeper incremental commits only (in both zk storage and dual commit modes) - its reads and mutations are now part of the commitToZk method exclusively in the suggested approach.&lt;/p&gt;

&lt;p&gt;Right now, the rearrangement of topic topicRegistry into offsettsToCommit, a different reified structure with no more filtering in the process of its reification seems a bit futile, but because we got .size if, and we got usage of the structure below, and to minimize changes brought by this task (and leave them for an obvious future need of refactoring on the bigger scale this class), I let it like this.&lt;/p&gt;

&lt;p&gt;The other optimization I could do, but not included in the patch, is to keep a state of the commit timestamp for each partition, and use that for filtering commits to kafka, based on a configurable maximum idleness of the partition offset commit for each partition.&lt;/p&gt;

&lt;p&gt;A more primitive form of the same optimization, that would only protect from repeatedly committing to good brokers because of the broken ones, I could have such a state in a local structure for the duration of the method, just to make sure we keep retrying only the failed commits.&lt;/p&gt;</comment>
                            <comment id="14118294" author="nmarasoi" created="Tue, 2 Sep 2014 16:06:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjkoshy&quot; class=&quot;user-hover&quot; rel=&quot;jjkoshy&quot;&gt;jjkoshy&lt;/a&gt; Hi, can you check my patch + comments &amp;amp; provide feedback pls?&lt;/p&gt;</comment>
                            <comment id="14122243" author="jjkoshy" created="Fri, 5 Sep 2014 01:12:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt; Your patch looks good to me - however, it does not cleanly apply on the latest trunk. Would you mind rebasing? If you don&apos;t have time I can take care of it as well.&lt;/p&gt;</comment>
                            <comment id="14122443" author="nmarasoi" created="Fri, 5 Sep 2014 05:07:53 +0000"  >&lt;p&gt;Attached rebased patch&lt;/p&gt;</comment>
                            <comment id="14123423" author="jjkoshy" created="Fri, 5 Sep 2014 19:21:15 +0000"  >&lt;p&gt;Thanks for the patch. +1 and committed to trunk.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12665351" name="Patch_to_push_unfiltered_offsets_to_both_Kafka_and_potentially_Zookeeper_when_Kafka_is_con.patch" size="1966" author="nmarasoiu" created="Fri, 29 Aug 2014 14:57:04 +0000"/>
                            <attachment id="12666669" name="Unfiltered_offsets_commit_to_kafka_rebased.patch" size="4343" author="nmarasoi" created="Fri, 5 Sep 2014 05:07:53 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>402371</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 11 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1x987:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>402433</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>jjkoshy</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>