<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:25:01 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-10179] State Store Passes Wrong Changelog Topic to Serde for Optimized Source Tables</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-10179</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;&lt;tt&gt;MeteredKeyValueStore&lt;/tt&gt; passes the name of the changelog topic of the state store to the state store serdes. Currently, it always passes &lt;tt&gt;&amp;lt;application ID&amp;gt;-&amp;lt;store name&amp;gt;-changelog&lt;/tt&gt; as the changelog topic name. However, for optimized source tables the changelog topic is the source topic. &lt;br/&gt;
Most serdes do not use the topic name passed to them. However, if the serdes actually use the topic name for (de)serialization, e.g., when Kafka Streams is used with Confluent&apos;s Schema Registry, a &lt;tt&gt;org.apache.kafka.common.errors.SerializationException&lt;/tt&gt; is thrown.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13311987">KAFKA-10179</key>
            <summary>State Store Passes Wrong Changelog Topic to Serde for Optimized Source Tables</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cadonna">Bruno Cadonna</assignee>
                                    <reporter username="cadonna">Bruno Cadonna</reporter>
                        <labels>
                    </labels>
                <created>Wed, 17 Jun 2020 15:36:06 +0000</created>
                <updated>Tue, 9 Mar 2021 09:14:57 +0000</updated>
                            <resolved>Mon, 13 Jul 2020 15:54:52 +0000</resolved>
                                    <version>2.5.0</version>
                                    <fixVersion>2.7.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="17139926" author="desai.p.rohan" created="Thu, 18 Jun 2020 19:05:48 +0000"  >&lt;p&gt;I&apos;m not sure it&apos;s correct to use the same &quot;topic&quot; name for materializing optimized source tables, as it&apos;s logically different data. In the normal flow (not recovery), we&apos;re taking the topic data, validating/transforming it by deserializing it (which might apply some transforms like projecting just fields of interest), and then serializing it, and then writing it into the store. So the &quot;topic&quot; we pass to the serializer should be different since it represents different data from the source topic.&lt;/p&gt;

&lt;p&gt;This has consequences in practice when used with a schema registry using the confluent serializers. If we use the same topic, `serialize` might register a different schema with the source subject, which we probably don&apos;t want.&lt;/p&gt;

&lt;p&gt;I think the technically correct thing to do (though this is of course more expensive) would be (when the source table is optimized) to deserialize and serialize each record when restoring.&lt;/p&gt;

&lt;p&gt;Another issue that I think exists (need to try to reproduce) that deserializing/serializing would solve is skipped validation. The source topic deserializer functions as a sort of validator for records from the source topic. When the streams app is configured to skip on deserialization errors, bad source records are just skipped. However if we restore by just writing those records to the state store, we now hit the deserialization error when reading the state store, which is a query-killing error.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17140075" author="mjsax" created="Fri, 19 Jun 2020 00:42:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m not sure it&apos;s correct to use the same &quot;topic&quot; name for materializing optimized source tables, as it&apos;s logically different data. In the normal flow (not recovery), we&apos;re taking the topic data, validating/transforming it by deserializing it (which might apply some transforms like projecting just fields of interest), and then serializing it, and then writing it into the store. So the &quot;topic&quot; we pass to the serializer should be different since it represents different data from the source topic.&lt;/p&gt;

&lt;p&gt;For this case, the soure-topic-changelog optimization does no apply, and the store would always have its own changelog topic. And thus, the input-topic schema registered in the SR should not be &quot;touched&quot;, and the write to the changelog topic should register a new scheme using the changelog topic name. Thus, no naming issue in SR should happen.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The source-topic-changelog optimization really only applies, if the data in the input topic is exactly the same as in the changelog topic and thus, we avoid creating the changelog topic. To ensure this, we don&apos;t allow any processing to happen in between. The data would be deserialized and re-serialized using the same Serde (this is inefficiency we pay, as we also need to send the de-serialized data downstream for further processing).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Another issue that I think exists (need to try to reproduce) that deserializing/serializing would solve is skipped validation. The source topic deserializer functions as a sort of validator for records from the source topic. When the streams app is configured to skip on deserialization errors, bad source records are just skipped. However if we restore by just writing those records to the state store, we now hit the deserialization error when reading the state store, which is a query-killing error.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is a known issue and tracked via:&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8037&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8037&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17140158" author="desai.p.rohan" created="Fri, 19 Jun 2020 03:18:41 +0000"  >&lt;p&gt;Deserialization may itself be a transformation. For example, suppose I have source data with 10 fields, but only care about 3 of them for my stream processing app. It seems that it would be reasonable to provide a deserializer that just extracts those 3 fields. I suppose you could express this as a projection after creating the table, but that does preclude optimizations that use selective deserialization. And it may be much more expensive to do the materialization (since you&apos;re potentially materializing lots of data unnecessarily). I think there should be some way to achieve each of the following:&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;optimized and the data in the store is exactly the same as the topic data . In this case (what&apos;s implemented today) the data can be restored by writing the source records into the store&lt;/li&gt;
	&lt;li&gt;optimized and the deserializer transforms the data somehow. In this case the data can be restored by deserializing/serializing each row from the source topic before writing it into the store. I don&apos;t think this is possible today.&lt;/li&gt;
	&lt;li&gt;not optimized (w/ which you could have a transforming deserializer and faster recovery, at the cost of extra data in kafka). I don&apos;t think this is possible today without turning all optimizations off.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&amp;gt;&#160;This is a known issue and tracked via:&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8037&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8037&lt;/a&gt;&lt;br/&gt;
 &#160;ack - thanks!&lt;/p&gt;</comment>
                            <comment id="17140166" author="desai.p.rohan" created="Fri, 19 Jun 2020 03:30:28 +0000"  >&lt;p&gt;Also, it&apos;s not really clear from the documentation that `serialize(deserialize())` is assumed to be the identity function&#160; for `ktable(..)`.&lt;/p&gt;</comment>
                            <comment id="17140208" author="mjsax" created="Fri, 19 Jun 2020 04:37:40 +0000"  >&lt;p&gt;What you say is fair I guess. Given the current code, if you want to do any of those, you need to disable the optimization.&lt;/p&gt;

&lt;p&gt;However, for the actual bug this ticket is about, the problem seems to be, that if the optimization is turned on, at some point in the code we pass the changelog topic name into the serde instead of the source topic name. And thus the schema cannot be found and the serde crashes. Thus, this ticket should focus on this bug.&lt;/p&gt;

&lt;p&gt;Not sure if &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8037&quot; title=&quot;KTable restore may load bad data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8037&quot;&gt;KAFKA-8037&lt;/a&gt; covers all cases you describe. Maybe you want to follow up on this ticket (so we can extent its scope) or create a new ticket that describes the shortcomings of the current implementation.&lt;/p&gt;</comment>
                            <comment id="17140328" author="cadonna" created="Fri, 19 Jun 2020 08:14:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=desai.p.rohan&quot; class=&quot;user-hover&quot; rel=&quot;desai.p.rohan&quot;&gt;desai.p.rohan&lt;/a&gt; While I find the idea of optimizing the materialization in the deserializer intriguing, I think the performance penalty that we would pay by deserializing and serializing each record during restoration is not worthwhile. Additionally &amp;#8211; if optimization is turned on &amp;#8211; we would need to read the original data from the source topic instead of the projected data from the changelog topic during each restoration which would again hit performance. Of course, we would need experiments to better understand the implications. &lt;/p&gt;

&lt;p&gt;An alternative idea would be to allow to plugin a byte-based transformation that does not need to deserialize and serialize each record. However, that would not solve the issue of having to read the unprojected data during each restoration.&lt;/p&gt;

&lt;p&gt;If you are concerned with the amount of data to materialize a solution could be to optimize on topology-level by introducing a &lt;tt&gt;map()&lt;/tt&gt; that makes the projection followed by a &lt;tt&gt;toTable()&lt;/tt&gt; to materialize the data. That data read from the input topic would be the unprojected data but the one materialized is the projected one and also during restoration we would just read the projected data. An additional advantage of this method is that you can leave the source table optimization turned on, because it would not apply to this case.&lt;/p&gt;

&lt;p&gt;In summary, the source table optimization was not introduced for the case you describe. IMO, it is not even an optimization in that case. &lt;/p&gt;</comment>
                            <comment id="17143238" author="ableegoldman" created="Tue, 23 Jun 2020 19:21:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=desai.p.rohan&quot; class=&quot;user-hover&quot; rel=&quot;desai.p.rohan&quot;&gt;desai.p.rohan&lt;/a&gt;&#160;I&apos;m not sure I understand why it&apos;s a problem for the deserializer to modify the value slightly, by dropping fields to take your example. We would end up restoring the full bytes into the store, sure, but the plain bytes are never actually used right? We would always go through the deserializer when reading the value from the store and using it in an operation. So the &quot;extra&quot; fields would still get dropped.&lt;/p&gt;

&lt;p&gt;Maybe if your values are bloated with a lot of useful information that you didn&apos;t want to store, this could blow up the disk usage. But I think there&apos;s a difference between a simple operation on data to extract only the relevant bits &#8211; eg dropping a field you don&apos;t care about &#8211; and fundamentally transforming the data to get it into a different form. The former seems reasonable to do during a deserialization, but the latter should be its own operation in the topology.&lt;/p&gt;

&lt;p&gt;Of course, this just applies to modifying the values. If your deserializer modifies the key in any way, this would be a problem since lookups by key would fail after a restoration copies over the plain bytes. But I would argue that it&apos;s illegal to modify the key during de/serialization at all, not because of the restoration issue but because it can cause incorrect partitioning.&lt;/p&gt;

&lt;p&gt;Anyways, I&apos;m probably overlooking something obvious, but I&apos;m struggling to see exactly where and how this breaks. That said I do agree we should clarify that&#160;`serialize(deserialize())` must be the identity for keys&lt;/p&gt;</comment>
                            <comment id="17146707" author="agavra" created="Sat, 27 Jun 2020 00:48:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ableegoldman&quot; class=&quot;user-hover&quot; rel=&quot;ableegoldman&quot;&gt;ableegoldman&lt;/a&gt; I confirmed locally that nothing &quot;breaks&quot; if we use a deserializer that projects a subset of the fields in the record, as you suspected, but consider the following points:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Some of the most popular serdes are asymmetric (e.g. avro builds in the concept of reader/writer schema into their APIs)&lt;/li&gt;
	&lt;li&gt;It may be impossible to determine, for a given serde, whether it is symmetric&lt;/li&gt;
	&lt;li&gt;State after recovery should be identical to before recovery for predictable operations (especially in cloud environments)&lt;/li&gt;
	&lt;li&gt;Some of the most popular serdes have side effects (e.g. Confluent schema registry serdes will create subjects on your behalf)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In practice, the first three points in conjunction with what &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; said (the source-topic-changelog optimization really only applies, if the data in the input topic is exactly the same as in the changelog topic and thus, we avoid creating the changelog topic), means that we can&apos;t safely turn on the source-topic-changelog optimization unless the user indicates either (a) they are using a symmetrical serde or (b) they are willing to waive 3 in order to speed up recovery (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cadonna&quot; class=&quot;user-hover&quot; rel=&quot;cadonna&quot;&gt;cadonna&lt;/a&gt; if we consider 3 a matter of correctness, we can&apos;t sacrifice correctness for performance without the user&apos;s consent).&lt;/p&gt;

&lt;p&gt;Even if the user indicates (a) or (b) above, I still don&apos;t think we can implement the fix described here because of the fourth point. It may be possible that the user is using a symmetric serde but their schema is not identical to the one that wrote to the kafka topic (e.g. ksql, for example, generates a new schema where all the fields are the same but the schema has a different name, I can also easily imagine a schema with &lt;em&gt;more&lt;/em&gt; fields that would write the same value as it read from an event with fewer fields).&lt;/p&gt;

&lt;p&gt;I&apos;m not sure I understand this comment: &quot;The data would be deserialized and re-serialized using the same Serde (this is inefficiency we pay, as we also need to send the de-serialized data downstream for further processing).&quot; Why can&apos;t we just always pass-through the data into the state store if the optimization is enabled?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17161423" author="guozhang" created="Mon, 20 Jul 2020 17:37:16 +0000"  >&lt;p&gt;I agree with Almog and Rohan&#8217;s arguments here. What I&#8217;m thinking is how we could define a principle for users to indicate that:&lt;/p&gt;

&lt;p&gt;1) the bytes in the source topic are exactly the same as bytes in the state store (i.e. the serdes are symmetric).&lt;br/&gt;
2) there&#8217;s no side-effects that serde incurs; only 1) and 2) together means it is safe to skip serde during restoration.&lt;br/&gt;
3) and also, there&#8217;s no corrupted or ill-formatted data from source topics that should be skipped when loading into state stores. This is &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8037&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8037&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;During restoration time, compared with during normal processing time.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13315806">KAFKA-10252</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 17 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0fxz4:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>