<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:15 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-405] Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-405</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-46&quot; title=&quot;Commit thread, ReplicaFetcherThread for intra-cluster replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-46&quot;&gt;&lt;del&gt;KAFKA-46&lt;/del&gt;&lt;/a&gt; introduced per partition leader high watermarks. But it stores those in one file per partition. A more performant solution would be to store all high watermarks in a single file on disk&lt;/p&gt;</description>
                <environment></environment>
        <key id="12599038">KAFKA-405</key>
            <summary>Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nehanarkhede">Neha Narkhede</assignee>
                                    <reporter username="nehanarkhede">Neha Narkhede</reporter>
                        <labels>
                    </labels>
                <created>Mon, 16 Jul 2012 21:10:26 +0000</created>
                <updated>Wed, 1 Aug 2012 18:10:55 +0000</updated>
                            <resolved>Wed, 25 Jul 2012 23:42:58 +0000</resolved>
                                    <version>0.8.0</version>
                                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13416894" author="nehanarkhede" created="Wed, 18 Jul 2012 06:20:26 +0000"  >&lt;p&gt;This patch improves the high watermark persistence strategy to store the high watermark values for all partitions in a single file.&lt;/p&gt;

&lt;p&gt;The changes include -&lt;/p&gt;

&lt;p&gt;1. Log.scala&lt;br/&gt;
1.1 Moved the highwatermark file out of the Log. Since highwatermark is related to replication state and not log persistence, it makes sense for it to not be part of kafka.log&lt;br/&gt;
1.2 Since the log recovery logic requires to modify log segments, recoverUptoLastCheckpointedHighWatermark() API stays in Log.scala. It is passed in the last checkpointed high watermark from ReplicaManager during make follower state change&lt;/p&gt;

&lt;p&gt;2. ReplicaManager&lt;br/&gt;
2.1 Added a startup API to ReplicaManager to be consistent with all other components.&lt;br/&gt;
2.2 Added a scheduler that will checkpoint high watermarks at defaultFlushIntervalMs rate. I didn&apos;t think it was useful to introduce another config to control the rate at which high watermarks are flushed to disk, so I reused the one we have for flushing log segments&lt;br/&gt;
2.3 Added a checkpointHighwaterMark() API that will iterate through all the local replicas for each partition and write the high watermark file in the following format&lt;br/&gt;
number of entries  (4 bytes)&lt;br/&gt;
topic              (UTF)&lt;br/&gt;
partition          (4 bytes)&lt;br/&gt;
highwatermark      (8 bytes)&lt;br/&gt;
2.4 Added a readCheckpointedHighWatermark() API that reads the high watermark file to get the latest high watermark for a particular topic/partition. This method is called once per partition on startup, and during every make follower state change.&lt;/p&gt;

&lt;p&gt;3. HighWatermarkPersistenceTest&lt;br/&gt;
Added a couple of unit tests to verify that the new high watermark persistence code is working.&lt;br/&gt;
This will further get tested during system testing, once &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt; is checked in&lt;/p&gt;</comment>
                            <comment id="13417913" author="jkreps" created="Thu, 19 Jul 2012 00:49:32 +0000"  >&lt;p&gt;Some of these comments are not directly related to this change but this is just the first time I have looked at this code in detail&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I think Log.recoverUptoLastCheckpointedHighWatermark(lastKnownHW: Long) is misnamed. I don&apos;t think the log should know anything about hw marks, and it also isn&apos;t doing recovery (e.g. checking the validity of the log), I think it is just blindly truncating the log. Can we change it to Log.truncateTo&lt;/li&gt;
	&lt;li&gt;Is it possible that we need to truncate more than one segment? I.e. couldn&apos;t the segment to be truncated not be the last segment (unlikely with 1gb segments, but still a problem)&lt;/li&gt;
	&lt;li&gt;Can we change the api in MessageSet.truncateUpto should be truncateUpTo or truncateTo&lt;/li&gt;
	&lt;li&gt;Can you make a wrapper for the RandomAccessFile called HighwaterMarkCheckpoint and put the logic related to that there. Intuitively the logic for serializing a checkpoint shouldn&apos;t be mixed into ReplicaManager. Can you also document the file format? Is there a reason this can&apos;t be plain text? Also I think a better approach to the updates would be to create a new file, write to it, and then move it over the old one; this will make the update atomic. Not sure if that is needed...&lt;/li&gt;
	&lt;li&gt;It would be good to have a test that covered the HighwaterMarkCheckpoint file read and write. Just basic reading/writing, nothing fancy.&lt;/li&gt;
	&lt;li&gt;Do we need to version the hw mark checkpoint file format? I.e. maybe the first line of the file is version=x or something... Not sure if that is needed but I am paranoid about persistent formats after blowing that and being stuck. This would let format changes be handled automatically.&lt;/li&gt;
	&lt;li&gt;Can we fix the setters/getters in Replica.scala and make changes to the leo update the leoUpdateTime. Currently I think it is encumbant on the caller to do these two things together which is odd...&lt;/li&gt;
	&lt;li&gt;I think the use of KafkaScheduler is not quite right. This is a thread pool meant to execute many tasks. There should really only be one for all of kafka, not one per background thread. You should probably pass in a central instance as an argument rather than making two new ones.&lt;/li&gt;
	&lt;li&gt;Also I notice that ReplicaManager.getReplica returns an Option. But then everyone who calls it needs to check the option and return an exception if it is not found. Can we just have getReplica either return the Replica or throw the exception?&lt;/li&gt;
	&lt;li&gt;I think abbreviations should be in the form updateLeo not updateLEO and updateIsr not updateISR. Let&apos;s standardize that.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13418886" author="nehanarkhede" created="Fri, 20 Jul 2012 02:39:05 +0000"  >&lt;p&gt;1. Good point. Changed the name to Log.truncateTo(targetOffset)&lt;br/&gt;
2. It is possible to truncate multiple segments. The truncateTo API handles that. It deletes segments that have start offset &amp;gt; targetOffset. Only one segment will ever need to be truncated. Rest will have to be deleted.&lt;br/&gt;
3. Changed FileMessageSet.truncateUpto to truncateTo to make it consistent with Log.truncateTo&lt;br/&gt;
4. Created wrapper HighwatermarkCheckpoint. Documented the file format here. I had thought about atomic updates but there is only one thread that serializes the checkpoints, so didn&apos;t think swapping the old file with the new one would be required, no ?&lt;br/&gt;
5. As for unit testing, I&apos;ve added a new test HighwatermarkPersistenceTest that tests the writing/reading high watermark values for single as well as multiple partitions.&lt;br/&gt;
6. I think it is a good idea to version the high watermark file, just in case we didn&apos;t cover something that we need to in the future&lt;br/&gt;
7. We have a JIRA open for fixing all getters/setters, so I&apos;ll defer that change. The logEndOffset logic is a little tricky. It seems correct to not expose a separate API to set the logEndOffsetUpdateTime and just let the logEndOffset setter API do it. But, here is the problem. The leader needs to record its own log end offset update time while appending messages to local log. However, since the Log doesn&apos;t know anything about logEndOffsetUpdateTime, its append API cannot set the udpate time. Also, the leader cannot use the logEndOffset setter API since its log end offset is recorded by its local Log object. The logEndOffset setter API is meant only to record the follower&apos;s log end offset. But since it makes sense for the update time to be updated while setting the logEndOffset, I&apos;ve fixed it. Basically, the logEndOffset() setter API updates only the logEndOffset time when a local log exists for the replica. For all other cases, it updates both the logEndOffset as well as the logEndOffsetUpdateTime &lt;br/&gt;
8. Yeah, there are several callers that use the getReplica() API and don&apos;t always re-throw an exception. Some are re-throwing an error while others are using the Option to return some default value for some state of the Replica (highwatermark). And case match in Scala is good for that since it always evaluates to a value, a try catch block doesn&apos;t. But if all the callers throw an exception, then it makes sense to have getReplica throw it instead. &lt;br/&gt;
9. You have a valid point about KafkaScheduler usage. However, we name the thread appropriately with every instance of the scheduler. Ideally, if there was a way to override the base thread name independently with the same scheduler, it would be possible to use a single scheduler.&lt;br/&gt;
10. Good point about abbreviations. Fixed that. Let&apos;s standardize on this.&lt;/p&gt;

&lt;p&gt;Also, changed the name to updateLEO to updateLeo&lt;/p&gt;</comment>
                            <comment id="13420731" author="jkreps" created="Mon, 23 Jul 2012 16:02:06 +0000"  >&lt;p&gt;4. The concern I have is that fs writes are not atomic unless they are &amp;lt; 1 block. What happens if the broker fails in the middle of a write? Also is there a reason we can&apos;t just have a plain text file? That will be a little bulkier, but the good thing is you can cat it and see what is there. I think that will be a lot nicer operationally then another binary format...&lt;br/&gt;
5. Can we do that without the Thread.sleeps? For example it would seem this would be accomplished by not using a time based flush interval. Also&lt;br/&gt;
9. Can you add that facility then to KafkaScheduler? Use Thread.setName() with a wrapper runnable. I think making lots of single-threaded thread pools is too hacky.&lt;/p&gt;

&lt;p&gt;Also&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FileChannel.truncateTo sets the size to whatever is given and calls truncate, it would be good to be a little more defensive. If the offset given is larger than size we should handle that gracefully (throw illegalargumentexception or something). Currently it would call truncate() on the filechannel which would have no effect but it would set the size to the new size which would not match the size of the file, which might cause odd things to happen.&lt;/li&gt;
	&lt;li&gt;HighwaterMarkCheckpoint.scala: new RandomAccessFile(path + &quot;/&quot; + HighwaterMarkCheckpoint.highWatermarkFileName. Should use new File(path, filename) for portability.&lt;/li&gt;
	&lt;li&gt;Can you mark any method not in the public interface for ReplicaManager as private? It is currently really hard to tell what the capabilities it provides...&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13420740" author="jkreps" created="Mon, 23 Jul 2012 16:09:59 +0000"  >&lt;p&gt;Also, ReplicaManager:&lt;/p&gt;

&lt;p&gt;This class has a very odd public interface. Instead of managing replicas it has a bunch of passive calls--addLocalReplica(), addRemoteReplica(), etc. Who calls these? KafkaServer seems to have its own wrapper for these. Then it passes these methods on as arguments to KafkaZookeeper. Does this make sense? I think it would be good if ReplicaManager handled replica management, even if that means it depends on zookeeper.&lt;/p&gt;
</comment>
                            <comment id="13421086" author="junrao" created="Tue, 24 Jul 2012 01:30:34 +0000"  >&lt;p&gt;Thanks for patch v2. Some comments:&lt;br/&gt;
20. Log.truncateTo(): The following code seems to be used just for getting the first segment. Can we just use segmentToBeTruncated(0)?&lt;br/&gt;
      segmentToBeTruncated match &lt;/p&gt;
{
        case Some(segment) =&amp;gt;
          val truncatedSegmentIndex = segments.view.indexOf(segment)
          segments.truncLast(truncatedSegmentIndex)
        case None =&amp;gt;
      }

&lt;p&gt;21. FileMessageSet: Do we need setHighWaterMark? It seems it&apos;s always the same as setSize.&lt;/p&gt;

&lt;p&gt;22. ReplicaManager:&lt;br/&gt;
22.1 recordLeaderLogUpdate(): Could we rename it to recordLeaderLogEndOffset()?&lt;br/&gt;
22.2 close(): Could we rename it to shutdown to map startup()?&lt;br/&gt;
22.3 readCheckpointedHighWatermark(): We should just read the HW from memory. The on-disk version is only useful on broker startup when we populate the in-memory HW using the on disk version.&lt;/p&gt;

&lt;p&gt;23. HighwaterMarkCheckpoint: Is it better to name the file &quot;.highwaterMark&quot; so that it&apos;s hidden?&lt;/p&gt;</comment>
                            <comment id="13422566" author="nehanarkhede" created="Wed, 25 Jul 2012 20:09:12 +0000"  >&lt;p&gt;Jay&apos;s comments&lt;/p&gt;

&lt;p&gt;4. Changed the write operation for highwatermark file to be atomic.&lt;br/&gt;
5. The sleep is in place to allow the follower to send another fetch request to the leader to allow the leader to tell it the latest leader high watermark. It cannot be fixed by flushing more frequently&lt;/p&gt;

&lt;p&gt;9. Added the ability to set the name of a thread in KafkaScheduler. Also, saw that the KafkaScheduler took in a isDaemon variable, but didn&apos;t really use it. Refactored KafkaScheduler to create daemon/non-daemon threads with different names.&lt;/p&gt;

&lt;p&gt;10. There was an assertion that protects against this in the only API that called FileMessageSet.truncateTo. Moved that to FileMessageSet instead and changed it to throw KafkaException. Also, handled all exceptions in the become follower/become leader state change API to log an error stating that the state change failed. This will make debugging easier.&lt;/p&gt;

&lt;p&gt;11. Have marked methods not in the public interface for ReplicaManager as private? Agree that there is some room for refactoring. Added your suggestion to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-351&quot; title=&quot;Refactor some new components introduced for replication &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-351&quot;&gt;&lt;del&gt;KAFKA-351&lt;/del&gt;&lt;/a&gt; that we have filed to cover the refactoring of ReplicaManager and KafkaZookeeper. Currently, with the controller patch, KafkaZookeeper is going to look very different. So I&apos;d rather wait until controller patch is in.&lt;/p&gt;

&lt;p&gt;Jun&apos;s comments&lt;/p&gt;

&lt;p&gt;20. There will only be one segment that will fit this criteria =&amp;gt; segment.start &amp;gt;= hw &amp;amp;&amp;amp; segment.endOffset &amp;lt; hw. That code truncates the one and only segment that matches this criteria&lt;/p&gt;

&lt;p&gt;21. The setHighwatermark variable and its references are deleted as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;22. &lt;br/&gt;
1. Changed to recordLeaderLogEndOffset()&lt;br/&gt;
2. Changed close() to shutdown() for LogManager, ReplicaManager and KafkaZookeeper&lt;br/&gt;
3. Good point. Fixed it to read from the file only on startup&lt;/p&gt;

&lt;p&gt;23. Yes, it might be ok to have the file hidden.&lt;/p&gt;

&lt;p&gt;Other fixes -&lt;/p&gt;

&lt;p&gt;Log.scala&lt;br/&gt;
1. truncateTo() had bugs in that it used the size() API on the FileMessageSet of the segment to get the absolute end offset. Fixed it to use the absoluteEndOffset() API of LogSegment instead.&lt;/p&gt;</comment>
                            <comment id="13422713" author="jkreps" created="Wed, 25 Jul 2012 23:10:17 +0000"  >&lt;p&gt;+1 &lt;/p&gt;</comment>
                            <comment id="13422728" author="nehanarkhede" created="Wed, 25 Jul 2012 23:29:52 +0000"  >&lt;p&gt;Removed the sleeps in LogRecoveryTest. Will probably checkin this version of the patch&lt;/p&gt;</comment>
                            <comment id="13422732" author="nehanarkhede" created="Wed, 25 Jul 2012 23:42:58 +0000"  >&lt;p&gt;Thanks a lot for the timely reviews ! Checked in v4 &lt;/p&gt;</comment>
                            <comment id="13426790" author="junrao" created="Wed, 1 Aug 2012 18:10:55 +0000"  >&lt;p&gt;Just had a look of v4. A couple of minor comments:&lt;/p&gt;

&lt;p&gt;40. HighwaterMarkCheckpoint:&lt;br/&gt;
40.1 If tempHwFile already exists, we can just overwrite it since we know hwFile is always safe.&lt;br/&gt;
40.2 There is no need to delete hwFile first and then rename tempHwFile to it. Rename should do the deletion. Currently, if we fail at the bad time, we could end up without a hwFile.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12558877">KAFKA-355</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12536958" name="kafka-405-v1.patch" size="21507" author="nehanarkhede" created="Wed, 18 Jul 2012 06:20:26 +0000"/>
                            <attachment id="12537431" name="kafka-405-v2.patch" size="34367" author="nehanarkhede" created="Sat, 21 Jul 2012 00:38:04 +0000"/>
                            <attachment id="12537887" name="kafka-405-v3.patch" size="75368" author="nehanarkhede" created="Wed, 25 Jul 2012 20:09:12 +0000"/>
                            <attachment id="12537921" name="kafka-405-v4.patch" size="76325" author="nehanarkhede" created="Wed, 25 Jul 2012 23:29:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>248175</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 16 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i09m0n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>54000</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>