<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:54:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-3718] propagate all KafkaConfig __consumer_offsets configs to OffsetConfig instantiation</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-3718</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Kafka has two configurable compression codecs: the one used by the client (source codec) and the one finally used when storing into the log (target codec). The target codec defaults to KafkaConfig.compressionType and can be dynamically configured through zookeeper.&lt;/p&gt;

&lt;p&gt;The GroupCoordinator appends group membership information into the __consumer_offsets topic by:&lt;br/&gt;
1. making a message with group membership information&lt;br/&gt;
2. making a MessageSet with the single message compressed with the source codec&lt;br/&gt;
3. doing a log.append on the MessageSet&lt;/p&gt;

&lt;p&gt;Without this patch, KafkaConfig.offsetsTopicCompressionCodec doesn&apos;t get propagated to OffsetConfig instantiation, so GroupMetadataManager uses a source codec of NoCompressionCodec when making the MessageSet. Let&apos;s say we have enough group information such that the message formed exceeds KafkaConfig.messageMaxBytes before compression but would fall below the threshold after compression using our source codec. Even if we had dynamically configured __consumer_offsets with our favorite compression codec, the log.append will throw RecordTooLargeException during analyzeAndValidateMessageSet since the message was unexpectedly uncompressed instead of having been compressed with the source codec defined by KafkaConfig.offsetsTopicCompressionCodec.&lt;/p&gt;

&lt;p&gt;NOTE: even after this issue is resolved, preliminary tests show that LinkedIn will still hit RecordTooLargeException with large groups that consume many topics (like MirrorMakers with wildcard consumption of .*) since fully expanded subscription and assignment state for each member is put into a single record. But this is a first step in the right direction.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12970304">KAFKA-3718</key>
            <summary>propagate all KafkaConfig __consumer_offsets configs to OffsetConfig instantiation</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="onurkaraman">Onur Karaman</assignee>
                                    <reporter username="onurkaraman">Onur Karaman</reporter>
                        <labels>
                    </labels>
                <created>Tue, 17 May 2016 09:09:53 +0000</created>
                <updated>Thu, 26 May 2016 08:18:52 +0000</updated>
                            <resolved>Thu, 26 May 2016 08:18:52 +0000</resolved>
                                                    <fixVersion>0.10.0.1</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15286269" author="githubbot" created="Tue, 17 May 2016 09:12:32 +0000"  >&lt;p&gt;GitHub user onurkaraman opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1394&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1394&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3718&quot; title=&quot;propagate all KafkaConfig __consumer_offsets configs to OffsetConfig instantiation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3718&quot;&gt;&lt;del&gt;KAFKA-3718&lt;/del&gt;&lt;/a&gt;: propagate all KafkaConfig __consumer_offsets configs to OffsetConfig instantiation&lt;/p&gt;

&lt;p&gt;    Kafka has two configurable compression codecs: the one used by the client (source codec) and the one finally used when storing into the log (target codec). The target codec defaults to KafkaConfig.compressionType and can be dynamically configured through zookeeper.&lt;/p&gt;

&lt;p&gt;    The GroupCoordinator appends group membership information into the __consumer_offsets topic by:&lt;br/&gt;
    1. making a message with group membership information&lt;br/&gt;
    2. making a MessageSet with the single message compressed with the source codec&lt;br/&gt;
    3. doing a log.append on the MessageSet&lt;/p&gt;

&lt;p&gt;    Without this patch, KafkaConfig.offsetsTopicCompressionCodec doesn&apos;t get propagated to OffsetConfig instantiation, so GroupMetadataManager uses a source codec of NoCompressionCodec when making the MessageSet. Let&apos;s say we have enough group information such that the message formed exceeds KafkaConfig.messageMaxBytes before compression but would fall below the threshold after compression using our source codec. Even if we had dynamically configured __consumer_offsets with our favorite compression codec, the log.append will throw RecordTooLargeException during analyzeAndValidateMessageSet since the message was unexpectedly uncompressed instead of having been compressed with the source codec defined by KafkaConfig.offsetsTopicCompressionCodec.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/onurkaraman/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/onurkaraman/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3718&quot; title=&quot;propagate all KafkaConfig __consumer_offsets configs to OffsetConfig instantiation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3718&quot;&gt;&lt;del&gt;KAFKA-3718&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1394.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1394.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1394&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit d65726de2e4a196837de7bb2e11f397482826dca&lt;br/&gt;
Author: Onur Karaman &amp;lt;okaraman@linkedin.com&amp;gt;&lt;br/&gt;
Date:   2016-05-13T06:43:02Z&lt;/p&gt;

&lt;p&gt;    propagate all KafkaConfig __consumer_offsets configs to OffsetConfig instantiation&lt;/p&gt;

&lt;p&gt;    Kafka has two configurable compression codecs: the one used by the client (source codec) and the one finally used when storing into the log (target codec). The target codec defaults to KafkaConfig.compressionType and can be dynamically configured through zookeeper.&lt;/p&gt;

&lt;p&gt;    The GroupCoordinator appends group membership information into the __consumer_offsets topic by:&lt;br/&gt;
    1. making a message with group membership information&lt;br/&gt;
    2. making a MessageSet with the single message compressed with the source codec&lt;br/&gt;
    3. doing a log.append on the MessageSet&lt;/p&gt;

&lt;p&gt;    Without this patch, KafkaConfig.offsetsTopicCompressionCodec doesn&apos;t get propagated to OffsetConfig instantiation, so GroupMetadataManager uses a source codec of NoCompressionCodec when making the MessageSet. Let&apos;s say we have enough group information such that the message formed exceeds KafkaConfig.messageMaxBytes before compression but would fall below the threshold after compression using our source codec. Even if we had dynamically configured __consumer_offsets with our favorite compression codec, the log.append will throw RecordTooLargeException during analyzeAndValidateMessageSet since the message was unexpectedly uncompressed instead of having been compressed with the source codec defined by KafkaConfig.offsetsTopicCompressionCodec.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15293251" author="ijuma" created="Fri, 20 May 2016 12:18:27 +0000"  >&lt;p&gt;This is too late for 0.10.0.0, so updated the fix version.&lt;/p&gt;</comment>
                            <comment id="15301740" author="githubbot" created="Thu, 26 May 2016 08:18:14 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1394&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1394&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12826302">KAFKA-2159</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 25 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2y2g7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>ijuma</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>