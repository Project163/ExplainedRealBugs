<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:41:25 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-1282] Disconnect idle socket connection in Selector</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-1282</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;To reduce # socket connections, it would be useful for the new producer to close socket connections that are idle. We can introduce a new producer config for the idle time.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12697266">KAFKA-1282</key>
            <summary>Disconnect idle socket connection in Selector</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nmarasoi">nicu marasoiu</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                            <label>newbie++</label>
                    </labels>
                <created>Wed, 26 Feb 2014 00:34:49 +0000</created>
                <updated>Fri, 16 Oct 2015 01:54:45 +0000</updated>
                            <resolved>Fri, 16 Oct 2015 01:54:45 +0000</resolved>
                                    <version>0.8.2.0</version>
                                    <fixVersion>0.9.0.0</fixVersion>
                                    <component>producer </component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="14066315" author="nmarasoiu" created="Fri, 18 Jul 2014 13:07:56 +0000"  >&lt;p&gt;Right, the limitation is more critical on the client side of a client-server connection due to port count limitation, and/or socket/file count restrictions of the client env.&lt;/p&gt;

&lt;p&gt;On the other hand, the brokers could close the connections too on such condition, rather than relying on the clients(producers) to protect it.&lt;/p&gt;

&lt;p&gt;However, what is any other reason to reduce the socket connections count? To make the NIO select lighter on the server, on a lesser number of connections? I think epoll is quite relaxed on this.&lt;/p&gt;

&lt;p&gt;I would like to work on this, but also understand the original problem(s) / concern(s) to see if we can also see any more suitable solutions to the particular concern?&lt;/p&gt;</comment>
                            <comment id="14066461" author="jkreps" created="Fri, 18 Jul 2014 15:54:28 +0000"  >&lt;p&gt;The goal is just to reduce server connection count. In our environment there might be a single Kafka producer in each process we run publishing to a small Kafka cluster (say ~20 servers). However there are tens of thousands of client processes. Connections can end up going unused when leadership migrates and we should eventually close these out rather than retaining them indefinitely.&lt;/p&gt;

&lt;p&gt;As you say it is not critical as the server seems to do a good job of dealing with high connection counts, but it seems like a good thing to do.&lt;/p&gt;

&lt;p&gt;I agree that doing this on the server might be better. This does mean it is possible that the server will attempt to close the socket while the client is attempting to send something. But if the timeout is 10 mins, it is unlikely that this will happen often (i.e. if nothing was sent in the last 10 mins, it will not likely happen in the 0.5 ms it takes to do the close). The advantage of doing it on the server is that it will work for all clients.&lt;/p&gt;

&lt;p&gt;This change would be in core/.../kafka/network/SocketServer.scala.&lt;/p&gt;

&lt;p&gt;The only gotcha is that we likely need to avoid iterating over all connections to avoid latency impact (there could be 100k connections). One way to do this would be to use java.util.LinkedHashMap to implement an LRU hash map of the SelectionKeys, and access this every time the selection key comes up in a select operation. (There are a ton of details in LinkedHashMap--needs to be &quot;access order&quot;, etc). Then every 5-10 select loop iterations we would iterate the map expiring connections until we come to a connection that doesn&apos;t need expiring, then stop.&lt;/p&gt;</comment>
                            <comment id="14066481" author="nmarasoiu" created="Fri, 18 Jul 2014 16:15:58 +0000"  >&lt;p&gt;Beautiful, I can&apos;t wait to work this out, so I take this to code right?&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14071718" author="nmarasoiu" created="Wed, 23 Jul 2014 13:49:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; You agree with the approach, do you?&lt;/p&gt;</comment>
                            <comment id="14071803" author="junrao" created="Wed, 23 Jul 2014 14:48:12 +0000"  >&lt;p&gt;Yes. Thanks for picking it up.&lt;/p&gt;</comment>
                            <comment id="14087676" author="nehanarkhede" created="Wed, 6 Aug 2014 13:46:25 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoiu&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoiu&quot;&gt;nmarasoiu&lt;/a&gt;, are you actively working on this patch yet? If not, do you mind if we have someone else pick it up?&lt;/p&gt;</comment>
                            <comment id="14090832" author="nmarasoi" created="Fri, 8 Aug 2014 14:52:46 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I will spend up to 4 hours per day the next week (11-15 august), when I have this time.&lt;br/&gt;
So I would like to keep this nice task.&lt;br/&gt;
My estimate, I will have a first working solution to put up for review in ~3 days, so Thursday.&lt;/p&gt;

&lt;p&gt;Does that sound good?&lt;/p&gt;</comment>
                            <comment id="14094686" author="nmarasoi" created="Tue, 12 Aug 2014 21:11:40 +0000"  >&lt;p&gt;I attached a first version of the patch.&lt;br/&gt;
I am still thinking on any other implications, but wanted to share a first draft to collect some feedback already.&lt;br/&gt;
Thanks&lt;/p&gt;</comment>
                            <comment id="14097012" author="nehanarkhede" created="Thu, 14 Aug 2014 14:33:24 +0000"  >&lt;p&gt;Thanks for picking this up &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt;. Assigning to myself for review. &lt;/p&gt;</comment>
                            <comment id="14097029" author="nehanarkhede" created="Thu, 14 Aug 2014 14:49:16 +0000"  >&lt;p&gt;Took a look at the patch. How about the following -&lt;br/&gt;
1. Limit the LRU cache size to the number of active connections that should be supported by the Kafka server. I&apos;m guessing this should be a config. &lt;br/&gt;
2. Override removeEldestEntry to evict the oldest entry if the cache size exceeds the configured number of LRU connections.&lt;/p&gt;

&lt;p&gt;That way, we don&apos;t have to traverse the map several times in the main loop, which can be expensive.&lt;/p&gt;</comment>
                            <comment id="14097047" author="nmarasoi" created="Thu, 14 Aug 2014 15:05:42 +0000"  >&lt;p&gt;Traversing is quite cheap (it is traversing a linked list underneath, and only a prefix of it) and can be done every 1000 selects.&lt;br/&gt;
The intent of your suggestion is to optimize, I understand, but the effects is a different behavior as I feel it (changes the expiration by time and switches it to an expiration by connection count), and to a low performance benefit (I think traversing is much cheaper than blocking close on each channel, that would happen either way).&lt;br/&gt;
The idea of limited connection count can be used complementary to the existing traversing, but if you mean to take out the traversing every n selects, that changes the expiration by time and switches it to an expiration by connection count - is it an agreed requirements change with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;? I must warn that it is dangerous in my view to configure a maximum connection count per broker, because in event many brokers go down, and many clients need to use the system, this connection thrashing would not help anybody, and be a worse effect than not having this connection expiration at all, in such a scenario, relevant to a highly available system.&lt;/p&gt;</comment>
                            <comment id="14097380" author="nmarasoi" created="Thu, 14 Aug 2014 18:56:54 +0000"  >&lt;p&gt;To make the ~O(1) cost of &quot;traversing&quot; more clear, typically only the first element in the linked list is accessed, and it will typically be used in the last 10 minutes, and in this case nothing happens anymore. Of course, this is if the low volume topics do not generate many connections, which they won&apos;t, with this cleaning up in place. And I am checking now that map() and the rest are lazy, or else for sure I can make so that only the relevant &quot;prefix/first&quot; part of the collection is iterated, typically first element only.&lt;/p&gt;</comment>
                            <comment id="14097971" author="nehanarkhede" created="Fri, 15 Aug 2014 00:31:36 +0000"  >&lt;p&gt;My suggestion was not just to address the performance concern which is somewhat of an issue nevertheless. The motivation was that there is an upper bound on the number of open connections you can support on the broker. That number is the # of open file handles configured on the box. Since that number is known anyway, you probably would want to configure your server so that the connections never exceed a certain percentage of that upper limit. Currently, if the server runs out of open file handles, it effectively stays alive, but is unable to serve any data and becomes a &apos;zombie&apos;. &lt;/p&gt;

&lt;p&gt;But a downside of the expiration based on the connection count is that it doesn&apos;t necessarily achieve the goal of expiring really old connections. Instead it tries to solve the problem of preventing the broker from running out of available file handles, in which case we probably need a fairer strategy for expiring connections. &lt;/p&gt;

&lt;p&gt;Thinking more, I think it might be sufficient to override removeEldestEntry and check if the oldest entry is older than the threshold and let the map remove it. If the oldest entry is not above the threshold, traversing the map doesn&apos;t buy you anything. The downside is that if no new activity takes place on any of the connections all of a sudden, the server wouldn&apos;t proactively drop all connections, which is less of a concern. &lt;/p&gt;

&lt;p&gt;The advantage is that you will still get the same benefit of expiring older connections and it removes the need to traverse.&lt;/p&gt;</comment>
                            <comment id="14098324" author="nmarasoiu" created="Fri, 15 Aug 2014 08:08:50 +0000"  >&lt;p&gt;Hi, I am sorry, but traversing will be limited to the connections that will actually be expired, so there is no traversing of non-expiring connections (please see the detailed example below). &lt;/p&gt;

&lt;p&gt;I do agree on the other hand that there will be a polling on the first entry until it expires, but this is how we can implement the requirement exactly as intended (expiration taking into account just time as per stated &quot;stale connections&quot; issue, not connection count or activity as well), and it can be done every 1000 selects. &lt;/p&gt;

&lt;p&gt;If we want to protect brokers from becoming zombies, this is a different concern I feel. However, I completely agree that we can do the LRU limiting as well to avoid zombeing (as part of this jira or another one). Both mechanisms to expire can be at work and solve both problems with no overhead in doing so (there would just be 2 contexts in which an evict+close would be performed, if we do not count the evict done in a normal close call).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt;, what do you think?&lt;/p&gt;

&lt;p&gt;Say the server hold 100K connections. Say 100 connections are not used in the last 10 minutes.&lt;/p&gt;

&lt;p&gt;What the program does (or I will make sure it does) is just iterate through the first 101 connections, the first 100 will be expired and it will stop at number 101.&lt;br/&gt;
I think this is an exact achievement of expected behavior of the jira task, as intended, and there is no performance penalty to that really!&lt;/p&gt;

&lt;p&gt;I will rewrite with a loop /(tail-)recursive function, to check the first entry, and if stale call close (which also does a remove on the map anyways), and retry the next entry. This would be to avoid copying of the first 100 selectionKeys as well as to avoid any overhead/eagerness in map function.&lt;/p&gt;</comment>
                            <comment id="14105606" author="nmarasoi" created="Thu, 21 Aug 2014 17:09:58 +0000"  >&lt;p&gt;After discussion with Neha, we agreed that using the removeEldestEntry approach works better in the sense that avoids disruption caused by potentially many connections being up for close at once, and evens out that overhead. The disadvantage remains that an inactive server will not close connections but seems less than the advantage of closing overhead leveling and of performance plus of not traversing and of not polling the oldest entry.&lt;/p&gt;</comment>
                            <comment id="14108716" author="junrao" created="Mon, 25 Aug 2014 04:24:45 +0000"  >&lt;p&gt;Thanks for the patch. Looks good to me overall. Some minor comments below.&lt;/p&gt;

&lt;p&gt;1. Could we make connectionsLruTimeout a broker side configuration?&lt;/p&gt;

&lt;p&gt;2. Do we need to insert the key to lruConnections in write()? It seems to me doing that in read() (for incoming requests) is enough.&lt;/p&gt;

&lt;p&gt;3. The patch doesn&apos;t seem to apply for me. Could you rebase?&lt;/p&gt;

&lt;p&gt;git  apply -p0 ~/Downloads/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1282&quot; title=&quot;Disconnect idle socket connection in Selector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1282&quot;&gt;&lt;del&gt;KAFKA-1282&lt;/del&gt;&lt;/a&gt;_Disconnect_idle_socket_connection_in_Selector.patch &lt;br/&gt;
/Users/jrao/Downloads/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1282&quot; title=&quot;Disconnect idle socket connection in Selector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1282&quot;&gt;&lt;del&gt;KAFKA-1282&lt;/del&gt;&lt;/a&gt;_Disconnect_idle_socket_connection_in_Selector.patch:13: trailing whitespace.&lt;br/&gt;
import java.util&lt;br/&gt;
/Users/jrao/Downloads/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1282&quot; title=&quot;Disconnect idle socket connection in Selector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1282&quot;&gt;&lt;del&gt;KAFKA-1282&lt;/del&gt;&lt;/a&gt;_Disconnect_idle_socket_connection_in_Selector.patch:21: trailing whitespace.&lt;br/&gt;
import java.util.Map.Entry&lt;br/&gt;
/Users/jrao/Downloads/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1282&quot; title=&quot;Disconnect idle socket connection in Selector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1282&quot;&gt;&lt;del&gt;KAFKA-1282&lt;/del&gt;&lt;/a&gt;_Disconnect_idle_socket_connection_in_Selector.patch:30: trailing whitespace.&lt;br/&gt;
  private val connectionsLruTimeout: Long = TimeUnit.MINUTES.toNanos(10)&lt;br/&gt;
/Users/jrao/Downloads/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1282&quot; title=&quot;Disconnect idle socket connection in Selector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1282&quot;&gt;&lt;del&gt;KAFKA-1282&lt;/del&gt;&lt;/a&gt;_Disconnect_idle_socket_connection_in_Selector.patch:31: trailing whitespace.&lt;br/&gt;
  private var currentTime: Long = SystemTime.nanoseconds&lt;br/&gt;
/Users/jrao/Downloads/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1282&quot; title=&quot;Disconnect idle socket connection in Selector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1282&quot;&gt;&lt;del&gt;KAFKA-1282&lt;/del&gt;&lt;/a&gt;_Disconnect_idle_socket_connection_in_Selector.patch:32: trailing whitespace.&lt;br/&gt;
  private val lruConnections = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;SelectionKey, Long&amp;#93;&lt;/span&gt;(100, .75F, true) {&lt;br/&gt;
error: patch failed: core/src/main/scala/kafka/network/SocketServer.scala:16&lt;br/&gt;
error: core/src/main/scala/kafka/network/SocketServer.scala: patch does not apply&lt;/p&gt;</comment>
                            <comment id="14110261" author="nmarasoi" created="Tue, 26 Aug 2014 04:35:54 +0000"  >&lt;p&gt;Hi, Thank you, for 2. I agree for producers but I am not sure if the same SocketServer is used to serve consumers as well, and in this case, for consumers, the read/write ratio may be well in favor of writes making it risky perhaps to account just the reads?&lt;/p&gt;</comment>
                            <comment id="14110753" author="junrao" created="Tue, 26 Aug 2014 14:30:19 +0000"  >&lt;p&gt;Nicu,&lt;/p&gt;

&lt;p&gt;Similar to producers, consumers just issue fetch requests. The SocketServer first reads the fetch request from the network and then writes the fetch response to the network once the fetch request is served by the broker. So, there is a 1-to-1 mapping btw reads and writes and writes typically happen within a second after the reads. &lt;/p&gt;</comment>
                            <comment id="14112679" author="nmarasoi" created="Wed, 27 Aug 2014 19:20:48 +0000"  >&lt;p&gt;uploaded with parametrization and no more access-touch from write&lt;/p&gt;</comment>
                            <comment id="14114248" author="nmarasoi" created="Thu, 28 Aug 2014 19:59:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nehanarkhede&quot; class=&quot;user-hover&quot; rel=&quot;nehanarkhede&quot;&gt;nehanarkhede&lt;/a&gt; Hi, I implemented our discussion and applied Jun Rao suggestions, can you check and perhaps commit it if looks good? Hope for more tasks like this, do you have any suggestions?&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14114390" author="junrao" created="Thu, 28 Aug 2014 21:36:27 +0000"  >&lt;p&gt;Thanks for the patch. The following should be * 1000000, right?&lt;/p&gt;

&lt;p&gt;  private val connectionsLruTimeout: Long = connectionsMaxIdleMs * 1000&lt;/p&gt;</comment>
                            <comment id="14115623" author="nmarasoi" created="Fri, 29 Aug 2014 18:26:46 +0000"  >&lt;p&gt;Patch updated. Configurable max idleness of a connection since the last read on it. On creating new N connections, the server will be Closing at most N idle connections too, if they are idle for more than the mentioned threshold, default 10 minutes.&lt;/p&gt;</comment>
                            <comment id="14117621" author="junrao" created="Mon, 1 Sep 2014 18:24:48 +0000"  >&lt;p&gt;Looking at the patch again, in removeEldestEntry(), shouldn&apos;t we close the socket for eldest if the entry is to be removed? Right now, it seems that we only remove the entry from LRU w/o actually closing the idle socket connection.&lt;/p&gt;</comment>
                            <comment id="14117638" author="nmarasoi" created="Mon, 1 Sep 2014 18:56:57 +0000"  >&lt;p&gt;I am sorry, Yes, that was the intent! I will write unit tests from now on to avoid such slips.&lt;/p&gt;

&lt;p&gt;Moreover, the removeEldestEntry will return false all the time, because it keeps the responsability of mutating the map for itself, as part of calling the close method. &lt;/p&gt;

&lt;p&gt;Attached the patch, tests pass.&lt;/p&gt;</comment>
                            <comment id="14118317" author="junrao" created="Tue, 2 Sep 2014 16:30:34 +0000"  >&lt;p&gt;Thanks for the latest patch. I was trying to do some local testing. The following are my observations.&lt;/p&gt;

&lt;p&gt;1. I first started a local ZK and broker (setting connections.max.idle.ms 10secs). I then started a console-producer and a console-consumer. Then, I typed in sth in console-producer every 15 secs. However, I don&apos;t see the producer connection gets killed. I added sth instrumentation. It doesn&apos;t seem that removeEldestEntry() is called on every fetch request.&lt;/p&gt;

&lt;p&gt;2. As I was debugging this, I realized that it&apos;s kind of weird to kill idle connections only when there is another non-idle connection. This makes debugging harder since one can&apos;t just test this out with a single connection. It&apos;s much simpler to understand if the idle connection can just be killed after the connection idle time, independent of other connections to the broker. To address the concern of closing many sockets in one iteration of the selector, we can calculate the time that a socket entry is expected to be killed (this is the access time of the oldest entry + maxIdleTime, or maxIdleTime if no entry exists). When that time comes during the iteration of the selector, we can just check the oldest entry and see if it needs to be closed.&lt;/p&gt;

&lt;p&gt;3. It would be good to check if our clients (especially the producer, both old and new) can handle a closed idle connection properly. For example, when detecting an already closed socket, the producer should be able to resend the message and therefore we shouldn&apos;t see any data loss.&lt;/p&gt;</comment>
                            <comment id="14122456" author="nmarasoi" created="Fri, 5 Sep 2014 05:25:35 +0000"  >&lt;p&gt;Hi, I am not completely sure I fully understood your solution in point 2: &lt;/p&gt;

&lt;p&gt;Do you mean to close at most one connection per iteration, right? This is ok, the worst case scenario is closing 100K old connections in 10 hours, one per select.&lt;/p&gt;

&lt;p&gt;On storing the time to close in a local variable, the access of the oldest entry every iteration is O(1) super cheap so I would skip this optimization. &lt;/p&gt;</comment>
                            <comment id="14125465" author="nmarasoi" created="Mon, 8 Sep 2014 12:47:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, hi, can you answer please? I agree with what you say if I understood all of it, I am doing a small patch right now&lt;/p&gt;</comment>
                            <comment id="14125466" author="nmarasoi" created="Mon, 8 Sep 2014 12:49:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nehanarkhede&quot; class=&quot;user-hover&quot; rel=&quot;nehanarkhede&quot;&gt;nehanarkhede&lt;/a&gt; Hi, can you also check the new idea? It is consistent with my initial approach and solves the potential overhead of closing too many connections on a single iteration.&lt;/p&gt;</comment>
                            <comment id="14133259" author="nehanarkhede" created="Sun, 14 Sep 2014 16:11:33 +0000"  >&lt;p&gt;Thanks for the patch, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt;! Looks good overall. Few review comments -&lt;/p&gt;

&lt;p&gt;1. Do we really need connectionsLruTimeout in addition to connectionsMaxIdleMs? It seems to me that we are translating the idle connection timeout plugged in by the user to 1000000x times more than what is configured. That&apos;s probably why Jun saw the behavior he reported earlier. &lt;br/&gt;
2. I don&apos;t really share Jun&apos;s concern in #2 and we can state that more clearly in the comment that describes the new config in KafkaConfig. Connections that are idle for more than connections.max.idle.ms &lt;b&gt;may&lt;/b&gt; get killed. I don&apos;t think the users particularly care about a hard guarantee of their connections getting killed here. So the simplicity of this approach is well justified.&lt;br/&gt;
3. I do think that adding a produce and fetch test where the connections get killed will be great &lt;/p&gt;</comment>
                            <comment id="14134947" author="junrao" created="Tue, 16 Sep 2014 04:45:24 +0000"  >&lt;p&gt;Nicu,&lt;/p&gt;

&lt;p&gt;On #2, I wasn&apos;t worried about any performance optimization. My concern is mostly on testing and ease of understanding. Since removeEldestEntry is only called on update, you can&apos;t test the logic on a single connection to the broker. It&apos;s a bit weird that if there is only a single idle connection, that connection is never killed. But as soon as a second connection is added, the idle connection will be killed. For the user&apos;s perspective, it&apos;s simpler to understand how idle connections are killed if they are not tied to # of connection.&lt;/p&gt;

&lt;p&gt;Also, could you explain how you fixed #1 in the latest patch? It wasn&apos;t obvious to me.&lt;/p&gt;</comment>
                            <comment id="14135561" author="nmarasoiu" created="Tue, 16 Sep 2014 14:53:32 +0000"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I have understood what you say and I agree it is highly unintuitive and we should change that. I just saw you propose a solution which included a precomputation of the time to close, and it was bit confusion, looked like an attempt of micro optimization.&lt;/p&gt;

&lt;p&gt;I have not made any patch yet, I waited for feedback from Neha too, but I will do the patch today: it looks ok to me the idea of closing at most one old connection per selector iteration.&lt;/p&gt;

&lt;p&gt;So the solution will look more like the previous patch, but instead of traversing n+1 entries to close n old connections, it will just pick the oldest and check if it is time to close.&lt;/p&gt;

&lt;p&gt;For #1, the way Neha and me discussed, and the way you understood it works (for the latest patch), is that an old connection is taken into consideration for close only when a new connection is being opened up (or activity exists on an existing connection too). But this will no longer be the case.&lt;/p&gt;</comment>
                            <comment id="14136098" author="nmarasoi" created="Tue, 16 Sep 2014 19:56:29 +0000"  >&lt;p&gt;Attached patch: every select iteration, zero or one connections are closed for being idle for too long.&lt;br/&gt;
The units pass well, but&lt;br/&gt;
For the moment I am blocked by:&lt;br/&gt;
./kafka-console-producer.sh&lt;br/&gt;
Error: Could not find or load main class kafka.tools.ConsoleProducer&lt;/p&gt;</comment>
                            <comment id="14136767" author="junrao" created="Wed, 17 Sep 2014 04:36:23 +0000"  >&lt;p&gt;Did you do &quot;./gradlew jar&quot; first?&lt;/p&gt;</comment>
                            <comment id="14137011" author="nmarasoi" created="Wed, 17 Sep 2014 09:47:28 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Unfortunately the client used in console-producer is not very robust with respect to disconnections, as will detail below. Is this the &quot;old&quot; scala producer, and can we hope for a resilient behaviour that I can test with the new java producer?&lt;/p&gt;

&lt;p&gt;More specifically, the connection is closed from the broker side, but the producer is unaware of this. The first message after the close is lost (and is not retried later). The second message sees the broken channel, outputs the exception below, and reconnects and is succesfully retried, I can see it consumed.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-17 12:44:12,009&amp;#93;&lt;/span&gt; WARN Failed to send producer request with correlation id 15 to broker 0 with data for partitions &lt;span class=&quot;error&quot;&gt;&amp;#91;topi,0&amp;#93;&lt;/span&gt; (kafka.producer.async.DefaultEventHandler)&lt;br/&gt;
java.io.IOException: Broken pipe&lt;br/&gt;
	at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)&lt;br/&gt;
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)&lt;br/&gt;
	at sun.nio.ch.IOUtil.write(IOUtil.java:149)&lt;br/&gt;
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:483)&lt;br/&gt;
	at java.nio.channels.SocketChannel.write(SocketChannel.java:493)&lt;br/&gt;
	at kafka.network.BoundedByteBufferSend.writeTo(BoundedByteBufferSend.scala:56)&lt;br/&gt;
	at kafka.network.Send$class.writeCompletely(Transmission.scala:75)&lt;br/&gt;
	at kafka.network.BoundedByteBufferSend.writeCompletely(BoundedByteBufferSend.scala:26)&lt;br/&gt;
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:100)&lt;/p&gt;</comment>
                            <comment id="14137019" author="nmarasoi" created="Wed, 17 Sep 2014 10:00:13 +0000"  >&lt;p&gt;re-attached fixed patch, but we may have a blocker to the whole solution on the broker side, pls see comment above/below (first message after disconnect is lost on the client used in console-prod)&lt;/p&gt;</comment>
                            <comment id="14137118" author="nmarasoi" created="Wed, 17 Sep 2014 12:08:30 +0000"  >&lt;p&gt;here is a time line:&lt;/p&gt;

&lt;p&gt;he -&amp;gt; produced&lt;br/&gt;
he -&amp;gt; consumed&lt;br/&gt;
[ wait beyond timeout here, connection got closed underneath by the other side]&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-17 15:02:28,689&amp;#93;&lt;/span&gt; INFO Got user-level KeeperException when processing sessionid:0x148837ce1800001 type:setData cxid:0x24 zxid:0xec txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-87959/offsets/topi/0 Error:KeeperErrorCode = NoNode for /consumers/console-consumer-87959/offsets/topi/0 (org.apache.zookeeper.server.PrepRequestProcessor)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-17 15:02:28,691&amp;#93;&lt;/span&gt; INFO Got user-level KeeperException when processing sessionid:0x148837ce1800001 type:create cxid:0x25 zxid:0xed txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-87959/offsets Error:KeeperErrorCode = NoNode for /consumers/console-consumer-87959/offsets (org.apache.zookeeper.server.PrepRequestProcessor)&lt;br/&gt;
dddddddddddddd --&amp;gt; produce attempt (never retried, or never reached the broker or at least never reached the consumer)&lt;br/&gt;
[ many seconds wait, to see if the message is being retried, apparently not, even though the default retry is 3 times]&lt;br/&gt;
wwwwwwwwwwwwwwwww --&amp;gt; new attempt (immediattely I see the message below with the stack trace, and reconnect + retry is instantly sucesfull)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-17 15:03:12,599&amp;#93;&lt;/span&gt; WARN Failed to send producer request with correlation id 9 to broker 0 with data for partitions &lt;span class=&quot;error&quot;&gt;&amp;#91;topi,0&amp;#93;&lt;/span&gt; (kafka.producer.async.DefaultEventHandler)&lt;br/&gt;
java.io.IOException: Broken pipe&lt;br/&gt;
	at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)&lt;br/&gt;
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)&lt;br/&gt;
	at sun.nio.ch.IOUtil.write(IOUtil.java:149)&lt;br/&gt;
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:483)&lt;br/&gt;
	at java.nio.channels.SocketChannel.write(SocketChannel.java:493)&lt;br/&gt;
	at kafka.network.BoundedByteBufferSend.writeTo(BoundedByteBufferSend.scala:56)&lt;br/&gt;
	at kafka.network.Send$class.writeCompletely(Transmission.scala:75)&lt;br/&gt;
	at kafka.network.BoundedByteBufferSend.writeCompletely(BoundedByteBufferSend.scala:26)&lt;br/&gt;
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:100)&lt;br/&gt;
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:72)&lt;br/&gt;
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:71)&lt;br/&gt;
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:102)&lt;br/&gt;
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)&lt;br/&gt;
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)&lt;br/&gt;
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)&lt;br/&gt;
	at kafka.producer.SyncProducer$$anonfun$send$1.apply$mcV$sp(SyncProducer.scala:101)&lt;br/&gt;
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)&lt;br/&gt;
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)&lt;br/&gt;
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)&lt;br/&gt;
	at kafka.producer.SyncProducer.send(SyncProducer.scala:100)&lt;br/&gt;
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$send(DefaultEventHandler.scala:255)&lt;br/&gt;
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:106)&lt;br/&gt;
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:100)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:100)&lt;br/&gt;
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:72)&lt;br/&gt;
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)&lt;br/&gt;
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)&lt;br/&gt;
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)&lt;br/&gt;
	at scala.collection.immutable.Stream.foreach(Stream.scala:547)&lt;br/&gt;
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)&lt;br/&gt;
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-17 15:03:12,712&amp;#93;&lt;/span&gt; INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)&lt;br/&gt;
wwwwwwwwwwwwwwwww&lt;/p&gt;</comment>
                            <comment id="14137130" author="nmarasoiu" created="Wed, 17 Sep 2014 12:18:06 +0000"  >&lt;p&gt;in fact, this is something that needs fixing in the producer(s) anyway, but the issue is with the currently deployed producers.&lt;br/&gt;
One of the main reasons to go with a broker side close of the idle connections was that it is easier to redeploy brokers then producers.&lt;br/&gt;
But if this is indeed a bug in the producer(s) as I reproduced, those producers would need redeploy.&lt;br/&gt;
So moving this to the producer side as a configuration may again be an option on the table.&lt;/p&gt;</comment>
                            <comment id="14137399" author="junrao" created="Wed, 17 Sep 2014 15:36:07 +0000"  >&lt;p&gt;Interesting. The data loss may have to do with ack=0, which is the default in console producer. Could you try ack=1?&lt;/p&gt;</comment>
                            <comment id="14137796" author="nmarasoi" created="Wed, 17 Sep 2014 19:33:38 +0000"  >&lt;p&gt;Indeed, ack=1 solves it for most times but not for all:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;in 6 of 7 tests it gets a reset by peer and a socket timeout on fetch meta, than re connects and sends message.&lt;/li&gt;
	&lt;li&gt;in one test, after leaving one night the laptop, I entered:&lt;br/&gt;
sdfgsdfgdsfg --&amp;gt; that never returned, no exception, nothing at all reported&lt;br/&gt;
aaaaaaaaaaa&lt;br/&gt;
aaaaaaaaaaa&lt;br/&gt;
ff&lt;br/&gt;
ff&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The &quot;ok&quot; flow, which reproduces most of the time with ack=1 is (sometimes with just one of the 2 expcetions):&lt;br/&gt;
gffhgfhgfjfgjhfhjfgjhf&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-18 08:22:35,057&amp;#93;&lt;/span&gt; WARN Failed to send producer request with correlation id 43 to broker 0 with data for partitions &lt;span class=&quot;error&quot;&gt;&amp;#91;topi,0&amp;#93;&lt;/span&gt; (kafka.producer.async.DefaultEventHandler)&lt;br/&gt;
java.io.IOException: Connection reset by peer&lt;br/&gt;
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)&lt;br/&gt;
..&lt;br/&gt;
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-18 08:22:36,663&amp;#93;&lt;/span&gt; WARN Fetching topic metadata with correlation id 44 for topics &lt;span class=&quot;error&quot;&gt;&amp;#91;Set(topi)&amp;#93;&lt;/span&gt; from broker &lt;span class=&quot;error&quot;&gt;&amp;#91;id:0,host:localhost,port:9092&amp;#93;&lt;/span&gt; failed (kafka.client.ClientUtils$)&lt;br/&gt;
java.net.SocketTimeoutException&lt;br/&gt;
	at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:226)&lt;br/&gt;
..&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2014-09-18 08:22:36,664&amp;#93;&lt;/span&gt; ERROR fetching topic metadata for topics &lt;span class=&quot;error&quot;&gt;&amp;#91;Set(topi)&amp;#93;&lt;/span&gt; from broker &lt;span class=&quot;error&quot;&gt;&amp;#91;ArrayBuffer(id:0,host:localhost,port:9092)&amp;#93;&lt;/span&gt; failed (kafka.utils.Utils$)&lt;br/&gt;
kafka.common.KafkaException: fetching topic metadata for topics &lt;span class=&quot;error&quot;&gt;&amp;#91;Set(topi)&amp;#93;&lt;/span&gt; from broker &lt;span class=&quot;error&quot;&gt;&amp;#91;ArrayBuffer(id:0,host:localhost,port:9092)&amp;#93;&lt;/span&gt; failed&lt;br/&gt;
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:71)&lt;br/&gt;
..&lt;br/&gt;
Caused by: java.net.SocketTimeoutException&lt;br/&gt;
	at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:226)&lt;br/&gt;
	.. kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)&lt;br/&gt;
	at kafka.network.BlockingChannel.receive(BlockingChannel.scala:108)&lt;br/&gt;
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:74)&lt;br/&gt;
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:71)&lt;br/&gt;
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)&lt;br/&gt;
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:57)&lt;br/&gt;
	... 12 more&lt;br/&gt;
gffhgfhgfjfgjhfhjfgjhf&lt;/p&gt;</comment>
                            <comment id="14138972" author="nehanarkhede" created="Thu, 18 Sep 2014 14:02:09 +0000"  >&lt;p&gt;Thanks for the updated patch. Overall, looks great. Few comments -&lt;br/&gt;
1. Can you rename initialNextIdleCloseCheckTimeValue to nextIdleCloseCheckTimeValue?&lt;br/&gt;
2. It will be easier to understand the code if we rename currentTime to currentTimeNanos.&lt;/p&gt;</comment>
                            <comment id="14139356" author="nmarasoi" created="Thu, 18 Sep 2014 19:13:55 +0000"  >&lt;p&gt;attached, renamed time and for the &quot;initial/reset value of the nextIdleCheck&quot;, i just inlined the function, the code is more clear like this i think&lt;/p&gt;</comment>
                            <comment id="14139513" author="junrao" created="Thu, 18 Sep 2014 21:04:35 +0000"  >&lt;p&gt;Do you think you can reproduce that data loss issue in 1 out of your 7 tests? With ack=1 and retries, this shouldn&apos;t happen. Perhaps it&apos;s useful to enable the trace logging in the producer to see what&apos;s exactly happening there.&lt;/p&gt;

&lt;p&gt;Could you also do the same test by enabling the new producer in console producer?&lt;/p&gt;</comment>
                            <comment id="14142174" author="nehanarkhede" created="Sat, 20 Sep 2014 19:48:56 +0000"  >&lt;p&gt;+1 on your latest patch. I&apos;m leaning towards accepting the patch since the test above points to an issue that seems unrelated to the patch. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt;, it will be great if you can follow Jun&apos;s suggestion to reproduce the issue. Then file a JIRA to track it. I&apos;m guessing killing idle connections shouldn&apos;t lead to data loss.&lt;/p&gt;</comment>
                            <comment id="14142199" author="nehanarkhede" created="Sat, 20 Sep 2014 20:52:08 +0000"  >&lt;p&gt;Pushed the latest patch to trunk.&lt;/p&gt;</comment>
                            <comment id="14201475" author="junrao" created="Fri, 7 Nov 2014 02:37:26 +0000"  >&lt;p&gt;Nicu,&lt;/p&gt;

&lt;p&gt;I was doing some manual testing of this feature. What I observed is that sometimes, the idle connections are not closed. The following was what I did.&lt;/p&gt;

&lt;p&gt;1. Configure a small connections.max.idle.ms = 10000.&lt;br/&gt;
2. start ZK and Kafka broker&lt;br/&gt;
3. start a console consumer&lt;br/&gt;
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic topic1 --from-beginning&lt;br/&gt;
4. start a console producer and type in sth every 15 secs or so. &lt;br/&gt;
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topic1 --request-required-acks 1&lt;/p&gt;

&lt;p&gt;What I observed was that initially, the producer connections kept getting killed by the broker correctly after being idle for 10 secs. The next producer send would hit an IOException and trigger a resend. However, after typing in 10 or so messages, at some point, no idle connections were killed by the broker any more and the producer send always succeeded.&lt;/p&gt;</comment>
                            <comment id="14203456" author="nmarasoi" created="Sat, 8 Nov 2014 14:54:47 +0000"  >&lt;p&gt;Indeed, I can reproduce this. I did saw an instance where no exception was thrown by the producer but still the broker mentioned new connection being listened to suggesting close took place. However, checking with required-acks 0 I can see that after some time the connection does not close anymore.&lt;/p&gt;</comment>
                            <comment id="14203494" author="nmarasoi" created="Sat, 8 Nov 2014 16:41:05 +0000"  >&lt;p&gt;Fixed it - I have mistakenly deleted at some point the fact that the linked hash map needs to be in access order &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;br/&gt;
I tested with your scenario and looks ok now.&lt;/p&gt;</comment>
                            <comment id="14204246" author="nehanarkhede" created="Mon, 10 Nov 2014 02:15:13 +0000"  >&lt;p&gt;good catch &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt;. +1 on your change&lt;/p&gt;</comment>
                            <comment id="14205156" author="junrao" created="Mon, 10 Nov 2014 18:56:38 +0000"  >&lt;p&gt;Nicu,&lt;/p&gt;

&lt;p&gt;Thanks for the patch. Do you think it&apos;s easy to add a unit test on Processor?&lt;/p&gt;</comment>
                            <comment id="14208810" author="nmarasoi" created="Wed, 12 Nov 2014 22:25:58 +0000"  >&lt;p&gt;I want, yes, I will add a few tests this week.&lt;/p&gt;</comment>
                            <comment id="14216827" author="jjkoshy" created="Tue, 18 Nov 2014 21:36:00 +0000"  >&lt;p&gt;This is already in 0.8.2 so we should incorporate the follow-ups there as well I think.&lt;/p&gt;</comment>
                            <comment id="14260486" author="nehanarkhede" created="Mon, 29 Dec 2014 22:21:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; This is marked for 0.8.2. Is anyone working or planning to work on this?&lt;/p&gt;</comment>
                            <comment id="14260516" author="nmarasoiu" created="Mon, 29 Dec 2014 22:59:40 +0000"  >&lt;p&gt;I will do unit tests tommorow / day after. The fix should be ok otherwise,&lt;br/&gt;
and ready to be pushed on trunk and 0.8.2. I will announce when done with&lt;br/&gt;
units.&lt;/p&gt;

&lt;p&gt;On Tue, Dec 30, 2014 at 12:21 AM, Neha Narkhede (JIRA) &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="14567322" author="nmarasoi" created="Mon, 1 Jun 2015 13:57:00 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nehanarkhede&quot; class=&quot;user-hover&quot; rel=&quot;nehanarkhede&quot;&gt;nehanarkhede&lt;/a&gt;, I added a test, please review. The patch has 2 variations (latest 2 patches), explained at point 2 below, while the latest implements 1&apos; below.&lt;/p&gt;

&lt;p&gt;1. I wanted to sleep on MockTime, but here we actually need to physically wait at leat one epoll/select cycle. Since I have put 10ms idle time &amp;amp; it works, mocked time would not bring benefits, i.e. only the select time needs to be waited over. &lt;/p&gt;

&lt;p&gt;1&apos;. Because of potentially large &amp;amp; not deterministically bounded select times, I implemented a mechanism to try a few times, waiting 50% more time every time.&lt;/p&gt;

&lt;p&gt;2. Seems to work with low (10ms) idle timeout for all current test methods. However, I attach a patch with separate test class for this (and yet another utils class for reuse), to isolate configuration between group of test methods.&lt;/p&gt;

&lt;p&gt;3. Shall I do a multiple connections test?&lt;/p&gt;</comment>
                            <comment id="14568257" author="junrao" created="Mon, 1 Jun 2015 23:59:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt;, thanks for the patch. We are changing SocketServer to reuse Selector right now in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1928&quot; title=&quot;Move kafka.network over to using the network classes in org.apache.kafka.common.network&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1928&quot;&gt;&lt;del&gt;KAFKA-1928&lt;/del&gt;&lt;/a&gt;. Once that&apos;s done, the idle connection logic will be moved into Selector and should be easier to test since Selector supports mock time. That patch is almost ready. Perhaps you can wait until it&apos;s committed and submit a new patch. &lt;/p&gt;</comment>
                            <comment id="14705068" author="nmarasoi" created="Thu, 20 Aug 2015 15:00:04 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I noticed that the dependencies are done and I will resume this task.&lt;br/&gt;
The task contributions had been:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;a fix&lt;/li&gt;
	&lt;li&gt;unit test(s)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As far as the fix is concerned, I noticed that it is already fixed in the current Selector, namely the lruConnections is a LinkedHashMap with accessOrder=true. This was the only fix needed, and I am 100% convinced that the fix is already done.&lt;/p&gt;

&lt;p&gt;I already have a unit test too, I will try to put a patch here this week.&lt;/p&gt;

&lt;p&gt;Just wanted to mention that the old connections should be closed by the kafka installations using the new reusable network code.&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Nicu&lt;/p&gt;</comment>
                            <comment id="14960025" author="junrao" created="Fri, 16 Oct 2015 01:54:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nmarasoi&quot; class=&quot;user-hover&quot; rel=&quot;nmarasoi&quot;&gt;nmarasoi&lt;/a&gt;, yes, the actual problem is now fixed in trunk. We just need to add a unit test. I created a followup jira &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2661&quot; title=&quot;Add a unit test for disconnecting idle socket connections &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2661&quot;&gt;&lt;del&gt;KAFKA-2661&lt;/del&gt;&lt;/a&gt; for that. Resolving this jira.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12773976">KAFKA-1941</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12773261">KAFKA-1928</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12736556" name="1282_access-order_+_test_(same_class).patch" size="4671" author="nmarasoi" created="Mon, 1 Jun 2015 13:57:00 +0000"/>
                            <attachment id="12663665" name="KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch" size="2903" author="nmarasoiu" created="Fri, 22 Aug 2014 14:48:46 +0000"/>
                            <attachment id="12736558" name="access-order_+_test.patch" size="10075" author="nmarasoi" created="Mon, 1 Jun 2015 14:01:58 +0000"/>
                            <attachment id="12736567" name="access_order_+_test_waiting_from_350ms_to_1100ms.patch" size="4724" author="nmarasoi" created="Mon, 1 Jun 2015 14:24:01 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>375740</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 5 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1sqxz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>376036</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>nehanarkhede</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>