<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:08:38 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6052] Windows: Consumers not polling when isolation.level=read_committed </title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6052</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;&lt;b&gt;The same code is running fine in Linux.&lt;/b&gt; I am trying to send a transactional record with exactly once schematics. These are my producer, consumer and broker setups. &lt;br/&gt;
public void sendWithTTemp(String topic, EHEvent event) {&lt;br/&gt;
    Properties props = new Properties();&lt;br/&gt;
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,&lt;br/&gt;
              &quot;localhost:9092,localhost:9093,localhost:9094&quot;);&lt;br/&gt;
//    props.put(&quot;bootstrap.servers&quot;, &quot;34.240.248.190:9092,52.50.95.30:9092,52.50.95.30:9092&quot;);&lt;br/&gt;
    props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);&lt;br/&gt;
    props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);&lt;br/&gt;
    props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, &quot;1&quot;);&lt;br/&gt;
    props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);&lt;br/&gt;
    props.put(ProducerConfig.LINGER_MS_CONFIG, 1);&lt;br/&gt;
    props.put(&quot;transactional.id&quot;, &quot;TID&quot; + transactionId.incrementAndGet());&lt;br/&gt;
    props.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, &quot;5000&quot;);&lt;/p&gt;

&lt;p&gt;    Producer&amp;lt;String, String&amp;gt; producer =&lt;br/&gt;
        new KafkaProducer&amp;lt;&amp;gt;(props,&lt;br/&gt;
                            new StringSerializer(),&lt;br/&gt;
                            new StringSerializer());&lt;/p&gt;

&lt;p&gt;    Logger.log(this, &quot;Initializing transaction...&quot;);&lt;/p&gt;

&lt;p&gt;    producer.initTransactions();&lt;/p&gt;

&lt;p&gt;    Logger.log(this, &quot;Initializing done.&quot;);&lt;/p&gt;

&lt;p&gt;    try &lt;/p&gt;
{
      Logger.log(this, &quot;Begin transaction...&quot;);
      producer.beginTransaction();
      Logger.log(this, &quot;Begin transaction done.&quot;);
      Logger.log(this, &quot;Sending events...&quot;);
      producer.send(new ProducerRecord&amp;lt;&amp;gt;(topic,
                                         event.getKey().toString(),
                                         event.getValue().toString()));
      Logger.log(this, &quot;Sending events done.&quot;);
      Logger.log(this, &quot;Committing...&quot;);
      producer.commitTransaction();
      Logger.log(this, &quot;Committing done.&quot;);
    }
&lt;p&gt; catch (ProducerFencedException | OutOfOrderSequenceException&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; AuthorizationException e) 
{
      producer.close();
      e.printStackTrace();
    }
&lt;p&gt; catch (KafkaException e) &lt;/p&gt;
{
      producer.abortTransaction();
      e.printStackTrace();
    }&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;    producer.close();&lt;br/&gt;
  }&lt;/p&gt;

&lt;p&gt;&lt;b&gt;In Consumer&lt;/b&gt;&lt;br/&gt;
I have set isolation.level=read_committed&lt;br/&gt;
&lt;b&gt;In 3 Brokers&lt;/b&gt;&lt;br/&gt;
I&apos;m running with the following properties&lt;br/&gt;
      Properties props = new Properties();&lt;br/&gt;
      props.setProperty(&quot;broker.id&quot;, &quot;&quot; + i);&lt;br/&gt;
      props.setProperty(&quot;listeners&quot;, &quot;PLAINTEXT://:909&quot; + (2 + i));&lt;br/&gt;
      props.setProperty(&quot;log.dirs&quot;, Configuration.KAFKA_DATA_PATH + &quot;&lt;br class=&quot;atl-forced-newline&quot; /&gt;B&quot; + i);&lt;br/&gt;
      props.setProperty(&quot;num.partitions&quot;, &quot;1&quot;);&lt;br/&gt;
      props.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);&lt;br/&gt;
      props.setProperty(&quot;zookeeper.connection.timeout.ms&quot;, &quot;6000&quot;);&lt;br/&gt;
      props.setProperty(&quot;min.insync.replicas&quot;, &quot;2&quot;);&lt;br/&gt;
      props.setProperty(&quot;offsets.topic.replication.factor&quot;, &quot;2&quot;);&lt;br/&gt;
      props.setProperty(&quot;offsets.topic.num.partitions&quot;, &quot;1&quot;);&lt;br/&gt;
      props.setProperty(&quot;transaction.state.log.num.partitions&quot;, &quot;2&quot;);&lt;br/&gt;
      props.setProperty(&quot;transaction.state.log.replication.factor&quot;, &quot;2&quot;);&lt;br/&gt;
      props.setProperty(&quot;transaction.state.log.min.isr&quot;, &quot;2&quot;);&lt;/p&gt;

&lt;p&gt;I am not getting any records in the consumer. When I set isolation.level=read_uncommitted, I get the records. I assume that the records are not getting commited. What could be the problem? log attached&lt;/p&gt;</description>
                <environment>Windows 10. All processes running in embedded mode.</environment>
        <key id="13108636">KAFKA-6052</key>
            <summary>Windows: Consumers not polling when isolation.level=read_committed </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vahid">Vahid Hashemian</assignee>
                                    <reporter username="zandegran">Ansel Zandegran</reporter>
                        <labels>
                            <label>transactions</label>
                            <label>windows</label>
                    </labels>
                <created>Wed, 11 Oct 2017 14:43:02 +0000</created>
                <updated>Wed, 25 Apr 2018 09:41:58 +0000</updated>
                            <resolved>Sat, 24 Mar 2018 21:15:07 +0000</resolved>
                                    <version>0.11.0.0</version>
                    <version>1.0.1</version>
                                    <fixVersion>1.1.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>consumer</component>
                    <component>producer </component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="16201845" author="ijuma" created="Thu, 12 Oct 2017 11:55:15 +0000"  >&lt;p&gt;Any ideas &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurva&quot; class=&quot;user-hover&quot; rel=&quot;apurva&quot;&gt;apurva&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="16202605" author="apurva" created="Thu, 12 Oct 2017 20:52:24 +0000"  >&lt;p&gt;Can you share the client logs (both producer and consumer) for Windows with TRACE level logging? In the windows case, the entire setup is running on Windows? or just the clients?&lt;/p&gt;

&lt;p&gt;If the broker is also running windows, can you share the actual data log files as well?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Apurva&lt;/p&gt;</comment>
                            <comment id="16203299" author="zandegran" created="Fri, 13 Oct 2017 09:33:13 +0000"  >&lt;p&gt;Yes, the entire setup is running in windows not just the clients. I have attached the new TRACE level log. The log includes producer and consumer logs along with 3 brokers and zookeeper host. I have also added the TRACE level log for just the producer and consumer. Data log files are also attached. Thanks. &lt;/p&gt;</comment>
                            <comment id="16204093" author="apurva" created="Fri, 13 Oct 2017 19:54:46 +0000"  >&lt;p&gt;Hi Ansel, &lt;/p&gt;

&lt;p&gt;the logs you shared don&apos;t correlate with each other. For instance, your `Producer_consumer.log` map transactionalId &quot;TID1&quot; to producerId 2000, as can be seen in these lines: &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2017-10-13 11:45:50 DEBUG Sender:347 - [TransactionalId TID1] Sending transactional request (type=InitProducerIdRequest, transactionalId=TID1, transactionTimeoutMs=5000) to node 192.168.56.1:9093 (id: 1 rack: null)
2017-10-13 11:45:50 TRACE NetworkClient:389 - Sending INIT_PRODUCER_ID {transactional_id=TID1,transaction_timeout_ms=5000} to node 1.
2017-10-13 11:45:50 TRACE NetworkClient:658 - Completed receive from node 1, for key 22, received {throttle_time_ms=0,error_code=0,producer_id=2000,producer_epoch=0}
2017-10-13 11:45:50 TRACE TransactionManager:645 - [TransactionalId TID1] Received transactional response InitProducerIdResponse(error=NONE, producerId=2000, producerEpoch=0, throttleTimeMs=0) for request (type=InitProducerIdRequest, transactionalId=TID1, transactionTimeoutMs=5000)
2017-10-13 11:45:50 INFO  TransactionManager:341 - [TransactionalId TID1] ProducerId set to 2000 with epoch 0
2017-10-13 11:45:50 DEBUG TransactionManager:512 - [TransactionalId TID1] Transition from state INITIALIZING to READY
2017-10-13 11:45:50 DEBUG TransactionManager:512 - [TransactionalId TID1] Transition from state READY to IN_TRANSACTION
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, the transaction log maps transactionalId TID1 to producerId 1000:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;offset: 0 position: 0 CreateTime: 1507893603185 isvalid: true keysize: 8 valuesize: 37 magic: 2 compresscodec: NONE producerId: -1 sequence: -1 isTransactional: false headerKeys: [] key: transactionalId=TID1 payload: producerId:1000,producerEpoch:0,state=Empty,partitions=Set(),txnLastUpdateTimestamp=1507893603185,txnTimeoutMs=5000
offset: 1 position: 113 CreateTime: 1507893603242 isvalid: true keysize: 8 valuesize: 57 magic: 2 compresscodec: NONE producerId: -1 sequence: -1 isTransactional: false headerKeys: [] key: transactionalId=TID1 payload: producerId:1000,producerEpoch:0,state=Ongoing,partitions=Set(Topic_AVRO-0),txnLastUpdateTimestamp=1507893603240,txnTimeoutMs=5000
offset: 2 position: 247 CreateTime: 1507893603317 isvalid: true keysize: 8 valuesize: 57 magic: 2 compresscodec: NONE producerId: -1 sequence: -1 isTransactional: false headerKeys: [] key: transactionalId=TID1 payload: producerId:1000,producerEpoch:0,state=PrepareCommit,partitions=Set(Topic_AVRO-0),txnLastUpdateTimestamp=1507893603315,txnTimeoutMs=5000
o
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is definitely curious that the transaction state in the transaction log is left at `PrepareCommit`. This would explain why read committed consumers don&apos;t return any data.&lt;/p&gt;

&lt;p&gt;However, your client logs don&apos;t match the server log, so they can&apos;t be used for debugging. Also, in the client logs you sent `TID1` successfully commits transaction by getting the proper response for the broker. If the client received the successful commit response, it is highly unlikely for the state not to change on the broker, especially since other state changes happened on the broker.&lt;/p&gt;

&lt;p&gt;Finally, it would be helpful to have each broker&apos;s server.log, and the producer, and the consumer log in separate files.&lt;/p&gt;

&lt;p&gt;thanks,&lt;br/&gt;
Apurva&lt;/p&gt;
</comment>
                            <comment id="16204571" author="zandegran" created="Sat, 14 Oct 2017 10:41:40 +0000"  >&lt;p&gt;Hi Apurva,&lt;br/&gt;
Thanks for the comments.&lt;br/&gt;
Yes they won&apos;t correlate as they are from different executions. Since you asked only the client logs on trace level, I added the others just in case. &lt;br/&gt;
Regarding the transaction ID, we tried different approaches including creating producers with different transaction ID&apos;s for each producer. Now, since we figured that the problem is with the environment, it doesn&apos;t make sense. So, I have cleaned up those codes and created &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12892211/12892211_Separate_Logs.zip&quot; title=&quot;Separate_Logs.zip attached to KAFKA-6052&quot;&gt;Separate_Logs.zip&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; that should correlate with each other. Hopefully this helps. Thanks!&lt;/p&gt;</comment>
                            <comment id="16235055" author="banqjdon" created="Thu, 2 Nov 2017 01:31:09 +0000"  >&lt;p&gt;me too, if consumer is set to props.put(&quot;isolation.level&quot;, &quot;read_uncommitted&quot;); but not with &quot;read_committed&quot; set. I&apos;m running windows 7. kafka 1.0 not yet solve it.&lt;/p&gt;</comment>
                            <comment id="16249151" author="vmallavarapu" created="Mon, 13 Nov 2017 06:33:10 +0000"  >&lt;p&gt;Hi All&lt;/p&gt;

&lt;p&gt;We are encountering the same issue and it is a blocker for kafka upgrade.  Our setup also in windows completely.  The same scenario is tested on Ubuntu 14.04 and it is working as expected.&lt;/p&gt;

&lt;p&gt;Any quick resolution to this issue is very much appreciated.&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Veeru&lt;/p&gt;</comment>
                            <comment id="16374153" author="vmallavarapu" created="Fri, 23 Feb 2018 09:50:13 +0000"  >&lt;p&gt;Hi All&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Please let us know when are you planning to provide the fix ?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;Veeru&lt;/p&gt;</comment>
                            <comment id="16375380" author="hachikuji" created="Sat, 24 Feb 2018 07:16:25 +0000"  >&lt;p&gt;It is clear that the transaction markers are failing to be written. I see in one of the posted logs that the transaction coordinator continually retries sending, but it always fails. The most puzzling part is that I see it initiate the connection to the destination broker (which happens to be itself), but I never see it complete the connection, nor do I see it disconnect or attempt to reconnect. It is as if the connection is being established successfully but the notification is never reaching the NetworkClient. That made me think of&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3378&quot; title=&quot;Client blocks forever if SocketChannel connects instantly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3378&quot;&gt;&lt;del&gt;KAFKA-3378&lt;/del&gt;&lt;/a&gt;, but that was fixed well before 0.11, so I&apos;m not sure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vmallavarapu&quot; class=&quot;user-hover&quot; rel=&quot;vmallavarapu&quot;&gt;vmallavarapu&lt;/a&gt; Could you post broker logs as well? It would be helpful to confirm whether it looks like the same problem.&lt;/p&gt;</comment>
                            <comment id="16376462" author="vmallavarapu" created="Mon, 26 Feb 2018 07:18:35 +0000"  >&lt;p&gt;HI &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;can you please refer the &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6153&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-6153&lt;/a&gt;.&#160; We pasted and attached the logs there&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;Veeru&lt;/p&gt;</comment>
                            <comment id="16376515" author="hachikuji" created="Mon, 26 Feb 2018 08:25:46 +0000"  >&lt;p&gt;I had one admittedly far-fetched idea. The inter-broker send thread which is responsible for sending transaction markers does not block for connection establishment. The first time a request is sent to a node, the connection is initiated, but unless the connection is established extremely quickly, we&apos;ll just fail the request and depend on it being retried some time later after the connection has been established. The problem is that the request&apos;s completion handler is immediately invoked, which causes the request to be re-enqueued, and the selector to be woken up. I am wondering if the wakeup might be treated differently on windows: in particular, maybe it is shortcutting the select operation and not allowing the connection to establish. On the next iteration of the send thread&apos;s&#160;loop, the same thing would happen and we&apos;d never be able to send the marker.&lt;/p&gt;

&lt;p&gt;An easy way to test this theory is to disable the wakeup and switch to a busier poll model. I&apos;ve done this here: &lt;a href=&quot;https://github.com/hachikuji/kafka/commit/7bdb499b83b74e4c263458d0f1f90d68536fe7db&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/hachikuji/kafka/commit/7bdb499b83b74e4c263458d0f1f90d68536fe7db&lt;/a&gt;.&#160;If any users who are seeing this bug can test this out, I&apos;d appreciate it.&lt;/p&gt;</comment>
                            <comment id="16377149" author="hachikuji" created="Mon, 26 Feb 2018 16:46:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vmallavarapu&quot; class=&quot;user-hover&quot; rel=&quot;vmallavarapu&quot;&gt;vmallavarapu&lt;/a&gt; I looked at the logs, but they don&apos;t offer any new insight. Would you be willing to test with the patch that I linked to above?&lt;/p&gt;</comment>
                            <comment id="16387652" author="vmallavarapu" created="Tue, 6 Mar 2018 11:38:51 +0000"  >&lt;p&gt;Hi Jason&lt;/p&gt;

&lt;p&gt;This fix worked for us. Thanks for&#160;everything.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16388270" author="hachikuji" created="Tue, 6 Mar 2018 18:35:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vmallavarapu&quot; class=&quot;user-hover&quot; rel=&quot;vmallavarapu&quot;&gt;vmallavarapu&lt;/a&gt; Great, thanks for confirming! I&apos;ll try to find some time to&#160;work on a proper patch and get it into trunk.&lt;/p&gt;</comment>
                            <comment id="16388422" author="apurva" created="Tue, 6 Mar 2018 19:49:26 +0000"  >&lt;p&gt;Great sleuthing &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;!&lt;/p&gt;</comment>
                            <comment id="16389049" author="vmallavarapu" created="Wed, 7 Mar 2018 05:45:54 +0000"  >&lt;p&gt;Hi Jason&lt;/p&gt;

&lt;p&gt;We are planning to migrate to Kafka 1.0.0 version , however we stopped due to this issue. &#160;Can you please let me know if there any expected date. &#160;So that I can wait.&lt;/p&gt;

&lt;p&gt;Thanks inadvance.&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;Veeru&lt;/p&gt;</comment>
                            <comment id="16397789" author="githubbot" created="Tue, 13 Mar 2018 22:47:33 +0000"  >&lt;p&gt;vahidhashemian opened a new pull request #4705: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6052&quot; title=&quot;Windows: Consumers not polling when isolation.level=read_committed &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6052&quot;&gt;&lt;del&gt;KAFKA-6052&lt;/del&gt;&lt;/a&gt;: Fix the request retry issue (on Windows) in InterBrokerSendThread&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4705&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4705&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   This resolves the issue detected on Windows. It&apos;s a follow-up to the investigation done by @hachikuji (details on the JIRA).&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16408105" author="pegerto" created="Wed, 21 Mar 2018 15:38:35 +0000"  >&lt;p&gt;Hello&lt;/p&gt;

&lt;p&gt;We did some test on 1.0.1 and it seems the issue is resolved but this ticket is open, can anybody else confirm if this is solved with 1.0.1?&lt;/p&gt;

&lt;p&gt;Regards.&lt;/p&gt;</comment>
                            <comment id="16408411" author="vahid" created="Wed, 21 Mar 2018 18:49:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Pegerto&quot; class=&quot;user-hover&quot; rel=&quot;Pegerto&quot;&gt;Pegerto&lt;/a&gt; I just tested this with Kafka 1.0.1 on my Windows 10 environment and could still reproduce the issue.&lt;/p&gt;</comment>
                            <comment id="16408971" author="banqjdon" created="Thu, 22 Mar 2018 02:57:13 +0000"  >&lt;p&gt;&#160;issue problem still exist in Kafka 1.0.1 on my Windows 10, I just tested it.&lt;/p&gt;</comment>
                            <comment id="16412793" author="githubbot" created="Sat, 24 Mar 2018 21:04:11 +0000"  >&lt;p&gt;hachikuji closed pull request #4705: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6052&quot; title=&quot;Windows: Consumers not polling when isolation.level=read_committed &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6052&quot;&gt;&lt;del&gt;KAFKA-6052&lt;/del&gt;&lt;/a&gt;: Fix the request retry issue (on Windows) in InterBrokerSendThread&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4705&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4705&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/common/InterBrokerSendThread.scala b/core/src/main/scala/kafka/common/InterBrokerSendThread.scala&lt;br/&gt;
index 70dae354aa4..c65e5572974 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/common/InterBrokerSendThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/InterBrokerSendThread.scala&lt;br/&gt;
@@ -16,13 +16,17 @@&lt;br/&gt;
  */&lt;br/&gt;
 package kafka.common&lt;/p&gt;

&lt;p&gt;+import java.util.&lt;/p&gt;
{ArrayDeque, ArrayList, Collection, Collections, HashMap, Iterator}
&lt;p&gt;+import java.util.Map.Entry&lt;br/&gt;
+&lt;br/&gt;
 import kafka.utils.ShutdownableThread&lt;br/&gt;
-import org.apache.kafka.clients.&lt;/p&gt;
{ClientResponse, NetworkClient, RequestCompletionHandler}
&lt;p&gt;+import org.apache.kafka.clients.&lt;/p&gt;
{ClientRequest, ClientResponse, NetworkClient, RequestCompletionHandler}&lt;br/&gt;
 import org.apache.kafka.common.Node&lt;br/&gt;
 import org.apache.kafka.common.internals.FatalExitError&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest&lt;br/&gt;
 import org.apache.kafka.common.utils.Time&lt;br/&gt;
 &lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  *  Class for inter-broker send thread that utilize a non-blocking network client.&lt;br/&gt;
@@ -34,6 +38,10 @@ abstract class InterBrokerSendThread(name: String,&lt;br/&gt;
   extends ShutdownableThread(name, isInterruptible) {&lt;br/&gt;
 &lt;br/&gt;
   def generateRequests(): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;RequestAndCompletionHandler&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  def unsentExpiryMs: Int&lt;br/&gt;
+  private val unsentRequests = new UnsentRequests&lt;br/&gt;
+&lt;br/&gt;
+  def hasUnsentRequests = unsentRequests.iterator().hasNext&lt;br/&gt;
 &lt;br/&gt;
   override def shutdown(): Unit = {
     initiateShutdown()
@@ -43,35 +51,21 @@ abstract class InterBrokerSendThread(name: String,
   }&lt;br/&gt;
 &lt;br/&gt;
   override def doWork() {&lt;br/&gt;
-    val now = time.milliseconds()&lt;br/&gt;
-    var pollTimeout = Long.MaxValue&lt;br/&gt;
+    var now = time.milliseconds()&lt;br/&gt;
+&lt;br/&gt;
+    generateRequests().foreach { request =&amp;gt;
+      val completionHandler = request.handler
+      unsentRequests.put(request.destination,
+        networkClient.newClientRequest(request.destination.idString, request.request, now, true, completionHandler))
+    }&lt;br/&gt;
 &lt;br/&gt;
     try {&lt;br/&gt;
-      for (request: RequestAndCompletionHandler &amp;lt;- generateRequests()) {&lt;br/&gt;
-        val destination = Integer.toString(request.destination.id())&lt;br/&gt;
-        val completionHandler = request.handler&lt;br/&gt;
-        val clientRequest = networkClient.newClientRequest(destination,&lt;br/&gt;
-          request.request,&lt;br/&gt;
-          now,&lt;br/&gt;
-          true,&lt;br/&gt;
-          completionHandler)&lt;br/&gt;
-&lt;br/&gt;
-        if (networkClient.ready(request.destination, now)) {
-          networkClient.send(clientRequest, now)
-        } else {
-          val header = clientRequest.makeHeader(request.request.latestAllowedVersion)
-          val disconnectResponse: ClientResponse = new ClientResponse(header, completionHandler, destination,
-            now /* createdTimeMs */ , now /* receivedTimeMs */ , true /* disconnected */ , null /* versionMismatch */ ,
-            null /* responseBody */)
-
-          // poll timeout would be the minimum of connection delay if there are any dest yet to be reached;
-          // otherwise it is infinity
-          pollTimeout = Math.min(pollTimeout, networkClient.connectionDelay(request.destination, now))
-
-          completionHandler.onComplete(disconnectResponse)
-        }&lt;br/&gt;
-      }&lt;br/&gt;
-      networkClient.poll(pollTimeout, now)&lt;br/&gt;
+      val timeout = sendRequests(now)&lt;br/&gt;
+      networkClient.poll(timeout, now)&lt;br/&gt;
+      now = time.milliseconds()&lt;br/&gt;
+      checkDisconnects(now)&lt;br/&gt;
+      failExpiredRequests(now)&lt;br/&gt;
+      unsentRequests.clean()&lt;br/&gt;
     } catch {
       case e: FatalExitError =&amp;gt; throw e
       case t: Throwable =&amp;gt;
@@ -84,9 +78,113 @@ abstract class InterBrokerSendThread(name: String,
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def wakeup(): Unit = networkClient.wakeup()&lt;br/&gt;
+  private def sendRequests(now: Long): Long = {&lt;br/&gt;
+    var pollTimeout = Long.MaxValue&lt;br/&gt;
+    for (node &amp;lt;- unsentRequests.nodes.asScala) {&lt;br/&gt;
+      val requestIterator = unsentRequests.requestIterator(node)&lt;br/&gt;
+      while (requestIterator.hasNext) {&lt;br/&gt;
+        val request = requestIterator.next&lt;br/&gt;
+        if (networkClient.ready(node, now)) {
+          networkClient.send(request, now)
+          requestIterator.remove()
+        } else&lt;br/&gt;
+          pollTimeout = Math.min(pollTimeout, networkClient.connectionDelay(node, now))&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    pollTimeout&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def checkDisconnects(now: Long): Unit = {&lt;br/&gt;
+    // any disconnects affecting requests that have already been transmitted will be handled&lt;br/&gt;
+    // by NetworkClient, so we just need to check whether connections for any of the unsent&lt;br/&gt;
+    // requests have been disconnected; if they have, then we complete the corresponding future&lt;br/&gt;
+    // and set the disconnect flag in the ClientResponse&lt;br/&gt;
+    val iterator = unsentRequests.iterator()&lt;br/&gt;
+    while (iterator.hasNext) {&lt;br/&gt;
+      val entry = iterator.next&lt;br/&gt;
+      val (node, requests) = (entry.getKey, entry.getValue)&lt;br/&gt;
+      if (!requests.isEmpty &amp;amp;&amp;amp; networkClient.connectionFailed(node)) {&lt;br/&gt;
+        iterator.remove()&lt;br/&gt;
+        for (request &amp;lt;- requests.asScala) {
+          if (networkClient.authenticationException(node) != null)
+            error(s&quot;Failed to send the following request due to authentication error: $request&quot;)
+          completeWithDisconnect(request, now)
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
 &lt;br/&gt;
+  private def failExpiredRequests(now: Long): Unit = {&lt;br/&gt;
+    // clear all expired unsent requests&lt;br/&gt;
+    val expiredRequests = unsentRequests.removeExpiredRequests(now, unsentExpiryMs)&lt;br/&gt;
+    for (request &amp;lt;- expiredRequests.asScala) {
+      debug(s&quot;Failed to send the following request after $unsentExpiryMs ms: $request&quot;)
+      completeWithDisconnect(request, now)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def completeWithDisconnect(request: ClientRequest, now: Long): Unit = {
+    val handler = request.callback
+    handler.onComplete(new ClientResponse(request.makeHeader(request.requestBuilder().latestAllowedVersion()),
+      handler, request.destination, now /* createdTimeMs */ , now /* receivedTimeMs */ , true /* disconnected */ ,
+      null /* versionMismatch */ , null /* responseBody */))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def wakeup(): Unit = networkClient.wakeup()&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 case class RequestAndCompletionHandler(destination: Node, request: AbstractRequest.Builder&lt;span class=&quot;error&quot;&gt;&amp;#91;_ &amp;lt;: AbstractRequest&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-                                       handler: RequestCompletionHandler)&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
+                                       handler: RequestCompletionHandler)&lt;br/&gt;
+&lt;br/&gt;
+private class UnsentRequests {&lt;br/&gt;
+  private val unsent = new HashMap[Node, ArrayDeque&lt;span class=&quot;error&quot;&gt;&amp;#91;ClientRequest&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+&lt;br/&gt;
+  def put(node: Node, request: ClientRequest): Unit = {&lt;br/&gt;
+    var requests = unsent.get(node)&lt;br/&gt;
+    if (requests == null) {
+      requests = new ArrayDeque[ClientRequest]
+      unsent.put(node, requests)
+    }&lt;br/&gt;
+    requests.add(request)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def removeExpiredRequests(now: Long, unsentExpiryMs: Long): Collection&lt;span class=&quot;error&quot;&gt;&amp;#91;ClientRequest&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    val expiredRequests = new ArrayList&lt;span class=&quot;error&quot;&gt;&amp;#91;ClientRequest&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    for (requests &amp;lt;- unsent.values.asScala) {&lt;br/&gt;
+      val requestIterator = requests.iterator&lt;br/&gt;
+      var foundExpiredRequest = false&lt;br/&gt;
+      while (requestIterator.hasNext &amp;amp;&amp;amp; !foundExpiredRequest) {&lt;br/&gt;
+        val request = requestIterator.next&lt;br/&gt;
+        if (request.createdTimeMs &amp;lt; now - unsentExpiryMs) {
+          expiredRequests.add(request)
+          requestIterator.remove()
+          foundExpiredRequest = true
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    expiredRequests&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def clean(): Unit = {&lt;br/&gt;
+    val iterator = unsent.values.iterator&lt;br/&gt;
+    while (iterator.hasNext) {
+      val requests = iterator.next
+      if (requests.isEmpty)
+        iterator.remove()
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def iterator(): Iterator[Entry[Node, ArrayDeque&lt;span class=&quot;error&quot;&gt;&amp;#91;ClientRequest&amp;#93;&lt;/span&gt;]] = {
+    unsent.entrySet().iterator()
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def requestIterator(node: Node): Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;ClientRequest&amp;#93;&lt;/span&gt; = {
+    val requests = unsent.get(node)
+    if (requests == null)
+      Collections.emptyIterator[ClientRequest]
+    else
+      requests.iterator
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def nodes = unsent.keySet&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala b/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala&lt;br/&gt;
index fa9d2c3fed5..7059ced5b3c 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala&lt;br/&gt;
@@ -135,6 +135,8 @@ class TransactionMarkerChannelManager(config: KafkaConfig,&lt;br/&gt;
 &lt;br/&gt;
   private val txnLogAppendRetryQueue = new LinkedBlockingQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;TxnLogAppend&amp;#93;&lt;/span&gt;()&lt;br/&gt;
 &lt;br/&gt;
+  override val unsentExpiryMs: Int = config.requestTimeoutMs&lt;br/&gt;
+&lt;br/&gt;
   newGauge(&lt;br/&gt;
     &quot;UnknownDestinationQueueSize&quot;,&lt;br/&gt;
     new Gauge&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; {&lt;br/&gt;
diff --git a/core/src/test/scala/kafka/common/InterBrokerSendThreadTest.scala b/core/src/test/scala/kafka/common/InterBrokerSendThreadTest.scala&lt;br/&gt;
index c6ebdd17c36..710686693e3 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/kafka/common/InterBrokerSendThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/kafka/common/InterBrokerSendThreadTest.scala&lt;br/&gt;
@@ -16,14 +16,15 @@&lt;br/&gt;
  */&lt;br/&gt;
 package kafka.common&lt;br/&gt;
 &lt;br/&gt;
-import org.junit.{Assert, Test}&lt;br/&gt;
 import kafka.utils.MockTime&lt;br/&gt;
 import org.apache.kafka.clients.{ClientRequest, ClientResponse, NetworkClient, RequestCompletionHandler}
&lt;p&gt; import org.apache.kafka.common.Node&lt;br/&gt;
+import org.apache.kafka.common.errors.AuthenticationException&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils&lt;br/&gt;
 import org.easymock.EasyMock&lt;br/&gt;
+import org.junit.&lt;/p&gt;
{Assert, Test}

&lt;p&gt; import scala.collection.mutable&lt;/p&gt;

&lt;p&gt;@@ -35,18 +36,20 @@ class InterBrokerSendThreadTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldNotSendAnythingWhenNoRequests(): Unit = {&lt;br/&gt;
     val sendThread = new InterBrokerSendThread(&quot;name&quot;, networkClient, time) &lt;/p&gt;
{
+      override val unsentExpiryMs: Int = 1000
       override def generateRequests() = mutable.Iterable.empty
     }

&lt;p&gt;     // poll is always called but there should be no further invocations on NetworkClient&lt;br/&gt;
     EasyMock.expect(networkClient.poll(EasyMock.anyLong(), EasyMock.anyLong()))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.andReturn(Utils.mkList())&lt;br/&gt;
+      .andReturn(Utils.mkList())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     EasyMock.replay(networkClient)&lt;/p&gt;

&lt;p&gt;     sendThread.doWork()&lt;/p&gt;

&lt;p&gt;     EasyMock.verify(networkClient)&lt;br/&gt;
+    Assert.assertFalse(completionHandler.executedWithDisconnectedResponse)&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
@@ -55,6 +58,7 @@ class InterBrokerSendThreadTest {&lt;br/&gt;
     val node = new Node(1, &quot;&quot;, 8080)&lt;br/&gt;
     val handler = RequestAndCompletionHandler(node, request, completionHandler)&lt;br/&gt;
     val sendThread = new InterBrokerSendThread(&quot;name&quot;, networkClient, time) &lt;/p&gt;
{
+      override val unsentExpiryMs: Int = 1000
       override def generateRequests() = List[RequestAndCompletionHandler](handler)
     }

&lt;p&gt;@@ -65,10 +69,10 @@ class InterBrokerSendThreadTest {&lt;br/&gt;
       EasyMock.anyLong(),&lt;br/&gt;
       EasyMock.eq(true),&lt;br/&gt;
       EasyMock.same(handler.handler)))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.andReturn(clientRequest)&lt;br/&gt;
+      .andReturn(clientRequest)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     EasyMock.expect(networkClient.ready(node, time.milliseconds()))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.andReturn(true)&lt;br/&gt;
+      .andReturn(true)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     EasyMock.expect(networkClient.send(clientRequest, time.milliseconds()))&lt;/p&gt;

&lt;p&gt;@@ -80,15 +84,16 @@ class InterBrokerSendThreadTest &lt;/p&gt;
{
     sendThread.doWork()
 
     EasyMock.verify(networkClient)
+    Assert.assertFalse(completionHandler.executedWithDisconnectedResponse)
   }

&lt;p&gt;-&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldCallCompletionHandlerWithDisconnectedResponseWhenNodeNotReady(): Unit = {&lt;br/&gt;
     val request = new StubRequestBuilder&lt;br/&gt;
     val node = new Node(1, &quot;&quot;, 8080)&lt;br/&gt;
     val requestAndCompletionHandler = RequestAndCompletionHandler(node, request, completionHandler)&lt;br/&gt;
     val sendThread = new InterBrokerSendThread(&quot;name&quot;, networkClient, time) &lt;/p&gt;
{
+      override val unsentExpiryMs: Int = 1000
       override def generateRequests() = List[RequestAndCompletionHandler](requestAndCompletionHandler)
     }

&lt;p&gt;@@ -105,17 +110,66 @@ class InterBrokerSendThreadTest &lt;/p&gt;
{
       .andReturn(false)
 
     EasyMock.expect(networkClient.connectionDelay(EasyMock.anyObject(), EasyMock.anyLong()))
-    .andReturn(0)
+      .andReturn(0)
 
     EasyMock.expect(networkClient.poll(EasyMock.anyLong(), EasyMock.anyLong()))
       .andReturn(Utils.mkList())
 
+    EasyMock.expect(networkClient.connectionFailed(node))
+      .andReturn(true)
+
+    EasyMock.expect(networkClient.authenticationException(node))
+      .andReturn(new AuthenticationException(&quot;&quot;))
+
+    EasyMock.replay(networkClient)
+
+    sendThread.doWork()
+
+    EasyMock.verify(networkClient)
+    Assert.assertTrue(completionHandler.executedWithDisconnectedResponse)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testFailingExpiredRequests(): Unit = {&lt;br/&gt;
+    val request = new StubRequestBuilder()&lt;br/&gt;
+    val node = new Node(1, &quot;&quot;, 8080)&lt;br/&gt;
+    val handler = RequestAndCompletionHandler(node, request, completionHandler)&lt;br/&gt;
+    val sendThread = new InterBrokerSendThread(&quot;name&quot;, networkClient, time) &lt;/p&gt;
{
+      override val unsentExpiryMs: Int = 1000
+      override def generateRequests() = List[RequestAndCompletionHandler](handler)
+    }
&lt;p&gt;+&lt;br/&gt;
+    val clientRequest = new ClientRequest(&quot;dest&quot;, request, 0, &quot;1&quot;, time.milliseconds(), true, handler.handler)&lt;br/&gt;
+    time.sleep(1500)&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.expect(networkClient.newClientRequest(EasyMock.eq(&quot;1&quot;),&lt;br/&gt;
+      EasyMock.same(handler.request),&lt;br/&gt;
+      EasyMock.eq(time.milliseconds()),&lt;br/&gt;
+      EasyMock.eq(true),&lt;br/&gt;
+      EasyMock.same(handler.handler)))&lt;br/&gt;
+      .andReturn(clientRequest)&lt;br/&gt;
+&lt;br/&gt;
+    // make the node unready so the request is not cleared&lt;br/&gt;
+    EasyMock.expect(networkClient.ready(node, time.milliseconds()))&lt;br/&gt;
+      .andReturn(false)&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.expect(networkClient.connectionDelay(EasyMock.anyObject(), EasyMock.anyLong()))&lt;br/&gt;
+      .andReturn(0)&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.expect(networkClient.poll(EasyMock.anyLong(), EasyMock.anyLong()))&lt;br/&gt;
+      .andReturn(Utils.mkList())&lt;br/&gt;
+&lt;br/&gt;
+    // rule out disconnects so the request stays for the expiry check&lt;br/&gt;
+    EasyMock.expect(networkClient.connectionFailed(node))&lt;br/&gt;
+      .andReturn(false)&lt;br/&gt;
+&lt;br/&gt;
     EasyMock.replay(networkClient)&lt;/p&gt;

&lt;p&gt;     sendThread.doWork()&lt;/p&gt;

&lt;p&gt;     EasyMock.verify(networkClient)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Assert.assertTrue(completionHandler.response.wasDisconnected())&lt;br/&gt;
+    Assert.assertFalse(sendThread.hasUnsentRequests)&lt;br/&gt;
+    Assert.assertTrue(completionHandler.executedWithDisconnectedResponse)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;@@ -124,8 +178,10 @@ class InterBrokerSendThreadTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   private class StubCompletionHandler extends RequestCompletionHandler {&lt;br/&gt;
+    var executedWithDisconnectedResponse = false&lt;br/&gt;
     var response: ClientResponse = _&lt;br/&gt;
     override def onComplete(response: ClientResponse): Unit = &lt;/p&gt;
{
+      this.executedWithDisconnectedResponse = response.wasDisconnected()
       this.response = response
     }
&lt;p&gt;   }&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16412798" author="hachikuji" created="Sat, 24 Mar 2018 21:15:07 +0000"  >&lt;p&gt;The fix for this issue has been merged to trunk as well as the 1.1 and 1.0 release branches. Most likely the next release that will have this fix is 1.1.1 since we have probably missed 1.1.0.&lt;/p&gt;</comment>
                            <comment id="16430366" author="vmallavarapu" created="Mon, 9 Apr 2018 10:58:24 +0000"  >&lt;p&gt;Hi Jason&lt;/p&gt;

&lt;p&gt;Could you please confirm the release date of either 1.1.1. or 1.2.0 ? &#160;will wait if it happens in this month or else we take the fix and apply to 1.0.0 version.&#160;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;

&lt;p&gt;Veeru&lt;/p&gt;</comment>
                            <comment id="16434927" author="vmallavarapu" created="Thu, 12 Apr 2018 04:58:23 +0000"  >&lt;p&gt;Hi Jason / Vahid&#160;&lt;/p&gt;

&lt;p&gt;Can you please update about the release date of 1.1.1 or 1.2.0 ?&#160;&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;Veeru&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13117478">KAFKA-6196</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13113290">KAFKA-6153</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13110196">KAFKA-6076</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12892022" name="Prducer_Consumer.log" size="620339" author="zandegran" created="Fri, 13 Oct 2017 09:47:49 +0000"/>
                            <attachment id="12892211" name="Separate_Logs.zip" size="611853" author="zandegran" created="Sat, 14 Oct 2017 10:37:12 +0000"/>
                            <attachment id="12892040" name="kafka-logs.zip" size="237875" author="zandegran" created="Fri, 13 Oct 2017 11:21:17 +0000"/>
                            <attachment id="12892019" name="logFile.log" size="61344622" author="zandegran" created="Fri, 13 Oct 2017 09:27:20 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 31 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3l53b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>