<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:38:44 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-992] Double Check on Broker Registration to Avoid False NodeExist Exception</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-992</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The current behavior of zookeeper for ephemeral nodes is that session expiration and ephemeral node deletion is not an atomic operation. &lt;/p&gt;

&lt;p&gt;The side-effect of the above zookeeper behavior in Kafka, for certain corner cases, is that ephemeral nodes can be lost even if the session is not expired. The sequence of events that can lead to lossy ephemeral nodes is as follows -&lt;/p&gt;

&lt;p&gt;1. The session expires on the client, it assumes the ephemeral nodes are deleted, so it establishes a new session with zookeeper and tries to re-create the ephemeral nodes. &lt;br/&gt;
2. However, when it tries to re-create the ephemeral node,zookeeper throws back a NodeExists error code. Now this is legitimate during a session disconnect event (since zkclient automatically retries the&lt;br/&gt;
operation and raises a NodeExists error). Also by design, Kafka server doesn&apos;t have multiple zookeeper clients create the same ephemeral node, so Kafka server assumes the NodeExists is normal. &lt;br/&gt;
3. However, after a few seconds zookeeper deletes that ephemeral node. So from the client&apos;s perspective, even though the client has a new valid session, its ephemeral node is gone.&lt;/p&gt;

&lt;p&gt;This behavior is triggered due to very long fsync operations on the zookeeper leader. When the leader wakes up from such a long fsync operation, it has several sessions to expire. And the time between the session expiration and the ephemeral node deletion is magnified. Between these 2 operations, a zookeeper client can issue a ephemeral node creation operation, that could&apos;ve appeared to have succeeded, but the leader later deletes the ephemeral node leading to permanent ephemeral node loss from the client&apos;s perspective. &lt;/p&gt;

&lt;p&gt;Thread from zookeeper mailing list: &lt;a href=&quot;http://zookeeper.markmail.org/search/?q=Zookeeper+3.3.4#query:Zookeeper%203.3.4%20date%3A201307%20+page:1+mid:zma242a2qgp6gxvx+state:results&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://zookeeper.markmail.org/search/?q=Zookeeper+3.3.4#query:Zookeeper%203.3.4%20date%3A201307%20+page:1+mid:zma242a2qgp6gxvx+state:results&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12660847">KAFKA-992</key>
            <summary>Double Check on Broker Registration to Avoid False NodeExist Exception</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="guozhang">Guozhang Wang</assignee>
                                    <reporter username="nehanarkhede">Neha Narkhede</reporter>
                        <labels>
                    </labels>
                <created>Wed, 31 Jul 2013 02:39:47 +0000</created>
                <updated>Thu, 27 Nov 2014 00:11:07 +0000</updated>
                            <resolved>Mon, 5 Aug 2013 17:06:33 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="13724789" author="guozhang" created="Wed, 31 Jul 2013 02:42:38 +0000"  >&lt;p&gt;We can differentiate this edge case from a temporal connection loss by adding a timestamp into the broker ZK string so that the conflict will be reflected. Then we can check if the host:port are the same. If this is the case, then we can treat this ephemeral node as written by the broker itself but from a previous session, hence backoff for it to be deleted on session timeout and retry creating the ephemeral node. This will make the temporal connection loss a false positive case, but it should be fine since this case happens rarely.&lt;/p&gt;</comment>
                            <comment id="13724793" author="guozhang" created="Wed, 31 Jul 2013 02:45:30 +0000"  >&lt;p&gt;In ZkUtils.registerBrokerInZk:&lt;/p&gt;

&lt;p&gt;1. Add the current timestamp into the Json string.&lt;/p&gt;

&lt;p&gt;2. Upon ZkNodeExistsException, read the broker id path:&lt;/p&gt;

&lt;p&gt;2.1. If hit a ZkNoNodeException, then retry immediately.&lt;/p&gt;

&lt;p&gt;2.2. Compare the read broker hostname and port, if they are the same, backoff and retry since it will be deleted eventually; otherwise throw the RuntimeException.&lt;/p&gt;
</comment>
                            <comment id="13726648" author="guozhang" created="Thu, 1 Aug 2013 17:40:42 +0000"  >&lt;p&gt;Unit tests passed&lt;/p&gt;</comment>
                            <comment id="13727438" author="swapnilghike" created="Fri, 2 Aug 2013 07:09:53 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I am not completely clear on why timestamp is required to be stored in zookeeper along with other broker info. If I am not wrong, znode stores the session that created it in the field ephemeralOwner. There should be a way to get its value when we read broker znode info.&lt;/li&gt;
	&lt;li&gt;Perhaps we should have fixed number of retries. If zookeeper cannot delete the znode after session expiration after sufficient amount of time, we would probably like to know that we are dealing with a buggy zookeeper setup.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Then this should suffice:&lt;/p&gt;

&lt;p&gt;catch ZkNodeExistsException =&amp;gt;&lt;br/&gt;
for (numRetries) {&lt;br/&gt;
 if (broker.host == host &amp;amp;&amp;amp; broker.port == port &amp;amp;&amp;amp; sessionId == lastSessionId) &lt;/p&gt;
{
   Thread.sleep(..)
 }
&lt;p&gt; else &lt;/p&gt;
{
   throw new RuntimeException(...)
 }
&lt;p&gt;}&lt;/p&gt;</comment>
                            <comment id="13728218" author="nehanarkhede" created="Fri, 2 Aug 2013 22:49:51 +0000"  >&lt;p&gt;Thanks for patch v2, Guozhang. Few review suggestions -&lt;/p&gt;

&lt;p&gt;1. How about keeping the unix timestamp as is. All we have to make sure is that it is the equal to what was written. I&apos;m not sure there is an advantage to converting it to some date format. &lt;br/&gt;
2. Typo =&amp;gt; ephermeral&lt;br/&gt;
3. The following log statement is not completely correct -&lt;/p&gt;

&lt;p&gt;                info(&quot;I wrote this conflicted ephermeral node a while back in a different session, &quot;&lt;br/&gt;
                  + &quot;hence I will backoff for this node to be deleted by Zookeeper after session timeout and retry&quot;)&lt;/p&gt;

&lt;p&gt;The reason is because there are 3 cases when the broker might get NodeExists and the ephemeral node will have the same host and port -&lt;br/&gt;
3.1 It ran into one of the recoverable zookeeper errors while creating the ephemeral nodes, in which case ZkClient retried the operation under the covers, and it got a NodeExists error on the 2nd retry. In this case, the timestamp will be useful as it will match what was written and we do not need to retry.&lt;br/&gt;
3.2 It hit the zookeeper non-atomic session expiration problem. In this case, the timestamp will not match and we just have to retry.&lt;br/&gt;
3.3 The server was killed and restarted within the session timeout. In this case, it is useful to back off for session timeout and retry ephemeral node creation. &lt;/p&gt;

&lt;p&gt;It will be useful from a logging perspective if we can distinguish between these 3.1 &amp;amp; 3.2/3 cases and retry accordingly. Another way to look at this is to not store the timestamp and just retry on any NodeExists as that has to go through at some point, but we will not get meaningful logging which is not ideal.&lt;/p&gt;

&lt;p&gt;4. Regarding the backoff time, I think it is better to backoff for the session timeout&lt;/p&gt;

&lt;p&gt;5. Regarding the case where the broker host and port do not match -&lt;/p&gt;

&lt;p&gt;                throw new RuntimeException(&quot;A broker is already registered on the path &quot; + brokerIdPath&lt;br/&gt;
                  + &quot;. This probably indicates that you either have configured a brokerid that is already in use, or &quot;&lt;br/&gt;
                  + &quot;else you have shutdown this broker and restarted it faster than the zookeeper &quot;&lt;br/&gt;
                  + &quot;timeout so it appears to be re-registering.&quot;)&lt;/p&gt;

&lt;p&gt;The else part of this statement is incorrect since if you shutdown and restarted the same broker, the broker host and port should in fact match. We should fix the exception message to reflect that another&lt;br/&gt;
 broker &lt;span class=&quot;error&quot;&gt;&amp;#91;host, port&amp;#93;&lt;/span&gt; is registered under that id.&lt;/p&gt;</comment>
                            <comment id="13728221" author="nehanarkhede" created="Fri, 2 Aug 2013 22:54:47 +0000"  >&lt;p&gt;Swapnil,&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;You are right in observing that zookeeper stores the session id as part of the znode. However, when a session is established, we don&apos;t have access to the session id through ZkClient. So even though session id comparison is the best way to fix the bug, we can&apos;t do that.&lt;/li&gt;
	&lt;li&gt;There are a lot of things that will go wrong if zookeeper is not able to create or expire ephemeral nodes. In such cases, Kafka server will backoff and retry registering, the controller will trigger leader elections repeatedly. So we will know this by through the LeaderElectionRate and UnderReplicatedPartitionCount metrics.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13728230" author="guozhang" created="Fri, 2 Aug 2013 22:59:42 +0000"  >&lt;p&gt;Swapnil, we also considered this option. The problem is that zkClient does not expose such kind of information. Hence we came out with the timestamp approach.&lt;/p&gt;</comment>
                            <comment id="13728255" author="guozhang" created="Fri, 2 Aug 2013 23:25:37 +0000"  >&lt;p&gt;Thanks for the comments Neha.&lt;/p&gt;

&lt;p&gt;1,2,5: Done&lt;/p&gt;

&lt;p&gt;3. 3.1 and 3.2/3 are distincted since for 3.1 the createEphemeralPathExpectConflict would not throw the ZkNodeExistsException.&lt;/p&gt;

&lt;p&gt;4. It is not possible to get the session timeout from zkClient, so I use the default value (6000ms).&lt;/p&gt;</comment>
                            <comment id="13728354" author="swapnilghike" created="Sat, 3 Aug 2013 00:57:51 +0000"  >&lt;p&gt;Makes sense. Just one comment, you can use the session timeout from KafkaConfig, it will give you the value that is being used at runtime.&lt;/p&gt;</comment>
                            <comment id="13728358" author="swapnilghike" created="Sat, 3 Aug 2013 01:07:25 +0000"  >&lt;p&gt;Also the while loop should be fixed, the first sleep will lead to return. &lt;/p&gt;</comment>
                            <comment id="13728583" author="jkreps" created="Sat, 3 Aug 2013 16:50:42 +0000"  >&lt;p&gt;This is good, this can go on trunk, right?&lt;/p&gt;</comment>
                            <comment id="13728905" author="nehanarkhede" created="Sun, 4 Aug 2013 15:39:17 +0000"  >&lt;p&gt;Thanks for patch v3. Few more review comments -&lt;/p&gt;

&lt;p&gt;6. We should get session timeout from KafkaConfig, instead of hardcoding it.&lt;br/&gt;
7. It seems like the return should actually be moved inside the try block. That is the only time we don&apos;t want to retry since the operation is successful&lt;br/&gt;
8. You are right about createEphemeralPathExpectConflict. It already handles 3.1 (in my comments above)&lt;/p&gt;

&lt;p&gt;This bug is very serious that can halt correct operation of a 0.8 cluster. In a typical production deployment of Kafka where there are many consumers writing offsets to the same zookeeper cluster that the 08 cluster is connected to, there is a higher risk of hitting this bug. On the other hand, you can always increase the session timeout enough to get around this. However, in that case, if a broker crashes or has to be killed, it takes as long as session timeout for the consumers to recover. We have hit this bug in production at LinkedIn several times and have also had to kill 08 brokers due to bugs in controlled shutdown (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-999&quot; title=&quot;Controlled shutdown never succeeds until the broker is killed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-999&quot;&gt;&lt;del&gt;KAFKA-999&lt;/del&gt;&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;I understand that we want to stop taking patches on 08. We are still on 08 beta in open source. Until trunk is ready to be released, companies that have Kafka 08-beta running in production can run into blocker bugs (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-992&quot; title=&quot;Double Check on Broker Registration to Avoid False NodeExist Exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-992&quot;&gt;&lt;del&gt;KAFKA-992&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-999&quot; title=&quot;Controlled shutdown never succeeds until the broker is killed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-999&quot;&gt;&lt;del&gt;KAFKA-999&lt;/del&gt;&lt;/a&gt;). What release pattern can we follow here ? Does it make sense to only take critical fixes on 0.8 and leave other changes to trunk. That allows critical bug fixes to go to production before 0.8.1 is ready for release.&lt;/p&gt;



</comment>
                            <comment id="13729569" author="junrao" created="Mon, 5 Aug 2013 15:25:29 +0000"  >&lt;p&gt;It seems that it&apos;s going to take some time before this issue is fixed in ZK. So, I suggest that we patch this in 0.8.&lt;/p&gt;</comment>
                            <comment id="13729600" author="guozhang" created="Mon, 5 Aug 2013 16:08:20 +0000"  >&lt;p&gt;Thanks for the comments Neha.&lt;/p&gt;

&lt;p&gt;6,7: Done.&lt;/p&gt;

&lt;p&gt;Going to deploy for scenario-recreation testing.&lt;/p&gt;</comment>
                            <comment id="13729607" author="swapnilghike" created="Mon, 5 Aug 2013 16:15:15 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13729652" author="nehanarkhede" created="Mon, 5 Aug 2013 17:06:19 +0000"  >&lt;p&gt;+1 on v4&lt;/p&gt;</comment>
                            <comment id="13729653" author="nehanarkhede" created="Mon, 5 Aug 2013 17:06:33 +0000"  >&lt;p&gt;Committed to 0.8&lt;/p&gt;</comment>
                            <comment id="13730916" author="junrao" created="Tue, 6 Aug 2013 16:33:51 +0000"  >&lt;p&gt;Thinking about this more. The same ZK issue can affect the controller and the consumer re-registration too. Should those be handled too?&lt;/p&gt;</comment>
                            <comment id="13731187" author="jjkoshy" created="Tue, 6 Aug 2013 19:59:35 +0000"  >&lt;p&gt;Delayed review - looks good to me, although I still don&apos;t see a benefit in                                                                                                                                  &lt;br/&gt;
storing the timestamp. i.e., the approach to retry on nodeexists if the host                                                                                                                                &lt;br/&gt;
and port are the same would remain the same. i.e., it seems more for                                                                                                                                        &lt;br/&gt;
informative purposes. Let me know if I&apos;m missing something.                                                                                                                                                 &lt;/p&gt;

&lt;p&gt;@Jun, you have a point about the controller. It seems it may not be a                                                                                                                                       &lt;br/&gt;
problem there since controller re-election will happen only after the data                                                                                                                                  &lt;br/&gt;
is actually deleted. For consumers it may not be an issue either given that                                                                                                                                 &lt;br/&gt;
the consumer id string includes a random uuid.                                                                                                                                                              &lt;/p&gt;</comment>
                            <comment id="13731282" author="jjkoshy" created="Tue, 6 Aug 2013 21:18:37 +0000"  >&lt;p&gt;ok nm the comment about timestamp. I had forgotten that nodeexists wouldn&apos;t be thrown if the data is the same.&lt;/p&gt;</comment>
                            <comment id="13731337" author="jjkoshy" created="Tue, 6 Aug 2013 21:44:17 +0000"  >&lt;p&gt;and nm for my comments about controller/consumers as well. For consumers, we&lt;br/&gt;
don&apos;t regenerate the consumer id string.&lt;/p&gt;

&lt;p&gt;For controller, what can end up happening is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;controller session expires and becomes the controller again (with the&lt;br/&gt;
  stale ephemeral node)&lt;/li&gt;
	&lt;li&gt;another broker (whose session may not have expired) receives a watch when the&lt;br/&gt;
  stale ephemeral node is actually deleted&lt;/li&gt;
	&lt;li&gt;so we can end up with two controllers in this scenario.&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="13731352" author="nehanarkhede" created="Tue, 6 Aug 2013 21:58:24 +0000"  >&lt;p&gt;We just found a way to reliably reproduce the zookeeper bug and verify that &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-992&quot; title=&quot;Double Check on Broker Registration to Avoid False NodeExist Exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-992&quot;&gt;&lt;del&gt;KAFKA-992&lt;/del&gt;&lt;/a&gt; fix works. Now, we can fix the controller and consumer the same way.&lt;/p&gt;</comment>
                            <comment id="13731353" author="guozhang" created="Tue, 6 Aug 2013 22:02:53 +0000"  >&lt;p&gt;The zookeeper bug can be reproduced as follows:&lt;/p&gt;

&lt;p&gt;1. Checkout a clean 0.8 branch, revert back the &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-992&quot; title=&quot;Double Check on Broker Registration to Avoid False NodeExist Exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-992&quot;&gt;&lt;del&gt;KAFKA-992&lt;/del&gt;&lt;/a&gt; fix:&lt;/p&gt;

&lt;p&gt;2. Build and create a server connecting to a Zookeeper instance (make sure maxClientCnxns = 0 in ZK config so that one IP address can create as many connections as wanted)&lt;/p&gt;

&lt;p&gt;3. Load the Zookeeper with dummy sessions, each creates and maintains a thousand ephemeral nodes.&lt;/p&gt;

&lt;p&gt;4. Write a script that pause and resume the Zookeeper process continuously, for example:&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;br/&gt;
while true&lt;br/&gt;
do&lt;br/&gt;
        kill -STOP $1&lt;br/&gt;
        sleep 8&lt;br/&gt;
        kill -CONT $1&lt;br/&gt;
        sleep 60&lt;br/&gt;
done&lt;br/&gt;
&amp;#8212;&lt;/p&gt;

&lt;p&gt;5.  Then when the Zookeeper process resumes, it will mark all sessions as timeout, but since the ephemeral nodes to delete are too many, the server&apos;s registration node may not be deleted yet when the servers tries to re-register itself and the server think himself as registered successfully.&lt;/p&gt;

&lt;p&gt;6. And later Zookeeper will delete the server&apos;s registration node without the server&apos;s awareness.&lt;/p&gt;

&lt;p&gt;7. If we re-apply &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-992&quot; title=&quot;Double Check on Broker Registration to Avoid False NodeExist Exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-992&quot;&gt;&lt;del&gt;KAFKA-992&lt;/del&gt;&lt;/a&gt;&apos;s patch, and redo the same testing setup. Under similar conditions the server will wait and retry.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Since we can now re-produce the bug and verify the fix, the same fix will be applied to Controller and Consumer.&lt;/p&gt;</comment>
                            <comment id="13732132" author="nehanarkhede" created="Wed, 7 Aug 2013 16:25:49 +0000"  >&lt;p&gt;Thanks for the follow up patch. The changes to consumer look good. I have a few concerns about the changes to controller -&lt;/p&gt;

&lt;p&gt;1. ZookeeperLeaderElector&lt;/p&gt;

&lt;p&gt;1.1 This change is backwards incompatible. Unfortunately, when we versioned the zookeeper data, we left out the controller path. So we have to handle both the previous format and the new format in the code until the old format can be phased out. This will be hacky but we cannot accept this patch without handling this correctly, since that would require downtime at relase&lt;br/&gt;
1.2 We have moved to using json for zookeeper data. It will be good if we can follow that while making this change to the controller path&lt;br/&gt;
1.3 The while loop has a lot of return statements. How about refactoring it to have while(!writeSucceeded) {} and keep the return amILeader at the very end ?&lt;/p&gt;</comment>
                            <comment id="13733771" author="guozhang" created="Thu, 8 Aug 2013 17:56:27 +0000"  >&lt;p&gt;1.1-3. Done.&lt;/p&gt;</comment>
                            <comment id="13734114" author="nehanarkhede" created="Thu, 8 Aug 2013 22:43:23 +0000"  >&lt;p&gt;Thanks for the follow up patch Guozhang. Overall, looks correct. Few minor suggestions -&lt;/p&gt;

&lt;p&gt;9. ZkUtils&lt;/p&gt;

&lt;p&gt;9.1. Could you add more details in the log message when the json parsing of the controller path fails? Since we know we are changing the format, something along the lines of &quot;Json parsing of the controller path failed. Probably this controller is still using the old format &lt;span class=&quot;error&quot;&gt;&amp;#91;%s&amp;#93;&lt;/span&gt; of storing the broker id in the zookeeper path&quot;&lt;br/&gt;
9.2 We don&apos;t need to convert the controller variable to string since it is already a string&lt;br/&gt;
9.3 Improve the error message when both json parsing and the toInt conversion fails. &quot;Failed to parse the leader leaderinfo &quot; doesn&apos;t say that we failed to parse the controller&apos;s&lt;br/&gt;
 leader election path.&lt;/p&gt;

&lt;p&gt;10. ZookeeperLeaderElector&lt;br/&gt;
10.1 Remove unused import BrokerNotAvailableException&lt;br/&gt;
10.2 In elect() API, should&apos;nt we use readDataMaybeNull instead of readData? That covers the case if the ephemeral node disappears before you get a chance to read it.&lt;br/&gt;
10.3 Since the changes to elect() are new, I suggest we convert the debug to info or warn statements. This elect() is rarely called, this will not pollute the log.&lt;br/&gt;
10.4 One suggestion to reduce code and make it somewhat cleaner - If we change electFinished to electionNotDone, we need to change it only in one place - where we don&apos;t need to retry. Currently we have to change electFinished multiple times at different places&lt;/p&gt;</comment>
                            <comment id="13734232" author="guozhang" created="Fri, 9 Aug 2013 00:17:12 +0000"  >&lt;p&gt;Thanks for the comment Neha.&lt;/p&gt;

&lt;p&gt;9.1: Done. &lt;br/&gt;
9.2,3 Done.&lt;/p&gt;

&lt;p&gt;10.1,2,3: Done&lt;br/&gt;
10.4: Done. Great point!&lt;/p&gt;</comment>
                            <comment id="13734903" author="junrao" created="Fri, 9 Aug 2013 15:30:12 +0000"  >&lt;p&gt;Thanks for the patch. It doesn&apos;t seem to apply for me. Do you need to rebase? Just one quick comment. It seems there is common code in the re-registering logic of broker, controller and consumer. Instead of duplicating the code, could we create a common util to share the code?&lt;/p&gt;</comment>
                            <comment id="13734994" author="guozhang" created="Fri, 9 Aug 2013 17:04:26 +0000"  >&lt;p&gt;Thanks for the comments Jun. I think the re-registering logic is slightly different for broker, controller and consumer:&lt;/p&gt;

&lt;p&gt;Broker: need to check hostname + port&lt;br/&gt;
Controller: only need to check brokerId&lt;br/&gt;
Consumer: need not check anything since the consumer info like hostname and port is encoded in the ZkPath.&lt;/p&gt;

&lt;p&gt;So I think it is hard to unify consumer&apos;s logic with broker and controller&apos;s logic; it is possible to unify the broker and controller&apos;s logic though, by passing the list of json fields that we need to check. But I am not sure it worth the effort.&lt;/p&gt;</comment>
                            <comment id="13734999" author="nehanarkhede" created="Fri, 9 Aug 2013 17:08:24 +0000"  >&lt;p&gt;I agree with Guozhang that the logic to ensure we get around the de-registration issue is very nuanced to the specific path and semantics of that path. &lt;/p&gt;

&lt;p&gt;+1 on the latest patch.&lt;/p&gt;</comment>
                            <comment id="13735384" author="guozhang" created="Fri, 9 Aug 2013 22:08:33 +0000"  >&lt;p&gt;Incremental patch uploaded. Fixed a small issue that Json.parse will not throw exception but instead returns None. System test passed.&lt;/p&gt;</comment>
                            <comment id="13735437" author="nehanarkhede" created="Fri, 9 Aug 2013 22:52:00 +0000"  >&lt;p&gt;+1 on v8. Good catch!&lt;/p&gt;</comment>
                            <comment id="13736988" author="junrao" created="Mon, 12 Aug 2013 16:09:46 +0000"  >&lt;p&gt;Thanks for patch v8. I think the code can still be made cleaner.&lt;/p&gt;

&lt;p&gt;80. If you look at the code in ZkUtils.registerBrokerInZk(), ZookeeperConsumerConnecter.registerConsumerInZK() and ZookeeperLeaderEelection.elect(), they all have the logic for handling the ZK bug. They only differ slightly because the way that they check whether the registration is from the same client is different. I was thinking that we can write a new util function called sth like createEphemeralPathExpectConflictHandleZKBug(). This function will take a function that checks if the value in a ZK path is from the caller. The function will then keep trying to create the path until either it detects a value is put in by a different caller or the creation succeeds. We will get several benefits if you do that: (1) there is a centralized place to handle the ZK bug and therefore we avoid code duplication; (2) this separates the logic of handling the ZK bug from the rest of the logic in the caller, which will make the latter easier to understand; (3) it makes it easier to remove the logic in the future when the ZK bug is fixed.&lt;/p&gt;

&lt;p&gt;81. In ZookeeperLeaderEelection.elect(), we also have the logic to handle different formats of the value of the controller path. It seems that can probably be simplified a bit too. Basically, if we read the old format (in the new code), we can treat it as if someone else already did the registration.&lt;/p&gt;

&lt;p&gt;82. There is code duplication in ZkUtils.getController() and ZookeeperLeaderElection.LeaderChangeListener.handleDataChange(). Could we share the logic in a separate util?&lt;/p&gt;</comment>
                            <comment id="13737228" author="nehanarkhede" created="Mon, 12 Aug 2013 19:00:06 +0000"  >&lt;p&gt;+1 on 80. That&apos;s a great suggestion, Jun!&lt;/p&gt;</comment>
                            <comment id="13737391" author="guozhang" created="Mon, 12 Aug 2013 21:43:39 +0000"  >&lt;p&gt;Thanks for the comments.&lt;/p&gt;

&lt;p&gt;80. Added createEphemeralPathExpectConflictHandleZKBug, which takes the checker function, but also a caller info reference for comparison.&lt;/p&gt;

&lt;p&gt;81. We need to read the leader Id in this case, since it is needed in shutting down.&lt;/p&gt;

&lt;p&gt;82. Added a new Controller object, which takes the string and parse the int value. This function can remove its old-new version handling later.&lt;/p&gt;</comment>
                            <comment id="13738655" author="guozhang" created="Tue, 13 Aug 2013 18:44:29 +0000"  >&lt;p&gt;Add the Controller.scala file to kafka.cluster&lt;/p&gt;</comment>
                            <comment id="13739857" author="junrao" created="Wed, 14 Aug 2013 16:45:01 +0000"  >&lt;p&gt;v10 doesn&apos;t seem to apply to current 0.8. &lt;/p&gt;</comment>
                            <comment id="13740003" author="guozhang" created="Wed, 14 Aug 2013 18:24:48 +0000"  >&lt;p&gt;Forgot to pull before rebase, the new version apply to latest 0.8 now.&lt;/p&gt;</comment>
                            <comment id="13741179" author="nehanarkhede" created="Thu, 15 Aug 2013 16:56:40 +0000"  >&lt;p&gt;Overall, v11 is a good refactor. Few minor formatting comments -&lt;br/&gt;
1. Broker&lt;br/&gt;
I think getZkString can be removed. This is a nice to have clean up item, not introduced in your patch&lt;/p&gt;

&lt;p&gt;2. ZkUtils&lt;br/&gt;
2.1 Can we break the long log line that says &quot;A broker is already registered...&quot;&lt;br/&gt;
2.2 Typo in the comments above createEphemeralPathExpectConflictHandleZKBug() =&amp;gt; NodeExistEception&lt;/p&gt;

&lt;p&gt;3. KafkaController&lt;br/&gt;
Typo =&amp;gt; zkSessionTimout&lt;/p&gt;

&lt;p&gt;4. Controller&lt;br/&gt;
Can we add back the following statement in warn. It was helpful for me to know this while testing a cluster upgrade with this patch -&lt;/p&gt;

&lt;p&gt;            warn(&quot;Failed to parse the controller info as json. &quot; +&lt;br/&gt;
              &quot;Probably this controller is still using the old format &lt;span class=&quot;error&quot;&gt;&amp;#91;%s&amp;#93;&lt;/span&gt; of storing the broker id in the zookeeper path&quot;.format(controller))&lt;/p&gt;</comment>
                            <comment id="13741206" author="junrao" created="Thu, 15 Aug 2013 17:18:23 +0000"  >&lt;p&gt;Thanks for patch v11. Much better, but can still be improved. Some more comments:&lt;/p&gt;

&lt;p&gt;110. ZkUtils.createEphemeralPathExpectConflictHandleZKBug: &lt;br/&gt;
110.1 Could we list the ZK jira related to this bug? If that doesn&apos;t exist, create a new one. That way, we can track when the bug is fixed.&lt;br/&gt;
110.2 Typos in the comment: ata and NodeExistEception&lt;br/&gt;
110.3 Is it better to change caller to expectedCallerData&lt;br/&gt;
110.4 Could you explain what checker() does in the comment?&lt;br/&gt;
110.5 It would be useful to log the ZK path in addition to the value.&lt;/p&gt;

&lt;p&gt;111. ZkUtils.registerBrokerInZk: Is it better to rename selfBroker to expectedBroker?&lt;/p&gt;

&lt;p&gt;112. KafkaController: typo zkSessionTimout&lt;/p&gt;

&lt;p&gt;113. ZookeeperLeaderElector.elect(): I am bit confused what resign() should do. It seems that it needs to either reset leaderId or throw an exception to the caller.&lt;/p&gt;

&lt;p&gt;114. Controller: I think it&apos;s better to rename it to ControllerUtils. Also, remove unused imports.&lt;/p&gt;


</comment>
                            <comment id="13741587" author="guozhang" created="Thu, 15 Aug 2013 22:28:57 +0000"  >&lt;p&gt;Thanks for the comments, Neha, Jun. And sorry for these typos..&lt;/p&gt;

&lt;p&gt;Neha:&lt;/p&gt;

&lt;p&gt;1. Done.&lt;br/&gt;
2. Done.&lt;br/&gt;
3. Done.&lt;br/&gt;
4. Done.&lt;/p&gt;

&lt;p&gt;Jun:&lt;/p&gt;

&lt;p&gt;110.1 Done.&lt;br/&gt;
110.2. Done.&lt;br/&gt;
110.3. Done.&lt;br/&gt;
110.4. Done.&lt;br/&gt;
110.5. Done.&lt;br/&gt;
111. Done.&lt;br/&gt;
112. Done.&lt;br/&gt;
113. As by the meaning of &quot;resign&quot;, which indicates a valid leader actively resign its role as the leader, deleting its election path is the correct way of resigning. The question here is that upon receiving a non-ZkNodeExistsException should we really call resign or not. I am proposing not and instead just logging the error and setting leaderId = -1.&lt;br/&gt;
114. Removed unused imports. As for renaming, my expectation is that Controller object might be extend just as Broker object in Broker.scala , which will be used to create a Controller class instance when more fields are added to Controller besides just the ID. So I would propose keep its name and its location in kafka.cluster for now.&lt;/p&gt;</comment>
                            <comment id="13742296" author="guozhang" created="Fri, 16 Aug 2013 15:01:25 +0000"  >&lt;p&gt;As few more thoughts about 113: currently leaderId is only read by amILeader, which is only called by the end of elect to determine if election succeeds or not. At controller shutdown, it will try to read the controller id from ZK again instead of directly use leaderId. Hence if we do not want to make this exception a fatal one and shutdown the whole broker setting leaderId to -1 and logging the error is OK. &lt;/p&gt;

&lt;p&gt;This conclusion is based on the assumption that if all the brokers failed on election and no one becomes the leader then it is supposed to be a fatal error and should be detected by monitoring.&lt;/p&gt;</comment>
                            <comment id="13742329" author="junrao" created="Fri, 16 Aug 2013 15:51:51 +0000"  >&lt;p&gt;Thanks for patch v12. A few more comments.&lt;/p&gt;

&lt;p&gt;114. If that&apos;s the intention, we should put the logic in object KafkaController, which already exists. Also, the comment above the class is incorrect.&lt;/p&gt;

&lt;p&gt;120. ZookeeperLeaderElector.resign() is no longer being used and can be removed.&lt;/p&gt;

&lt;p&gt;121. ZookeeperLeaderElector.elect(): Just to be consistent with case e2, shouldn&apos;t we just log an error and set leaderId to -1 in the following case too?&lt;br/&gt;
            case None =&amp;gt; throw new KafkaException(&quot;Controller doesn&apos;t exist&quot;)&lt;/p&gt;

</comment>
                            <comment id="13742356" author="guozhang" created="Fri, 16 Aug 2013 16:25:15 +0000"  >&lt;p&gt;Thanks for the comments Jun.&lt;/p&gt;

&lt;p&gt;114. Agreed, deleted the Controller.scala and moved logic to KafkaController object.&lt;/p&gt;

&lt;p&gt;120. I thought ZookeeperLeaderElector.resign() is a public function that can be called by the parent process of the Elector. Currently ZookeeperLeaderElector is dependent on KafkaController (it takes controllerContext as its parameters), but I think it would be refactored in the future as an independent election module?&lt;/p&gt;

&lt;p&gt;121. In this case what really happens is that another broker has elected as the leader but somehow gets &quot;resigned&quot;. This will trigger another election round. So instead of log an error we would better log it as warn and set leaderId to -1?&lt;/p&gt;</comment>
                            <comment id="13742385" author="junrao" created="Fri, 16 Aug 2013 16:57:36 +0000"  >&lt;p&gt;Thanks for patch v13. Looks good. My only suggestion is to set leaderId to -1 in ZookeeperLeaderElector.resign() if we want to keep it.&lt;/p&gt;</comment>
                            <comment id="13742711" author="guozhang" created="Fri, 16 Aug 2013 23:28:51 +0000"  >&lt;p&gt;Thanks for the comments, added to resign, rebased on 0.8.&lt;/p&gt;</comment>
                            <comment id="13742837" author="junrao" created="Sat, 17 Aug 2013 04:48:25 +0000"  >&lt;p&gt;Thanks for patch v14. Committed to 0.8.&lt;/p&gt;</comment>
                            <comment id="14227033" author="guozhang" created="Thu, 27 Nov 2014 00:11:07 +0000"  >&lt;p&gt;We have seen some scenarios which are not fully resolved by this patch: under certain cases the ephemeral node are not deleted ever after the session has expired (there is a ticket &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-1208&quot; title=&quot;Ephemeral node not removed after the client session is long gone&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-1208&quot;&gt;&lt;del&gt;ZOOKEEPER-1208&lt;/del&gt;&lt;/a&gt; for this and it is marked to be fixed in 3.3.4, but we are still seeing this issue with a newer version).&lt;/p&gt;

&lt;p&gt;For this corner case one thing we can do (or more precisely hack around) is to force-delete the ZK path when the written timestamp and the current timestamp&apos;s difference is larger than the ZK session timeout value already.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12595336" name="KAFKA-992.v1.patch" size="3481" author="guozhang" created="Thu, 1 Aug 2013 03:42:25 +0000"/>
                            <attachment id="12597785" name="KAFKA-992.v10.patch" size="15117" author="guozhang" created="Tue, 13 Aug 2013 18:44:29 +0000"/>
                            <attachment id="12598021" name="KAFKA-992.v11.patch" size="17208" author="guozhang" created="Wed, 14 Aug 2013 18:24:48 +0000"/>
                            <attachment id="12598311" name="KAFKA-992.v12.patch" size="17737" author="guozhang" created="Thu, 15 Aug 2013 22:30:32 +0000"/>
                            <attachment id="12598487" name="KAFKA-992.v13.patch" size="18106" author="guozhang" created="Fri, 16 Aug 2013 16:25:27 +0000"/>
                            <attachment id="12598549" name="KAFKA-992.v14.patch" size="18362" author="guozhang" created="Fri, 16 Aug 2013 23:28:51 +0000"/>
                            <attachment id="12595452" name="KAFKA-992.v2.patch" size="3498" author="guozhang" created="Thu, 1 Aug 2013 17:40:42 +0000"/>
                            <attachment id="12595697" name="KAFKA-992.v3.patch" size="3263" author="guozhang" created="Fri, 2 Aug 2013 23:25:37 +0000"/>
                            <attachment id="12596136" name="KAFKA-992.v4.patch" size="4140" author="guozhang" created="Mon, 5 Aug 2013 16:08:20 +0000"/>
                            <attachment id="12596455" name="KAFKA-992.v5.patch" size="10467" author="guozhang" created="Tue, 6 Aug 2013 23:41:23 +0000"/>
                            <attachment id="12596897" name="KAFKA-992.v6.patch" size="13437" author="guozhang" created="Thu, 8 Aug 2013 17:56:27 +0000"/>
                            <attachment id="12596993" name="KAFKA-992.v7.patch" size="16284" author="guozhang" created="Fri, 9 Aug 2013 00:17:12 +0000"/>
                            <attachment id="12597186" name="KAFKA-992.v8.patch" size="12539" author="guozhang" created="Fri, 9 Aug 2013 22:08:33 +0000"/>
                            <attachment id="12597569" name="KAFKA-992.v9.patch" size="13042" author="guozhang" created="Mon, 12 Aug 2013 21:43:39 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>14.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>341036</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 51 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1mthb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>341354</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>