<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:13:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7192] State-store can desynchronise with changelog</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7192</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;n.b. this bug has been verified with exactly-once processing enabled&lt;/p&gt;

&lt;p&gt;Consider the following scenario:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A record, N is read into a Kafka topology&lt;/li&gt;
	&lt;li&gt;the state store is updated&lt;/li&gt;
	&lt;li&gt;the topology crashes&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;&lt;a name=&quot;Expectedbehaviour%3A&quot;&gt;&lt;/a&gt;&lt;b&gt;Expected behaviour:&lt;/b&gt;&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;Node is restarted&lt;/li&gt;
	&lt;li&gt;Offset was never updated, so record N is reprocessed&lt;/li&gt;
	&lt;li&gt;State-store is reset to position N-1&lt;/li&gt;
	&lt;li&gt;Record is reprocessed&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;&lt;a name=&quot;ActualBehaviour&quot;&gt;&lt;/a&gt;&lt;b&gt;Actual Behaviour&lt;/b&gt;&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;Node is restarted&lt;/li&gt;
	&lt;li&gt;Record N is reprocessed (good)&lt;/li&gt;
	&lt;li&gt;The state store has the state from the previous processing&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I&apos;d consider this a corruption of the state-store, hence the critical Priority, although High may be more appropriate.&lt;/p&gt;

&lt;p&gt;I wrote a proof-of-concept here, which demonstrates the problem on Linux:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/spadger/kafka-streams-sad-state-store&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/spadger/kafka-streams-sad-state-store&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13173765">KAFKA-7192</key>
            <summary>State-store can desynchronise with changelog</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="guozhang">Guozhang Wang</assignee>
                                    <reporter username="jonmbates">Jon Bates</reporter>
                        <labels>
                            <label>bugs</label>
                    </labels>
                <created>Mon, 23 Jul 2018 09:56:34 +0000</created>
                <updated>Fri, 5 Apr 2019 15:34:02 +0000</updated>
                            <resolved>Fri, 27 Jul 2018 00:59:18 +0000</resolved>
                                    <version>0.11.0.3</version>
                    <version>1.0.2</version>
                    <version>1.1.1</version>
                    <version>2.0.0</version>
                                    <fixVersion>0.11.0.4</fixVersion>
                    <fixVersion>1.0.3</fixVersion>
                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="16555045" author="githubbot" created="Wed, 25 Jul 2018 02:40:19 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5421: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5421&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5421&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   1. As titled and as described in comments.&lt;br/&gt;
   2. Modified unit test slightly to insert for new keys in committed data to expose this issue.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16555049" author="guozhang" created="Wed, 25 Jul 2018 02:41:49 +0000"  >&lt;p&gt;Hello &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jonmbates&quot; class=&quot;user-hover&quot; rel=&quot;jonmbates&quot;&gt;jonmbates&lt;/a&gt; Thanks for reporting this issue. I&apos;ve successfully reproduced this issue and found the root cause.&lt;/p&gt;

&lt;p&gt;I&apos;ve submitted the PR and added a unit test to expose this issue, and confirmed that without the PR that unit test will fail.&lt;/p&gt;</comment>
                            <comment id="16555650" author="jonmbates" created="Wed, 25 Jul 2018 13:27:09 +0000"  >&lt;p&gt;Great; thanks! We came up with a similar general solution (delete state stores that don&apos;t have a checkpoint file), but don&apos;t have the insight into Kafka&apos;s internals.&lt;/p&gt;

&lt;p&gt;Just as a sanity check, if the checkpoint file points to a offset behind the changelog tail, does Kafka Streams continue syncing the state store before consuming from the source topics? (I believe this happens already, but its just to aid my understanding)1`&lt;/p&gt;</comment>
                            <comment id="16556021" author="mjsax" created="Wed, 25 Jul 2018 17:32:42 +0000"  >&lt;p&gt;It&apos;s expected that the checkpoint file is one lower, because the last &quot;record&quot; is the log is the commit marker eating up one offset. However, the checkpoint file always contains &quot;last-restored-offset plus 1&quot;. Thus, the overall restore will not be corrupted by the offset being off by one.&lt;/p&gt;</comment>
                            <comment id="16559096" author="githubbot" created="Fri, 27 Jul 2018 00:58:32 +0000"  >&lt;p&gt;guozhangwang closed pull request #5421: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5421&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5421&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index 188ff473038..94e4c71d9c2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -137,6 +137,10 @@ public String toString() &lt;/p&gt;
{
         return toString(&quot;&quot;);
     }

&lt;p&gt;+    public boolean isEosEnabled() &lt;/p&gt;
{
+        return eosEnabled;
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Produces a string representation containing useful information about a Task starting with the given indent.&lt;/li&gt;
	&lt;li&gt;This is useful in debugging scenarios.&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
index 33dce9e7558..c1a41cefc23 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
@@ -55,6 +55,10 @@ public TopicPartition partition() 
{
         return partition;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    public String storeName() &lt;/p&gt;
{
+        return storeName;
+    }
&lt;p&gt;+&lt;br/&gt;
     long checkpoint() &lt;/p&gt;
{
         return checkpoint == null ? NO_CHECKPOINT : checkpoint;
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
index 07af8019aef..1927b5a7af7 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
@@ -71,7 +71,7 @@ public void register(final StateRestorer restorer) {&lt;/p&gt;

&lt;p&gt;     public Collection&amp;lt;TopicPartition&amp;gt; restore(final RestoringTasks active) {&lt;br/&gt;
         if (!needsInitializing.isEmpty()) &lt;/p&gt;
{
-            initialize();
+            initialize(active);
         }

&lt;p&gt;         if (needsRestoring.isEmpty()) {&lt;br/&gt;
@@ -111,7 +111,7 @@ public void register(final StateRestorer restorer) &lt;/p&gt;
{
         return completed();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void initialize() {&lt;br/&gt;
+    private void initialize(final RestoringTasks active) {&lt;br/&gt;
         if (!restoreConsumer.subscription().isEmpty()) 
{
             throw new StreamsException(&quot;Restore consumer should not be subscribed to any topics (&quot; + restoreConsumer.subscription() + &quot;)&quot;);
         }
&lt;p&gt;@@ -165,11 +165,12 @@ private void initialize() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // set up restorer for those initializable&lt;br/&gt;
         if (!initializable.isEmpty()) &lt;/p&gt;
{
-            startRestoration(initializable);
+            startRestoration(initializable, active);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized) {&lt;br/&gt;
+    private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized,&lt;br/&gt;
+                                  final RestoringTasks active) {&lt;br/&gt;
         log.debug(&quot;Start restoring state stores from changelog topics {}&quot;, initialized.keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Set&amp;lt;TopicPartition&amp;gt; assignment = new HashSet&amp;lt;&amp;gt;(restoreConsumer.assignment());&lt;br/&gt;
@@ -186,6 +187,18 @@ private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializ&lt;br/&gt;
                 restorer.setStartingOffset(restoreConsumer.position(restorer.partition()));&lt;br/&gt;
                 restorer.restoreStarted();&lt;br/&gt;
             } else {&lt;br/&gt;
+                final StreamTask task = active.restoringTaskFor(restorer.partition());&lt;br/&gt;
+&lt;br/&gt;
+                // If checkpoint does not exist it means the task was not shutdown gracefully before;&lt;br/&gt;
+                // and in this case if EOS is turned on we should wipe out the state and re-initialize the task&lt;br/&gt;
+                if (task.isEosEnabled()) {&lt;br/&gt;
+                    log.info(&quot;No checkpoint found for task {} state store {} changelog {} with EOS turned on. &quot; +&lt;br/&gt;
+                            &quot;Reinitializing the task and restore its state from the beginning.&quot;, task.id, restorer.storeName(), restorer.partition());&lt;br/&gt;
+                    task.reinitializeStateStoresForPartitions(Collections.singleton(restorer.partition()));&lt;br/&gt;
+                } else {&lt;br/&gt;
+                    log.info(&quot;Restoring task {}&apos;s state store {} from beginning of the changelog {} &quot;, task.id, restorer.storeName(), restorer.partition());&lt;br/&gt;
+                }&lt;br/&gt;
+&lt;br/&gt;
                 restoreConsumer.seekToBeginning(Collections.singletonList(restorer.partition()));&lt;br/&gt;
                 needsPositionUpdate.add(restorer);&lt;br/&gt;
             }&lt;br/&gt;
@@ -280,6 +293,9 @@ private long processNext(final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records,&lt;br/&gt;
         if (!restoreRecords.isEmpty()) {&lt;br/&gt;
             restorer.restore(restoreRecords);&lt;br/&gt;
             restorer.restoreBatchCompleted(lastRestoredOffset, records.size());&lt;br/&gt;
+&lt;br/&gt;
+            log.trace(&quot;Restored from {} to {} with {} records, ending offset is {}, next starting position is {}&quot;,&lt;br/&gt;
+                    restorer.partition(), restorer.storeName(), records.size(), lastRestoredOffset, nextPosition);&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         return nextPosition;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
index 30c90c23bb3..770f579ad98 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
@@ -391,8 +391,9 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
         // the app is supposed to emit all 40 update records into the output topic&lt;br/&gt;
         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes&lt;br/&gt;
         // and store updates (ie, another 5 uncommitted writes to a changelog topic per partition)&lt;br/&gt;
+        // in the uncommitted batch sending some data for the new key to validate that upon resuming they will not be shown up in the store&lt;br/&gt;
         //&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// the failure gets inject after 20 committed and 30 uncommitted records got received&lt;br/&gt;
+        // the failure gets inject after 20 committed and 10 uncommitted records got received&lt;br/&gt;
         // -&amp;gt; the failure only kills one thread&lt;br/&gt;
         // after fail over, we should read 40 committed records and the state stores should contain the correct sums&lt;br/&gt;
         // per key (even if some records got processed twice)&lt;br/&gt;
@@ -402,7 +403,7 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
             streams.start();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; committedDataBeforeFailure = prepareData(0L, 10L, 0L, 1L);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L);&lt;br/&gt;
+            final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L, 2L, 3L);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; dataBeforeFailure = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             dataBeforeFailure.addAll(committedDataBeforeFailure);&lt;br/&gt;
@@ -610,10 +611,6 @@ public void init(final ProcessorContext context) {&lt;/p&gt;

&lt;p&gt;                     @Override&lt;br/&gt;
                     public KeyValue&amp;lt;Long, Long&amp;gt; transform(final Long key, final Long value) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (errorInjected.compareAndSet(true, false)) 
{
-                            // only tries to fail once on one of the task
-                            throw new RuntimeException(&quot;Injected test exception.&quot;);
-                        }
&lt;p&gt;                         if (gcInjected.compareAndSet(true, false)) {&lt;br/&gt;
                             while (doGC) {&lt;br/&gt;
                                 try {&lt;br/&gt;
@@ -631,16 +628,27 @@ public void init(final ProcessorContext context) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                         if (state != null) {&lt;br/&gt;
                             Long sum = state.get(key);&lt;br/&gt;
+&lt;br/&gt;
                             if (sum == null) &lt;/p&gt;
{
                                 sum = value;
                             }
&lt;p&gt; else &lt;/p&gt;
{
                                 sum += value;
                             }
&lt;p&gt;                             state.put(key, sum);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.forward(key, sum);&lt;/li&gt;
	&lt;li&gt;return null;&lt;br/&gt;
+                            state.flush();&lt;br/&gt;
+                        }&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+                        if (errorInjected.compareAndSet(true, false)) 
{
+                            // only tries to fail once on one of the task
+                            throw new RuntimeException(&quot;Injected test exception.&quot;);
+                        }
&lt;p&gt;+&lt;br/&gt;
+                        if (state != null) &lt;/p&gt;
{
+                            return new KeyValue&amp;lt;&amp;gt;(key, state.get(key));
+                        }
&lt;p&gt; else &lt;/p&gt;
{
+                            return new KeyValue&amp;lt;&amp;gt;(key, value);
                         }&lt;/li&gt;
	&lt;li&gt;return new KeyValue&amp;lt;&amp;gt;(key, value);&lt;br/&gt;
                     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
index 90abf32477f..1e74d47808e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
@@ -119,7 +119,10 @@ public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() &lt;/p&gt;
{
         final int messages = 10;
         setupConsumer(messages, topicPartition);
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);
+        replay(active, task);
         changelogReader.restore(active);
+
         assertThat(callback.restored.size(), equalTo(messages));
     }

&lt;p&gt;@@ -136,8 +139,8 @@ public void shouldRecoverFromInvalidOffsetExceptionAndFinishRestore() {&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,&lt;br/&gt;
                                                    &quot;storeName&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;/li&gt;
	&lt;li&gt;EasyMock.replay(active);&lt;br/&gt;
+        EasyMock.expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        EasyMock.replay(active, task);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // first restore call &quot;fails&quot; but we should not die with an exception&lt;br/&gt;
         assertEquals(0, changelogReader.restore(active).size());&lt;br/&gt;
@@ -164,7 +167,8 @@ public void shouldClearAssignmentAtEndOfRestore() &lt;/p&gt;
{
         setupConsumer(messages, topicPartition);
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,
                                                    &quot;storeName&quot;));
-
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);
+        replay(active, task);
         changelogReader.restore(active);
         assertThat(consumer.assignment(), equalTo(Collections.&amp;lt;TopicPartition&amp;gt;emptySet()));
     }
&lt;p&gt;@@ -175,6 +179,8 @@ public void shouldRestoreToLimitWhenSupplied() {&lt;br/&gt;
         final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, null, 3, true,&lt;br/&gt;
                                                          &quot;storeName&quot;);&lt;br/&gt;
         changelogReader.register(restorer);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         assertThat(callback.restored.size(), equalTo(3));&lt;br/&gt;
         assertThat(restorer.restoredOffset(), equalTo(3L));&lt;br/&gt;
@@ -192,14 +198,14 @@ public void shouldRestoreMultipleStores() {&lt;br/&gt;
         setupConsumer(5, one);&lt;br/&gt;
         setupConsumer(3, two);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader&lt;/li&gt;
	&lt;li&gt;.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName1&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName1&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(one, restoreListener1, null, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(two, restoreListener2, null, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(one)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(two)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;replay(active);&lt;br/&gt;
+        expect(active.restoringTaskFor(one)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(two)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored.size(), equalTo(10));&lt;br/&gt;
@@ -224,9 +230,13 @@ public void shouldRestoreAndNotifyMultipleStores() throws Exception {&lt;br/&gt;
         changelogReader.register(new StateRestorer(one, restoreListener1, null, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(two, restoreListener2, null, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(one)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(two)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;replay(active);&lt;br/&gt;
+        expect(active.restoringTaskFor(one)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(two)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
+        changelogReader.restore(active);&lt;br/&gt;
+&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored.size(), equalTo(10));&lt;br/&gt;
@@ -248,6 +258,8 @@ public void shouldOnlyReportTheLastRestoredOffset() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;br/&gt;
         changelogReader&lt;br/&gt;
             .register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName1&quot;));&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;/p&gt;

&lt;p&gt;         assertThat(callback.restored.size(), equalTo(5));&lt;br/&gt;
@@ -306,7 +318,10 @@ public void shouldNotRestoreAnythingWhenCheckpointAtEndOffset() {&lt;br/&gt;
     public void shouldReturnRestoredOffsetsForPersistentStores() &lt;/p&gt;
{
         setupConsumer(10, topicPartition);
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);
+        replay(active, task);
         changelogReader.restore(active);
+
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();
         assertThat(restoredOffsets, equalTo(Collections.singletonMap(topicPartition, 10L)));
     }
&lt;p&gt;@@ -315,6 +330,8 @@ public void shouldReturnRestoredOffsetsForPersistentStores() {&lt;br/&gt;
     public void shouldNotReturnRestoredOffsetsForNonPersistentStore() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();&lt;br/&gt;
         assertThat(restoredOffsets, equalTo(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap()));&lt;br/&gt;
@@ -330,6 +347,8 @@ public void shouldIgnoreNullKeysWhenRestoring() {&lt;br/&gt;
         consumer.assign(Collections.singletonList(topicPartition));&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false,&lt;br/&gt;
                                                    &quot;storeName&quot;));&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;/p&gt;

&lt;p&gt;         assertThat(callback.restored, CoreMatchers.equalTo(Utils.mkList(KeyValue.pair(bytes, bytes), KeyValue.pair(bytes, bytes))));&lt;br/&gt;
@@ -340,6 +359,9 @@ public void shouldCompleteImmediatelyWhenEndOffsetIs0() &lt;/p&gt;
{
         final Collection&amp;lt;TopicPartition&amp;gt; expected = Collections.singleton(topicPartition);
         setupConsumer(0, topicPartition);
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;store&quot;));
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);
+        replay(active, task);
+
         final Collection&amp;lt;TopicPartition&amp;gt; restored = changelogReader.restore(active);
         assertThat(restored, equalTo(expected));
     }
&lt;p&gt;@@ -354,10 +376,9 @@ public void shouldRestorePartitionsRegisteredPostInitialization() {&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;/p&gt;

&lt;p&gt;         final TopicPartition postInitialization = new TopicPartition(&quot;other&quot;, 0);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(postInitialization)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;replay(active);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(postInitialization)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertTrue(changelogReader.restore(active).isEmpty());&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16560326" author="githubbot" created="Fri, 27 Jul 2018 21:06:26 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5430: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt; Follow-up: update checkpoint to the reset beginning offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5430&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5430&lt;/a&gt;&lt;/p&gt;




&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16560622" author="githubbot" created="Sat, 28 Jul 2018 05:11:58 +0000"  >&lt;p&gt;guozhangwang closed pull request #5430: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt; Follow-up: update checkpoint to the reset beginning offset&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5430&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5430&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
index c1a41cefc23..3bbf42ead27 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
@@ -26,13 +26,13 @@&lt;/p&gt;

&lt;p&gt;     static final int NO_CHECKPOINT = -1;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Long checkpoint;&lt;br/&gt;
     private final long offsetLimit;&lt;br/&gt;
     private final boolean persistent;&lt;br/&gt;
     private final String storeName;&lt;br/&gt;
     private final TopicPartition partition;&lt;br/&gt;
     private final CompositeRestoreListener compositeRestoreListener;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private long checkpointOffset;&lt;br/&gt;
     private long restoredOffset;&lt;br/&gt;
     private long startingOffset;&lt;br/&gt;
     private long endingOffset;&lt;br/&gt;
@@ -45,7 +45,7 @@&lt;br/&gt;
                   final String storeName) {&lt;br/&gt;
         this.partition = partition;&lt;br/&gt;
         this.compositeRestoreListener = compositeRestoreListener;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.checkpoint = checkpoint;&lt;br/&gt;
+        this.checkpointOffset = checkpoint == null ? NO_CHECKPOINT : checkpoint;&lt;br/&gt;
         this.offsetLimit = offsetLimit;&lt;br/&gt;
         this.persistent = persistent;&lt;br/&gt;
         this.storeName = storeName;&lt;br/&gt;
@@ -60,7 +60,11 @@ public String storeName() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     long checkpoint() &lt;/p&gt;
{
-        return checkpoint == null ? NO_CHECKPOINT : checkpoint;
+        return checkpointOffset;
+    }
&lt;p&gt;+&lt;br/&gt;
+    void setCheckpointOffset(final long checkpointOffset) &lt;/p&gt;
{
+        this.checkpointOffset = checkpointOffset;
     }

&lt;p&gt;     void restoreStarted() {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
index 1927b5a7af7..9185920f242 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
@@ -48,8 +48,9 @@&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, Long&amp;gt; endOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Map&amp;lt;String, List&amp;lt;PartitionInfo&amp;gt;&amp;gt; partitionInfo = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; stateRestorers = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; needsRestoring = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; needsInitializing = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Set&amp;lt;TopicPartition&amp;gt; needsRestoring = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Set&amp;lt;TopicPartition&amp;gt; needsInitializing = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Set&amp;lt;TopicPartition&amp;gt; completedRestorers = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Duration pollTime;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public StoreChangelogReader(final Consumer&amp;lt;byte[], byte[]&amp;gt; restoreConsumer,&lt;br/&gt;
@@ -64,9 +65,14 @@ public StoreChangelogReader(final Consumer&amp;lt;byte[], byte[]&amp;gt; restoreConsumer,&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public void register(final StateRestorer restorer) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;restorer.setUserRestoreListener(userStateRestoreListener);&lt;/li&gt;
	&lt;li&gt;stateRestorers.put(restorer.partition(), restorer);&lt;/li&gt;
	&lt;li&gt;needsInitializing.put(restorer.partition(), restorer);&lt;br/&gt;
+        if (!stateRestorers.containsKey(restorer.partition())) {&lt;br/&gt;
+            restorer.setUserRestoreListener(userStateRestoreListener);&lt;br/&gt;
+            stateRestorers.put(restorer.partition(), restorer);&lt;br/&gt;
+&lt;br/&gt;
+            log.trace(&quot;Added restorer for changelog {}&quot;, restorer.partition());&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        needsInitializing.add(restorer.partition());&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public Collection&amp;lt;TopicPartition&amp;gt; restore(final RestoringTasks active) {&lt;br/&gt;
@@ -81,16 +87,15 @@ public void register(final StateRestorer restorer) {&lt;/p&gt;

&lt;p&gt;         try {&lt;br/&gt;
             final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = restoreConsumer.poll(pollTime);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Iterator&amp;lt;TopicPartition&amp;gt; iterator = needsRestoring.keySet().iterator();&lt;/li&gt;
	&lt;li&gt;while (iterator.hasNext()) {&lt;/li&gt;
	&lt;li&gt;final TopicPartition partition = iterator.next();&lt;br/&gt;
+&lt;br/&gt;
+            for (final TopicPartition partition : needsRestoring) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                 final StateRestorer restorer = stateRestorers.get(partition);                 final long pos = processNext(records.records(partition), restorer, endOffsets.get(partition));                 restorer.setRestoredOffset(pos);                 if (restorer.hasCompleted(pos, endOffsets.get(partition))) {
                     restorer.restoreDone();
                     endOffsets.remove(partition);
-                    iterator.remove();
+                    completedRestorers.add(partition);
                 }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;         } catch (final InvalidOffsetException recoverableException) {&lt;br/&gt;
@@ -98,12 +103,18 @@ public void register(final StateRestorer restorer) {&lt;br/&gt;
             final Set&amp;lt;TopicPartition&amp;gt; partitions = recoverableException.partitions();&lt;br/&gt;
             for (final TopicPartition partition : partitions) {&lt;br/&gt;
                 final StreamTask task = active.restoringTaskFor(partition);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;log.info(&quot;Reinitializing StreamTask {}&quot;, task);&lt;br/&gt;
+                log.info(&quot;Reinitializing StreamTask {} for changelog {}&quot;, task, partition);&lt;br/&gt;
+&lt;br/&gt;
+                needsInitializing.remove(partition);&lt;br/&gt;
+                needsRestoring.remove(partition);&lt;br/&gt;
+&lt;br/&gt;
                 task.reinitializeStateStoresForPartitions(recoverableException.partitions());&lt;br/&gt;
             }&lt;br/&gt;
             restoreConsumer.seekToBeginning(partitions);&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        needsRestoring.removeAll(completedRestorers);&lt;br/&gt;
+&lt;br/&gt;
         if (needsRestoring.isEmpty()) &lt;/p&gt;
{
             restoreConsumer.unsubscribe();
         }
&lt;p&gt;@@ -120,25 +131,24 @@ private void initialize(final RestoringTasks active) {&lt;br/&gt;
         // the needsInitializing map is not empty, meaning we do not know the metadata for some of them yet&lt;br/&gt;
         refreshChangelogInfo();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializable = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (final Map.Entry&amp;lt;TopicPartition, StateRestorer&amp;gt; entry : needsInitializing.entrySet()) {&lt;/li&gt;
	&lt;li&gt;final TopicPartition topicPartition = entry.getKey();&lt;br/&gt;
+        final Set&amp;lt;TopicPartition&amp;gt; initializable = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final TopicPartition topicPartition : needsInitializing) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {             if (hasPartition(topicPartition)) {
-                initializable.put(entry.getKey(), entry.getValue());
+                initializable.add(topicPartition);
             }         }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // try to fetch end offsets for the initializable restorers and remove any partitions&lt;br/&gt;
         // where we already have all of the data&lt;br/&gt;
         try &lt;/p&gt;
{
-            endOffsets.putAll(restoreConsumer.endOffsets(initializable.keySet()));
+            endOffsets.putAll(restoreConsumer.endOffsets(initializable));
         }
&lt;p&gt; catch (final TimeoutException e) {&lt;br/&gt;
             // if timeout exception gets thrown we just give up this time and retry in the next run loop&lt;br/&gt;
             log.debug(&quot;Could not fetch end offset for {}; will fall back to partition by partition fetching&quot;, initializable);&lt;br/&gt;
             return;&lt;br/&gt;
         }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Iterator&amp;lt;TopicPartition&amp;gt; iter = initializable.keySet().iterator();&lt;br/&gt;
+        final Iterator&amp;lt;TopicPartition&amp;gt; iter = initializable.iterator();&lt;br/&gt;
         while (iter.hasNext()) {&lt;br/&gt;
             final TopicPartition topicPartition = iter.next();&lt;br/&gt;
             final Long endOffset = endOffsets.get(topicPartition);&lt;br/&gt;
@@ -146,13 +156,15 @@ private void initialize(final RestoringTasks active) {&lt;br/&gt;
             // offset should not be null; but since the consumer API does not guarantee it&lt;br/&gt;
             // we add this check just in case&lt;br/&gt;
             if (endOffset != null) {&lt;/li&gt;
	&lt;li&gt;final StateRestorer restorer = needsInitializing.get(topicPartition);&lt;br/&gt;
+                final StateRestorer restorer = stateRestorers.get(topicPartition);&lt;br/&gt;
                 if (restorer.checkpoint() &amp;gt;= endOffset) 
{
                     restorer.setRestoredOffset(restorer.checkpoint());
                     iter.remove();
+                    completedRestorers.add(topicPartition);
                 }
&lt;p&gt; else if (restorer.offsetLimit() == 0 || endOffset == 0) &lt;/p&gt;
{
                     restorer.setRestoredOffset(0);
                     iter.remove();
+                    completedRestorers.add(topicPartition);
                 }
&lt;p&gt; else &lt;/p&gt;
{
                     restorer.setEndingOffset(endOffset);
                 }
&lt;p&gt;@@ -169,51 +181,59 @@ private void initialize(final RestoringTasks active) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized,&lt;br/&gt;
+    private void startRestoration(final Set&amp;lt;TopicPartition&amp;gt; initialized,&lt;br/&gt;
                                   final RestoringTasks active) {&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;Start restoring state stores from changelog topics {}&quot;, initialized.keySet());&lt;br/&gt;
+        log.debug(&quot;Start restoring state stores from changelog topics {}&quot;, initialized);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Set&amp;lt;TopicPartition&amp;gt; assignment = new HashSet&amp;lt;&amp;gt;(restoreConsumer.assignment());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assignment.addAll(initialized.keySet());&lt;br/&gt;
+        assignment.addAll(initialized);&lt;br/&gt;
         restoreConsumer.assign(assignment);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;StateRestorer&amp;gt; needsPositionUpdate = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (final StateRestorer restorer : initialized.values()) {&lt;br/&gt;
+&lt;br/&gt;
+        for (final TopicPartition partition : initialized) {&lt;br/&gt;
+            final StateRestorer restorer = stateRestorers.get(partition);&lt;br/&gt;
             if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) 
{
-                restoreConsumer.seek(restorer.partition(), restorer.checkpoint());
-                logRestoreOffsets(restorer.partition(),
-                                  restorer.checkpoint(),
-                                  endOffsets.get(restorer.partition()));
-                restorer.setStartingOffset(restoreConsumer.position(restorer.partition()));
+                restoreConsumer.seek(partition, restorer.checkpoint());
+                logRestoreOffsets(partition,
+                        restorer.checkpoint(),
+                        endOffsets.get(partition));
+                restorer.setStartingOffset(restoreConsumer.position(partition));
                 restorer.restoreStarted();
             }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;final StreamTask task = active.restoringTaskFor(restorer.partition());&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// If checkpoint does not exist it means the task was not shutdown gracefully before;&lt;/li&gt;
	&lt;li&gt;// and in this case if EOS is turned on we should wipe out the state and re-initialize the task&lt;/li&gt;
	&lt;li&gt;if (task.isEosEnabled()) {&lt;/li&gt;
	&lt;li&gt;log.info(&quot;No checkpoint found for task {} state store {} changelog {} with EOS turned on. &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;Reinitializing the task and restore its state from the beginning.&quot;, task.id, restorer.storeName(), restorer.partition());&lt;/li&gt;
	&lt;li&gt;task.reinitializeStateStoresForPartitions(Collections.singleton(restorer.partition()));&lt;/li&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;log.info(&quot;Restoring task {}&apos;s state store {} from beginning of the changelog {} &quot;, task.id, restorer.storeName(), restorer.partition());&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;restoreConsumer.seekToBeginning(Collections.singletonList(restorer.partition()));&lt;br/&gt;
+                restoreConsumer.seekToBeginning(Collections.singletonList(partition));&lt;br/&gt;
                 needsPositionUpdate.add(restorer);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         for (final StateRestorer restorer : needsPositionUpdate) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long position = restoreConsumer.position(restorer.partition());&lt;/li&gt;
	&lt;li&gt;logRestoreOffsets(restorer.partition(),&lt;/li&gt;
	&lt;li&gt;position,&lt;/li&gt;
	&lt;li&gt;endOffsets.get(restorer.partition()));&lt;/li&gt;
	&lt;li&gt;restorer.setStartingOffset(position);&lt;/li&gt;
	&lt;li&gt;restorer.restoreStarted();&lt;br/&gt;
+            final TopicPartition partition = restorer.partition();&lt;br/&gt;
+&lt;br/&gt;
+            // If checkpoint does not exist it means the task was not shutdown gracefully before;&lt;br/&gt;
+            // and in this case if EOS is turned on we should wipe out the state and re-initialize the task&lt;br/&gt;
+            final StreamTask task = active.restoringTaskFor(partition);&lt;br/&gt;
+            if (task.isEosEnabled()) {&lt;br/&gt;
+                log.info(&quot;No checkpoint found for task {} state store {} changelog {} with EOS turned on. &quot; +&lt;br/&gt;
+                        &quot;Reinitializing the task and restore its state from the beginning.&quot;, task.id, restorer.storeName(), partition);&lt;br/&gt;
+&lt;br/&gt;
+                needsInitializing.remove(partition);&lt;br/&gt;
+                initialized.remove(partition);&lt;br/&gt;
+                restorer.setCheckpointOffset(restoreConsumer.position(partition));&lt;br/&gt;
+&lt;br/&gt;
+                task.reinitializeStateStoresForPartitions(Collections.singleton(partition));&lt;br/&gt;
+            } else {&lt;br/&gt;
+                log.info(&quot;Restoring task {}&apos;s state store {} from beginning of the changelog {} &quot;, task.id, restorer.storeName(), partition);&lt;br/&gt;
+&lt;br/&gt;
+                final long position = restoreConsumer.position(restorer.partition());&lt;br/&gt;
+                logRestoreOffsets(restorer.partition(),&lt;br/&gt;
+                        position,&lt;br/&gt;
+                        endOffsets.get(restorer.partition()));&lt;br/&gt;
+                restorer.setStartingOffset(position);&lt;br/&gt;
+                restorer.restoreStarted();&lt;br/&gt;
+            }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;needsRestoring.putAll(initialized);&lt;br/&gt;
+        needsRestoring.addAll(initialized);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private void logRestoreOffsets(final TopicPartition partition,&lt;br/&gt;
@@ -226,10 +246,7 @@ private void logRestoreOffsets(final TopicPartition partition,&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private Collection&amp;lt;TopicPartition&amp;gt; completed() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Set&amp;lt;TopicPartition&amp;gt; completed = new HashSet&amp;lt;&amp;gt;(stateRestorers.keySet());&lt;/li&gt;
	&lt;li&gt;completed.removeAll(needsRestoring.keySet());&lt;/li&gt;
	&lt;li&gt;log.trace(&quot;The set of restoration completed partitions so far: {}&quot;, completed);&lt;/li&gt;
	&lt;li&gt;return completed;&lt;br/&gt;
+        return completedRestorers;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private void refreshChangelogInfo() {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index 31de839b1a1..428aa1d938b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -1129,7 +1129,7 @@ private void maybeUpdateStandbyTasks(final long now) &lt;/p&gt;
{
                         throw new TaskMigratedException(task);
                     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.info(&quot;Reinitializing StandbyTask {}&quot;, task);&lt;br/&gt;
+                    log.info(&quot;Reinitializing StandbyTask {} from changelogs {}&quot;, task, recoverableException.partitions());&lt;br/&gt;
                     task.reinitializeStateStoresForPartitions(recoverableException.partitions());&lt;br/&gt;
                 }&lt;br/&gt;
                 restoreConsumer.seekToBeginning(partitions);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
index 1e74d47808e..ae48f57db31 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
@@ -100,7 +100,11 @@ public void shouldRequestTopicsAndHandleTimeoutException() {&lt;br/&gt;
     public void shouldThrowExceptionIfConsumerHasCurrentSubscription() {&lt;br/&gt;
         final StateRestorer mockRestorer = EasyMock.mock(StateRestorer.class);&lt;br/&gt;
         mockRestorer.setUserRestoreListener(stateRestoreListener);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;expect(mockRestorer.partition()).andReturn(new TopicPartition(&quot;sometopic&quot;, 0)).andReturn(new TopicPartition(&quot;sometopic&quot;, 0));&lt;br/&gt;
+        expect(mockRestorer.partition())&lt;br/&gt;
+                .andReturn(new TopicPartition(&quot;sometopic&quot;, 0))&lt;br/&gt;
+                .andReturn(new TopicPartition(&quot;sometopic&quot;, 0))&lt;br/&gt;
+                .andReturn(new TopicPartition(&quot;sometopic&quot;, 0))&lt;br/&gt;
+                .andReturn(new TopicPartition(&quot;sometopic&quot;, 0));&lt;br/&gt;
         EasyMock.replay(mockRestorer);&lt;br/&gt;
         changelogReader.register(mockRestorer);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -144,6 +148,9 @@ public void shouldRecoverFromInvalidOffsetExceptionAndFinishRestore() {&lt;/p&gt;

&lt;p&gt;         // first restore call &quot;fails&quot; but we should not die with an exception&lt;br/&gt;
         assertEquals(0, changelogReader.restore(active).size());&lt;br/&gt;
+&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,&lt;br/&gt;
+                &quot;storeName&quot;));&lt;br/&gt;
         // retry restore should succeed&lt;br/&gt;
         assertEquals(1, changelogReader.restore(active).size());&lt;br/&gt;
         assertThat(callback.restored.size(), equalTo(messages));&lt;br/&gt;
@@ -226,9 +233,9 @@ public void shouldRestoreAndNotifyMultipleStores() throws Exception {&lt;br/&gt;
         setupConsumer(3, two);&lt;/p&gt;

&lt;p&gt;         changelogReader&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName1&quot;));&lt;/li&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(one, restoreListener1, null, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;/li&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(two, restoreListener2, null, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;br/&gt;
+            .register(new StateRestorer(topicPartition, restoreListener, 0L, Long.MAX_VALUE, true, &quot;storeName1&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(one, restoreListener1, 0L, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(two, restoreListener2, 0L, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         expect(active.restoringTaskFor(one)).andReturn(task);&lt;br/&gt;
         expect(active.restoringTaskFor(two)).andReturn(task);&lt;br/&gt;
@@ -257,7 +264,7 @@ public void shouldRestoreAndNotifyMultipleStores() throws Exception {&lt;br/&gt;
     public void shouldOnlyReportTheLastRestoredOffset() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;br/&gt;
         changelogReader&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName1&quot;));&lt;br/&gt;
+            .register(new StateRestorer(topicPartition, restoreListener, 0L, 5, true, &quot;storeName1&quot;));&lt;br/&gt;
         expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
         replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16611418" author="githubbot" created="Wed, 12 Sep 2018 00:56:54 +0000"  >&lt;p&gt;mjsax opened a new pull request #5641: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5641&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5641&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Unified backport of PRs #5421 and #5430&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16615257" author="githubbot" created="Fri, 14 Sep 2018 19:05:27 +0000"  >&lt;p&gt;mjsax closed pull request #5641: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5641&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5641&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java&lt;br/&gt;
index 04af9f269b7..c094b838a1c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java&lt;br/&gt;
@@ -193,4 +193,10 @@ public ThreadCache getCache() {&lt;br/&gt;
     public void initialized() &lt;/p&gt;
{
         initialized = true;
     }
&lt;p&gt;+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public void uninitialize() &lt;/p&gt;
{
+        initialized = false;
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index 7f6ac7ca614..f8f6416b3e5 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -226,6 +226,10 @@ void initStateStores() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    void reinitializeStateStoresForPartitions(final TopicPartition partitions) &lt;/p&gt;
{
+        stateMgr.reinitializeStateStoresForPartitions(partitions, processorContext);
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws ProcessorStateException if there is an error while closing the state manager&lt;/li&gt;
	&lt;li&gt;@param writeCheckpoint boolean indicating if a checkpoint file should be written&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
index d59ec2b2c1b..ad4868f8fce 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
@@ -298,7 +298,7 @@ private void describe(final StringBuilder builder,&lt;br/&gt;
         return suspended.values();&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collection&amp;lt;T&amp;gt; restoringTasks() {&lt;br/&gt;
+    public Collection&amp;lt;T&amp;gt; restoringTasks() 
{
         return Collections.unmodifiableCollection(restoring.values());
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java&lt;br/&gt;
index 5ebc34c4e92..e82ee2c354f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java&lt;br/&gt;
@@ -37,7 +37,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Restore all registered state stores by reading from their changelogs.&lt;/li&gt;
	&lt;li&gt;@return all topic partitions that have been restored&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collection&amp;lt;TopicPartition&amp;gt; restore();&lt;br/&gt;
+    Collection&amp;lt;TopicPartition&amp;gt; restore(final Collection&amp;lt;StreamTask&amp;gt; restoringTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return the restored offsets for all persistent stores.&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
index 57bb3ac81a6..b5719b111f0 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
@@ -53,4 +53,9 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Mark this contex as being initialized&lt;br/&gt;
      */&lt;br/&gt;
     void initialized();&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Mark this context as being uninitialized&lt;br/&gt;
+     */&lt;br/&gt;
+    void uninitialize();&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
index 34a87ce03be..93e7ffc5a06 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
 import org.apache.kafka.streams.errors.LockException;&lt;br/&gt;
 import org.apache.kafka.streams.errors.ProcessorStateException;&lt;br/&gt;
 import org.apache.kafka.streams.errors.StreamsException;&lt;br/&gt;
@@ -33,9 +34,11 @@&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Collection;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
+import java.util.HashSet;&lt;br/&gt;
 import java.util.LinkedHashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Set;&lt;/p&gt;


&lt;p&gt; public class ProcessorStateManager implements StateManager {&lt;br/&gt;
@@ -50,6 +53,7 @@&lt;br/&gt;
     private final String logPrefix;&lt;br/&gt;
     private final boolean isStandby;&lt;br/&gt;
     private final ChangelogReader changelogReader;&lt;br/&gt;
+    private final boolean eosEnabled;&lt;br/&gt;
     private final Map&amp;lt;String, StateStore&amp;gt; stores;&lt;br/&gt;
     private final Map&amp;lt;String, StateStore&amp;gt; globalStores;&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, Long&amp;gt; offsetLimits;&lt;br/&gt;
@@ -106,6 +110,7 @@ public ProcessorStateManager(final TaskId taskId,&lt;br/&gt;
         checkpoint = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));&lt;br/&gt;
         checkpointedOffsets = new HashMap&amp;lt;&amp;gt;(checkpoint.read());&lt;/p&gt;

&lt;p&gt;+        this.eosEnabled = eosEnabled;&lt;br/&gt;
         if (eosEnabled) &lt;/p&gt;
{
             // delete the checkpoint file after finish loading its stored offsets
             checkpoint.delete();
@@ -169,7 +174,8 @@ public void register(final StateStore store,
                                                              stateRestoreCallback,
                                                              checkpointedOffsets.get(storePartition),
                                                              offsetLimit(storePartition),
-                                                             store.persistent()
+                                                             store.persistent(),
+                                                             store.name()
             );
 
             changelogReader.register(restorer);
@@ -178,6 +184,61 @@ public void register(final StateStore store,
         stores.put(store.name(), store);
     }

&lt;p&gt;+    void reinitializeStateStoresForPartitions(final TopicPartition topicPartition,&lt;br/&gt;
+                                              final InternalProcessorContext processorContext) {&lt;br/&gt;
+        final Map&amp;lt;String, String&amp;gt; changelogTopicToStore = inverseOneToOneMap(storeToChangelogTopic);&lt;br/&gt;
+        final Set&amp;lt;String&amp;gt; storeToBeReinitialized = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, StateStore&amp;gt; storesCopy = new HashMap&amp;lt;&amp;gt;(stores);&lt;br/&gt;
+&lt;br/&gt;
+        checkpointedOffsets.remove(topicPartition);&lt;br/&gt;
+        storeToBeReinitialized.add(changelogTopicToStore.get(topicPartition.topic()));&lt;br/&gt;
+&lt;br/&gt;
+        if (!eosEnabled) {&lt;br/&gt;
+            try &lt;/p&gt;
{
+                checkpoint.write(checkpointedOffsets);
+            }
&lt;p&gt; catch (final IOException fatalException) {&lt;br/&gt;
+                log.error(&quot;Failed to write offset checkpoint file to {} while re-initializing {}: {}&quot;, checkpoint, stores, fatalException);&lt;br/&gt;
+                throw new StreamsException(&quot;Failed to reinitialize stores.&quot;, fatalException);&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, StateStore&amp;gt; entry : storesCopy.entrySet()) {&lt;br/&gt;
+            final StateStore stateStore = entry.getValue();&lt;br/&gt;
+            final String storeName = stateStore.name();&lt;br/&gt;
+            if (storeToBeReinitialized.contains(storeName)) {&lt;br/&gt;
+                try &lt;/p&gt;
{
+                    stateStore.close();
+                }
&lt;p&gt; catch (final RuntimeException ignoreAndSwallow) &lt;/p&gt;
{ /* ignore */ }
&lt;p&gt;+                processorContext.uninitialize();&lt;br/&gt;
+                stores.remove(entry.getKey());&lt;br/&gt;
+&lt;br/&gt;
+                try &lt;/p&gt;
{
+                    Utils.delete(new File(baseDir + File.separator + &quot;rocksdb&quot; + File.separator + storeName));
+                }
&lt;p&gt; catch (final IOException fatalException) {&lt;br/&gt;
+                    log.error(&quot;Failed to reinitialize store {}.&quot;, storeName, fatalException);&lt;br/&gt;
+                    throw new StreamsException(String.format(&quot;Failed to reinitialize store %s.&quot;, storeName), fatalException);&lt;br/&gt;
+                }&lt;br/&gt;
+&lt;br/&gt;
+                try &lt;/p&gt;
{
+                    Utils.delete(new File(baseDir + File.separator + storeName));
+                }
&lt;p&gt; catch (final IOException fatalException) {&lt;br/&gt;
+                    log.error(&quot;Failed to reinitialize store {}.&quot;, storeName, fatalException);&lt;br/&gt;
+                    throw new StreamsException(String.format(&quot;Failed to reinitialize store %s.&quot;, storeName), fatalException);&lt;br/&gt;
+                }&lt;br/&gt;
+&lt;br/&gt;
+                stateStore.init(processorContext, stateStore);&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private Map&amp;lt;String, String&amp;gt; inverseOneToOneMap(final Map&amp;lt;String, String&amp;gt; origin) {&lt;br/&gt;
+        final Map&amp;lt;String, String&amp;gt; reversedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, String&amp;gt; entry : origin.entrySet()) &lt;/p&gt;
{
+            reversedMap.put(entry.getValue(), entry.getKey());
+        }
&lt;p&gt;+        return reversedMap;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Override&lt;br/&gt;
     public Map&amp;lt;TopicPartition, Long&amp;gt; checkpointed() {&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; partitionsAndOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
index 79bfd1d0f49..3c5efe8a1e7 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
@@ -22,12 +22,13 @@&lt;br/&gt;
 public class StateRestorer {&lt;br/&gt;
     static final int NO_CHECKPOINT = -1;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Long checkpoint;&lt;br/&gt;
     private final long offsetLimit;&lt;br/&gt;
     private final boolean persistent;&lt;br/&gt;
+    private final String storeName;&lt;br/&gt;
     private final TopicPartition partition;&lt;br/&gt;
     private final StateRestoreCallback stateRestoreCallback;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private long checkpointOffset;&lt;br/&gt;
     private long restoredOffset;&lt;br/&gt;
     private long startingOffset;&lt;/p&gt;

&lt;p&gt;@@ -35,12 +36,14 @@&lt;br/&gt;
                   final StateRestoreCallback stateRestoreCallback,&lt;br/&gt;
                   final Long checkpoint,&lt;br/&gt;
                   final long offsetLimit,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final boolean persistent) {&lt;br/&gt;
+                  final boolean persistent,&lt;br/&gt;
+                  final String storeName) 
{
         this.partition = partition;
         this.stateRestoreCallback = stateRestoreCallback;
-        this.checkpoint = checkpoint;
+        this.checkpointOffset = checkpoint == null ? NO_CHECKPOINT : checkpoint;
         this.offsetLimit = offsetLimit;
         this.persistent = persistent;
+        this.storeName = storeName;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public TopicPartition partition() {&lt;br/&gt;
@@ -48,7 +51,15 @@ public TopicPartition partition() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     long checkpoint() &lt;/p&gt;
{
-        return checkpoint == null ? NO_CHECKPOINT : checkpoint;
+        return checkpointOffset;
+    }
&lt;p&gt;+&lt;br/&gt;
+    void setCheckpointOffset(final long checkpointOffset) &lt;/p&gt;
{
+        this.checkpointOffset = checkpointOffset;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public String storeName() &lt;/p&gt;
{
+        return storeName;
     }

&lt;p&gt;     void restore(final byte[] key, final byte[] value) {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
index 34dcb759d8a..305bf10997d 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
@@ -60,13 +60,16 @@ public StoreChangelogReader(final Consumer&amp;lt;byte[], byte[]&amp;gt; consumer) {&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public void register(final StateRestorer restorer) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stateRestorers.put(restorer.partition(), restorer);&lt;br/&gt;
+        if (!stateRestorers.containsKey(restorer.partition())) {&lt;br/&gt;
+            stateRestorers.put(restorer.partition(), restorer);&lt;br/&gt;
+            log.trace(&quot;Added restorer for changelog {}&quot;, restorer.partition());&lt;br/&gt;
+        }&lt;br/&gt;
         needsInitializing.put(restorer.partition(), restorer);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Collection&amp;lt;TopicPartition&amp;gt; restore() {&lt;br/&gt;
+    public Collection&amp;lt;TopicPartition&amp;gt; restore(final Collection&amp;lt;StreamTask&amp;gt; restoringTasks) {&lt;br/&gt;
         if (!needsInitializing.isEmpty()) 
{
-            initialize();
+            initialize(restoringTasks);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (needsRestoring.isEmpty()) {&lt;br/&gt;
@@ -87,7 +90,7 @@ public void register(final StateRestorer restorer) &lt;/p&gt;
{
         return completed();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void initialize() {&lt;br/&gt;
+    private void initialize(final Collection&amp;lt;StreamTask&amp;gt; restoringTasks) {&lt;br/&gt;
         if (!consumer.subscription().isEmpty()) 
{
             throw new IllegalStateException(&quot;Restore consumer should not be subscribed to any topics (&quot; + consumer.subscription() + &quot;)&quot;);
         }
&lt;p&gt;@@ -139,11 +142,12 @@ private void initialize() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // set up restorer for those initializable&lt;br/&gt;
         if (!initializable.isEmpty()) &lt;/p&gt;
{
-            startRestoration(initializable);
+            startRestoration(initializable, restoringTasks);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized) {&lt;br/&gt;
+    private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized,&lt;br/&gt;
+                                  final Collection&amp;lt;StreamTask&amp;gt; restoringTasks) {&lt;br/&gt;
         log.debug(&quot;{} Start restoring state stores from changelog topics {}&quot;, logPrefix, initialized.keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Set&amp;lt;TopicPartition&amp;gt; assignment = new HashSet&amp;lt;&amp;gt;(consumer.assignment());&lt;br/&gt;
@@ -152,24 +156,48 @@ private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializ&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;StateRestorer&amp;gt; needsPositionUpdate = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (final StateRestorer restorer : initialized.values()) {&lt;br/&gt;
+            final TopicPartition restoringPartition = restorer.partition();&lt;br/&gt;
             if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) &lt;/p&gt;
{
-                consumer.seek(restorer.partition(), restorer.checkpoint());
-                logRestoreOffsets(restorer.partition(),
-                        restorer.checkpoint(),
-                        endOffsets.get(restorer.partition()));
-                restorer.setStartingOffset(consumer.position(restorer.partition()));
+                consumer.seek(restoringPartition, restorer.checkpoint());
+                logRestoreOffsets(
+                    restoringPartition,
+                    restorer.checkpoint(),
+                    endOffsets.get(restoringPartition));
+                restorer.setStartingOffset(consumer.position(restoringPartition));
             }
&lt;p&gt; else &lt;/p&gt;
{
-                consumer.seekToBeginning(Collections.singletonList(restorer.partition()));
+                consumer.seekToBeginning(Collections.singletonList(restoringPartition));
                 needsPositionUpdate.add(restorer);
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;         for (final StateRestorer restorer : needsPositionUpdate) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long position = consumer.position(restorer.partition());&lt;/li&gt;
	&lt;li&gt;logRestoreOffsets(restorer.partition(),&lt;/li&gt;
	&lt;li&gt;position,&lt;/li&gt;
	&lt;li&gt;endOffsets.get(restorer.partition()));&lt;/li&gt;
	&lt;li&gt;restorer.setStartingOffset(position);&lt;br/&gt;
+            final TopicPartition restoringPartition = restorer.partition();&lt;br/&gt;
+&lt;br/&gt;
+            for (final StreamTask task : restoringTasks) {&lt;br/&gt;
+                if (task.changelogPartitions().contains(restoringPartition) || task.partitions().contains(restoringPartition)) {&lt;br/&gt;
+                    if (task.eosEnabled) {&lt;br/&gt;
+                        log.info(&quot;No checkpoint found for task {} state store {} changelog {} with EOS turned on. &quot; +&lt;br/&gt;
+                            &quot;Reinitializing the task and restore its state from the beginning.&quot;, task.id, restorer.storeName(), restorer.partition());&lt;br/&gt;
+&lt;br/&gt;
+                        needsInitializing.remove(restoringPartition);&lt;br/&gt;
+                        initialized.put(restoringPartition, restorer);&lt;br/&gt;
+                        restorer.setCheckpointOffset(consumer.position(restoringPartition));&lt;br/&gt;
+&lt;br/&gt;
+                        task.reinitializeStateStoresForPartitions(restoringPartition);&lt;br/&gt;
+                    } else {&lt;br/&gt;
+                        log.info(&quot;Restoring task {}&apos;s state store {} from beginning of the changelog {} &quot;, task.id, restorer.storeName(), restorer.partition());&lt;br/&gt;
+&lt;br/&gt;
+                        final long position = consumer.position(restoringPartition);&lt;br/&gt;
+                        logRestoreOffsets(&lt;br/&gt;
+                            restoringPartition,&lt;br/&gt;
+                            position,&lt;br/&gt;
+                            endOffsets.get(restoringPartition));&lt;br/&gt;
+                        restorer.setStartingOffset(position);&lt;br/&gt;
+                    }&lt;br/&gt;
+                }&lt;br/&gt;
+            }&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         needsRestoring.putAll(initialized);&lt;br/&gt;
@@ -220,7 +248,7 @@ public void reset() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private void restorePartition(final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; allRecords,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final TopicPartition topicPartition) {&lt;br/&gt;
+                                  final TopicPartition topicPartition) {&lt;br/&gt;
         final StateRestorer restorer = stateRestorers.get(topicPartition);&lt;br/&gt;
         final Long endOffset = endOffsets.get(topicPartition);&lt;br/&gt;
         final long pos = processNext(allRecords.records(topicPartition), restorer, endOffset);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index 210b070e852..6b7f4f1934e 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -532,7 +532,7 @@ private void tryTransitToRunning() {&lt;br/&gt;
         active.initializeNewTasks();&lt;br/&gt;
         standby.initializeNewTasks();&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Collection&amp;lt;TopicPartition&amp;gt; restored = storeChangelogReader.restore();&lt;br/&gt;
+        final Collection&amp;lt;TopicPartition&amp;gt; restored = storeChangelogReader.restore(active.restoringTasks());&lt;br/&gt;
         final Set&amp;lt;TopicPartition&amp;gt; resumed = active.updateRestored(restored);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (!resumed.isEmpty()) {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
index 0c3b36aa84b..7f709507e35 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
@@ -392,7 +392,7 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes&lt;br/&gt;
         // and store updates (ie, another 5 uncommitted writes to a changelog topic per partition)&lt;br/&gt;
         //&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// the failure gets inject after 20 committed and 30 uncommitted records got received&lt;br/&gt;
+        // the failure gets inject after 20 committed and 10 uncommitted records got received&lt;br/&gt;
         // -&amp;gt; the failure only kills one thread&lt;br/&gt;
         // after fail over, we should read 40 committed records and the state stores should contain the correct sums&lt;br/&gt;
         // per key (even if some records got processed twice)&lt;br/&gt;
@@ -402,7 +402,7 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
             streams.start();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; committedDataBeforeFailure = prepareData(0L, 10L, 0L, 1L);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L);&lt;br/&gt;
+            final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L, 2L, 3L);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; dataBeforeFailure = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             dataBeforeFailure.addAll(committedDataBeforeFailure);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java&lt;br/&gt;
index 6968f3319ec..3a8ebdaf22f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java&lt;br/&gt;
@@ -28,7 +28,7 @@&lt;/p&gt;

&lt;p&gt;     private static final long OFFSET_LIMIT = 50;&lt;br/&gt;
     private final MockRestoreCallback callback = new MockRestoreCallback();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final StateRestorer restorer = new StateRestorer(new TopicPartition(&quot;topic&quot;, 1), callback, null, OFFSET_LIMIT, true);&lt;br/&gt;
+    private final StateRestorer restorer = new StateRestorer(new TopicPartition(&quot;topic&quot;, 1), callback, null, OFFSET_LIMIT, true, &quot;store&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldCallRestoreOnRestoreCallback() throws Exception {&lt;br/&gt;
@@ -53,7 +53,7 @@ public void shouldBeCompletedIfEndOffsetAndRecordOffsetAreZero() throws Exceptio&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldBeCompletedIfOffsetAndOffsetLimitAreZero() throws Exception &lt;/p&gt;
{
-        final StateRestorer restorer = new StateRestorer(new TopicPartition(&quot;topic&quot;, 1), callback, null, 0, true);
+        final StateRestorer restorer = new StateRestorer(new TopicPartition(&quot;topic&quot;, 1), callback, null, 0, true, &quot;store&quot;);
         assertTrue(restorer.hasCompleted(0, 10));
     }

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
index a43f08344bf..24820f840b2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
@@ -59,8 +59,8 @@ public void shouldRequestTopicsAndHandleTimeoutException() throws Exception {&lt;br/&gt;
         };&lt;/p&gt;

&lt;p&gt;         final StoreChangelogReader changelogReader = new StoreChangelogReader(consumer);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));&lt;/li&gt;
	&lt;li&gt;changelogReader.restore();&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true, &quot;store&quot;));&lt;br/&gt;
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());&lt;br/&gt;
         assertTrue(functionCalled.get());&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -68,7 +68,7 @@ public void shouldRequestTopicsAndHandleTimeoutException() throws Exception {&lt;br/&gt;
     public void shouldThrowExceptionIfConsumerHasCurrentSubscription() throws Exception {&lt;br/&gt;
         consumer.subscribe(Collections.singleton(&quot;sometopic&quot;));&lt;br/&gt;
         try &lt;/p&gt;
{
-            changelogReader.restore();
+            changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
             fail(&quot;Should have thrown IllegalStateException&quot;);
         }
&lt;p&gt; catch (final IllegalStateException e) {&lt;br/&gt;
             // ok&lt;br/&gt;
@@ -79,9 +79,9 @@ public void shouldThrowExceptionIfConsumerHasCurrentSubscription() throws Except&lt;br/&gt;
     public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() throws Exception &lt;/p&gt;
{
         final int messages = 10;
         setupConsumer(messages, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true, &quot;store&quot;));
 
-        changelogReader.restore();
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         assertThat(callback.restored.size(), equalTo(messages));
     }

&lt;p&gt;@@ -89,9 +89,9 @@ public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() throws Exc&lt;br/&gt;
     public void shouldRestoreMessagesFromCheckpoint() throws Exception &lt;/p&gt;
{
         final int messages = 10;
         setupConsumer(messages, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, callback, 5L, Long.MAX_VALUE, true));
+        changelogReader.register(new StateRestorer(topicPartition, callback, 5L, Long.MAX_VALUE, true, &quot;store&quot;));
 
-        changelogReader.restore();
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         assertThat(callback.restored.size(), equalTo(5));
     }

&lt;p&gt;@@ -99,18 +99,18 @@ public void shouldRestoreMessagesFromCheckpoint() throws Exception {&lt;br/&gt;
     public void shouldClearAssignmentAtEndOfRestore() throws Exception &lt;/p&gt;
{
         final int messages = 1;
         setupConsumer(messages, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true, &quot;store&quot;));
 
-        changelogReader.restore();
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         assertThat(consumer.assignment(), equalTo(Collections.&amp;lt;TopicPartition&amp;gt;emptySet()));
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldRestoreToLimitWhenSupplied() throws Exception &lt;/p&gt;
{
         setupConsumer(10, topicPartition);
-        final StateRestorer restorer = new StateRestorer(topicPartition, callback, null, 3, true);
+        final StateRestorer restorer = new StateRestorer(topicPartition, callback, null, 3, true, &quot;store&quot;);
         changelogReader.register(restorer);
-        changelogReader.restore();
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         assertThat(callback.restored.size(), equalTo(3));
         assertThat(restorer.restoredOffset(), equalTo(3L));
     }
&lt;p&gt;@@ -125,11 +125,11 @@ public void shouldRestoreMultipleStores() throws Exception {&lt;br/&gt;
         setupConsumer(5, one);&lt;br/&gt;
         setupConsumer(3, two);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));&lt;/li&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(one, callbackOne, null, Long.MAX_VALUE, true));&lt;/li&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(two, callbackTwo, null, Long.MAX_VALUE, true));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true, &quot;store&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(one, callbackOne, null, Long.MAX_VALUE, true, &quot;store&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(two, callbackTwo, null, Long.MAX_VALUE, true, &quot;store&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.restore();&lt;br/&gt;
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored.size(), equalTo(10));&lt;br/&gt;
         assertThat(callbackOne.restored.size(), equalTo(5));&lt;br/&gt;
@@ -138,11 +138,11 @@ public void shouldRestoreMultipleStores() throws Exception {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldNotRestoreAnythingWhenPartitionIsEmpty() throws Exception &lt;/p&gt;
{
-        final StateRestorer restorer = new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true);
+        final StateRestorer restorer = new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true, &quot;store&quot;);
         setupConsumer(0, topicPartition);
         changelogReader.register(restorer);
 
-        changelogReader.restore();
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         assertThat(callback.restored.size(), equalTo(0));
         assertThat(restorer.restoredOffset(), equalTo(0L));
     }
&lt;p&gt;@@ -151,11 +151,11 @@ public void shouldNotRestoreAnythingWhenPartitionIsEmpty() throws Exception {&lt;br/&gt;
     public void shouldNotRestoreAnythingWhenCheckpointAtEndOffset() throws Exception &lt;/p&gt;
{
         final Long endOffset = 10L;
         setupConsumer(endOffset, topicPartition);
-        final StateRestorer restorer = new StateRestorer(topicPartition, callback, endOffset, Long.MAX_VALUE, true);
+        final StateRestorer restorer = new StateRestorer(topicPartition, callback, endOffset, Long.MAX_VALUE, true, &quot;store&quot;);
 
         changelogReader.register(restorer);
 
-        changelogReader.restore();
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         assertThat(callback.restored.size(), equalTo(0));
         assertThat(restorer.restoredOffset(), equalTo(endOffset));
     }
&lt;p&gt;@@ -163,8 +163,8 @@ public void shouldNotRestoreAnythingWhenCheckpointAtEndOffset() throws Exception&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldReturnRestoredOffsetsForPersistentStores() throws Exception &lt;/p&gt;
{
         setupConsumer(10, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
-        changelogReader.restore();
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true, &quot;store&quot;));
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();
         assertThat(restoredOffsets, equalTo(Collections.singletonMap(topicPartition, 10L)));
     }
&lt;p&gt;@@ -172,8 +172,8 @@ public void shouldReturnRestoredOffsetsForPersistentStores() throws Exception {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldNotReturnRestoredOffsetsForNonPersistentStore() throws Exception &lt;/p&gt;
{
         setupConsumer(10, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false));
-        changelogReader.restore();
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false, &quot;store&quot;));
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();
         assertThat(restoredOffsets, equalTo(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap()));
     }
&lt;p&gt;@@ -186,8 +186,8 @@ public void shouldIgnoreNullKeysWhenRestoring() throws Exception &lt;/p&gt;
{
         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), 1, (byte[]) null, bytes));
         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), 2, bytes, bytes));
         consumer.assign(Collections.singletonList(topicPartition));
-        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false));
-        changelogReader.restore();
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false, &quot;store&quot;));
+        changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
 
         assertThat(callback.restored, CoreMatchers.equalTo(Utils.mkList(KeyValue.pair(bytes, bytes), KeyValue.pair(bytes, bytes))));
     }
&lt;p&gt;@@ -200,15 +200,15 @@ public void shouldReturnCompletedPartitionsOnEachRestoreCall() &lt;/p&gt;
{
             consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), i, bytes, bytes));
         }&lt;br/&gt;
         consumer.assign(Collections.singletonList(topicPartition));&lt;br/&gt;
-        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false, &quot;store&quot;));&lt;br/&gt;
 &lt;br/&gt;
-        final Collection&amp;lt;TopicPartition&amp;gt; completedFirstTime = changelogReader.restore();&lt;br/&gt;
+        final Collection&amp;lt;TopicPartition&amp;gt; completedFirstTime = changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());&lt;br/&gt;
         assertTrue(completedFirstTime.isEmpty());&lt;br/&gt;
         for (int i = 5; i &amp;lt; 10; i++) {             consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), i, bytes, bytes));         }
&lt;p&gt;         final Collection&amp;lt;TopicPartition&amp;gt; expected = Collections.singleton(topicPartition);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertThat(changelogReader.restore(), equalTo(expected));&lt;br/&gt;
+        assertThat(changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet()), equalTo(expected));&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private void setupConsumer(final long messages, final TopicPartition topicPartition) {&lt;br/&gt;
@@ -237,8 +237,8 @@ private void assignPartition(final long messages, final TopicPartition topicPart&lt;br/&gt;
     public void shouldCompleteImmediatelyWhenEndOffsetIs0() &lt;/p&gt;
{
         final Collection&amp;lt;TopicPartition&amp;gt; expected = Collections.singleton(topicPartition);
         setupConsumer(0, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true));
-        final Collection&amp;lt;TopicPartition&amp;gt; restored = changelogReader.restore();
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, true, &quot;store&quot;));
+        final Collection&amp;lt;TopicPartition&amp;gt; restored = changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet());
         assertThat(restored, equalTo(expected));
     }

&lt;p&gt;@@ -248,9 +248,9 @@ public void shouldRestorePartitionsRegisteredPostInitialization() {&lt;/p&gt;

&lt;p&gt;         setupConsumer(1, topicPartition);&lt;br/&gt;
         consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 10L));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, callback, null, Long.MAX_VALUE, false, &quot;store&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(changelogReader.restore().isEmpty());&lt;br/&gt;
+        assertTrue(changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet()).isEmpty());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         addRecords(9, topicPartition, 1);&lt;/p&gt;

&lt;p&gt;@@ -259,12 +259,12 @@ public void shouldRestorePartitionsRegisteredPostInitialization() &lt;/p&gt;
{
         consumer.updateBeginningOffsets(Collections.singletonMap(postInitialization, 0L));
         consumer.updateEndOffsets(Collections.singletonMap(postInitialization, 3L));
 
-        changelogReader.register(new StateRestorer(postInitialization, callbackTwo, null, Long.MAX_VALUE, false));
+        changelogReader.register(new StateRestorer(postInitialization, callbackTwo, null, Long.MAX_VALUE, false, &quot;store&quot;));
 
         final Collection&amp;lt;TopicPartition&amp;gt; expected = Utils.mkSet(topicPartition, postInitialization);
         consumer.assign(expected);
 
-        assertThat(changelogReader.restore(), equalTo(expected));
+        assertThat(changelogReader.restore(Collections.&amp;lt;StreamTask&amp;gt;emptySet()), equalTo(expected));
         assertThat(callback.restored.size(), equalTo(10));
         assertThat(callbackTwo.restored.size(), equalTo(3));
     }
&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/test/MockChangelogReader.java b/streams/src/test/java/org/apache/kafka/test/MockChangelogReader.java&lt;br/&gt;
index 54fd858393c..93ce801be1c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/test/MockChangelogReader.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/test/MockChangelogReader.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.streams.processor.internals.ChangelogReader;&lt;br/&gt;
 import org.apache.kafka.streams.processor.internals.StateRestorer;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StreamTask;&lt;/p&gt;

&lt;p&gt; import java.util.Collection;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
@@ -35,7 +36,7 @@ public void register(final StateRestorer restorer) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Collection&amp;lt;TopicPartition&amp;gt; restore() {&lt;br/&gt;
+    public Collection&amp;lt;TopicPartition&amp;gt; restore(final Collection&amp;lt;StreamTask&amp;gt; restoringTasks) 
{
         return registered;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/test/MockProcessorContext.java b/streams/src/test/java/org/apache/kafka/test/MockProcessorContext.java&lt;br/&gt;
index cb56fa1b1a6..fed8752ba40 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/test/MockProcessorContext.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/test/MockProcessorContext.java&lt;br/&gt;
@@ -142,6 +142,9 @@ public ThreadCache getCache() {&lt;br/&gt;
     @Override&lt;br/&gt;
     public void initialized() {}&lt;/p&gt;

&lt;p&gt;+    @Override&lt;br/&gt;
+    public void uninitialize() {}&lt;br/&gt;
+&lt;br/&gt;
     @Override&lt;br/&gt;
     public File stateDir() {&lt;br/&gt;
         if (stateDir == null) {&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16615537" author="githubbot" created="Sat, 15 Sep 2018 00:38:21 +0000"  >&lt;p&gt;mjsax opened a new pull request #5657: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5657&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5657&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Unified backport of PRs #5421 and #5430&lt;/p&gt;

&lt;p&gt;   Also compare backport to `0.11.0` #5641&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16642553" author="githubbot" created="Mon, 8 Oct 2018 22:14:23 +0000"  >&lt;p&gt;mjsax closed pull request #5657: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5657&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5657&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java&lt;br/&gt;
index 410212e1ff0..eeaed893482 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java&lt;br/&gt;
@@ -198,4 +198,10 @@ public ThreadCache getCache() {&lt;br/&gt;
     public void initialized() &lt;/p&gt;
{
         initialized = true;
     }
&lt;p&gt;+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public void uninitialize() &lt;/p&gt;
{
+        initialized = false;
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index baea4af62f1..fcf2f6b13b7 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -107,7 +107,7 @@ public final String applicationId() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public final Set&amp;lt;TopicPartition&amp;gt; partitions() {&lt;br/&gt;
+    public Set&amp;lt;TopicPartition&amp;gt; partitions() 
{
         return partitions;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -226,6 +226,9 @@ void registerStateStores() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    void reinitializeStateStoresForPartitions(final TopicPartition partitions) &lt;/p&gt;
{
+        stateMgr.reinitializeStateStoresForPartitions(partitions, processorContext);
+    }

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws ProcessorStateException if there is an error while closing the state manager&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
index 0d9d04de5cc..cfce57588e0 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
@@ -50,7 +50,7 @@&lt;br/&gt;
     // IQ may access this map.&lt;br/&gt;
     private Map&amp;lt;TaskId, Task&amp;gt; running = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private Map&amp;lt;TopicPartition, Task&amp;gt; runningByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Map&amp;lt;TopicPartition, Task&amp;gt; restoringByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private Map&amp;lt;TopicPartition, StreamTask&amp;gt; restoringByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private int committed = 0;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;@@ -122,7 +122,8 @@ void addNewTask(final Task task) {&lt;br/&gt;
             try {&lt;br/&gt;
                 if (!entry.getValue().initializeStateStores()) {&lt;br/&gt;
                     log.debug(&quot;Transitioning {} {} to restoring&quot;, taskTypeName, entry.getKey());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;addToRestoring(entry.getValue());&lt;br/&gt;
+                    // cast is safe, because StandbyTasks always returns `true` in `initializeStateStores()` above&lt;br/&gt;
+                    addToRestoring((StreamTask) entry.getValue());&lt;br/&gt;
                 } else 
{
                     transitionToRunning(entry.getValue(), readyPartitions);
                 }
&lt;p&gt;@@ -278,7 +279,7 @@ boolean maybeResumeSuspendedTask(final TaskId taskId, final Set&amp;lt;TopicPartition&amp;gt;&lt;br/&gt;
         return false;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void addToRestoring(final Task task) {&lt;br/&gt;
+    private void addToRestoring(final StreamTask task) {&lt;br/&gt;
         restoring.put(task.id(), task);&lt;br/&gt;
         for (TopicPartition topicPartition : task.partitions()) 
{
             restoringByPartition.put(topicPartition, task);
@@ -307,7 +308,7 @@ private void transitionToRunning(final Task task, final Set&amp;lt;TopicPartition&amp;gt; read
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Task restoringTaskFor(final TopicPartition partition) {&lt;br/&gt;
+    public StreamTask restoringTaskFor(final TopicPartition partition) 
{
         return restoringByPartition.get(partition);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
index 57bb3ac81a6..b5719b111f0 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
@@ -53,4 +53,9 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Mark this contex as being initialized&lt;br/&gt;
      */&lt;br/&gt;
     void initialized();&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Mark this context as being uninitialized&lt;br/&gt;
+     */&lt;br/&gt;
+    void uninitialize();&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
index 5536ac12be7..9ccb458cf66 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
@@ -19,8 +19,10 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
 import org.apache.kafka.streams.errors.ProcessorStateException;&lt;br/&gt;
+import org.apache.kafka.streams.errors.StreamsException;&lt;br/&gt;
 import org.apache.kafka.streams.processor.BatchingStateRestoreCallback;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StateRestoreCallback;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StateStore;&lt;br/&gt;
@@ -33,9 +35,11 @@&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Collection;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
+import java.util.HashSet;&lt;br/&gt;
 import java.util.LinkedHashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Set;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt; public class ProcessorStateManager implements StateManager {&lt;br/&gt;
@@ -49,6 +53,7 @@&lt;br/&gt;
     private final String logPrefix;&lt;br/&gt;
     private final boolean isStandby;&lt;br/&gt;
     private final ChangelogReader changelogReader;&lt;br/&gt;
+    private final boolean eosEnabled;&lt;br/&gt;
     private final Map&amp;lt;String, StateStore&amp;gt; stores;&lt;br/&gt;
     private final Map&amp;lt;String, StateStore&amp;gt; globalStores;&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, Long&amp;gt; offsetLimits;&lt;br/&gt;
@@ -98,6 +103,7 @@ public ProcessorStateManager(final TaskId taskId,&lt;br/&gt;
         checkpoint = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));&lt;br/&gt;
         checkpointedOffsets = new HashMap&amp;lt;&amp;gt;(checkpoint.read());&lt;/p&gt;

&lt;p&gt;+        this.eosEnabled = eosEnabled;&lt;br/&gt;
         if (eosEnabled) &lt;/p&gt;
{
             // delete the checkpoint file after finish loading its stored offsets
             checkpoint.delete();
@@ -176,6 +182,61 @@ public void register(final StateStore store,
         return partitionsAndOffsets;
     }

&lt;p&gt;+    void reinitializeStateStoresForPartitions(final TopicPartition topicPartition,&lt;br/&gt;
+                                              final InternalProcessorContext processorContext) {&lt;br/&gt;
+        final Map&amp;lt;String, String&amp;gt; changelogTopicToStore = inverseOneToOneMap(storeToChangelogTopic);&lt;br/&gt;
+        final Set&amp;lt;String&amp;gt; storeToBeReinitialized = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, StateStore&amp;gt; storesCopy = new HashMap&amp;lt;&amp;gt;(stores);&lt;br/&gt;
+&lt;br/&gt;
+        checkpointedOffsets.remove(topicPartition);&lt;br/&gt;
+        storeToBeReinitialized.add(changelogTopicToStore.get(topicPartition.topic()));&lt;br/&gt;
+&lt;br/&gt;
+        if (!eosEnabled) {&lt;br/&gt;
+            try &lt;/p&gt;
{
+                checkpoint.write(checkpointedOffsets);
+            }
&lt;p&gt; catch (final IOException fatalException) {&lt;br/&gt;
+                log.error(&quot;Failed to write offset checkpoint file to {} while re-initializing {}: {}&quot;, checkpoint, stores, fatalException);&lt;br/&gt;
+                throw new StreamsException(&quot;Failed to reinitialize stores.&quot;, fatalException);&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, StateStore&amp;gt; entry : storesCopy.entrySet()) {&lt;br/&gt;
+            final StateStore stateStore = entry.getValue();&lt;br/&gt;
+            final String storeName = stateStore.name();&lt;br/&gt;
+            if (storeToBeReinitialized.contains(storeName)) {&lt;br/&gt;
+                try &lt;/p&gt;
{
+                    stateStore.close();
+                }
&lt;p&gt; catch (final RuntimeException ignoreAndSwallow) &lt;/p&gt;
{ /* ignore */ }
&lt;p&gt;+                processorContext.uninitialize();&lt;br/&gt;
+                stores.remove(entry.getKey());&lt;br/&gt;
+&lt;br/&gt;
+                try &lt;/p&gt;
{
+                    Utils.delete(new File(baseDir + File.separator + &quot;rocksdb&quot; + File.separator + storeName));
+                }
&lt;p&gt; catch (final IOException fatalException) {&lt;br/&gt;
+                    log.error(&quot;Failed to reinitialize store {}.&quot;, storeName, fatalException);&lt;br/&gt;
+                    throw new StreamsException(String.format(&quot;Failed to reinitialize store %s.&quot;, storeName), fatalException);&lt;br/&gt;
+                }&lt;br/&gt;
+&lt;br/&gt;
+                try &lt;/p&gt;
{
+                    Utils.delete(new File(baseDir + File.separator + storeName));
+                }
&lt;p&gt; catch (final IOException fatalException) {&lt;br/&gt;
+                    log.error(&quot;Failed to reinitialize store {}.&quot;, storeName, fatalException);&lt;br/&gt;
+                    throw new StreamsException(String.format(&quot;Failed to reinitialize store %s.&quot;, storeName), fatalException);&lt;br/&gt;
+                }&lt;br/&gt;
+&lt;br/&gt;
+                stateStore.init(processorContext, stateStore);&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private Map&amp;lt;String, String&amp;gt; inverseOneToOneMap(final Map&amp;lt;String, String&amp;gt; origin) {&lt;br/&gt;
+        final Map&amp;lt;String, String&amp;gt; reversedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, String&amp;gt; entry : origin.entrySet()) &lt;/p&gt;
{
+            reversedMap.put(entry.getValue(), entry.getKey());
+        }
&lt;p&gt;+        return reversedMap;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; updateStandbyStates(final TopicPartition storePartition,&lt;br/&gt;
                                                              final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records) {&lt;br/&gt;
         final long limit = offsetLimit(storePartition);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RestoringTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RestoringTasks.java&lt;br/&gt;
index 6ed28fdf63c..3671b493f19 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RestoringTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RestoringTasks.java&lt;br/&gt;
@@ -19,5 +19,5 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;/p&gt;

&lt;p&gt; public interface RestoringTasks &lt;/p&gt;
{
-    Task restoringTaskFor(final TopicPartition partition);
+    StreamTask restoringTaskFor(final TopicPartition partition);
 }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
index 33dce9e7558..e0bac939b8b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
@@ -26,13 +26,13 @@&lt;/p&gt;

&lt;p&gt;     static final int NO_CHECKPOINT = -1;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Long checkpoint;&lt;br/&gt;
     private final long offsetLimit;&lt;br/&gt;
     private final boolean persistent;&lt;br/&gt;
     private final String storeName;&lt;br/&gt;
     private final TopicPartition partition;&lt;br/&gt;
     private final CompositeRestoreListener compositeRestoreListener;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private long checkpointOffset;&lt;br/&gt;
     private long restoredOffset;&lt;br/&gt;
     private long startingOffset;&lt;br/&gt;
     private long endingOffset;&lt;br/&gt;
@@ -45,7 +45,7 @@&lt;br/&gt;
                   final String storeName) {&lt;br/&gt;
         this.partition = partition;&lt;br/&gt;
         this.compositeRestoreListener = compositeRestoreListener;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.checkpoint = checkpoint;&lt;br/&gt;
+        this.checkpointOffset = checkpoint == null ? NO_CHECKPOINT : checkpoint;&lt;br/&gt;
         this.offsetLimit = offsetLimit;&lt;br/&gt;
         this.persistent = persistent;&lt;br/&gt;
         this.storeName = storeName;&lt;br/&gt;
@@ -56,7 +56,15 @@ public TopicPartition partition() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     long checkpoint() &lt;/p&gt;
{
-        return checkpoint == null ? NO_CHECKPOINT : checkpoint;
+        return checkpointOffset;
+    }
&lt;p&gt;+&lt;br/&gt;
+    void setCheckpointOffset(final long checkpointOffset) &lt;/p&gt;
{
+        this.checkpointOffset = checkpointOffset;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public String storeName() &lt;/p&gt;
{
+        return storeName;
     }

&lt;p&gt;     void restoreStarted() {&lt;br/&gt;
@@ -67,7 +75,8 @@ void restoreDone() &lt;/p&gt;
{
         compositeRestoreListener.onRestoreEnd(partition, storeName, restoredNumRecords());
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void restoreBatchCompleted(long currentRestoredOffset, int numRestored) {&lt;br/&gt;
+    void restoreBatchCompleted(final long currentRestoredOffset,&lt;br/&gt;
+                               final int numRestored) 
{
         compositeRestoreListener.onBatchRestored(partition, storeName, currentRestoredOffset, numRestored);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -79,7 +88,7 @@ boolean isPersistent() &lt;/p&gt;
{
         return persistent;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void setUserRestoreListener(StateRestoreListener userRestoreListener) {&lt;br/&gt;
+    void setUserRestoreListener(final StateRestoreListener userRestoreListener) 
{
         this.compositeRestoreListener.setUserRestoreListener(userRestoreListener);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
index 8d85b1d8faf..34350c17eb0 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
@@ -69,7 +69,7 @@ public void register(final StateRestorer restorer) {&lt;br/&gt;
      */&lt;br/&gt;
     public Collection&amp;lt;TopicPartition&amp;gt; restore(final RestoringTasks active) {&lt;br/&gt;
         if (!needsInitializing.isEmpty()) &lt;/p&gt;
{
-            initialize();
+            initialize(active);
         }

&lt;p&gt;         if (needsRestoring.isEmpty()) {&lt;br/&gt;
@@ -90,7 +90,7 @@ public void register(final StateRestorer restorer) &lt;/p&gt;
{
         return completed();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void initialize() {&lt;br/&gt;
+    private void initialize(final RestoringTasks active) {&lt;br/&gt;
         if (!consumer.subscription().isEmpty()) 
{
             throw new IllegalStateException(&quot;Restore consumer should not be subscribed to any topics (&quot; + consumer.subscription() + &quot;)&quot;);
         }
&lt;p&gt;@@ -99,8 +99,8 @@ private void initialize() {&lt;br/&gt;
         // the needsInitializing map is not empty, meaning we do not know the metadata for some of them yet&lt;br/&gt;
         refreshChangelogInfo();&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializable = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, StateRestorer&amp;gt; entry : needsInitializing.entrySet()) {&lt;br/&gt;
+        final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializable = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TopicPartition, StateRestorer&amp;gt; entry : needsInitializing.entrySet()) {&lt;br/&gt;
             final TopicPartition topicPartition = entry.getKey();&lt;br/&gt;
             if (hasPartition(topicPartition)) {&lt;br/&gt;
                 initializable.put(entry.getKey(), entry.getValue());&lt;br/&gt;
@@ -144,11 +144,12 @@ private void initialize() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // set up restorer for those initializable&lt;br/&gt;
         if (!initializable.isEmpty()) &lt;/p&gt;
{
-            startRestoration(initializable);
+            startRestoration(initializable, active);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized) {&lt;br/&gt;
+    private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized,&lt;br/&gt;
+                                  final RestoringTasks active) {&lt;br/&gt;
         log.debug(&quot;Start restoring state stores from changelog topics {}&quot;, initialized.keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Set&amp;lt;TopicPartition&amp;gt; assignment = new HashSet&amp;lt;&amp;gt;(consumer.assignment());&lt;br/&gt;
@@ -157,26 +158,47 @@ private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializ&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;StateRestorer&amp;gt; needsPositionUpdate = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (final StateRestorer restorer : initialized.values()) {&lt;br/&gt;
+            final TopicPartition restoringPartition = restorer.partition();&lt;br/&gt;
             if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) &lt;/p&gt;
{
-                consumer.seek(restorer.partition(), restorer.checkpoint());
-                logRestoreOffsets(restorer.partition(),
-                        restorer.checkpoint(),
-                        endOffsets.get(restorer.partition()));
-                restorer.setStartingOffset(consumer.position(restorer.partition()));
+                consumer.seek(restoringPartition, restorer.checkpoint());
+                logRestoreOffsets(restoringPartition,
+                                  restorer.checkpoint(),
+                                  endOffsets.get(restoringPartition));
+                restorer.setStartingOffset(consumer.position(restoringPartition));
                 restorer.restoreStarted();
             }
&lt;p&gt; else &lt;/p&gt;
{
-                consumer.seekToBeginning(Collections.singletonList(restorer.partition()));
+                consumer.seekToBeginning(Collections.singletonList(restoringPartition));
                 needsPositionUpdate.add(restorer);
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;         for (final StateRestorer restorer : needsPositionUpdate) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long position = consumer.position(restorer.partition());&lt;/li&gt;
	&lt;li&gt;logRestoreOffsets(restorer.partition(),&lt;/li&gt;
	&lt;li&gt;position,&lt;/li&gt;
	&lt;li&gt;endOffsets.get(restorer.partition()));&lt;/li&gt;
	&lt;li&gt;restorer.setStartingOffset(position);&lt;/li&gt;
	&lt;li&gt;restorer.restoreStarted();&lt;br/&gt;
+            final TopicPartition restoringPartition = restorer.partition();&lt;br/&gt;
+            final StreamTask task = active.restoringTaskFor(restoringPartition);&lt;br/&gt;
+&lt;br/&gt;
+            // If checkpoint does not exist it means the task was not shutdown gracefully before;&lt;br/&gt;
+            // and in this case if EOS is turned on we should wipe out the state and re-initialize the task&lt;br/&gt;
+            if (task.eosEnabled) {&lt;br/&gt;
+                log.info(&quot;No checkpoint found for task {} state store {} changelog {} with EOS turned on. &quot; +&lt;br/&gt;
+                    &quot;Reinitializing the task and restore its state from the beginning.&quot;, task.id, restorer.storeName(), restoringPartition);&lt;br/&gt;
+&lt;br/&gt;
+                // we move the partitions here, because they will be added back within&lt;br/&gt;
+                // `task.reinitializeStateStoresForPartitions()` that calls `register()` internally again&lt;br/&gt;
+                needsInitializing.remove(restoringPartition);&lt;br/&gt;
+                restorer.setCheckpointOffset(consumer.position(restoringPartition));&lt;br/&gt;
+&lt;br/&gt;
+                task.reinitializeStateStoresForPartitions(restoringPartition);&lt;br/&gt;
+                stateRestorers.get(restoringPartition).restoreStarted();&lt;br/&gt;
+            } else {&lt;br/&gt;
+                log.info(&quot;Restoring task {}&apos;s state store {} from beginning of the changelog {} &quot;, task.id, restorer.storeName(), restoringPartition);&lt;br/&gt;
+&lt;br/&gt;
+                final long position = consumer.position(restoringPartition);&lt;br/&gt;
+                logRestoreOffsets(restoringPartition,&lt;br/&gt;
+                                  position,&lt;br/&gt;
+                                  endOffsets.get(restoringPartition));&lt;br/&gt;
+                restorer.setStartingOffset(position);&lt;br/&gt;
+                restorer.restoreStarted();&lt;br/&gt;
+            }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         needsRestoring.putAll(initialized);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
index 6c7b2b43c9d..d07781f5b24 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
@@ -392,7 +392,7 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes&lt;br/&gt;
         // and store updates (ie, another 5 uncommitted writes to a changelog topic per partition)&lt;br/&gt;
         //&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// the failure gets inject after 20 committed and 30 uncommitted records got received&lt;br/&gt;
+        // the failure gets inject after 20 committed and 10 uncommitted records got received&lt;br/&gt;
         // -&amp;gt; the failure only kills one thread&lt;br/&gt;
         // after fail over, we should read 40 committed records and the state stores should contain the correct sums&lt;br/&gt;
         // per key (even if some records got processed twice)&lt;br/&gt;
@@ -402,7 +402,7 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
             streams.start();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; committedDataBeforeFailure = prepareData(0L, 10L, 0L, 1L);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L);&lt;br/&gt;
+            final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L, 2L, 3L);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; dataBeforeFailure = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             dataBeforeFailure.addAll(committedDataBeforeFailure);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedTasksTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedTasksTest.java&lt;br/&gt;
index 5c8b7c44033..01439add706 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedTasksTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedTasksTest.java&lt;br/&gt;
@@ -40,8 +40,8 @@&lt;/p&gt;

&lt;p&gt; public class AssignedTasksTest {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Task t1 = EasyMock.createMock(Task.class);&lt;/li&gt;
	&lt;li&gt;private final Task t2 = EasyMock.createMock(Task.class);&lt;br/&gt;
+    private final StreamTask t1 = EasyMock.createMock(StreamTask.class);&lt;br/&gt;
+    private final StreamTask t2 = EasyMock.createMock(StreamTask.class);&lt;br/&gt;
     private final TopicPartition tp1 = new TopicPartition(&quot;t1&quot;, 0);&lt;br/&gt;
     private final TopicPartition tp2 = new TopicPartition(&quot;t2&quot;, 0);&lt;br/&gt;
     private final TopicPartition changeLog1 = new TopicPartition(&quot;cl1&quot;, 0);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
index bb0c51e1546..a0e2140e92d 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
@@ -59,7 +59,7 @@&lt;br/&gt;
     @Mock(type = MockType.NICE)&lt;br/&gt;
     private RestoringTasks active;&lt;br/&gt;
     @Mock(type = MockType.NICE)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;private Task task;&lt;br/&gt;
+    private StreamTask task;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final MockStateRestoreListener callback = new MockStateRestoreListener();&lt;br/&gt;
     private final CompositeRestoreListener restoreListener = new CompositeRestoreListener(callback);&lt;br/&gt;
@@ -107,6 +107,10 @@ public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() &lt;/p&gt;
{
         final int messages = 10;
         setupConsumer(messages, topicPartition);
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);
+        replay(active, task);
+
         changelogReader.restore(active);
         assertThat(callback.restored.size(), equalTo(messages));
     }
&lt;p&gt;@@ -115,8 +119,7 @@ public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() {&lt;br/&gt;
     public void shouldRestoreMessagesFromCheckpoint() {&lt;br/&gt;
         final int messages = 10;&lt;br/&gt;
         setupConsumer(messages, topicPartition);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, restoreListener, 5L, Long.MAX_VALUE, true,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, 5L, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         changelogReader.restore(active);&lt;br/&gt;
         assertThat(callback.restored.size(), equalTo(5));&lt;br/&gt;
@@ -126,9 +129,9 @@ public void shouldRestoreMessagesFromCheckpoint() {&lt;br/&gt;
     public void shouldClearAssignmentAtEndOfRestore() &lt;/p&gt;
{
         final int messages = 1;
         setupConsumer(messages, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,
-                                                   &quot;storeName&quot;));
-
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);
+        replay(active, task);
         changelogReader.restore(active);
         assertThat(consumer.assignment(), equalTo(Collections.&amp;lt;TopicPartition&amp;gt;emptySet()));
     }
&lt;p&gt;@@ -136,9 +139,10 @@ public void shouldClearAssignmentAtEndOfRestore() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldRestoreToLimitWhenSupplied() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, null, 3, true,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;);&lt;br/&gt;
+        final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, null, 3, true, &quot;storeName&quot;);&lt;br/&gt;
         changelogReader.register(restorer);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         assertThat(callback.restored.size(), equalTo(3));&lt;br/&gt;
         assertThat(restorer.restoredOffset(), equalTo(3L));&lt;br/&gt;
@@ -156,14 +160,14 @@ public void shouldRestoreMultipleStores() {&lt;br/&gt;
         setupConsumer(5, one);&lt;br/&gt;
         setupConsumer(3, two);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader&lt;/li&gt;
	&lt;li&gt;.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName1&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName1&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(one, restoreListener1, null, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(two, restoreListener2, null, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(one)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(two)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;replay(active);&lt;br/&gt;
+        expect(active.restoringTaskFor(one)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(two)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored.size(), equalTo(10));&lt;br/&gt;
@@ -188,9 +192,10 @@ public void shouldRestoreAndNotifyMultipleStores() throws Exception {&lt;br/&gt;
         changelogReader.register(new StateRestorer(one, restoreListener1, null, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(two, restoreListener2, null, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(one)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(two)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;replay(active);&lt;br/&gt;
+        expect(active.restoringTaskFor(one)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(two)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored.size(), equalTo(10));&lt;br/&gt;
@@ -210,8 +215,10 @@ public void shouldRestoreAndNotifyMultipleStores() throws Exception {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldOnlyReportTheLastRestoredOffset() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader&lt;/li&gt;
	&lt;li&gt;.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName1&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName1&quot;));&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
+&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored.size(), equalTo(5));&lt;br/&gt;
@@ -270,6 +277,8 @@ public void shouldNotRestoreAnythingWhenCheckpointAtEndOffset() {&lt;br/&gt;
     public void shouldReturnRestoredOffsetsForPersistentStores() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();&lt;br/&gt;
         assertThat(restoredOffsets, equalTo(Collections.singletonMap(topicPartition, 10L)));&lt;br/&gt;
@@ -279,6 +288,8 @@ public void shouldReturnRestoredOffsetsForPersistentStores() {&lt;br/&gt;
     public void shouldNotReturnRestoredOffsetsForNonPersistentStore() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();&lt;br/&gt;
         assertThat(restoredOffsets, equalTo(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap()));&lt;br/&gt;
@@ -292,8 +303,9 @@ public void shouldIgnoreNullKeysWhenRestoring() {&lt;br/&gt;
         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), 1, (byte[]) null, bytes));&lt;br/&gt;
         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), 2, bytes, bytes));&lt;br/&gt;
         consumer.assign(Collections.singletonList(topicPartition));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored, CoreMatchers.equalTo(Utils.mkList(KeyValue.pair(bytes, bytes), KeyValue.pair(bytes, bytes))));&lt;br/&gt;
@@ -318,10 +330,11 @@ public void shouldRestorePartitionsRegisteredPostInitialization() {&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;/p&gt;

&lt;p&gt;         final TopicPartition postInitialization = new TopicPartition(&quot;other&quot;, 0);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(postInitialization)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;replay(active);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(postInitialization)).andStubReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertTrue(changelogReader.restore(active).isEmpty());&lt;/p&gt;

&lt;p&gt;@@ -348,7 +361,7 @@ public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestore&lt;br/&gt;
         consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 5L));&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
         replay(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         try {&lt;br/&gt;
@@ -371,8 +384,8 @@ public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestore&lt;br/&gt;
         consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 6L));&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;/li&gt;
	&lt;li&gt;replay(active);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);&lt;br/&gt;
+        replay(active, task);&lt;br/&gt;
         try {&lt;br/&gt;
             changelogReader.restore(active);&lt;br/&gt;
             fail(&quot;Should have thrown task migrated exception&quot;);&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16644133" author="githubbot" created="Tue, 9 Oct 2018 21:39:04 +0000"  >&lt;p&gt;mjsax opened a new pull request #5767: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5767&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5767&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Unified backport of PRs #5421 and #5430&lt;/p&gt;

&lt;p&gt;   Also compare backport to `0.11.0` #5641 and `1.0` #5657&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16668859" author="tobiajo" created="Tue, 30 Oct 2018 15:16:28 +0000"  >&lt;p&gt;Will this be fixed in Kafka v1.1.2?&lt;/p&gt;</comment>
                            <comment id="16669064" author="mjsax" created="Tue, 30 Oct 2018 17:12:51 +0000"  >&lt;p&gt;I hope so. Did not have time yet, to port the fix into 1.1 branch.&lt;/p&gt;</comment>
                            <comment id="16671223" author="tobiajo" created="Thu, 1 Nov 2018 07:55:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; thanks! Is the issue only on client side (would updated client lib fix it)? And would disabling RocksDB mitigate the issue in the meantime? I haven&apos;t been able to re-produce the issue with RocksDB disabled.&lt;/p&gt;</comment>
                            <comment id="16672226" author="mjsax" created="Thu, 1 Nov 2018 21:17:56 +0000"  >&lt;p&gt;Yes, it&apos;s a client side bug and related to RocksDB. If you run with in-memory store, you won&apos;t see this issue.&lt;/p&gt;</comment>
                            <comment id="16687443" author="githubbot" created="Thu, 15 Nov 2018 03:40:08 +0000"  >&lt;p&gt;mjsax opened a new pull request #5915: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5915&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5915&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Follow up PR to #5657. Fixes buggy back port.&lt;/p&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16689059" author="githubbot" created="Fri, 16 Nov 2018 06:01:53 +0000"  >&lt;p&gt;guozhangwang closed pull request #5915: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5915&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5915&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
index 34350c17eb0..c03de2d4a2d 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
@@ -59,9 +59,14 @@ public StoreChangelogReader(final Consumer&amp;lt;byte[], byte[]&amp;gt; consumer,&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public void register(final StateRestorer restorer) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;restorer.setUserRestoreListener(userStateRestoreListener);&lt;/li&gt;
	&lt;li&gt;stateRestorers.put(restorer.partition(), restorer);&lt;/li&gt;
	&lt;li&gt;needsInitializing.put(restorer.partition(), restorer);&lt;br/&gt;
+        final StateRestorer existingRestorer = stateRestorers.get(restorer.partition());&lt;br/&gt;
+        if (existingRestorer == null) 
{
+            restorer.setUserRestoreListener(userStateRestoreListener);
+            stateRestorers.put(restorer.partition(), restorer);
+            needsInitializing.put(restorer.partition(), restorer);
+        }
&lt;p&gt; else &lt;/p&gt;
{
+            needsInitializing.put(restorer.partition(), existingRestorer);
+        }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -188,7 +193,6 @@ private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializ&lt;br/&gt;
                 restorer.setCheckpointOffset(consumer.position(restoringPartition));&lt;/p&gt;

&lt;p&gt;                 task.reinitializeStateStoresForPartitions(restoringPartition);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stateRestorers.get(restoringPartition).restoreStarted();&lt;br/&gt;
             } else {&lt;br/&gt;
                 log.info(&quot;Restoring task {}&apos;s state store {} from beginning of the changelog {} &quot;, task.id, restorer.storeName(), restoringPartition);&lt;/li&gt;
&lt;/ul&gt;






&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16692414" author="githubbot" created="Mon, 19 Nov 2018 23:21:06 +0000"  >&lt;p&gt;mjsax closed pull request #5767: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Wipe out state store if EOS is turned on and checkpoint file does not exist&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5767&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5767&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java&lt;br/&gt;
index a99e45147b9..40ce79c289e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java&lt;br/&gt;
@@ -24,4 +24,8 @@&lt;br/&gt;
         super(logContext, &quot;standby task&quot;);&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    void addToRestoring(final StandbyTask task) &lt;/p&gt;
{
+        throw new UnsupportedOperationException(&quot;Standby tasks cannot be restored actively.&quot;);
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
index 7b05f6488e7..98c2bbd2563 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
@@ -23,12 +23,21 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
 import org.slf4j.Logger;&lt;/p&gt;

&lt;p&gt;+import java.util.Collection;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
+import java.util.HashSet;&lt;br/&gt;
 import java.util.Iterator;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Set;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicReference;&lt;/p&gt;

&lt;p&gt; class AssignedStreamsTasks extends AssignedTasks&amp;lt;StreamTask&amp;gt; implements RestoringTasks {&lt;br/&gt;
     private final Logger log;&lt;br/&gt;
+    private final Map&amp;lt;TaskId, StreamTask&amp;gt; restoring = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Set&amp;lt;TopicPartition&amp;gt; restoredPartitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Map&amp;lt;TopicPartition, StreamTask&amp;gt; restoringByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final TaskAction&amp;lt;StreamTask&amp;gt; maybeCommitAction;&lt;br/&gt;
     private int committed = 0;&lt;/p&gt;

&lt;p&gt;@@ -59,6 +68,52 @@ public StreamTask restoringTaskFor(final TopicPartition partition) &lt;/p&gt;
{
         return restoringByPartition.get(partition);
     }

&lt;p&gt;+    void updateRestored(final Collection&amp;lt;TopicPartition&amp;gt; restored) {&lt;br/&gt;
+        if (restored.isEmpty()) &lt;/p&gt;
{
+            return;
+        }
&lt;p&gt;+        log.trace(&quot;Stream task changelog partitions that have completed restoring so far: {}&quot;, restored);&lt;br/&gt;
+        restoredPartitions.addAll(restored);&lt;br/&gt;
+        for (final Iterator&amp;lt;Map.Entry&amp;lt;TaskId, StreamTask&amp;gt;&amp;gt; it = restoring.entrySet().iterator(); it.hasNext(); ) {&lt;br/&gt;
+            final Map.Entry&amp;lt;TaskId, StreamTask&amp;gt; entry = it.next();&lt;br/&gt;
+            final StreamTask task = entry.getValue();&lt;br/&gt;
+            if (restoredPartitions.containsAll(task.changelogPartitions())) {&lt;br/&gt;
+                transitionToRunning(task);&lt;br/&gt;
+                it.remove();&lt;br/&gt;
+                log.trace(&quot;Stream task {} completed restoration as all its changelog partitions {} have been applied to restore state&quot;,&lt;br/&gt;
+                    task.id(),&lt;br/&gt;
+                    task.changelogPartitions());&lt;br/&gt;
+            } else {&lt;br/&gt;
+                if (log.isTraceEnabled()) {&lt;br/&gt;
+                    final HashSet&amp;lt;TopicPartition&amp;gt; outstandingPartitions = new HashSet&amp;lt;&amp;gt;(task.changelogPartitions());&lt;br/&gt;
+                    outstandingPartitions.removeAll(restoredPartitions);&lt;br/&gt;
+                    log.trace(&quot;Stream task {} cannot resume processing yet since some of its changelog partitions have not completed restoring: {}&quot;,&lt;br/&gt;
+                        task.id(),&lt;br/&gt;
+                        outstandingPartitions);&lt;br/&gt;
+                }&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+        if (allTasksRunning()) &lt;/p&gt;
{
+            restoredPartitions.clear();
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    void addToRestoring(final StreamTask task) {&lt;br/&gt;
+        restoring.put(task.id(), task);&lt;br/&gt;
+        for (final TopicPartition topicPartition : task.partitions()) &lt;/p&gt;
{
+            restoringByPartition.put(topicPartition, task);
+        }&lt;br/&gt;
+        for (final TopicPartition topicPartition : task.changelogPartitions()) {+            restoringByPartition.put(topicPartition, task);+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    boolean allTasksRunning() &lt;/p&gt;
{
+        return created.isEmpty()
+            &amp;amp;&amp;amp; suspended.isEmpty()
+            &amp;amp;&amp;amp; restoring.isEmpty();
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if committing offsets failed (non-EOS)&lt;/li&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
@@ -139,4 +194,43 @@ int punctuate() 
{
         return punctuated;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    RuntimeException suspend() {&lt;br/&gt;
+        final AtomicReference&amp;lt;RuntimeException&amp;gt; firstException = new AtomicReference&amp;lt;&amp;gt;(super.suspend());&lt;br/&gt;
+        log.trace(&quot;Close restoring stream task {}&quot;, restoring.keySet());&lt;br/&gt;
+        firstException.compareAndSet(null, closeNonRunningTasks(restoring.values()));&lt;br/&gt;
+        restoring.clear();&lt;br/&gt;
+        restoringByPartition.clear();&lt;br/&gt;
+        return firstException.get();&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public String toString(final String indent) &lt;/p&gt;
{
+        final StringBuilder builder = new StringBuilder();
+        builder.append(super.toString(indent));
+        describe(builder, restoring.values(), indent, &quot;Restoring:&quot;);
+        return builder.toString();
+    }
&lt;p&gt;+&lt;br/&gt;
+    List&amp;lt;StreamTask&amp;gt; allTasks() &lt;/p&gt;
{
+        final List&amp;lt;StreamTask&amp;gt; tasks = super.allTasks();
+        tasks.addAll(restoring.values());
+        return tasks;
+    }
&lt;p&gt;+&lt;br/&gt;
+    Collection&amp;lt;StreamTask&amp;gt; restoringTasks() &lt;/p&gt;
{
+        return Collections.unmodifiableCollection(restoring.values());
+    }
&lt;p&gt;+&lt;br/&gt;
+    Set&amp;lt;TaskId&amp;gt; allAssignedTaskIds() &lt;/p&gt;
{
+        final Set&amp;lt;TaskId&amp;gt; taskIds = super.allAssignedTaskIds();
+        taskIds.addAll(restoring.keySet());
+        return taskIds;
+    }
&lt;p&gt;+&lt;br/&gt;
+    void clear() &lt;/p&gt;
{
+        super.clear();
+        restoring.clear();
+        restoringByPartition.clear();
+        restoredPartitions.clear();
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
index da402bc251c..52b1335f951 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
@@ -40,15 +40,12 @@&lt;br/&gt;
     private final Logger log;&lt;br/&gt;
     private final String taskTypeName;&lt;br/&gt;
     private final TaskAction&amp;lt;T&amp;gt; commitAction;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Map&amp;lt;TaskId, T&amp;gt; created = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Map&amp;lt;TaskId, T&amp;gt; suspended = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Map&amp;lt;TaskId, T&amp;gt; restoring = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Set&amp;lt;TopicPartition&amp;gt; restoredPartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Set&amp;lt;TaskId&amp;gt; previousActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+    Map&amp;lt;TaskId, T&amp;gt; created = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    Map&amp;lt;TaskId, T&amp;gt; suspended = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
     // IQ may access this map.&lt;br/&gt;
     Map&amp;lt;TaskId, T&amp;gt; running = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Map&amp;lt;TopicPartition, T&amp;gt; runningByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, T&amp;gt; restoringByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Map&amp;lt;TopicPartition, T&amp;gt; runningByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     AssignedTasks(final LogContext logContext,&lt;br/&gt;
                   final String taskTypeName) {&lt;br/&gt;
@@ -99,42 +96,11 @@ void initializeNewTasks() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void updateRestored(final Collection&amp;lt;TopicPartition&amp;gt; restored) {&lt;/li&gt;
	&lt;li&gt;if (restored.isEmpty()) 
{
-            return;
-        }&lt;/li&gt;
	&lt;li&gt;log.trace(&quot;{} changelog partitions that have completed restoring so far: {}&quot;, taskTypeName, restored);&lt;/li&gt;
	&lt;li&gt;restoredPartitions.addAll(restored);&lt;/li&gt;
	&lt;li&gt;for (final Iterator&amp;lt;Map.Entry&amp;lt;TaskId, T&amp;gt;&amp;gt; it = restoring.entrySet().iterator(); it.hasNext(); ) {&lt;/li&gt;
	&lt;li&gt;final Map.Entry&amp;lt;TaskId, T&amp;gt; entry = it.next();&lt;/li&gt;
	&lt;li&gt;final T task = entry.getValue();&lt;/li&gt;
	&lt;li&gt;if (restoredPartitions.containsAll(task.changelogPartitions())) {&lt;/li&gt;
	&lt;li&gt;transitionToRunning(task);&lt;/li&gt;
	&lt;li&gt;it.remove();&lt;/li&gt;
	&lt;li&gt;log.trace(&quot;{} {} completed restoration as all its changelog partitions {} have been applied to restore state&quot;,&lt;/li&gt;
	&lt;li&gt;taskTypeName,&lt;/li&gt;
	&lt;li&gt;task.id(),&lt;/li&gt;
	&lt;li&gt;task.changelogPartitions());&lt;/li&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;if (log.isTraceEnabled()) {&lt;/li&gt;
	&lt;li&gt;final HashSet&amp;lt;TopicPartition&amp;gt; outstandingPartitions = new HashSet&amp;lt;&amp;gt;(task.changelogPartitions());&lt;/li&gt;
	&lt;li&gt;outstandingPartitions.removeAll(restoredPartitions);&lt;/li&gt;
	&lt;li&gt;log.trace(&quot;{} {} cannot resume processing yet since some of its changelog partitions have not completed restoring: {}&quot;,&lt;/li&gt;
	&lt;li&gt;taskTypeName,&lt;/li&gt;
	&lt;li&gt;task.id(),&lt;/li&gt;
	&lt;li&gt;outstandingPartitions);&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;if (allTasksRunning()) 
{
-            restoredPartitions.clear();
-        }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
+    abstract void addToRestoring(final T task);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     boolean allTasksRunning() &lt;/p&gt;
{
         return created.isEmpty()
-                &amp;amp;&amp;amp; suspended.isEmpty()
-                &amp;amp;&amp;amp; restoring.isEmpty();
+                &amp;amp;&amp;amp; suspended.isEmpty();
     }

&lt;p&gt;     Collection&amp;lt;T&amp;gt; running() {&lt;br/&gt;
@@ -145,21 +111,17 @@ RuntimeException suspend() {&lt;br/&gt;
         final AtomicReference&amp;lt;RuntimeException&amp;gt; firstException = new AtomicReference&amp;lt;&amp;gt;(null);&lt;br/&gt;
         log.trace(&quot;Suspending running {} {}&quot;, taskTypeName, runningTaskIds());&lt;br/&gt;
         firstException.compareAndSet(null, suspendTasks(running.values()));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.trace(&quot;Close restoring {} {}&quot;, taskTypeName, restoring.keySet());&lt;/li&gt;
	&lt;li&gt;firstException.compareAndSet(null, closeNonRunningTasks(restoring.values()));&lt;br/&gt;
         log.trace(&quot;Close created {} {}&quot;, taskTypeName, created.keySet());&lt;br/&gt;
         firstException.compareAndSet(null, closeNonRunningTasks(created.values()));&lt;br/&gt;
         previousActiveTasks.clear();&lt;br/&gt;
         previousActiveTasks.addAll(running.keySet());&lt;br/&gt;
         running.clear();&lt;/li&gt;
	&lt;li&gt;restoring.clear();&lt;br/&gt;
         created.clear();&lt;br/&gt;
         runningByPartition.clear();&lt;/li&gt;
	&lt;li&gt;restoringByPartition.clear();&lt;br/&gt;
         return firstException.get();&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private RuntimeException closeNonRunningTasks(final Collection&amp;lt;T&amp;gt; tasks) {&lt;br/&gt;
+    RuntimeException closeNonRunningTasks(final Collection&amp;lt;T&amp;gt; tasks) {&lt;br/&gt;
         RuntimeException exception = null;&lt;br/&gt;
         for (final T task : tasks) {&lt;br/&gt;
             try {&lt;br/&gt;
@@ -176,7 +138,7 @@ private RuntimeException closeNonRunningTasks(final Collection&amp;lt;T&amp;gt; tasks) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private RuntimeException suspendTasks(final Collection&amp;lt;T&amp;gt; tasks) {&lt;br/&gt;
         final AtomicReference&amp;lt;RuntimeException&amp;gt; firstException = new AtomicReference&amp;lt;&amp;gt;(null);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Iterator&amp;lt;T&amp;gt; it = tasks.iterator(); it.hasNext(); ) {&lt;br/&gt;
+        for (final Iterator&amp;lt;T&amp;gt; it = tasks.iterator(); it.hasNext(); ) {&lt;br/&gt;
             final T task = it.next();&lt;br/&gt;
             try 
{
                 task.suspend();
@@ -247,27 +209,17 @@ boolean maybeResumeSuspendedTask(final TaskId taskId, final Set&amp;lt;TopicPartition&amp;gt;
         return false;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void addToRestoring(final T task) {&lt;/li&gt;
	&lt;li&gt;restoring.put(task.id(), task);&lt;/li&gt;
	&lt;li&gt;for (TopicPartition topicPartition : task.partitions()) 
{
-            restoringByPartition.put(topicPartition, task);
-        }&lt;br/&gt;
-        for (TopicPartition topicPartition : task.changelogPartitions()) {-            restoringByPartition.put(topicPartition, task);-        }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if the task producer got fenced (EOS only)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void transitionToRunning(final T task) {&lt;br/&gt;
+    void transitionToRunning(final T task) {&lt;br/&gt;
         log.debug(&quot;transitioning {} {} to running&quot;, taskTypeName, task.id());&lt;br/&gt;
         running.put(task.id(), task);&lt;br/&gt;
         task.initializeTopology();&lt;/li&gt;
	&lt;li&gt;for (TopicPartition topicPartition : task.partitions()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        for (final TopicPartition topicPartition }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -288,15 +240,14 @@ public String toString(final String indent) &lt;/p&gt;
{
         final StringBuilder builder = new StringBuilder();
         describe(builder, running.values(), indent, &quot;Running:&quot;);
         describe(builder, suspended.values(), indent, &quot;Suspended:&quot;);
-        describe(builder, restoring.values(), indent, &quot;Restoring:&quot;);
         describe(builder, created.values(), indent, &quot;New:&quot;);
         return builder.toString();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void describe(final StringBuilder builder,&lt;/li&gt;
	&lt;li&gt;final Collection&amp;lt;T&amp;gt; tasks,&lt;/li&gt;
	&lt;li&gt;final String indent,&lt;/li&gt;
	&lt;li&gt;final String name) {&lt;br/&gt;
+    void describe(final StringBuilder builder,&lt;br/&gt;
+                  final Collection&amp;lt;T&amp;gt; tasks,&lt;br/&gt;
+                  final String indent,&lt;br/&gt;
+                  final String name) {&lt;br/&gt;
         builder.append(indent).append(name);&lt;br/&gt;
         for (final T t : tasks) 
{
             builder.append(indent).append(t.toString(indent + &quot;\t\t&quot;));
@@ -304,35 +255,27 @@ private void describe(final StringBuilder builder,
         builder.append(&quot;\n&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private List&amp;lt;T&amp;gt; allTasks() {&lt;br/&gt;
+    List&amp;lt;T&amp;gt; allTasks() 
{
         final List&amp;lt;T&amp;gt; tasks = new ArrayList&amp;lt;&amp;gt;();
         tasks.addAll(running.values());
         tasks.addAll(suspended.values());
-        tasks.addAll(restoring.values());
         tasks.addAll(created.values());
         return tasks;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collection&amp;lt;T&amp;gt; restoringTasks() 
{
-        return Collections.unmodifiableCollection(restoring.values());
-    }
&lt;p&gt;-&lt;br/&gt;
     Set&amp;lt;TaskId&amp;gt; allAssignedTaskIds() &lt;/p&gt;
{
         final Set&amp;lt;TaskId&amp;gt; taskIds = new HashSet&amp;lt;&amp;gt;();
         taskIds.addAll(running.keySet());
         taskIds.addAll(suspended.keySet());
-        taskIds.addAll(restoring.keySet());
         taskIds.addAll(created.keySet());
         return taskIds;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     void clear() &lt;/p&gt;
{
         runningByPartition.clear();
-        restoringByPartition.clear();
         running.clear();
         created.clear();
         suspended.clear();
-        restoredPartitions.clear();
     }

&lt;p&gt;     Set&amp;lt;TaskId&amp;gt; previousTaskIds() {&lt;br/&gt;
@@ -351,7 +294,7 @@ int commit() {&lt;br/&gt;
     void applyToRunningTasks(final TaskAction&amp;lt;T&amp;gt; action) {&lt;br/&gt;
         RuntimeException firstException = null;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Iterator&amp;lt;T&amp;gt; it = running().iterator(); it.hasNext(); ) {&lt;br/&gt;
+        for (final Iterator&amp;lt;T&amp;gt; it = running().iterator(); it.hasNext(); ) {&lt;br/&gt;
             final T task = it.next();&lt;br/&gt;
             try {&lt;br/&gt;
                 action.apply(task);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
index 33dce9e7558..e0bac939b8b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java&lt;br/&gt;
@@ -26,13 +26,13 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     static final int NO_CHECKPOINT = -1;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Long checkpoint;&lt;br/&gt;
     private final long offsetLimit;&lt;br/&gt;
     private final boolean persistent;&lt;br/&gt;
     private final String storeName;&lt;br/&gt;
     private final TopicPartition partition;&lt;br/&gt;
     private final CompositeRestoreListener compositeRestoreListener;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private long checkpointOffset;&lt;br/&gt;
     private long restoredOffset;&lt;br/&gt;
     private long startingOffset;&lt;br/&gt;
     private long endingOffset;&lt;br/&gt;
@@ -45,7 +45,7 @@&lt;br/&gt;
                   final String storeName) {&lt;br/&gt;
         this.partition = partition;&lt;br/&gt;
         this.compositeRestoreListener = compositeRestoreListener;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.checkpoint = checkpoint;&lt;br/&gt;
+        this.checkpointOffset = checkpoint == null ? NO_CHECKPOINT : checkpoint;&lt;br/&gt;
         this.offsetLimit = offsetLimit;&lt;br/&gt;
         this.persistent = persistent;&lt;br/&gt;
         this.storeName = storeName;&lt;br/&gt;
@@ -56,7 +56,15 @@ public TopicPartition partition() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     long checkpoint() &lt;/p&gt;
{
-        return checkpoint == null ? NO_CHECKPOINT : checkpoint;
+        return checkpointOffset;
+    }
&lt;p&gt;+&lt;br/&gt;
+    void setCheckpointOffset(final long checkpointOffset) &lt;/p&gt;
{
+        this.checkpointOffset = checkpointOffset;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public String storeName() &lt;/p&gt;
{
+        return storeName;
     }

&lt;p&gt;     void restoreStarted() {&lt;br/&gt;
@@ -67,7 +75,8 @@ void restoreDone() &lt;/p&gt;
{
         compositeRestoreListener.onRestoreEnd(partition, storeName, restoredNumRecords());
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void restoreBatchCompleted(long currentRestoredOffset, int numRestored) {&lt;br/&gt;
+    void restoreBatchCompleted(final long currentRestoredOffset,&lt;br/&gt;
+                               final int numRestored) 
{
         compositeRestoreListener.onBatchRestored(partition, storeName, currentRestoredOffset, numRestored);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -79,7 +88,7 @@ boolean isPersistent() &lt;/p&gt;
{
         return persistent;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void setUserRestoreListener(StateRestoreListener userRestoreListener) {&lt;br/&gt;
+    void setUserRestoreListener(final StateRestoreListener userRestoreListener) 
{
         this.compositeRestoreListener.setUserRestoreListener(userRestoreListener);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
index 5fcba76570e..e0fc82de016 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
@@ -61,9 +61,14 @@ public StoreChangelogReader(final Consumer&amp;lt;byte[], byte[]&amp;gt; restoreConsumer,&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public void register(final StateRestorer restorer) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;restorer.setUserRestoreListener(userStateRestoreListener);&lt;/li&gt;
	&lt;li&gt;stateRestorers.put(restorer.partition(), restorer);&lt;/li&gt;
	&lt;li&gt;needsInitializing.put(restorer.partition(), restorer);&lt;br/&gt;
+        final StateRestorer existingRestorer = stateRestorers.get(restorer.partition());&lt;br/&gt;
+        if (existingRestorer == null) 
{
+            restorer.setUserRestoreListener(userStateRestoreListener);
+            stateRestorers.put(restorer.partition(), restorer);
+            needsInitializing.put(restorer.partition(), restorer);
+        }
&lt;p&gt; else &lt;/p&gt;
{
+            needsInitializing.put(restorer.partition(), existingRestorer);
+        }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -71,7 +76,7 @@ public void register(final StateRestorer restorer) {&lt;br/&gt;
      */&lt;br/&gt;
     public Collection&amp;lt;TopicPartition&amp;gt; restore(final RestoringTasks active) {&lt;br/&gt;
         if (!needsInitializing.isEmpty()) &lt;/p&gt;
{
-            initialize();
+            initialize(active);
         }

&lt;p&gt;         if (needsRestoring.isEmpty()) {&lt;br/&gt;
@@ -103,7 +108,7 @@ public void register(final StateRestorer restorer) &lt;/p&gt;
{
         return completed();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void initialize() {&lt;br/&gt;
+    private void initialize(final RestoringTasks active) {&lt;br/&gt;
         if (!restoreConsumer.subscription().isEmpty()) 
{
             throw new StreamsException(&quot;Restore consumer should not be subscribed to any topics (&quot; + restoreConsumer.subscription() + &quot;)&quot;);
         }
&lt;p&gt;@@ -112,8 +117,8 @@ private void initialize() {&lt;br/&gt;
         // the needsInitializing map is not empty, meaning we do not know the metadata for some of them yet&lt;br/&gt;
         refreshChangelogInfo();&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializable = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, StateRestorer&amp;gt; entry : needsInitializing.entrySet()) {&lt;br/&gt;
+        final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializable = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TopicPartition, StateRestorer&amp;gt; entry : needsInitializing.entrySet()) {&lt;br/&gt;
             final TopicPartition topicPartition = entry.getKey();&lt;br/&gt;
             if (hasPartition(topicPartition)) {&lt;br/&gt;
                 initializable.put(entry.getKey(), entry.getValue());&lt;br/&gt;
@@ -157,11 +162,12 @@ private void initialize() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // set up restorer for those initializable&lt;br/&gt;
         if (!initializable.isEmpty()) &lt;/p&gt;
{
-            startRestoration(initializable);
+            startRestoration(initializable, active);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized) {&lt;br/&gt;
+    private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initialized,&lt;br/&gt;
+                                  final RestoringTasks active) {&lt;br/&gt;
         log.debug(&quot;Start restoring state stores from changelog topics {}&quot;, initialized.keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Set&amp;lt;TopicPartition&amp;gt; assignment = new HashSet&amp;lt;&amp;gt;(restoreConsumer.assignment());&lt;br/&gt;
@@ -170,26 +176,42 @@ private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializ&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;StateRestorer&amp;gt; needsPositionUpdate = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (final StateRestorer restorer : initialized.values()) {&lt;br/&gt;
+            final TopicPartition restoringPartition = restorer.partition();&lt;br/&gt;
             if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) &lt;/p&gt;
{
-                restoreConsumer.seek(restorer.partition(), restorer.checkpoint());
-                logRestoreOffsets(restorer.partition(),
+                restoreConsumer.seek(restoringPartition, restorer.checkpoint());
+                logRestoreOffsets(restoringPartition,
                                   restorer.checkpoint(),
-                                  endOffsets.get(restorer.partition()));
-                restorer.setStartingOffset(restoreConsumer.position(restorer.partition()));
+                                  endOffsets.get(restoringPartition));
+                restorer.setStartingOffset(restoreConsumer.position(restoringPartition));
                 restorer.restoreStarted();
             }
&lt;p&gt; else &lt;/p&gt;
{
-                restoreConsumer.seekToBeginning(Collections.singletonList(restorer.partition()));
+                restoreConsumer.seekToBeginning(Collections.singletonList(restoringPartition));
                 needsPositionUpdate.add(restorer);
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;         for (final StateRestorer restorer : needsPositionUpdate) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long position = restoreConsumer.position(restorer.partition());&lt;/li&gt;
	&lt;li&gt;logRestoreOffsets(restorer.partition(),&lt;/li&gt;
	&lt;li&gt;position,&lt;/li&gt;
	&lt;li&gt;endOffsets.get(restorer.partition()));&lt;/li&gt;
	&lt;li&gt;restorer.setStartingOffset(position);&lt;/li&gt;
	&lt;li&gt;restorer.restoreStarted();&lt;br/&gt;
+            final TopicPartition restoringPartition = restorer.partition();&lt;br/&gt;
+            final StreamTask task = active.restoringTaskFor(restoringPartition);&lt;br/&gt;
+            // If checkpoint does not exist it means the task was not shutdown gracefully before;&lt;br/&gt;
+            // and in this case if EOS is turned on we should wipe out the state and re-initialize the task&lt;br/&gt;
+            if (task.eosEnabled) {&lt;br/&gt;
+                log.info(&quot;No checkpoint found for task {} state store {} changelog {} with EOS turned on. &quot; +&lt;br/&gt;
+                    &quot;Reinitializing the task and restore its state from the beginning.&quot;, task.id, restorer.storeName(), restoringPartition);&lt;br/&gt;
+                // we move the partitions here, because they will be added back within&lt;br/&gt;
+                // `task.reinitializeStateStoresForPartitions()` that calls `register()` internally again&lt;br/&gt;
+                needsInitializing.remove(restoringPartition);&lt;br/&gt;
+                restorer.setCheckpointOffset(restoreConsumer.position(restoringPartition));&lt;br/&gt;
+                task.reinitializeStateStoresForPartitions(Collections.singleton(restoringPartition));&lt;br/&gt;
+            } else {&lt;br/&gt;
+                log.info(&quot;Restoring task {}&apos;s state store {} from beginning of the changelog {} &quot;, task.id, restorer.storeName(), restoringPartition);&lt;br/&gt;
+                final long position = restoreConsumer.position(restoringPartition);&lt;br/&gt;
+                logRestoreOffsets(restoringPartition,&lt;br/&gt;
+                    position,&lt;br/&gt;
+                    endOffsets.get(restoringPartition));&lt;br/&gt;
+                restorer.setStartingOffset(position);&lt;br/&gt;
+                restorer.restoreStarted();&lt;br/&gt;
+            }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         needsRestoring.putAll(initialized);&lt;br/&gt;
@@ -280,7 +302,7 @@ private long processNext(final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records,&lt;br/&gt;
                              final Long endOffset) {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; restoreRecords = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         long nextPosition = -1;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int numberRecords = records.size();&lt;br/&gt;
+        final int numberRecords = records.size();&lt;br/&gt;
         int numberRestored = 0;&lt;br/&gt;
         long lastRestoredOffset = -1;&lt;br/&gt;
         for (final ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : records) {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
index 6c7b2b43c9d..f6811c66702 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java&lt;br/&gt;
@@ -317,7 +317,7 @@ public void shouldNotViolateEosIfOneTaskFails() throws Exception {&lt;br/&gt;
         // the app is supposed to copy all 40 records into the output topic&lt;br/&gt;
         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes&lt;br/&gt;
         //&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;// the failure gets inject after 20 committed and 30 uncommitted records got received&lt;br/&gt;
+        // the failure gets inject after 20 committed and 10 uncommitted records got received&lt;br/&gt;
         // -&amp;gt; the failure only kills one thread&lt;br/&gt;
         // after fail over, we should read 40 committed records (even if 50 record got written)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -392,7 +392,7 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes&lt;br/&gt;
         // and store updates (ie, another 5 uncommitted writes to a changelog topic per partition)&lt;br/&gt;
         //&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// the failure gets inject after 20 committed and 30 uncommitted records got received&lt;br/&gt;
+        // the failure gets inject after 20 committed and 10 uncommitted records got received&lt;br/&gt;
         // -&amp;gt; the failure only kills one thread&lt;br/&gt;
         // after fail over, we should read 40 committed records and the state stores should contain the correct sums&lt;br/&gt;
         // per key (even if some records got processed twice)&lt;br/&gt;
@@ -402,7 +402,7 @@ public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {&lt;br/&gt;
             streams.start();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; committedDataBeforeFailure = prepareData(0L, 10L, 0L, 1L);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L);&lt;br/&gt;
+            final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; uncommittedDataBeforeFailure = prepareData(10L, 15L, 0L, 1L, 2L, 3L);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final List&amp;lt;KeyValue&amp;lt;Long, Long&amp;gt;&amp;gt; dataBeforeFailure = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             dataBeforeFailure.addAll(committedDataBeforeFailure);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
index c65d4efadb1..a117dc30adf 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
@@ -100,7 +100,10 @@ public void shouldRequestTopicsAndHandleTimeoutException() {&lt;br/&gt;
     public void shouldThrowExceptionIfConsumerHasCurrentSubscription() {&lt;br/&gt;
         final StateRestorer mockRestorer = EasyMock.mock(StateRestorer.class);&lt;br/&gt;
         mockRestorer.setUserRestoreListener(stateRestoreListener);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(mockRestorer.partition()).andReturn(new TopicPartition(&quot;sometopic&quot;, 0)).andReturn(new TopicPartition(&quot;sometopic&quot;, 0));&lt;br/&gt;
+        expect(mockRestorer.partition())&lt;br/&gt;
+            .andReturn(new TopicPartition(&quot;sometopic&quot;, 0))&lt;br/&gt;
+            .andReturn(new TopicPartition(&quot;sometopic&quot;, 0))&lt;br/&gt;
+            .andReturn(new TopicPartition(&quot;sometopic&quot;, 0));&lt;br/&gt;
         EasyMock.replay(mockRestorer);&lt;br/&gt;
         changelogReader.register(mockRestorer);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -119,6 +122,10 @@ public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() &lt;/p&gt;
{
         final int messages = 10;
         setupConsumer(messages, topicPartition);
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
         changelogReader.restore(active);
         assertThat(callback.restored.size(), equalTo(messages));
     }
&lt;p&gt;@@ -133,10 +140,9 @@ public void shouldRecoverFromInvalidOffsetExceptionAndFinishRestore() &lt;/p&gt;
{
                 return Collections.singleton(topicPartition);
             }
&lt;p&gt;         });&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        EasyMock.expect(active.restoringTaskFor(topicPartition)).andReturn(task).andReturn(task);&lt;br/&gt;
         EasyMock.replay(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // first restore call &quot;fails&quot; but we should not die with an exception&lt;br/&gt;
@@ -165,6 +171,9 @@ public void shouldClearAssignmentAtEndOfRestore() &lt;/p&gt;
{
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,
                                                    &quot;storeName&quot;));
 
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
         changelogReader.restore(active);
         assertThat(consumer.assignment(), equalTo(Collections.&amp;lt;TopicPartition&amp;gt;emptySet()));
     }
&lt;p&gt;@@ -175,6 +184,10 @@ public void shouldRestoreToLimitWhenSupplied() {&lt;br/&gt;
         final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, null, 3, true,&lt;br/&gt;
                                                          &quot;storeName&quot;);&lt;br/&gt;
         changelogReader.register(restorer);&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         assertThat(callback.restored.size(), equalTo(3));&lt;br/&gt;
         assertThat(restorer.restoredOffset(), equalTo(3L));&lt;br/&gt;
@@ -197,8 +210,9 @@ public void shouldRestoreMultipleStores() {&lt;br/&gt;
         changelogReader.register(new StateRestorer(one, restoreListener1, null, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(two, restoreListener2, null, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(one)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(two)).andReturn(null);&lt;br/&gt;
+        expect(active.restoringTaskFor(one)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(two)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
         replay(active);&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -224,8 +238,9 @@ public void shouldRestoreAndNotifyMultipleStores() throws Exception {&lt;br/&gt;
         changelogReader.register(new StateRestorer(one, restoreListener1, null, Long.MAX_VALUE, true, &quot;storeName2&quot;));&lt;br/&gt;
         changelogReader.register(new StateRestorer(two, restoreListener2, null, Long.MAX_VALUE, true, &quot;storeName3&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(one)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(two)).andReturn(null);&lt;br/&gt;
+        expect(active.restoringTaskFor(one)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(two)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
         replay(active);&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -246,8 +261,11 @@ public void shouldRestoreAndNotifyMultipleStores() throws Exception {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldOnlyReportTheLastRestoredOffset() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader&lt;/li&gt;
	&lt;li&gt;.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName1&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName1&quot;));&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored.size(), equalTo(5));&lt;br/&gt;
@@ -306,6 +324,10 @@ public void shouldNotRestoreAnythingWhenCheckpointAtEndOffset() {&lt;br/&gt;
     public void shouldReturnRestoredOffsetsForPersistentStores() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();&lt;br/&gt;
         assertThat(restoredOffsets, equalTo(Collections.singletonMap(topicPartition, 10L)));&lt;br/&gt;
@@ -315,6 +337,10 @@ public void shouldReturnRestoredOffsetsForPersistentStores() {&lt;br/&gt;
     public void shouldNotReturnRestoredOffsetsForNonPersistentStore() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();&lt;br/&gt;
         assertThat(restoredOffsets, equalTo(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap()));&lt;br/&gt;
@@ -328,8 +354,11 @@ public void shouldIgnoreNullKeysWhenRestoring() {&lt;br/&gt;
         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), 1, (byte[]) null, bytes));&lt;br/&gt;
         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topicPartition.topic(), topicPartition.partition(), 2, bytes, bytes));&lt;br/&gt;
         consumer.assign(Collections.singletonList(topicPartition));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+&lt;br/&gt;
         changelogReader.restore(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(callback.restored, CoreMatchers.equalTo(Utils.mkList(KeyValue.pair(bytes, bytes), KeyValue.pair(bytes, bytes))));&lt;br/&gt;
@@ -354,9 +383,10 @@ public void shouldRestorePartitionsRegisteredPostInitialization() {&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;/p&gt;

&lt;p&gt;         final TopicPartition postInitialization = new TopicPartition(&quot;other&quot;, 0);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(topicPartition)).andReturn(null);&lt;/li&gt;
	&lt;li&gt;expect(active.restoringTaskFor(postInitialization)).andReturn(null);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(postInitialization)).andReturn(task);&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
         replay(active);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertTrue(changelogReader.restore(active).isEmpty());&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16692417" author="mjsax" created="Mon, 19 Nov 2018 23:22:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tobiajo&quot; class=&quot;user-hover&quot; rel=&quot;tobiajo&quot;&gt;tobiajo&lt;/a&gt; Back ported to 1.1 branch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16810946" author="githubbot" created="Fri, 5 Apr 2019 15:34:02 +0000"  >&lt;p&gt;guozhangwang commented on pull request #6546: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt;: Cherry-pick 5430 to 1.1&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/6546&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6546&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   The first PR of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7192&quot; title=&quot;State-store can desynchronise with changelog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7192&quot;&gt;&lt;del&gt;KAFKA-7192&lt;/del&gt;&lt;/a&gt; is cherry-picked to 1.1 but the follow-up (&lt;a href=&quot;https://github.com/apache/kafka/pull/5430&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5430&lt;/a&gt;) is not. This is causing flaky EOS system test failures.&lt;/p&gt;

&lt;p&gt;   Some test results:&lt;/p&gt;

&lt;p&gt;   In 2.0 branch, running 25 times (the streams_eos_test has 4 tests, so = 100 tests), no failures:&lt;/p&gt;

&lt;p&gt;   &lt;a href=&quot;http://confluent-kafka-2-0-system-test-results.s3-us-west-2.amazonaws.com/2019-04-05--001.1554466177--apache--2.0--db22e3d/report.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://confluent-kafka-2-0-system-test-results.s3-us-west-2.amazonaws.com/2019-04-05--001.1554466177--apache--2.0--db22e3d/report.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;   In 1.1 branch before this PR, running 5 times, failed 10 tests:&lt;/p&gt;

&lt;p&gt;   &lt;a href=&quot;http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-04-02--001.1554239700--guozhangwang--KMinor-1.1-eos-test--8395fce/report.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-04-02--001.1554239700--guozhangwang--KMinor-1.1-eos-test--8395fce/report.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;   In this branch (after this PR), running 25 times, no failures:&lt;/p&gt;

&lt;p&gt;   &lt;a href=&quot;http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-04-05--001.1554465488--guozhangwang--KMinor-1.1-eos-test--897aa03/report.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-04-05--001.1554465488--guozhangwang--KMinor-1.1-eos-test--897aa03/report.html&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 32 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3w60v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>