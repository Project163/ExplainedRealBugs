<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:13:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-5037] Infinite loop if all input topics are unknown at startup</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-5037</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;See discusion: &lt;a href=&quot;https://github.com/apache/kafka/pull/2815&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/2815&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We will need some rewrite on &lt;tt&gt;StreamPartitionsAssignor&lt;/tt&gt; and to add much more test for all kind of corner cases, including pattern subscriptions.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13062290">KAFKA-5037</key>
            <summary>Infinite loop if all input topics are unknown at startup</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tedyu">Zhihong Yu</assignee>
                                    <reporter username="mjsax">Matthias J. Sax</reporter>
                        <labels>
                            <label>newbie++</label>
                            <label>user-experience</label>
                    </labels>
                <created>Thu, 6 Apr 2017 23:19:16 +0000</created>
                <updated>Fri, 20 Jul 2018 18:04:06 +0000</updated>
                            <resolved>Thu, 19 Jul 2018 22:23:13 +0000</resolved>
                                    <version>0.10.2.0</version>
                                    <fixVersion>2.1.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15967817" author="mjsax" created="Thu, 13 Apr 2017 16:19:47 +0000"  >&lt;p&gt;A similar issue got reported here: &lt;a href=&quot;http://search-hadoop.com/m/Kafka/uyzND14MkzKYvKSh2?subj=Kafka+Streams+Application+does+not+start+after+10+1+to+10+2+update+if+topics+need+to+be+auto+created&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://search-hadoop.com/m/Kafka/uyzND14MkzKYvKSh2?subj=Kafka+Streams+Application+does+not+start+after+10+1+to+10+2+update+if+topics+need+to+be+auto+created&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16026413" author="enothereska" created="Fri, 26 May 2017 15:39:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; is this still for 0.11?&lt;/p&gt;</comment>
                            <comment id="16153928" author="miguno" created="Tue, 5 Sep 2017 16:30:59 +0000"  >&lt;p&gt;Ping &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt;: do we target this for 1.0.0?&lt;/p&gt;</comment>
                            <comment id="16447385" author="miguno" created="Sun, 22 Apr 2018 21:42:41 +0000"  >&lt;p&gt;Ping &#8211; any update on this?&lt;/p&gt;</comment>
                            <comment id="16447546" author="guozhang" created="Mon, 23 Apr 2018 04:10:08 +0000"  >&lt;p&gt;Hey Michael, we have not yet started working on this. Did you see more and more people complaining about it?&lt;/p&gt;</comment>
                            <comment id="16524389" author="guozhang" created="Wed, 27 Jun 2018 00:46:18 +0000"  >&lt;p&gt;I&apos;m now grouping these three tickets: 5037, 6587 and 6437 into a single general issue, and here is my summary of it:&lt;/p&gt;

&lt;p&gt;1. If there is a repartition topic whose num.partitions is dependent on the corresponding sub-topology&apos;s input topic, and that input topic does not exist in &lt;tt&gt;metadata&lt;/tt&gt;, then it will be assigned as &lt;tt&gt;NOT_AVAILABLE&lt;/tt&gt; and the corresponding topic will not be created, and any tasks that needs it as input topics will be silently dropped (see &lt;tt&gt;DefaultPartitionGrouper#maxNumPartitions&lt;/tt&gt;). This is reported as in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6437&quot; title=&quot;Streams does not warn about missing input topics, but hangs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6437&quot;&gt;&lt;del&gt;KAFKA-6437&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;1.1. If we happen to pass the while loop because of the &lt;tt&gt;NOT_AVAILABLE&lt;/tt&gt; tag, we may still hit the issue in &lt;tt&gt;CopartitionedTopicsValidator#validate&lt;/tt&gt; that will cause runtime exception being thrown.&lt;/p&gt;

&lt;p&gt;2. If there is a repartition topic whose num.partitions is dependent on the corresponding sub-topology&apos;s input topic, and that input topic is also a repartition topic with num.partition as &lt;tt&gt;NOT_AVAILABLE&lt;/tt&gt;, then there is a bug that will cause the while loop to never finish. This is reported as in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5037&quot; title=&quot;Infinite loop if all input topics are unknown at startup&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5037&quot;&gt;&lt;del&gt;KAFKA-5037&lt;/del&gt;&lt;/a&gt; and the PR 2815 contains the fix of it already.&lt;/p&gt;

&lt;p&gt;3. Before 1.1, we have another while loop inside &lt;tt&gt;prepareTopic&lt;/tt&gt;. If the metadata cannot be fetched then we will also fall into a blocking scenario. This is more related to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6587&quot; title=&quot;Kafka Streams hangs when not able to access internal topics&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6587&quot;&gt;&lt;del&gt;KAFKA-6587&lt;/del&gt;&lt;/a&gt; and should have been fixed automatically now.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
            &lt;span class=&quot;code-comment&quot;&gt;// wait until each one of the topic metadata has been propagated to at least one broker
&lt;/span&gt;            &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (!allTopicsCreated(topicNamesToMakeReady, topicsToMakeReady)) {
                &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
                    &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(50L);
                } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
                    &lt;span class=&quot;code-comment&quot;&gt;// ignore
&lt;/span&gt;                }
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16524421" author="guozhang" created="Wed, 27 Jun 2018 01:17:21 +0000"  >&lt;p&gt;If we want to only fix-forward this issue beyond 2.0. I&apos;d propose the following to refactor the &lt;tt&gt;StreamsPartitionAssignor#assign()&lt;/tt&gt; function as follows. The principle is that, unlike consumer who would assign partial available topics to members hoping that when the metadata gets refreshed again another rebalance is triggered to complete the full subscription, in Streams we would not start at all if not all the requested source topics are available. This is a simpler rule and easier for users to reason: in the past the most complaints are that &quot;I need to be notified when this happens, but I still see application in RUNNING, and was not aware we are actually only executing partial topology&quot;.&lt;/p&gt;

&lt;p&gt;1. At the beginning of &lt;tt&gt;assign&lt;/tt&gt;, we first check that all the non-repartition source topics are included in the &lt;tt&gt;metadata&lt;/tt&gt;. If not, we log an error at the leader and set an error in the Assignment userData bytes, indicating that leader cannot complete assignment and the error code would indicate the root cause of it.&lt;/p&gt;

&lt;p&gt;2. Upon receiving the assignment, if the error is not NONE the streams will shutdown itself with a log entry re-stating the root cause interpreted from the error code.&lt;/p&gt;

&lt;p&gt;3. With 1) / 2) above, the non repartition source topics should always been available, and hence we should never encounter the `NOT_AVAILABLE` case for num.partitions of those repartition topics, we will remove `NOT_AVAILABLE` from the possible values, and remove the corresponding logic in DefaultPartitionGrouper for `NOT_AVAILABLE`.&lt;/p&gt;

&lt;p&gt;Note that 1) above requires us to bump up the version for assignment userData again, which should be fine with version probing added in 2.0. There is an alternative walkaround, to use negative version numbers as the error codes, and as long as clients knows how to interpret them that is fine. Personally I felt it is a bit hacky and would suggest we just bump up the version.&lt;/p&gt;

&lt;p&gt;WDYT?&lt;/p&gt;</comment>
                            <comment id="16530617" author="guozhang" created="Tue, 3 Jul 2018 00:28:40 +0000"  >&lt;p&gt;I&apos;m adding the tag `newbie++` for anyone wanting to pick it up. Note the proposed solution is targeted to fix all three JIRAs: 5037, 6587, 6437.&lt;/p&gt;</comment>
                            <comment id="16530724" author="yuzhihong@gmail.com" created="Tue, 3 Jul 2018 02:28:00 +0000"  >&lt;p&gt;Originally I threw exception in the if block for the following line in &lt;tt&gt;assign&lt;/tt&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
                                        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (numPartitionsCandidate == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In patch v2, I construct assignment map. See if this is close to what you propose.&lt;/p&gt;</comment>
                            <comment id="16530807" author="yuzhihong@gmail.com" created="Tue, 3 Jul 2018 04:25:12 +0000"  >&lt;p&gt;Currently both SubscriptionInfo.LATEST_SUPPORTED_VERSION and AssignmentInfo.LATEST_SUPPORTED_VERSION are 3.&lt;br/&gt;
There may be some intricacy w.r.t. &lt;tt&gt;minReceivedMetadataVersion&lt;/tt&gt; which is initialized to SubscriptionInfo.LATEST_SUPPORTED_VERSION.&lt;br/&gt;
With my patch, I see:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.kafka.streams.processor.internals.StreamsPartitionAssignorTest &amp;gt; shouldReturnUnchangedAssignmentForOldInstancesAndEmptyAssignmentForFutureInstances FAILED
    java.lang.AssertionError:
    Expected: &amp;lt;[version=4, supported version=4, active tasks=[0_0, 0_1], standby tasks={0_2=[topic1-2]}, global assignment={}]&amp;gt;
         but: was &amp;lt;[version=3, supported version=4, active tasks=[0_0, 0_1], standby tasks={0_2=[topic1-2]}, global assignment={}]&amp;gt;
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
        at org.junit.Assert.assertThat(Assert.java:956)
        at org.junit.Assert.assertThat(Assert.java:923)
        at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignorTest.                                                                                           shouldReturnUnchangedAssignmentForOldInstancesAndEmptyAssignmentForFutureInstances(StreamsPartitionAssignorTest.java:1280)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16530894" author="yuzhihong@gmail.com" created="Tue, 3 Jul 2018 06:25:49 +0000"  >&lt;p&gt;Patch v4 rewrites assertions in shouldReturnUnchangedAssignmentForOldInstancesAndEmptyAssignmentForFutureInstances so that the test passes.&lt;/p&gt;</comment>
                            <comment id="16530926" author="githubbot" created="Tue, 3 Jul 2018 07:05:23 +0000"  >&lt;p&gt;tedyu opened a new pull request #5322: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5037&quot; title=&quot;Infinite loop if all input topics are unknown at startup&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5037&quot;&gt;&lt;del&gt;KAFKA-5037&lt;/del&gt;&lt;/a&gt; Infinite loop if all input topics are unknown at startup&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5322&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5322&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   1. At the beginning of assign, we first check that all the non-repartition source topics are included in the metadata. If not, we log an error at the leader and set an error in the Assignment userData bytes, indicating that leader cannot complete assignment and the error code would indicate the root cause of it.&lt;/p&gt;

&lt;p&gt;   2. Upon receiving the assignment, if the error is not NONE the streams will shutdown itself with a log entry re-stating the root cause interpreted from the error code.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16545477" author="yuzhihong@gmail.com" created="Mon, 16 Jul 2018 16:57:58 +0000"  >&lt;p&gt;In KafkaStreams, I am adding the following method in order to detect state transition of StreamThread:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    /**
     * An app can set a single {@link StreamThread.StateListener} so that the app is notified when state changes.
     *
     * @param listener a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamThread state listener
     * @&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IllegalStateException &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; {@code KafkaStreams} instance is not in state {@link State#CREATED CREATED}.
     */
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void setStreamThreadStateListener(StreamThread.StateListener listener) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let me know whether a KIP is needed.&lt;/p&gt;</comment>
                            <comment id="16545735" author="guozhang" created="Mon, 16 Jul 2018 21:07:39 +0000"  >&lt;p&gt;Since &lt;tt&gt;StreamThread&lt;/tt&gt; is an internal class, adding any public functions on that class should not require a KIP.&lt;/p&gt;</comment>
                            <comment id="16549950" author="guozhang" created="Thu, 19 Jul 2018 22:23:13 +0000"  >&lt;p&gt;Issue resolved by pull request 5322&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/5322&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5322&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16549953" author="githubbot" created="Thu, 19 Jul 2018 22:24:30 +0000"  >&lt;p&gt;guozhangwang closed pull request #5322: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5037&quot; title=&quot;Infinite loop if all input topics are unknown at startup&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5037&quot;&gt;&lt;del&gt;KAFKA-5037&lt;/del&gt;&lt;/a&gt; Infinite loop if all input topics are unknown at startup&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5322&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5322&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java b/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java&lt;br/&gt;
index cef8116e880..8ed80dc524e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java&lt;br/&gt;
@@ -136,7 +136,7 @@&lt;br/&gt;
     private final String clientId;&lt;br/&gt;
     private final Metrics metrics;&lt;br/&gt;
     private final StreamsConfig config;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final StreamThread[] threads;&lt;br/&gt;
+    protected final StreamThread[] threads;&lt;br/&gt;
     private final StateDirectory stateDirectory;&lt;br/&gt;
     private final StreamsMetadataState streamsMetadataState;&lt;br/&gt;
     private final ScheduledExecutorService stateDirCleaner;&lt;br/&gt;
@@ -209,7 +209,7 @@ public boolean isValidTransition(final State newState) {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final Object stateLock = new Object();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private volatile State state = State.CREATED;&lt;br/&gt;
+    protected volatile State state = State.CREATED;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private boolean waitOnState(final State targetState, final long waitMs) {&lt;br/&gt;
         long begin = time.milliseconds();&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index eee5bc630b6..d1f7d93786f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -727,7 +727,7 @@&lt;/p&gt;

&lt;p&gt;     public static class InternalConfig &lt;/p&gt;
{
         public static final String TASK_MANAGER_FOR_PARTITION_ASSIGNOR = &quot;__task.manager.instance__&quot;;
-        public static final String VERSION_PROBING_FLAG = &quot;__version.probing.flag__&quot;;
+        public static final String ASSIGNMENT_ERROR_CODE = &quot;__assignment.error.code__&quot;;
     }

&lt;p&gt;     /**&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/DefaultPartitionGrouper.java b/streams/src/main/java/org/apache/kafka/streams/processor/DefaultPartitionGrouper.java&lt;br/&gt;
index cee94886854..712f8a75514 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/DefaultPartitionGrouper.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/DefaultPartitionGrouper.java&lt;br/&gt;
@@ -20,7 +20,6 @@&lt;br/&gt;
 import org.apache.kafka.common.PartitionInfo;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.streams.errors.StreamsException;&lt;br/&gt;
-import org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;/p&gt;

&lt;p&gt;@@ -80,18 +79,14 @@ protected int maxNumPartitions(Cluster metadata, Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         int maxNumPartitions = 0;&lt;br/&gt;
         for (String topic : topics) {&lt;br/&gt;
             List&amp;lt;PartitionInfo&amp;gt; partitions = metadata.partitionsForTopic(topic);&lt;br/&gt;
-&lt;br/&gt;
             if (partitions.isEmpty()) {&lt;br/&gt;
+                log.error(&quot;Empty partitions for topic {}&quot;, topic);&lt;br/&gt;
+                throw new RuntimeException(&quot;Empty partitions for topic &quot; + topic);&lt;br/&gt;
+            }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.warn(&quot;Skipping creating tasks for the topic group {} since topic {}&apos;s metadata is not available yet;&quot;&lt;/li&gt;
	&lt;li&gt;+ &quot; no tasks for this topic group will be assigned to any client.\n&quot;&lt;/li&gt;
	&lt;li&gt;+ &quot; Make sure all supplied topics in the topology are created before starting&quot;&lt;/li&gt;
	&lt;li&gt;+ &quot; as this could lead to unexpected results&quot;, topics, topic);&lt;/li&gt;
	&lt;li&gt;return StreamsPartitionAssignor.NOT_AVAILABLE;&lt;/li&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;int numPartitions = partitions.size();&lt;/li&gt;
	&lt;li&gt;if (numPartitions &amp;gt; maxNumPartitions)&lt;/li&gt;
	&lt;li&gt;maxNumPartitions = numPartitions;&lt;br/&gt;
+            int numPartitions = partitions.size();&lt;br/&gt;
+            if (numPartitions &amp;gt; maxNumPartitions) 
{
+                maxNumPartitions = numPartitions;
             }
&lt;p&gt;         }&lt;br/&gt;
         return maxNumPartitions;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index f425bb4e617..31de839b1a1 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -63,7 +63,6 @@&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
 import java.util.concurrent.TimeUnit;&lt;br/&gt;
-import java.util.concurrent.atomic.AtomicBoolean;&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicInteger;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static java.util.Collections.singleton;&lt;br/&gt;
@@ -261,12 +260,18 @@ public void onPartitionsAssigned(final Collection&amp;lt;TopicPartition&amp;gt; assignment) {&lt;br/&gt;
                 taskManager.suspendedActiveTaskIds(),&lt;br/&gt;
                 taskManager.suspendedStandbyTaskIds());&lt;/p&gt;

&lt;p&gt;+            if (streamThread.assignmentErrorCode.get() == StreamsPartitionAssignor.Error.INCOMPLETE_SOURCE_TOPIC_METADATA.code()) {&lt;br/&gt;
+                log.debug(&quot;Received error code {} - shutdown&quot;, streamThread.assignmentErrorCode.get());&lt;br/&gt;
+                streamThread.shutdown();&lt;br/&gt;
+                streamThread.setStateListener(null);&lt;br/&gt;
+                return;&lt;br/&gt;
+            }&lt;br/&gt;
             final long start = time.milliseconds();&lt;br/&gt;
             try {&lt;br/&gt;
                 if (streamThread.setState(State.PARTITIONS_ASSIGNED) == null) &lt;/p&gt;
{
                     return;
                 }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!streamThread.versionProbingFlag.get()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                if (streamThread.assignmentErrorCode.get() == StreamsPartitionAssignor.Error.NONE.code()) {
                     taskManager.createTasks(assignment);
                 }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; catch (final Throwable t) {&lt;br/&gt;
@@ -302,8 +307,8 @@ public void onPartitionsRevoked(final Collection&amp;lt;TopicPartition&amp;gt; assignment) {&lt;br/&gt;
                 final long start = time.milliseconds();&lt;br/&gt;
                 try {&lt;br/&gt;
                     // suspend active tasks&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (streamThread.versionProbingFlag.get()) {&lt;/li&gt;
	&lt;li&gt;streamThread.versionProbingFlag.set(false);&lt;br/&gt;
+                    if (streamThread.assignmentErrorCode.get() == StreamsPartitionAssignor.Error.VERSION_PROBING.code()) 
{
+                        streamThread.assignmentErrorCode.set(StreamsPartitionAssignor.Error.NONE.code());
                     }
&lt;p&gt; else &lt;/p&gt;
{
                         taskManager.suspendTasksAndState();
                     }
&lt;p&gt;@@ -563,7 +568,7 @@ StandbyTask createTask(final Consumer&amp;lt;byte[], byte[]&amp;gt; consumer,&lt;br/&gt;
     private final String logPrefix;&lt;br/&gt;
     private final TaskManager taskManager;&lt;br/&gt;
     private final StreamsMetricsThreadImpl streamsMetrics;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private final AtomicBoolean versionProbingFlag;&lt;br/&gt;
+    private final AtomicInteger assignmentErrorCode;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private long lastCommitMs;&lt;br/&gt;
     private long timerStartedMs;&lt;br/&gt;
@@ -657,8 +662,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,&lt;br/&gt;
         final String applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = config.getMainConsumerConfigs(applicationId, threadClientId);&lt;br/&gt;
         consumerConfigs.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final AtomicBoolean versionProbingFlag = new AtomicBoolean();&lt;/li&gt;
	&lt;li&gt;consumerConfigs.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, versionProbingFlag);&lt;br/&gt;
+        final AtomicInteger assignmentErrorCode = new AtomicInteger();&lt;br/&gt;
+        consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);&lt;br/&gt;
         String originalReset = null;&lt;br/&gt;
         if (!builder.latestResetTopicsPattern().pattern().equals(&quot;&quot;) || !builder.earliestResetTopicsPattern().pattern().equals(&quot;&quot;)) 
{
             originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);
@@ -679,7 +684,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,
             builder,
             threadClientId,
             logContext,
-            versionProbingFlag);
+            assignmentErrorCode);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public StreamThread(final Time time,&lt;br/&gt;
@@ -693,7 +698,7 @@ public StreamThread(final Time time,&lt;br/&gt;
                         final InternalTopologyBuilder builder,&lt;br/&gt;
                         final String threadClientId,&lt;br/&gt;
                         final LogContext logContext,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final AtomicBoolean versionProbingFlag) {&lt;br/&gt;
+                        final AtomicInteger assignmentErrorCode) {&lt;br/&gt;
         super(threadClientId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         this.stateLock = new Object();&lt;br/&gt;
@@ -710,7 +715,7 @@ public StreamThread(final Time time,&lt;br/&gt;
         this.restoreConsumer = restoreConsumer;&lt;br/&gt;
         this.consumer = consumer;&lt;br/&gt;
         this.originalReset = originalReset;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.versionProbingFlag = versionProbingFlag;&lt;br/&gt;
+        this.assignmentErrorCode = assignmentErrorCode;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         this.pollTime = Duration.ofMillis(config.getLong(StreamsConfig.POLL_MS_CONFIG));&lt;br/&gt;
         this.commitTimeMs = config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG);&lt;br/&gt;
@@ -765,7 +770,7 @@ private void runLoop() {&lt;br/&gt;
         while (isRunning()) {&lt;br/&gt;
             try {&lt;br/&gt;
                 recordsProcessedBeforeCommit = runOnce(recordsProcessedBeforeCommit);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (versionProbingFlag.get()) {&lt;br/&gt;
+                if (assignmentErrorCode.get() == StreamsPartitionAssignor.Error.VERSION_PROBING.code()) 
{
                     log.info(&quot;Version probing detected. Triggering new rebalance.&quot;);
                     enforceRebalance();
                 }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
index db94ac0c852..d81d4f1511f 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
@@ -51,7 +51,7 @@&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
-import java.util.concurrent.atomic.AtomicBoolean;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicInteger;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static org.apache.kafka.common.utils.Utils.getHost;&lt;br/&gt;
 import static org.apache.kafka.common.utils.Utils.getPort;&lt;br/&gt;
@@ -59,16 +59,44 @@&lt;br/&gt;
 public class StreamsPartitionAssignor implements PartitionAssignor, Configurable {&lt;/p&gt;

&lt;p&gt;     private final static int UNKNOWN = -1;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public final static int NOT_AVAILABLE = -2;&lt;br/&gt;
     private final static int VERSION_ONE = 1;&lt;br/&gt;
     private final static int VERSION_TWO = 2;&lt;br/&gt;
     private final static int VERSION_THREE = 3;&lt;br/&gt;
+    private final static int VERSION_FOUR = 4;&lt;br/&gt;
     private final static int EARLIEST_PROBEABLE_VERSION = VERSION_THREE;&lt;br/&gt;
     private int minReceivedMetadataVersion = UNKNOWN;&lt;br/&gt;
     protected Set&amp;lt;Integer&amp;gt; supportedVersions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private Logger log;&lt;br/&gt;
     private String logPrefix;&lt;br/&gt;
+    public enum Error {&lt;br/&gt;
+        NONE(0),&lt;br/&gt;
+        INCOMPLETE_SOURCE_TOPIC_METADATA(1),&lt;br/&gt;
+        VERSION_PROBING(2);&lt;br/&gt;
+&lt;br/&gt;
+        private final int code;&lt;br/&gt;
+&lt;br/&gt;
+        Error(final int code) &lt;/p&gt;
{
+            this.code = code;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public int code() &lt;/p&gt;
{
+            return code;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public static Error fromCode(final int code) {&lt;br/&gt;
+            switch (code) &lt;/p&gt;
{
+                case 0:
+                    return NONE;
+                case 1:
+                    return INCOMPLETE_SOURCE_TOPIC_METADATA;
+                case 2:
+                    return VERSION_PROBING;
+                default:
+                    throw new IllegalArgumentException(&quot;Unknown error code: &quot; + code);
+            }
&lt;p&gt;+        }&lt;br/&gt;
+    }&lt;/p&gt;

&lt;p&gt;     private static class AssignedPartition implements Comparable&amp;lt;AssignedPartition&amp;gt; {&lt;br/&gt;
         public final TaskId taskId;&lt;br/&gt;
@@ -185,7 +213,7 @@ public int compare(final TopicPartition p1,&lt;/p&gt;

&lt;p&gt;     private TaskManager taskManager;&lt;br/&gt;
     private PartitionGrouper partitionGrouper;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private AtomicBoolean versionProbingFlag;&lt;br/&gt;
+    private AtomicInteger assignmentErrorCode;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     protected int usedSubscriptionMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;&lt;/p&gt;

&lt;p&gt;@@ -250,20 +278,20 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;/p&gt;

&lt;p&gt;         taskManager = (TaskManager) o;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Object o2 = configs.get(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG);&lt;/li&gt;
	&lt;li&gt;if (o2 == null) {&lt;/li&gt;
	&lt;li&gt;final KafkaException fatalException = new KafkaException(&quot;VersionProbingFlag is not specified&quot;);&lt;br/&gt;
+        final Object ai = configs.get(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE);&lt;br/&gt;
+        if (ai == null) 
{
+            final KafkaException fatalException = new KafkaException(&quot;assignmentErrorCode is not specified&quot;);
             log.error(fatalException.getMessage(), fatalException);
             throw fatalException;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!(o2 instanceof AtomicBoolean)) {&lt;/li&gt;
	&lt;li&gt;final KafkaException fatalException = new KafkaException(String.format(&quot;%s is not an instance of %s&quot;, o2.getClass().getName(), AtomicBoolean.class.getName()));&lt;br/&gt;
+        if (!(ai instanceof AtomicInteger)) 
{
+            final KafkaException fatalException = new KafkaException(String.format(&quot;%s is not an instance of %s&quot;,
+                ai.getClass().getName(), AtomicInteger.class.getName()));
             log.error(fatalException.getMessage(), fatalException);
             throw fatalException;
         }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;versionProbingFlag = (AtomicBoolean) o2;&lt;br/&gt;
+        assignmentErrorCode = (AtomicInteger) ai;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         numStandbyReplicas = streamsConfig.getInt(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);&lt;/p&gt;

&lt;p&gt;@@ -319,6 +347,26 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) &lt;/p&gt;
{
         return new Subscription(new ArrayList&amp;lt;&amp;gt;(topics), data.encode());
     }

&lt;p&gt;+    Map&amp;lt;String, Assignment&amp;gt; errorAssignment(final Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata,&lt;br/&gt;
+                                            final String topic,&lt;br/&gt;
+                                            final int errorCode) {&lt;br/&gt;
+        log.error(&quot;{} is unknown yet during rebalance,&quot; +&lt;br/&gt;
+            &quot; please make sure they have been pre-created before starting the Streams application.&quot;, topic);&lt;br/&gt;
+        final Map&amp;lt;String, Assignment&amp;gt; assignment = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final ClientMetadata clientMetadata : clientsMetadata.values()) {&lt;br/&gt;
+            for (final String consumerId : clientMetadata.consumers) &lt;/p&gt;
{
+                assignment.put(consumerId, new Assignment(
+                    Collections.emptyList(),
+                    new AssignmentInfo(AssignmentInfo.LATEST_SUPPORTED_VERSION,
+                        Collections.emptyList(),
+                        Collections.emptyMap(),
+                        Collections.emptyMap(),
+                        errorCode).encode()
+                ));
+            }
&lt;p&gt;+        }&lt;br/&gt;
+        return assignment;&lt;br/&gt;
+    }&lt;br/&gt;
     /*&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This assigns tasks to consumer clients in the following steps.&lt;br/&gt;
      *&lt;br/&gt;
@@ -409,6 +457,12 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Map&amp;lt;String, InternalTopicMetadata&amp;gt; repartitionTopicMetadata = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {&lt;br/&gt;
+            for (final String topic : topicsInfo.sourceTopics) {&lt;br/&gt;
+                if (!topicsInfo.repartitionSourceTopics.keySet().contains(topic) &amp;amp;&amp;amp;&lt;br/&gt;
+                    !metadata.topics().contains(topic)) &lt;/p&gt;
{
+                    return errorAssignment(clientsMetadata, topic, Error.INCOMPLETE_SOURCE_TOPIC_METADATA.code);
+                }
&lt;p&gt;+            }&lt;br/&gt;
             for (final InternalTopicConfig topic: topicsInfo.repartitionSourceTopics.values()) &lt;/p&gt;
{
                 repartitionTopicMetadata.put(topic.name(), new InternalTopicMetadata(topic));
             }
&lt;p&gt;@@ -438,12 +492,9 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) &lt;/p&gt;
{
                                         numPartitionsCandidate = repartitionTopicMetadata.get(sourceTopicName).numPartitions;
                                     }
&lt;p&gt; else {&lt;br/&gt;
                                         numPartitionsCandidate = metadata.partitionCountForTopic(sourceTopicName);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numPartitionsCandidate == null) 
{
-                                            repartitionTopicMetadata.get(topicName).numPartitions = NOT_AVAILABLE;
-                                        }
&lt;p&gt;                                     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numPartitionsCandidate != null &amp;amp;&amp;amp; numPartitionsCandidate &amp;gt; numPartitions) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                                    if (numPartitionsCandidate &amp;gt; numPartitions) {
                                         numPartitions = numPartitionsCandidate;
                                     }                                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -582,7 +633,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // construct the global partition assignment per host map&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (minReceivedMetadataVersion == 2 || minReceivedMetadataVersion == 3) {&lt;br/&gt;
+        if (minReceivedMetadataVersion &amp;gt;= 2) {&lt;br/&gt;
             for (final Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
                 final HostInfo hostInfo = entry.getValue().hostInfo;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -658,7 +709,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) &lt;/p&gt;
{
                 // finally, encode the assignment before sending back to coordinator
                 assignment.put(consumer, new Assignment(
                     activePartitions,
-                    new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));
+                    new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState, 0).encode()));
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;@@ -698,7 +749,8 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) &lt;/p&gt;
{
                         minUserMetadataVersion,
                         activeTasks,
                         standbyTasks,
-                        partitionsByHostState)
+                        partitionsByHostState,
+                        0)
                         .encode()
                 ));
             }
&lt;p&gt;@@ -744,6 +796,11 @@ public void onAssignment(final Assignment assignment) {&lt;br/&gt;
         Collections.sort(partitions, PARTITION_COMPARATOR);&lt;/p&gt;

&lt;p&gt;         final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());&lt;br/&gt;
+        if (info.errCode() != Error.NONE.code) &lt;/p&gt;
{
+            // set flag to shutdown streams app
+            assignmentErrorCode.set(info.errCode());
+            return;
+        }
&lt;p&gt;         final int receivedAssignmentMetadataVersion = info.version();&lt;br/&gt;
         final int leaderSupportedVersion = info.latestSupportedVersion();&lt;/p&gt;

&lt;p&gt;@@ -770,7 +827,7 @@ public void onAssignment(final Assignment assignment) &lt;/p&gt;
{
                 usedSubscriptionMetadataVersion = leaderSupportedVersion;
             }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;versionProbingFlag.set(true);&lt;br/&gt;
+            assignmentErrorCode.set(Error.VERSION_PROBING.code);&lt;br/&gt;
             return;&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -801,6 +858,18 @@ public void onAssignment(final Assignment assignment) {&lt;br/&gt;
                 processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);&lt;br/&gt;
                 partitionsByHost = info.partitionsByHost();&lt;br/&gt;
                 break;&lt;br/&gt;
+            case VERSION_FOUR:&lt;br/&gt;
+                if (leaderSupportedVersion &amp;gt; usedSubscriptionMetadataVersion) {&lt;br/&gt;
+                    log.info(&quot;Sent a version {} subscription and group leader&apos;s latest supported version is {}. &quot; +&lt;br/&gt;
+                        &quot;Upgrading subscription metadata version to {} for next rebalance.&quot;,&lt;br/&gt;
+                        usedSubscriptionMetadataVersion,&lt;br/&gt;
+                        leaderSupportedVersion,&lt;br/&gt;
+                        leaderSupportedVersion);&lt;br/&gt;
+                    usedSubscriptionMetadataVersion = leaderSupportedVersion;&lt;br/&gt;
+                }&lt;br/&gt;
+                processVersionFourAssignment(info, partitions, activeTasks, topicToPartitionInfo);&lt;br/&gt;
+                partitionsByHost = info.partitionsByHost();&lt;br/&gt;
+                break;&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalStateException(&quot;This code should never be reached. Please file a bug report at &lt;a href=&quot;https://issues.apache.org/jira/projects/KAFKA/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/projects/KAFKA/&lt;/a&gt;&quot;);&lt;br/&gt;
         }&lt;br/&gt;
@@ -854,6 +923,13 @@ private void processVersionThreeAssignment(final AssignmentInfo info,&lt;br/&gt;
         processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    private void processVersionFourAssignment(final AssignmentInfo info,&lt;br/&gt;
+                                              final List&amp;lt;TopicPartition&amp;gt; partitions,&lt;br/&gt;
+                                              final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks,&lt;br/&gt;
+                                              final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo) &lt;/p&gt;
{
+        processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+    }
&lt;p&gt;+&lt;br/&gt;
     // for testing&lt;br/&gt;
     protected void processLatestVersionAssignment(final AssignmentInfo info,&lt;br/&gt;
                                                   final List&amp;lt;TopicPartition&amp;gt; partitions,&lt;br/&gt;
@@ -877,9 +953,6 @@ private void prepareTopic(final Map&amp;lt;String, InternalTopicMetadata&amp;gt; topicPartitio&lt;br/&gt;
             final InternalTopicConfig topic = metadata.config;&lt;br/&gt;
             final int numPartitions = metadata.numPartitions;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numPartitions == NOT_AVAILABLE) 
{
-                continue;
-            }
&lt;p&gt;             if (numPartitions &amp;lt; 0) &lt;/p&gt;
{
                 throw new StreamsException(String.format(&quot;%sTopic [%s] number of partitions not defined&quot;, logPrefix, topic.name()));
             }
&lt;p&gt;@@ -905,9 +978,12 @@ private void ensureCopartitioning(final Collection&amp;lt;Set&amp;lt;String&amp;gt;&amp;gt; copartitionGroup&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     static class CopartitionedTopicsValidator {&lt;br/&gt;
         private final String logPrefix;&lt;br/&gt;
+        private Logger log;&lt;/p&gt;

&lt;p&gt;         CopartitionedTopicsValidator(final String logPrefix) &lt;/p&gt;
{
             this.logPrefix = logPrefix;
+            final LogContext logContext = new LogContext(logPrefix);
+            log = logContext.logger(getClass());
         }

&lt;p&gt;         void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,&lt;br/&gt;
@@ -918,9 +994,10 @@ void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,&lt;br/&gt;
             for (final String topic : copartitionGroup) {&lt;br/&gt;
                 if (!allRepartitionTopicsNumPartitions.containsKey(topic)) {&lt;br/&gt;
                     final Integer partitions = metadata.partitionCountForTopic(topic);&lt;br/&gt;
-&lt;br/&gt;
                     if (partitions == null) &lt;/p&gt;
{
-                        throw new org.apache.kafka.streams.errors.TopologyException(String.format(&quot;%sTopic not found: %s&quot;, logPrefix, topic));
+                        String str = String.format(&quot;%sTopic not found: %s&quot;, logPrefix, topic);
+                        log.error(str);
+                        throw new IllegalStateException(str);
                     }

&lt;p&gt;                     if (numPartitions == UNKNOWN) &lt;/p&gt;
{
@@ -930,9 +1007,6 @@ void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,
                         Arrays.sort(topics);
                         throw new org.apache.kafka.streams.errors.TopologyException(String.format(&quot;%sTopics not co-partitioned: [%s]&quot;, logPrefix, Utils.join(Arrays.asList(topics), &quot;,&quot;)));
                     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;} else if (allRepartitionTopicsNumPartitions.get(topic).numPartitions == NOT_AVAILABLE) 
{
-                    numPartitions = NOT_AVAILABLE;
-                    break;
                 }
&lt;p&gt;             }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index c577830e3e2..1179ca04097 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -41,11 +41,12 @@&lt;/p&gt;

&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(AssignmentInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final int LATEST_SUPPORTED_VERSION = 3;&lt;br/&gt;
+    public static final int LATEST_SUPPORTED_VERSION = 4;&lt;br/&gt;
     static final int UNKNOWN = -1;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final int usedVersion;&lt;br/&gt;
     private final int latestSupportedVersion;&lt;br/&gt;
+    private int errCode;&lt;br/&gt;
     private List&amp;lt;TaskId&amp;gt; activeTasks;&lt;br/&gt;
     private Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks;&lt;br/&gt;
     private Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost;&lt;br/&gt;
@@ -55,27 +56,29 @@ private AssignmentInfo(final int version,&lt;br/&gt;
                            final int latestSupportedVersion) &lt;/p&gt;
{
         this.usedVersion = version;
         this.latestSupportedVersion = latestSupportedVersion;
+        this.errCode = 0;
     }

&lt;p&gt;     public AssignmentInfo(final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
                           final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
                           final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) &lt;/p&gt;
{
-        this(LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);
+        this(LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState, 0);
     }

&lt;p&gt;     public AssignmentInfo() &lt;/p&gt;
{
         this(LATEST_SUPPORTED_VERSION,
             Collections.emptyList(),
             Collections.emptyMap(),
-            Collections.emptyMap());
+            Collections.emptyMap(),
+            0);
     }

&lt;p&gt;     public AssignmentInfo(final int version,&lt;br/&gt;
                           final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
                           final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;/li&gt;
	&lt;li&gt;this(version, LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
-&lt;br/&gt;
+                          final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState,&lt;br/&gt;
+                          final int errCode) {&lt;br/&gt;
+        this(version, LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState, errCode);&lt;br/&gt;
         if (version &amp;lt; 1 || version &amp;gt; LATEST_SUPPORTED_VERSION) {&lt;br/&gt;
             throw new IllegalArgumentException(&quot;version must be between 1 and &quot; + LATEST_SUPPORTED_VERSION&lt;br/&gt;
                 + &quot;; was: &quot; + version);&lt;br/&gt;
@@ -87,18 +90,24 @@ public AssignmentInfo(final int version,&lt;br/&gt;
                    final int latestSupportedVersion,&lt;br/&gt;
                    final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
                    final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;/li&gt;
	&lt;li&gt;final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
+                   final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState,&lt;br/&gt;
+                   final int errCode) 
{
         this.usedVersion = version;
         this.latestSupportedVersion = latestSupportedVersion;
         this.activeTasks = activeTasks;
         this.standbyTasks = standbyTasks;
         this.partitionsByHost = hostState;
+        this.errCode = errCode;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public int version() &lt;/p&gt;
{
         return usedVersion;
     }

&lt;p&gt;+    public int errCode() &lt;/p&gt;
{
+        return errCode;
+    }
&lt;p&gt;+&lt;br/&gt;
     public int latestSupportedVersion() &lt;/p&gt;
{
         return latestSupportedVersion;
     }
&lt;p&gt;@@ -133,6 +142,9 @@ public ByteBuffer encode() {&lt;br/&gt;
                 case 3:&lt;br/&gt;
                     encodeVersionThree(out);&lt;br/&gt;
                     break;&lt;br/&gt;
+                case 4:&lt;br/&gt;
+                    encodeVersionFour(out);&lt;br/&gt;
+                    break;&lt;br/&gt;
                 default:&lt;br/&gt;
                     throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion&lt;br/&gt;
                         + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);&lt;br/&gt;
@@ -203,6 +215,14 @@ private void encodeVersionThree(final DataOutputStream out) throws IOException &lt;/p&gt;
{
         encodePartitionsByHost(out);
     }

&lt;p&gt;+    private void encodeVersionFour(final DataOutputStream out) throws IOException &lt;/p&gt;
{
+        out.writeInt(4);
+        out.writeInt(LATEST_SUPPORTED_VERSION);
+        encodeActiveAndStandbyTaskAssignment(out);
+        encodePartitionsByHost(out);
+        out.writeInt(errCode);
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskAssignmentException if method fails to decode the data or if the data version is unknown&lt;br/&gt;
      */&lt;br/&gt;
@@ -214,6 +234,7 @@ public static AssignmentInfo decode(final ByteBuffer data) {&lt;br/&gt;
             final AssignmentInfo assignmentInfo;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final int usedVersion = in.readInt();&lt;br/&gt;
+            int latestSupportedVersion;&lt;br/&gt;
             switch (usedVersion) {&lt;br/&gt;
                 case 1:&lt;br/&gt;
                     assignmentInfo = new AssignmentInfo(usedVersion, UNKNOWN);&lt;br/&gt;
@@ -224,10 +245,15 @@ public static AssignmentInfo decode(final ByteBuffer data) &lt;/p&gt;
{
                     decodeVersionTwoData(assignmentInfo, in);
                     break;
                 case 3:
-                    final int latestSupportedVersion = in.readInt();
+                    latestSupportedVersion = in.readInt();
                     assignmentInfo = new AssignmentInfo(usedVersion, latestSupportedVersion);
                     decodeVersionThreeData(assignmentInfo, in);
                     break;
+                case 4:
+                    latestSupportedVersion = in.readInt();
+                    assignmentInfo = new AssignmentInfo(usedVersion, latestSupportedVersion);
+                    decodeVersionFourData(assignmentInfo, in);
+                    break;
                 default:
                     final TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode assignment data: &quot; +
                         &quot;used version: &quot; + usedVersion + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);
@@ -300,9 +326,16 @@ private static void decodeVersionThreeData(final AssignmentInfo assignmentInfo,
         decodeGlobalAssignmentData(assignmentInfo, in);
     }

&lt;p&gt;+    private static void decodeVersionFourData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
+                                              final DataInputStream in) throws IOException &lt;/p&gt;
{
+        decodeVersionThreeData(assignmentInfo, in);
+        assignmentInfo.errCode = in.readInt();
+    }
&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public int hashCode() &lt;/p&gt;
{
-        return usedVersion ^ latestSupportedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
+        return usedVersion ^ latestSupportedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode()
+            ^ partitionsByHost.hashCode() ^ errCode;
     }

&lt;p&gt;     @Override&lt;br/&gt;
@@ -311,6 +344,7 @@ public boolean equals(final Object o) {&lt;br/&gt;
             final AssignmentInfo other = (AssignmentInfo) o;&lt;br/&gt;
             return usedVersion == other.usedVersion &amp;amp;&amp;amp;&lt;br/&gt;
                     latestSupportedVersion == other.latestSupportedVersion &amp;amp;&amp;amp;&lt;br/&gt;
+                    errCode == other.errCode &amp;amp;&amp;amp;&lt;br/&gt;
                     activeTasks.equals(other.activeTasks) &amp;amp;&amp;amp;&lt;br/&gt;
                     standbyTasks.equals(other.standbyTasks) &amp;amp;&amp;amp;&lt;br/&gt;
                     partitionsByHost.equals(other.partitionsByHost);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index 4ebc95674b0..b4ad19f0204 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -32,7 +32,7 @@&lt;/p&gt;

&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final int LATEST_SUPPORTED_VERSION = 3;&lt;br/&gt;
+    public static final int LATEST_SUPPORTED_VERSION = 4;&lt;br/&gt;
     static final int UNKNOWN = -1;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final int usedVersion;&lt;br/&gt;
@@ -124,6 +124,9 @@ public ByteBuffer encode() {&lt;br/&gt;
             case 3:&lt;br/&gt;
                 buf = encodeVersionThree();&lt;br/&gt;
                 break;&lt;br/&gt;
+            case 4:&lt;br/&gt;
+                buf = encodeVersionFour();&lt;br/&gt;
+                break;&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion&lt;br/&gt;
                     + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);&lt;br/&gt;
@@ -205,7 +208,7 @@ protected void encodeUserEndPoint(final ByteBuffer buf,&lt;br/&gt;
     private ByteBuffer encodeVersionThree() {&lt;br/&gt;
         final byte[] endPointBytes = prepareUserEndPoint();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeByteLength(endPointBytes));&lt;br/&gt;
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeAndFourByteLength(endPointBytes));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         buf.putInt(3); // used version&lt;br/&gt;
         buf.putInt(LATEST_SUPPORTED_VERSION); // supported version&lt;br/&gt;
@@ -217,7 +220,22 @@ private ByteBuffer encodeVersionThree() &lt;/p&gt;
{
         return buf;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected int getVersionThreeByteLength(final byte[] endPointBytes) {&lt;br/&gt;
+    private ByteBuffer encodeVersionFour() 
{
+        final byte[] endPointBytes = prepareUserEndPoint();
+
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeAndFourByteLength(endPointBytes));
+
+        buf.putInt(4); // used version
+        buf.putInt(LATEST_SUPPORTED_VERSION); // supported version
+        encodeClientUUID(buf);
+        encodeTasks(buf, prevTasks);
+        encodeTasks(buf, standbyTasks);
+        encodeUserEndPoint(buf, endPointBytes);
+
+        return buf;
+    }
&lt;p&gt;+&lt;br/&gt;
+    protected int getVersionThreeAndFourByteLength(final byte[] endPointBytes) {&lt;br/&gt;
         return 4 + // used version&lt;br/&gt;
                4 + // latest supported version version&lt;br/&gt;
                16 + // client ID&lt;br/&gt;
@@ -247,6 +265,7 @@ public static SubscriptionInfo decode(final ByteBuffer data) {&lt;br/&gt;
                 decodeVersionTwoData(subscriptionInfo, data);&lt;br/&gt;
                 break;&lt;br/&gt;
             case 3:&lt;br/&gt;
+            case 4:&lt;br/&gt;
                 latestSupportedVersion = data.getInt();&lt;br/&gt;
                 subscriptionInfo = new SubscriptionInfo(usedVersion, latestSupportedVersion);&lt;br/&gt;
                 decodeVersionThreeData(subscriptionInfo, data);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsWrapper.java b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsWrapper.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..9dd1fc1dcee&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsWrapper.java&lt;br/&gt;
@@ -0,0 +1,51 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams.State;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StreamThread;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ *  This class allows to access the 
{@link KafkaStreams}
&lt;p&gt; a &lt;/p&gt;
{@link StreamThread.StateListener} object.&lt;br/&gt;
+ *&lt;br/&gt;
+ */&lt;br/&gt;
+public class KafkaStreamsWrapper extends KafkaStreams {&lt;br/&gt;
+&lt;br/&gt;
+    public KafkaStreamsWrapper(final Topology topology,&lt;br/&gt;
+                               final Properties props) {
+        super(topology, props);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * An app can set a single {@link StreamThread.StateListener}
&lt;p&gt; so that the app is notified when state changes.&lt;br/&gt;
+     *&lt;br/&gt;
+     * @param listener a new StreamThread state listener&lt;br/&gt;
+     * @throws IllegalStateException if this &lt;/p&gt;
{@code KafkaStreams}
&lt;p&gt; instance is not in state &lt;/p&gt;
{@link State#CREATED CREATED}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public void setStreamThreadStateListener(final StreamThread.StateListener listener) {&lt;br/&gt;
+        if (state == State.CREATED) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (final StreamThread thread }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else &lt;/p&gt;
{
+            throw new IllegalStateException(&quot;Can only set StateListener in CREATED state. &quot; +
+                &quot;Current state is: &quot; + state);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
index 2ab6639ce05..a0b8f3d5aca 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
@@ -37,6 +37,8 @@&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StreamThread;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.ThreadStateTransitionValidator;&lt;br/&gt;
 import org.apache.kafka.test.TestCondition;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import scala.Option;&lt;br/&gt;
@@ -61,6 +63,32 @@&lt;br/&gt;
     public static final long DEFAULT_TIMEOUT = 30 * 1000L;&lt;br/&gt;
     public static final String INTERNAL_LEAVE_GROUP_ON_CLOSE = &quot;internal.leave.group.on.close&quot;;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /*&lt;br/&gt;
+     * Records state transition for StreamThread&lt;br/&gt;
+     */&lt;br/&gt;
+    public static class StateListenerStub implements StreamThread.StateListener {&lt;br/&gt;
+        boolean runningToRevokedSeen = false;&lt;br/&gt;
+        boolean revokedToPendingShutdownSeen = false;&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onChange(final Thread thread,&lt;br/&gt;
+                             final ThreadStateTransitionValidator newState,&lt;br/&gt;
+                             final ThreadStateTransitionValidator oldState) {&lt;br/&gt;
+            if (oldState == StreamThread.State.RUNNING &amp;amp;&amp;amp; newState == StreamThread.State.PARTITIONS_REVOKED) &lt;/p&gt;
{
+                runningToRevokedSeen = true;
+            }
&lt;p&gt; else if (oldState == StreamThread.State.PARTITIONS_REVOKED &amp;amp;&amp;amp; newState == StreamThread.State.PENDING_SHUTDOWN) &lt;/p&gt;
{
+                revokedToPendingShutdownSeen = true;
+            }
&lt;p&gt;+        }&lt;br/&gt;
+&lt;br/&gt;
+        public boolean revokedToPendingShutdownSeen() &lt;/p&gt;
{
+            return revokedToPendingShutdownSeen;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public boolean runningToRevokedSeen() &lt;/p&gt;
{
+            return runningToRevokedSeen;
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Removes local state stores.  Useful to reset state in-between integration test runs.&lt;br/&gt;
      *&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/DefaultPartitionGrouperTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/DefaultPartitionGrouperTest.java&lt;br/&gt;
index de197fe3573..b1c36843db6 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/DefaultPartitionGrouperTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/DefaultPartitionGrouperTest.java&lt;br/&gt;
@@ -91,16 +91,14 @@ public void shouldComputeGroupingForSingleGroupWithMultipleTopics() 
{
         assertEquals(expectedPartitionsForTask, grouper.partitionGroups(topicGroups, metadata));
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;br/&gt;
+    @Test(expected = RuntimeException.class)&lt;br/&gt;
     public void shouldNotCreateAnyTasksBecauseOneTopicHasUnknownPartitions() 
{
         final PartitionGrouper grouper = new DefaultPartitionGrouper();
-        final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; expectedPartitionsForTask = new HashMap&amp;lt;&amp;gt;();
         final Map&amp;lt;Integer, Set&amp;lt;String&amp;gt;&amp;gt; topicGroups = new HashMap&amp;lt;&amp;gt;();
-
+    
         final int topicGroupId = 0;
-
+    
         topicGroups.put(topicGroupId, mkSet(&quot;topic1&quot;, &quot;unknownTopic&quot;, &quot;topic2&quot;));
-
-        assertEquals(expectedPartitionsForTask, grouper.partitionGroups(topicGroups, metadata));
+        grouper.partitionGroups(topicGroups, metadata);
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java&lt;br/&gt;
index b8221d8642a..a71f4883c64 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java&lt;br/&gt;
@@ -46,7 +46,7 @@ public void before() 
{
         partitions.put(new TopicPartition(&quot;second&quot;, 1), new PartitionInfo(&quot;second&quot;, 1, null, null, null));
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = TopologyException.class)&lt;br/&gt;
+    @Test(expected = IllegalStateException.class)&lt;br/&gt;
     public void shouldThrowTopologyBuilderExceptionIfNoPartitionsFoundForCoPartitionedTopic() 
{
         validator.validate(Collections.singleton(&quot;topic&quot;),
                            Collections.&amp;lt;String, StreamsPartitionAssignor.InternalTopicMetadata&amp;gt;emptyMap(),
@@ -98,27 +98,6 @@ public void shouldSetNumPartitionsToMaximumPartitionsWhenAllTopicsAreRepartition
         assertThat(three.numPartitions, equalTo(15));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldSetRepartitionTopicsPartitionCountToNotAvailableIfAnyNotAvaliable() 
{
-        final StreamsPartitionAssignor.InternalTopicMetadata one = createTopicMetadata(&quot;one&quot;, 1);
-        final StreamsPartitionAssignor.InternalTopicMetadata two = createTopicMetadata(&quot;two&quot;, StreamsPartitionAssignor.NOT_AVAILABLE);
-        final Map&amp;lt;String, StreamsPartitionAssignor.InternalTopicMetadata&amp;gt; repartitionTopicConfig = new HashMap&amp;lt;&amp;gt;();
-
-        repartitionTopicConfig.put(one.config.name(), one);
-        repartitionTopicConfig.put(two.config.name(), two);
-
-        validator.validate(Utils.mkSet(&quot;first&quot;,
-                                       &quot;second&quot;,
-                                       one.config.name(),
-                                       two.config.name()),
-                           repartitionTopicConfig,
-                           cluster.withPartitions(partitions));
-
-        assertThat(one.numPartitions, equalTo(StreamsPartitionAssignor.NOT_AVAILABLE));
-        assertThat(two.numPartitions, equalTo(StreamsPartitionAssignor.NOT_AVAILABLE));
-
-    }
&lt;p&gt;-&lt;br/&gt;
     private StreamsPartitionAssignor.InternalTopicMetadata createTopicMetadata(final String repartitionTopic,&lt;br/&gt;
                                                                                final int partitions) {&lt;br/&gt;
         final InternalTopicConfig repartitionTopicConfig&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
index 2ccc89348fb..93d4e94a234 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
@@ -82,7 +82,7 @@&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
-import java.util.concurrent.atomic.AtomicBoolean;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicInteger;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static java.util.Collections.singletonList;&lt;br/&gt;
 import static org.apache.kafka.common.utils.Utils.mkEntry;&lt;br/&gt;
@@ -305,7 +305,7 @@ public void shouldNotCommitBeforeTheCommitInterval() {&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;br/&gt;
             new LogContext(&quot;&quot;),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new AtomicBoolean()&lt;br/&gt;
+            new AtomicInteger()&lt;br/&gt;
         );&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
         mockTime.sleep(commitInterval - 10L);&lt;br/&gt;
@@ -339,7 +339,7 @@ public void shouldNotCauseExceptionIfNothingCommitted() {&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;br/&gt;
             new LogContext(&quot;&quot;),&lt;/li&gt;
	&lt;li&gt;new AtomicBoolean()&lt;br/&gt;
+            new AtomicInteger()&lt;br/&gt;
         );&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
         mockTime.sleep(commitInterval - 10L);&lt;br/&gt;
@@ -374,7 +374,7 @@ public void shouldCommitAfterTheCommitInterval() {&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;br/&gt;
             new LogContext(&quot;&quot;),&lt;/li&gt;
	&lt;li&gt;new AtomicBoolean()&lt;br/&gt;
+            new AtomicInteger()&lt;br/&gt;
         );&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
         mockTime.sleep(commitInterval + 1);&lt;br/&gt;
@@ -523,7 +523,7 @@ public void shouldShutdownTaskManagerOnClose() {&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;br/&gt;
             new LogContext(&quot;&quot;),&lt;/li&gt;
	&lt;li&gt;new AtomicBoolean()&lt;br/&gt;
+            new AtomicInteger()&lt;br/&gt;
         );&lt;br/&gt;
         thread.setStateListener(&lt;br/&gt;
             new StreamThread.StateListener() {&lt;br/&gt;
@@ -560,7 +560,7 @@ public void shouldShutdownTaskManagerOnCloseWithoutStart() {&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;br/&gt;
             new LogContext(&quot;&quot;),&lt;/li&gt;
	&lt;li&gt;new AtomicBoolean()&lt;br/&gt;
+            new AtomicInteger()&lt;br/&gt;
         );&lt;br/&gt;
         thread.shutdown();&lt;br/&gt;
         EasyMock.verify(taskManager);&lt;br/&gt;
@@ -588,7 +588,7 @@ public void shouldOnlyShutdownOnce() {&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;br/&gt;
             new LogContext(&quot;&quot;),&lt;/li&gt;
	&lt;li&gt;new AtomicBoolean()&lt;br/&gt;
+            new AtomicInteger()&lt;br/&gt;
         );&lt;br/&gt;
         thread.shutdown();&lt;br/&gt;
         // Execute the run method. Verification of the mock will check that shutdown was only done once&lt;br/&gt;
@@ -1288,7 +1288,8 @@ public void producerMetricsVerificationWithoutEOS() {&lt;br/&gt;
                 internalTopologyBuilder,&lt;br/&gt;
                 clientId,&lt;br/&gt;
                 new LogContext(&quot;&quot;),&lt;/li&gt;
	&lt;li&gt;new AtomicBoolean());&lt;br/&gt;
+                new AtomicInteger()&lt;br/&gt;
+                );&lt;br/&gt;
         final MetricName testMetricName = new MetricName(&quot;test_metric&quot;, &quot;&quot;, &quot;&quot;, new HashMap&amp;lt;String, String&amp;gt;());&lt;br/&gt;
         final Metric testMetric = new KafkaMetric(&lt;br/&gt;
                 new Object(),&lt;br/&gt;
@@ -1331,7 +1332,8 @@ public void adminClientMetricsVerification() 
{
                 internalTopologyBuilder,
                 clientId,
                 new LogContext(&quot;&quot;),
-                new AtomicBoolean());
+                new AtomicInteger()
+                );
         final MetricName testMetricName = new MetricName(&quot;test_metric&quot;, &quot;&quot;, &quot;&quot;, new HashMap&amp;lt;String, String&amp;gt;());
         final Metric testMetric = new KafkaMetric(
                 new Object(),
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java
index 4327e8f1ee4..2577bb84abf 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java
@@ -58,7 +58,7 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.UUID;
-import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.not;
@@ -122,7 +122,7 @@
         configurationMap.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
         configurationMap.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, userEndPoint);
         configurationMap.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);
-        configurationMap.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, new AtomicBoolean());
+        configurationMap.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, new AtomicInteger());
         return configurationMap;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -993,20 +993,9 @@ public Object apply(final Object value1, final Object value2) &lt;/p&gt;
{
 
         final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
 
-        final Map&amp;lt;String, Integer&amp;gt; expectedCreatedInternalTopics = new HashMap&amp;lt;&amp;gt;();
-        expectedCreatedInternalTopics.put(applicationId + &quot;-count-repartition&quot;, 3);
-        expectedCreatedInternalTopics.put(applicationId + &quot;-count-changelog&quot;, 3);
-        assertThat(mockInternalTopicManager.readyTopics, equalTo(expectedCreatedInternalTopics));
+        assertThat(mockInternalTopicManager.readyTopics.isEmpty(), equalTo(true));
 
-        final List&amp;lt;TopicPartition&amp;gt; expectedAssignment = Arrays.asList(
-            new TopicPartition(&quot;topic1&quot;, 0),
-            new TopicPartition(&quot;topic1&quot;, 1),
-            new TopicPartition(&quot;topic1&quot;, 2),
-            new TopicPartition(applicationId + &quot;-count-repartition&quot;, 0),
-            new TopicPartition(applicationId + &quot;-count-repartition&quot;, 1),
-            new TopicPartition(applicationId + &quot;-count-repartition&quot;, 2)
-        );
-        assertThat(new HashSet&amp;lt;&amp;gt;(assignment.get(client).partitions()), equalTo(new HashSet&amp;lt;&amp;gt;(expectedAssignment)));
+        assertThat(assignment.get(client).partitions().isEmpty(), equalTo(true));
     }

&lt;p&gt;     @Test&lt;br/&gt;
@@ -1109,29 +1098,29 @@ public void shouldThrowKafkaExceptionIfTaskMangerConfigIsNotTaskManagerInstance(&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowKafkaExceptionVersionProbingFlagNotConfigured() {&lt;br/&gt;
+    public void shouldThrowKafkaExceptionAssignmentErrorCodeNotConfigured() {&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; config = configProps();&lt;/li&gt;
	&lt;li&gt;config.remove(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG);&lt;br/&gt;
+        config.remove(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         try &lt;/p&gt;
{
             partitionAssignor.configure(config);
             fail(&quot;Should have thrown KafkaException&quot;);
         } catch (final KafkaException expected) {
-            assertThat(expected.getMessage(), equalTo(&quot;VersionProbingFlag is not specified&quot;));
+            assertThat(expected.getMessage(), equalTo(&quot;assignmentErrorCode is not specified&quot;));
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
-    public void shouldThrowKafkaExceptionIfVersionProbingFlagConfigIsNotAtomicBoolean() {&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfVersionProbingFlagConfigIsNotAtomicInteger() {&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; config = configProps();&lt;br/&gt;
-        config.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, &quot;i am not an AtomicBoolean&quot;);&lt;br/&gt;
+        config.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, &quot;i am not an AtomicInteger&quot;);&lt;br/&gt;
 &lt;br/&gt;
         try {             partitionAssignor.configure(config);             fail(&quot;Should have thrown KafkaException&quot;);         }
&lt;p&gt; catch (final KafkaException expected) &lt;/p&gt;
{
             assertThat(expected.getMessage(),
-                equalTo(&quot;java.lang.String is not an instance of java.util.concurrent.atomic.AtomicBoolean&quot;));
+                equalTo(&quot;java.lang.String is not an instance of java.util.concurrent.atomic.AtomicInteger&quot;));
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index c7382e7671c..8b990659da1 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -59,33 +59,39 @@ public void shouldUseLatestSupportedVersionByDefault() {&lt;/p&gt;

&lt;p&gt;     @Test(expected = IllegalArgumentException.class)&lt;br/&gt;
     public void shouldThrowForUnknownVersion1() &lt;/p&gt;
{
-        new AssignmentInfo(0, activeTasks, standbyTasks, globalAssignment);
+        new AssignmentInfo(0, activeTasks, standbyTasks, globalAssignment, 0);
     }

&lt;p&gt;     @Test(expected = IllegalArgumentException.class)&lt;br/&gt;
     public void shouldThrowForUnknownVersion2() &lt;/p&gt;
{
-        new AssignmentInfo(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1, activeTasks, standbyTasks, globalAssignment);
+        new AssignmentInfo(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1, activeTasks, standbyTasks, globalAssignment, 0);
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldEncodeAndDecodeVersion1() &lt;/p&gt;
{
-        final AssignmentInfo info = new AssignmentInfo(1, activeTasks, standbyTasks, globalAssignment);
-        final AssignmentInfo expectedInfo = new AssignmentInfo(1, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, Collections.&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap());
+        final AssignmentInfo info = new AssignmentInfo(1, activeTasks, standbyTasks, globalAssignment, 0);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(1, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, Collections.&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(), 0);
         assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldEncodeAndDecodeVersion2() &lt;/p&gt;
{
-        final AssignmentInfo info = new AssignmentInfo(2, activeTasks, standbyTasks, globalAssignment);
-        final AssignmentInfo expectedInfo = new AssignmentInfo(2, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo info = new AssignmentInfo(2, activeTasks, standbyTasks, globalAssignment, 0);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(2, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, globalAssignment, 0);
         assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldEncodeAndDecodeVersion3() &lt;/p&gt;
{
-        final AssignmentInfo info = new AssignmentInfo(3, activeTasks, standbyTasks, globalAssignment);
-        final AssignmentInfo expectedInfo = new AssignmentInfo(3, AssignmentInfo.LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo info = new AssignmentInfo(3, activeTasks, standbyTasks, globalAssignment, 0);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(3, AssignmentInfo.LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, globalAssignment, 0);
         assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion4() &lt;/p&gt;
{
+        final AssignmentInfo info = new AssignmentInfo(4, activeTasks, standbyTasks, globalAssignment, 2);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(4, AssignmentInfo.LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, globalAssignment, 2);
+        assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
+    }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
index 0611bfc4d5c..2a75c57b237 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
@@ -76,6 +76,13 @@ public void shouldEncodeAndDecodeVersion3() &lt;/p&gt;
{
         assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion4() &lt;/p&gt;
{
+        final SubscriptionInfo info = new SubscriptionInfo(4, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+        final SubscriptionInfo expectedInfo = new SubscriptionInfo(4, SubscriptionInfo.LATEST_SUPPORTED_VERSION, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+        assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldAllowToDecodeFutureSupportedVersion() {&lt;br/&gt;
         final SubscriptionInfo info = SubscriptionInfo.decode(encodeFutureVersion());&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
index 1b01a7300a1..33e9b9771c2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -274,7 +274,7 @@ public ByteBuffer encode() {&lt;br/&gt;
         private ByteBuffer encodeFutureVersion() {&lt;br/&gt;
             final byte[] endPointBytes = prepareUserEndPoint();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeByteLength(endPointBytes));&lt;br/&gt;
+            final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeAndFourByteLength(endPointBytes));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             buf.putInt(LATEST_SUPPORTED_VERSION + 1); // used version&lt;br/&gt;
             buf.putInt(LATEST_SUPPORTED_VERSION + 1); // supported version&lt;br/&gt;
diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..78ed591073a&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
@@ -0,0 +1,137 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Copyright (C) 2018 Lightbend Inc. &amp;lt;&lt;a href=&quot;https://www.lightbend.com&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.lightbend.com&lt;/a&gt;&amp;gt;&lt;br/&gt;
+ * Copyright (C) 2017-2018 Alexis Seigneurin.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);&lt;br/&gt;
+ * you may not use this file except in compliance with the License.&lt;br/&gt;
+ * You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.scala&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.clients.consumer.ConsumerConfig&lt;br/&gt;
+import org.apache.kafka.clients.producer.ProducerConfig&lt;br/&gt;
+import org.apache.kafka.common.serialization._&lt;br/&gt;
+import org.apache.kafka.common.utils.MockTime&lt;br/&gt;
+import org.apache.kafka.streams._&lt;br/&gt;
+import org.apache.kafka.streams.integration.utils.&lt;/p&gt;
{EmbeddedKafkaCluster, IntegrationTestUtils}&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StreamThread&lt;br/&gt;
+import org.apache.kafka.streams.scala.ImplicitConversions._&lt;br/&gt;
+import org.apache.kafka.streams.scala.kstream._&lt;br/&gt;
+import org.apache.kafka.test.TestUtils&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
+import org.junit._&lt;br/&gt;
+import org.junit.rules.TemporaryFolder&lt;br/&gt;
+import org.scalatest.junit.JUnitSuite&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Test suite base that prepares Kafka cluster for stream-table joins in Kafka Streams&lt;br/&gt;
+ * &amp;lt;p&amp;gt;&lt;br/&gt;
+ */&lt;br/&gt;
+class StreamToTableJoinScalaIntegrationTestBase extends JUnitSuite with StreamToTableJoinTestData {&lt;br/&gt;
+&lt;br/&gt;
+  private val privateCluster: EmbeddedKafkaCluster = new EmbeddedKafkaCluster(1)&lt;br/&gt;
+&lt;br/&gt;
+  @Rule def cluster: EmbeddedKafkaCluster = privateCluster&lt;br/&gt;
+&lt;br/&gt;
+  final val alignedTime = (System.currentTimeMillis() / 1000 + 1) * 1000&lt;br/&gt;
+  val mockTime: MockTime = cluster.time&lt;br/&gt;
+  mockTime.setCurrentTimeMs(alignedTime)&lt;br/&gt;
+&lt;br/&gt;
+  val tFolder: TemporaryFolder = new TemporaryFolder(TestUtils.tempDirectory())&lt;br/&gt;
+  @Rule def testFolder: TemporaryFolder = tFolder&lt;br/&gt;
+&lt;br/&gt;
+  @Before&lt;br/&gt;
+  def startKafkaCluster(): Unit = {
+    cluster.createTopic(userClicksTopic)
+    cluster.createTopic(userRegionsTopic)
+    cluster.createTopic(outputTopic)
+    cluster.createTopic(userClicksTopicJ)
+    cluster.createTopic(userRegionsTopicJ)
+    cluster.createTopic(outputTopicJ)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def getStreamsConfiguration(): Properties = {
+    val streamsConfiguration: Properties = new Properties()
+
+    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;stream-table-join-scala-integration-test&quot;)
+    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
+    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, &quot;1000&quot;)
+    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
+    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, testFolder.getRoot.getPath)
+
+    streamsConfiguration
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def getUserRegionsProducerConfig(): Properties = {
+    val p = new Properties()
+    p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
+    p.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;)
+    p.put(ProducerConfig.RETRIES_CONFIG, &quot;0&quot;)
+    p.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])
+    p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])
+    p
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def getUserClicksProducerConfig(): Properties = {
+    val p = new Properties()
+    p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
+    p.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;)
+    p.put(ProducerConfig.RETRIES_CONFIG, &quot;0&quot;)
+    p.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])
+    p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[LongSerializer])
+    p
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def getConsumerConfig(): Properties = {
+    val p = new Properties()
+    p.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
+    p.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;join-scala-integration-test-standard-consumer&quot;)
+    p.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
+    p.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer])
+    p.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[LongDeserializer])
+    p
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def produceNConsume(userClicksTopic: String,&lt;br/&gt;
+                              userRegionsTopic: String,&lt;br/&gt;
+                              outputTopic: String,&lt;br/&gt;
+                              waitTillRecordsReceived: Boolean = true): java.util.List[KeyValue&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
+&lt;br/&gt;
+    import collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+    // Publish user-region information.&lt;br/&gt;
+    val userRegionsProducerConfig: Properties = getUserRegionsProducerConfig()&lt;br/&gt;
+    IntegrationTestUtils.produceKeyValuesSynchronously(userRegionsTopic,&lt;br/&gt;
+                                                       userRegions.asJava,&lt;br/&gt;
+                                                       userRegionsProducerConfig,&lt;br/&gt;
+                                                       mockTime,&lt;br/&gt;
+                                                       false)&lt;br/&gt;
+&lt;br/&gt;
+    // Publish user-click information.&lt;br/&gt;
+    val userClicksProducerConfig: Properties = getUserClicksProducerConfig()&lt;br/&gt;
+    IntegrationTestUtils.produceKeyValuesSynchronously(userClicksTopic,&lt;br/&gt;
+                                                       userClicks.asJava,&lt;br/&gt;
+                                                       userClicksProducerConfig,&lt;br/&gt;
+                                                       mockTime,&lt;br/&gt;
+                                                       false)&lt;br/&gt;
+&lt;br/&gt;
+    if (waitTillRecordsReceived) {
+      // consume and verify result
+      val consumerConfig = getConsumerConfig()
+
+          IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig, outputTopic, expectedClicksPerRegion.size)
+    } else {
+      java.util.Collections.emptyList()
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
index 7891131aa9e..e5253f95d45 100644&lt;br/&gt;
&amp;#8212; a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
+++ b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
@@ -24,6 +24,7 @@ import org.apache.kafka.common.serialization._&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime&lt;br/&gt;
 import org.apache.kafka.streams._&lt;br/&gt;
 import org.apache.kafka.streams.integration.utils.{EmbeddedKafkaCluster, IntegrationTestUtils}
&lt;p&gt;+import org.apache.kafka.streams.processor.internals.StreamThread&lt;br/&gt;
 import org.apache.kafka.streams.scala.ImplicitConversions._&lt;br/&gt;
 import org.apache.kafka.streams.scala.kstream._&lt;br/&gt;
 import org.apache.kafka.test.TestUtils&lt;br/&gt;
@@ -41,28 +42,7 @@ import org.scalatest.junit.JUnitSuite&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Note: In the current project settings SAM type conversion is turned off as it&apos;s experimental in Scala 2.11.&lt;/li&gt;
	&lt;li&gt;Hence the native Java API based version is more verbose.&lt;br/&gt;
  */&lt;br/&gt;
-class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends JUnitSuite with StreamToTableJoinTestData {&lt;br/&gt;
-&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private val privateCluster: EmbeddedKafkaCluster = new EmbeddedKafkaCluster(1)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@Rule def cluster: EmbeddedKafkaCluster = privateCluster&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final val alignedTime = (System.currentTimeMillis() / 1000 + 1) * 1000&lt;/li&gt;
	&lt;li&gt;val mockTime: MockTime = cluster.time&lt;/li&gt;
	&lt;li&gt;mockTime.setCurrentTimeMs(alignedTime)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val tFolder: TemporaryFolder = new TemporaryFolder(TestUtils.tempDirectory())&lt;/li&gt;
	&lt;li&gt;@Rule def testFolder: TemporaryFolder = tFolder&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@Before&lt;/li&gt;
	&lt;li&gt;def startKafkaCluster(): Unit = 
{
-    cluster.createTopic(userClicksTopic)
-    cluster.createTopic(userRegionsTopic)
-    cluster.createTopic(outputTopic)
-    cluster.createTopic(userClicksTopicJ)
-    cluster.createTopic(userRegionsTopicJ)
-    cluster.createTopic(outputTopicJ)
-  }
&lt;p&gt;+class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends StreamToTableJoinScalaIntegrationTestBase {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test def testShouldCountClicksPerRegion(): Unit = &lt;/p&gt;
{
 
@@ -101,7 +81,6 @@ class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends JUnitSuite wit
 
     val actualClicksPerRegion: java.util.List[KeyValue[String, Long]] =
       produceNConsume(userClicksTopic, userRegionsTopic, outputTopic)
-
     streams.close()
 
     import collection.JavaConverters._
@@ -172,74 +151,4 @@ class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends JUnitSuite wit
     streams.close()
     assertEquals(actualClicksPerRegion.asScala.sortBy(_.key), expectedClicksPerRegion.sortBy(_.key))
   }
&lt;p&gt;-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def getStreamsConfiguration(): Properties = 
{
-    val streamsConfiguration: Properties = new Properties()
-
-    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;stream-table-join-scala-integration-test&quot;)
-    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
-    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, &quot;1000&quot;)
-    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
-    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, testFolder.getRoot.getPath)
-
-    streamsConfiguration
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def getUserRegionsProducerConfig(): Properties = 
{
-    val p = new Properties()
-    p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
-    p.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;)
-    p.put(ProducerConfig.RETRIES_CONFIG, &quot;0&quot;)
-    p.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])
-    p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])
-    p
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def getUserClicksProducerConfig(): Properties = 
{
-    val p = new Properties()
-    p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
-    p.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;)
-    p.put(ProducerConfig.RETRIES_CONFIG, &quot;0&quot;)
-    p.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])
-    p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[LongSerializer])
-    p
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def getConsumerConfig(): Properties = 
{
-    val p = new Properties()
-    p.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers())
-    p.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;join-scala-integration-test-standard-consumer&quot;)
-    p.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
-    p.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer])
-    p.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[LongDeserializer])
-    p
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def produceNConsume(userClicksTopic: String,&lt;/li&gt;
	&lt;li&gt;userRegionsTopic: String,&lt;/li&gt;
	&lt;li&gt;outputTopic: String): java.util.List[KeyValue&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt;] = 
{
-
-    import collection.JavaConverters._
-
-    // Publish user-region information.
-    val userRegionsProducerConfig: Properties = getUserRegionsProducerConfig()
-    IntegrationTestUtils.produceKeyValuesSynchronously(userRegionsTopic,
-                                                       userRegions.asJava,
-                                                       userRegionsProducerConfig,
-                                                       mockTime,
-                                                       false)
-
-    // Publish user-click information.
-    val userClicksProducerConfig: Properties = getUserClicksProducerConfig()
-    IntegrationTestUtils.produceKeyValuesSynchronously(userClicksTopic,
-                                                       userClicks.asJava,
-                                                       userClicksProducerConfig,
-                                                       mockTime,
-                                                       false)
-
-    // consume and verify result
-    val consumerConfig = getConsumerConfig()
-
-    IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig, outputTopic, expectedClicksPerRegion.size)
-  }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..f5a098b3870&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala&lt;br/&gt;
@@ -0,0 +1,88 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Copyright (C) 2018 Lightbend Inc. &amp;lt;&lt;a href=&quot;https://www.lightbend.com&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.lightbend.com&lt;/a&gt;&amp;gt;&lt;br/&gt;
+ * Copyright (C) 2017-2018 Alexis Seigneurin.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);&lt;br/&gt;
+ * you may not use this file except in compliance with the License.&lt;br/&gt;
+ * You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.scala&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.clients.consumer.ConsumerConfig&lt;br/&gt;
+import org.apache.kafka.clients.producer.ProducerConfig&lt;br/&gt;
+import org.apache.kafka.common.serialization._&lt;br/&gt;
+import org.apache.kafka.common.utils.MockTime&lt;br/&gt;
+import org.apache.kafka.streams._&lt;br/&gt;
+import org.apache.kafka.streams.integration.utils.
{EmbeddedKafkaCluster, IntegrationTestUtils}
&lt;p&gt;+import org.apache.kafka.streams.processor.internals.StreamThread&lt;br/&gt;
+import org.apache.kafka.streams.scala.ImplicitConversions._&lt;br/&gt;
+import org.apache.kafka.streams.scala.kstream._&lt;br/&gt;
+import org.apache.kafka.test.TestUtils&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
+import org.junit._&lt;br/&gt;
+import org.junit.rules.TemporaryFolder&lt;br/&gt;
+import org.scalatest.junit.JUnitSuite&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Test suite that verifies the shutdown of StreamThread when metadata is incomplete during stream-table joins in Kafka Streams&lt;br/&gt;
+ * &amp;lt;p&amp;gt;&lt;br/&gt;
+ */&lt;br/&gt;
+class StreamToTableJoinWithIncompleteMetadataIntegrationTest extends StreamToTableJoinScalaIntegrationTestBase {&lt;br/&gt;
+&lt;br/&gt;
+  @Test def testShouldAutoShutdownOnIncompleteMetadata(): Unit = &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {++    // DefaultSerdes brings into scope implicit serdes (mostly for primitives) that will set up all Serialized, Produced,+    // Consumed and Joined instances. So all APIs below that accept Serialized, Produced, Consumed or Joined will+    // get these instances automatically+    import Serdes._++    val streamsConfiguration}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+}&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
index 41134672e98..4d7215f6823 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -510,22 +510,22 @@ def do_rolling_bounce(self, processor, counter, current_generation):&lt;br/&gt;
                     monitors&lt;span class=&quot;error&quot;&gt;&amp;#91;first_other_processor&amp;#93;&lt;/span&gt; = first_other_monitor&lt;br/&gt;
                     monitors&lt;span class=&quot;error&quot;&gt;&amp;#91;second_other_processor&amp;#93;&lt;/span&gt; = second_other_monitor&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leader_monitor.wait_until(&quot;Received a future (version probing) subscription (version: 4). Sending empty assignment back (with supported version 3).&quot;,&lt;br/&gt;
+                    leader_monitor.wait_until(&quot;Received a future (version probing) subscription (version: 5). Sending empty assignment back (with supported version 4).&quot;,&lt;br/&gt;
                                               timeout_sec=60,&lt;br/&gt;
                                               err_msg=&quot;Could not detect &apos;version probing&apos; attempt at leader &quot; + str(self.leader.node.account))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     if len(self.old_processors) &amp;gt; 0:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log_monitor.wait_until(&quot;Sent a version 4 subscription and got version 3 assignment back (successful version probing). Downgrading subscription metadata to received version and trigger new rebalance.&quot;,&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Sent a version 5 subscription and got version 4 assignment back (successful version probing). Downgrading subscription metadata to received version and trigger new rebalance.&quot;,&lt;br/&gt;
                                                timeout_sec=60,&lt;br/&gt;
                                                err_msg=&quot;Could not detect &apos;successful version probing&apos; at upgrading node &quot; + str(node.account))&lt;br/&gt;
                     else:&lt;/li&gt;
	&lt;li&gt;log_monitor.wait_until(&quot;Sent a version 4 subscription and got version 3 assignment back (successful version probing). Setting subscription metadata to leaders supported version 4 and trigger new rebalance.&quot;,&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Sent a version 5 subscription and got version 4 assignment back (successful version probing). Setting subscription metadata to leaders supported version 5 and trigger new rebalance.&quot;,&lt;br/&gt;
                                                timeout_sec=60,&lt;br/&gt;
                                                err_msg=&quot;Could not detect &apos;successful version probing with upgraded leader&apos; at upgrading node &quot; + str(node.account))&lt;/li&gt;
	&lt;li&gt;first_other_monitor.wait_until(&quot;Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance.&quot;,&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;Sent a version 4 subscription and group leader.s latest supported version is 5. Upgrading subscription metadata version to 5 for next rebalance.&quot;,&lt;br/&gt;
                                                        timeout_sec=60,&lt;br/&gt;
                                                        err_msg=&quot;Never saw output &apos;Upgrade metadata to version 4&apos; on&quot; + str(first_other_node.account))&lt;/li&gt;
	&lt;li&gt;second_other_monitor.wait_until(&quot;Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance.&quot;,&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;Sent a version 4 subscription and group leader.s latest supported version is 5. Upgrading subscription metadata version to 5 for next rebalance.&quot;,&lt;br/&gt;
                                                         timeout_sec=60,&lt;br/&gt;
                                                         err_msg=&quot;Never saw output &apos;Upgrade metadata to version 4&apos; on&quot; + str(second_other_node.account))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -553,6 +553,6 @@ def do_rolling_bounce(self, processor, counter, current_generation):&lt;/p&gt;

&lt;p&gt;     def verify_metadata_no_upgraded_yet(self):&lt;br/&gt;
         for p in self.processors:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;found = list(p.node.account.ssh_capture(&quot;grep \&quot;Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance.\&quot; &quot; + p.LOG_FILE, allow_fail=True))&lt;br/&gt;
+            found = list(p.node.account.ssh_capture(&quot;grep \&quot;Sent a version 4 subscription and group leader.s latest supported version is 5. Upgrading subscription metadata version to 5 for next rebalance.\&quot; &quot; + p.LOG_FILE, allow_fail=True))&lt;br/&gt;
             if len(found) &amp;gt; 0:&lt;br/&gt;
                 raise Exception(&quot;Kafka Streams failed with &apos;group member upgraded to metadata 4 too early&apos;&quot;)&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16549978" author="githubbot" created="Thu, 19 Jul 2018 22:57:28 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5399: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5037&quot; title=&quot;Infinite loop if all input topics are unknown at startup&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5037&quot;&gt;&lt;del&gt;KAFKA-5037&lt;/del&gt;&lt;/a&gt; Follow-up: move Scala test to Java&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5399&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5399&lt;/a&gt;&lt;/p&gt;




&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16551030" author="githubbot" created="Fri, 20 Jul 2018 17:37:59 +0000"  >&lt;p&gt;guozhangwang closed pull request #5399: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5037&quot; title=&quot;Infinite loop if all input topics are unknown at startup&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5037&quot;&gt;&lt;del&gt;KAFKA-5037&lt;/del&gt;&lt;/a&gt; Follow-up: move Scala test to Java&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5399&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5399&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java&lt;br/&gt;
index 5fc768dc82b..61bbb8b5dac 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java&lt;br/&gt;
@@ -16,11 +16,14 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.integration;&lt;/p&gt;

&lt;p&gt;+import org.apache.kafka.streams.KafkaStreamsWrapper;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KTable;&lt;br/&gt;
 import org.apache.kafka.test.IntegrationTest;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.Before;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 import org.junit.experimental.categories.Category;&lt;br/&gt;
@@ -31,6 +34,8 @@&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
 import java.util.List;&lt;/p&gt;

&lt;p&gt;+import static org.junit.Assert.assertEquals;&lt;br/&gt;
+&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Tests all available joins of Kafka Streams DSL.&lt;br/&gt;
@@ -56,6 +61,31 @@ public void prepareTopology() throws InterruptedException 
{
         leftStream = builder.stream(INPUT_TOPIC_LEFT);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testShouldAutoShutdownOnIncompleteMetadata() throws InterruptedException &lt;/p&gt;
{
+        STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + &quot;-incomplete&quot;);
+
+        final KStream&amp;lt;Long, String&amp;gt; notExistStream = builder.stream(INPUT_TOPIC_LEFT + &quot;-not-existed&quot;);
+
+        final KTable&amp;lt;Long, String&amp;gt; aggregatedTable = notExistStream.leftJoin(rightTable, valueJoiner)
+                .groupBy((key, value) -&amp;gt; key)
+                .reduce((value1, value2) -&amp;gt; value1 + value2);
+
+        // Write the (continuously updating) results to the output topic.
+        aggregatedTable.toStream().to(OUTPUT_TOPIC);
+
+        final KafkaStreamsWrapper streams = new KafkaStreamsWrapper(builder.build(), STREAMS_CONFIG);
+        final IntegrationTestUtils.StateListenerStub listener = new IntegrationTestUtils.StateListenerStub();
+        streams.setStreamThreadStateListener(listener);
+        streams.start();
+
+        TestUtils.waitForCondition(listener::revokedToPendingShutdownSeen, &quot;Did not seen thread state transited to PENDING_SHUTDOWN&quot;);
+
+        streams.close();
+        assertEquals(listener.runningToRevokedSeen(), true);
+        assertEquals(listener.revokedToPendingShutdownSeen(), true);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testInner() throws Exception {&lt;br/&gt;
         STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + &quot;-inner&quot;);&lt;br/&gt;
diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 3bf597738a9..00000000000&lt;br/&gt;
&amp;#8212; a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,88 +0,0 @@&lt;br/&gt;
-/*&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Copyright (C) 2018 Lightbend Inc. &amp;lt;&lt;a href=&quot;https://www.lightbend.com&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.lightbend.com&lt;/a&gt;&amp;gt;&lt;/li&gt;
	&lt;li&gt;* Copyright (C) 2017-2018 Alexis Seigneurin.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);&lt;/li&gt;
	&lt;li&gt;* you may not use this file except in compliance with the License.&lt;/li&gt;
	&lt;li&gt;* You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-package org.apache.kafka.streams.scala&lt;br/&gt;
-&lt;br/&gt;
-import java.util.Properties&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.kafka.clients.consumer.ConsumerConfig&lt;br/&gt;
-import org.apache.kafka.clients.producer.ProducerConfig&lt;br/&gt;
-import org.apache.kafka.common.serialization._&lt;br/&gt;
-import org.apache.kafka.common.utils.MockTime&lt;br/&gt;
-import org.apache.kafka.streams._&lt;br/&gt;
-import org.apache.kafka.streams.integration.utils.
{EmbeddedKafkaCluster, IntegrationTestUtils}
&lt;p&gt;-import org.apache.kafka.streams.processor.internals.StreamThread&lt;br/&gt;
-import org.apache.kafka.streams.scala.ImplicitConversions._&lt;br/&gt;
-import org.apache.kafka.streams.scala.kstream._&lt;br/&gt;
-import org.apache.kafka.test.TestUtils&lt;br/&gt;
-import org.junit.Assert._&lt;br/&gt;
-import org.junit._&lt;br/&gt;
-import org.junit.rules.TemporaryFolder&lt;br/&gt;
-import org.scalatest.junit.JUnitSuite&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* Test suite that verifies the shutdown of StreamThread when metadata is incomplete during stream-table joins in Kafka Streams&lt;/li&gt;
	&lt;li&gt;* &amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-class StreamToTableJoinWithIncompleteMetadataIntegrationTest extends StreamToTableJoinScalaIntegrationTestBase {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@Test def testShouldAutoShutdownOnIncompleteMetadata(): Unit = {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// DefaultSerdes brings into scope implicit serdes (mostly for primitives) that will set up all Serialized, Produced,&lt;/li&gt;
	&lt;li&gt;// Consumed and Joined instances. So all APIs below that accept Serialized, Produced, Consumed or Joined will&lt;/li&gt;
	&lt;li&gt;// get these instances automatically&lt;/li&gt;
	&lt;li&gt;import Serdes._&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val streamsConfiguration: Properties = getStreamsConfiguration()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val builder = new StreamsBuilder()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val userClicksStream: KStream&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt; = builder.stream(userClicksTopic)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val userRegionsTable: KTable&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt; = builder.table(userRegionsTopic + &quot;1&quot;)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// Compute the total per region by summing the individual click counts per region.&lt;/li&gt;
	&lt;li&gt;val clicksPerRegion: KTable&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt; =&lt;/li&gt;
	&lt;li&gt;userClicksStream&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// Join the stream against the table.&lt;/li&gt;
	&lt;li&gt;.leftJoin(userRegionsTable)((clicks, region) =&amp;gt; (if (region == null) &quot;UNKNOWN&quot; else region, clicks))&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// Change the stream from &amp;lt;user&amp;gt; -&amp;gt; &amp;lt;region, clicks&amp;gt; to &amp;lt;region&amp;gt; -&amp;gt; &amp;lt;clicks&amp;gt;&lt;/li&gt;
	&lt;li&gt;.map((_, regionWithClicks) =&amp;gt; regionWithClicks)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// Compute the total per region by summing the individual click counts per region.&lt;/li&gt;
	&lt;li&gt;.groupByKey&lt;/li&gt;
	&lt;li&gt;.reduce(_ + _)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// Write the (continuously updating) results to the output topic.&lt;/li&gt;
	&lt;li&gt;clicksPerRegion.toStream.to(outputTopic)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val streams: KafkaStreamsWrapper = new KafkaStreamsWrapper(builder.build(), streamsConfiguration)&lt;/li&gt;
	&lt;li&gt;val listener = new IntegrationTestUtils.StateListenerStub()&lt;/li&gt;
	&lt;li&gt;streams.setStreamThreadStateListener(listener)&lt;/li&gt;
	&lt;li&gt;streams.start()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val actualClicksPerRegion: java.util.List[KeyValue&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt;] =&lt;/li&gt;
	&lt;li&gt;produceNConsume(userClicksTopic, userRegionsTopic, outputTopic, false)&lt;/li&gt;
	&lt;li&gt;while (!listener.revokedToPendingShutdownSeen()) 
{
-      Thread.sleep(3)
-    }&lt;/li&gt;
	&lt;li&gt;streams.close()&lt;/li&gt;
	&lt;li&gt;assertEquals(listener.runningToRevokedSeen(), true)&lt;/li&gt;
	&lt;li&gt;assertEquals(listener.revokedToPendingShutdownSeen(), true)&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-}&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16551071" author="githubbot" created="Fri, 20 Jul 2018 18:04:06 +0000"  >&lt;p&gt;mjsax closed pull request #2815: WIP DO NOT MERGE &amp;#8211; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5037&quot; title=&quot;Infinite loop if all input topics are unknown at startup&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5037&quot;&gt;&lt;del&gt;KAFKA-5037&lt;/del&gt;&lt;/a&gt;: Infinite loop if all input topics are unknown at startup&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/2815&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/2815&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
index 004926fd8c9..53007e378d3 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
@@ -333,11 +333,12 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                                     } else {&lt;br/&gt;
                                         numPartitionsCandidate = metadata.partitionCountForTopic(sourceTopicName);&lt;br/&gt;
                                         if (numPartitionsCandidate == null) &lt;/p&gt;
{
-                                            repartitionTopicMetadata.get(topicName).numPartitions = NOT_AVAILABLE;
+                                            numPartitionsCandidate = NOT_AVAILABLE;
                                         }
&lt;p&gt;                                     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numPartitionsCandidate != null &amp;amp;&amp;amp; numPartitionsCandidate &amp;gt; numPartitions) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                                    // assign NOT_AVAILABLE to avoid infinite loop and to propagate this information downstream+                                    if (numPartitionsCandidate == NOT_AVAILABLE || numPartitionsCandidate &amp;gt; numPartitions) {
                                         numPartitions = numPartitionsCandidate;
                                     }                                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -345,15 +346,21 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                         }&lt;br/&gt;
                         // if we still have not find the right number of partitions,&lt;br/&gt;
                         // another iteration is needed&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (numPartitions == UNKNOWN)&lt;br/&gt;
+                        if (numPartitions == UNKNOWN) 
{
                             numPartitionsNeeded = true;
-                        else
+                        }
&lt;p&gt; else &lt;/p&gt;
{
                             repartitionTopicMetadata.get(topicName).numPartitions = numPartitions;
+                        }
&lt;p&gt;                     }&lt;br/&gt;
                 }&lt;br/&gt;
             }&lt;br/&gt;
         } while (numPartitionsNeeded);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        // ensure the co-partitioning of topics within the group so that they have the same number of partitions,&lt;br/&gt;
+        // and enforce the number of partitions for those repartition topics to be the same if they&lt;br/&gt;
+        // are co-partitioned as well.&lt;br/&gt;
+        ensureCopartitioning(streamThread.builder.copartitionGroups(), repartitionTopicMetadata, metadata);&lt;br/&gt;
+&lt;br/&gt;
         // augment the metadata with the newly computed number of partitions for all the&lt;br/&gt;
         // repartition source topics&lt;br/&gt;
         Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; allRepartitionTopicPartitions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -367,11 +374,6 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// ensure the co-partitioning topics within the group have the same number of partitions,&lt;/li&gt;
	&lt;li&gt;// and enforce the number of partitions for those repartition topics to be the same if they&lt;/li&gt;
	&lt;li&gt;// are co-partitioned as well.&lt;/li&gt;
	&lt;li&gt;ensureCopartitioning(streamThread.builder.copartitionGroups(), repartitionTopicMetadata, metadata);&lt;br/&gt;
-&lt;br/&gt;
         // make sure the repartition source topics exist with the right number of partitions,&lt;br/&gt;
         // create these topics if necessary&lt;br/&gt;
         prepareTopic(repartitionTopicMetadata);&lt;br/&gt;
@@ -737,7 +739,7 @@ void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,&lt;br/&gt;
                     final Integer partitions = metadata.partitionCountForTopic(topic);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     if (partitions == null) &lt;/p&gt;
{
-                        throw new TopologyBuilderException(String.format(&quot;stream-thread [%s] Topic not found: %s&quot;, threadName, topic));
+                        continue;
                     }

&lt;p&gt;                     if (numPartitions == UNKNOWN) {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java&lt;br/&gt;
index 77001ce72f1..587ecc6006b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsValidatorTest.java&lt;br/&gt;
@@ -46,13 +46,6 @@ public void before() &lt;/p&gt;
{
         partitions.put(new TopicPartition(&quot;second&quot;, 1), new PartitionInfo(&quot;second&quot;, 1, null, null, null));
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = TopologyBuilderException.class)&lt;/li&gt;
	&lt;li&gt;public void shouldThrowTopologyBuilderExceptionIfNoPartitionsFoundForCoPartitionedTopic() throws Exception 
{
-        validator.validate(Collections.singleton(&quot;topic&quot;),
-                           Collections.&amp;lt;String, StreamPartitionAssignor.InternalTopicMetadata&amp;gt;emptyMap(),
-                           cluster);
-    }
&lt;p&gt;-&lt;br/&gt;
     @Test(expected = TopologyBuilderException.class)&lt;br/&gt;
     public void shouldThrowTopologyBuilderExceptionIfPartitionCountsForCoPartitionedTopicsDontMatch() throws Exception &lt;/p&gt;
{
         partitions.remove(new TopicPartition(&quot;second&quot;, 0));
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java
index 4cc7b92cdf9..29d0cba3d8f 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java
@@ -798,7 +798,31 @@ public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() throws Exceptio
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() {&lt;br/&gt;
+    public void shouldCreateAllTasksAndInternalTopicIfAllInputTopicsAreKnown() 
{
+        assertThatPartitionsAreAssignedAndInternalTopicsAreCreated(&quot;topic1&quot;, true, &quot;topic3&quot;, true);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotLoopInfinitelyOnPartlyMissingMetadataAndShouldNotCreateRelatedTasks1() &lt;/p&gt;
{
+        assertThatPartitionsAreAssignedAndInternalTopicsAreCreated(&quot;topic1&quot;, true, &quot;unknownTopic&quot;, false);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotLoopInfinitelyOnPartlyMissingMetadataAndShouldNotCreateRelatedTasks2() &lt;/p&gt;
{
+        assertThatPartitionsAreAssignedAndInternalTopicsAreCreated(&quot;unknownTopic&quot;, false, &quot;topic3&quot;, true);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotLoopInfinitelyOnMissingMetadataForAllInputTopicsAndShouldNotCreateRelatedTasks() &lt;/p&gt;
{
+        assertThatPartitionsAreAssignedAndInternalTopicsAreCreated(&quot;unknownTopic1&quot;, false, &quot;unknownTopic2&quot;, false);
+    }
&lt;p&gt;+&lt;br/&gt;
+    private void assertThatPartitionsAreAssignedAndInternalTopicsAreCreated(&lt;br/&gt;
+        final String sourceTopic1,&lt;br/&gt;
+        final boolean src1Known,&lt;br/&gt;
+        final String sourceTopic2,&lt;br/&gt;
+        final boolean src2Known) {&lt;br/&gt;
+&lt;br/&gt;
         final String applicationId = &quot;application-id&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
@@ -806,9 +830,11 @@ public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTas&lt;/p&gt;

&lt;p&gt;         KStream&amp;lt;Object, Object&amp;gt; stream1 = builder&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Task 1 (should get created):&lt;/li&gt;
	&lt;li&gt;.stream(&quot;topic1&quot;)&lt;br/&gt;
+            // Task 1 (should get created if `sourceTopic1` is known)&lt;br/&gt;
+            .stream(sourceTopic1)&lt;br/&gt;
+&lt;br/&gt;
             // force repartitioning for aggregation&lt;br/&gt;
+            // -&amp;gt; should create internal repartitioning topic only if `sourceTopic1` is known&lt;br/&gt;
             .selectKey(new KeyValueMapper&amp;lt;Object, Object, Object&amp;gt;() {&lt;br/&gt;
                 @Override&lt;br/&gt;
                 public Object apply(Object key, Object value) {&lt;br/&gt;
@@ -817,12 +843,13 @@ public Object apply(Object key, Object value) {&lt;br/&gt;
             })&lt;br/&gt;
             .groupByKey()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Task 2 (should get created):&lt;/li&gt;
	&lt;li&gt;// create repartioning and changelog topic as task 1 exists&lt;br/&gt;
+            // Task 2 (should only get created if `sourceTopic1` is known):&lt;br/&gt;
+            // -&amp;gt; create repartioning and changelog topic if task2 is created&lt;br/&gt;
             .count(&quot;count&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// force repartitioning for join, but second join input topic unknown&lt;/li&gt;
	&lt;li&gt;// -&amp;gt; internal repartitioning topic should not get created&lt;br/&gt;
+            // force repartitioning for join&lt;br/&gt;
+            // -&amp;gt; internal repartitioning topic should only be create if `sourceTopic1` and sourceTopic2` are both known&lt;br/&gt;
+            // (cf. Task4 below)&lt;br/&gt;
             .toStream()&lt;br/&gt;
             .map(new KeyValueMapper&amp;lt;Object, Long, KeyValue&amp;lt;Object, Object&amp;gt;&amp;gt;() {&lt;br/&gt;
                 @Override&lt;br/&gt;
@@ -832,11 +859,11 @@ public Object apply(Object key, Object value) {&lt;br/&gt;
             });&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         builder&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Task 3 (should not get created because input topic unknown)&lt;/li&gt;
	&lt;li&gt;.stream(&quot;unknownTopic&quot;)&lt;br/&gt;
+            // Task 3 (should get created if `sourceTopic2` is known)&lt;br/&gt;
+            .stream(sourceTopic2)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// force repartitioning for join, but input topic unknown&lt;/li&gt;
	&lt;li&gt;// -&amp;gt; thus should not create internal repartitioning topic&lt;br/&gt;
+            // force repartitioning for join&lt;br/&gt;
+            // -&amp;gt; should create internal repartitioning topic only if `sourceTopic2` is known&lt;br/&gt;
             .selectKey(new KeyValueMapper&amp;lt;Object, Object, Object&amp;gt;() {&lt;br/&gt;
                 @Override&lt;br/&gt;
                 public Object apply(Object key, Object value) {&lt;br/&gt;
@@ -844,8 +871,8 @@ public Object apply(Object key, Object value) {&lt;br/&gt;
                 }&lt;br/&gt;
             })&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Task 4 (should not get created because input topics unknown)&lt;/li&gt;
	&lt;li&gt;// should not create any of both input repartition topics or any of both changelog topics&lt;br/&gt;
+            // Task 4 (should only get created if `sourceTopic1` and `sourceTopic2` are both known)&lt;br/&gt;
+            // -&amp;gt; should create of both input repartition topics if task4 is created&lt;br/&gt;
             .join(&lt;br/&gt;
                 stream1,&lt;br/&gt;
                 new ValueJoiner() {&lt;br/&gt;
@@ -877,20 +904,51 @@ public Object apply(Object value1, Object value2) {&lt;br/&gt;
         );&lt;br/&gt;
         final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        final HashSet&amp;lt;TopicPartition&amp;gt; expectedAssignment = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         final Map&amp;lt;String, Integer&amp;gt; expectedCreatedInternalTopics = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expectedCreatedInternalTopics.put(applicationId + &quot;-count-repartition&quot;, 3);&lt;/li&gt;
	&lt;li&gt;expectedCreatedInternalTopics.put(applicationId + &quot;-count-changelog&quot;, 3);&lt;/li&gt;
	&lt;li&gt;assertThat(mockInternalTopicManager.readyTopics, equalTo(expectedCreatedInternalTopics));&lt;br/&gt;
+        if (src1Known) 
{
+            expectedAssignment.add(new TopicPartition(sourceTopic1, 0));
+            expectedAssignment.add(new TopicPartition(sourceTopic1, 1));
+            expectedAssignment.add(new TopicPartition(sourceTopic1, 2));
+
+            // force repartitioning for aggregation (selectKey() on `sourceTopic1`)
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-count-repartition&quot;, 0));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-count-repartition&quot;, 1));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-count-repartition&quot;, 2));
+            expectedCreatedInternalTopics.put(applicationId + &quot;-count-repartition&quot;, 3);
+
+            // changelog topic for groupBy-count on `sourceTopic1`
+            expectedCreatedInternalTopics.put(applicationId + &quot;-count-changelog&quot;, 3);
+        }
&lt;p&gt;+        if (src2Known) &lt;/p&gt;
{
+            expectedAssignment.add(new TopicPartition(sourceTopic2, 0));
+            expectedAssignment.add(new TopicPartition(sourceTopic2, 1));
+            expectedAssignment.add(new TopicPartition(sourceTopic2, 2));
+            expectedAssignment.add(new TopicPartition(sourceTopic2, 3));
+        }
&lt;p&gt;+        if (src1Known &amp;amp;&amp;amp; src2Known) &lt;/p&gt;
{
+            // force repartitioning for join (map() after groupBY-count on `sourceTopic1`)
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-MAP-0000000007-repartition&quot;, 0));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-MAP-0000000007-repartition&quot;, 1));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-MAP-0000000007-repartition&quot;, 2));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-MAP-0000000007-repartition&quot;, 3));
+            expectedCreatedInternalTopics.put(applicationId + &quot;-KSTREAM-MAP-0000000007-repartition&quot;, 4);
+
+            // force repartitioning for join (selectKey() on `sourceTopic2`)
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-KEY-SELECT-0000000009-repartition&quot;, 0));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-KEY-SELECT-0000000009-repartition&quot;, 1));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-KEY-SELECT-0000000009-repartition&quot;, 2));
+            expectedAssignment.add(new TopicPartition(applicationId + &quot;-KSTREAM-KEY-SELECT-0000000009-repartition&quot;, 3));
+            expectedCreatedInternalTopics.put(applicationId + &quot;-KSTREAM-KEY-SELECT-0000000009-repartition&quot;, 4);
+
+            // both join changelog topics
+            expectedCreatedInternalTopics.put(applicationId + &quot;-KSTREAM-JOINTHIS-0000000018-store-changelog&quot;, 4);
+            expectedCreatedInternalTopics.put(applicationId + &quot;-KSTREAM-JOINOTHER-0000000019-store-changelog&quot;, 4);
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final List&amp;lt;TopicPartition&amp;gt; expectedAssignment = Arrays.asList(&lt;/li&gt;
	&lt;li&gt;new TopicPartition(&quot;topic1&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new TopicPartition(&quot;topic1&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new TopicPartition(&quot;topic1&quot;, 2),&lt;/li&gt;
	&lt;li&gt;new TopicPartition(applicationId + &quot;-count-repartition&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new TopicPartition(applicationId + &quot;-count-repartition&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new TopicPartition(applicationId + &quot;-count-repartition&quot;, 2)&lt;/li&gt;
	&lt;li&gt;);&lt;/li&gt;
	&lt;li&gt;assertThat(new HashSet(assignment.get(client).partitions()), equalTo(new HashSet(expectedAssignment)));&lt;br/&gt;
+        assertThat(new HashSet&amp;lt;&amp;gt;(assignment.get(client).partitions()),&lt;br/&gt;
+                   equalTo(expectedAssignment));&lt;br/&gt;
+        assertThat(mockInternalTopicManager.readyTopics, equalTo(expectedCreatedInternalTopics));&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13140592">KAFKA-6587</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13129880">KAFKA-6437</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12930051" name="5037.v2.txt" size="10643" author="yuzhihong@gmail.com" created="Tue, 3 Jul 2018 02:22:31 +0000"/>
                            <attachment id="12930065" name="5037.v4.txt" size="13262" author="yuzhihong@gmail.com" created="Tue, 3 Jul 2018 06:25:05 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 17 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3dbyn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>