<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:21:44 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-8764] LogCleanerManager endless loop while compacting/cleaning segments</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-8764</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;&lt;tt&gt;LogCleanerManager stuck in endless loop while clearing segments for one partition resulting with many log outputs and heavy disk read/writes/IOPS.&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Issue appeared on follower brokers, and it happens on every (new) broker if partition assignment is changed.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Original issue setup:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;kafka_2.12-2.2.1 deployed as statefulset on kubernetes, 5 brokers&lt;/li&gt;
	&lt;li&gt;log directory is (AWS) EBS mounted PV, gp2 (ssd) kind of 750GiB&lt;/li&gt;
	&lt;li&gt;5 zookeepers&lt;/li&gt;
	&lt;li&gt;topic created with config:
	&lt;ul&gt;
		&lt;li&gt;name = &quot;backup_br_domain_squad&quot;&lt;br/&gt;
partitions = 36&lt;br/&gt;
replication_factor = 3&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;config = &lt;/p&gt;
{
 &quot;cleanup.policy&quot; = &quot;compact&quot;
 &quot;min.compaction.lag.ms&quot; = &quot;86400000&quot;
 &quot;min.cleanable.dirty.ratio&quot; = &quot;0.3&quot;
}

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Log excerpt:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,895&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Log partition=backup_br_domain_squad-14, dir=/var/lib/kafka/data/topics&amp;#93;&lt;/span&gt; Deleting segment 0 (kafka.log.Log)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,895&amp;#93;&lt;/span&gt; INFO Deleted log /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.log.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,896&amp;#93;&lt;/span&gt; INFO Deleted offset index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.index.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,896&amp;#93;&lt;/span&gt; INFO Deleted time index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,964&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Log partition=backup_br_domain_squad-14, dir=/var/lib/kafka/data/topics&amp;#93;&lt;/span&gt; Deleting segment 0 (kafka.log.Log)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,964&amp;#93;&lt;/span&gt; INFO Deleted log /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.log.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,964&amp;#93;&lt;/span&gt; INFO Deleted offset index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.index.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:53,964&amp;#93;&lt;/span&gt; INFO Deleted time index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,031&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Log partition=backup_br_domain_squad-14, dir=/var/lib/kafka/data/topics&amp;#93;&lt;/span&gt; Deleting segment 0 (kafka.log.Log)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,032&amp;#93;&lt;/span&gt; INFO Deleted log /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.log.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,032&amp;#93;&lt;/span&gt; INFO Deleted offset index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.index.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,032&amp;#93;&lt;/span&gt; INFO Deleted time index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,101&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Log partition=backup_br_domain_squad-14, dir=/var/lib/kafka/data/topics&amp;#93;&lt;/span&gt; Deleting segment 0 (kafka.log.Log)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,101&amp;#93;&lt;/span&gt; INFO Deleted log /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.log.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,101&amp;#93;&lt;/span&gt; INFO Deleted offset index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.index.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,101&amp;#93;&lt;/span&gt; INFO Deleted time index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,173&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Log partition=backup_br_domain_squad-14, dir=/var/lib/kafka/data/topics&amp;#93;&lt;/span&gt; Deleting segment 0 (kafka.log.Log)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,173&amp;#93;&lt;/span&gt; INFO Deleted log /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.log.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,173&amp;#93;&lt;/span&gt; INFO Deleted offset index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.index.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 12:10:54,173&amp;#93;&lt;/span&gt; INFO Deleted time index /var/lib/kafka/data/topics/backup_br_domain_squad-14/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;And such log keeps repeating forever.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I&apos;ve been able to extract segment files from (running) leader broker, and replicated same behaviour locally.&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;%C2%A0&quot;&gt;&lt;/a&gt;&#160;&lt;/h1&gt;
&lt;h1&gt;&lt;a name=&quot;Reproductionsetup%3A&quot;&gt;&lt;/a&gt;&lt;b&gt;Reproduction setup:&lt;/b&gt;&lt;/h1&gt;
&lt;ul&gt;
	&lt;li&gt;start single broker kafka_2.12-2.2.1&lt;/li&gt;
	&lt;li&gt;create topic
	&lt;ul&gt;
		&lt;li&gt;&lt;tt&gt;./bin/kafka-topics.sh --bootstrap-server &lt;b&gt;_&lt;em&gt;BOOTSTRAP_SERVER&lt;/em&gt;_&lt;/b&gt;:9092 --create --topic backup_br_domain_squad --partitions 1 --replication-factor 1 --config cleanup.policy=compact --config min.compaction.lag.ms=86400000 --config min.cleanable.dirty.ratio=0.3&lt;/tt&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;stop broker&lt;/li&gt;
	&lt;li&gt;copy segment files (attachment content, under segments) to backup_br_domain_squad-0 log folder&lt;/li&gt;
	&lt;li&gt;rerun broker again&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;After rerun, fun starts with endless repeating logging outputs:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,610&amp;#93;&lt;/span&gt; DEBUG Checking if log segment may be cleaned: log=&apos;backup_br_domain_squad-0&apos; segment.baseOffset=0 segment.largestTimestamp=1563203503987; now - compactionLag=1565096737609; is uncleanable=false (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,610&amp;#93;&lt;/span&gt; DEBUG Finding range of cleanable offsets for log=backup_br_domain_squad-0 topicPartition=backup_br_domain_squad-0. Last clean offset=Some(0) now=1565183137609 =&amp;gt; firstDirtyOffset=0 firstUncleanableOffset=233 activeSegment.baseOffset=233 (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,613&amp;#93;&lt;/span&gt; INFO Cleaner 0: Beginning cleaning of log backup_br_domain_squad-0. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,613&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for backup_br_domain_squad-0... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,628&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log backup_br_domain_squad-0 for 1 segments in offset range [0, 233). (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,643&amp;#93;&lt;/span&gt; INFO Cleaner 0: Offset map for log backup_br_domain_squad-0 complete. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,644&amp;#93;&lt;/span&gt; INFO Cleaner 0: Cleaning log backup_br_domain_squad-0 (cleaning prior to Wed Aug 07 13:05:22 GMT 2019, discarding tombstones prior to Thu Jan 01 00:00:00 GMT 1970)... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,645&amp;#93;&lt;/span&gt; DEBUG Loaded index file /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.index.cleaned with maxEntries = 1310720, maxIndexSize = 10485760, entries = 0, lastOffset = 0, file position = 0 (kafka.log.OffsetIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,646&amp;#93;&lt;/span&gt; DEBUG Loaded index file /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.timeindex.cleaned with maxEntries = 873813, maxIndexSize = 10485760, entries = 0, lastOffset = TimestampOffset(-1,0), file position = 0 (kafka.log.TimeIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,647&amp;#93;&lt;/span&gt; INFO Cleaner 0: Cleaning segment 0 in log backup_br_domain_squad-0 (largest timestamp Mon Jul 15 15:11:43 GMT 2019) into 0, retaining deletes. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,656&amp;#93;&lt;/span&gt; DEBUG Resized /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.index.cleaned to 32, position is 32 and limit is 32 (kafka.log.OffsetIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,657&amp;#93;&lt;/span&gt; DEBUG Resized /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.timeindex.cleaned to 36, position is 36 and limit is 36 (kafka.log.TimeIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,669&amp;#93;&lt;/span&gt; INFO Cleaner 0: Swapping in cleaned segment LogSegment(baseOffset=0, size=1044021) for segment(s) List(LogSegment(baseOffset=0, size=1044021)) in log Log(/runtime/logs/kafka_1/backup_br_domain_squad-0) (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,672&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-log-cleaner-thread-0&amp;#93;&lt;/span&gt;:&lt;/tt&gt;&lt;br/&gt;
{{ Log cleaner thread 0 cleaned log backup_br_domain_squad-0 (dirty section = &lt;span class=&quot;error&quot;&gt;&amp;#91;0, 0&amp;#93;&lt;/span&gt;)}}&lt;br/&gt;
{{ 1.0 MB of log processed in 0.1 seconds (17.2 MB/sec).}}&lt;br/&gt;
{{ Indexed 1.0 MB in 0.0 seconds (33.2 Mb/sec, 51.7% of total time)}}&lt;br/&gt;
{{ Buffer utilization: 0.0%}}&lt;br/&gt;
{{ Cleaned 1.0 MB in 0.0 seconds (35.6 Mb/sec, 48.3% of total time)}}&lt;br/&gt;
{{ Start size: 1.0 MB (231 messages)}}&lt;br/&gt;
{{ End size: 1.0 MB (231 messages)}}&lt;br/&gt;
{{ 0.0% size reduction (0.0% fewer messages)}}&lt;br/&gt;
{{ (kafka.log.LogCleaner)}}&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,676&amp;#93;&lt;/span&gt; DEBUG Checking if log segment may be cleaned: log=&apos;backup_br_domain_squad-0&apos; segment.baseOffset=0 segment.largestTimestamp=1563203503987; now - compactionLag=1565096737676; is uncleanable=false (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,677&amp;#93;&lt;/span&gt; DEBUG Finding range of cleanable offsets for log=backup_br_domain_squad-0 topicPartition=backup_br_domain_squad-0. Last clean offset=Some(232) now=1565183137676 =&amp;gt; firstDirtyOffset=232 firstUncleanableOffset=233 activeSegment.baseOffset=233 (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,677&amp;#93;&lt;/span&gt; INFO Cleaner 0: Beginning cleaning of log backup_br_domain_squad-0. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,678&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for backup_br_domain_squad-0... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,680&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Created data-plane acceptor and processors for endpoint : EndPoint(null,9082,ListenerName(INSIDE_DOCKER),PLAINTEXT) (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,680&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Created data-plane acceptor and processors for endpoint : EndPoint(null,9082,ListenerName(INSIDE_DOCKER),PLAINTEXT) (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,681&amp;#93;&lt;/span&gt; INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,681&amp;#93;&lt;/span&gt; INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,700&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(OUTSIDE_DOCKER),PLAINTEXT) (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,700&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(OUTSIDE_DOCKER),PLAINTEXT) (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,701&amp;#93;&lt;/span&gt; INFO Awaiting socket connections on s0.0.0.0:19092. (kafka.network.Acceptor)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,701&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log backup_br_domain_squad-0 for 1 segments in offset range [232, 233). (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,701&amp;#93;&lt;/span&gt; INFO Awaiting socket connections on s0.0.0.0:19092. (kafka.network.Acceptor)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,702&amp;#93;&lt;/span&gt; INFO Cleaner 0: Offset map for log backup_br_domain_squad-0 complete. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,702&amp;#93;&lt;/span&gt; INFO Cleaner 0: Cleaning log backup_br_domain_squad-0 (cleaning prior to Wed Aug 07 13:05:22 GMT 2019, discarding tombstones prior to Tue Aug 06 13:05:22 GMT 2019)... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,703&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-log-cleaner-thread-0&amp;#93;&lt;/span&gt;:&lt;/tt&gt;&lt;br/&gt;
{{ Log cleaner thread 0 cleaned log backup_br_domain_squad-0 (dirty section = &lt;span class=&quot;error&quot;&gt;&amp;#91;232, 232&amp;#93;&lt;/span&gt;)}}&lt;br/&gt;
{{ 0.0 MB of log processed in 0.0 seconds (0.0 MB/sec).}}&lt;br/&gt;
{{ Indexed 0.0 MB in 0.0 seconds (0.3 Mb/sec, 96.2% of total time)}}&lt;br/&gt;
{{ Buffer utilization: 0.0%}}&lt;br/&gt;
{{ Cleaned 0.0 MB in 0.0 seconds (0.0 Mb/sec, 3.8% of total time)}}&lt;br/&gt;
{{ Start size: 0.0 MB (0 messages)}}&lt;br/&gt;
{{ End size: 0.0 MB (0 messages)}}&lt;br/&gt;
{{ NaN% size reduction (NaN% fewer messages)}}&lt;br/&gt;
{{ (kafka.log.LogCleaner)}}&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,707&amp;#93;&lt;/span&gt; DEBUG Checking if log segment may be cleaned: log=&apos;backup_br_domain_squad-0&apos; segment.baseOffset=0 segment.largestTimestamp=1563203503987; now - compactionLag=1565096737707; is uncleanable=false (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,708&amp;#93;&lt;/span&gt; DEBUG Finding range of cleanable offsets for log=backup_br_domain_squad-0 topicPartition=backup_br_domain_squad-0. Last clean offset=Some(0) now=1565183137707 =&amp;gt; firstDirtyOffset=0 firstUncleanableOffset=233 activeSegment.baseOffset=233 (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,708&amp;#93;&lt;/span&gt; INFO Cleaner 0: Beginning cleaning of log backup_br_domain_squad-0. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,708&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for backup_br_domain_squad-0... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,719&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Created data-plane acceptor and processors for endpoint : EndPoint(null,19092,ListenerName(MAC_OS_DOCKER),PLAINTEXT) (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,719&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Created data-plane acceptor and processors for endpoint : EndPoint(null,19092,ListenerName(MAC_OS_DOCKER),PLAINTEXT) (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,721&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log backup_br_domain_squad-0 for 1 segments in offset range [0, 233). (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,722&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Started 3 acceptor threads for data-plane (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,722&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;SocketServer brokerId=1001&amp;#93;&lt;/span&gt; Started 3 acceptor threads for data-plane (kafka.network.SocketServer)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,730&amp;#93;&lt;/span&gt; INFO Cleaner 0: Offset map for log backup_br_domain_squad-0 complete. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,730&amp;#93;&lt;/span&gt; INFO Cleaner 0: Cleaning log backup_br_domain_squad-0 (cleaning prior to Wed Aug 07 13:05:22 GMT 2019, discarding tombstones prior to Thu Jan 01 00:00:00 GMT 1970)... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,732&amp;#93;&lt;/span&gt; DEBUG Loaded index file /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.index.cleaned with maxEntries = 1310720, maxIndexSize = 10485760, entries = 0, lastOffset = 0, file position = 0 (kafka.log.OffsetIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,733&amp;#93;&lt;/span&gt; DEBUG Loaded index file /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.timeindex.cleaned with maxEntries = 873813, maxIndexSize = 10485760, entries = 0, lastOffset = TimestampOffset(-1,0), file position = 0 (kafka.log.TimeIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,733&amp;#93;&lt;/span&gt; INFO Cleaner 0: Cleaning segment 0 in log backup_br_domain_squad-0 (largest timestamp Mon Jul 15 15:11:43 GMT 2019) into 0, retaining deletes. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,739&amp;#93;&lt;/span&gt; DEBUG Resized /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.index.cleaned to 32, position is 32 and limit is 32 (kafka.log.OffsetIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,739&amp;#93;&lt;/span&gt; DEBUG Resized /runtime/logs/kafka_1/backup_br_domain_squad-0/00000000000000000000.timeindex.cleaned to 36, position is 36 and limit is 36 (kafka.log.TimeIndex)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,764&amp;#93;&lt;/span&gt; INFO Cleaner 0: Swapping in cleaned segment LogSegment(baseOffset=0, size=1044021) for segment(s) List(LogSegment(baseOffset=0, size=1044021)) in log Log(/runtime/logs/kafka_1/backup_br_domain_squad-0) (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,767&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-log-cleaner-thread-0&amp;#93;&lt;/span&gt;:&lt;/tt&gt;&lt;br/&gt;
{{ Log cleaner thread 0 cleaned log backup_br_domain_squad-0 (dirty section = &lt;span class=&quot;error&quot;&gt;&amp;#91;0, 0&amp;#93;&lt;/span&gt;)}}&lt;br/&gt;
{{ 1.0 MB of log processed in 0.1 seconds (17.2 MB/sec).}}&lt;br/&gt;
{{ Indexed 1.0 MB in 0.0 seconds (45.3 Mb/sec, 37.9% of total time)}}&lt;br/&gt;
{{ Buffer utilization: 0.0%}}&lt;br/&gt;
{{ Cleaned 1.0 MB in 0.0 seconds (27.7 Mb/sec, 62.1% of total time)}}&lt;br/&gt;
{{ Start size: 1.0 MB (231 messages)}}&lt;br/&gt;
{{ End size: 1.0 MB (231 messages)}}&lt;br/&gt;
{{ 0.0% size reduction (0.0% fewer messages)}}&lt;br/&gt;
{{ (kafka.log.LogCleaner)}}&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,772&amp;#93;&lt;/span&gt; DEBUG Checking if log segment may be cleaned: log=&apos;backup_br_domain_squad-0&apos; segment.baseOffset=0 segment.largestTimestamp=1563203503987; now - compactionLag=1565096737771; is uncleanable=false (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,772&amp;#93;&lt;/span&gt; DEBUG Finding range of cleanable offsets for log=backup_br_domain_squad-0 topicPartition=backup_br_domain_squad-0. Last clean offset=Some(232) now=1565183137771 =&amp;gt; firstDirtyOffset=232 firstUncleanableOffset=233 activeSegment.baseOffset=233 (kafka.log.LogCleanerManager$)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,773&amp;#93;&lt;/span&gt; INFO Cleaner 0: Beginning cleaning of log backup_br_domain_squad-0. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,774&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for backup_br_domain_squad-0... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,795&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-Produce&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,795&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-Produce&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,800&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-Fetch&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,800&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-Fetch&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,803&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-DeleteRecords&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,803&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-DeleteRecords&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,805&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-ElectPreferredLeader&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,805&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ExpirationReaper-1001-ElectPreferredLeader&amp;#93;&lt;/span&gt;: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,806&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log backup_br_domain_squad-0 for 1 segments in offset range [232, 233). (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,807&amp;#93;&lt;/span&gt; INFO Cleaner 0: Offset map for log backup_br_domain_squad-0 complete. (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,808&amp;#93;&lt;/span&gt; INFO Cleaner 0: Cleaning log backup_br_domain_squad-0 (cleaning prior to Wed Aug 07 13:05:22 GMT 2019, discarding tombstones prior to Tue Aug 06 13:05:22 GMT 2019)... (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-07 13:05:37,809&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-log-cleaner-thread-0&amp;#93;&lt;/span&gt;:&lt;/tt&gt;&lt;br/&gt;
{{ Log cleaner thread 0 cleaned log backup_br_domain_squad-0 (dirty section = &lt;span class=&quot;error&quot;&gt;&amp;#91;232, 232&amp;#93;&lt;/span&gt;)}}&lt;br/&gt;
{{ 0.0 MB of log processed in 0.0 seconds (0.0 MB/sec).}}&lt;br/&gt;
{{ Indexed 0.0 MB in 0.0 seconds (0.2 Mb/sec, 97.1% of total time)}}&lt;br/&gt;
{{ Buffer utilization: 0.0%}}&lt;br/&gt;
{{ Cleaned 0.0 MB in 0.0 seconds (0.0 Mb/sec, 2.9% of total time)}}&lt;br/&gt;
{{ Start size: 0.0 MB (0 messages)}}&lt;br/&gt;
{{ End size: 0.0 MB (0 messages)}}&lt;br/&gt;
{{ NaN% size reduction (NaN% fewer messages)}}&lt;br/&gt;
{{ (kafka.log.LogCleaner)}}&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment>docker base image: openjdk:8-jre-alpine base image, kafka from &lt;a href=&quot;http://ftp.carnet.hr/misc/apache/kafka/2.2.1/kafka_2.12-2.2.1.tgz&quot;&gt;http://ftp.carnet.hr/misc/apache/kafka/2.2.1/kafka_2.12-2.2.1.tgz&lt;/a&gt;</environment>
        <key id="13249351">KAFKA-8764</key>
            <summary>LogCleanerManager endless loop while compacting/cleaning segments</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="trajakovic">Tomislav Rajakovic</reporter>
                        <labels>
                            <label>patch</label>
                    </labels>
                <created>Wed, 7 Aug 2019 13:16:57 +0000</created>
                <updated>Wed, 2 Sep 2020 20:00:33 +0000</updated>
                            <resolved>Fri, 31 Jan 2020 11:38:46 +0000</resolved>
                                    <version>2.2.1</version>
                    <version>2.3.0</version>
                    <version>2.4.0</version>
                                    <fixVersion>2.2.3</fixVersion>
                    <fixVersion>2.3.2</fixVersion>
                    <fixVersion>2.4.1</fixVersion>
                    <fixVersion>2.5.0</fixVersion>
                                    <component>log cleaner</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="16903102" author="trajakovic" created="Thu, 8 Aug 2019 16:06:44 +0000"  >&lt;h1&gt;&lt;a name=&quot;Clue%3A&quot;&gt;&lt;/a&gt;Clue:&lt;/h1&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;By further investigation, it looks like:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;segment&#160;00000000000000000000 has end (existing) offset at 231&lt;/li&gt;
	&lt;li&gt;record with offset 232 is missing?!&#160;&lt;/li&gt;
	&lt;li&gt;while (active) segment&#160;00000000000000000233 has first offset 233&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;When LogCleaner starts, it first looks for [0,233) to cleanup, but while building&#160;buildOffsetMap, it ends up with offset 231 (that&apos;s last written offset in&#160;00000000000000000000 segment), and new endOffset = 231 + 1 = 232, and this is written in&#160;cleaner-offset-checkpoint file.&lt;/p&gt;

&lt;p&gt;Next time, LogCleaner starts with [232,233) to cleanup, but while building&#160;buildOffsetMap, it ends up with offset -1, since 232 is non-existing in&#160;00000000000000000000 segment, and new endOffset= -1 + 1 = 0, and this is written in&#160;cleaner-offset-checkpoint.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;Elaboration%3A&quot;&gt;&lt;/a&gt;Elaboration:&lt;/h1&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;By looking at &lt;b&gt;kafka_2.12_2.2.1&lt;/b&gt; source code,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;all this happens inside LogCleaner.doClean (lines: 492-520), where&#160;buildOffsetMap(line: 501) in second clean run (probably also in first run, since 232 is non-existing offset) builds wrong&#160;offsetMap since reading&#160;&lt;/li&gt;
	&lt;li&gt;segment.log.readInto(readBuffer, position) (line: 899) and val records = MemoryRecords.readableRecords(readBuffer) (line: 905)&#160;&lt;/li&gt;
	&lt;li&gt;yields with offset 231 record, and&#160;&lt;/li&gt;
	&lt;li&gt;for (batch &amp;lt;- records.batches.asScala) (line: 909-934) is skipped, while&#160;&lt;/li&gt;
	&lt;li&gt;map: OffsetMap remains unfilled with default lastOffset = -1 which is later used to determine endOffset for next cleaning (thus endOffset = -1 + 1 = 0, later written in&#160;cleaner-offset-checkpoint)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;Summary%3A&quot;&gt;&lt;/a&gt;Summary:&lt;/h1&gt;

&lt;p&gt;Since last record offset in segment 00000000000000000000&#160;is 231 (232 is missing) and next rolled segment is&#160;00000000000000000233, LogCleaner never progress past segment 00000000000000000000 ending in endless loop cleaning segment 00000000000000000000 with&#160;&lt;b&gt;cleaner-offset-checkpoint&lt;/b&gt; 0-&amp;gt;232-&amp;gt;0-&amp;gt;232....causing high disk read/write/iops operations.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16912178" author="trajakovic" created="Wed, 21 Aug 2019 11:06:01 +0000"  >&lt;p&gt;Well it happened again on totally separated environment (production this time), same version of kafka (2.2.1). Issue started after I&apos;ve removed one broker from cluster, wipe out data, and return it to cluster.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;When checked log segment (kafka.tools.DumpLogSegments) 00000000000000000000 of troublesome partition, last offset was&#160;179959, while next (active) segment was rolled with&#160;180015 offset. So log cleaner again was jumping:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Building offset map for log X for 1 segments in offset range [0, 180015)&lt;/li&gt;
	&lt;li&gt;Building offset map for log X for 1 segments in offset range [179960, 180015)&lt;/li&gt;
	&lt;li&gt;Building offset map for log X for 1 segments in offset range [0, 180015)&lt;/li&gt;
	&lt;li&gt;...etc&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Since I&apos;m not familiar with LOG structure, I don&apos;t know if empty offsets (from [179960,&#160;180015)) in 0 segment are allowed, and if it&apos;s problem in LogCleaner, LOG structure or discrepancy between them.&lt;/p&gt;</comment>
                            <comment id="16912204" author="trajakovic" created="Wed, 21 Aug 2019 11:31:59 +0000"  >&lt;p&gt;Btw. &lt;b&gt;solution&lt;/b&gt; for this issue, while I&apos;m waiting for someone smart to find/fix the bug, is to manually update&#160;&lt;b&gt;cleaner-offset-checkpoint.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;Steps%3A&quot;&gt;&lt;/a&gt;Steps:&lt;/h2&gt;
&lt;ol&gt;
	&lt;li&gt;Stop broker(s) with described issue&lt;/li&gt;
	&lt;li&gt;find partition(s) that bugged LogCleaner
	&lt;ul&gt;
		&lt;li&gt;good place for search them is log-cleaner.log output, something like endless lines of this:&lt;/li&gt;
		&lt;li&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-21 09:33:32,060&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log br_domain_match-22 for 1 segments in offset range [0, 180015). (kafka.log.LogCleaner)&lt;/li&gt;
		&lt;li&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-21 09:33:32,167&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log br_domain_match-22 for 1 segments in offset range [179960, 180015). (kafka.log.LogCleaner)&lt;/li&gt;
		&lt;li&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-21 09:33:32,195&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log br_domain_match-22 for 1 segments in offset range [0, 180015). (kafka.log.LogCleaner)&lt;/li&gt;
		&lt;li&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-08-21 09:33:32,440&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log br_domain_match-22 for 1 segments in offset range [179960, 180015). (kafka.log.LogCleaner)&lt;/li&gt;
		&lt;li&gt;...&lt;/li&gt;
		&lt;li&gt;in this case, partition br_domain_match-22 is troubled one, and LogCleaner flip-flops between&#160;[0, 180015),&#160;[179960, 180015),&#160;[0, 180015), ....&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;remember/writeup last offset for partition in ranges (in my case&#160;[179960, 180015), so it&apos;s &lt;b&gt;NEXT_CLEANING_OFFSET=180015&lt;/b&gt;)&lt;/li&gt;
	&lt;li&gt;open cleaner-offset-checkpoint file (placed at root of log.dirs folder), and find line with topic-partition
	&lt;ul&gt;
		&lt;li&gt;in my case, it&apos;s topic &lt;b&gt;br_domain_match&lt;/b&gt;, partition &lt;b&gt;22&lt;/b&gt;, so line contains&#160;&lt;b&gt;br_domain_match 22 XXXXX&lt;/b&gt;&lt;/li&gt;
		&lt;li&gt;replace &lt;b&gt;XXXXX&lt;/b&gt; with &lt;b&gt;NEXT_CLEANING_OFFSET&lt;/b&gt; (from point 3.)&lt;/li&gt;
		&lt;li&gt;save file&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;and repeat that for each partition with issue&lt;/li&gt;
	&lt;li&gt;start broker and watch cleaner log file, with little luck, everything should be ok&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="17009262" author="jnadler" created="Tue, 7 Jan 2020 00:46:18 +0000"  >&lt;p&gt;I wanted to throw my .02 in here:&#160; &#160;We are seeing the same issue, also with compact topics.&#160; &#160;The compact topics in question are quite low traffic.&lt;/p&gt;

&lt;p&gt;LogCleaner is running flat-out, with only a few ms between attempts to clean a single topic-partition.&lt;/p&gt;

&lt;p&gt;The log cleaner thread is consuming almost an entire core:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
14857 kafka &#160; &#160; 20 &#160; 0 8894844 2.187g&#160; 12396 R 87.5 56.8&#160; 47:04.92 kafka-log-clean 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We&apos;re running 2.4.0, openjdk11, happy to provide any add&apos;l info to help with this issue.&#160; &#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17012667" author="seva.f" created="Fri, 10 Jan 2020 10:09:09 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;We have the exact same issue with __consumer_offset compacted topic which kills our consumer groups. Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trajakovic&quot; class=&quot;user-hover&quot; rel=&quot;trajakovic&quot;&gt;trajakovic&lt;/a&gt;,&#160;for the solution on manually update&#160;&lt;b&gt;cleaner-offset-checkpoint file.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;BR&lt;/p&gt;</comment>
                            <comment id="17012683" author="trajakovic" created="Fri, 10 Jan 2020 10:22:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jnadler&quot; class=&quot;user-hover&quot; rel=&quot;jnadler&quot;&gt;jnadler&lt;/a&gt;&#160;and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=seva.f&quot; class=&quot;user-hover&quot; rel=&quot;seva.f&quot;&gt;seva.f&lt;/a&gt;&#160;so it seems that they still didn&apos;t fix this issue, or it happens in rare condition(s). I know that in time when I was solving this issue, I&apos;ve tested it with multiple versions (of 2.x.x) and all had same problem. Although I&apos;m not so much fluent in Scala, I could probably compile latest version and give some hints about patching this issue out (got some clues back then when issue occurred).&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Btw. voting for this issue might get faster help/attention &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="17012962" author="trajakovic" created="Fri, 10 Jan 2020 15:22:46 +0000"  >&lt;p&gt;Kafka (2.4.0) LogCleaner patch for issue&lt;/p&gt;</comment>
                            <comment id="17013052" author="jnadler" created="Fri, 10 Jan 2020 16:42:16 +0000"  >&lt;p&gt;I just attached a graph of CPU usage in a small, 3-node cluster that shows this issue.&#160; &#160;You can see that for all 3 nodes a massive increase in CPU usage when the log cleaner is going nuts - the high CPU periods correspond to tons of log entries in &apos;log-cleaner.log&apos;, and high CPU usage of the &apos;kafka-log-clean&apos; thread.&lt;/p&gt;</comment>
                            <comment id="17013058" author="junrao" created="Fri, 10 Jan 2020 16:53:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trajakovic&quot; class=&quot;user-hover&quot; rel=&quot;trajakovic&quot;&gt;trajakovic&lt;/a&gt;: Thanks for the investigation. Normally, the offset map is built from the dirty portion of the log that shouldn&apos;t contain holes in offsets. If segment&#160;[0,233) misses offset 232, it means that this segment has been cleaned and the dirty offset should have moved to 233 after the first round of cleaning. Was the dirty offset ever reset manually? In any case, I agree that it would be better to make the code more defensive. Could you submit the patch as a PR (details can be found in&#160;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes&lt;/a&gt;)?&lt;/p&gt;</comment>
                            <comment id="17013073" author="trajakovic" created="Fri, 10 Jan 2020 17:22:02 +0000"  >&lt;p&gt;Hy &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;. This issue happened on relatively busy kafka cluster, but I&apos;ve never tinkered with kafka files, or&#160; did any manual actions. And what&apos;s more indicative, this issue first time happened on follower brokers/replicas (ISRs), while master broker for topic-partition didn&apos;t &quot;feel&quot; same effect. Maybe origin of the problem was &quot;first&quot; ever cleanup on master broker, leaving holes, but that&apos;s just my guess.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;And yes, I&apos;m gonna make PR, just need to see &quot;first good PR&quot; and Contribution guideline.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17013081" author="junrao" created="Fri, 10 Jan 2020 17:32:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trajakovic&quot; class=&quot;user-hover&quot; rel=&quot;trajakovic&quot;&gt;trajakovic&lt;/a&gt;: Thanks for providing the additional info. That makes sense now. Currently, the log cleaner runs independently on each replica. If a follower has been down for some time, it is possible that the leader has already cleaned the data and left holes in some log segments. When those segments get replicated to the follower, the follower will clean the same data again and potentially hit the above issue.&lt;/p&gt;</comment>
                            <comment id="17013179" author="githubbot" created="Fri, 10 Jan 2020 20:14:12 +0000"  >&lt;p&gt;trajakovic commented on pull request #7932: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8764&quot; title=&quot;LogCleanerManager endless loop while compacting/cleaning segments&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8764&quot;&gt;&lt;del&gt;KAFKA-8764&lt;/del&gt;&lt;/a&gt;: LogCleanerManager endless loop while compacting/clea&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/7932&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/7932&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   This PR fixes LogCleaner&apos;s endless loop while clearing LogSegemnts with holes.&lt;/p&gt;


&lt;p&gt;   In rare cases, when clearing LogSegments with missing records, LogCleaner was unable to progress resulting with high CPU usage, high disk read/writes and excessive cleaner logs (if enabled). This PR addresses such situation by skipping missing record(s) and, as result, avoiding endless loop while clearing such Logs.&lt;/p&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="17013187" author="trajakovic" created="Fri, 10 Jan 2020 20:35:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jnadler&quot; class=&quot;user-hover&quot; rel=&quot;jnadler&quot;&gt;jnadler&lt;/a&gt;&#160;for now, try to &quot;help&quot; LogCleaner by following solution steps from above. Idea is to move it&apos;s state file (cleaner-offset-checkpoint inside topic-partition folder) on all brokers that experiencing issue. Once when next LogSegment becomes available for cleaning, LogCleaner would fix himself and continue to work as expected (unless you&apos;ll have new record &quot;holes&quot;, but that is, as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;&#160;stated, is rare event happening in edge cases).&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Advanced solution is to checkout kafka 2.4.0 from github, apply patch attached to this issue, rebuild kafka and run patched version of kafka with your data, and issue, hopefully, should be gone.&#160;&lt;/p&gt;</comment>
                            <comment id="17015432" author="githubbot" created="Tue, 14 Jan 2020 22:03:50 +0000"  >&lt;p&gt;mumrah commented on pull request #7932: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8764&quot; title=&quot;LogCleanerManager endless loop while compacting/cleaning segments&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8764&quot;&gt;&lt;del&gt;KAFKA-8764&lt;/del&gt;&lt;/a&gt;: LogCleanerManager endless loop while compacting/clea&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/7932&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/7932&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on to GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="17189628" author="wushujames" created="Wed, 2 Sep 2020 18:36:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, in the comment &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8764?focusedCommentId=17013081&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17013081&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-8764?focusedCommentId=17013081&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17013081&lt;/a&gt;&#160;above, you said&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Currently, the log cleaner runs independently on each replica. If a follower has been down for some time, it is possible that the leader has already cleaned the data and left holes in some log segments. When those segments get replicated to the follower, the follower will clean the same data again and potentially hit the above issue.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Would this also happen if you added a new follower? For example, if I have a compacted topic on Brokers 1 2 3 where compaction has already happened, and then I use kafka-reassign-partitions to move the brokers 4 5 6, it sounds like compaction will happen independently on brokers 4 5 6, and will run into this issue.&lt;/p&gt;</comment>
                            <comment id="17189684" author="junrao" created="Wed, 2 Sep 2020 20:00:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wushujames&quot; class=&quot;user-hover&quot; rel=&quot;wushujames&quot;&gt;wushujames&lt;/a&gt;: Yes, reassigned replica can have the same issue.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12990541" name="Screen Shot 2020-01-10 at 8.38.25 AM.png" size="321010" author="jnadler" created="Fri, 10 Jan 2020 16:39:52 +0000"/>
                            <attachment id="12990538" name="kafka2.4.0-KAFKA-8764.patch" size="1291" author="trajakovic" created="Fri, 10 Jan 2020 15:21:49 +0000"/>
                            <attachment id="12990536" name="kafka2.4.0-KAFKA-8764.patch" size="1291" author="trajakovic" created="Fri, 10 Jan 2020 15:19:44 +0000"/>
                            <attachment id="12976937" name="log-cleaner-bug-reproduction.zip" size="391296" author="trajakovic" created="Wed, 7 Aug 2019 13:13:23 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 10 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z05exk:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>