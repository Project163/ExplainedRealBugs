<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:37:01 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-693] Consumer rebalance fails if no leader available for a partition and stops all fetchers</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-693</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;I am currently experiencing this with the MirrorMaker but I assume it happens for any rebalance. The symptoms are:&lt;/p&gt;

&lt;p&gt;I have replication factor of 1&lt;/p&gt;

&lt;p&gt;1. If i start the MirrorMaker (bin/kafka-run-class.sh kafka.tools.MirrorMaker --consumer.config mirror-consumer.properties  --producer.config mirror-producer.properties --blacklist &apos;xdummyx&apos; --num.streams=1 --num.producers=1) with a broker down&lt;br/&gt;
1.1 I set the refresh.leader.backoff.ms to 600000 (10min) so that the ConsumerFetcherManager doesn&apos;t retry to often to get the unavailable partitions&lt;br/&gt;
1.2 The rebalance starts at the init step and fails: Exception in thread &quot;main&quot; kafka.common.ConsumerRebalanceFailedException: KafkaMirror_mirror-01-1357893495345-fac86b15 can&apos;t rebalance after 4 retries&lt;br/&gt;
1.3 After the exception, everything stops (fetchers and queues)&lt;br/&gt;
1.4 I attached the full logs (info &amp;amp; debug) for this case&lt;/p&gt;

&lt;p&gt;2. If i start the MirrorMaker with all the brokers up and then kill a broker&lt;br/&gt;
2.1 The first rebalance is successful&lt;br/&gt;
2.2 The consumer will handle correctly the broker down and stop the associated ConsumerFetcherThread&lt;br/&gt;
2.3 The refresh.leader.backoff.ms to 600000 works correctly&lt;br/&gt;
2.4 If something triggers a rebalance (new topic, partition reassignment...), then we go back to 1., the rebalance fails and stops everything.&lt;/p&gt;

&lt;p&gt;I think the desired behavior is to consumer whatever is available, and try later at some intervals. I would be glad to help on that issue although the Consumer code seems a little tough to get on.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12627146">KAFKA-693</key>
            <summary>Consumer rebalance fails if no leader available for a partition and stops all fetchers</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="brugidou">Maxime Brugidou</assignee>
                                    <reporter username="brugidou">Maxime Brugidou</reporter>
                        <labels>
                            <label>p2</label>
                    </labels>
                <created>Fri, 11 Jan 2013 09:22:20 +0000</created>
                <updated>Thu, 24 Jan 2013 04:53:16 +0000</updated>
                            <resolved>Thu, 17 Jan 2013 18:16:17 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13552341" author="junrao" created="Sun, 13 Jan 2013 22:32:29 +0000"  >&lt;p&gt;Ok. This is actually a real problem. During rebalance, we actually try to get the leader even though we don&apos;t really need it at rebalancing time. The fix seems easy.&lt;/p&gt;

&lt;p&gt;In ZKRebalancerListener.addPartitionTopicInfo(), we don&apos;t really need to get the leaderId, which is not used in PartitionTopicInfo. So, we can just get rid of that code. We can also get rid of the code in rebalance() that computes leaderIdForPartitionsMap.&lt;/p&gt;</comment>
                            <comment id="13552781" author="brugidou" created="Mon, 14 Jan 2013 15:27:21 +0000"  >&lt;p&gt;I looked up the code in details and I am stuck because during the rebalance() operation, the ZookeeperConsumerConnector&apos;s topicRegistry is updated with some PartitionTopicInfo that needs to store the consumerOffset and fetchOffset. During addPartitionTopicInfo(), the consumer offset is read from Zookeeper, however it needs to be initialized if no offsetString is available on Zookeeper (first time starting a consumer), and we need to access the broker/leader to get the start offset (using SimpleConsumer.earliestOrLatestOffset() in addPartitionTopicInfo()).&lt;/p&gt;

&lt;p&gt;I digged a bit and we could probably initialize the offset later in the ConsumerFetcherManager? I could help with a patch if i get general directions because i&apos;m not 100% familiar with the codebase yet.&lt;/p&gt;</comment>
                            <comment id="13552854" author="junrao" created="Mon, 14 Jan 2013 16:37:02 +0000"  >&lt;p&gt;That&apos;s a good point. I overlooked this. Your understanding is correct. We could move the offset initialization logic into AbstractFetcherThread. The following is one way to do this. Not sure if this is the best way.&lt;/p&gt;

&lt;p&gt;1. In AbstractFetcher: &lt;br/&gt;
Change addPartition to pass in initialOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;.&lt;br/&gt;
If initialOffset is none, we call handleOffsetOutOfRange to get the offset. If we hit any exception while doing this, we pass the exception to the caller without adding the partition to partitionMap.&lt;/p&gt;

&lt;p&gt;2. In ConsumerFetcherManager.doWork():&lt;br/&gt;
If we hit any exception when calling addFetcher, we add the partition back to noLeaderPartitionSet.&lt;/p&gt;

&lt;p&gt;3. In ConsumerFetcherThread.handleOffsetOutOfRange():&lt;br/&gt;
We need to check if the offset response has any error. If so, we throw an exception to the caller.&lt;/p&gt;

&lt;p&gt;4. In ZookeeperConsumerConnector.addPartitionTopicInfo(): If initial offset doesn&apos;t exist in ZK, we pass in none to PartitionTopicInfo.&lt;/p&gt;

&lt;p&gt;5. In PartitionTopicInfo: Make fetchedOffset Option&lt;span class=&quot;error&quot;&gt;&amp;#91;AtomicLong&amp;#93;&lt;/span&gt;.&lt;/p&gt;</comment>
                            <comment id="13552951" author="brugidou" created="Mon, 14 Jan 2013 18:18:35 +0000"  >&lt;p&gt;Looks good, It should work but I still have a pain point about PartitionTopicInfo that uses AtomicLong to track consume/fetch offsets. Using Option&lt;span class=&quot;error&quot;&gt;&amp;#91;AtomicLong&amp;#93;&lt;/span&gt; looks strange, because I have to change the 2 counters and make them variables... And it&apos;s probably not thread safe at all so I would need some sort of lock to &quot;initialize&quot; the counters.&lt;/p&gt;</comment>
                            <comment id="13553136" author="junrao" created="Mon, 14 Jan 2013 21:29:58 +0000"  >&lt;p&gt;Another quick thought on this. Instead of using Option for offset, we could still use AtomicLong and pass in sth like -1 to indicate a non-exist offset.&lt;/p&gt;</comment>
                            <comment id="13553751" author="brugidou" created="Tue, 15 Jan 2013 12:54:02 +0000"  >&lt;p&gt;Here is a patch:&lt;/p&gt;

&lt;p&gt;1. AbstractFetchThread.addPartition(): call handleOffsetOutOfRange if initialOffset &amp;lt; 0&lt;/p&gt;

&lt;p&gt;2. I didnt touch ConsumerFetcherManager.doWork() since addFetcher() is called for partitions with leaders only (which is why 3 is unnecessary).&lt;/p&gt;

&lt;p&gt;3. ConsumerFetcherThrad.handleOffsetOutOfRange: check partitionErrorAndOffset.error and throw appropriate exception (which should have been done anyway, I don&apos;t think this is necessary for the patch)&lt;br/&gt;
3.1 Note: this should probably be done in the ReplicaFetcherThread too?&lt;/p&gt;

&lt;p&gt;4. ZookeeperConsumerConnector.ZkRebalanceListener: Do not compute leaderIdForPartitionMap in rebalance() and set PartitionTopicInfo offsets to -1 if not in Zk (new consumer)&lt;/p&gt;

&lt;p&gt;5. PartitionTopicInfo: removed brokerId&lt;/p&gt;

&lt;p&gt;6. Fixed tests for compilation (I am having a hard time running tests since ./sbt test does not seem to work for me very well)&lt;/p&gt;

&lt;p&gt;7. Should we increase the default refresh.leader.backoff.ms ? It&apos;s tradeoff between being able to pick fast a new leader to consume (useful when replication is on) and not flooding the broker when there is no leader (or replication is off). 200ms is very short, but something hybrid like &quot;try 5 times at 200ms backoff, then every 5min&quot; would get all use cases.&lt;/p&gt;

&lt;p&gt;I am running this on test clusters with a mirrormaker andthe error that I had in my initial test case (in the description) does not occur anymore.&lt;/p&gt;</comment>
                            <comment id="13554068" author="junrao" created="Tue, 15 Jan 2013 18:12:43 +0000"  >&lt;p&gt;Thanks for the patch. Some comments:&lt;/p&gt;

&lt;p&gt;10. ZookeeperConsumerConnector: Let&apos;s define a constant InvalidOffset, instead of using -1 directly.&lt;/p&gt;

&lt;p&gt;11. ConsumerFetcherManager.doWork(): After we identify the leader of a partition, the leader could change immediately. So, we may hit the exception when calling addFetcher(). When this happens, we haven&apos;t added the partition to the fetcher and we don&apos;t want to lose it. So, we should add it back to noLeaderPartitionSet so that we can find the new leader later.&lt;/p&gt;

&lt;p&gt;12. ReplicaFetcherThread: Yes, it should also throw an exception if getOffsetBefore returns an error.&lt;/p&gt;

&lt;p&gt;13. AbstractFetcherThread.doWork(): We need to handle the exception when calling handleOffsetOutOfRange(). If we get an exception, we should add the partition to partitionsWithError. This will cover both ConsumerFetcherThread and ReplicaFetcherThread.&lt;/p&gt;</comment>
                            <comment id="13554852" author="brugidou" created="Wed, 16 Jan 2013 08:50:44 +0000"  >&lt;p&gt;10. Created PartitionTopicInfo.InvalidOffset&lt;/p&gt;

&lt;p&gt;11. In ConsumerFetcherManager.doWork(), I believe that addFetcher() is called before the partition is removed from noLeaderPartitionSet, if an exception is caught the partition will still be in the noLeaderPartitionSet, so I didn&apos;t change anything&lt;/p&gt;

&lt;p&gt;12. done&lt;/p&gt;

&lt;p&gt;13. done&lt;/p&gt;</comment>
                            <comment id="13555251" author="junrao" created="Wed, 16 Jan 2013 17:46:58 +0000"  >&lt;p&gt;Thanks for patch v2. Looks good. Some minor comments:&lt;/p&gt;

&lt;p&gt;11. I think we still need to change ConsumerFetcherManager.doWork(): Currently, if we hit an exception when calling addFetcher(), we won&apos;t remove any partition from noLeaderPartitionSet, include those that have been processed successfully. We can change it so that we remove each partition from noLeaderPartitionSet after calling addFetcher() successfully.&lt;/p&gt;

&lt;p&gt;20. AbstractFetcherThread: Instead of doing initialOffset &amp;lt; 0, could we define an isOffsetInvalid() method?&lt;/p&gt;</comment>
                            <comment id="13556170" author="brugidou" created="Thu, 17 Jan 2013 13:52:34 +0000"  >&lt;p&gt;Added v3 with your remarks&lt;/p&gt;</comment>
                            <comment id="13556433" author="junrao" created="Thu, 17 Jan 2013 18:16:17 +0000"  >&lt;p&gt;Thanks for the patch. Committed to 0.8 with the following minor changes.&lt;/p&gt;

&lt;p&gt;1. ConsumerFetcherManager: fixed the bug in the new warn logging.&lt;br/&gt;
2. AbstractFetcherThread: moved isOffsetInvalid() to where InvalidOffset is defined.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12626789">KAFKA-691</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12565097" name="KAFKA-693-v2.patch" size="13527" author="brugidou" created="Wed, 16 Jan 2013 08:48:15 +0000"/>
                            <attachment id="12565320" name="KAFKA-693-v3.patch" size="15013" author="brugidou" created="Thu, 17 Jan 2013 13:52:34 +0000"/>
                            <attachment id="12564916" name="KAFKA-693.patch" size="10058" author="brugidou" created="Tue, 15 Jan 2013 12:41:35 +0000"/>
                            <attachment id="12564382" name="mirror.log" size="71461" author="brugidou" created="Fri, 11 Jan 2013 09:22:40 +0000"/>
                            <attachment id="12564383" name="mirror_debug.log" size="154639" author="brugidou" created="Fri, 11 Jan 2013 09:22:40 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>303915</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 44 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i17guv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>251700</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>