<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:37:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-695] Broker shuts down due to attempt to read a closed index file</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-695</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Broker shuts down with the following error message -&lt;/p&gt;


&lt;p&gt;013/01/11 01:43:51.320 ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaApis&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;request-expiration-task&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; []  &lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaApi-277&amp;#93;&lt;/span&gt; error when processing request (service_metrics,2,39192,2000000)&lt;br/&gt;
java.nio.channels.ClosedChannelException&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:613)&lt;br/&gt;
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:82)&lt;br/&gt;
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:76)&lt;br/&gt;
        at kafka.log.LogSegment.read(LogSegment.scala:106)&lt;br/&gt;
        at kafka.log.Log.read(Log.scala:386)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:369)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:327)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:323)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:323)&lt;br/&gt;
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:519)&lt;br/&gt;
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:501)&lt;br/&gt;
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:222)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
2013/01/11 01:43:52.815 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Processor&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-processor-10251-2&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; []  Closing socket connection to /172.20.72.244.&lt;br/&gt;
2013/01/11 01:43:54.286 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Processor&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-processor-10251-3&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; []  Closing socket connection to /172.20.72.243.&lt;br/&gt;
2013/01/11 01:43:54.385 ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-logflusher-1&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; []  &lt;span class=&quot;error&quot;&gt;&amp;#91;Log Manager on Broker 277&amp;#93;&lt;/span&gt; Error flushing topic service_metrics&lt;br/&gt;
java.nio.channels.ClosedChannelException&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:349)&lt;br/&gt;
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply$mcV$sp(FileMessageSet.scala:154)&lt;br/&gt;
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)&lt;br/&gt;
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)&lt;br/&gt;
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)&lt;br/&gt;
        at kafka.log.FileMessageSet.flush(FileMessageSet.scala:153)&lt;br/&gt;
        at kafka.log.LogSegment.flush(LogSegment.scala:151)&lt;br/&gt;
        at kafka.log.Log.flush(Log.scala:493)&lt;br/&gt;
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:319)&lt;br/&gt;
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:310)&lt;br/&gt;
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
        at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)&lt;br/&gt;
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)&lt;br/&gt;
        at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)&lt;br/&gt;
        at kafka.log.LogManager.kafka$log$LogManager$$flushDirtyLogs(LogManager.scala:310)&lt;br/&gt;
        at kafka.log.LogManager$$anonfun$startup$2.apply$mcV$sp(LogManager.scala:144)&lt;br/&gt;
        at kafka.utils.Utils$$anon$2.run(Utils.scala:66)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)&lt;br/&gt;
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)&lt;br/&gt;
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
2013/01/11 01:43:54.447 FATAL &lt;span class=&quot;error&quot;&gt;&amp;#91;LogManager&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-logflusher-1&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; []  &lt;span class=&quot;error&quot;&gt;&amp;#91;Log Manager on Broker 277&amp;#93;&lt;/span&gt; Halting due to unrecoverable I/O error while flushing logs: null&lt;br/&gt;
java.nio.channels.ClosedChannelException&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:349)&lt;br/&gt;
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply$mcV$sp(FileMessageSet.scala:154)&lt;br/&gt;
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)&lt;br/&gt;
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)&lt;br/&gt;
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)&lt;br/&gt;
        at kafka.log.FileMessageSet.flush(FileMessageSet.scala:153)&lt;br/&gt;
        at kafka.log.LogSegment.flush(LogSegment.scala:151)&lt;br/&gt;
        at kafka.log.Log.flush(Log.scala:493)&lt;/p&gt;

&lt;p&gt;       at kafka.log.LogSegment.flush(LogSegment.scala:151)&lt;br/&gt;
        at kafka.log.Log.flush(Log.scala:493)&lt;br/&gt;
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:319)&lt;br/&gt;
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:310)&lt;br/&gt;
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
        at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)&lt;br/&gt;
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)&lt;br/&gt;
        at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)&lt;br/&gt;
        at kafka.log.LogManager.kafka$log$LogManager$$flushDirtyLogs(LogManager.scala:310)&lt;br/&gt;
        at kafka.log.LogManager$$anonfun$startup$2.apply$mcV$sp(LogManager.scala:144)&lt;br/&gt;
        at kafka.utils.Utils$$anon$2.run(Utils.scala:66)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)&lt;br/&gt;
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)&lt;br/&gt;
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
2013/01/11 01:43:54.512 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ComponentsContextLoaderListener&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-2&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; []  Shutting down...&lt;/p&gt;</description>
                <environment></environment>
        <key id="12627225">KAFKA-695</key>
            <summary>Broker shuts down due to attempt to read a closed index file</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="junrao">Jun Rao</assignee>
                                    <reporter username="nehanarkhede">Neha Narkhede</reporter>
                        <labels>
                            <label>p1</label>
                    </labels>
                <created>Fri, 11 Jan 2013 18:17:46 +0000</created>
                <updated>Wed, 6 Feb 2013 04:02:55 +0000</updated>
                            <resolved>Wed, 6 Feb 2013 04:02:52 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                    <component>log</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="13552135" author="jkreps" created="Sun, 13 Jan 2013 04:00:53 +0000"  >&lt;p&gt;Hmm, so this can happen. Currently in 0.8 when we delete a file there is nothing that block reads on that file. The assumption is that this only happens at the tail of the log and is rare. This bug problem is fixed on trunk by the async delete patch.&lt;/p&gt;

&lt;p&gt;However the question is whether this is really what is happening...?&lt;/p&gt;</comment>
                            <comment id="13552342" author="junrao" created="Sun, 13 Jan 2013 22:36:09 +0000"  >&lt;p&gt;The weird thing is that the file actually exists on disk (this is a low volume topic that has only 1 segment) and there was no logging that shows the segment has been deleted.&lt;/p&gt;</comment>
                            <comment id="13559935" author="jkreps" created="Tue, 22 Jan 2013 19:45:16 +0000"  >&lt;p&gt;Some research:&lt;br/&gt;
1. Taking also &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-719&quot; title=&quot;Kafka broker shuts down due to irrecoverable IO error&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-719&quot;&gt;&lt;del&gt;KAFKA-719&lt;/del&gt;&lt;/a&gt; we have examples of this happening in the background flush, the read, and in append(). Flush and append only happen on the active segment so that complicates things.&lt;br/&gt;
2. The FileChannel in FileMessageSet is private so the only way to close it is to either call close() or delete() on the message set.&lt;br/&gt;
3. Since delete() first calls close() it is hard to say which code path was taken (you would get the same error message).&lt;br/&gt;
4. The only call to close() is LogSegment.close() the only call to that is in Log.close() so that&apos;s not it.&lt;br/&gt;
5. There are several calls to delete() but all are inside the lock except for deleteSegments.&lt;br/&gt;
6. deleteSegments should, intuitively, not delete the active segment since we force a segment roll if needed inside the lock inside markDeleteWhile()&lt;br/&gt;
7. But it is possible for deleteSegments to collide with truncateTo, but not sure how feasible this is.&lt;/p&gt;</comment>
                            <comment id="13561888" author="junrao" created="Thu, 24 Jan 2013 19:30:29 +0000"  >&lt;p&gt;Another piece of info is that the log segment seems to exist after the broker is shut down (it&apos;s loaded on broker restart). So, it didn&apos;t seem that the segment was deleted. After restart, the same problem shows up after the broker is running for about 1 day.&lt;/p&gt;</comment>
                            <comment id="13563952" author="junrao" created="Sun, 27 Jan 2013 23:10:38 +0000"  >&lt;p&gt;I have a theory of what&apos;s happening here. What we overlooked is that there is another possibility for us to get a closed channel, other than explicitly closing it. If a thread is in the middle of a read/write of a file channel and the thread is interrupted. The channel will be closed automatically. I guess the following is what has happened.&lt;/p&gt;

&lt;p&gt;The ExpiredRequestReaper thread is in the middle of expiring a FetchRequest and gets interrupted by ExpiredRequestReaper.forcePurge(). When the interruption occurs, the reaper thread could be reading the file channel (see stracktrace below). This will cause the file channel to be closed. All subsequent reads and writes on this file channel will fail due to ClosedChannelException.&lt;/p&gt;

&lt;p&gt;java.nio.channels.ClosedChannelException&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)&lt;br/&gt;
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:613)&lt;br/&gt;
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:83)&lt;br/&gt;
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:76)&lt;br/&gt;
        at kafka.log.LogSegment.read(LogSegment.scala:91)&lt;br/&gt;
        at kafka.log.Log.read(Log.scala:390)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:372)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:330)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:326)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:326)&lt;br/&gt;
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:528)&lt;br/&gt;
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:510)&lt;br/&gt;
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:222)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/p&gt;

&lt;p&gt;Attach a quick fix. The problem is that we shouldn&apos;t be using interrupt to communicate with the ExpiredRequestReaper thread since it has dangerous side effects. The patch basically uses a boolean flag to indicate that a full purge is needed and changes the ExpiredRequestReaper thread not to block for more than 500ms (so that it gets a chance to do the full purge).&lt;/p&gt;

&lt;p&gt;Not sure what&apos;s the best way to unit test this though.&lt;/p&gt;</comment>
                            <comment id="13564030" author="jkreps" created="Mon, 28 Jan 2013 04:20:59 +0000"  >&lt;p&gt;This is a pretty brilliant catch.&lt;/p&gt;

&lt;p&gt;That code should not be there. It looks like the interrupt from shutdown (which is okay maybe cause you are shutting down) somehow got pasted into elsewhere.&lt;/p&gt;</comment>
                            <comment id="13564393" author="junrao" created="Mon, 28 Jan 2013 16:37:59 +0000"  >&lt;p&gt;So, do you think the patch is ok or do you plan to provide a better patch?&lt;/p&gt;</comment>
                            <comment id="13564400" author="jkreps" created="Mon, 28 Jan 2013 16:44:55 +0000"  >&lt;p&gt;I&apos;m not sure. I think I need to rewind back to what forcePurge is trying to do. From what I see I think we are replaying an immediate interrupt with a 500ms check which may be fine, though obviously its higher latency. The purpose of forcePurge is to try to clean out dead memory?&lt;/p&gt;</comment>
                            <comment id="13564463" author="nehanarkhede" created="Mon, 28 Jan 2013 18:10:23 +0000"  >&lt;p&gt;Great catch! One question just to see if I understood the problem correctly. So the problem happens if the forcePurge() interrupts the expired request reaper while reading message set from the log. In that case, we should also see &quot;error when processing request..&quot; in the kafka log, because we catch all throwables in the readMessageSet() API. Do you see that ?&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt; The purpose of forcePurge is to try to clean out dead memory?&lt;/p&gt;

&lt;p&gt;Yes, this was added as part of fixing &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-664&quot; title=&quot;Kafka server threads die due to OOME during long running test&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-664&quot;&gt;&lt;del&gt;KAFKA-664&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think the patch looks good, it is fine to delay the full purge by a few ms.&lt;/p&gt;</comment>
                            <comment id="13564485" author="nehanarkhede" created="Mon, 28 Jan 2013 18:21:46 +0000"  >&lt;p&gt;The LogRecoveryTest throws the following new error messages now -&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-01-28 10:15:54,534&amp;#93;&lt;/span&gt; ERROR ExpiredRequestReaper-0 Error in long poll expiry thread:  (kafka.server.RequestPurgatory$ExpiredRequestReaper:102)&lt;br/&gt;
java.lang.InterruptedException&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1961)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2038)&lt;br/&gt;
	at java.util.concurrent.DelayQueue.poll(DelayQueue.java:209)&lt;br/&gt;
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.pollExpired(RequestPurgatory.scala:269)&lt;br/&gt;
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:221)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:680)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2013-01-28 10:15:54,534&amp;#93;&lt;/span&gt; ERROR ExpiredRequestReaper-0 Error in long poll expiry thread:  (kafka.server.RequestPurgatory$ExpiredRequestReaper:102)&lt;br/&gt;
java.lang.InterruptedException&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1961)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2038)&lt;br/&gt;
	at java.util.concurrent.DelayQueue.poll(DelayQueue.java:209)&lt;br/&gt;
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.pollExpired(RequestPurgatory.scala:269)&lt;br/&gt;
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:221)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:680)&lt;/p&gt;</comment>
                            <comment id="13564757" author="junrao" created="Mon, 28 Jan 2013 22:18:57 +0000"  >&lt;p&gt;Attach patch v2. Simplified the logic a bit. ExpiredRequestReaper now checks if we need to do a full purge directly (since it wakes up periodically) and RequestPurgatory.watch() just increments the request count. Also, reduced the wait from 500ms to 200ms.&lt;/p&gt;

&lt;p&gt;The new error in LogRecoveryTest happens when we interrupt the ExpiredRequestReaper thread during shutdown. So, this is fine. The test still fails transiently, but the problem has been there for some time.&lt;/p&gt;</comment>
                            <comment id="13564823" author="nehanarkhede" created="Mon, 28 Jan 2013 23:14:41 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13565524" author="junrao" created="Tue, 29 Jan 2013 16:57:25 +0000"  >&lt;p&gt;Just realized there are a couple of other corner cases that we need to handle. First, we shut down ExpiredRequestReaper by interrupting the thread. This of course, can close a filechannel, which can cause a KafkaStorageException during a subsequent log append. This means that we may unnecessarily do an unclean shutdown. Second, the replicaFetcher thread is shut down through interruption too. When the thread is interrupted, it can be in the middle of a log append. Similarly, this means another unnecessary unclean shutdown.&lt;/p&gt;

&lt;p&gt;We can remove interruption during shutdown and just rely on the isRunning flag. This is fine for the first case since ExpiredRequestReaper wakes up every 200ms. In the second case, this means that we may have to wait for the last outstanding fetch request to complete. This can potentially make shutdown longer. Not sure if this is a big concern.&lt;/p&gt;</comment>
                            <comment id="13566781" author="junrao" created="Wed, 30 Jan 2013 19:06:20 +0000"  >&lt;p&gt;Attach a follow up patch.&lt;/p&gt;

&lt;p&gt;1. Make ExpiredRequestReaper a non-daemon thread since we want to shut it down explicitly. Remove interrupt since the thread now wakes up periodically.&lt;/p&gt;

&lt;p&gt;2. AbstractFetcher: Make it non-interruptible. Just wait for the last fetch request to complete.&lt;/p&gt;

&lt;p&gt;3. ReplicaFetcher: handle KafkaStorageException properly.&lt;/p&gt;</comment>
                            <comment id="13566785" author="junrao" created="Wed, 30 Jan 2013 19:06:51 +0000"  >&lt;p&gt;reopen it to deal with followup issue.s&lt;/p&gt;</comment>
                            <comment id="13568927" author="nehanarkhede" created="Fri, 1 Feb 2013 18:15:15 +0000"  >&lt;p&gt;Thanks for the follow up patch, most of the changes are good. I have a few questions -&lt;/p&gt;

&lt;p&gt;1. ReplicaFetcherThread&lt;br/&gt;
Shouldn&apos;t we also catch Throwable and at least log an error saying why the fetcher thread died ? Otherwise, if there is some code bug, it will die anyway, but we will not know the reason why.&lt;/p&gt;

&lt;p&gt;2. AbstractFetcherThread&lt;br/&gt;
Why are we overriding isInterruptible in AbstractFetcherThread. I think you want ReplicaFetcherThread to be uninterruptible, but ConsumerThread can be interruptible, no ?&lt;/p&gt;

&lt;p&gt;3. RequestPurgatory&lt;br/&gt;
Shouldn&apos;t expiration thread be a daemon thread ? We don&apos;t really want the expiration thread to block the JVM from shutting down, do we ?&lt;/p&gt;</comment>
                            <comment id="13571164" author="junrao" created="Tue, 5 Feb 2013 09:08:01 +0000"  >&lt;p&gt;Attach followup patch v2.&lt;/p&gt;</comment>
                            <comment id="13571167" author="junrao" created="Tue, 5 Feb 2013 09:15:14 +0000"  >&lt;p&gt;1. Any unexpected exception in ReplicaFetcherThread will kill the thread and the error will be logged in ShutdownableThread.&lt;/p&gt;

&lt;p&gt;2. This is a good point and is fixed in the v2 patch.&lt;/p&gt;

&lt;p&gt;3. We are shutting down the expiration thread explicitly. If the thread can&apos;t be shut down, it indicates a bug. It&apos;s probably better to expose the bug than making this a daemon thread.&lt;/p&gt;

&lt;p&gt;So, the only change in patch v2 is to address #2.&lt;/p&gt;</comment>
                            <comment id="13571413" author="nehanarkhede" created="Tue, 5 Feb 2013 16:11:19 +0000"  >&lt;p&gt;1,3. Makes sense and the follow up patch v2 looks good. &lt;br/&gt;
+1&lt;/p&gt;</comment>
                            <comment id="13571427" author="jkreps" created="Tue, 5 Feb 2013 16:22:44 +0000"  >&lt;p&gt;Can we just remove the interrupt entirely instead of making it optional. Unless we want to write tests for both cases and maintain this functionality in perpetuity...seems like we should just stop using interrupt?&lt;/p&gt;</comment>
                            <comment id="13571449" author="junrao" created="Tue, 5 Feb 2013 16:46:55 +0000"  >&lt;p&gt;It&apos;s possible. However, some subclasses of ShutdownableThread such as ConsumerFetchThread does blocking writes to a queue. So, if the queue is full, not sure if we can shut down the consumer thread without sending interrupts.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12628760">KAFKA-719</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12566696" name="kafka-695.patch" size="2401" author="junrao" created="Sun, 27 Jan 2013 23:10:38 +0000"/>
                            <attachment id="12567187" name="kafka-695_followup.patch" size="8289" author="junrao" created="Wed, 30 Jan 2013 19:06:20 +0000"/>
                            <attachment id="12567980" name="kafka-695_followup_v2.patch" size="9906" author="junrao" created="Tue, 5 Feb 2013 09:08:01 +0000"/>
                            <attachment id="12566836" name="kafka-695_v2.patch" size="2674" author="junrao" created="Mon, 28 Jan 2013 22:18:57 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>303995</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 41 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i17hd3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>251782</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>