<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:41:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-17299] Kafka Streams consumer stops consumption</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-17299</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We are using kafka clients version (3.5.2). However, the bug looks to exist in current version as well from our code review.&#160;&lt;/p&gt;

&lt;p&gt;In one of our clusters, kafka streams consumption completely stops and doesn&apos;t recover even after restart of the consumer instance/pod. These are our findings/observations from our debugging.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;We have some deserialisation errors while the streams consuming.&#160;&lt;/li&gt;
	&lt;li&gt;We configured LogAndContinueExceptionHandler as exception handler to handle deserialisation failures.&lt;/li&gt;
	&lt;li&gt;Streams consumption doesn&apos;t stop on every deserialisation failure/error.&#160;&lt;/li&gt;
	&lt;li&gt;We are noticing the consumption hangs, only when the first message in the current batch is faulty and fails to deserialise.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We did a thorough inspection of the kafka clients code and debugged by patching with additional logs, we found the following lines of code from StreamTask.java seems to be the issue.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Original&lt;/b&gt; - Original code &lt;a href=&quot;https://github.com/apache/kafka/blob/3.5.2/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L750&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;StreamTask.java&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
            &lt;span class=&quot;code-comment&quot;&gt;// after processing &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; record, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; its partition queue&apos;s buffered size has been
&lt;/span&gt;            &lt;span class=&quot;code-comment&quot;&gt;// decreased to the threshold, we can then resume the consumption on &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; partition
&lt;/span&gt;            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (recordInfo.queue().size() == maxBufferedSize) {
                mainConsumer.resume(singleton(partition));
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;br/&gt;
&lt;b&gt;Patched&lt;/b&gt; - Issue resolved after this fix for us.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
            &lt;span class=&quot;code-comment&quot;&gt;// after processing &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; record, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; its partition queue&apos;s buffered size has been
&lt;/span&gt;            &lt;span class=&quot;code-comment&quot;&gt;// decreased to the threshold, we can then resume the consumption on &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; partition
&lt;/span&gt;            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (recordInfo.queue().size() &amp;lt;= maxBufferedSize) {
                mainConsumer.resume(singleton(partition));
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;We are resuming consumption only when queue size is exactly matching max buffered size. I think some record accounting has gone wrong especially when there is an issue with deserialising the first record in the batch.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13588268">KAFKA-17299</key>
            <summary>Kafka Streams consumer stops consumption</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lakshman">Laxman</assignee>
                                    <reporter username="lakshman">Laxman</reporter>
                        <labels>
                    </labels>
                <created>Thu, 8 Aug 2024 10:45:46 +0000</created>
                <updated>Wed, 27 Nov 2024 00:42:14 +0000</updated>
                            <resolved>Mon, 25 Nov 2024 21:03:54 +0000</resolved>
                                    <version>3.5.2</version>
                    <version>3.6.2</version>
                    <version>3.7.1</version>
                    <version>3.8.0</version>
                    <version>3.9.0</version>
                                    <fixVersion>3.7.2</fixVersion>
                    <fixVersion>3.8.2</fixVersion>
                    <fixVersion>3.9.1</fixVersion>
                    <fixVersion>4.0.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="17872077" author="JIRAUSER302322" created="Thu, 8 Aug 2024 16:52:45 +0000"  >&lt;p&gt;Hey. The change looks safe to me. &lt;/p&gt;

&lt;p&gt;However, it would be good to understand why the deserialization error causes the consumer to not resume. Have you been able to determine the problem in the record accounting?&lt;/p&gt;</comment>
                            <comment id="17872200" author="lakshman" created="Fri, 9 Aug 2024 06:39:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;Have you been able to determine the problem in the record accounting?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Before we patched, issue was reproducible consistently in our cluster. After the patch, application is able to make progress.&lt;/p&gt;

&lt;p&gt;However, I couldn&apos;t locate the accounting problem in kafka code.&lt;/p&gt;

&lt;p&gt;I&apos;m new to contributing to kafka codebase. So, can you please point me to the existing unit/integration tests for this/similar scenarios.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17875700" author="mjsax" created="Thu, 22 Aug 2024 01:39:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;We are noticing the consumption hangs, only when the first message in the current batch is faulty and fails to deserialise.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Looking into the code, this bug might be there for a long time, and it&apos;s actually a weird race condition &#8211; only if we did exceed the input partition buffer size, and if have a corrupted record at the exact point when we go below the buffer limit (and need to resume) we might skip resuming the partition accidentally. Of course, if there is longer &quot;burst&quot; of failing records, the probability to hit this bug is larger. It should not be too hard to write a unit test for this... (ie, inside StreamTaskTest.java).&lt;/p&gt;

&lt;p&gt;I am not sure though, if we should change the condition to `&amp;lt;=` as it would mean we call `resume()` for basically every input record... I believe we did use `==` to avoid unnecessary overhead as we are on the hot code path.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lianetm&quot; class=&quot;user-hover&quot; rel=&quot;lianetm&quot;&gt;lianetm&lt;/a&gt; do you know what the overhead of calling `resume()` is &#8211; if it&apos;s cheap it might be ok (I also don&apos;t want to complicate the code with pre-mature optimizations...)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17875814" author="JIRAUSER300183" created="Thu, 22 Aug 2024 11:15:40 +0000"  >&lt;p&gt;Hello all! Calling resume for a single partitions is indeed cheap, it only flips a boolean flag internally in the consumer (and that makes that it starts fetching again on the next poll). On a large collection of partitions it would affect differently though, given that it does iterate the set of partitions to resume, but not the case here as I see.&#160;&lt;/p&gt;

&lt;p&gt;By looking at the code the change makes sense to me, and it&apos;s actually playing safe. If for any reason the records consumed are not processed 1 by 1 as expected, we ensure that we still call resume. It may definitely not be needed in some cases, but harmless if the partition is not paused.&#160;&lt;/p&gt;</comment>
                            <comment id="17875972" author="mjsax" created="Thu, 22 Aug 2024 15:58:26 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lianetm&quot; class=&quot;user-hover&quot; rel=&quot;lianetm&quot;&gt;lianetm&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lakshman&quot; class=&quot;user-hover&quot; rel=&quot;lakshman&quot;&gt;lakshman&lt;/a&gt; You wanna pickup this ticket?&lt;/p&gt;</comment>
                            <comment id="17878867" author="lakshman" created="Tue, 3 Sep 2024 12:39:10 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lianetm&quot; class=&quot;user-hover&quot; rel=&quot;lianetm&quot;&gt;lianetm&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; : I can create a patch but I&apos;m not yet sure of the unit test though for this particular case. Need little help there.&lt;/p&gt;</comment>
                            <comment id="17881716" author="mjsax" created="Sat, 14 Sep 2024 00:00:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lakshman&quot; class=&quot;user-hover&quot; rel=&quot;lakshman&quot;&gt;lakshman&lt;/a&gt; &#8211; feel free to open a PR w/o unit test, and we can take it from there. W/o seeing the code changes, it&apos;s hard to advice.&lt;/p&gt;</comment>
                            <comment id="17882335" author="lakshman" created="Tue, 17 Sep 2024 09:42:30 +0000"  >&lt;p&gt;Attached the patch. Please review.&lt;/p&gt;</comment>
                            <comment id="17899362" author="lakshman" created="Tue, 19 Nov 2024 06:31:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lianetm&quot; class=&quot;user-hover&quot; rel=&quot;lianetm&quot;&gt;lianetm&lt;/a&gt; : Please review and merge this. Will work on this if you have any comments.&lt;/p&gt;</comment>
                            <comment id="17899907" author="mjsax" created="Thu, 21 Nov 2024 01:16:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lakshman&quot; class=&quot;user-hover&quot; rel=&quot;lakshman&quot;&gt;lakshman&lt;/a&gt; &#8211; we don&apos;t do patches via Jira tickets. Please open a &quot;pull request&quot; against &lt;a href=&quot;https://github.com/apache/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka&lt;/a&gt; `trunk` branch, and we can review the code there.&lt;/p&gt;</comment>
                            <comment id="17899909" author="mjsax" created="Thu, 21 Nov 2024 01:20:20 +0000"  >&lt;p&gt;Btw: I would expect that `StreamTaskTest#shouldPauseAndResumeBasedOnBufferedRecords` need to be updates &#8211; I expect you patch to break to test, due to an &quot;off by one error&quot;. Please include an update to this test, too, to reflect the new &quot;resuming&quot; condition.&lt;/p&gt;</comment>
                            <comment id="17900144" author="lakshman" created="Thu, 21 Nov 2024 18:12:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; : Added the PR as suggested.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/pull/17899&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/17899&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I also ran the unit tests available under `StreamTaskTest`. None of them are failing with/without the fix.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="13071597" name="KAFKA-17299.patch" size="888" author="lakshman" created="Tue, 17 Sep 2024 09:41:41 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310250" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10431"><![CDATA[Important]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            50 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1qrg8:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>