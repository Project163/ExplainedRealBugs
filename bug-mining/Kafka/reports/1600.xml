<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:02:50 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-5431] LogCleaner stopped due to org.apache.kafka.common.errors.CorruptRecordException</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-5431</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Hey all,&lt;br/&gt;
i have a strange problem with our uat cluster of 3 kafka brokers.&lt;/p&gt;

&lt;p&gt;the __consumer_offsets topic was replicated to two instances and our disks ran full due to a wrong configuration of the log cleaner. We fixed the configuration and updated from 0.10.1.1 to 0.10.2.1 .&lt;/p&gt;

&lt;p&gt;Today i increased the replication of the __consumer_offsets topic to 3 and triggered replication to the third cluster via kafka-reassign-partitions.sh. &lt;/p&gt;

&lt;p&gt;That went well but i get many errors like&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2017-06-12 09:59:50,342] ERROR Found invalid messages during fetch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition [__consumer_offsets,18] offset 0 error Record size is less than the minimum record overhead (14) (kafka.server.ReplicaFetcherThread)
[2017-06-12 09:59:50,342] ERROR Found invalid messages during fetch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition [__consumer_offsets,24] offset 0 error Record size is less than the minimum record overhead (14) (kafka.server.ReplicaFetcherThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Which i think are due to the full disk event.&lt;/p&gt;

&lt;p&gt;The log cleaner threads died on these wrong messages:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2017-06-12 09:59:50,722] ERROR [kafka-log-cleaner-thread-0], Error due to  (kafka.log.LogCleaner)
org.apache.kafka.common.errors.CorruptRecordException: Record size is less than the minimum record overhead (14)
[2017-06-12 09:59:50,722] INFO [kafka-log-cleaner-thread-0], Stopped  (kafka.log.LogCleaner)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Looking at the file is see that some are truncated and some are jsut empty:&lt;br/&gt;
$ ls -lsh 00000000000000594653.log&lt;br/&gt;
0 &lt;del&gt;rw-r&lt;/del&gt;&lt;del&gt;r&lt;/del&gt;- 1 user user 100M Jun 12 11:00 00000000000000594653.log&lt;/p&gt;

&lt;p&gt;Sadly i do not have the logs any more from the disk full event itsself.&lt;/p&gt;

&lt;p&gt;I have three questions:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;What is the best way to clean this up? Deleting the old log files and restarting the brokers?&lt;/li&gt;
	&lt;li&gt;Why did kafka not handle the disk full event well? Is this only affecting the cleanup or may we also loose data?&lt;/li&gt;
	&lt;li&gt;Is this maybe caused by the combination of upgrade and disk full?&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;And last but not least: Keep up the good work. Kafka is really performing well while being easy to administer and has good documentation!&lt;/p&gt;</description>
                <environment></environment>
        <key id="13079175">KAFKA-5431</key>
            <summary>LogCleaner stopped due to org.apache.kafka.common.errors.CorruptRecordException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="huxi_2b">huxihx</assignee>
                                    <reporter username="crietz">Carsten Rietz</reporter>
                        <labels>
                            <label>reliability</label>
                    </labels>
                <created>Mon, 12 Jun 2017 10:25:19 +0000</created>
                <updated>Sun, 25 Jul 2021 17:33:06 +0000</updated>
                            <resolved>Fri, 21 Jul 2017 04:49:57 +0000</resolved>
                                    <version>0.10.2.1</version>
                                    <fixVersion>0.11.0.1</fixVersion>
                    <fixVersion>1.0.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="16046480" author="ijuma" created="Mon, 12 Jun 2017 12:04:35 +0000"  >&lt;p&gt;`LogSegment.recover` discards invalid bytes from the end of the segment. Not sure why that didn&apos;t happen here. cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16046650" author="junrao" created="Mon, 12 Jun 2017 14:56:03 +0000"  >&lt;p&gt;Interesting. First, in general, if there is an IOException during writing to the log, the broker will shut down immediately. Second, if the cleaner hits an IOException, currently, we just abort the current cleaning job. The existing log should be intact. The &quot;invalid message&quot; from the fetch follower seems a bit weird. That suggests that the log segment at offset 0 is corrupted somehow. Could you use the DumpLogSegment tool (&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/System+Tools#SystemTools-DumpLogSegment&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/System+Tools#SystemTools-DumpLogSegment&lt;/a&gt;) on that segment in the leader to see if there is any log corruption?&lt;/p&gt;</comment>
                            <comment id="16047637" author="huxi_2b" created="Tue, 13 Jun 2017 09:25:11 +0000"  >&lt;p&gt;Could you run commands below to see whether there exists a corrupt record for `__consumer_offsets` topic?&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;bin/kafka-simple-consumer-shell.sh --topic __consumer_offsets --partition 18 --broker-list *** --formatter &quot;kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter&quot;
bin/kafka-simple-consumer-shell.sh --topic __consumer_offsets --partition 24 --broker-list *** --formatter &quot;kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16047871" author="crietz" created="Tue, 13 Jun 2017 13:15:30 +0000"  >&lt;p&gt;Thanks for the fast reponse. We did some more digging today and it seems related to log.preallocate=true.&lt;br/&gt;
The log files which tip over the LogCleaner are not compacted even if they were rolled. Here is a example with 1706631 being faulty.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[user@host ~]$ ls -lsh data/__consumer_offsets-26/*.log
328K -rw-r--r-- 1 jboss jboss 328K Jun 13 09:29 00000000000001701717.log
332K -rw-r--r-- 1 jboss jboss 330K Jun 13 09:29 00000000000001704168.log
 32K -rw-r--r-- 1 jboss jboss 100M Jun 13 09:29 00000000000001706631.log
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the kafka log we see the normal&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2017-06-13 09:29:09,345] INFO Rolled &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log segment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&apos;__consumer_offsets-26&apos;&lt;/span&gt; in 1 ms. (kafka.log.Log)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As i understand the code this should not be possible &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;We worked around for now by setting log.preallocate=false, deleting all __consumer_offsets on one broker and restarting it. Now eerything seems to run stable.&lt;/p&gt;

&lt;p&gt;I will try to find another occurrence on our test environment and check for corrupted records.&lt;/p&gt;</comment>
                            <comment id="16049156" author="crietz" created="Wed, 14 Jun 2017 13:10:20 +0000"  >&lt;p&gt;I still have no idea what exactly to do to trigger this behaviour. Maybe its some strange behaviour of our producers. I have been able to find one instance on our dev system with a low roll interval, a one broker cluster and some restarts.&lt;/p&gt;


&lt;p&gt;Here is the corrupt log file:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[user@host __consumer_offsets-12# ll -hs | head
total 2.7M
   0 -rw-r--r-- 1 jboss jboss    0 Jun 14 13:31 00000000000000000000.index
4.0K -rw-r--r-- 1 jboss jboss 100M Jun 14 12:30 00000000000000000000.log
4.0K -rw-r--r-- 1 jboss jboss   12 Jun 14 13:31 00000000000000000000.timeindex
   0 -rw-r--r-- 1 jboss jboss    0 Jun 14 13:31 00000000000000000001.index
4.0K -rw-r--r-- 1 jboss jboss  122 Jun 14 12:30 00000000000000000001.log
4.0K -rw-r--r-- 1 jboss jboss   12 Jun 14 13:31 00000000000000000001.timeindex
   0 -rw-r--r-- 1 jboss jboss    0 Jun 14 13:31 00000000000000000002.index
4.0K -rw-r--r-- 1 jboss jboss  122 Jun 14 12:30 00000000000000000002.log
4.0K -rw-r--r-- 1 jboss jboss   12 Jun 14 13:31 00000000000000000002.timeindex
[user@host __consumer_offsets-10]# stat 00000000000000000000.log
  File: `00000000000000000000.log&apos;
  Size: 104857600 	Blocks: 8          IO Block: 4096   regular file
Device: fd00h/64768d	Inode: 4718956     Links: 1
Access: (0644/-rw-r--r--)  Uid: (  494/   jboss)   Gid: (  488/   jboss)
Access: 2017-06-14 12:31:03.585079439 +0100
Modify: 2017-06-14 12:30:42.000000000 +0100
Change: 2017-06-14 12:30:48.520886205 +0100
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[user@host __consumer_offsets-10]# kafka-run-&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;sh kafka.tools.DumpLogSegments --files 00000000000000000000.log
Dumping 00000000000000000000.log
Starting offset: 0
offset: 0 position: 0 CreateTime: 1497439842157 isvalid: &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; payloadsize: 257 magic: 1 compresscodec: NONE crc: 1102674092 keysize: 26
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.kafka.common.errors.CorruptRecordException: Record size is smaller than minimum record overhead (14).
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;hexdumping the file also shows that its still a sparse file with trailing zero content&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[user@host __consumer_offsets-10]# hexdump -C 00000000000000000000.log
00000000  00 00 00 00 00 00 00 00  00 00 01 31 41 b9 78 ac  |...........1A.x.|
00000010  01 00 00 00 01 5c a6 5e  af 6d 00 00 00 1a 00 02  |.....\.^.m......|
00000020  00 16 63 6f 6e 73 6f 6c  65 2d 63 6f 6e 73 75 6d  |..console-consum|
00000030  65 72 2d 35 39 37 37 33  00 00 01 01 00 01 00 08  |er-59773........|
00000040  63 6f 6e 73 75 6d 65 72  00 00 00 01 00 05 72 61  |consumer......ra|
00000050  6e 67 65 00 2f 63 6f 6e  73 75 6d 65 72 2d 31 2d  |nge./consumer-1-|
00000060  39 39 36 33 62 31 36 36  2d 39 32 39 39 2d 34 34  |9963b166-9299-44|
00000070  30 66 2d 38 64 39 37 2d  35 62 34 38 31 33 64 35  |0f-8d97-5b4813d5|
00000080  63 39 38 34 00 00 00 01  00 2f 63 6f 6e 73 75 6d  |c984...../consum|
00000090  65 72 2d 31 2d 39 39 36  33 62 31 36 36 2d 39 32  |er-1-9963b166-92|
000000a0  39 39 2d 34 34 30 66 2d  38 64 39 37 2d 35 62 34  |99-440f-8d97-5b4|
000000b0  38 31 33 64 35 63 39 38  34 00 0a 63 6f 6e 73 75  |813d5c984..consu|
000000c0  6d 65 72 2d 31 00 0e 2f  31 30 2e 32 34 38 2e 34  |mer-1../10.248.4|
000000d0  33 2e 32 33 33 00 04 93  e0 00 00 27 10 00 00 00  |3.233......&apos;....|
000000e0  28 00 00 00 00 00 01 00  1c 74 63 2e 69 6e 2e 64  |(........tc.in.d|
000000f0  65 76 2e 73 74 61 74 69  73 74 69 63 73 2e 67 72  |ev.statistics.gr|
00000100  61 70 68 69 74 00 00 00  00 00 00 00 30 00 00 00  |aphit.......0...|
00000110  00 00 01 00 1c 74 63 2e  69 6e 2e 64 65 76 2e 73  |.....tc.in.dev.s|
00000120  74 61 74 69 73 74 69 63  73 2e 67 72 61 70 68 69  |tatistics.graphi|
00000130  74 00 00 00 01 00 00 00  00 00 00 00 00 00 00 00  |t...............|
00000140  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
*
06400000
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are running kafka_2.12-0.10.2.1 on jdk1.8.0_121 on Red Hat Enterprise Linux Server release 6.9.&lt;br/&gt;
Relevant parameters:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;log.segment.bytes=10485760
log.retention.check.interval.ms=1000
log.roll.ms=10000
log.preallocate=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
log.retention.bytes=10485760
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And finally the log files. I removed all __consumer_offsets and restarted the kafka broker due to full disk before.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2017-06-14 12:28:46,198] INFO Started Kafka Graphite metrics reporter with polling period 1 seconds (com.criteo.kafka.KafkaGraphiteMetricsReporter)
...
[2017-06-14 12:28:51,170] INFO [Group Metadata Manager on Broker 1]: Loading offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.GroupMetadataManager)
[2017-06-14 12:28:51,189] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets from __consumer_offsets-12 in 19 milliseconds. (kafka.coordinator.GroupMetadataManager)
[2017-06-14 12:30:47,164] INFO Rolled &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log segment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&apos;__consumer_offsets-12&apos;&lt;/span&gt; in 1 ms. (kafka.log.Log)
[2017-06-14 12:30:48,431] INFO Cleaner 0: Beginning cleaning of log __consumer_offsets-12. (kafka.log.LogCleaner)
[2017-06-14 12:30:48,432] INFO Cleaner 0: Building offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; __consumer_offsets-12... (kafka.log.LogCleaner)
[2017-06-14 12:30:48,470] INFO Cleaner 0: Building offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; log __consumer_offsets-12 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 1 segments in offset range [0, 1). (kafka.log.LogCleaner)
[2017-06-14 12:30:48,499] INFO Cleaner 0: Offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; log __consumer_offsets-12 complete. (kafka.log.LogCleaner)
[2017-06-14 12:30:48,503] INFO Cleaner 0: Cleaning log __consumer_offsets-12 (cleaning prior to Wed Jun 14 12:30:42 BST 2017, discarding tombstones prior to Thu Jan 01 01:00:00 GMT 1970)... (kafka.log.LogCleaner)
[2017-06-14 12:30:48,507] INFO Cleaner 0: Cleaning segment 0 in log __consumer_offsets-12 (largest timestamp Wed Jun 14 12:30:42 BST 2017) into 0, retaining deletes. (kafka.log.LogCleaner)
[2017-06-14 12:30:48,521] INFO Cleaner 0: Swapping in cleaned segment 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; segment(s) 0 in log __consumer_offsets-12. (kafka.log.LogCleaner)
        Log cleaner thread 0 cleaned log __consumer_offsets-12 (dirty section = [0, 0])
[2017-06-14 12:30:52,161] INFO Rolled &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log segment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&apos;__consumer_offsets-12&apos;&lt;/span&gt; in 1 ms. (kafka.log.Log)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here the log file seems to have been rolled and not truncated. What tips over the logcleaner a bit later&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2017-06-14 12:31:03,564] INFO Cleaner 0: Beginning cleaning of log __consumer_offsets-12. (kafka.log.LogCleaner)
[2017-06-14 12:31:03,564] INFO Cleaner 0: Building offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; __consumer_offsets-12... (kafka.log.LogCleaner)
[2017-06-14 12:31:03,583] INFO Cleaner 0: Building offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; log __consumer_offsets-12 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 3 segments in offset range [1, 4). (kafka.log.LogCleaner)
[2017-06-14 12:31:03,584] INFO Cleaner 0: Offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; log __consumer_offsets-12 complete. (kafka.log.LogCleaner)
[2017-06-14 12:31:03,584] INFO Cleaner 0: Cleaning log __consumer_offsets-12 (cleaning prior to Wed Jun 14 12:31:02 BST 2017, discarding tombstones prior to Tue Jun 13 12:30:42 BST 2017)... (kafka.log.LogCleaner)
[2017-06-14 12:31:03,585] INFO Cleaner 0: Cleaning segment 0 in log __consumer_offsets-12 (largest timestamp Wed Jun 14 12:30:42 BST 2017) into 0, retaining deletes. (kafka.log.LogCleaner)
[2017-06-14 12:31:03,589] ERROR [kafka-log-cleaner-thread-0], Error due to  (kafka.log.LogCleaner)
org.apache.kafka.common.errors.CorruptRecordException: Record size is less than the minimum record overhead (14)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;edit: Its likely that i cannot have another look before monday. I hope someone has a brigth idea. Deactivating the preallocation works fine as a workaround.&lt;/p&gt;</comment>
                            <comment id="16049229" author="junrao" created="Wed, 14 Jun 2017 14:27:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=crietz&quot; class=&quot;user-hover&quot; rel=&quot;crietz&quot;&gt;crietz&lt;/a&gt;, thanks for the info. I guess the preallocated size is 100MB? It&apos;s not clear why 00000000000000000000.log has the preallocated size. Normally, we shrink the file size to its actual size after rolling. In fact, the size for other segments like 00000000000000000001.log does look normal. It&apos;s also interesting that each segment has only 1 message in it. Did you set a really small log segment size? &lt;/p&gt;</comment>
                            <comment id="16053936" author="crietz" created="Mon, 19 Jun 2017 12:55:48 +0000"  >&lt;p&gt;Yes the preallocatiion size is 100MB. I set the roll time to 1s to provoke many log rolls to more easily trigger this problem. Therefore only one offset makes it into a new log. The segment size is normal (log.segment.bytes=10485760).&lt;/p&gt;</comment>
                            <comment id="16054686" author="junrao" created="Mon, 19 Jun 2017 20:30:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=crietz&quot; class=&quot;user-hover&quot; rel=&quot;crietz&quot;&gt;crietz&lt;/a&gt;, in Log.roll(), we call LogSegment.trim() to reset the size of the log file to the actual size, which eventually calls FileRecords.truncateTo(). If this is reproducible, could you add some instrumentation in FileRecords.truncateTo() to see if the logic is actually called during log rolling?&lt;/p&gt;</comment>
                            <comment id="16085357" author="huxi_2b" created="Thu, 13 Jul 2017 08:27:55 +0000"  >&lt;p&gt;Seems this only happens when preallocate is enabled and topic is configured with &apos;compact&apos;. &lt;/p&gt;

&lt;p&gt;I think only one tiny code change could solve both this issue and &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5582&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;KAFKA-5582&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16085380" author="githubbot" created="Thu, 13 Jul 2017 08:38:00 +0000"  >&lt;p&gt;GitHub user huxihx opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/3525&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3525&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5431&quot; title=&quot;LogCleaner stopped due to org.apache.kafka.common.errors.CorruptRecordException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5431&quot;&gt;&lt;del&gt;KAFKA-5431&lt;/del&gt;&lt;/a&gt;: cleanSegments should not set length for cleanable segment files&lt;/p&gt;

&lt;p&gt;    For a compacted topic with preallocate enabled, during log cleaning, LogCleaner.cleanSegments does not have to pre-allocate the underlying file size since we only want to store the cleaned data in the file.&lt;/p&gt;

&lt;p&gt;    It&apos;s believed that this fix should also solve &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5582&quot; title=&quot;Log compaction with preallocation enabled does not trim segments&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5582&quot;&gt;&lt;del&gt;KAFKA-5582&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/huxihx/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/huxihx/kafka&lt;/a&gt; log_compact_test&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/3525.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3525.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #3525&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit e14436a2abb25c5b324efba5e431e5e1afb6e05a&lt;br/&gt;
Author: huxihx &amp;lt;huxi_2b@hotmail.com&amp;gt;&lt;br/&gt;
Date:   2017-07-13T08:28:50Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5431&quot; title=&quot;LogCleaner stopped due to org.apache.kafka.common.errors.CorruptRecordException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5431&quot;&gt;&lt;del&gt;KAFKA-5431&lt;/del&gt;&lt;/a&gt;: LogCleaner stopped due to org.apache.kafka.common.errors.CorruptRecordException&lt;/p&gt;

&lt;p&gt;    For a compacted topic with preallocate enabled, during log cleaning, LogCleaner.cleanSegments does not have to pre-allocate the underlying file size since we only want to store the cleaned data in the file.&lt;/p&gt;

&lt;p&gt;    It&apos;s believed that this fix should also solve &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5582&quot; title=&quot;Log compaction with preallocation enabled does not trim segments&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5582&quot;&gt;&lt;del&gt;KAFKA-5582&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16095774" author="junrao" created="Fri, 21 Jul 2017 04:49:57 +0000"  >&lt;p&gt;Issue resolved by pull request 3525&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/3525&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3525&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16095775" author="githubbot" created="Fri, 21 Jul 2017 04:50:35 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/3525&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/3525&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16722728" author="mswathi" created="Mon, 17 Dec 2018 06:54:58 +0000"  >&lt;p&gt;hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=huxi_2b&quot; class=&quot;user-hover&quot; rel=&quot;huxi_2b&quot;&gt;huxi_2b&lt;/a&gt;, we are currently on .11.0.0 and are seeing this issue with the default value of log.preallocate which is false. We have a large number of segement files in the __consumer_offsets that are not getting compacted.&#160;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-12-12 00:11:04,597&amp;#93;&lt;/span&gt; INFO Cleaner 0: Building offset map for log __consumer_offsets-45 for 124 segments in offset range [16446991, 85239736). (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-12-12 00:11:04,831&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-log-cleaner-thread-0&amp;#93;&lt;/span&gt;: Error due to (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;org.apache.kafka.common.errors.CorruptRecordException: Record size is less than the minimum record overhead (14)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-12-12 00:11:04,837&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-log-cleaner-thread-0&amp;#93;&lt;/span&gt;: Stopped (kafka.log.LogCleaner)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;We previously deleted the segment files and restarted our consumers. But this didn&apos;t help and we are running towards a disk full issue. Can you please help.&lt;/p&gt;</comment>
                            <comment id="17386931" author="rana6627" created="Sun, 25 Jul 2021 17:32:00 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mswathi&quot; class=&quot;user-hover&quot; rel=&quot;mswathi&quot;&gt;mswathi&lt;/a&gt; , please update the status of above issue.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13086363">KAFKA-5582</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 16 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3g5tz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>