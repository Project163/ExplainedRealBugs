<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:06:59 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6554] Broker doesn&apos;t reject Produce request with inconsistent state</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6554</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Produce messages of type v3 have offset deltas in each record along with a LastOffsetDelta for the topic/partition set. In investigating an issue with missing offsets, I found a bug in a producer library where it would send multiple records, but leave LastOffsetDelta at 0. This causes various problems including holes in the offsets fetched by the consumer.&#160;&lt;/p&gt;

&lt;p&gt;As lastOffsetDelta can be computed by looking at the records, it seems like the broker should at least validate the LastOffsetDelta field against the contained records to stop this bad data getting in.&lt;/p&gt;

&lt;p&gt;I&apos;ve attached a decode v3 produce message that was causing the problems, and was accepted by the broker.&lt;/p&gt;

&lt;p&gt;Here&apos;s a link to the issue in the&#160;kafka library we were using which has more context if you need it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shopify/sarama/issues/1032&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/Shopify/sarama/issues/1032&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13137942">KAFKA-6554</key>
            <summary>Broker doesn&apos;t reject Produce request with inconsistent state</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="superfell">Simon Fell</reporter>
                        <labels>
                    </labels>
                <created>Mon, 12 Feb 2018 19:29:10 +0000</created>
                <updated>Tue, 20 Feb 2018 17:03:58 +0000</updated>
                            <resolved>Tue, 20 Feb 2018 17:03:58 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>producer </component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16368484" author="hachikuji" created="Sun, 18 Feb 2018 09:08:19 +0000"  >&lt;p&gt;Thanks for the report. It does look like we&apos;re missing some validation here. I will submit a patch.&lt;/p&gt;</comment>
                            <comment id="16368487" author="githubbot" created="Sun, 18 Feb 2018 09:11:28 +0000"  >&lt;p&gt;hachikuji opened a new pull request #4585: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6554&quot; title=&quot;Broker doesn&amp;#39;t reject Produce request with inconsistent state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6554&quot;&gt;&lt;del&gt;KAFKA-6554&lt;/del&gt;&lt;/a&gt;; Missing lastOffsetDelta validation before log append&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4585&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4585&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Add validation checks that the offset range is valid and aligned with the batch count prior to appending to the log. I&apos;ve added several unit tests to verify the various invalid cases.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16369845" author="githubbot" created="Tue, 20 Feb 2018 09:32:13 +0000"  >&lt;p&gt;rajinisivaram closed pull request #4585: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6554&quot; title=&quot;Broker doesn&amp;#39;t reject Produce request with inconsistent state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6554&quot;&gt;&lt;del&gt;KAFKA-6554&lt;/del&gt;&lt;/a&gt;; Missing lastOffsetDelta validation before log append&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4585&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4585&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java b/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java&lt;br/&gt;
index ff8d3b990ba..71e668e45da 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java&lt;br/&gt;
@@ -106,7 +106,7 @@&lt;br/&gt;
     static final int CRC_LENGTH = 4;&lt;br/&gt;
     static final int ATTRIBUTES_OFFSET = CRC_OFFSET + CRC_LENGTH;&lt;br/&gt;
     static final int ATTRIBUTE_LENGTH = 2;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static final int LAST_OFFSET_DELTA_OFFSET = ATTRIBUTES_OFFSET + ATTRIBUTE_LENGTH;&lt;br/&gt;
+    public static final int LAST_OFFSET_DELTA_OFFSET = ATTRIBUTES_OFFSET + ATTRIBUTE_LENGTH;&lt;br/&gt;
     static final int LAST_OFFSET_DELTA_LENGTH = 4;&lt;br/&gt;
     static final int FIRST_TIMESTAMP_OFFSET = LAST_OFFSET_DELTA_OFFSET + LAST_OFFSET_DELTA_LENGTH;&lt;br/&gt;
     static final int FIRST_TIMESTAMP_LENGTH = 8;&lt;br/&gt;
@@ -118,7 +118,7 @@&lt;br/&gt;
     static final int PRODUCER_EPOCH_LENGTH = 2;&lt;br/&gt;
     static final int BASE_SEQUENCE_OFFSET = PRODUCER_EPOCH_OFFSET + PRODUCER_EPOCH_LENGTH;&lt;br/&gt;
     static final int BASE_SEQUENCE_LENGTH = 4;&lt;/li&gt;
	&lt;li&gt;static final int RECORDS_COUNT_OFFSET = BASE_SEQUENCE_OFFSET + BASE_SEQUENCE_LENGTH;&lt;br/&gt;
+    public static final int RECORDS_COUNT_OFFSET = BASE_SEQUENCE_OFFSET + BASE_SEQUENCE_LENGTH;&lt;br/&gt;
     static final int RECORDS_COUNT_LENGTH = 4;&lt;br/&gt;
     static final int RECORDS_OFFSET = RECORDS_COUNT_OFFSET + RECORDS_COUNT_LENGTH;&lt;br/&gt;
     public static final int RECORD_BATCH_OVERHEAD = RECORDS_OFFSET;&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogValidator.scala b/core/src/main/scala/kafka/log/LogValidator.scala&lt;br/&gt;
index 15750e9cd06..1beb2bdda37 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/LogValidator.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogValidator.scala&lt;br/&gt;
@@ -74,15 +74,27 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; object LogValidator extends Logging {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def validateBatch(batch: RecordBatch, isFromClient: Boolean, toMagic: Byte): Unit = {&lt;br/&gt;
     if (isFromClient) {&lt;br/&gt;
+      if (batch.magic &amp;gt;= RecordBatch.MAGIC_VALUE_V2) {&lt;br/&gt;
+        val countFromOffsets = batch.lastOffset - batch.baseOffset + 1&lt;br/&gt;
+        if (countFromOffsets &amp;lt;= 0)&lt;br/&gt;
+          throw new InvalidRecordException(s&quot;Batch has an invalid offset range: &lt;span class=&quot;error&quot;&gt;&amp;#91;${batch.baseOffset}, ${batch.lastOffset}&amp;#93;&lt;/span&gt;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+        // v2 and above messages always have a non-null count&lt;br/&gt;
+        val count = batch.countOrNull&lt;br/&gt;
+        if (count &amp;lt;= 0)&lt;br/&gt;
+          throw new InvalidRecordException(s&quot;Invalid reported count for record batch: $count&quot;)&lt;br/&gt;
+&lt;br/&gt;
+        if (countFromOffsets != batch.countOrNull)&lt;br/&gt;
+          throw new InvalidRecordException(s&quot;Inconsistent batch offset range &lt;span class=&quot;error&quot;&gt;&amp;#91;${batch.baseOffset}, ${batch.lastOffset}&amp;#93;&lt;/span&gt; &quot; +&lt;br/&gt;
+            s&quot;and count of records $count&quot;)&lt;br/&gt;
+      }&lt;br/&gt;
+&lt;br/&gt;
       if (batch.hasProducerId &amp;amp;&amp;amp; batch.baseSequence &amp;lt; 0)&lt;br/&gt;
         throw new InvalidRecordException(s&quot;Invalid sequence number ${batch.baseSequence} in record batch &quot; +&lt;br/&gt;
           s&quot;with producerId ${batch.producerId}&quot;)&lt;/p&gt;

&lt;p&gt;       if (batch.isControlBatch)&lt;br/&gt;
         throw new InvalidRecordException(&quot;Clients are not allowed to write control records&quot;)&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (Option(batch.countOrNull).contains(0))&lt;/li&gt;
	&lt;li&gt;throw new InvalidRecordException(&quot;Record batches must contain at least one record&quot;)&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (batch.isTransactional &amp;amp;&amp;amp; toMagic &amp;lt; RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala b/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala&lt;br/&gt;
index 131152af430..04e89528b12 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala&lt;br/&gt;
@@ -26,6 +26,7 @@ import org.apache.kafka.common.utils.Time&lt;br/&gt;
 import org.apache.kafka.test.TestUtils&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.Test&lt;br/&gt;
+import org.scalatest.Assertions.intercept&lt;/p&gt;

&lt;p&gt; import scala.collection.JavaConverters._&lt;/p&gt;

&lt;p&gt;@@ -147,6 +148,50 @@ class LogValidatorTest &lt;/p&gt;
{
       compressed = true)
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testInvalidOffsetRangeAndRecordCount(): Unit = &lt;/p&gt;
{
+    // The batch to be written contains 3 records, so the correct lastOffsetDelta is 2
+    validateRecordBatchWithCountOverrides(lastOffsetDelta = 2, count = 3)
+
+    // Count and offset range are inconsistent or invalid
+    assertInvalidBatchCountOverrides(lastOffsetDelta = 0, count = 3)
+    assertInvalidBatchCountOverrides(lastOffsetDelta = 15, count = 3)
+    assertInvalidBatchCountOverrides(lastOffsetDelta = -3, count = 3)
+    assertInvalidBatchCountOverrides(lastOffsetDelta = 2, count = -3)
+    assertInvalidBatchCountOverrides(lastOffsetDelta = 2, count = 6)
+    assertInvalidBatchCountOverrides(lastOffsetDelta = 2, count = 0)
+    assertInvalidBatchCountOverrides(lastOffsetDelta = -3, count = -2)
+
+    // Count and offset range are consistent, but do not match the actual number of records
+    assertInvalidBatchCountOverrides(lastOffsetDelta = 5, count = 6)
+    assertInvalidBatchCountOverrides(lastOffsetDelta = 1, count = 2)
+  }
&lt;p&gt;+&lt;br/&gt;
+  private def assertInvalidBatchCountOverrides(lastOffsetDelta: Int, count: Int): Unit = {&lt;br/&gt;
+    intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;InvalidRecordException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
+      validateRecordBatchWithCountOverrides(lastOffsetDelta, count)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def validateRecordBatchWithCountOverrides(lastOffsetDelta: Int, count: Int) &lt;/p&gt;
{
+    val records = createRecords(magicValue = RecordBatch.MAGIC_VALUE_V2, timestamp = 1234L, codec = CompressionType.NONE)
+    records.buffer.putInt(DefaultRecordBatch.RECORDS_COUNT_OFFSET, count)
+    records.buffer.putInt(DefaultRecordBatch.LAST_OFFSET_DELTA_OFFSET, lastOffsetDelta)
+    LogValidator.validateMessagesAndAssignOffsets(
+      records,
+      offsetCounter = new LongRef(0),
+      time = time,
+      now = time.milliseconds(),
+      sourceCodec = DefaultCompressionCodec,
+      targetCodec = DefaultCompressionCodec,
+      compactedTopic = false,
+      magic = RecordBatch.MAGIC_VALUE_V2,
+      timestampType = TimestampType.LOG_APPEND_TIME,
+      timestampDiffMaxMs = 1000L,
+      partitionLeaderEpoch = RecordBatch.NO_PARTITION_LEADER_EPOCH,
+      isFromClient = true)
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testLogAppendTimeWithoutRecompressionV2() {&lt;br/&gt;
     checkLogAppendTimeWithoutRecompression(RecordBatch.MAGIC_VALUE_V2)&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12910273" name="produce_v3.txt" size="1156" author="superfell" created="Mon, 12 Feb 2018 19:27:18 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 39 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3q32n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>