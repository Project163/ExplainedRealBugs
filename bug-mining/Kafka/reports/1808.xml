<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:06:28 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6489] Fetcher.retrieveOffsetsByTimes() should add all the topics to the metadata refresh topics set.</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6489</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Currently if users call KafkaConsumer.offsetsForTimes() with a large set of partitions. The consumer will add one topic at a time for the metadata refresh. We should add all the topics to the metadata topics and just do one metadata refresh.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13133922">KAFKA-6489</key>
            <summary>Fetcher.retrieveOffsetsByTimes() should add all the topics to the metadata refresh topics set.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="becket_qin">Jiangjie Qin</assignee>
                                    <reporter username="becket_qin">Jiangjie Qin</reporter>
                        <labels>
                    </labels>
                <created>Fri, 26 Jan 2018 04:32:10 +0000</created>
                <updated>Thu, 1 Feb 2018 18:02:23 +0000</updated>
                            <resolved>Thu, 1 Feb 2018 18:02:23 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>clients</component>
                    <component>consumer</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16340587" author="githubbot" created="Fri, 26 Jan 2018 05:20:13 +0000"  >&lt;p&gt;becketqin closed pull request #4478: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6489&quot; title=&quot;Fetcher.retrieveOffsetsByTimes() should add all the topics to the metadata refresh topics set.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6489&quot;&gt;&lt;del&gt;KAFKA-6489&lt;/del&gt;&lt;/a&gt;; Fetcher.retrieveOffsetsByTimes() should batch the metadata fetch.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4478&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4478&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index 5dc0b26f8fb..7de85811b95 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -450,6 +450,10 @@ private void resetOffsets(final Set&amp;lt;TopicPartition&amp;gt; partitions) {&lt;/p&gt;

&lt;p&gt;         long startMs = time.milliseconds();&lt;br/&gt;
         long remaining = timeout;&lt;br/&gt;
+        // Add the topics to the metadata to do a single metadata fetch.&lt;br/&gt;
+        for (TopicPartition tp : timestampsToSearch.keySet()) &lt;/p&gt;
{
+            metadata.add(tp.topic());
+        }
&lt;p&gt;         do {&lt;br/&gt;
             RequestFuture&amp;lt;Map&amp;lt;TopicPartition, OffsetData&amp;gt;&amp;gt; future =&lt;br/&gt;
                     sendListOffsetRequests(requireTimestamps, timestampsToSearch);&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16340588" author="githubbot" created="Fri, 26 Jan 2018 05:20:16 +0000"  >&lt;p&gt;becketqin opened a new pull request #4478: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6489&quot; title=&quot;Fetcher.retrieveOffsetsByTimes() should add all the topics to the metadata refresh topics set.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6489&quot;&gt;&lt;del&gt;KAFKA-6489&lt;/del&gt;&lt;/a&gt;; Fetcher.retrieveOffsetsByTimes() should batch the metadata fetch.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4478&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4478&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   *More detailed description of your change,&lt;br/&gt;
   if necessary. The PR title and PR message become&lt;br/&gt;
   the squashed commit message, so use a separate&lt;br/&gt;
   comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   for the feature or bug fix. Unit and/or integration&lt;br/&gt;
   tests are expected for any behaviour change and&lt;br/&gt;
   system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16348905" author="githubbot" created="Thu, 1 Feb 2018 17:00:24 +0000"  >&lt;p&gt;becketqin closed pull request #4478: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6489&quot; title=&quot;Fetcher.retrieveOffsetsByTimes() should add all the topics to the metadata refresh topics set.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6489&quot;&gt;&lt;del&gt;KAFKA-6489&lt;/del&gt;&lt;/a&gt;; Fetcher.retrieveOffsetsByTimes() should batch the metadata fetch.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4478&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4478&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index 5dc0b26f8fb..6d56139118a 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -604,11 +604,14 @@ private long endTimestamp() {&lt;br/&gt;
             final Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSearch) {&lt;br/&gt;
         // Group the partitions by node.&lt;br/&gt;
         final Map&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; timestampsToSearchByNode = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        // Add the topics to the metadata to do a single metadata fetch.&lt;br/&gt;
+        for (TopicPartition tp : timestampsToSearch.keySet())&lt;br/&gt;
+            metadata.add(tp.topic());&lt;br/&gt;
+&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry: timestampsToSearch.entrySet()) {&lt;br/&gt;
             TopicPartition tp  = entry.getKey();&lt;br/&gt;
             PartitionInfo info = metadata.fetch().partition(tp);&lt;br/&gt;
             if (info == null) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;metadata.add(tp.topic());&lt;br/&gt;
                 log.debug(&quot;Partition {} is unknown for fetching offset, wait for metadata refresh&quot;, tp);&lt;br/&gt;
                 return RequestFuture.staleMetadata();&lt;br/&gt;
             } else if (info.leader() == null) {&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/MockClient.java b/clients/src/test/java/org/apache/kafka/clients/MockClient.java&lt;br/&gt;
index 8b334729247..d843414fd7a 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/MockClient.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/MockClient.java&lt;br/&gt;
@@ -198,6 +198,12 @@ public void send(ClientRequest request, long now) {&lt;br/&gt;
             if (metadataUpdate == null)&lt;br/&gt;
                 metadata.update(metadata.fetch(), this.unavailableTopics, time.milliseconds());&lt;br/&gt;
             else 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                if (metadataUpdate.expectMatchRefreshTopics+                    &amp;amp;&amp;amp; !metadata.topics().equals(metadataUpdate.cluster.topics())) {
+                    throw new IllegalStateException(&quot;The metadata topics does not match expectation. &quot;
+                                                        + &quot;Expected topics: &quot; + metadataUpdate.cluster.topics()
+                                                        + &quot;, asked topics: &quot; + metadata.topics());
+                }                 this.unavailableTopics = metadataUpdate.unavailableTopics;                 metadata.update(metadataUpdate.cluster, metadataUpdate.unavailableTopics, time.milliseconds());             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -344,7 +350,13 @@ public void reset() {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public void prepareMetadataUpdate(Cluster cluster, Set&amp;lt;String&amp;gt; unavailableTopics) &lt;/p&gt;
{
-        metadataUpdates.add(new MetadataUpdate(cluster, unavailableTopics));
+        metadataUpdates.add(new MetadataUpdate(cluster, unavailableTopics, false));
+    }
&lt;p&gt;+&lt;br/&gt;
+    public void prepareMetadataUpdate(Cluster cluster,&lt;br/&gt;
+                                      Set&amp;lt;String&amp;gt; unavailableTopics,&lt;br/&gt;
+                                      boolean expectMatchMetadataTopics) &lt;/p&gt;
{
+        metadataUpdates.add(new MetadataUpdate(cluster, unavailableTopics, expectMatchMetadataTopics));
     }

&lt;p&gt;     public void setNode(Node node) {&lt;br/&gt;
@@ -433,9 +445,11 @@ public void setNodeApiVersions(NodeApiVersions nodeApiVersions) {&lt;br/&gt;
     private static class MetadataUpdate {&lt;br/&gt;
         final Cluster cluster;&lt;br/&gt;
         final Set&amp;lt;String&amp;gt; unavailableTopics;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MetadataUpdate(Cluster cluster, Set&amp;lt;String&amp;gt; unavailableTopics) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        final boolean expectMatchRefreshTopics;+        MetadataUpdate(Cluster cluster, Set&amp;lt;String&amp;gt; unavailableTopics, boolean expectMatchRefreshTopics) {
             this.cluster = cluster;
             this.unavailableTopics = unavailableTopics;
+            this.expectMatchRefreshTopics = expectMatchRefreshTopics;
         }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 26d7a50cf60..a3ea79356f9 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.clients.ApiVersions;&lt;br/&gt;
 import org.apache.kafka.clients.ClientRequest;&lt;br/&gt;
+import org.apache.kafka.clients.ClientUtils;&lt;br/&gt;
 import org.apache.kafka.clients.Metadata;&lt;br/&gt;
 import org.apache.kafka.clients.MockClient;&lt;br/&gt;
 import org.apache.kafka.clients.NetworkClient;&lt;br/&gt;
@@ -1942,41 +1943,51 @@ private int abortTransaction(ByteBuffer buffer, long producerId, long baseOffset&lt;br/&gt;
         return 1;&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void testGetOffsetsForTimesWithError(Errors errorForTp0,&lt;/li&gt;
	&lt;li&gt;Errors errorForTp1,&lt;/li&gt;
	&lt;li&gt;long offsetForTp0,&lt;/li&gt;
	&lt;li&gt;long offsetForTp1,&lt;/li&gt;
	&lt;li&gt;Long expectedOffsetForTp0,&lt;/li&gt;
	&lt;li&gt;Long expectedOffsetForTp1) {&lt;br/&gt;
+    private void testGetOffsetsForTimesWithError(Errors errorForP0,&lt;br/&gt;
+                                                 Errors errorForP1,&lt;br/&gt;
+                                                 long offsetForP0,&lt;br/&gt;
+                                                 long offsetForP1,&lt;br/&gt;
+                                                 Long expectedOffsetForP0,&lt;br/&gt;
+                                                 Long expectedOffsetForP1) {&lt;br/&gt;
         client.reset();&lt;/li&gt;
	&lt;li&gt;// Ensure metadata has both partition.&lt;/li&gt;
	&lt;li&gt;Cluster cluster = TestUtils.clusterWith(2, topicName, 2);&lt;/li&gt;
	&lt;li&gt;metadata.update(cluster, Collections.&amp;lt;String&amp;gt;emptySet(), time.milliseconds());&lt;br/&gt;
+        String topicName2 = &quot;topic2&quot;;&lt;br/&gt;
+        TopicPartition t2p0 = new TopicPartition(topicName2, 0);&lt;br/&gt;
+        // Expect a metadata refresh.&lt;br/&gt;
+        metadata.update(Cluster.bootstrap(ClientUtils.parseAndValidateAddresses(Collections.singletonList(&quot;1.1.1.1:1111&quot;))),&lt;br/&gt;
+                        Collections.&amp;lt;String&amp;gt;emptySet(),&lt;br/&gt;
+                        time.milliseconds());&lt;br/&gt;
+&lt;br/&gt;
+        Map&amp;lt;String, Integer&amp;gt; partitionNumByTopic = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        partitionNumByTopic.put(topicName, 2);&lt;br/&gt;
+        partitionNumByTopic.put(topicName2, 1);&lt;br/&gt;
+        cluster = TestUtils.clusterWith(2, partitionNumByTopic);&lt;br/&gt;
+        // The metadata refresh should contain all the topics.&lt;br/&gt;
+        client.prepareMetadataUpdate(cluster, Collections.&amp;lt;String&amp;gt;emptySet(), true);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // First try should fail due to metadata error.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;client.prepareResponseFrom(listOffsetResponse(tp0, errorForTp0, offsetForTp0, offsetForTp0), cluster.leaderFor(tp0));&lt;/li&gt;
	&lt;li&gt;client.prepareResponseFrom(listOffsetResponse(tp1, errorForTp1, offsetForTp1, offsetForTp1), cluster.leaderFor(tp1));&lt;br/&gt;
+        client.prepareResponseFrom(listOffsetResponse(t2p0, errorForP0, offsetForP0, offsetForP0), cluster.leaderFor(t2p0));&lt;br/&gt;
+        client.prepareResponseFrom(listOffsetResponse(tp1, errorForP1, offsetForP1, offsetForP1), cluster.leaderFor(tp1));&lt;br/&gt;
         // Second try should succeed.&lt;/li&gt;
	&lt;li&gt;client.prepareResponseFrom(listOffsetResponse(tp0, Errors.NONE, offsetForTp0, offsetForTp0), cluster.leaderFor(tp0));&lt;/li&gt;
	&lt;li&gt;client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, offsetForTp1, offsetForTp1), cluster.leaderFor(tp1));&lt;br/&gt;
+        client.prepareResponseFrom(listOffsetResponse(t2p0, Errors.NONE, offsetForP0, offsetForP0), cluster.leaderFor(t2p0));&lt;br/&gt;
+        client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, offsetForP1, offsetForP1), cluster.leaderFor(tp1));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;TopicPartition, Long&amp;gt; timestampToSearch = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;timestampToSearch.put(tp0, 0L);&lt;br/&gt;
+        timestampToSearch.put(t2p0, 0L);&lt;br/&gt;
         timestampToSearch.put(tp1, 0L);&lt;br/&gt;
         Map&amp;lt;TopicPartition, OffsetAndTimestamp&amp;gt; offsetAndTimestampMap = fetcher.getOffsetsByTimes(timestampToSearch, Long.MAX_VALUE);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (expectedOffsetForTp0 == null)&lt;/li&gt;
	&lt;li&gt;assertNull(offsetAndTimestampMap.get(tp0));&lt;br/&gt;
+        if (expectedOffsetForP0 == null)&lt;br/&gt;
+            assertNull(offsetAndTimestampMap.get(t2p0));&lt;br/&gt;
         else 
{
-            assertEquals(expectedOffsetForTp0.longValue(), offsetAndTimestampMap.get(tp0).timestamp());
-            assertEquals(expectedOffsetForTp0.longValue(), offsetAndTimestampMap.get(tp0).offset());
+            assertEquals(expectedOffsetForP0.longValue(), offsetAndTimestampMap.get(t2p0).timestamp());
+            assertEquals(expectedOffsetForP0.longValue(), offsetAndTimestampMap.get(t2p0).offset());
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (expectedOffsetForTp1 == null)&lt;br/&gt;
+        if (expectedOffsetForP1 == null)&lt;br/&gt;
             assertNull(offsetAndTimestampMap.get(tp1));&lt;br/&gt;
         else 
{
-            assertEquals(expectedOffsetForTp1.longValue(), offsetAndTimestampMap.get(tp1).timestamp());
-            assertEquals(expectedOffsetForTp1.longValue(), offsetAndTimestampMap.get(tp1).offset());
+            assertEquals(expectedOffsetForP1.longValue(), offsetAndTimestampMap.get(tp1).timestamp());
+            assertEquals(expectedOffsetForP1.longValue(), offsetAndTimestampMap.get(tp1).offset());
         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;






&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16348949" author="becket_qin" created="Thu, 1 Feb 2018 17:22:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=damianguy&quot; class=&quot;user-hover&quot; rel=&quot;damianguy&quot;&gt;damianguy&lt;/a&gt; Can we include this in 1.1? This patch is important for usability of the consumer group tool.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13123312">KAFKA-6321</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 41 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3pebb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>