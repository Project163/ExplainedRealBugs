<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:23:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-9669] Kafka 2.4.0 Chokes on Filebeat 5.6 Produced Data</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-9669</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Hi&lt;/p&gt;

&lt;p&gt;In our environment, after upgrading to Kafka 2.4.0, we discovered the broker was not compatible with filebeat 5.&lt;/p&gt;

&lt;p&gt;Here is how to reproduce:&lt;/p&gt;

&lt;p&gt;1. Startup Kafka 2.4.0, all configurations are vanilla:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ kafka_2.13-2.4.0/bin/zookeeper-server-start.sh kafka_2.13-2.4.0/config/zookeeper.properties
$ kafka_2.13-2.4.0/bin/kafka-server-start.sh kafka_2.13-2.4.0/config/server.properties
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2. Startup filebeat 5.6.16 with the following configuration. (downloaded from &lt;a href=&quot;https://www.elastic.co/jp/downloads/past-releases/filebeat-5-6-16&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.elastic.co/jp/downloads/past-releases/filebeat-5-6-16&lt;/a&gt;)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ cat /tmp/filebeat.yml
name: test

output.kafka:
  enabled: &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
  hosts:
    - localhost:9092
  topic: test-3
  version: 0.10.0
  compression: gzip

filebeat:
  prospectors:
    - input_type: log
      paths:
        - /tmp/filebeat-in
      encoding: plain
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ filebeat-5.6.16-linux-x86_64/filebeat -e -c /tmp/filebeat.yml
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3. Write some lines to file &lt;tt&gt;/tmp/filebeat-in&lt;/tt&gt;. Looks like single line won&apos;t trigger the issue, but 30 lines are enough.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
seq 30 &amp;gt;&amp;gt; /tmp/filebeat-in
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4. Kafka throws the following error chunk, like, per produced record.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2020-03-06 05:17:40,129] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=1453875406, CreateTime=1583471854475, key=0 bytes, value=202 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 05:17:40,129] ERROR [KafkaApi-0] Error when handling request: clientId=beats, correlationId=102, api=PRODUCE, version=2, body={acks=1,timeout=10000,partitionSizes=[test-3-0=272]} (kafka.server.KafkaApis)
java.lang.NullPointerException: `field` must be non-null
	at java.base/java.util.Objects.requireNonNull(Objects.java:246)
	at org.apache.kafka.common.protocol.types.Struct.validateField(Struct.java:474)
	at org.apache.kafka.common.protocol.types.Struct.instance(Struct.java:418)
	at org.apache.kafka.common.protocol.types.Struct.instance(Struct.java:436)
	at org.apache.kafka.common.requests.ProduceResponse.toStruct(ProduceResponse.java:281)
	at org.apache.kafka.common.requests.AbstractResponse.toSend(AbstractResponse.java:35)
	at org.apache.kafka.common.requests.RequestContext.buildResponse(RequestContext.java:80)
	at kafka.server.KafkaApis.sendResponse(KafkaApis.scala:2892)
	at kafka.server.KafkaApis.sendResponseCallback$2(KafkaApis.scala:554)
	at kafka.server.KafkaApis.$anonfun$handleProduceRequest$11(KafkaApis.scala:576)
	at kafka.server.KafkaApis.$anonfun$handleProduceRequest$11$adapted(KafkaApis.scala:576)
	at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:546)
	at kafka.server.KafkaApis.handleProduceRequest(KafkaApis.scala:577)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:126)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:835)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</description>
                <environment></environment>
        <key id="13289957">KAFKA-9669</key>
            <summary>Kafka 2.4.0 Chokes on Filebeat 5.6 Produced Data</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="weichu">Wei Chu</reporter>
                        <labels>
                    </labels>
                <created>Fri, 6 Mar 2020 05:27:32 +0000</created>
                <updated>Tue, 12 May 2020 19:08:37 +0000</updated>
                            <resolved>Tue, 12 May 2020 19:08:37 +0000</resolved>
                                    <version>2.4.0</version>
                    <version>2.4.1</version>
                    <version>2.5.0</version>
                                    <fixVersion>2.4.2</fixVersion>
                    <fixVersion>2.5.1</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="17053066" author="ijuma" created="Fri, 6 Mar 2020 06:20:00 +0000"  >&lt;p&gt;Can you please try 2.4.1 RC0?&#160;&lt;a href=&quot;https://home.apache.org/~bbejeck/kafka-2.4.1-rc0/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://home.apache.org/~bbejeck/kafka-2.4.1-rc0/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17053077" author="weichu" created="Fri, 6 Mar 2020 06:40:16 +0000"  >&lt;p&gt;&lt;del&gt;With 2.4.1 RC0, the Kafka does provider prettier error logs, but still cannot produce:&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;Update:&lt;br/&gt;
Sorry for the misleading, so I double checked the result. Despite for the ERROR log, the messages were actually stored to the topic.&lt;br/&gt;
On the other hand, throwing one ERROR level message per produced record is kinda not acceptable for production usage, because the Kafka log would flood out the disk.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2020-03-06 06:36:37,159] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=1478844555, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 06:36:37,177] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=4137384030, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 06:36:37,181] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=4137384030, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 06:36:37,194] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=3885590418, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17104979" author="hachikuji" created="Mon, 11 May 2020 23:52:34 +0000"  >&lt;p&gt;I think this is the result of stricter validation on the broker from &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-8106&quot; title=&quot;Reducing the allocation and copying of ByteBuffer  when logValidator  do validation.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-8106&quot;&gt;&lt;del&gt;KAFKA-8106&lt;/del&gt;&lt;/a&gt;. The &quot;inner&quot; offsets in v1 messages are expected to increase sequentially, but previously we would silently ignore this and rewrite the batch. In 2.4, this become a validation error, which broke compatibility for older clients.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 27 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0c89k:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>