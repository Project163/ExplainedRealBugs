<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:14:27 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7044] kafka-consumer-groups.sh NullPointerException describing round robin or sticky assignors</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7044</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We&apos;ve recently moved to using the round robin assignor for one of our consumer groups, and started testing the sticky assignor. In both cases, using Kafka 1.1.0 we get a null pointer exception &lt;b&gt;unless&lt;/b&gt; the group being described is rebalancing:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
kafka-consumer-groups --bootstrap-server fqdn:9092 --describe --group groupname-&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;-consumer

Error: Executing consumer group command failed due to &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
[2018-06-12 01:32:34,179] DEBUG Exception in consumer group command (kafka.admin.ConsumerGroupCommand$)
java.lang.NullPointerException
at scala.Predef$.Long2long(Predef.scala:363)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService$$anonfun$getLogEndOffsets$2.apply(ConsumerGroupCommand.scala:612)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService$$anonfun$getLogEndOffsets$2.apply(ConsumerGroupCommand.scala:610)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.immutable.List.foreach(List.scala:392)
at scala.collection.TraversableLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;map(TraversableLike.scala:234)
at scala.collection.immutable.List.map(List.scala:296)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService.getLogEndOffsets(ConsumerGroupCommand.scala:610)
at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;describePartitions(ConsumerGroupCommand.scala:328)
at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;collectConsumerAssignment(ConsumerGroupCommand.scala:308)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService.collectConsumerAssignment(ConsumerGroupCommand.scala:544)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService$$anonfun$10$$anonfun$13.apply(ConsumerGroupCommand.scala:571)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService$$anonfun$10$$anonfun$13.apply(ConsumerGroupCommand.scala:565)
at scala.collection.immutable.List.flatMap(List.scala:338)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService$$anonfun$10.apply(ConsumerGroupCommand.scala:565)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService$$anonfun$10.apply(ConsumerGroupCommand.scala:558)
at scala.Option.map(Option.scala:146)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService.collectGroupOffsets(ConsumerGroupCommand.scala:558)
at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;describeGroup(ConsumerGroupCommand.scala:271)
at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService.describeGroup(ConsumerGroupCommand.scala:544)
at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:77)
at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)
[2018-06-12 01:32:34,255] DEBUG Removed sensor with name connections-closed: (org.apache.kafka.common.metrics.Metrics)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>CentOS 7.4, Oracle JDK 1.8</environment>
        <key id="13165470">KAFKA-7044</key>
            <summary>kafka-consumer-groups.sh NullPointerException describing round robin or sticky assignors</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="apovzner">Anna Povzner</assignee>
                                    <reporter username="tfield">Terra Field</reporter>
                        <labels>
                    </labels>
                <created>Tue, 12 Jun 2018 01:44:11 +0000</created>
                <updated>Tue, 11 Sep 2018 17:28:36 +0000</updated>
                            <resolved>Tue, 11 Sep 2018 17:28:24 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>tools</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="16510156" author="vahid" created="Tue, 12 Jun 2018 20:26:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jfield&quot; class=&quot;user-hover&quot; rel=&quot;jfield&quot;&gt;jfield&lt;/a&gt; I have not been able to reproduce the error you reported after running a couple of consumers with either of these assignors. Can you provide the steps to reproduce?&lt;/p&gt;</comment>
                            <comment id="16510405" author="jfield" created="Tue, 12 Jun 2018 23:50:55 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group groupname&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Is all I need to do to reproduce it. We&apos;ve got 18 consumers spread across 6 hosts (3 per host) in the consumer group reading from 2 topics with 12 partitions and 2 topics with 18 partitions (60 total) spread across 3 brokers. All partitions have received and are receiving data. Currently we&apos;re using burrow to get&#160;assignment&#160;information since we can&apos;t get it out of the kafka-consumer-groups command, I can post the burrow output for the group if that is helpful (but nothing stands out to me)&lt;/p&gt;</comment>
                            <comment id="16516534" author="brettrann" created="Tue, 19 Jun 2018 01:45:16 +0000"  >&lt;p&gt;We have the same problem with some consumer groups in our 1.1.0 clusters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jfield&quot; class=&quot;user-hover&quot; rel=&quot;jfield&quot;&gt;jfield&lt;/a&gt; if you pull out an older version of the cli tool that&apos;s still working for us, as an interim option rather than relying on burrow.&lt;/p&gt;</comment>
                            <comment id="16516556" author="brettrann" created="Tue, 19 Jun 2018 02:11:30 +0000"  >&lt;p&gt;for us it&apos;s only happening in 1 consumer group. it subscribes to many topics, they all should be round robin assigned, some are actually empty.&lt;/p&gt;

&lt;p&gt;we have many clusters with the same configs. it&apos;s happening in one, but not another, with the same consumer group. &#160;so i&apos;ll dig a bit and see if I can find a meaningful difference between the two.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16516610" author="brettrann" created="Tue, 19 Jun 2018 03:16:35 +0000"  >&lt;p&gt;edited:&lt;/p&gt;

&lt;p&gt;my edited comment&#160;was flawed due to a bad test.&lt;/p&gt;

&lt;p&gt;Running a git bisect to find the sha. I&apos;m pretty sure it was after your change to that line&apos;s block, Vahid.&lt;/p&gt;</comment>
                            <comment id="16516727" author="brettrann" created="Tue, 19 Jun 2018 07:07:13 +0000"  >&lt;p&gt;OK for my reproducible case, after doing a git bisect the bug is introduced in this commit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/commit/677881afc52485aef94150be50d6258d7a340071#diff-b45245913eaae46aa847d2615d62cde0R164&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/commit/677881afc52485aef94150be50d6258d7a340071#diff-b45245913eaae46aa847d2615d62cde0R164&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pinging &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; who authored the commit and&#160; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rsivaram&quot; class=&quot;user-hover&quot; rel=&quot;rsivaram&quot;&gt;rsivaram&lt;/a&gt; who reviewed it.&lt;/p&gt;

&lt;p&gt;The PR is:&#160;&lt;a href=&quot;https://github.com/apache/kafka/pull/4557&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4557&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;( that commit is&#160;interestingly is after your change to that :612 block&#160;of&#160;ConsumerGroupCommand.scala &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vahid&quot; class=&quot;user-hover&quot; rel=&quot;vahid&quot;&gt;vahid&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I was interested by the Long2long part of the debug output (that I get and Jeff posted also) so searched for big Long that in that commit.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;+ final Long timestamp; // null if the broker does not support returning timestamps&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;While I was trying to read code I don&apos;t understand much, I changed that to little long and kicked off a build. Obviously not the right fix, it&apos;s referenced as Long in many places in that code and other files, but builds are cheap right?&lt;/p&gt;

&lt;p&gt;Then I saw that it wasn&apos;t introduced in that PR, it was just moved a little to make the comment a single liner:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;- final Long timestamp;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The thing is. That build with it changed to little long works. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160;Whaa?&lt;/p&gt;

&lt;p&gt;My scala dev here says &quot;assigning null to a java long actually sets it to 0 so you&apos;re accidentally&#160;getting rid of the null before it has a chance to cause heck&quot;.&lt;/p&gt;

&lt;p&gt;So:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I don&apos;t know what it that commit&#160;introduced the issue; It&apos;s somewhere in that commit for me.&lt;/li&gt;
	&lt;li&gt;given what my dev says, setting it to 0 instead of null might be&#160;one way to avoid the null&#160;conversion&lt;/li&gt;
	&lt;li&gt;or working out how that PR broke it and fixing it&#160;properly&#160;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It does seem that having null Long&apos;s is a trap waiting if there&apos;s going to be a conversion to long at some other point though.&lt;/p&gt;

&lt;p&gt;I&quot;m happy to build and test a fix if someone comes up with one, against the group&#160;where we&apos;re having the issue. Or to collect more info if needed.&lt;/p&gt;</comment>
                            <comment id="16518169" author="ijuma" created="Wed, 20 Jun 2018 14:03:32 +0000"  >&lt;p&gt;The issue happens in the following lines:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        val logEndOffset = offsets.get(topicPartition)
        topicPartition -&amp;gt; LogOffsetResult.LogOffset(logEndOffset)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;`logEndOffset` can be `null` and we are not expecting that. Since we have a PR open to use the Java AdminClient in `ConsumerGroupCommand`, it makes sense to merge that first and then tackle this.&lt;/p&gt;</comment>
                            <comment id="16518176" author="ijuma" created="Wed, 20 Jun 2018 14:05:17 +0000"  >&lt;p&gt;The odd thing is that the `endOffsets` should never return `null`, which is what we are using here.&lt;/p&gt;</comment>
                            <comment id="16549675" author="ejpearson" created="Thu, 19 Jul 2018 18:44:42 +0000"  >&lt;p&gt;We are running Confluent Platform 4.1.1 and are seeing the same error when describing certain consumer groups.&lt;/p&gt;

&lt;p&gt;Error: Executing consumer group command failed due to null&lt;/p&gt;

&lt;p&gt;It might be under different circumstances, however.&#160; We can run kafka-consumer-groups from a Confluent Platform 3.3.1 deployment and the same describe command works but Confluent 4.1.1 doesn&apos;t.&#160; It seems like a regression.&lt;/p&gt;

&lt;p&gt;Here is what we notice on our group, running the current Apache origin/master:&lt;/p&gt;

&lt;p&gt;When getLogEndOffsets(topicPartitions: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;) is called, topicPartitions has 16 elements.&#160; In our case it contains one such topicPartition &apos;&lt;em&gt;sometopic&lt;/em&gt;-11&apos;&lt;/p&gt;

&lt;p&gt;The converted .asJava collection still has 16 elements, including &apos;&lt;em&gt;sometopic&lt;/em&gt;-11&apos;.&lt;/p&gt;

&lt;p&gt;But the call to&#160;getConsumer.endOffsets() using the converted Java collection only returns a 15 element map.&#160; There is no entry for key &apos;&lt;em&gt;sometopic&lt;/em&gt;-11&apos; in the map.&lt;/p&gt;

&lt;p&gt;Eventually topicPartitions (16 elements) is traversed again, and when it processes that one missing element in the map (15 elements) it hits the null exception.&lt;/p&gt;

&lt;p&gt;We don&apos;t know what is different with partition 11 on that topic.&#160; All other ones have their offsets returned.&lt;/p&gt;

&lt;p&gt;I hope this helps.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16553496" author="vahid" created="Mon, 23 Jul 2018 22:39:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ejpearson&quot; class=&quot;user-hover&quot; rel=&quot;ejpearson&quot;&gt;ejpearson&lt;/a&gt;, thanks for providing the additional info. It seems that the call to &lt;tt&gt;endOffsets(...)&lt;/tt&gt; leads to a call to &lt;tt&gt;Fetcher.fetchOffsetsByTimes(...)&lt;/tt&gt;&#160;that involves a timeout.&#160;The default timeout that is currently in place is quite&#160;long (30 secs), but I wonder if that somehow kicks in causing the call to finding the end offset for&#160;the given partitions return with a partial list of end offsets. Is it possible for you to increase the timeout and observe if things will change on your side (for example, you can use &lt;tt&gt;getConsumer.endOffsets(topicPartitions.asJava, 60000)&lt;/tt&gt;&#160;to double the timeout)?&lt;/p&gt;

&lt;p&gt;Is it always the same partition 11 that causes the NPE for you? Is there anything (size, lag, ...) different about that partition?&lt;/p&gt;

&lt;p&gt;I&apos;m also interested to know what comes back from this call in &lt;tt&gt;Fetcher.sendListOffsetsRequest(...)&lt;/tt&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
RequestFuture&amp;lt;ListOffsetResult&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt; =
    sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Does it return a valid offset for each partition?&lt;/p&gt;

&lt;p&gt;BTW, this seems to be similar to the issue reported in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6662&quot; title=&quot;Consumer use offsetsForTimes() get offset return None.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6662&quot;&gt;&lt;del&gt;KAFKA-6662&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16554905" author="ejpearson" created="Tue, 24 Jul 2018 23:48:34 +0000"  >&lt;p&gt;I tried increasing the Duration explicitly in getConsumer.endOffsets() but I get the same error.&lt;/p&gt;

&lt;p&gt;I narrowed down where the request loses the partition.&#160; It looks like Fetcher.groupListOffsetRequests() is where the topicPartition is being dropped.&#160; When the function is called the second time with a timestampsToSearch Map of 16, the sum of topicPartitions in the returned in the Map of Maps is 15.&lt;/p&gt;

&lt;p&gt;It looks like&#160;Fetcher.groupListOffsetRequests() is called for each Client ID in the consumer group.&#160; On the first invocation there is no metadata and an empty map is returned, which eventually causes a metadata refresh in Fetcher.sendListOffsetsRequests()&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Map&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; timestampsToSearchByNode = groupListOffsetRequests(timestampsToSearch);&lt;/p&gt;

&lt;p&gt;if (timestampsToSearchByNode.isEmpty())&lt;br/&gt;
&#160; &#160; return RequestFuture.failure(new StaleMetadataException());&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;On the second invocation of Fetcher.groupListsOffsetRequests(), the dropped partition is the last partition in the for loop.&#160; It&apos;s also the first reference to a specific topic, whose remaining partitions are being handled by a yet-to-processed Client ID.&#160; The topic wasn&apos;t refreshed from the first invocation since the first Client ID didn&apos;t process any partitions for this topic.&#160; Calling:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;PartitionInfo info = metadata.fetch().partition(tp);&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;returns null.&#160; There&apos;s a call to add the topic to the metadata and request a metadata refresh, but the topicPartition is never added to the result.&#160; The loop ends and the result is returned with 1 less partition in the Map of Maps.&lt;/p&gt;

&lt;p&gt;I&apos;m not too familiar with the code, but I wanted to see if a metadata refresh would fix it since the topic was added for metadata refresh.&#160; I added a second check after the .isEmpty check to verify the number of offsets to search are the same.&#160; Please take this will many grains of salt; there probably are better ways to fix this issue:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;// Hacking around to avoid null exception. Assume if the timestampsToSearchByNode&lt;br/&gt;
// does NOT have the same number of offsets as timestampsToSearch then the metadata is stale.&lt;br/&gt;
int sumTopicPartitionsByNode = 0;&lt;br/&gt;
for (Map&amp;lt;TopicPartition, Long&amp;gt; nodeMap : timestampsToSearchByNode.values()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {
&#160; &#160; sumTopicPartitionsByNode += nodeMap.size();
}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;if (sumTopicPartitionsByNode != timestampsToSearch.size()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {
&#160; &#160; log.warn(&amp;quot;Expected offsets}&lt;/span&gt; &lt;/div&gt;&lt;/blockquote&gt;
&lt;p&gt;Now the kafka-consumer-group command works.&#160; It also shows it hits that log.warn() at least on another Client ID in my problematic group as well.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16554909" author="ejpearson" created="Tue, 24 Jul 2018 23:51:01 +0000"  >&lt;p&gt;Seems I needed to escape those leading braces in the for/if.&#160; Sorry.&lt;/p&gt;</comment>
                            <comment id="16604519" author="tom@confluent.io" created="Wed, 5 Sep 2018 14:59:21 +0000"  >&lt;p&gt;I hit this with Confluent Platform 4.1.2, using the kafka-consumer-groups tool from Confluent 4.0.1 works as a workaround.&lt;/p&gt;</comment>
                            <comment id="16604639" author="hachikuji" created="Wed, 5 Sep 2018 16:30:28 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ejpearson&quot; class=&quot;user-hover&quot; rel=&quot;ejpearson&quot;&gt;ejpearson&lt;/a&gt; for the investigation. I think the problem is the early return &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L410&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;. We need to check whether &lt;tt&gt;remainingToSearch&lt;/tt&gt; is empty before returning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vahid&quot; class=&quot;user-hover&quot; rel=&quot;vahid&quot;&gt;vahid&lt;/a&gt; Does that make sense?&lt;/p&gt;
</comment>
                            <comment id="16606375" author="apovzner" created="Thu, 6 Sep 2018 20:52:35 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vahid&quot; class=&quot;user-hover&quot; rel=&quot;vahid&quot;&gt;vahid&lt;/a&gt;, We are seeing this bug a lot recently, and really want to get it fixed asap. I will try Jason&apos;s suggestion and open a PR if it works. I hope that&apos;s ok with you.&#160;&lt;/p&gt;</comment>
                            <comment id="16608368" author="githubbot" created="Sun, 9 Sep 2018 08:02:06 +0000"  >&lt;p&gt;apovzner opened a new pull request #5627: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7044&quot; title=&quot;kafka-consumer-groups.sh NullPointerException describing round robin or sticky assignors&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7044&quot;&gt;&lt;del&gt;KAFKA-7044&lt;/del&gt;&lt;/a&gt;: Fix Fetcher.fetchOffsetsByTimes and NPE in describe consumer group&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5627&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5627&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   `kafka-consumer-groups --describe --group ...` can result in NullPointerException for two reasons:&lt;br/&gt;
   1)  Fetcher.fetchOffsetsByTimes() may return too early, without sending list offsets request for topic partitions that are not in cached metadata.&lt;br/&gt;
   2) `ConsumerGroupCommand.getLogEndOffsets()` and `getLogStartOffsets()` assumed that endOffsets()/beginningOffsets() which eventually call Fetcher.fetchOffsetsByTimes(), would return a map with all the topic partitions passed to endOffsets()/beginningOffsets() and that values are not null.&lt;br/&gt;
   Because of #1, null values were possible if some of the topic partitions were already known (in metadata cache) and some not (metadata cache did not have entries for some of the topic partitions). However, even with fixing #1, endOffsets()/beginningOffsets() may return a map with some topic partitions missing, when list offset request returns a non-retriable error. This happens in corner cases such as message format on broker is before 0.10, or maybe in cases of some other errors. &lt;/p&gt;

&lt;p&gt;   Testing:&lt;br/&gt;
   &amp;#8211; added unit test to verify fix in Fetcher.fetchOffsetsByTimes() &lt;br/&gt;
   &amp;#8211; did some manual testing with `kafka-consumer-groups --describe`, causing NPE. Was not able to reproduce any NPE cases with DescribeConsumerGroupTest.scala,&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16610807" author="vahid" created="Tue, 11 Sep 2018 15:35:50 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apovzner&quot; class=&quot;user-hover&quot; rel=&quot;apovzner&quot;&gt;apovzner&lt;/a&gt;, Sorry for missing the last few comments. Thanks a lot for submitting the PR.&lt;/p&gt;</comment>
                            <comment id="16610945" author="githubbot" created="Tue, 11 Sep 2018 17:10:45 +0000"  >&lt;p&gt;hachikuji closed pull request #5627: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7044&quot; title=&quot;kafka-consumer-groups.sh NullPointerException describing round robin or sticky assignors&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7044&quot;&gt;&lt;del&gt;KAFKA-7044&lt;/del&gt;&lt;/a&gt;: Fix Fetcher.fetchOffsetsByTimes and NPE in describe consumer group&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5627&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5627&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml&lt;br/&gt;
index 3ed4a9c1f3f..2fa499c2165 100644&lt;br/&gt;
&amp;#8212; a/checkstyle/suppressions.xml&lt;br/&gt;
+++ b/checkstyle/suppressions.xml&lt;br/&gt;
@@ -70,7 +70,7 @@&lt;br/&gt;
               files=&quot;MockAdminClient.java&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;     &amp;lt;suppress checks=&quot;JavaNCSS&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;files=&quot;RequestResponseTest.java&quot;/&amp;gt;&lt;br/&gt;
+              files=&quot;RequestResponseTest.java|FetcherTest.java&quot;/&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;suppress checks=&quot;NPathComplexity&quot;&lt;br/&gt;
               files=&quot;MemoryRecordsTest|MetricsTest&quot;/&amp;gt;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index dc0daa233ab..9fa64462d85 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -414,7 +414,7 @@ private ListOffsetResult fetchOffsetsByTimes(Map&amp;lt;TopicPartition, Long&amp;gt; timestamp&lt;br/&gt;
                 if (value.partitionsToRetry.isEmpty())&lt;br/&gt;
                     return result;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;remainingToSearch.keySet().removeAll(result.fetchedOffsets.keySet());&lt;br/&gt;
+                remainingToSearch.keySet().retainAll(value.partitionsToRetry);&lt;br/&gt;
             } else if (!future.isRetriable()) 
{
                 throw future.exception();
             }
&lt;p&gt;@@ -575,7 +575,7 @@ private void resetOffsetsAsync(Map&amp;lt;TopicPartition, Long&amp;gt; partitionResetTimestamp&lt;br/&gt;
             metadata.add(tp.topic());&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; timestampsToSearchByNode =&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupListOffsetRequests(partitionResetTimestamps);&lt;br/&gt;
+                groupListOffsetRequests(partitionResetTimestamps, new HashSet&amp;lt;&amp;gt;());&lt;br/&gt;
         for (Map.Entry&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; entry : timestampsToSearchByNode.entrySet()) {&lt;br/&gt;
             Node node = entry.getKey();&lt;br/&gt;
             final Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; resetTimestamps = entry.getValue();&lt;br/&gt;
@@ -624,19 +624,19 @@ public void onFailure(RuntimeException e) {&lt;br/&gt;
         for (TopicPartition tp : timestampsToSearch.keySet())&lt;br/&gt;
             metadata.add(tp.topic());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        final Set&amp;lt;TopicPartition&amp;gt; partitionsToRetry = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; timestampsToSearchByNode =&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupListOffsetRequests(timestampsToSearch);&lt;br/&gt;
+                groupListOffsetRequests(timestampsToSearch, partitionsToRetry);&lt;br/&gt;
         if (timestampsToSearchByNode.isEmpty())&lt;br/&gt;
             return RequestFuture.failure(new StaleMetadataException());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final RequestFuture&amp;lt;ListOffsetResult&amp;gt; listOffsetRequestsFuture = new RequestFuture&amp;lt;&amp;gt;();&lt;br/&gt;
         final Map&amp;lt;TopicPartition, OffsetData&amp;gt; fetchedTimestampOffsets = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Set&amp;lt;TopicPartition&amp;gt; partitionsToRetry = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         final AtomicInteger remainingResponses = new AtomicInteger(timestampsToSearchByNode.size());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         for (Map.Entry&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; entry : timestampsToSearchByNode.entrySet()) {&lt;br/&gt;
             RequestFuture&amp;lt;ListOffsetResult&amp;gt; future =&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);&lt;br/&gt;
+                sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);&lt;br/&gt;
             future.addListener(new RequestFutureListener&amp;lt;ListOffsetResult&amp;gt;() {&lt;br/&gt;
                 @Override&lt;br/&gt;
                 public void onSuccess(ListOffsetResult partialResult) {&lt;br/&gt;
@@ -663,8 +663,16 @@ public void onFailure(RuntimeException e) 
{
         return listOffsetRequestsFuture;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /**&lt;br/&gt;
+     * Groups timestamps to search by node for topic partitions in `timestampsToSearch` that have&lt;br/&gt;
+     * leaders available. Topic partitions from `timestampsToSearch` that do not have their leader&lt;br/&gt;
+     * available are added to `partitionsToRetry`&lt;br/&gt;
+     * @param timestampsToSearch The mapping from partitions ot the target timestamps&lt;br/&gt;
+     * @param partitionsToRetry A set of topic partitions that will be extended with partitions&lt;br/&gt;
+     *                          that need metadata update or re-connect to the leader.&lt;br/&gt;
+     */&lt;br/&gt;
     private Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; groupListOffsetRequests(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSearch) {&lt;br/&gt;
+            Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSearch, Set&amp;lt;TopicPartition&amp;gt; partitionsToRetry) {&lt;br/&gt;
         final Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; timestampsToSearchByNode = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry: timestampsToSearch.entrySet()) {&lt;br/&gt;
             TopicPartition tp  = entry.getKey();&lt;br/&gt;
@@ -673,9 +681,11 @@ public void onFailure(RuntimeException e) {&lt;br/&gt;
                 metadata.add(tp.topic());&lt;br/&gt;
                 log.debug(&quot;Leader for partition {} is unknown for fetching offset&quot;, tp);&lt;br/&gt;
                 metadata.requestUpdate();&lt;br/&gt;
+                partitionsToRetry.add(tp);&lt;br/&gt;
             } else if (info.leader() == null) {&lt;br/&gt;
                 log.debug(&quot;Leader for partition {} is unavailable for fetching offset&quot;, tp);&lt;br/&gt;
                 metadata.requestUpdate();&lt;br/&gt;
+                partitionsToRetry.add(tp);&lt;br/&gt;
             } else if (client.isUnavailable(info.leader())) {&lt;br/&gt;
                 client.maybeThrowAuthFailure(info.leader());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -683,7 +693,8 @@ public void onFailure(RuntimeException e) {&lt;br/&gt;
                 // try again. No need to request a metadata update since the disconnect will have&lt;br/&gt;
                 // done so already.&lt;br/&gt;
                 log.debug(&quot;Leader {} for partition {} is unavailable for fetching offset until reconnect backoff expires&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info.leader(), tp);&lt;br/&gt;
+                          info.leader(), tp);&lt;br/&gt;
+                partitionsToRetry.add(tp);&lt;br/&gt;
             } else {&lt;br/&gt;
                 Node node = info.leader();&lt;br/&gt;
                 Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; topicData =&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index d314a4d8c9f..5edffa9dd93 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -108,6 +108,7 @@&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertFalse;&lt;br/&gt;
 import static org.junit.Assert.assertNull;&lt;br/&gt;
+import static org.junit.Assert.assertNotNull;&lt;br/&gt;
 import static org.junit.Assert.assertTrue;&lt;br/&gt;
 import static org.junit.Assert.fail;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1919,6 +1920,48 @@ public void testGetOffsetsForTimes() &lt;/p&gt;
{
         testGetOffsetsForTimesWithError(Errors.BROKER_NOT_AVAILABLE, Errors.NONE, 10L, 100L, 10L, 100L);
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testGetOffsetsForTimesWhenSomeTopicPartitionLeadersNotKnownInitially() &lt;/p&gt;
{
+        final String anotherTopic = &quot;another-topic&quot;;
+        final TopicPartition t2p0 = new TopicPartition(anotherTopic, 0);
+
+        client.reset();
+
+        // Metadata initially has one topic
+        Cluster cluster = TestUtils.clusterWith(3, topicName, 2);
+        metadata.update(cluster, Collections.&amp;lt;String&amp;gt;emptySet(), time.milliseconds());
+
+        // The first metadata refresh should contain one topic
+        client.prepareMetadataUpdate(cluster, Collections.&amp;lt;String&amp;gt;emptySet(), false);
+        client.prepareResponseFrom(listOffsetResponse(tp0, Errors.NONE, 1000L, 11L), cluster.leaderFor(tp0));
+        client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, 1000L, 32L), cluster.leaderFor(tp1));
+
+        // Second metadata refresh should contain two topics
+        Map&amp;lt;String, Integer&amp;gt; partitionNumByTopic = new HashMap&amp;lt;&amp;gt;();
+        partitionNumByTopic.put(topicName, 2);
+        partitionNumByTopic.put(anotherTopic, 1);
+        Cluster updatedCluster = TestUtils.clusterWith(3, partitionNumByTopic);
+        client.prepareMetadataUpdate(updatedCluster, Collections.&amp;lt;String&amp;gt;emptySet(), false);
+        client.prepareResponseFrom(listOffsetResponse(t2p0, Errors.NONE, 1000L, 54L), cluster.leaderFor(t2p0));
+
+        Map&amp;lt;TopicPartition, Long&amp;gt; timestampToSearch = new HashMap&amp;lt;&amp;gt;();
+        timestampToSearch.put(tp0, ListOffsetRequest.LATEST_TIMESTAMP);
+        timestampToSearch.put(tp1, ListOffsetRequest.LATEST_TIMESTAMP);
+        timestampToSearch.put(t2p0, ListOffsetRequest.LATEST_TIMESTAMP);
+        Map&amp;lt;TopicPartition, OffsetAndTimestamp&amp;gt; offsetAndTimestampMap =
+            fetcher.offsetsByTimes(timestampToSearch, time.timer(Long.MAX_VALUE));
+
+        assertNotNull(&quot;Expect Fetcher.offsetsByTimes() to return non-null result for &quot; + tp0,
+                      offsetAndTimestampMap.get(tp0));
+        assertNotNull(&quot;Expect Fetcher.offsetsByTimes() to return non-null result for &quot; + tp1,
+                      offsetAndTimestampMap.get(tp1));
+        assertNotNull(&quot;Expect Fetcher.offsetsByTimes() to return non-null result for &quot; + t2p0,
+                      offsetAndTimestampMap.get(t2p0));
+        assertEquals(11L, offsetAndTimestampMap.get(tp0).offset());
+        assertEquals(32L, offsetAndTimestampMap.get(tp1).offset());
+        assertEquals(54L, offsetAndTimestampMap.get(t2p0).offset());
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test(expected = TimeoutException.class)&lt;br/&gt;
     public void testBatchedListOffsetsMetadataErrors() {&lt;br/&gt;
         Map&amp;lt;TopicPartition, ListOffsetResponse.PartitionData&amp;gt; partitionData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala b/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala&lt;br/&gt;
index 1d61720bfa9..c0f6797eddd 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala&lt;br/&gt;
@@ -292,12 +292,8 @@ object ConsumerGroupCommand extends Logging {&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;       getLogEndOffsets(topicPartitions).map {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;logEndOffsetResult =&amp;gt;&lt;/li&gt;
	&lt;li&gt;logEndOffsetResult._2 match 
{
-            case LogOffsetResult.LogOffset(logEndOffset) =&amp;gt; getDescribePartitionResult(logEndOffsetResult._1, Some(logEndOffset))
-            case LogOffsetResult.Unknown =&amp;gt; getDescribePartitionResult(logEndOffsetResult._1, None)
-            case LogOffsetResult.Ignore =&amp;gt; null
-          }
&lt;p&gt;+        case (topicPartition, LogOffsetResult.LogOffset(offset)) =&amp;gt; getDescribePartitionResult(topicPartition, Some(offset))&lt;br/&gt;
+        case (topicPartition, _) =&amp;gt; getDescribePartitionResult(topicPartition, None)&lt;br/&gt;
       }.toArray&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -399,16 +395,20 @@ object ConsumerGroupCommand extends Logging {&lt;br/&gt;
     private def getLogEndOffsets(topicPartitions: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, LogOffsetResult&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
       val offsets = getConsumer.endOffsets(topicPartitions.asJava)&lt;br/&gt;
       topicPartitions.map { topicPartition =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logEndOffset = offsets.get(topicPartition)&lt;/li&gt;
	&lt;li&gt;topicPartition -&amp;gt; LogOffsetResult.LogOffset(logEndOffset)&lt;br/&gt;
+        Option(offsets.get(topicPartition)) match 
{
+          case Some(logEndOffset) =&amp;gt; topicPartition -&amp;gt; LogOffsetResult.LogOffset(logEndOffset)
+          case _ =&amp;gt; topicPartition -&amp;gt; LogOffsetResult.Unknown
+        }
&lt;p&gt;       }.toMap&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private def getLogStartOffsets(topicPartitions: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, LogOffsetResult&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
       val offsets = getConsumer.beginningOffsets(topicPartitions.asJava)&lt;br/&gt;
       topicPartitions.map { topicPartition =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logStartOffset = offsets.get(topicPartition)&lt;/li&gt;
	&lt;li&gt;topicPartition -&amp;gt; LogOffsetResult.LogOffset(logStartOffset)&lt;br/&gt;
+        Option(offsets.get(topicPartition)) match 
{
+          case Some(logStartOffset) =&amp;gt; topicPartition -&amp;gt; LogOffsetResult.LogOffset(logStartOffset)
+          case _ =&amp;gt; topicPartition -&amp;gt; LogOffsetResult.Unknown
+        }
&lt;p&gt;       }.toMap&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;






&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13172491">KAFKA-7170</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 10 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3ur5b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>