<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-350] Enable message replication in the presence of controlled failures</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-350</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-46&quot; title=&quot;Commit thread, ReplicaFetcherThread for intra-cluster replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-46&quot;&gt;&lt;del&gt;KAFKA-46&lt;/del&gt;&lt;/a&gt; introduced message replication feature in the absence of server failures. This JIRA will improve the log recovery logic and fix other bugs to enable message replication to happen in the presence of controlled server failures&lt;/p&gt;</description>
                <environment></environment>
        <key id="12558226">KAFKA-350</key>
            <summary>Enable message replication in the presence of controlled failures</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nehanarkhede">Neha Narkhede</assignee>
                                    <reporter username="nehanarkhede">Neha Narkhede</reporter>
                        <labels>
                    </labels>
                <created>Sat, 26 May 2012 02:22:52 +0000</created>
                <updated>Tue, 24 Jul 2012 18:13:32 +0000</updated>
                            <resolved>Tue, 24 Jul 2012 18:13:32 +0000</resolved>
                                    <version>0.8.0</version>
                                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13287007" author="nehanarkhede" created="Thu, 31 May 2012 22:28:34 +0000"  >&lt;p&gt;Handle the logic of deleting segments that have start offset &amp;gt; highwatermark during log recovery. &lt;/p&gt;</comment>
                            <comment id="13415643" author="nehanarkhede" created="Mon, 16 Jul 2012 21:16:28 +0000"  >&lt;p&gt;This patch contains fixes to bugs in various components to make message replication work in the presence of controlled server failures. The system test under system_test/single_host_multiple_brokers passes with server failures enabled. &lt;/p&gt;

&lt;p&gt;This patch contains the following changes -&lt;/p&gt;

&lt;p&gt;1. Topic metadata request bug&lt;/p&gt;

&lt;p&gt;1.1. While responding to a topic metadata request bug, the server uses the AdminUtils to query ZK for the host-port info for the leader and other replicas. However, it can happen that one of the brokers in the replica is offline and hence its broker registration in ZK is unavailable. Since the system test simulates exactly this scenario, the server&apos;s request handler thread was exiting due to a NoNodeException while reading the /brokers/ids/&lt;span class=&quot;error&quot;&gt;&amp;#91;id&amp;#93;&lt;/span&gt; path. The general problem seems to be handling error codes correctly in the request handlers and sending them to the client. I think once &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-402&quot; title=&quot;Adding handling various kind of exception support at server side&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-402&quot;&gt;&lt;del&gt;KAFKA-402&lt;/del&gt;&lt;/a&gt; is resolved, all error codes will be handled cleanly by the server. For now, I&apos;ve modified topic metadata response to have an error code per topic as well as per partition. So, if the leader is not available for some partitions, it will set the LeaderNotAvailable error code at the partition level. If some other replica is not available, it will set the ReplicaNotAvailable error code in the response instead of throwing an exception and exiting.&lt;br/&gt;
1.2. On the producer side, it fails the produce request retry if the leader for that partition is not available. It logs all other kinds of errors and ignores them. &lt;br/&gt;
1.3. Changed some unit tests in AdminTest to elect a leader before making topic metadata requests, so that the test passes.&lt;br/&gt;
1.4. Fixed another bug in the deserialization of the topic metadata response that read the versionId and errorCode in incorrect order.&lt;br/&gt;
1.5. In DefaultEventHandler, during a retry, it relied on the topic cache inside BrokerPartitionInfo for refreshing topic metadata. However, if the topic cache is empty (when no previous send request has succeeded), it doesn&apos;t end up refreshing the metadata for the required topics in the produce request. Fixed this by passing the list of topics explicitly in the call to updateInfo()&lt;br/&gt;
1.6. In general, it seems like a good idea to have a global error code for the entire response (to handle global errors like illegal request format), then a per topic error code (to handle error codes like UnknownTopic), a per-partition error code (to handle partition-level error codes like LeaderNotAvailable) and also a per-replica error code (to handle ReplicaNotAvailable). Jun had a good suggestion about the format of TopicMetadata response. I would like to file another JIRA to improve the format of topic metadata request.&lt;/p&gt;

&lt;p&gt;2. Bug in TestUtils waitUntilLeaderIsElected was signalling a condition object without acquiring the corresponding lock. This error message was probably getting masked since we had turned off ERROR log4j level for unit tests. Fixed this.&lt;/p&gt;

&lt;p&gt;3. Log recovery&lt;br/&gt;
3.1. Removed the highWatermark() API in Log.scala, since we used &#8220;highwatermark&#8221; to indicate the offset of the latest message flushed to disk. This was causing the server to prevent the replicas from fetching unflushed data on the leader. With replication, we use highwatermark to denote the offset of the latest committed message. I removed all references to the older API (highWatermark). To remain consistent with setHW, I added an API called getHW. As I understand it, these APIs will be refactored/renamed to match conventions, when KAFKA354 is resolved&lt;br/&gt;
3.2. To handle truncating a log segment, I added a truncateUpto API that takes in the checkpointed highwatermark value for that partition,  computes the correct end offset for that log segment and truncates data after the computed end offset&lt;br/&gt;
3.3. Improved log recovery to delete segments that have start offset &amp;gt; highwatermark&lt;br/&gt;
3.4. Fixed logEndOffset to return the absolute offset of the last message in the log for that partition.&lt;br/&gt;
3.5. To limit the changes in this patch, it does not move the highwatermarks for all partitions to a single file. This will be done as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-405&quot; title=&quot;Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-405&quot;&gt;&lt;del&gt;KAFKA-405&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
3.6. Added a LogRecoveryTest to test recovery of log segments with and without failures&lt;/p&gt;

&lt;p&gt;4. Config options&lt;br/&gt;
4.1. We might want to revisit the defaults for all config options. For example, the isr.keep.in.sync.time.ms defaulted to 30s which seems way too optimistic.  While running the system tests, most messages timed out since the isr.keep.in.sync.time.ms and the frequency of bouncing the replicas was also roughly 30s. The socket timeout for the producer also defaults to 30s which seems very long to block a producer for.&lt;br/&gt;
4.2.  The socket.timeout.ms should probably be set to producer.request.ack.timeout.ms, if it is a non-negative value. If producer.request.ack.timeout.ms  = -1, socket.timeout.ms should probably default to a meaningful value. While running the system test, I observed that the socket timeout was 30s and the producer.request.ack.timeout.ms was -1, which means that the producer would always block for 30s if the server failed to send back an ACK. Since the frequency of bouncing the brokers was also 30s, most produce requests were timing out.&lt;/p&gt;

&lt;p&gt;5. Socket server bugs&lt;br/&gt;
5.1. A bug in processNewResponses() causes the SocketServer to process responses inspite of it going through a shutdown. It probably makes sense to let outstanding requests timeout and shutdown immediately&lt;br/&gt;
5.2. Another bug in processNewResponses() causes it to go in an infinite loop when the selection key processing throws an exception. It failed to move to the next key in this case. I fixed it by moving the next key processing in the finally block.&lt;/p&gt;

&lt;p&gt;6. Moved the zookeeper client connection from startup() API in KafkaZookeeper to startup() API  in KafkaServer.scala. This is because the ReplicaManager is instantiated right after KafkaZookeeper and was passed in the zkclient object created by KafkaZookeeper. Since KafkaZookeeper started the zkclient only in startup() API, ReplicaManager&apos;s API&apos;s got NPE while trying to access the passed-in zkclient. With the fix, we can create the zookeeper client connection once in the KafkaServer startup, pass it around and tear it down in the shutdown API of KafkaServer. &lt;/p&gt;

&lt;p&gt;7. System testing&lt;br/&gt;
7.1. Hacked ProducerPerformance to turn off forceful use of the async producer. I guess ProducerPerformance has grown over time into a complex blob of if-else statements. Will file another JIRA to refactor it and fix it so that all config options work well with both sync and async producer.&lt;br/&gt;
7.2. Added producer ACK config options to ProducerPerformance. Right now, they are hardcoded. I&apos;m hoping this can be fixed with the above JIRA. &lt;br/&gt;
7.3. The single_host_multiple_brokers system test needs to be changed after &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-353&quot; title=&quot;tie producer-side ack with high watermark and progress of replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-353&quot;&gt;&lt;del&gt;KAFKA-353&lt;/del&gt;&lt;/a&gt;. Basically, it needs to count the successfully sent messages correctly. Right now, there is no good way to do this in the script. One way is to have the system test script grep the logs to find the successfully ACKed producer requests. To get it working for now, hacked it to use sync producer. Hence, before these issues are fixed it will be pretty slow.&lt;br/&gt;
7.4. Improved the system test to start the console consumer at the very end of the test for verification&lt;/p&gt;

&lt;p&gt;8. Another thing that can be fixed in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-402&quot; title=&quot;Adding handling various kind of exception support at server side&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-402&quot;&gt;&lt;del&gt;KAFKA-402&lt;/del&gt;&lt;/a&gt; is the following -&lt;br/&gt;
8.1. When a broker gets a request for a partition for which it is not the leader, it should sent back a response with an error code immediately. Right now, I see a produce request timing out in LogRecoveryTest since the producer never gets a response. It doesn&apos;t break the test since the producer retries, but it is adding unnecessary delay.&lt;br/&gt;
8.2. In quite a few places, we log an error and then retry, or log an error which is actually meant to be a warning. Due to this, it is hard to spot real errors during testing. We can probably fix this too as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-402&quot; title=&quot;Adding handling various kind of exception support at server side&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-402&quot;&gt;&lt;del&gt;KAFKA-402&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;8. Found a NPE while running ReplicaFetchTest. Would like to file a bug for it&lt;/p&gt;

&lt;p&gt;9. Controller bugs &lt;br/&gt;
9.1. Unit tests fail intermittently due to NoSuchElementException thrown by testZKSendWithDeadBroker() in KafkaController. Due to this, the zookeeper server doesn&apos;t shut down and rest of the unit tests fail. The root cause is absence of a broker id as a key in the java map. I think Scala maps should be used as it forces the user to handle invalid values cleanly through Option variables.&lt;br/&gt;
9.2. ControllerBasicTest tests throw tons of zookeeper warnings that complain a client not cleanly closing a zookeeper session. The sessions are forcefully closed only when the zookeeper server is shutdown. These warnings should be paid attention to and fixed. LogTest is also throwing similar zk warnings.&lt;br/&gt;
9.3. Since controller bugs were not in the critical path to getting replication-with-failures to work, I&apos;d like to file another JIRA to fix it.&lt;/p&gt;

&lt;p&gt;If the above future work sounds good, I will go ahead and file the JIRAs&lt;/p&gt;</comment>
                            <comment id="13418065" author="jkreps" created="Thu, 19 Jul 2012 05:03:36 +0000"  >&lt;p&gt;This is a pretty hard to review due to the large number of changes and also because I don&apos;t know some of this code well.&lt;/p&gt;

&lt;p&gt;A lot of things like bad logging/naming that I think you could probably catch just perusing the diff.&lt;/p&gt;

&lt;p&gt;Log:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Log should not know about hw right? We seem to be adding more hw stuff there?&lt;/li&gt;
	&lt;li&gt;This adds a getHW() that just returns a private val, why not make the val public? Please fix these. Regardless of cleanup being done get/set methods have been against the style guide for a long time, lets not add more. Ditto getEndOffset() which in addition to being a getter is inconsistent with Log.logEndOffset&lt;/li&gt;
	&lt;li&gt;There is debug statement in a for loop in Log.scala that needs to be removed&lt;/li&gt;
	&lt;li&gt;I don&apos;t understand the difference between nextAppendOffset and logEndOffset. Can you make it clear in the javadoc and explain on why we need both of these. Our public interface to Log is getting incredibly complex, which is really sad so I think we should really think through deeply what is added here and why.&lt;/li&gt;
	&lt;li&gt;The javadoc on line 138 of Log.scala doesn&apos;t match the style of javadoc for the preceeding 5 variables.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does making isr.keep.in.sync.time.ms more stringent actually make sense? 10 seconds is pretty tight. I think what you are saying is that every server bounce will introduce 30 seconds of latency. But I think that is kind of a weakness of our design. If we lower that timeout we may just get spurious dropped replicas, no?&lt;/li&gt;
	&lt;li&gt;Can we change the name of isr.keep.in.sync.time.ms to replica.max.lag.time.ms?&lt;/li&gt;
	&lt;li&gt;Good point about the socket timeouts. We can&apos;t set socket timeout equal to request timeout, though, as there may be a large network latency. I recommend we just default the socket timeout to something large (like 10x the request timeout), and throw an exception if it is less than the request timeout (since that is certainly wrong). I don&apos;t think we should be using the socket timeout except as an emergency escape for a totally hung broker now that we have the nice per-request timeout.&lt;/li&gt;
	&lt;li&gt;Can we change producer.request.ack.timeout.ms to producer.request.timeout.ms so it is more intuitive? I don&apos;t think the word &quot;ack&quot; is going to be self-explanatory to users.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;SocketServer.scala&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Please remove: info(&quot;Shut down acceptor completed&quot;)&lt;/li&gt;
	&lt;li&gt;Is there a reason to add the port into the thread name? That seems extremely odd...is it to simplify testing where there are multiple servers on a machine?&lt;/li&gt;
	&lt;li&gt;Why is it a bug for processNewResponses() to happen while a shutdown is occuring. I don&apos;t think that is a bug. That is called in the event loop. It is the loop that should stop, no? Is there any ill effect of this?&lt;/li&gt;
	&lt;li&gt;Good catch on the infinite loop&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;System testing&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I think we should fix the performance test hacks. The performance tool is critical. I have watched this play out before. No one ever budgets time for making the performance test stuff usable and then it just gets re-written umpteen times and never does what is needed. Most of these are just a matter of some basic cleanup and adding options. Let&apos;s work clean.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;AdminUtils&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I don&apos;t understand the change in getTopicMetaDataFromZK&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Replica.scala&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Can you remove trace(&quot;Returning leader replica for leader &quot; + leaderReplicaId) unless you think it is of value going forward&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;ErrorMapping.scala&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;getMaxErrorCodeValue - seems to be recomputed for each TopicMetadata. Let&apos;s get rid of this, I don&apos;t think we need it. We already have an unknown entry in the mapping, we should use that and get rid of the Utils.getShortInRange&lt;/li&gt;
	&lt;li&gt;If we do want to keep it, fix the name&lt;/li&gt;
	&lt;li&gt;We should really give a base class KafkaException to all exceptions so the client can handle them more easily&lt;/li&gt;
	&lt;li&gt;Instead of having the client get an IllegalArgumentException we should just throw new KafkaException(&quot;Unknown server error (error code &quot; + code + &quot;)&quot;)&lt;/li&gt;
	&lt;li&gt;The file NoLeaderForPartitionException seems to be empty now, I think you meant to delete it&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;ConsoleConsumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;What does moving those things into a finally block do? We just caught all exceptions...&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;FileMessageSet&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;You added more log spam. Please format it so it is intelligible to someone not working on the code or remove: debug(&quot;flush size:&quot; + sizeInBytes())&lt;/li&gt;
	&lt;li&gt;Ditto info(&quot;recover upto size:&quot; + sizeInBytes())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;BrokerPartitionInfos is a really weird class&lt;/p&gt;

&lt;p&gt;DefaultEventHandler&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;This seems to have grown into a big glump of proceedural logic.&lt;/li&gt;
	&lt;li&gt;Inconsistent spacing of parens should be cleaned up&lt;/li&gt;
	&lt;li&gt;partitionAndCollate is extemely complex&lt;/li&gt;
	&lt;li&gt;Option[Map[Int, Map[(String, Int), Seq[ProducerData&lt;span class=&quot;error&quot;&gt;&amp;#91;K,Message&amp;#93;&lt;/span&gt;]]]]&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;ZkUtils&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LeaderExists class needs a name that is a noun&lt;/li&gt;
	&lt;li&gt;Also we have a class with the same name in TopicMetadata&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I really like TestUtils.waitUntilLeaderIsElected. God bless you for not adding sleeps. We should consider repeating this pattern for other cases like this.&lt;/p&gt;

&lt;p&gt;ZooKeeperTestHarness.scala&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Can we replace the thread.sleep with a waitUntilZkIsUp call?&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="13419280" author="guozhang" created="Fri, 20 Jul 2012 15:51:36 +0000"  >&lt;p&gt;Would it be better to have a general read/write formatting for boolean, like writeBoolean/readBoolean just like writeShortString/readShortString? Then we do not need to specify, for example, in TopicMetadata:&lt;/p&gt;

&lt;p&gt;sealed trait LeaderRequest &lt;/p&gt;
{ def requestId: Byte }
&lt;p&gt;case object LeaderExists extends LeaderRequest &lt;/p&gt;
{ val requestId: Byte = 1 }
&lt;p&gt;case object LeaderDoesNotExist extends LeaderRequest &lt;/p&gt;
{ val requestId: Byte = 0 }


&lt;p&gt;which would need to be done for every request/response that has a boolean.&lt;/p&gt;</comment>
                            <comment id="13419537" author="nehanarkhede" created="Fri, 20 Jul 2012 20:34:02 +0000"  >&lt;p&gt;Thanks for the review, Jay ! Here is another patch fixing almost all review comments -&lt;/p&gt;

&lt;p&gt;1. Log&lt;br/&gt;
1. Refactoring of the hw logic is part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-405&quot; title=&quot;Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-405&quot;&gt;&lt;del&gt;KAFKA-405&lt;/del&gt;&lt;/a&gt;. It is moved to a new file HighwaterMarkCheckpoint and is controlled by the ReplicaManager. The Log does not and should not know about high watermarks.&lt;br/&gt;
2. Getter/setter for hw is removed as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-405&quot; title=&quot;Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-405&quot;&gt;&lt;del&gt;KAFKA-405&lt;/del&gt;&lt;/a&gt; anyways.&lt;br/&gt;
3. Agree with you on the weak public interface, especially Log needs a cleanup. I think you attempted that as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-371&quot; title=&quot;Creating topic of empty string puts broker in a bad state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-371&quot;&gt;&lt;del&gt;KAFKA-371&lt;/del&gt;&lt;/a&gt;. I&apos;ve cleaned up quite a few things as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-405&quot; title=&quot;Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-405&quot;&gt;&lt;del&gt;KAFKA-405&lt;/del&gt;&lt;/a&gt; and this patch. Nevertheless, fixed the nextAppendOffset as part of this patch. It is not required when we have logEndOffset. Also, removed the getEndOffset from FileMessageSet. Added endOffset() API to the LogSegment in addition to a size() API. This is useful during truncation of the log based on high watermark.&lt;br/&gt;
4. Fixed the javadoc and removed the debug statement.&lt;/p&gt;

&lt;p&gt;2. Config options&lt;br/&gt;
1. The isr.keep.in.sync.time.ms set to 10 seconds is also very lax. A healthy follower should be able to catch up in 100s of milliseconds even with a lot of topics, with a worst case of maybe 4-5 seconds. We will know the latency better when we run some large scale tests. But yeah, the issue with the system test is independent of what the right value should be. I was just explaining how I discovered this issue. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
2. Good point about renaming it to replica.max.lag.time.ms. Also changed isr.keep.in.sync.bytes to replica.max.lag.bytes.&lt;br/&gt;
3. Since the producer does a blocking read on the socket, the socket timeout cannot be greater than the request timeout. If it is, then the request timeout guarantee would be violated, no ?&lt;/p&gt;

&lt;p&gt;3. SocketServer.scala&lt;br/&gt;
1. Removed info log statement&lt;br/&gt;
2. Yes, wanted to simplify testing/debugging (thread dumps) when there are multiple servers on one machine. Not sure if this is the best way to do that.&lt;br/&gt;
3. processNewResponses() doesn&apos;t have to process outstanding requests during shutdown. It can shutdown by ignoring them and those requests will timeout anyways. But yes, good point about the event loop doing it instead. Fixed it.&lt;/p&gt;

&lt;p&gt;4. System testing&lt;br/&gt;
1. Fixed the hacky change. I still need ProducerPerformance needs a complete redo. Filed bug &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-408&quot; title=&quot;ProducerPerformance does not work with all producer config options&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-408&quot;&gt;&lt;del&gt;KAFKA-408&lt;/del&gt;&lt;/a&gt; to do that.&lt;/p&gt;

&lt;p&gt;5. AdminUtils&lt;br/&gt;
1. Need this change to return appropriate error codes for topic metadata request. Without this, all produce requests are timing out while fetching metadata, since in the system test, at any point of time, one replica is always down.&lt;/p&gt;

&lt;p&gt;6. Partition.scala&lt;br/&gt;
1. Removed the trace statement.&lt;/p&gt;

&lt;p&gt;7. ErrorMapping&lt;br/&gt;
1. Removed getMaxErrorCode and its usage from ErrorMapping&lt;br/&gt;
2. Good point. Introduced a new class KafkaException and converted IllegalStateException and IllegalArgumentException to it.&lt;br/&gt;
3. NoLeaderForPartitionException is in fact marked as deleted in the patch&lt;/p&gt;

&lt;p&gt;8. ConsoleConsumer&lt;br/&gt;
1. Good point. The finally block there didn&apos;t really make any sense.&lt;/p&gt;

&lt;p&gt;9. FileMessageSet&lt;br/&gt;
1. I haven&apos;t added the log statements in this patch, we always had it. &lt;br/&gt;
2. The purpose of adding that was help with debugging producer side queue full exceptions. We turned on DEBUG to see what the server side flush sizes and latencies were. I think these statements were added when we didn&apos;t have an ability to monitor these. However, whoever added the log flush monitoring maybe forgot to remove these statements. I removed it in this patch.&lt;br/&gt;
3. Removed the &#8220;recover upto&#8221; log statement too&lt;/p&gt;

&lt;p&gt;10. DefaultEventHandler&lt;br/&gt;
1. Agree with the unwieldly procedural code. Filed bug &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-409&quot; title=&quot;Refactor DefaultEventHandler &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-409&quot;&gt;&lt;del&gt;KAFKA-409&lt;/del&gt;&lt;/a&gt; to clean this up. &lt;/p&gt;

&lt;p&gt;11. ZkUtils&lt;br/&gt;
1. Renamed LeaderExists -&amp;gt; LeaderExistsListener&lt;/p&gt;

&lt;p&gt;12. ZookeeperTestHarness&lt;br/&gt;
1. The sleep is actually not required. The EmbeddedZookeeper constructor returns only after the zk server is completely started.&lt;/p&gt;</comment>
                            <comment id="13420734" author="junrao" created="Mon, 23 Jul 2012 16:03:25 +0000"  >&lt;p&gt;Thanks for patch v2. Some comments:&lt;/p&gt;

&lt;p&gt;20. PartitionMetadata: Could we make getLeaderRequest() private?&lt;/p&gt;

&lt;p&gt;21. DefaultEventHandler.partitionAndCollate(): Is it necessary for this method to return an Option? It seems that if this method hits any exception, it&apos;s simpler to just pass on the exception and let the caller deal with it. &lt;/p&gt;

&lt;p&gt;22. LogSegment: Ideally this class should only deal with offsets local to the segment. Global offsets are only used at the Log level. So it&apos;s probably better to use local offset in the input of truncateUpto(). Similarly, we probably don&apos;t need endOffset since it returns global offset. If we do want to keep it, it probably should be named globalEndOffset.&lt;/p&gt;

&lt;p&gt;23. Log: Since we removed the HW check in FileMessageSet.read. We will need to add a guard in Log.read() so that we only expose messages up to HW to the consumer.&lt;/p&gt;

&lt;p&gt;24. LogManager: remove unused import&lt;/p&gt;

&lt;p&gt;25. ControllerChannelManager.removeBroker: the error message is not correct since this method is called in places other than shutdown too.&lt;/p&gt;

&lt;p&gt;26. system_test/single_host_multi_brokers/bin/run-test.sh: We should remove comments saying &quot;If &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt; is fixed&quot;.&lt;/p&gt;

&lt;p&gt;27. Unit tests pass on my desktop but fail on my laptop at the following test consistently. Without the patch, unit tests pass on my laptop too. This seems to be due to the change in ZooKeeperTestHarness.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;info&amp;#93;&lt;/span&gt; Test Starting: testFetcher(kafka.consumer.FetcherTest)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; Test Failed: testFetcher(kafka.consumer.FetcherTest)&lt;br/&gt;
org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 2000&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.&amp;lt;init&amp;gt;(ZkClient.java:98)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.&amp;lt;init&amp;gt;(ZkClient.java:84)&lt;br/&gt;
	at kafka.zk.ZooKeeperTestHarness$class.setUp(ZooKeeperTestHarness.scala:31)&lt;br/&gt;
	at kafka.consumer.FetcherTest.kafka$integration$KafkaServerTestHarness$$super$setUp(FetcherTest.scala:35)&lt;br/&gt;
	at kafka.integration.KafkaServerTestHarness$class.setUp(KafkaServerTestHarness.scala:35)&lt;br/&gt;
	at kafka.consumer.FetcherTest.setUp(FetcherTest.scala:57)&lt;/p&gt;</comment>
                            <comment id="13420756" author="jkreps" created="Mon, 23 Jul 2012 16:31:05 +0000"  >&lt;p&gt;1.1, 1.2, 2.1, 2.2 Awesome!&lt;/p&gt;

&lt;p&gt;2.3 Hmm, well so but that doesn&apos;t make sense because we would always pop the socket timeout and fully close out the connection, which is not good. Basically the socket timeout would ALWAYS go off before the server-side request processing timeout due to the network latency both ways. I think that timeout should just be for &quot;emergencies&quot;. I agree it is a bit of a muddle now that we have two.&lt;/p&gt;

&lt;p&gt;7.2 Hmm, but that changed ALL IllegalStateExceptions to KafkaException. IllegalStateException is used to say &quot;this should not be possible&quot;. For example in SocketServer those checks are in the form of assertions. I like the idea of KafkaException, and I think we should make all the other exceptions extend it to ease error handling on the client, but I don&apos;t think we should get rid of IllegalStateException or IllegalArgumentException, they are informative.&lt;/p&gt;</comment>
                            <comment id="13421118" author="nehanarkhede" created="Tue, 24 Jul 2012 02:29:17 +0000"  >&lt;p&gt;Thanks for the review !&lt;/p&gt;

&lt;p&gt;Jun&apos;s comments -&lt;/p&gt;

&lt;p&gt;20. Made getLeaderRequest and getLogSegmentMetadataRequest private&lt;br/&gt;
21. partitionAndCollate() is used by dispatchSerializedData() API. The contract of dispatchSerializedData() is to return the list of messages that were not sent successfully so that they can be used on the next retry. Now, if partitionAndCollate() fails, we need dispatchSerializedData to return the entire input list of messages. If something else fails after partitionAndCollate, we need to return a filtered list of messages back. To handle this, partitionAndCollate returns an Option. I agree that this class has very complex code that needs a cleanup. Have filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-409&quot; title=&quot;Refactor DefaultEventHandler &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-409&quot;&gt;&lt;del&gt;KAFKA-409&lt;/del&gt;&lt;/a&gt; for that.&lt;br/&gt;
22. We need to know the absolute end offset of the segment for truncation during make follower state change. Have changed the name of the API to absoluteEndOffset. &lt;br/&gt;
23. We have &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-376&quot; title=&quot;expose different data to fetch requests from the follower replicas and consumer clients&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-376&quot;&gt;&lt;del&gt;KAFKA-376&lt;/del&gt;&lt;/a&gt; filed to ensure that the consumer sees data only after the HW. This patch exposes data upto log end offset to replicas as well as consumers. So FileMessageSet.read() exposes all data upto log end offset.&lt;br/&gt;
24. Removed unused import.&lt;br/&gt;
25. Hmm, fixed the error message&lt;br/&gt;
26. Removed the TODOs related to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-350&quot; title=&quot;Enable message replication in the presence of controlled failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-350&quot;&gt;&lt;del&gt;KAFKA-350&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
27. Introduced the connection timeout and session timeout variables in the test harness and set it to 6s. &lt;/p&gt;

&lt;p&gt;Jay&apos;s comments -&lt;/p&gt;

&lt;p&gt;2.3 So we talked about this offline, stating it here so that others can follow. There are 2 choices here -&lt;br/&gt;
2.3.1. One is to keep request timeout = socket timeout. This would make us fully dependent on the socket for the timeout. So, any requests that took longer than that on the server for some reason (GC, bugs etc), would throw SocketTimeoutException on the producer client. The only downside to this is that the producer client wouldn&apos;t know why it failed or whether it got written to 0 or 1 or n replicas. Al though, this should be very rare and in the normal case, the server process the request and be able to send the response within the timeout. This is assuming the timeout value was set counting the network delay as well. So if the request is travelling cross-colo, it is expected that you set the timeout keeping in mind the cross-colo network latency. &lt;br/&gt;
2.3.2. The second choice is to set the socket timeout &amp;gt;&amp;gt; request timeout. This would ensure that in some rare failure cases on the server, the producer client would still be able to get back a response (most of the times) with a descriptive error message explaining the cause of the failure. However, it would also mean that under some failures like (server side kernel bug crashing the server etc), the timeout would actually be the socket timeout, which is usually set to a much higher value. This can confuse users who might expect the timeout to be the request timeout. Also, having two different timeouts also seems to complicate the guarantees provided by Kafka &lt;br/&gt;
2.3.3. I personally think option 1 is simpler and provides no worse guarantees than option 2. This patch just sets the socket timeout to be the request timeout. &lt;/p&gt;

&lt;p&gt;2.4 Yeah, I think differentiating between &#8220;this should not be possible&#8221; and &#8220;this should not be possible in Kafka&#8221; is a little tricky. On one hand, it seems nicer to know that any exception thrown by Kafka will either be KafkaException or some exception that extends KafkaException. On the other hand, some states are just impossible and must be treated like assertions, for example, a socketchannel key that is not readable or writable or even valid. And in such cases, it might be slightly more convenient to have IllegalStateException. And I&apos;m not sure I know the right answer here. I took a pass over all the IllegalStateException usages and converted the ones I think should be KafkaException, but I might not have done it in the best way possible.&lt;/p&gt;</comment>
                            <comment id="13421235" author="junrao" created="Tue, 24 Jul 2012 07:27:19 +0000"  >&lt;p&gt;Thanks for patch v3. &lt;/p&gt;

&lt;p&gt;For 21, my point is that the exceptions that can be thrown in partitionAndCollate() are non-recoverable and therefore retries won&apos;t help. partitionAndCollate() won&apos;t throw NoBrokersForPartitionException since only BrokerPartitionInfo.updateInfo can throw such an exception and updateInfo is not called here. partitionAndCollate() throws InvalidPartitionException that indicates a wrong partition of a topic. If we just retry, we will hit the same exception and fail again. It&apos;s simpler to just throw an exception and treat it as a failed case. Ditto for other exceptions that partitionAndCollate() may throw.&lt;/p&gt;

&lt;p&gt;A few new comments:&lt;br/&gt;
30. SyncProducerConfig.requestTimeoutMs: We should make sure that the value is a positive integer and change the comment accordingly.&lt;/p&gt;

&lt;p&gt;31. IteraterTemplate: The two KafkaExceptions are better reverted to IllegalStateException.&lt;/p&gt;

&lt;p&gt;32. ProducerPerformance: We should remove socketTimeoutMsOpt.&lt;/p&gt;

&lt;p&gt;33. SocketServer: no need to import illegalStateException&lt;/p&gt;

&lt;p&gt;34. I got the following exception when running system_test/single_host_multi_brokers/bin/run-test.sh&lt;/p&gt;

&lt;p&gt;2012-07-24 00:22:51 cleaning up kafka server log/data dir&lt;br/&gt;
2012-07-24 00:22:53 starting zookeeper&lt;br/&gt;
2012-07-24 00:22:55 starting cluster&lt;br/&gt;
2012-07-24 00:22:55 starting kafka server&lt;br/&gt;
2012-07-24 00:22:55   -&amp;gt; kafka_pids&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;: 75282&lt;br/&gt;
2012-07-24 00:22:55 starting kafka server&lt;br/&gt;
2012-07-24 00:22:55   -&amp;gt; kafka_pids&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;: 75286&lt;br/&gt;
2012-07-24 00:22:55 starting kafka server&lt;br/&gt;
2012-07-24 00:22:55   -&amp;gt; kafka_pids&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;: 75291&lt;br/&gt;
2012-07-24 00:22:57 creating topic &lt;span class=&quot;error&quot;&gt;&amp;#91;mytest&amp;#93;&lt;/span&gt; on &lt;span class=&quot;error&quot;&gt;&amp;#91;localhost:2181&amp;#93;&lt;/span&gt;&lt;br/&gt;
creation failed because of org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids&lt;br/&gt;
org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids&lt;br/&gt;
	at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409)&lt;br/&gt;
	at kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:363)&lt;br/&gt;
	at kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:80)&lt;br/&gt;
	at kafka.admin.CreateTopicCommand$.createTopic(CreateTopicCommand.scala:86)&lt;br/&gt;
	at kafka.admin.CreateTopicCommand$.main(CreateTopicCommand.scala:73)&lt;br/&gt;
	at kafka.admin.CreateTopicCommand.main(CreateTopicCommand.scala)&lt;br/&gt;
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids&lt;br/&gt;
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)&lt;br/&gt;
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)&lt;br/&gt;
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249)&lt;br/&gt;
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)&lt;br/&gt;
	... 7 more&lt;/p&gt;</comment>
                            <comment id="13421519" author="nehanarkhede" created="Tue, 24 Jul 2012 16:31:46 +0000"  >
&lt;p&gt;21. Not really. updateInfo() is not the only API that throws NoBrokersForPartitionException. partitionAndCollate can encounter that exception while calling the getPartitionListForTopic() API.  But you raised a good point here. What partitionAndCollate() should do (given its current design) is to log a retry warning for recoverable exceptions and return None, so that they are included in the retry. For non-recoverable exceptions, it should just throw the exception to the handle API and mark it failed.&lt;/p&gt;

&lt;p&gt;30. Added an error message for non-positive values for request timeout. Al though a value of zero means infinite timeout&lt;br/&gt;
31. I guess so. Changed it back to IllegalStateException&lt;br/&gt;
32. Removed socket timeout related stuff from ProducerPerformance&lt;br/&gt;
33. Removed the import.&lt;br/&gt;
34. Hmm, that can happen if you try to create a topic when the brokers haven&apos;t yet registered in zookeeper. The system tests waits for 2 seconds, which should be enough for couple of brokers to startup in the normal case. I haven&apos;t been able to reproduce this. Its possible that this is something that can be fixed in the system test. File a bug if you see it.&lt;/p&gt;</comment>
                            <comment id="13421590" author="junrao" created="Tue, 24 Jul 2012 17:59:48 +0000"  >&lt;p&gt;Thanks for patch v4. We are almost there.&lt;/p&gt;

&lt;p&gt;21. In partitionAndCollate(), UnknownTopicException doesn&apos;t seem to recoverable.&lt;/p&gt;

&lt;p&gt;30. It doesn&apos;t look like the broker handles requestTimeoutMs of 0 through RequestPurgatory. It&apos;s probably simpler if we just require timeout to be positive. If someone wants infinite timeout, MAX_INT can be used instead.&lt;/p&gt;

&lt;p&gt;If those issues are addressed, the patch can be committed without another round of review.&lt;/p&gt;

&lt;p&gt;34. This seems to only happen on my laptop. Will file another jira. &lt;/p&gt;</comment>
                            <comment id="13421601" author="nehanarkhede" created="Tue, 24 Jul 2012 18:12:37 +0000"  >&lt;p&gt;21. It is recoverable when there is auto create topic enabled on the server. Until we get rid of auto create, I think we can keep this. &lt;br/&gt;
30. Changed it to be non-negative and non-zero&lt;/p&gt;

&lt;p&gt;Committing it with these changes.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12536746" name="kafka-350-v1.patch" size="116852" author="nehanarkhede" created="Mon, 16 Jul 2012 23:33:01 +0000"/>
                            <attachment id="12537430" name="kafka-350-v2.patch" size="169370" author="nehanarkhede" created="Sat, 21 Jul 2012 00:26:18 +0000"/>
                            <attachment id="12537640" name="kafka-350-v3.patch" size="170243" author="nehanarkhede" created="Tue, 24 Jul 2012 02:29:17 +0000"/>
                            <attachment id="12537706" name="kafka-350-v4.patch" size="169834" author="nehanarkhede" created="Tue, 24 Jul 2012 16:31:46 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>248182</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 17 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i09m2f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>54008</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>