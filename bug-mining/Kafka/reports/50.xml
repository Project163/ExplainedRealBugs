<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-49] Add acknowledgement to the produce request.</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-49</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Currently, the produce request doesn&apos;t get acknowledged. We need to have a broker send a response to the producer and have the producer wait for the response before sending the next request.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12514686">KAFKA-49</key>
            <summary>Add acknowledgement to the produce request.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="prashanth.menon">Prashanth Menon</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Jul 2011 21:32:21 +0000</created>
                <updated>Thu, 2 May 2013 02:29:54 +0000</updated>
                            <resolved>Mon, 28 May 2012 18:02:45 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="13151039" author="jkreps" created="Wed, 16 Nov 2011 06:03:12 +0000"  >&lt;p&gt;If there are any other producer-related changes coming we should try to batch these together at the same time to avoid having to update clients too often.&lt;/p&gt;</comment>
                            <comment id="13155158" author="prashanth.menon" created="Tue, 22 Nov 2011 14:19:44 +0000"  >&lt;p&gt;So I&apos;ve started a little work on this.  Looks to me like the ProducerRequest is going to need an additional &quot;acknowledge&quot; boolean field (default false) which we send along with the rest of the fields.  On the producer side, there are a couple of options:&lt;/p&gt;

&lt;p&gt;1. We can leave the producer API and have producers (both sync and async) acknowledge all or non of their messages.  This behaviour will be driven by a configuration field or a class parameter.&lt;br/&gt;
2. We add the acknowledgement parameter (default false) to the producer send API for and leave the remaining behaviour the same.  The producer then handles waiting or not waiting for acks on a per-message case.&lt;/p&gt;

&lt;p&gt;I would prefer the second option as it&apos;s simple to do and gives the option to clients.  Waiting on the producing end shouldn&apos;t be an issue.  On the broker side, it becomes easy as we just send a boolean response after handling the request (ditto for the multi-produce request).  &lt;/p&gt;

&lt;p&gt;Did anyone else have any thoughts on this?&lt;/p&gt;</comment>
                            <comment id="13155240" author="junrao" created="Tue, 22 Nov 2011 16:36:51 +0000"  >&lt;p&gt;Prshanth, thanks for getting started on this. I agree with your second approach. Basically, add a new parameter in SyncProducer.send/multisend to indicate whether an ack is needed or not. The high level producer can then set that parameter based on ProducerConfig (probably true for sync mode and false for async mode). &lt;/p&gt;

&lt;p&gt;Another question is what kind of ack does the broker send back. A simple approach is to send back a boolean. Another possibility is to return for each partition in the produce request, the latest offset after the request is served. Some clients could potentially make use of the returned offset.&lt;/p&gt;</comment>
                            <comment id="13155256" author="tgautier" created="Tue, 22 Nov 2011 17:05:28 +0000"  >&lt;p&gt;I second returning the new offset.&lt;/p&gt;</comment>
                            <comment id="13155286" author="jkreps" created="Tue, 22 Nov 2011 17:36:40 +0000"  >&lt;p&gt;+1 for returning the offset&lt;/p&gt;

&lt;p&gt;If we are changing the request format it would also be good to think it through in some detail to get it right since these kinds of API changes are harder to rollout then to make. Some questions:&lt;br/&gt;
1. Are we trying to maintain compatibility for this change? If so we should bump up the request id number and ignore the new fields for the old request id. This is not too hard, but requires a little extra work.&lt;br/&gt;
2. Currently we have ProduceRequest and MultiProducerRequest and FetchRequest and MultiFetchRequest. I recommend we rename MultiProducerRequest to ProduceRequest and delete the existing ProduceRequest. The current ProduceRequest has no advantages over MultiProducerRequest and having both means each change we make has to be done for both. The two variations are just there for historical reasons-&lt;del&gt;originally there was no multi&lt;/del&gt;* version of the requests and we added that later. I recommend we do the same for the FetchRequest/MultiFetchRequest. This will make our lives simpler going forward.&lt;br/&gt;
3. Both of the MultiProducerRequest has the format &lt;span class=&quot;error&quot;&gt;&amp;#91;(topic, partition, messages), (topic, partition, messages), ...&amp;#93;&lt;/span&gt;. This is because it is just a bunch of repeated ProducerRequests. This is really inefficient, though, as a common case is that we are producing a bunch of messages for different partitions under the same topic (i.e. if we are doing the key-based partitioning). It would be better for the format to be [(topic, &lt;span class=&quot;error&quot;&gt;&amp;#91;(partition, messages), ...&amp;#93;&lt;/span&gt;, topic, &lt;span class=&quot;error&quot;&gt;&amp;#91;(partition, messages), ...&amp;#93;&lt;/span&gt;, ...]. This would mean that each topic is only given once.&lt;/p&gt;

&lt;p&gt;I would like to get a quick consensus on the desired format of the produce and fetch requests up front, then we can break this into appropriate sub tasks so we don&apos;t expand the scope of Prasanth&apos;s work too much.&lt;/p&gt;</comment>
                            <comment id="13155307" author="junrao" created="Tue, 22 Nov 2011 18:04:02 +0000"  >&lt;p&gt;Both 2 and 3 make sense. &lt;/p&gt;

&lt;p&gt;If we do this in a bug fix release in 0.7, we probably need to maintain backward compatibility. If we do this as part of the replication work, we probably can make a non-backward compatible change. My preference is the latter.&lt;/p&gt;</comment>
                            <comment id="13155311" author="tgautier" created="Tue, 22 Nov 2011 18:12:09 +0000"  >&lt;p&gt;Generally I think it&apos;s a good idea to have a version embedded into the protocol.  This allows for backwards and forwards compatibility.  In a sense, the request id works as such, so in some sense it&apos;s a matter of semantics, but the only way to identify that there are multiple versions of the same request is to have some kind of external mapping that says id 2 and id 8 are really the same request, just different versions.&lt;/p&gt;

&lt;p&gt;OTOH, if you use a version, you can then have:&lt;/p&gt;

&lt;p&gt;id 2 version 1&lt;br/&gt;
id 2 veriosn 2&lt;/p&gt;

&lt;p&gt;etc. and this is imho easier to manage.&lt;/p&gt;

&lt;p&gt;Usually, the version should in fact be the first value in the protocol, so that you never have formatting issues that lie outside the realm of the versioned data.&lt;/p&gt;

&lt;p&gt;Currently, all requests are preceded by a header, which contains the length of the data.  This is where I would start, we should either strive for:&lt;/p&gt;

&lt;p&gt;version: 2 bytes&lt;br/&gt;
length: 4 bytes&lt;/p&gt;

&lt;p&gt;or &lt;/p&gt;

&lt;p&gt;length: 4 bytes&lt;br/&gt;
version: 2 bytes&lt;/p&gt;

&lt;p&gt;Note that the message request already has a way to represent versions, using the magic field, but honestly I find it a little bit non explicit for my taste.&lt;/p&gt;

&lt;p&gt;I would also include a 64-bit &quot;flags&quot; area that will allow for future flags to be set to indicate various things.&lt;/p&gt;

&lt;p&gt;So, if I were to suggest a standard header for requests and responses it would look like:&lt;/p&gt;

&lt;p&gt;length: 2 bytes&lt;br/&gt;
version: 2 bytes&lt;br/&gt;
reuest id: 2 bytes&lt;br/&gt;
flags: 4 bytes&lt;br/&gt;
payload: n bytes&lt;/p&gt;</comment>
                            <comment id="13155314" author="tgautier" created="Tue, 22 Nov 2011 18:14:53 +0000"  >&lt;p&gt;It could be possible to split the current request id - 2 bytes - into a version and an id field one byte long each.   Assuming there&apos;s not much need for a vocabulary greater than 256 verbs, and versions &amp;gt; 256, this would probably work within the current binary protocol and give backwards compatibility to 0.6 and 0.7 clients...&lt;/p&gt;</comment>
                            <comment id="13155331" author="jkreps" created="Tue, 22 Nov 2011 18:38:50 +0000"  >&lt;p&gt;Hi Taylor, as you say the request id was meant to be the version. However in retrospect I do think I like the idea of separating the request and the version of the request. I agree it would be nice to split this.&lt;/p&gt;

&lt;p&gt;I think the open question here is whether we should try to maintain backwards compatibility for the next major release. It would probably be very convenient for us at code-writing time not to, but is more painful at rollout time.&lt;/p&gt;</comment>
                            <comment id="13155348" author="tgautier" created="Tue, 22 Nov 2011 19:01:57 +0000"  >&lt;p&gt;Well, how many versions do you think you want?  Maybe we could split the request field up into say the first 5 or 6 bits instead of 8 for the versions.&lt;/p&gt;</comment>
                            <comment id="13155687" author="jkreps" created="Wed, 23 Nov 2011 05:09:22 +0000"  >&lt;p&gt;So guys, my thought is the best plan of attack would be fully think through the protocol changes for all the use cases we currently know about and do those all at once (even if the features the new fields support aren&apos;t yet available). This will avoid doing this in lots of little patches that all conflict, and it will make us think things through holistically.&lt;/p&gt;

&lt;p&gt;I created a wiki with some of the outstanding ideas, I am going to move this discussion to the main dev and user lists to get broader feedback. It would be good if people could give their thoughts there so we can try to get these changes right.&lt;/p&gt;</comment>
                            <comment id="13180719" author="nehanarkhede" created="Thu, 5 Jan 2012 19:09:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-240&quot; title=&quot;implement new producer and consumer request format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-240&quot;&gt;&lt;del&gt;KAFKA-240&lt;/del&gt;&lt;/a&gt; implements the new wire format for producer and consumer. Since this JIRA requires the new format, it depends on &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-240&quot; title=&quot;implement new producer and consumer request format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-240&quot;&gt;&lt;del&gt;KAFKA-240&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13220436" author="nehanarkhede" created="Thu, 1 Mar 2012 22:33:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-239&quot; title=&quot;Wire existing producer and consumer to use the new ZK data structure&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-239&quot;&gt;&lt;del&gt;KAFKA-239&lt;/del&gt;&lt;/a&gt; is complete and &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-240&quot; title=&quot;implement new producer and consumer request format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-240&quot;&gt;&lt;del&gt;KAFKA-240&lt;/del&gt;&lt;/a&gt; is almost there. &lt;/p&gt;

&lt;p&gt;Prashanth, in your comment above, you&apos;ve mentioned you&apos;ve started work on this. If so, mind assigning this JIRA to yourself ? This looks like the next JIRA to work on after &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-240&quot; title=&quot;implement new producer and consumer request format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-240&quot;&gt;&lt;del&gt;KAFKA-240&lt;/del&gt;&lt;/a&gt; is in. (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-50?focusedCommentId=13180712&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13180712&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-50?focusedCommentId=13180712&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13180712&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="13220458" author="prashanth.menon" created="Thu, 1 Mar 2012 22:53:29 +0000"  >&lt;p&gt;Sure, but I actually can&apos;t change assignments.  Could be a privileges issue?&lt;/p&gt;</comment>
                            <comment id="13220503" author="nehanarkhede" created="Thu, 1 Mar 2012 23:56:17 +0000"  >&lt;p&gt;Looks like you didn&apos;t exist in the Apache Kafka JIRA list. Added you there, also assigned this JIRA to you&lt;/p&gt;</comment>
                            <comment id="13222156" author="prashanth.menon" created="Mon, 5 Mar 2012 03:51:07 +0000"  >&lt;p&gt;Hey all.  Getting the acknowledgements done was fairly straightforward - I&apos;ve &quot;todo-ed&quot; actually waiting for replica acknowledgements as part of kafka-44 when it introduces true replicas and partitions.  Having the sync producer wait for a response when it requires acknowledgements was trivial.  My question is what to do with the async producer.  Was the intent to perform some logic in the default event handler, or to expect clients to write custom event handlers that deal errors?  Should we bubble up the response to the producer level?&lt;/p&gt;</comment>
                            <comment id="13222700" author="junrao" created="Mon, 5 Mar 2012 22:43:45 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;That&apos;s a good question. Initially, I was just thinking that for async producers, if we get an error during send (after retries), we will just log the error without telling the client. Currently, the event handler is not really extensible. I can image that we add some kind of callback to return those errors. The question is what will the client do on those errors. Will it resend? If so, we will need to pass the failed requests through callback too. I am curious about how other messaging systems like activeMQ do in the async mode.&lt;/p&gt;</comment>
                            <comment id="13222752" author="prashanth.menon" created="Mon, 5 Mar 2012 23:20:01 +0000"  >&lt;p&gt;If I remember right, HornetQ allows  you to implement a callback interface to be notified of message acknowledgments.  We could do something similar, passing back the request and response for the erroneous parts.  To your second point, I suppose it depends on the error.  Some may be logged and skipped, some will require a refresh of topic metadata .  The default behaviour should cover the base cases.&lt;/p&gt;

&lt;p&gt;Curious to hear other thoughts&lt;/p&gt;</comment>
                            <comment id="13222779" author="junrao" created="Mon, 5 Mar 2012 23:33:48 +0000"  >&lt;p&gt;The current defaulteventhandler already refreshes topic metadata on retries. So, if we return any failed request to the client, there is probably not much the client can do, except for logging it. In any case, we should use a separate jira to track if we need any aysnc callback on the producer side.&lt;/p&gt;</comment>
                            <comment id="13222975" author="prashanth.menon" created="Tue, 6 Mar 2012 03:39:16 +0000"  >&lt;p&gt;Sounds good to me.  Doing some additional work on DefaultEventHandler, I noticed something off in the retry logic that I&apos;d like to get confirmed.  Consider the case where I&apos;ve got data destined for more than one broker, say three.  &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Enter handleSerializedData()&lt;/li&gt;
	&lt;li&gt;Partioning and collating makes a map with three key/value pairs (broker -&amp;gt; topic partition data and messages).&lt;/li&gt;
	&lt;li&gt;Enter for loop&lt;/li&gt;
	&lt;li&gt;Assume the first send works on the first try for broker #1.&lt;/li&gt;
	&lt;li&gt;Next iteration, the second send to broker #2 fails on the first try, we fall into the retry loop and recursive into handleSerializedData with requiredRetries = 0.&lt;/li&gt;
	&lt;li&gt;In handleSerializedData&lt;/li&gt;
	&lt;li&gt;This time, the partitioned data will one one key/value pair for the single broker (broker #2) we&apos;re attempting to resend data to.&lt;/li&gt;
	&lt;li&gt;Enter for loop&lt;/li&gt;
	&lt;li&gt;Attempt to send data to broker #2, the send succeeds&lt;/li&gt;
	&lt;li&gt;We exhaust the map entries and the for loop condition.&lt;/li&gt;
	&lt;li&gt;We return to the retry loop for retry=1 on broker #2 in the catch block.&lt;/li&gt;
	&lt;li&gt;The previous send succeeded on first try and now there&apos;s the &quot;return&quot; statement.  This exists the function, but we have one more broker (broker #3) to send data to.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Does the flow sound about right?  I think what needs to happen is to set a flag and break the retry while loop after a successful retry.  Then we check the flag after the loop and either throw the exception or continue the outer for loop.&lt;/p&gt;

&lt;p&gt;Am I crazy?  Am I missing something in my sleep-deprived state here?&lt;/p&gt;</comment>
                            <comment id="13223407" author="junrao" created="Tue, 6 Mar 2012 16:45:28 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Yes, that&apos;s actually a real bug. Instead of returning in retry, we should just set a boolean to indicate that a send has succeeded. At the end, we will throw FailedToSendMessageException if the boolean is not set. Otherwise, we will continue with the for loop. Could you file a separate jira to fix that? Thanks for catching the bug.&lt;/p&gt;</comment>
                            <comment id="13223609" author="prashanth.menon" created="Tue, 6 Mar 2012 20:12:16 +0000"  >&lt;p&gt;Done, created &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-295&quot; title=&quot;Bug in async producer DefaultEventHandler retry logic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-295&quot;&gt;&lt;del&gt;KAFKA-295&lt;/del&gt;&lt;/a&gt;.  Expect patch for this jira later tonight.&lt;/p&gt;</comment>
                            <comment id="13223944" author="prashanth.menon" created="Wed, 7 Mar 2012 03:42:07 +0000"  >&lt;p&gt;I&apos;ve attached a patch for this.  A few comments:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Modified ProducerResponse&lt;/li&gt;
	&lt;li&gt;Broker does not actually wait for replica ACKS.  This will be done with &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-44&quot; title=&quot;Various ZK listeners to support intra-cluster replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-44&quot;&gt;&lt;del&gt;KAFKA-44&lt;/del&gt;&lt;/a&gt;.&lt;/li&gt;
	&lt;li&gt;Sync producer has been modified to wait for response from broker.  Async producer isn&apos;t aware of request level errors, this will require a separate ticket.&lt;/li&gt;
	&lt;li&gt;Some general cleanup on producer request, async producer and removal of MultiProduce request key.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;One oddity is since we use Arrays in the request and response, it breaks the case class equality/hashcode logic since Java&apos;s arrays are broken.  We should probably make them Seq&apos;s (separate JIRA?) or WrappedArrays.&lt;/p&gt;</comment>
                            <comment id="13225321" author="junrao" created="Thu, 8 Mar 2012 17:25:05 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Thanks for the patch. Overall, it looks pretty good. Some comments:&lt;br/&gt;
1. KafkaApis: Even when the produce request requires no ack, we will still need to send error code back. So we should always send a ProduceResponse. When no ack is specified, we probably don&apos;t need to send offsets back.&lt;br/&gt;
2. ProduceRequest.getNumMessages: rename it to something like getNumTopicPartitions&lt;br/&gt;
3. AsyncProducerTeest.testDefaultHanlderRetryLogic: doesn&apos;t really test retry&lt;br/&gt;
4. SyncProducerTest.testProduceBlocksWhenRequired: Use createTopic instead of creating ZK path directly.&lt;br/&gt;
5. I agree that it&apos;s probably better to use Seq in our requests/response, instead of Array. Then we need a java version to convert Seq to java array and vice versa. Please open a separate jira to track this.&lt;/p&gt;</comment>
                            <comment id="13225801" author="prashanth.menon" created="Fri, 9 Mar 2012 02:50:19 +0000"  >&lt;p&gt;Thanks for the pointers!  &lt;/p&gt;

&lt;p&gt;1.  Hmmm, do you propose returning an empty offsets array back to the client when no ack is required?  That seems perfectly reasonable since the broker can&apos;t make guarantees as to the offsets; but it  does feel somehow incongrous since one would expect the errors and offsets array sizes to be equal.  I&apos;m fine with the idea as long as it&apos;s agreed in the wire format.  If I&apos;ve completely missed the point, forgive me!&lt;br/&gt;
2.  Done.&lt;br/&gt;
3.  Done.  Wow, that wasn&apos;t supposed to be included.  It was part of my sanity check for the incorrect retry logic I mentioned earlier.&lt;br/&gt;
4.  Done.&lt;br/&gt;
5.  Done.&lt;/p&gt;</comment>
                            <comment id="13225845" author="junrao" created="Fri, 9 Mar 2012 05:03:08 +0000"  >&lt;p&gt;1. Or we can just treat noacks the same as act=1 for now.&lt;/p&gt;</comment>
                            <comment id="13227277" author="prashanth.menon" created="Mon, 12 Mar 2012 00:18:08 +0000"  >&lt;p&gt;A few more concerns popped up as a result of making the send in syncproducer blocking.  &lt;/p&gt;

&lt;p&gt;1. Edit: So it turns out that using the channel in SyncProducer like we are to perform reads won&apos;t trigger socket timeouts though we set it and will block forever which is bad news(check &lt;a href=&quot;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802&lt;/a&gt; and &lt;a href=&quot;http://stackoverflow.com/questions/2866557/timeout-for-socketchannel&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://stackoverflow.com/questions/2866557/timeout-for-socketchannel&lt;/a&gt; for workaround).  The latter post has a simple solution that involves creating a separate ReadableByteChannel instance for timeout-enabled reads.  The other option being using non-blocking sockets with selectors which is more complex.&lt;br/&gt;
2.  It is conceivable that a broker listed in the replica set /brokers/topics/&lt;span class=&quot;error&quot;&gt;&amp;#91;topic&amp;#93;&lt;/span&gt;/partitions/&lt;span class=&quot;error&quot;&gt;&amp;#91;partition&amp;#93;&lt;/span&gt;/replicas is offline or shutdown which means their ephemeral entries are missing in ZK.  A problem then arises when an active broker attempts to pull metadata and node information for topics these brokers host since AdminUtils assumes any broker in AR or ISR must have paths/info in ZK /brokers/ids/&lt;span class=&quot;error&quot;&gt;&amp;#91;brokerId&amp;#93;&lt;/span&gt;, but since they don&apos;t an NoNodeException is thrown.  A corner case for sure, but something that should probably be fixed.&lt;/p&gt;</comment>
                            <comment id="13228111" author="prashanth.menon" created="Tue, 13 Mar 2012 01:01:17 +0000"  >&lt;p&gt;I&apos;ve attached an updated patch.  &lt;/p&gt;

&lt;p&gt;1. ProduceRequest always receives a ProducerResponse.  If acks=0, offsets in the response are treated as if acks=1, meaning only the leader has acked.&lt;br/&gt;
2. SyncProducer blocks waiting for a response from a ProducerRequest.&lt;br/&gt;
3. I&apos;ve commented out ProducerTest.testZKSendWithDeadBroker since it relies on SyncProducer logic that will need to change.  It also will need to be rewritten with the changes coming in as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-45&quot; title=&quot;Broker startup, leader election, becoming a leader/follower for intra-cluster replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-45&quot;&gt;&lt;del&gt;KAFKA-45&lt;/del&gt;&lt;/a&gt; leader election logic.&lt;/p&gt;

&lt;p&gt;Let me know if I&apos;ve missed anything!&lt;/p&gt;</comment>
                            <comment id="13228845" author="junrao" created="Wed, 14 Mar 2012 00:33:52 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;I am ready to commit your patch. A couple things:&lt;/p&gt;

&lt;p&gt;1. Your patch added a new class ResponseHandler, but it&apos;s not used. Should that be removed?&lt;br/&gt;
2. Your concern #1 is valid. Could you create another jira to track this?&lt;br/&gt;
3. For your concern #2, it&apos;s ok for getMetadataApi to return empty leader in a transition state. The client will simply backoff a bit and call getMetadataApi again.&lt;/p&gt;</comment>
                            <comment id="13228875" author="prashanth.menon" created="Wed, 14 Mar 2012 01:26:58 +0000"  >&lt;p&gt;Hi Jun, I&apos;ve attached a new patch.&lt;/p&gt;

&lt;p&gt;1. Yes, I&apos;ve removed it.&lt;br/&gt;
2. Done.&lt;br/&gt;
3. You are correct in the case of leaders, but I believe the problem stands when pulling topic metadata with atleast one offline broker listed in the assigned replicas.&lt;/p&gt;
</comment>
                            <comment id="13228882" author="junrao" created="Wed, 14 Mar 2012 01:38:08 +0000"  >&lt;p&gt;Thanks for the patch Prashanth. Just committed to 0.7 branch.&lt;/p&gt;

&lt;p&gt;For 3, if a broker is offline, then eventually it will not be in ISR. In the transition state, we could have an ISR broker without matching host and port.&lt;/p&gt;</comment>
                            <comment id="13229905" author="nehanarkhede" created="Thu, 15 Mar 2012 05:24:00 +0000"  >&lt;p&gt;Sorry for visiting this late. I have a few questions about producer ACK. &lt;/p&gt;

&lt;p&gt;1. In DefaultEventHandler, the producer ACK is not used. Shouldn&apos;t it be used to figure out if the send operation needs to be retried ? &lt;br/&gt;
2. In DefaultEventHandler, should the producer wait for ACK and timeout if it doesn&apos;t receive one ?&lt;br/&gt;
3. In KafkaApis, why doesn&apos;t the broker send an error back to the producer if it received a producer request for a partition that is not hosted on that broker ?&lt;/p&gt;</comment>
                            <comment id="13230156" author="prashanth.menon" created="Thu, 15 Mar 2012 13:21:16 +0000"  >&lt;p&gt;Better late than never.  A second review is always a plus!  To your points:&lt;/p&gt;

&lt;p&gt;1. Absolutely, this was overlooked.  Expect patch later tonight or tomorrow.&lt;br/&gt;
2. The DefaultEventHandler does wait for the ack by waiting for the response.  Unfortunately, the current SyncProduer doesn&apos;t timeout correctly for which &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-305&quot; title=&quot;SyncProducer does not correctly timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-305&quot;&gt;&lt;del&gt;KAFKA-305&lt;/del&gt;&lt;/a&gt; was created.&lt;br/&gt;
3. KafkaApis does not explicitly do the check, instead relying on LogManager which currently does.  It makes sense to move that piece of logic along with the TODO from LogManager into KafkaApis for clarity and to separate ZK from LogManager.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;</comment>
                            <comment id="13230603" author="nehanarkhede" created="Thu, 15 Mar 2012 21:57:48 +0000"  >&lt;p&gt;1. I&apos;m refactoring some part of DefaultEventHandler as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-300&quot; title=&quot;Implement leader election&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-300&quot;&gt;&lt;del&gt;KAFKA-300&lt;/del&gt;&lt;/a&gt;. I&apos;ll upload a patch tonight. You can either choose to work off of the changed code or not. Your call.&lt;br/&gt;
2. Sounds good&lt;br/&gt;
3. In handleProduceRequest, the logManager.append() throws InvalidPartitionException when it receives a request for a partition that it does not own. Does it make sense to send an ACK to the producer with an error code like  NotLeaderForPartitionException ?&lt;/p&gt;
</comment>
                            <comment id="13231400" author="nehanarkhede" created="Fri, 16 Mar 2012 17:16:52 +0000"  >&lt;p&gt;Reopening this issue to address some review suggestions and to fix &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-305&quot; title=&quot;SyncProducer does not correctly timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-305&quot;&gt;&lt;del&gt;KAFKA-305&lt;/del&gt;&lt;/a&gt; as part of this JIRA&lt;/p&gt;</comment>
                            <comment id="13240512" author="nehanarkhede" created="Wed, 28 Mar 2012 16:22:04 +0000"  >&lt;p&gt;Prashanth, thanks for resolving &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-305&quot; title=&quot;SyncProducer does not correctly timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-305&quot;&gt;&lt;del&gt;KAFKA-305&lt;/del&gt;&lt;/a&gt; ! Would you be up for finishing up the remaining work on this ? It seems like a good idea to complete earlier JIRAs, before moving to the later ones.&lt;/p&gt;</comment>
                            <comment id="13241436" author="prashanth.menon" created="Thu, 29 Mar 2012 17:54:28 +0000"  >&lt;p&gt;Okay, I&apos;ve attached a patch that should take care of the outstanding items.  A couple of points:&lt;/p&gt;

&lt;p&gt;1. KafkaApis not checks whether a partition has a leader on the broker.  It uses KafkaZooKeeper to check this, for now, but should probably rely on ReplicaManager and the Replica itself to determine this.  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-46&quot; title=&quot;Commit thread, ReplicaFetcherThread for intra-cluster replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-46&quot;&gt;&lt;del&gt;KAFKA-46&lt;/del&gt;&lt;/a&gt; should take care of this.&lt;br/&gt;
2. I&apos;ve removed the random partition check on the server-side since the partitioning is done in default event handler.  Producers should know which broker a partition belongs to.&lt;br/&gt;
3. I&apos;ve added a new NotLeaderForPartitionException and added it to ErrorMapping so clients can receive it.&lt;br/&gt;
4. DefaultEventHandler.send now returns a list of topic/partition tuples that represents those messages that need to be resent due to an error.&lt;br/&gt;
5. Due to the changes in KafkaApis produce check, some of the tests have been modified to ensure topics are in ZK and to wait for leadership.&lt;/p&gt;

&lt;p&gt;I think that covers all, please point out any issues!&lt;/p&gt;</comment>
                            <comment id="13241902" author="nehanarkhede" created="Thu, 29 Mar 2012 23:03:00 +0000"  >&lt;p&gt;+1. Thanks for incorporating the review suggestions !&lt;/p&gt;</comment>
                            <comment id="13245568" author="junrao" created="Tue, 3 Apr 2012 18:25:20 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Patch looks good: Just one minor thing.&lt;/p&gt;

&lt;p&gt;6. Unused imports: KafkaZookeeper&lt;/p&gt;

&lt;p&gt;While looking at the patch, I realized that there are a couple of other things that we will need to follow up.&lt;/p&gt;

&lt;p&gt;a. In DefaultEventHandler, it seems that we rely on the fact that broker.id is a non-negative integer. However, we don&apos;t enforce that in broker startup.&lt;br/&gt;
b. With the create topic ddl, some of the broker configs like topic.partition.count.map probably don&apos;t make sense anymore.&lt;/p&gt;

&lt;p&gt;I will create a jira for each item to follow up.&lt;/p&gt;</comment>
                            <comment id="13246894" author="prashanth.menon" created="Thu, 5 Apr 2012 00:50:55 +0000"  >&lt;p&gt;Thanks for the review!  I&apos;ve attached newest patch for #6 resolved.&lt;/p&gt;</comment>
                            <comment id="13247458" author="junrao" created="Thu, 5 Apr 2012 18:17:04 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Thanks for the patch. It seems that kafka-48 is almost ready. Since that&apos;s a relatively large patch, I will commit your patch after kafka-48 is committed.&lt;/p&gt;</comment>
                            <comment id="13270038" author="junrao" created="Mon, 7 May 2012 21:45:34 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Now that kafka-48 is committed to 0.8, we can commit your patch. Since you are a committer now, could you commit this yourself?&lt;/p&gt;</comment>
                            <comment id="13282133" author="prashanth.menon" created="Thu, 24 May 2012 02:20:33 +0000"  >&lt;p&gt;Sorry for the delay everyone.  I&apos;m planning to block off some time this weekend to commit this patch, and hoping I don&apos;t run into any access/permissions issues &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13284499" author="prashanth.menon" created="Mon, 28 May 2012 18:02:36 +0000"  >&lt;p&gt;Excellent, committed this to 0.8.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12514687">KAFKA-50</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12514694">KAFKA-57</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12546581">KAFKA-305</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12537383">KAFKA-240</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="12514816">KAFKA-73</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12521435" name="KAFKA-49-continued-v2.patch" size="35755" author="prashanth.menon" created="Thu, 5 Apr 2012 00:50:55 +0000"/>
                            <attachment id="12520461" name="KAFKA-49-continued.patch" size="35782" author="prashanth.menon" created="Thu, 29 Mar 2012 17:54:50 +0000"/>
                            <attachment id="12517358" name="KAFKA-49-v1.patch" size="47600" author="prashanth.menon" created="Wed, 7 Mar 2012 03:35:09 +0000"/>
                            <attachment id="12518130" name="KAFKA-49-v2.patch" size="50382" author="prashanth.menon" created="Tue, 13 Mar 2012 00:57:05 +0000"/>
                            <attachment id="12518276" name="KAFKA-49-v3.patch" size="48921" author="prashanth.menon" created="Wed, 14 Mar 2012 01:17:14 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>67068</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 26 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i09m33:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>54011</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>