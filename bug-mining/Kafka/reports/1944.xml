<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:11:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7023] Kafka Streams RocksDB bulk loading config may not be honored with customized RocksDBConfigSetter </title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7023</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We observed frequent L0 -&amp;gt; L1 compaction during Kafka Streams state recovery. Some sample log:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2018/06/08-00:04:50.892331 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892298) [db/compaction_picker_universal.cc:270] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: sorted runs files(6): files[3 0 0 0 1 1 38] max score 1.00
2018/06/08-00:04:50.892336 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892300) [db/compaction_picker_universal.cc:655] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: First candidate file 134[0] to reduce size amp.
2018/06/08-00:04:50.892338 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892302) [db/compaction_picker_universal.cc:686] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: size amp not needed. newer-files-total-size 13023497 earliest-file-size 2541530372
2018/06/08-00:04:50.892339 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892303) [db/compaction_picker_universal.cc:473] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Possible candidate file 134[0].
2018/06/08-00:04:50.892341 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892304) [db/compaction_picker_universal.cc:525] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Skipping file 134[0] with size 1007 (compensated size 1287)
2018/06/08-00:04:50.892343 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892306) [db/compaction_picker_universal.cc:473] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Possible candidate file 133[1].
2018/06/08-00:04:50.892344 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892307) [db/compaction_picker_universal.cc:525] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Skipping file 133[1] with size 4644 (compensated size 16124)
2018/06/08-00:04:50.892346 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892307) [db/compaction_picker_universal.cc:473] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Possible candidate file 126[2].
2018/06/08-00:04:50.892348 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892308) [db/compaction_picker_universal.cc:525] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Skipping file 126[2] with size 319764 (compensated size 319764)
2018/06/08-00:04:50.892349 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892309) [db/compaction_picker_universal.cc:473] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Possible candidate level 4[3].
2018/06/08-00:04:50.892351 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892310) [db/compaction_picker_universal.cc:525] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Skipping level 4[3] with size 2815574 (compensated size 2815574)
2018/06/08-00:04:50.892352 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892311) [db/compaction_picker_universal.cc:473] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Possible candidate level 5[4].
2018/06/08-00:04:50.892357 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892311) [db/compaction_picker_universal.cc:525] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Skipping level 5[4] with size 9870748 (compensated size 9870748)
2018/06/08-00:04:50.892358 7f8a6d7fa700 (Original Log Time 2018/06/08-00:04:50.892313) [db/compaction_picker_universal.cc:473] [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;] Universal: Possible candidate level 6[5].
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In customized&#160;RocksDBConfigSetter, we set&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
level0_file_num_compaction_trigger=6 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;During bulk loading, the following options are set: &lt;a href=&quot;https://github.com/facebook/rocksdb/blob/master/options/options.cc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/facebook/rocksdb/blob/master/options/options.cc&lt;/a&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Options*
Options::PrepareForBulkLoad()
{
&lt;span class=&quot;code-comment&quot;&gt;// never slowdown ingest.
&lt;/span&gt;level0_file_num_compaction_trigger = (1&amp;lt;&amp;lt;30);
level0_slowdown_writes_trigger = (1&amp;lt;&amp;lt;30);
level0_stop_writes_trigger = (1&amp;lt;&amp;lt;30);
soft_pending_compaction_bytes_limit = 0;
hard_pending_compaction_bytes_limit = 0;

&lt;span class=&quot;code-comment&quot;&gt;// no auto compactions please. The application should issue a
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// manual compaction after all data is loaded into L0.
&lt;/span&gt;disable_auto_compactions = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
&lt;span class=&quot;code-comment&quot;&gt;// A manual compaction run should pick all files in L0 in
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// a single compaction run.
&lt;/span&gt;max_compaction_bytes = (static_cast&amp;lt;uint64_t&amp;gt;(1) &amp;lt;&amp;lt; 60);

&lt;span class=&quot;code-comment&quot;&gt;// It is better to have only 2 levels, otherwise a manual
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// compaction would compact at every possible level, thereby
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// increasing the total time needed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; compactions.
&lt;/span&gt;num_levels = 2;

&lt;span class=&quot;code-comment&quot;&gt;// Need to allow more write buffers to allow more parallism
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// of flushes.
&lt;/span&gt;max_write_buffer_number = 6;
min_write_buffer_number_to_merge = 1;

&lt;span class=&quot;code-comment&quot;&gt;// When compaction is disabled, more parallel flush threads can
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// help with write throughput.
&lt;/span&gt;max_background_flushes = 4;

&lt;span class=&quot;code-comment&quot;&gt;// Prevent a memtable flush to automatically promote files
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// to L1. This is helpful so that all files that are
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// input to the manual compaction are all at L0.
&lt;/span&gt;max_background_compactions = 2;

&lt;span class=&quot;code-comment&quot;&gt;// The compaction would create large files in L1.
&lt;/span&gt;target_file_size_base = 256 * 1024 * 1024;
&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Especially, those values are set to a very large number to avoid compactions and ensures files are all on L0.&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
level0_file_num_compaction_trigger = (1&amp;lt;&amp;lt;30);
level0_slowdown_writes_trigger = (1&amp;lt;&amp;lt;30);
level0_stop_writes_trigger = (1&amp;lt;&amp;lt;30);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;However, in RockDBStore.java, openDB code, we first call:&lt;/p&gt;

&lt;p&gt;options.prepareForBulkLoad() and then use the configs from the customized&#160;customized RocksDBConfigSetter. This may overwrite the configs set in prepareBulkLoad call. The fix is to move prepareBulkLoad call after applying configs customized&#160;RocksDBConfigSetter.&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13164839">KAFKA-7023</key>
            <summary>Kafka Streams RocksDB bulk loading config may not be honored with customized RocksDBConfigSetter </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="liquanpei">Liquan Pei</assignee>
                                    <reporter username="liquanpei">Liquan Pei</reporter>
                        <labels>
                    </labels>
                <created>Fri, 8 Jun 2018 02:15:51 +0000</created>
                <updated>Mon, 18 Jun 2018 23:15:41 +0000</updated>
                            <resolved>Mon, 18 Jun 2018 23:15:41 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                    <workratio workratioPercent="0"/>
                                    <progress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                    <timeoriginalestimate seconds="86400">24h</timeoriginalestimate>
                            <timeestimate seconds="86400">24h</timeestimate>
                                        <comments>
                            <comment id="16505609" author="liquanpei" created="Fri, 8 Jun 2018 02:16:53 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bchen225242&quot; class=&quot;user-hover&quot; rel=&quot;bchen225242&quot;&gt;bchen225242&lt;/a&gt;.&#160;&lt;/p&gt;</comment>
                            <comment id="16506290" author="githubbot" created="Fri, 8 Jun 2018 17:22:47 +0000"  >&lt;p&gt;Ishiihara opened a new pull request #5166: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7023&quot; title=&quot;Kafka Streams RocksDB bulk loading config may not be honored with customized RocksDBConfigSetter &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7023&quot;&gt;&lt;del&gt;KAFKA-7023&lt;/del&gt;&lt;/a&gt;: Move prepareForBulkLoad() call after customized RocksDBConfigSetter&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5166&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5166&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   *Summary&lt;br/&gt;
   options.prepareForBulkLoad() and then use the configs from the customized customized RocksDBConfigSetter. This may overwrite the configs set in prepareBulkLoad call. The fix is to move prepareBulkLoad call after applying configs customized RocksDBConfigSetter. &lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   Unit test, test on dev environment on recovery time. &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16506316" author="guozhang" created="Fri, 8 Jun 2018 17:43:02 +0000"  >&lt;p&gt;Thanks for the report! I think this is indeed a perf regression.&lt;/p&gt;</comment>
                            <comment id="16508694" author="githubbot" created="Mon, 11 Jun 2018 20:32:06 +0000"  >&lt;p&gt;guozhangwang closed pull request #5166: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7023&quot; title=&quot;Kafka Streams RocksDB bulk loading config may not be honored with customized RocksDBConfigSetter &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7023&quot;&gt;&lt;del&gt;KAFKA-7023&lt;/del&gt;&lt;/a&gt;: Move prepareForBulkLoad() call after customized RocksDBConfigSetter&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5166&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5166&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
index cfef035a4fd..6084ecbf1e0 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
@@ -130,10 +130,6 @@ public void openDB(final ProcessorContext context) {&lt;br/&gt;
         // (this could be a bug in the RocksDB code and their devs have been contacted).&lt;br/&gt;
         options.setIncreaseParallelism(Math.max(Runtime.getRuntime().availableProcessors(), 2));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (prepareForBulkload) 
{
-            options.prepareForBulkLoad();
-        }
&lt;p&gt;-&lt;br/&gt;
         wOptions = new WriteOptions();&lt;br/&gt;
         wOptions.setDisableWAL(true);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -148,6 +144,11 @@ public void openDB(final ProcessorContext context) &lt;/p&gt;
{
             final RocksDBConfigSetter configSetter = Utils.newInstance(configSetterClass);
             configSetter.setConfig(name, options, configs);
         }
&lt;p&gt;+&lt;br/&gt;
+        if (prepareForBulkload) &lt;/p&gt;
{
+            options.prepareForBulkLoad();
+        }
&lt;p&gt;+&lt;br/&gt;
         this.dbDir = new File(new File(context.stateDir(), parentDir), this.name);&lt;/p&gt;

&lt;p&gt;         try {&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16509014" author="githubbot" created="Tue, 12 Jun 2018 00:56:04 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5197: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7023&quot; title=&quot;Kafka Streams RocksDB bulk loading config may not be honored with customized RocksDBConfigSetter &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7023&quot;&gt;&lt;del&gt;KAFKA-7023&lt;/del&gt;&lt;/a&gt;: Add unit test&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5197&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5197&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Add a unit test that validates after restoreStart, the options are set with bulk loading configs; and after restoreEnd, it resumes to the customized configs&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16509982" author="githubbot" created="Tue, 12 Jun 2018 18:12:10 +0000"  >&lt;p&gt;guozhangwang closed pull request #5197: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7023&quot; title=&quot;Kafka Streams RocksDB bulk loading config may not be honored with customized RocksDBConfigSetter &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7023&quot;&gt;&lt;del&gt;KAFKA-7023&lt;/del&gt;&lt;/a&gt;: Add unit test&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5197&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5197&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
index 6084ecbf1e0..e858ac07921 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
@@ -572,4 +572,9 @@ public void onRestoreEnd(final TopicPartition topicPartition,&lt;br/&gt;
             rocksDBStore.toggleDbForBulkLoading(false);&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
+    // for testing&lt;br/&gt;
+    public Options getOptions() &lt;/p&gt;
{
+        return options;
+    }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java&lt;br/&gt;
index b7a9d375d9c..63d877af5d6 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java&lt;br/&gt;
@@ -16,23 +16,21 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.state.internals;&lt;/p&gt;

&lt;p&gt;-import org.apache.kafka.common.metrics.Metrics;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Deserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.StringDeserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.StringSerializer;&lt;br/&gt;
 import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
-import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
 import org.apache.kafka.streams.errors.ProcessorStateException;&lt;br/&gt;
-import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;&lt;br/&gt;
+import org.apache.kafka.streams.processor.StateRestoreListener;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueIterator;&lt;br/&gt;
 import org.apache.kafka.streams.state.RocksDBConfigSetter;&lt;br/&gt;
 import org.apache.kafka.test.InternalMockProcessorContext;&lt;br/&gt;
-import org.apache.kafka.test.NoOpRecordCollector;&lt;br/&gt;
+import org.apache.kafka.test.StreamsTestUtils;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.After;&lt;br/&gt;
 import org.junit.Before;&lt;br/&gt;
@@ -43,10 +41,10 @@&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.io.UnsupportedEncodingException;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
-import java.util.HashMap;&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
 import java.util.Set;&lt;/p&gt;

&lt;p&gt; import static org.hamcrest.CoreMatchers.equalTo;&lt;br/&gt;
@@ -57,8 +55,6 @@&lt;br/&gt;
 import static org.junit.Assert.fail;&lt;/p&gt;

&lt;p&gt; public class RocksDBStoreTest {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final File tempDir = TestUtils.tempDirectory();&lt;br/&gt;
-&lt;br/&gt;
     private Serializer&amp;lt;String&amp;gt; stringSerializer = new StringSerializer();&lt;br/&gt;
     private Deserializer&amp;lt;String&amp;gt; stringDeserializer = new StringDeserializer();&lt;br/&gt;
     private RocksDBStore rocksDBStore;&lt;br/&gt;
@@ -67,13 +63,14 @@&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Before&lt;br/&gt;
     public void setUp() &lt;/p&gt;
{
+        final Properties props = StreamsTestUtils.minimalStreamsConfig();
+        props.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, MockRocksDbConfigSetter.class);
         rocksDBStore = new RocksDBStore(&quot;test&quot;);
         dir = TestUtils.tempDirectory();
         context = new InternalMockProcessorContext(dir,
             Serdes.String(),
             Serdes.String(),
-            new NoOpRecordCollector(),
-            new ThreadCache(new LogContext(&quot;testCache &quot;), 0, new MockStreamsMetrics(new Metrics())));
+            new StreamsConfig(props));
     }

&lt;p&gt;     @After&lt;br/&gt;
@@ -81,6 +78,21 @@ public void tearDown() &lt;/p&gt;
{
         rocksDBStore.close();
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldRespectBulkloadOptionsDuringInit() &lt;/p&gt;
{
+        rocksDBStore.init(context, rocksDBStore);
+
+        StateRestoreListener restoreListener = context.getRestoreListener(rocksDBStore.name());
+
+        restoreListener.onRestoreStart(null, rocksDBStore.name(), 0L, 0L);
+
+        assertThat(rocksDBStore.getOptions().level0FileNumCompactionTrigger(), equalTo(1 &amp;lt;&amp;lt; 30));
+
+        restoreListener.onRestoreEnd(null, rocksDBStore.name(), 0L);
+
+        assertThat(rocksDBStore.getOptions().level0FileNumCompactionTrigger(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldNotThrowExceptionOnRestoreWhenThereIsPreExistingRocksDbFiles() throws Exception &lt;/p&gt;
{
         rocksDBStore.init(context, rocksDBStore);
@@ -108,28 +120,27 @@ public void shouldNotThrowExceptionOnRestoreWhenThereIsPreExistingRocksDbFiles()
     }

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void verifyRocksDbConfigSetterIsCalled() {&lt;/li&gt;
	&lt;li&gt;final Map&amp;lt;String, Object&amp;gt; configs = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;configs.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;test-application&quot;);&lt;/li&gt;
	&lt;li&gt;configs.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;test-server:9092&quot;);&lt;/li&gt;
	&lt;li&gt;configs.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, MockRocksDbConfigSetter.class);&lt;br/&gt;
+    public void shouldCallRocksDbConfigSetter() 
{
         MockRocksDbConfigSetter.called = false;
-        rocksDBStore.openDB(new InternalMockProcessorContext(tempDir, new StreamsConfig(configs)));
+
+        rocksDBStore.openDB(context);
 
         assertTrue(MockRocksDbConfigSetter.called);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = ProcessorStateException.class)&lt;/li&gt;
	&lt;li&gt;public void shouldThrowProcessorStateExceptionOnOpeningReadOnlyDir() throws IOException {&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldThrowProcessorStateExceptionOnOpeningReadOnlyDir() {&lt;br/&gt;
         final File tmpDir = TestUtils.tempDirectory();&lt;/li&gt;
	&lt;li&gt;InternalMockProcessorContext tmpContext = new InternalMockProcessorContext(tmpDir,&lt;/li&gt;
	&lt;li&gt;Serdes.String(),&lt;/li&gt;
	&lt;li&gt;Serdes.Long(),&lt;/li&gt;
	&lt;li&gt;new NoOpRecordCollector(),&lt;/li&gt;
	&lt;li&gt;new ThreadCache(new LogContext(&quot;testCache &quot;), 0, new MockStreamsMetrics(new Metrics())));&lt;/li&gt;
	&lt;li&gt;tmpDir.setReadOnly();&lt;br/&gt;
+        InternalMockProcessorContext tmpContext = new InternalMockProcessorContext(tmpDir, new StreamsConfig(StreamsTestUtils.minimalStreamsConfig()));&lt;br/&gt;
+&lt;br/&gt;
+        assertTrue(tmpDir.setReadOnly());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;rocksDBStore.openDB(tmpContext);&lt;br/&gt;
+        try 
{
+            rocksDBStore.openDB(tmpContext);
+            fail(&quot;Should have thrown ProcessorStateException&quot;);
+        }
&lt;p&gt; catch (ProcessorStateException e) &lt;/p&gt;
{
+            // this is good, do nothing
+        }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
@@ -221,7 +232,7 @@ public void shouldRestoreAll() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldPutOnlyIfAbsentValue() throws Exception {&lt;br/&gt;
+    public void shouldPutOnlyIfAbsentValue() {&lt;br/&gt;
         rocksDBStore.init(context, rocksDBStore);&lt;br/&gt;
         final Bytes keyBytes = new Bytes(stringSerializer.serialize(null, &quot;one&quot;));&lt;br/&gt;
         final byte[] valueBytes = stringSerializer.serialize(null, &quot;A&quot;);&lt;br/&gt;
@@ -237,7 +248,7 @@ public void shouldPutOnlyIfAbsentValue() throws Exception {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldHandleDeletesOnRestoreAll() throws Exception {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; entries = getKeyValueEntries();&lt;/li&gt;
	&lt;li&gt;entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), (byte[]) null));&lt;br/&gt;
+        entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), null));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         rocksDBStore.init(context, rocksDBStore);&lt;br/&gt;
         context.restore(rocksDBStore.name(), entries);&lt;br/&gt;
@@ -258,7 +269,7 @@ public void shouldHandleDeletesAndPutbackOnRestoreAll() throws Exception {&lt;br/&gt;
         entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), &quot;a&quot;.getBytes(&quot;UTF-8&quot;)));&lt;br/&gt;
         entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;2&quot;.getBytes(&quot;UTF-8&quot;), &quot;b&quot;.getBytes(&quot;UTF-8&quot;)));&lt;br/&gt;
         // this will be deleted&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), (byte[]) null));&lt;br/&gt;
+        entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), null));&lt;br/&gt;
         entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;3&quot;.getBytes(&quot;UTF-8&quot;), &quot;c&quot;.getBytes(&quot;UTF-8&quot;)));&lt;br/&gt;
         // this will restore key &quot;1&quot; as WriteBatch applies updates in order&lt;br/&gt;
         entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), &quot;restored&quot;.getBytes(&quot;UTF-8&quot;)));&lt;br/&gt;
@@ -320,7 +331,7 @@ public void shouldRestoreThenDeleteOnRestoreAll() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;2&quot;.getBytes(&quot;UTF-8&quot;), &quot;b&quot;.getBytes(&quot;UTF-8&quot;)));&lt;br/&gt;
         entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;3&quot;.getBytes(&quot;UTF-8&quot;), &quot;c&quot;.getBytes(&quot;UTF-8&quot;)));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), (byte[]) null));&lt;br/&gt;
+        entries.add(new KeyValue&amp;lt;&amp;gt;(&quot;1&quot;.getBytes(&quot;UTF-8&quot;), null));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         context.restore(rocksDBStore.name(), entries);&lt;/p&gt;

&lt;p&gt;@@ -342,7 +353,9 @@ public void shouldThrowNullPointerExceptionOnNullPut() {&lt;br/&gt;
         try &lt;/p&gt;
{
             rocksDBStore.put(null, stringSerializer.serialize(null, &quot;someVal&quot;));
             fail(&quot;Should have thrown NullPointerException on null put()&quot;);
-        } catch (NullPointerException e) { }&lt;br/&gt;
+        } catch (NullPointerException e) {
+            // this is good
+        }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -351,7 +364,9 @@ public void shouldThrowNullPointerExceptionOnNullPutAll() {&lt;br/&gt;
         try {             rocksDBStore.put(null, stringSerializer.serialize(null, &quot;someVal&quot;));             fail(&quot;Should have thrown NullPointerException on null put()&quot;);-        }
&lt;p&gt; catch (NullPointerException e) { }&lt;br/&gt;
+        } catch (NullPointerException e) &lt;/p&gt;
{
+            // this is good
+        }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -360,7 +375,9 @@ public void shouldThrowNullPointerExceptionOnNullGet() {&lt;br/&gt;
         try {
             rocksDBStore.get(null);
             fail(&quot;Should have thrown NullPointerException on null get()&quot;);
-        } catch (NullPointerException e) { }&lt;br/&gt;
+        } catch (NullPointerException e) {+            // this is good+        }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
@@ -369,7 +386,9 @@ public void shouldThrowNullPointerExceptionOnDelete() {&lt;br/&gt;
         try &lt;/p&gt;
{
             rocksDBStore.delete(null);
             fail(&quot;Should have thrown NullPointerException on deleting null key&quot;);
-        }
&lt;p&gt; catch (NullPointerException e) { }&lt;br/&gt;
+        } catch (NullPointerException e) &lt;/p&gt;
{
+            // this is good
+        }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -378,7 +397,9 @@ public void shouldThrowNullPointerExceptionOnRange() {&lt;br/&gt;
         try {
             rocksDBStore.range(null, new Bytes(stringSerializer.serialize(null, &quot;2&quot;)));
             fail(&quot;Should have thrown NullPointerException on deleting null key&quot;);
-        } catch (NullPointerException e) { }&lt;br/&gt;
+        } catch (NullPointerException e) {+            // this is good+        }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;     @Test(expected = ProcessorStateException.class)&lt;br/&gt;
@@ -397,6 +418,8 @@ public void shouldThrowProcessorStateExceptionOnPutDeletedDir() throws IOExcepti&lt;br/&gt;
         @Override&lt;br/&gt;
         public void setConfig(final String storeName, final Options options, final Map&amp;lt;String, Object&amp;gt; configs) &lt;/p&gt;
{
             called = true;
+
+            options.setLevel0FileNumCompactionTrigger(10);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java b/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java&lt;br/&gt;
index e5571eb43c3..bb42d1c4a26 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java&lt;br/&gt;
@@ -78,6 +78,13 @@ public InternalMockProcessorContext(final File stateDir,&lt;br/&gt;
         this(stateDir, null, null, new StreamsMetricsImpl(new Metrics(), &quot;mock&quot;), config, null, null);&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    public InternalMockProcessorContext(final File stateDir,&lt;br/&gt;
+                                        final Serde&amp;lt;?&amp;gt; keySerde,&lt;br/&gt;
+                                        final Serde&amp;lt;?&amp;gt; valSerde,&lt;br/&gt;
+                                        final StreamsConfig config) &lt;/p&gt;
{
+        this(stateDir, keySerde, valSerde, new StreamsMetricsImpl(new Metrics(), &quot;mock&quot;), config, null, null);
+    }
&lt;p&gt;+&lt;br/&gt;
     public InternalMockProcessorContext(final StateSerdes&amp;lt;?, ?&amp;gt; serdes,&lt;br/&gt;
                                         final RecordCollector collector) {&lt;br/&gt;
         this(null, serdes.keySerde(), serdes.valueSerde(), collector, null);&lt;br/&gt;
@@ -293,10 +300,14 @@ public Headers headers() &lt;/p&gt;
{
         return Collections.unmodifiableMap(storeMap);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void restore(final String storeName, final Iterable&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; changeLog) {&lt;br/&gt;
+    public StateRestoreListener getRestoreListener(final String storeName) 
{
+        final BatchingStateRestoreCallback restoreCallback = getBatchingRestoreCallback(restoreFuncs.get(storeName));
+        return getStateRestoreListener(restoreCallback);
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    public void restore(final String storeName, final Iterable&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; changeLog) {&lt;br/&gt;
         final BatchingStateRestoreCallback restoreCallback = getBatchingRestoreCallback(restoreFuncs.get(storeName));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final StateRestoreListener restoreListener = getStateRestoreListener(restoreCallback);&lt;br/&gt;
+        final StateRestoreListener restoreListener = getRestoreListener(storeName);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         restoreListener.onRestoreStart(null, storeName, 0L, 0L);&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 22 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3un93:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>