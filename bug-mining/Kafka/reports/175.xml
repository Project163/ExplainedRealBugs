<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:36:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-691] Fault tolerance broken with replication factor 1</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-691</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;In 0.7 if a partition was down we would just send the message elsewhere. This meant that the partitioning was really more of a &quot;stickiness&quot; then a hard guarantee. This made it impossible to depend on it for partitioned, stateful processing.&lt;/p&gt;

&lt;p&gt;In 0.8 when running with replication this should not be a problem generally as the partitions are now highly available and fail over to other replicas. However in the case of replication factor = 1 no longer really works for most cases as now a dead broker will give errors for that broker.&lt;/p&gt;

&lt;p&gt;I am not sure of the best fix. Intuitively I think this is something that should be handled by the Partitioner interface. However currently the partitioner has no knowledge of which nodes are available. So you could use a random partitioner, but that would keep going back to the down node.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12626789">KAFKA-691</key>
            <summary>Fault tolerance broken with replication factor 1</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="brugidou">Maxime Brugidou</assignee>
                                    <reporter username="jkreps">Jay Kreps</reporter>
                        <labels>
                    </labels>
                <created>Wed, 9 Jan 2013 16:18:11 +0000</created>
                <updated>Thu, 17 Jan 2013 17:12:46 +0000</updated>
                            <resolved>Thu, 10 Jan 2013 19:09:20 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>2</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="13548667" author="brugidou" created="Wed, 9 Jan 2013 16:44:57 +0000"  >&lt;p&gt;I think the work-around is not really acceptable for me since it will consume 3x the resources (because replication of 3 is the minimum acceptable) and it will still make the cluster less available anyway (unless i have only 3 brokers).&lt;/p&gt;

&lt;p&gt;The thing is that 0.7 was making the cluster 100% available (for my use case, accepting data loss) as long a single broker was alive.&lt;/p&gt;

&lt;p&gt;A way to handle this would be to:&lt;br/&gt;
1. Have a lot of partitions per topic (more than the # of brokers)&lt;br/&gt;
2. Have something that rebalances the partitions and make sure a broker has a at least a partition for each topic (to make every topic &quot;available&quot;)&lt;br/&gt;
3. Have a setting in the consumer/producer that say &quot;I don&apos;t care about partitioning, just produce/consume wherever you can&quot;&lt;/p&gt;</comment>
                            <comment id="13548847" author="junrao" created="Wed, 9 Jan 2013 19:14:35 +0000"  >&lt;p&gt;One thing we can do is to change the partitioner api so that it takes # of partitions and for each partition, an indicator whether a partition is available or not. The we can change the default partitioner to only route a message to the available partitions, if a key is not provided.&lt;/p&gt;</comment>
                            <comment id="13549105" author="brugidou" created="Wed, 9 Jan 2013 22:27:57 +0000"  >&lt;p&gt;I agree with Jun solution, this would solve 3 (1 and 2 can be done manualy already &amp;#8211; just send a ReassignPartition command when you add a broker)&lt;/p&gt;

&lt;p&gt;I could probably implement this very quickly, I&apos;m just not sure of how you get the availability of a partition, but i&apos;ll try to figure it out and submit a first patch tomorrow.&lt;/p&gt;</comment>
                            <comment id="13549276" author="jkreps" created="Thu, 10 Jan 2013 01:37:37 +0000"  >&lt;p&gt;That would be awesome. If you don&apos;t mind just give the proposed set of changes on the JIRA first and lets get everyone on board with how it should work since it is a reasonably important change (or, if you don&apos;t mind revising your patch we can start with that).&lt;/p&gt;</comment>
                            <comment id="13549373" author="junrao" created="Thu, 10 Jan 2013 05:44:01 +0000"  >&lt;p&gt;DefaultEventHander.getPartitionListForTopic() returns Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionAndLeader&amp;#93;&lt;/span&gt;. If PartitionAndLeader.leaderBrokerIdOpt is none, the partition is not available. &lt;/p&gt;

&lt;p&gt;There is another tricky issue. If a partition is not available, when do we refresh the metadata to check if the partition becomes available again? Currently, we refresh the metadata if we fail to send the data. However, if we always route the messages to available partitions, we may never fail to send. One possible solution is that if there is at least one partition not available in Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionAndLeader&amp;#93;&lt;/span&gt;, we refresh the metadata if a configurable amount of time has passed (e.g., 10 mins).&lt;/p&gt;</comment>
                            <comment id="13549669" author="brugidou" created="Thu, 10 Jan 2013 14:37:08 +0000"  >&lt;p&gt;Here is a first draft (v1) patch.&lt;/p&gt;

&lt;p&gt;1. Added the consumer property &quot;producer.metadata.refresh.interval.ms&quot; defaults to 600000 (10min)&lt;/p&gt;

&lt;p&gt;2. The metadata is refreshed every 10min (only if a message is sent), and the set of topics to refresh is tracked in the topicMetadataToRefresh Set (cleared after every refresh) - I think the added value of refreshing regardless of partition availability is to detect new partitions&lt;/p&gt;

&lt;p&gt;3. The good news is that I didn&apos;t touch the Partitioner API, I only changed the code to use available partitions if the key is null (as suggested by Jun), it will also throw a UnknownTopicOrPartitionException(&quot;No leader for any partition&quot;) if no partition is available at all&lt;/p&gt;

&lt;p&gt;Let me know what you think about this patch. I ran a producer with that code successfully and tested with a broker down.&lt;/p&gt;

&lt;p&gt;I now have some concerns about the consumer: the refresh.leader.backoff.ms config could help me (if i increase it to say, 10min) BUT the rebalance fails in any case since there is no leader for some partitions&lt;/p&gt;

&lt;p&gt;I don&apos;t have a good workaround yet for that, any help/suggestion appreciated.&lt;/p&gt;</comment>
                            <comment id="13549842" author="junrao" created="Thu, 10 Jan 2013 17:59:30 +0000"  >&lt;p&gt;Thanks for the patch. Overall, the patch is pretty good and is well thought out. Some comments:&lt;/p&gt;

&lt;p&gt;1. DefaultEventHandler:&lt;br/&gt;
1.1 In handle(), I don&apos;t think we need to add the if test in the following statement. The reason is that a message could fail to be sent because the leader changes immediately after the previous metadata refresh. Normally, leaders are elected very quickly. So, it makes sense to refresh the metadata again.&lt;br/&gt;
          if (topicMetadataToRefresh.nonEmpty)&lt;br/&gt;
              Utils.swallowError(brokerPartitionInfo.updateInfo(outstandingProduceRequests.map(_.topic).toSet))&lt;br/&gt;
1.2 In handle(), it seems that it&apos;s better to call the following code before dispatchSerializedData().&lt;br/&gt;
        if (topicMetadataRefreshInterval &amp;gt;= 0 &amp;amp;&amp;amp;&lt;br/&gt;
            SystemTime.milliseconds - lastTopicMetadataRefresh &amp;gt; topicMetadataRefreshInterval) &lt;/p&gt;
{
          Utils.swallowError(brokerPartitionInfo.updateInfo(topicMetadataToRefresh.toSet))
          topicMetadataToRefresh.clear
          lastTopicMetadataRefresh = SystemTime.milliseconds
        }
&lt;p&gt;1.3 getPartition(): If none of the partitions is available, we should throw LeaderNotAvailableException, instead of UnknownTopicOrPartitionException.&lt;/p&gt;

&lt;p&gt;2. DefaultPartitioner: Since key is not expected to be null, we should remove the code that deals with null key. &lt;/p&gt;

&lt;p&gt;3. The consumer side logic is fine. The consumer rebalance is only triggered when there are changes in partitions, not when there are changes in the availability of the partition. The rebalance logic doesn&apos;t depend on a partition being available. If a partition is not available, ConsumerFetcherManager will keep refreshing metadata. If you have a replication factor of 1, you will need to set a larger refresh.leader.backoff.ms, if a broker is expected to go down for a long time. &lt;/p&gt;</comment>
                            <comment id="13549885" author="brugidou" created="Thu, 10 Jan 2013 18:38:35 +0000"  >&lt;p&gt;Thanks for your feedback, I updated it (v2) according to your notes (1. and 2.).&lt;/p&gt;

&lt;p&gt;for 3. I believe you are right, except that:&lt;br/&gt;
3.1 It seems (correct me if i&apos;m wrong) that a rebalance happen at the consumer initialization, so that means a consumer can&apos;t start if a broker is down&lt;br/&gt;
3.2 Can a rebalance be triggered when a partition is added or moved? Having a broker down shouldn&apos;t prevent me from reassigning partitions or adding partitions.&lt;/p&gt;</comment>
                            <comment id="13549942" author="junrao" created="Thu, 10 Jan 2013 19:09:20 +0000"  >&lt;p&gt;Thanks for patch v2. Committed to 0.8 by renaming lastTopicMetadataRefresh to lastTopicMetadataRefreshTime and removing an unused comment.&lt;/p&gt;

&lt;p&gt;3.1 Rebalance happens during consumer initialization. It only needs the partition data to be in ZK and doesn&apos;t require all brokers to be up. Of course, if a broker is not up, the consumer may not be able to consume data from it. ConsumerFetcherManager is responsible for checking if a partition becomes available again.&lt;/p&gt;

&lt;p&gt;3.2 If the partition path changes in ZK, a rebalance will be triggered.&lt;/p&gt;</comment>
                            <comment id="13550338" author="brugidou" created="Thu, 10 Jan 2013 19:57:35 +0000"  >&lt;p&gt;Thanks for committing the patch.&lt;/p&gt;

&lt;p&gt;3.1 Are you sure that the rebalance doesn&apos;t require all partitions to have a leader? My experience earlier today was that the rebalance would fail and throw ConsumerRebalanceFailedException after having stopped all fetchers and cleared all queues. If you are sure then i&apos;ll try to reproduce the behavior I encountered, and maybe open a separate JIRA?&lt;/p&gt;</comment>
                            <comment id="13550450" author="junrao" created="Thu, 10 Jan 2013 21:31:49 +0000"  >&lt;p&gt;3.1 It shouldn&apos;t. However, if you can reproduce this problem, please file a new jira.&lt;/p&gt;</comment>
                            <comment id="13551961" author="junrao" created="Sat, 12 Jan 2013 16:58:38 +0000"  >&lt;p&gt;Another potential issue is that for producers that produce many topics (like migrationTool and mirrorMaker), the time-based refreshing may need to get the metadata for many topics. This means that the metadata request is likely to timeout. One solution is to break topics into batches in BrokerPartitionInfo.updateInfo() and issue a metadata request per batch. &lt;/p&gt;</comment>
                            <comment id="13551978" author="brugidou" created="Sat, 12 Jan 2013 18:02:11 +0000"  >&lt;p&gt;Should i make another patch? I&apos;ll try on Monday.&lt;/p&gt;

&lt;p&gt;1. It would probably require yet another config variable like &quot;producer.metadata.request.batch.size&quot; or something like that.&lt;br/&gt;
2. Should it be batched for every updateInfo() or just during the metadata refresh? It could help if we do the former because failing messages from many different topics could probably never go through if the metadata request timeouts.&lt;br/&gt;
3. Isn&apos;it getting a little convoluted? Maybe i am missing something but the producer side is getting trickier.&lt;br/&gt;
4. Please note that I also opened &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-693&quot; title=&quot;Consumer rebalance fails if no leader available for a partition and stops all fetchers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-693&quot;&gt;&lt;del&gt;KAFKA-693&lt;/del&gt;&lt;/a&gt; about the consumer side. And I&apos;d love to submit a patch but the rebalance logic seems complex so I&apos;d prefer to have some insights first before going in the wrong direction.&lt;/p&gt;</comment>
                            <comment id="13552336" author="junrao" created="Sun, 13 Jan 2013 22:11:51 +0000"  >&lt;p&gt;It would be great if you can provide a patch.&lt;/p&gt;

&lt;p&gt;1,2,3. Yes, we will need a new config. We should do batching in updateinfo(). This does make the producer side logic a bit more complicated. We have been thinking about making getMetadata faster. When we get there, we can revisit the batching logic.&lt;/p&gt;</comment>
                            <comment id="13552363" author="jkreps" created="Mon, 14 Jan 2013 00:34:49 +0000"  >&lt;p&gt;Does batching make sense versus just having people increase the timeout?&lt;/p&gt;</comment>
                            <comment id="13552371" author="junrao" created="Mon, 14 Jan 2013 01:00:47 +0000"  >&lt;p&gt;That&apos;s a good point. Increasing the timeout will work for most cases. If a broker goes down, the client request will get a socket exception immediately, independent of the timeout. So setting a large timeout doesn&apos;t hurt. When the broker host goes down and the client is waiting for a response from the server, I think the client will have to wait until the timeout. If we set a larger timeout, it means that the client has to wait longer before realizing the broker is down. However, since this is a rarer case, I think setting a larger timeout for now is probably good enough.&lt;/p&gt;</comment>
                            <comment id="13552759" author="brugidou" created="Mon, 14 Jan 2013 14:53:35 +0000"  >&lt;p&gt;So I wait for your feedback first, but I guess that increasing the time out is good enough, although it&apos;s 1500ms by default which is very short.&lt;/p&gt;</comment>
                            <comment id="13555560" author="junrao" created="Wed, 16 Jan 2013 22:53:03 +0000"  >&lt;p&gt;The last patch introduced a bug. DefaultEventHander.getPartition() is expected to return the index of the partitionList, instead of the actual partition id. Attach a patch that fixes the issue.&lt;/p&gt;</comment>
                            <comment id="13555633" author="junrao" created="Thu, 17 Jan 2013 00:00:10 +0000"  >&lt;p&gt;Attach the right patch (kafka-691_extra.patch).&lt;/p&gt;</comment>
                            <comment id="13556380" author="junrao" created="Thu, 17 Jan 2013 17:12:46 +0000"  >&lt;p&gt;Actually, the current code works since partitionId is always btw 0 and num.partition-1 and therefore it happens to also be the index of the partitionList. This patch just makes the code a bit better to understand.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12627146">KAFKA-693</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12564173" name="KAFKA-691-v1.patch" size="6379" author="brugidou" created="Thu, 10 Jan 2013 14:37:08 +0000"/>
                            <attachment id="12564212" name="KAFKA-691-v2.patch" size="6868" author="brugidou" created="Thu, 10 Jan 2013 18:38:35 +0000"/>
                            <attachment id="12565216" name="kafka-691_extra.patch" size="3239" author="junrao" created="Thu, 17 Jan 2013 00:00:10 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>303407</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 44 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i17azj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>250748</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>