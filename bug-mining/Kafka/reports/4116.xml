<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:42:20 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-18723] KRaft must handle corrupted records in the fetch response</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-18723</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;It is possible for a KRaft replica to send corrupted records to the fetching replicas in the FETCH response. This is because there is a race between when the FETCH response gets generated by the KRaft IO thread and when the network thread, or linux kernel, reads the byte position in the log segment.&lt;/p&gt;

&lt;p&gt;This race can generated corrupted records if the KRaft replica performed a truncation after the FETCH response was created but before the network thread read the bytes from the log segment.&lt;/p&gt;

&lt;p&gt;I have seen the following errors:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 [ERROR] 2025-01-07 15:04:18,273 [kafka-0-raft-io-thread] org.apache.kafka.server.fault.ProcessTerminatingFaultHandler handleFault - Encountered fatal fault: Unexpected error in raft IO thread
org.apache.kafka.common.KafkaException: Append failed unexpectedly
	at kafka.raft.KafkaMetadataLog.handleAndConvertLogAppendInfo(KafkaMetadataLog.scala:117)
	at kafka.raft.KafkaMetadataLog.appendAsFollower(KafkaMetadataLog.scala:110)
	at org.apache.kafka.raft.KafkaRaftClient.appendAsFollower(KafkaRaftClient.java:1227)
	at org.apache.kafka.raft.KafkaRaftClient.handleFetchResponse(KafkaRaftClient.java:1209)
	at org.apache.kafka.raft.KafkaRaftClient.handleResponse(KafkaRaftClient.java:1644)
	at org.apache.kafka.raft.KafkaRaftClient.handleInboundMessage(KafkaRaftClient.java:1770)
	at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:2355)
	at kafka.raft.KafkaRaftManager$RaftIoThread.doWork(RaftManager.scala:71)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:138)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 [ERROR] 2025-01-07 18:06:20,121 [kafka-1-raft-io-thread] org.apache.kafka.server.fault.ProcessTerminatingFaultHandler handleFault - Encountered fatal fault: Unexpected error in raft IO thread&quot;
org.apache.kafka.common.errors.CorruptRecordException: Record size 0 is less than the minimum record overhead (14)&quot;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This race also exists with Kafka&apos;s ISR based topic partition. In that case the replica fetcher catches all CorruptRecordException and InvalidRecordException.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; {
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; ime@(_: CorruptRecordException | _: InvalidRecordException) =&amp;gt;
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// we log the error and &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;. This ensures two things
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// 1. If there is a corrupt message in a topic partition, it does not bring the fetcher thread
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// &#160; &#160;down and cause other topic partition to also lag
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// 2. If the message is corrupt due to a &lt;span class=&quot;code-keyword&quot;&gt;transient&lt;/span&gt; state in the log (truncation, partial writes
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// &#160; &#160;can cause &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;), we simply &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt; and should get fixed in the subsequent fetches
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; error(s&lt;span class=&quot;code-quote&quot;&gt;&quot;Found invalid messages during fetch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition $topicPartition &quot;&lt;/span&gt; +
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; s&lt;span class=&quot;code-quote&quot;&gt;&quot;offset ${currentFetchState.fetchOffset}&quot;&lt;/span&gt;, ime)
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; partitionsWithError += topicPartition
 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The KRaft implementation doesn&apos;t handle this case:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160; &#160; &#160; &#160; &#160; &#160; &#160; } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; Records records = FetchResponse.recordsOrFail(partitionResponse);
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (records.sizeInBytes() &amp;gt; 0) {
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; appendAsFollower(records);
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; }
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; OptionalLong highWatermark = partitionResponse.highWatermark() &amp;lt; 0 ?
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; OptionalLong.empty() :
                      OptionalLong.of(partitionResponse.highWatermark());
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; updateFollowerHighWatermark(state, highWatermark);
&#160; &#160; &#160; &#160; &#160; &#160; &#160; }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13607176">KAFKA-18723</key>
            <summary>KRaft must handle corrupted records in the fetch response</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jsancio">Jos&#233; Armando Garc&#237;a Sancio</assignee>
                                    <reporter username="jsancio">Jos&#233; Armando Garc&#237;a Sancio</reporter>
                        <labels>
                    </labels>
                <created>Mon, 3 Feb 2025 20:59:55 +0000</created>
                <updated>Thu, 24 Apr 2025 15:29:55 +0000</updated>
                            <resolved>Wed, 9 Apr 2025 22:56:10 +0000</resolved>
                                                    <fixVersion>3.9.1</fixVersion>
                    <fixVersion>4.0.1</fixVersion>
                    <fixVersion>4.1.0</fixVersion>
                                    <component>kraft</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="17923477" author="jsancio" created="Mon, 3 Feb 2025 21:05:10 +0000"  >&lt;p&gt;What do you think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jgus&quot; class=&quot;user-hover&quot; rel=&quot;jgus&quot;&gt;jgus&lt;/a&gt; ? Does this match your understanding?&lt;/p&gt;</comment>
                            <comment id="17923632" author="mimaison" created="Tue, 4 Feb 2025 09:33:45 +0000"  >&lt;p&gt;Is this only an issue with 3.x releases? or does this also impact 4.0?&lt;/p&gt;</comment>
                            <comment id="17923786" author="jsancio" created="Tue, 4 Feb 2025 17:28:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mimaison&quot; class=&quot;user-hover&quot; rel=&quot;mimaison&quot;&gt;mimaison&lt;/a&gt; , this bug also affects trunk and 4.0.0. I didn&apos;t add the 4.0.0 fix version because I didn&apos;t want to block the 4.0.0 release and it is not technically a regression since this has been there since at least 3.7.0. In my experience this bug is rare and only causes controller and broker availability. It doesn&apos;t cause metadata inconsistency.&lt;/p&gt;

&lt;p&gt;Depending of when I get this merged to trunk, I&apos;ll cherry pick it to 4.0.0 or 4.0.1.&lt;/p&gt;</comment>
                            <comment id="17924303" author="junrao" created="Wed, 5 Feb 2025 22:44:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jsancio&quot; class=&quot;user-hover&quot; rel=&quot;jsancio&quot;&gt;jsancio&lt;/a&gt; : Thanks for reporting this. This does seem to be an issue. When a fetch request is served in the middle of truncation/append, the follower could get different variants of incorrect data. The simple case is that the incorrect data is corrupted as reported in this jira. We could catch CorruptRecordException and handle it accordingly. A more subtle and rare case is that the truncation happens to align on the batch boundary and the incorrect data is not corrupted. This can cause incorrect data to be silently and permanently added to the log.&lt;/p&gt;

&lt;p&gt;Consider that following steps.&lt;/p&gt;

&lt;p&gt;1. Replica A is the leader with epoch 10 and appends the following data (offset, leader epoch) to its log. HWM is at offset 101.&lt;/p&gt;

&lt;p&gt;&#160; &#160;Replica A&apos;s log (100, 10), (101, 10), (102, 10), (103, 10)&lt;/p&gt;

&lt;p&gt;2. Replica B is a follower and fetches from A with lastEpoch=10 and fetchOffset=101&lt;/p&gt;

&lt;p&gt;3. Replica A creates a FileRecords including offset 101 to 103. While the response is being sent, Replica C takes over as the new leader with epoch 11. Replica C only has offsets up to 101 at epoch 10 and appends two new records starting at offset 102 with epoch 11.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&#160;Replica C&apos;s log (100, 10), (101, 10), (102, 11), (103, 11)&lt;/p&gt;

&lt;p&gt;4. Replica A follows replica C. It truncates offset 102 and then fetches the new data. Replica A&apos;s log becomes the following, same as C.&lt;/p&gt;

&lt;p&gt;&#160;&#160;Replica A&apos;s log (100, 10), (101, 10), (102, 11), (103, 11)&lt;/p&gt;

&lt;p&gt;5. Replica B finally receives the response from step 2. The response could include a mix of&#160; A&apos;s data in step 1 and in step 4, such as (101, 10), (102, 10), (103, 11). This will be appended to B&apos;s log, but (102, 10) doesn&apos;t match the data in replica C and is incorrect.&lt;/p&gt;

&lt;p&gt;6. Replica B then follows and fetches from replica C with lastEpoch=11 and fetchOffset=104. Replica C verifies that lastEpoch/fetchOffset match the epoch in its local log and accepts the fetch request without reporting divergent epoch. Now, the incorrect (102, 10) will be permanently in replica B.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;As for the fix, one potential solution is for a follower to reject any batches whose leader epoch is higher than its current leader epoch. Intuitively, any batch with a higher epoch should be obtained from the new leader when the follower receives the new leader epoch. If we do this, in step 5, replica B can only append (101, 10), (102, 10) to its log. (103, 11) has a higher epoch than replica B and will be rejected. Then in step 6, replica B will fetch from replica C with lastEpoch=10 and fetchOffset=103. Replica C will detect there is log divergence since epoch 10 ends at offset 101 in its log. Once replica B receives the diverging epoch in the response, it will remove (102, 10) from its log.&lt;/p&gt;</comment>
                            <comment id="17924342" author="jsancio" created="Thu, 6 Feb 2025 01:48:00 +0000"  >&lt;p&gt;Thanks for the comment &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; . Good catch on the example that results in an inconsistent log. I like your suggestion. It seems correct to me since the following replica, B in your example, can only append records up to the old leader&apos;s epoch (A). This is true in KRaft because replica B only handles the records in the FETCH responses if the local epoch matches the remote replica&apos;s epoch.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &#160; &#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (epoch &amp;lt; quorum.epoch() || error == Errors.UNKNOWN_LEADER_EPOCH) {
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// We have a larger epoch, so the response is no longer relevant
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; Optional.of(value:&#8195;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
&#160; &#160; &#160; &#160; &#160; } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (epoch &amp;gt; quorum.epoch()
&#160; &#160; &#160; &#160; &#160; &#160; &#160; || error == Errors.FENCED_LEADER_EPOCH
&#160; &#160; &#160; &#160; &#160; &#160; &#160; || error == Errors.NOT_LEADER_OR_FOLLOWER) {
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// The response indicates that the request had a stale epoch, but we need
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-comment&quot;&gt;// to validate the epoch from the response against our current state.
&lt;/span&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; maybeTransition(leaderId, epoch, leaderEndpoints, currentTimeMs);
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; Optional.of(value:&#8195;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
          } ... &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For this solution to work for ISR partitions the abstract replica fetcher needs to make sure that it&apos;s local partition leader epoch still matches the remote partition leader epoch. Right now the replica fetcher only does this check.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &#160; &#160; &#160; &#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (fetchPartitionData != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; fetchPartitionData.fetchOffset == currentFetchState.fetchOffset &amp;amp;&amp;amp; currentFetchState.isReadyForFetch) { &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;After that I think the implementation is relatively simple if we extend the &lt;tt&gt;UnifiedLog#appendAsFollower&lt;/tt&gt; to include the local leader epoch and slice the Records up to the last batch that has a partition leader epoch less than the current leader epoch. We don&apos;t need to introduce a new AppendOrigin since the same solution should apply to both KRaft and ISR partitions.&lt;/p&gt;</comment>
                            <comment id="17924642" author="junrao" created="Thu, 6 Feb 2025 17:35:53 +0000"  >&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;For this solution to work for ISR partitions the abstract replica fetcher needs to make sure that it&apos;s local partition leader epoch still matches the remote partition leader epoch.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;br/&gt;
For ISR fetch, Partition.checkCurrentLeaderEpoch() already ensures that the CurrentLeaderEpoch in the fetch request matches the epoch in the leader. So, it seems that this part is already covered.&lt;/p&gt;</comment>
                            <comment id="17926536" author="jsancio" created="Wed, 12 Feb 2025 18:29:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; right but that check is done on the leader. In essence, what we are trying to solve is the case when the partition leader epoch on the leader changed after the FETCH request was handled but before the log segment was read. Let&apos;s consider this ISR scenario but something similar can happen in KRaft:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Partition has replicas A, B, C and they all agreed that the leader epoch is 10 and the leader is A.&lt;/li&gt;
	&lt;li&gt;Replica B sends a FETCH request with current leader epoch of 10 and fetch offset of 101. Similar to your example.&lt;/li&gt;
	&lt;li&gt;Replica A, the leader, handles the FETCH request, all of its validations pass and sends the FETCH response to the network server.&lt;/li&gt;
	&lt;li&gt;Replica C gets elected leader by the controller and updates all of the replicas (A, B, C). So now the replicas have a leader epoch of 11 and leader of C.&lt;/li&gt;
	&lt;li&gt;Replica A truncates and appends its log in the same way as your example: (100, 10), (101, 10), (102, 11), (103, 11)&lt;/li&gt;
	&lt;li&gt;The network layer sends (101, 10), (102, 11), (103, 11) from A to B.&lt;/li&gt;
	&lt;li&gt;Replica B handles the FETCH response but appends up to and including (103, 11) because it&apos;s epoch is 11.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In short, the replicating replica cannot use its own leader epoch to determine up to which batch to append but instead must use the leader epoch when the FETCH request was sent and handled. What do you think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="17926585" author="junrao" created="Wed, 12 Feb 2025 21:45:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jsancio&quot; class=&quot;user-hover&quot; rel=&quot;jsancio&quot;&gt;jsancio&lt;/a&gt; : Good point. I think we can tighten this up on the follower side. AbstractFetcherThread keeps for each partition a PartitionFetchState, which includes the current leader epoch. The leader epoch is used to build the fetch request. In processFetchRequest(), we pass in the request as well as the fetch response. We can make a change such that if the leader epoch in the request doesn&apos;t match the one in PartitionFetchState, we will skip the processing of the fetch response since it&apos;s stable. If they match, we can use the leader epoch in PartitionFetchState to skip batches with higher epochs. Since the processing is done under partitionMapLock, we can be sure that the PartitionFetchState won&apos;t change in the meantime.&#160;&lt;/p&gt;</comment>
                            <comment id="17926622" author="jsancio" created="Thu, 13 Feb 2025 00:31:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; , thanks. I implemented something similar. I think we can move this conversation to the PR. I finished the src/main changes, if you want to take a look. I am still working through the new tests that I want to add.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="13614641">KAFKA-19114</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310660">
                    <name>Completes</name>
                                            <outwardlinks description="fixes">
                                        <issuelink>
            <issuekey id="13606688">KAFKA-18671</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            38 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1tzo0:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>