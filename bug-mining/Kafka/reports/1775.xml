<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:05:48 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-4335] FileStreamSource Connector not working for large files (~ 1GB)</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-4335</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;I was trying to sink large file about (1gb). FileStreamSource connector is not working for that it&apos;s working fine for small files.  &lt;/p&gt;</description>
                <environment></environment>
        <key id="13014743">KAFKA-4335</key>
            <summary>FileStreamSource Connector not working for large files (~ 1GB)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="shukla2009">Rahul Shukla</reporter>
                        <labels>
                    </labels>
                <created>Mon, 24 Oct 2016 14:33:12 +0000</created>
                <updated>Sat, 6 Jan 2018 02:33:40 +0000</updated>
                            <resolved>Sat, 6 Jan 2018 02:32:16 +0000</resolved>
                                    <version>0.10.0.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>connect</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="15602851" author="ewencp" created="Mon, 24 Oct 2016 18:42:42 +0000"  >&lt;p&gt;Can you be more specific about what isn&apos;t working? Does it throw an exception or some other error?&lt;/p&gt;</comment>
                            <comment id="15604115" author="shukla2009" created="Tue, 25 Oct 2016 04:11:33 +0000"  >&lt;p&gt;It did not throw any exception but not producing content to the topic as well. I looked into source code and find that it&apos;s trying to read the file in memory and then produce the record. Which I believe it&apos;s difficult for hold entire file in memory. Below is source code snippet which tries to do ...  &lt;/p&gt;

&lt;p&gt;            int nread = 0;&lt;br/&gt;
            while (readerCopy.ready()) {&lt;br/&gt;
                nread = readerCopy.read(buffer, offset, buffer.length - offset);&lt;br/&gt;
                log.trace(&quot;Read {} bytes from {}&quot;, nread, logFilename());&lt;/p&gt;

&lt;p&gt;                if (nread &amp;gt; 0) {&lt;br/&gt;
                    offset += nread;&lt;br/&gt;
                    if (offset == buffer.length) &lt;/p&gt;
{
                        char[] newbuf = new char[buffer.length * 2];
                        System.arraycopy(buffer, 0, newbuf, 0, buffer.length);
                        buffer = newbuf;
                    }

&lt;p&gt;                    String line;&lt;br/&gt;
                    do {&lt;br/&gt;
                        line = extractLine();&lt;br/&gt;
                        if (line != null) {&lt;br/&gt;
                            log.trace(&quot;Read a line from {}&quot;, logFilename());&lt;br/&gt;
                            if (records == null)&lt;br/&gt;
                                records = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
                            records.add(new SourceRecord(offsetKey(filename), offsetValue(streamOffset), topic, VALUE_SCHEMA, line));&lt;br/&gt;
                        }&lt;br/&gt;
                    } while (line != null);&lt;br/&gt;
                }&lt;br/&gt;
            }&lt;/p&gt;</comment>
                            <comment id="15604522" author="shukla2009" created="Tue, 25 Oct 2016 07:41:24 +0000"  >&lt;p&gt;Yes I got this exception on producer console &lt;/p&gt;

&lt;p&gt;java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
	at org.apache.kafka.connect.file.FileStreamSourceTask.poll(FileStreamSourceTask.java:135)&lt;br/&gt;
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:155)&lt;br/&gt;
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:140)&lt;br/&gt;
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:175)&lt;br/&gt;
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&lt;br/&gt;
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="16303141" author="githubbot" created="Mon, 25 Dec 2017 08:39:47 +0000"  >&lt;p&gt;phstudy opened a new pull request #4356: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-4335&quot; title=&quot;FileStreamSource Connector not working for large files (~ 1GB)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-4335&quot;&gt;&lt;del&gt;KAFKA-4335&lt;/del&gt;&lt;/a&gt;: Add batch.size to FileStreamSource connector to prevent OOM&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4356&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4356&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   When the source file of `FileStreamSource` is a large file, `FileStreamSourceTask.poll()` will result in OOM. This pull request added `batch.size` parameter which can restrict the poll size.&lt;/p&gt;

&lt;p&gt;   *More detailed description of your change,&lt;br/&gt;
   if necessary. The PR title and PR message become&lt;br/&gt;
   the squashed commit message, so use a separate&lt;br/&gt;
   comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   for the feature or bug fix. Unit and/or integration&lt;br/&gt;
   tests are expected for any behaviour change and&lt;br/&gt;
   system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16314313" author="ewencp" created="Sat, 6 Jan 2018 02:32:16 +0000"  >&lt;p&gt;Issue resolved by pull request 4356&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/4356&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4356&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16314316" author="githubbot" created="Sat, 6 Jan 2018 02:33:40 +0000"  >&lt;p&gt;ewencp closed pull request #4356: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-4335&quot; title=&quot;FileStreamSource Connector not working for large files (~ 1GB)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-4335&quot;&gt;&lt;del&gt;KAFKA-4335&lt;/del&gt;&lt;/a&gt;: Add batch.size to FileStreamSource connector to prevent OOM&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4356&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4356&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java b/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java&lt;br/&gt;
index 335fe925519..59006dae4f4 100644&lt;br/&gt;
&amp;#8212; a/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java&lt;br/&gt;
+++ b/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java&lt;br/&gt;
@@ -36,13 +36,18 @@&lt;br/&gt;
 public class FileStreamSourceConnector extends SourceConnector {&lt;br/&gt;
     public static final String TOPIC_CONFIG = &quot;topic&quot;;&lt;br/&gt;
     public static final String FILE_CONFIG = &quot;file&quot;;&lt;br/&gt;
+    public static final String TASK_BATCH_SIZE_CONFIG = &quot;batch.size&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    public static final int DEFAULT_TASK_BATCH_SIZE = 2000;&lt;/p&gt;

&lt;p&gt;     private static final ConfigDef CONFIG_DEF = new ConfigDef()&lt;br/&gt;
         .define(FILE_CONFIG, Type.STRING, null, Importance.HIGH, &quot;Source filename. If not specified, the standard input will be used&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.define(TOPIC_CONFIG, Type.STRING, Importance.HIGH, &quot;The topic to publish data to&quot;);&lt;br/&gt;
+        .define(TOPIC_CONFIG, Type.STRING, Importance.HIGH, &quot;The topic to publish data to&quot;)&lt;br/&gt;
+        .define(TASK_BATCH_SIZE_CONFIG, Type.INT, Importance.LOW, &quot;The maximum number of records the Source task can read from file one time&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private String filename;&lt;br/&gt;
     private String topic;&lt;br/&gt;
+    private int batchSize = DEFAULT_TASK_BATCH_SIZE;&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public String version() {&lt;br/&gt;
@@ -57,6 +62,14 @@ public void start(Map&amp;lt;String, String&amp;gt; props) {&lt;br/&gt;
             throw new ConnectException(&quot;FileStreamSourceConnector configuration must include &apos;topic&apos; setting&quot;);&lt;br/&gt;
         if (topic.contains(&quot;,&quot;))&lt;br/&gt;
             throw new ConnectException(&quot;FileStreamSourceConnector should only have a single topic when used as a source.&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        if (props.containsKey(TASK_BATCH_SIZE_CONFIG)) {&lt;br/&gt;
+            try &lt;/p&gt;
{
+                batchSize = Integer.parseInt(props.get(TASK_BATCH_SIZE_CONFIG));
+            }
&lt;p&gt; catch (NumberFormatException e) &lt;/p&gt;
{
+                throw new ConnectException(&quot;Invalid FileStreamSourceConnector configuration&quot;, e);
+            }
&lt;p&gt;+        }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
@@ -72,6 +85,7 @@ public void start(Map&amp;lt;String, String&amp;gt; props) &lt;/p&gt;
{
         if (filename != null)
             config.put(FILE_CONFIG, filename);
         config.put(TOPIC_CONFIG, topic);
+        config.put(TASK_BATCH_SIZE_CONFIG, String.valueOf(batchSize));
         configs.add(config);
         return configs;
     }
&lt;p&gt;diff --git a/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java b/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java&lt;br/&gt;
index 8edf385611a..482102f7859 100644&lt;br/&gt;
&amp;#8212; a/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java&lt;br/&gt;
+++ b/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java&lt;br/&gt;
@@ -50,6 +50,7 @@&lt;br/&gt;
     private char[] buffer = new char&lt;span class=&quot;error&quot;&gt;&amp;#91;1024&amp;#93;&lt;/span&gt;;&lt;br/&gt;
     private int offset = 0;&lt;br/&gt;
     private String topic = null;&lt;br/&gt;
+    private int batchSize = FileStreamSourceConnector.DEFAULT_TASK_BATCH_SIZE;&lt;/p&gt;

&lt;p&gt;     private Long streamOffset;&lt;/p&gt;

&lt;p&gt;@@ -70,6 +71,14 @@ public void start(Map&amp;lt;String, String&amp;gt; props) {&lt;br/&gt;
         topic = props.get(FileStreamSourceConnector.TOPIC_CONFIG);&lt;br/&gt;
         if (topic == null)&lt;br/&gt;
             throw new ConnectException(&quot;FileStreamSourceTask config missing topic setting&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        if (props.containsKey(FileStreamSourceConnector.TASK_BATCH_SIZE_CONFIG)) {&lt;br/&gt;
+            try &lt;/p&gt;
{
+                batchSize = Integer.parseInt(props.get(FileStreamSourceConnector.TASK_BATCH_SIZE_CONFIG));
+            }
&lt;p&gt; catch (NumberFormatException e) &lt;/p&gt;
{
+                throw new ConnectException(&quot;Invalid FileStreamSourceTask configuration&quot;, e);
+            }
&lt;p&gt;+        }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
@@ -146,6 +155,10 @@ public void start(Map&amp;lt;String, String&amp;gt; props) {&lt;br/&gt;
                                 records = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
                             records.add(new SourceRecord(offsetKey(filename), offsetValue(streamOffset), topic, null,&lt;br/&gt;
                                     null, null, VALUE_SCHEMA, line, System.currentTimeMillis()));&lt;br/&gt;
+&lt;br/&gt;
+                            if (records.size() &amp;gt;= batchSize) &lt;/p&gt;
{
+                                return records;
+                            }
&lt;p&gt;                         }&lt;br/&gt;
                     } while (line != null);&lt;br/&gt;
                 }&lt;br/&gt;
diff --git a/connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceTaskTest.java b/connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceTaskTest.java&lt;br/&gt;
index cde6c43ae9a..3cb7128e206 100644&lt;br/&gt;
&amp;#8212; a/connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceTaskTest.java&lt;br/&gt;
+++ b/connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceTaskTest.java&lt;br/&gt;
@@ -127,6 +127,30 @@ public void testNormalLifecycle() throws InterruptedException, IOException &lt;/p&gt;
{
         task.stop();
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testBatchSize() throws IOException, InterruptedException {&lt;br/&gt;
+        expectOffsetLookupReturnNone();&lt;br/&gt;
+        replay();&lt;br/&gt;
+&lt;br/&gt;
+        config.put(FileStreamSourceConnector.TASK_BATCH_SIZE_CONFIG, &quot;5000&quot;);&lt;br/&gt;
+        task.start(config);&lt;br/&gt;
+&lt;br/&gt;
+        FileOutputStream os = new FileOutputStream(tempFile);&lt;br/&gt;
+        for (int i = 0; i &amp;lt; 10_000; i++) &lt;/p&gt;
{
+            os.write(&quot;Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...\n&quot;.getBytes());
+        }
&lt;p&gt;+        os.flush();&lt;br/&gt;
+&lt;br/&gt;
+        List&amp;lt;SourceRecord&amp;gt; records = task.poll();&lt;br/&gt;
+        assertEquals(5000, records.size());&lt;br/&gt;
+&lt;br/&gt;
+        records = task.poll();&lt;br/&gt;
+        assertEquals(5000, records.size());&lt;br/&gt;
+&lt;br/&gt;
+        os.close();&lt;br/&gt;
+        task.stop();&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test(expected = ConnectException.class)&lt;br/&gt;
     public void testMissingTopic() throws InterruptedException {&lt;br/&gt;
         replay();&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 45 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i35axr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>