<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:37:08 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-727] broker can still expose uncommitted data to a consumer</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-727</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Even after kafka-698 is fixed, we still see consumer clients occasionally see uncommitted data. The following is how this can happen.&lt;/p&gt;

&lt;p&gt;1. In Log.read(), we pass in startOffset &amp;lt; HW and maxOffset = HW.&lt;br/&gt;
2. Then we call LogSegment.read(), in which we call translateOffset on the maxOffset. The offset doesn&apos;t exist and translateOffset returns null.&lt;br/&gt;
3. Continue in LogSegment.read(), we then call messageSet.sizeInBytes() to fetch and return the data.&lt;/p&gt;

&lt;p&gt;What can happen is that between step 2 and step 3, a new message is appended to the log and is not committed yet. Now, we have exposed uncommitted data to the client.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12628852">KAFKA-727</key>
            <summary>broker can still expose uncommitted data to a consumer</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jkreps">Jay Kreps</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                            <label>p1</label>
                    </labels>
                <created>Wed, 23 Jan 2013 05:22:07 +0000</created>
                <updated>Tue, 23 Dec 2014 17:57:21 +0000</updated>
                            <resolved>Wed, 23 Jan 2013 22:10:06 +0000</resolved>
                                    <version>0.8.0</version>
                                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13560402" author="junrao" created="Wed, 23 Jan 2013 05:29:17 +0000"  >&lt;p&gt;One way to fix this is for FileMessageSet.searchFor() to return OffsetPosition(-1L, the value of size) if offset is not found, instead of returning null. In LogSegment.read(), we can use the returned position to guard the length of messageSet.read.  &lt;/p&gt;</comment>
                            <comment id="13560410" author="jkreps" created="Wed, 23 Jan 2013 05:41:58 +0000"  >&lt;p&gt;Fantastic catch.&lt;/p&gt;

&lt;p&gt;I think another fix is to just save the size of the log prior to translating the hw mark and use this rather than dynamically checking log.sizeInBytes later in the method. This will effectively act as a valid lower bound.&lt;/p&gt;

&lt;p&gt;It might also be worthwhile to write a throw away torture test that has one thread do appends and another thread do reads and check that this condition is not violated in case there are any more of these subtleties. &lt;/p&gt;

&lt;p&gt;Happy to take this one on since it is my bad.&lt;/p&gt;</comment>
                            <comment id="13560412" author="junrao" created="Wed, 23 Jan 2013 05:49:12 +0000"  >&lt;p&gt;Jay, sure, you can take this on. The way we saw this is that we had a consumer client that uses minBytes=1 and a producer that produces data once every couple of secs.&lt;/p&gt;</comment>
                            <comment id="13560806" author="nehanarkhede" created="Wed, 23 Jan 2013 16:30:25 +0000"  >&lt;p&gt;&amp;gt;&amp;gt; I think another fix is to just save the size of the log prior to translating the hw mark and use this rather than dynamically checking log.sizeInBytes later in the method. This will effectively act as a valid lower bound.&lt;/p&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt; It might also be worthwhile to write a throw away torture test that has one thread do appends and another thread do reads and check that this condition is not violated in case there are any more of these subtleties.&lt;/p&gt;

&lt;p&gt;This will be really good to have going forward&lt;/p&gt;</comment>
                            <comment id="13561063" author="jkreps" created="Wed, 23 Jan 2013 20:29:02 +0000"  >&lt;p&gt;Patch v1.&lt;br/&gt;
1. Snapshot the size of the log prior to translating the maxOffset to a file position to give a consistent end point.&lt;br/&gt;
2. Fix docs on read so they match the code&lt;br/&gt;
3. Add a stress test that does reads and writes at the same point to validate fix&lt;/p&gt;</comment>
                            <comment id="13561139" author="nehanarkhede" created="Wed, 23 Jan 2013 22:02:40 +0000"  >&lt;p&gt;+1. Minor change before checkin&lt;br/&gt;
Remove unused import &quot;import joptsimple._&quot; from StressTestLog&lt;/p&gt;</comment>
                            <comment id="13561147" author="nehanarkhede" created="Wed, 23 Jan 2013 22:10:06 +0000"  >&lt;p&gt;Checked in patch v1 to proceed with build.&lt;/p&gt;</comment>
                            <comment id="14234678" author="lokeshbirla" created="Thu, 4 Dec 2014 21:46:18 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Is this really fixed? I still see this issue when I am using 4 topics, 3 partitions and 3 replication factor.  I am using kafka_2.9.2-0.8.1.1.&lt;br/&gt;
Currently I am using 3 node broker and 1 zookeeper. I did not see this issue when I used 1,2 or 3 topics. &lt;/p&gt;



&lt;p&gt;2014-08-18 06:43:58,356] ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaApi-1&amp;#93;&lt;/span&gt; Error when processing fetch request for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;mmetopic4,2&amp;#93;&lt;/span&gt; offset 1940029 from consumer with correlation id 21 (kafka.server.Kaf&lt;br/&gt;
kaApis)&lt;br/&gt;
java.lang.IllegalArgumentException: Attempt to read with a maximum offset (1818353) less than the start offset (1940029).&lt;br/&gt;
        at kafka.log.LogSegment.read(LogSegment.scala:136)&lt;br/&gt;
        at kafka.log.Log.read(Log.scala:386)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:530)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:476)&lt;br/&gt;
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:471)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:119)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)&lt;br/&gt;
        at scala.collection.immutable.Map$Map1.map(Map.scala:107)&lt;br/&gt;
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:471)&lt;br/&gt;
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:783)&lt;br/&gt;
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:765)&lt;br/&gt;
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:216)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;


&lt;p&gt;THanks for your help. &lt;/p&gt;</comment>
                            <comment id="14244581" author="junrao" created="Fri, 12 Dec 2014 18:37:22 +0000"  >&lt;p&gt;Is there an easy way to reproduce this issue? Thanks,&lt;/p&gt;</comment>
                            <comment id="14256154" author="lokeshbirla" created="Mon, 22 Dec 2014 20:27:06 +0000"  >&lt;p&gt;Hi Jun and Jay,&lt;br/&gt;
Also, I can see this issue with producer itself without running any consumer. &lt;/p&gt;

&lt;p&gt;I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1806&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-1806&lt;/a&gt; for this issue. Could you please have a look? &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1806?focusedCommentId=14254551&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14254551&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-1806?focusedCommentId=14254551&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14254551&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Please check my latest comment. &lt;/p&gt;

&lt;p&gt;Lokesh&lt;/p&gt;</comment>
                            <comment id="14257271" author="nehanarkhede" created="Tue, 23 Dec 2014 17:57:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lokeshbirla&quot; class=&quot;user-hover&quot; rel=&quot;lokeshbirla&quot;&gt;lokeshbirla&lt;/a&gt; Again, pasting &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;&apos;s comment here again&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Is there an easy way to reproduce this issue?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What all of us are looking for is steps (a reproducible test case) that we can run through on trunk, to see the same problem and error you do. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12628820">KAFKA-725</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12566183" name="KAFKA-727-v1.patch" size="6011" author="jkreps" created="Wed, 23 Jan 2013 20:29:02 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>308332</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 47 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1b6jr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>273359</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>