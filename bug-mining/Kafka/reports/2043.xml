<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:14:45 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7280] ConcurrentModificationException in FetchSessionHandler in heartbeat thread</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7280</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Request/response handling in FetchSessionHandler is not thread-safe. But we are using it in Kafka consumer without any synchronization even though poll() from heartbeat thread can process responses. Heartbeat thread holds the coordinator lock while processing its poll and responses, making other operations involving the group coordinator safe. We also need to lock FetchSessionHandler for the operations that update or read FetchSessionHandler#sessionPartitions.&lt;/p&gt;

&lt;p&gt;This exception is from a system test run on trunk of TestSecurityRollingUpgrade.test_rolling_upgrade_sasl_mechanism_phase_two:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Heartbeat thread failed due to unexpected error (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)&lt;br/&gt;
 java.util.ConcurrentModificationException&lt;br/&gt;
 at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)&lt;br/&gt;
 at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:742)&lt;br/&gt;
 at org.apache.kafka.clients.FetchSessionHandler.responseDataToLogString(FetchSessionHandler.java:362)&lt;br/&gt;
 at org.apache.kafka.clients.FetchSessionHandler.handleResponse(FetchSessionHandler.java:424)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:216)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:206)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:575)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:389)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:304)&lt;br/&gt;
 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:996)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The logs just prior to the exception show that a partition was removed from the session:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,315&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Skipping fetch for partition test_topic-1 because there is an in-flight request to worker4:9095 (id: 3 rack: null) (org.apache.kafka.clients.consumer.internals.Fetcher)&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Completed receive from node 2 for FETCH with correlation id 417, received {throttle_time_ms=0,error_code=0,session_id=109800960,responses=[{topic=test_topic,partition_responses=[&lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {partition_header=
Unknown macro}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,record_set=&lt;span class=&quot;error&quot;&gt;&amp;#91;(record=DefaultRecord(offset=183, timestamp=1534054402327, key=0 bytes, value=3 bytes))&amp;#93;&lt;/span&gt;}]}]} (org.apache.kafka.clients.NetworkClient)&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Added READ_UNCOMMITTED fetch request for partition test_topic-0 at offset 189 to node worker3:9095 (id: 2 rack: null) (org.apache.kafka.clients.consumer.internals.Fetcher)&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Built incremental fetch (sessionId=109800960, epoch=237) for node 2. Added (), altered (), removed (test_topic-2) out of (test_topic-0) (org.apache.kafka.clients.FetchSessionHandler)&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(), toForget=(test_topic-2), implied=(test_topic-0)) to broker worker3:9095 (id: 2 rack: null) (org.apache.kafka.clients.consumer.internals.Fetcher)&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Sending FETCH &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {replica_id=-1,max_wait_time=500,min_bytes=1,max_bytes=52428800,isolation_level=0,session_id=109800960,epoch=237,topics=[],forgotten_topics_data=[
Unknown macro}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;]} with correlation id 418 to node 2 (org.apache.kafka.clients.NetworkClient)&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; TRACE &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Skipping fetch for partition test_topic-2 because there is an in-flight request to worker3:9095 (id: 2 rack: null) (org.apache.kafka.clients.consumer.internals.Fetcher)&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;2018-08-12 06:13:22,316&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Consumer clientId=console-consumer, groupId=group&amp;#93;&lt;/span&gt; Heartbeat thread failed due to unexpected error (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)&lt;br/&gt;
 java.util.ConcurrentModificationException&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The sequence in the logs show&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;FETCH response received&lt;/li&gt;
	&lt;li&gt;FetchSessionHandler#sessionPartitions is updated (a partition is removed)&lt;/li&gt;
	&lt;li&gt;New FETCH request is sent&lt;/li&gt;
	&lt;li&gt;Heartbeat thread throws ConcurrentModificationException while iterating over FetchSessionHandler#sessionPartitions&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This could be because 1) and 4) were on the heartbeat thread and 2) and 3) on the thread processing Consumer#poll().&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13178524">KAFKA-7280</key>
            <summary>ConcurrentModificationException in FetchSessionHandler in heartbeat thread</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsivaram">Rajini Sivaram</assignee>
                                    <reporter username="rsivaram">Rajini Sivaram</reporter>
                        <labels>
                    </labels>
                <created>Mon, 13 Aug 2018 07:45:45 +0000</created>
                <updated>Fri, 16 Nov 2018 12:18:53 +0000</updated>
                            <resolved>Fri, 14 Sep 2018 19:01:27 +0000</resolved>
                                    <version>1.1.1</version>
                    <version>2.0.0</version>
                                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>consumer</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16578243" author="githubbot" created="Mon, 13 Aug 2018 12:55:35 +0000"  >&lt;p&gt;rajinisivaram opened a new pull request #5495: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7280&quot; title=&quot;ConcurrentModificationException in FetchSessionHandler in heartbeat thread&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7280&quot;&gt;&lt;del&gt;KAFKA-7280&lt;/del&gt;&lt;/a&gt;: Synchronize consumer fetch request/response handling&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5495&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5495&lt;/a&gt;&lt;/p&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16615242" author="githubbot" created="Fri, 14 Sep 2018 18:51:49 +0000"  >&lt;p&gt;hachikuji closed pull request #5495: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7280&quot; title=&quot;ConcurrentModificationException in FetchSessionHandler in heartbeat thread&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7280&quot;&gt;&lt;del&gt;KAFKA-7280&lt;/del&gt;&lt;/a&gt;: Synchronize consumer fetch request/response handling&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5495&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5495&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index c84dd6f9b7b..86ee74ef80d 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -93,7 +93,21 @@&lt;br/&gt;
 import static org.apache.kafka.common.serialization.ExtendedDeserializer.Wrapper.ensureExtended;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* This class manage the fetching process with the brokers.&lt;br/&gt;
+ * This class manages the fetching process with the brokers.&lt;br/&gt;
+ * &amp;lt;p&amp;gt;&lt;br/&gt;
+ * Thread-safety:&lt;br/&gt;
+ * Requests and responses of Fetcher may be processed by different threads since heartbeat&lt;br/&gt;
+ * thread may process responses. Other operations are single-threaded and invoked only from&lt;br/&gt;
+ * the thread polling the consumer.&lt;br/&gt;
+ * &amp;lt;ul&amp;gt;&lt;br/&gt;
+ *     &amp;lt;li&amp;gt;If a response handler accesses any shared state of the Fetcher (e.g. FetchSessionHandler),&lt;br/&gt;
+ *     all access to that state must be synchronized on the Fetcher instance.&amp;lt;/li&amp;gt;&lt;br/&gt;
+ *     &amp;lt;li&amp;gt;If a response handler accesses any shared state of the coordinator (e.g. SubscriptionState),&lt;br/&gt;
+ *     it is assumed that all access to that state is synchronized on the coordinator instance by&lt;br/&gt;
+ *     the caller.&amp;lt;/li&amp;gt;&lt;br/&gt;
+ *     &amp;lt;li&amp;gt;Responses that collate partial responses from multiple brokers (e.g. to list offsets) are&lt;br/&gt;
+ *     synchronized on the response future.&amp;lt;/li&amp;gt;&lt;br/&gt;
+ * &amp;lt;/ul&amp;gt;&lt;br/&gt;
  */&lt;br/&gt;
 public class Fetcher&amp;lt;K, V&amp;gt; implements SubscriptionState.Listener, Closeable {&lt;br/&gt;
     private final Logger log;&lt;br/&gt;
@@ -191,7 +205,7 @@ public boolean hasCompletedFetches() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;an in-flight fetch or pending fetch data.&lt;/li&gt;
	&lt;li&gt;@return number of fetches sent&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public int sendFetches() {&lt;br/&gt;
+    public synchronized int sendFetches() {&lt;br/&gt;
         Map&amp;lt;Node, FetchSessionHandler.FetchRequestData&amp;gt; fetchRequestMap = prepareFetchRequests();&lt;br/&gt;
         for (Map.Entry&amp;lt;Node, FetchSessionHandler.FetchRequestData&amp;gt; entry : fetchRequestMap.entrySet()) {&lt;br/&gt;
             final Node fetchTarget = entry.getKey();&lt;br/&gt;
@@ -209,39 +223,43 @@ public int sendFetches() {&lt;br/&gt;
                     .addListener(new RequestFutureListener&amp;lt;ClientResponse&amp;gt;() {&lt;br/&gt;
                         @Override&lt;br/&gt;
                         public void onSuccess(ClientResponse resp) {&lt;/li&gt;
	&lt;li&gt;FetchResponse&amp;lt;Records&amp;gt; response = (FetchResponse&amp;lt;Records&amp;gt;) resp.responseBody();&lt;/li&gt;
	&lt;li&gt;FetchSessionHandler handler = sessionHandlers.get(fetchTarget.id());&lt;/li&gt;
	&lt;li&gt;if (handler == null) {&lt;/li&gt;
	&lt;li&gt;log.error(&quot;Unable to find FetchSessionHandler for node {}. Ignoring fetch response.&quot;,&lt;/li&gt;
	&lt;li&gt;fetchTarget.id());&lt;/li&gt;
	&lt;li&gt;return;&lt;br/&gt;
+                            synchronized (Fetcher.this) {&lt;br/&gt;
+                                FetchResponse&amp;lt;Records&amp;gt; response = (FetchResponse&amp;lt;Records&amp;gt;) resp.responseBody();&lt;br/&gt;
+                                FetchSessionHandler handler = sessionHandler(fetchTarget.id());&lt;br/&gt;
+                                if (handler == null) {&lt;br/&gt;
+                                    log.error(&quot;Unable to find FetchSessionHandler for node {}. Ignoring fetch response.&quot;,&lt;br/&gt;
+                                            fetchTarget.id());&lt;br/&gt;
+                                    return;&lt;br/&gt;
+                                }&lt;br/&gt;
+                                if (!handler.handleResponse(response)) 
{
+                                    return;
+                                }
&lt;p&gt;+&lt;br/&gt;
+                                Set&amp;lt;TopicPartition&amp;gt; partitions = new HashSet&amp;lt;&amp;gt;(response.responseData().keySet());&lt;br/&gt;
+                                FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);&lt;br/&gt;
+&lt;br/&gt;
+                                for (Map.Entry&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;Records&amp;gt;&amp;gt; entry : response.responseData().entrySet()) {&lt;br/&gt;
+                                    TopicPartition partition = entry.getKey();&lt;br/&gt;
+                                    long fetchOffset = data.sessionPartitions().get(partition).fetchOffset;&lt;br/&gt;
+                                    FetchResponse.PartitionData fetchData = entry.getValue();&lt;br/&gt;
+&lt;br/&gt;
+                                    log.debug(&quot;Fetch {} at offset {} for partition {} returned fetch data {}&quot;,&lt;br/&gt;
+                                            isolationLevel, fetchOffset, partition, fetchData);&lt;br/&gt;
+                                    completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator,&lt;br/&gt;
+                                            resp.requestHeader().apiVersion()));&lt;br/&gt;
+                                }&lt;br/&gt;
+&lt;br/&gt;
+                                sensors.fetchLatency.record(resp.requestLatencyMs());&lt;br/&gt;
                             }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (!handler.handleResponse(response)) 
{
-                                return;
-                            }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; partitions = new HashSet&amp;lt;&amp;gt;(response.responseData().keySet());&lt;/li&gt;
	&lt;li&gt;FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;Records&amp;gt;&amp;gt; entry : response.responseData().entrySet()) {&lt;/li&gt;
	&lt;li&gt;TopicPartition partition = entry.getKey();&lt;/li&gt;
	&lt;li&gt;long fetchOffset = data.sessionPartitions().get(partition).fetchOffset;&lt;/li&gt;
	&lt;li&gt;FetchResponse.PartitionData fetchData = entry.getValue();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;Fetch {} at offset {} for partition {} returned fetch data {}&quot;,&lt;/li&gt;
	&lt;li&gt;isolationLevel, fetchOffset, partition, fetchData);&lt;/li&gt;
	&lt;li&gt;completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator,&lt;/li&gt;
	&lt;li&gt;resp.requestHeader().apiVersion()));&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;sensors.fetchLatency.record(resp.requestLatencyMs());&lt;br/&gt;
                         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                         @Override&lt;br/&gt;
                         public void onFailure(RuntimeException e) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchSessionHandler handler = sessionHandlers.get(fetchTarget.id());&lt;/li&gt;
	&lt;li&gt;if (handler != null) {&lt;/li&gt;
	&lt;li&gt;handler.handleError(e);&lt;br/&gt;
+                            synchronized (Fetcher.this) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                                FetchSessionHandler handler = sessionHandler(fetchTarget.id());+                                if (handler != null) {
+                                    handler.handleError(e);
+                                }                             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;                         }&lt;br/&gt;
                     });&lt;br/&gt;
@@ -864,7 +882,7 @@ public ListOffsetResult() {&lt;br/&gt;
                 // if there is a leader and no in-flight requests, issue a new fetch&lt;br/&gt;
                 FetchSessionHandler.Builder builder = fetchable.get(node);&lt;br/&gt;
                 if (builder == null) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;FetchSessionHandler handler = sessionHandlers.get(node.id());&lt;br/&gt;
+                    FetchSessionHandler handler = sessionHandler(node.id());&lt;br/&gt;
                     if (handler == null) 
{
                         handler = new FetchSessionHandler(logContext, node.id());
                         sessionHandlers.put(node.id(), handler);
@@ -1060,6 +1078,11 @@ public void clearBufferedDataForUnassignedTopics(Collection&amp;lt;String&amp;gt; assignedTopi
         clearBufferedDataForUnassignedPartitions(currentTopicPartitions);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // Visibilty for testing&lt;br/&gt;
+    protected FetchSessionHandler sessionHandler(int node) &lt;/p&gt;
{
+        return sessionHandlers.get(node);
+    }
&lt;p&gt;+&lt;br/&gt;
     public static Sensor throttleTimeSensor(Metrics metrics, FetcherMetricsRegistry metricsRegistry) {&lt;br/&gt;
         Sensor fetchThrottleTimeSensor = metrics.sensor(&quot;fetch-throttle-time&quot;);&lt;br/&gt;
         fetchThrottleTimeSensor.add(metrics.metricInstance(metricsRegistry.fetchThrottleTimeAvg), new Avg());&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 3bf3deba1aa..1ca0fde2698 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.clients.ApiVersions;&lt;br/&gt;
 import org.apache.kafka.clients.ClientRequest;&lt;br/&gt;
 import org.apache.kafka.clients.ClientUtils;&lt;br/&gt;
+import org.apache.kafka.clients.FetchSessionHandler;&lt;br/&gt;
 import org.apache.kafka.clients.Metadata;&lt;br/&gt;
 import org.apache.kafka.clients.MockClient;&lt;br/&gt;
 import org.apache.kafka.clients.NetworkClient;&lt;br/&gt;
@@ -88,6 +89,7 @@&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt; import java.io.DataOutputStream;&lt;br/&gt;
+import java.lang.reflect.Field;&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;br/&gt;
 import java.nio.charset.StandardCharsets;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
@@ -101,6 +103,13 @@&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
+import java.util.concurrent.ExecutorService;&lt;br/&gt;
+import java.util.concurrent.Executors;&lt;br/&gt;
+import java.util.concurrent.Future;&lt;br/&gt;
+import java.util.concurrent.TimeUnit;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicInteger;&lt;br/&gt;
+import java.util.function.Function;&lt;br/&gt;
+import java.util.stream.Collectors;&lt;/p&gt;

&lt;p&gt; import static java.util.Collections.singleton;&lt;br/&gt;
 import static org.apache.kafka.common.requests.FetchMetadata.INVALID_SESSION_ID;&lt;br/&gt;
@@ -112,6 +121,7 @@&lt;br/&gt;
 import static org.junit.Assert.assertTrue;&lt;br/&gt;
 import static org.junit.Assert.fail;&lt;/p&gt;

&lt;p&gt;+&lt;br/&gt;
 @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
 public class FetcherTest {&lt;br/&gt;
     private ConsumerRebalanceListener listener = new NoOpConsumerRebalanceListener();&lt;br/&gt;
@@ -149,38 +159,30 @@&lt;br/&gt;
     private Fetcher&amp;lt;byte[], byte[]&amp;gt; fetcher = createFetcher(subscriptions, metrics);&lt;br/&gt;
     private Metrics fetcherMetrics = new Metrics(time);&lt;br/&gt;
     private Fetcher&amp;lt;byte[], byte[]&amp;gt; fetcherNoAutoReset = createFetcher(subscriptionsNoAutoReset, fetcherMetrics);&lt;br/&gt;
+    private ExecutorService executorService;&lt;/p&gt;

&lt;p&gt;     @Before&lt;br/&gt;
     public void setup() throws Exception &lt;/p&gt;
{
         metadata.update(cluster, Collections.&amp;lt;String&amp;gt;emptySet(), time.milliseconds());
         client.setNode(node);
 
-        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 1L);
-        builder.append(0L, &quot;key&quot;.getBytes(), &quot;value-1&quot;.getBytes());
-        builder.append(0L, &quot;key&quot;.getBytes(), &quot;value-2&quot;.getBytes());
-        builder.append(0L, &quot;key&quot;.getBytes(), &quot;value-3&quot;.getBytes());
-        records = builder.build();
-
-        builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 4L);
-        builder.append(0L, &quot;key&quot;.getBytes(), &quot;value-4&quot;.getBytes());
-        builder.append(0L, &quot;key&quot;.getBytes(), &quot;value-5&quot;.getBytes());
-        nextRecords = builder.build();
-
-        builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);
-        emptyRecords = builder.build();
-
-        builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 4L);
-        builder.append(0L, &quot;key&quot;.getBytes(), &quot;value-0&quot;.getBytes());
-        partialRecords = builder.build();
+        records = buildRecords(1L, 3, 1);
+        nextRecords = buildRecords(4L, 2, 4);
+        emptyRecords = buildRecords(0L, 0, 0);
+        partialRecords = buildRecords(4L, 1, 0);
         partialRecords.buffer().putInt(Records.SIZE_OFFSET, 10000);
     }

&lt;p&gt;     @After&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void teardown() {&lt;br/&gt;
+    public void teardown() throws Exception 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         this.metrics.close();         this.fetcherMetrics.close();         this.fetcher.close();         this.fetcherMetrics.close();+        if (executorService != null) {
+            executorService.shutdownNow();
+            assertTrue(executorService.awaitTermination(5, TimeUnit.SECONDS));
+        }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
@@ -2509,6 +2511,141 @@ public void testConsumingViaIncrementalFetchRequests() &lt;/p&gt;
{
         assertEquals(5, records.get(1).offset());
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testFetcherConcurrency() throws Exception {&lt;br/&gt;
+        int numPartitions = 20;&lt;br/&gt;
+        Set&amp;lt;TopicPartition&amp;gt; topicPartitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (int i = 0; i &amp;lt; numPartitions; i++)&lt;br/&gt;
+            topicPartitions.add(new TopicPartition(topicName, i));&lt;br/&gt;
+        cluster = TestUtils.singletonCluster(topicName, numPartitions);&lt;br/&gt;
+        metadata.update(cluster, Collections.emptySet(), time.milliseconds());&lt;br/&gt;
+        client.setNode(node);&lt;br/&gt;
+        fetchSize = 10000;&lt;br/&gt;
+&lt;br/&gt;
+        Fetcher&amp;lt;byte[], byte[]&amp;gt; fetcher = new Fetcher&amp;lt;byte[], byte[]&amp;gt;(&lt;br/&gt;
+                new LogContext(),&lt;br/&gt;
+                consumerClient,&lt;br/&gt;
+                minBytes,&lt;br/&gt;
+                maxBytes,&lt;br/&gt;
+                maxWaitMs,&lt;br/&gt;
+                fetchSize,&lt;br/&gt;
+                2 * numPartitions,&lt;br/&gt;
+                true,&lt;br/&gt;
+                new ByteArrayDeserializer(),&lt;br/&gt;
+                new ByteArrayDeserializer(),&lt;br/&gt;
+                metadata,&lt;br/&gt;
+                subscriptions,&lt;br/&gt;
+                metrics,&lt;br/&gt;
+                metricsRegistry,&lt;br/&gt;
+                time,&lt;br/&gt;
+                retryBackoffMs,&lt;br/&gt;
+                requestTimeoutMs,&lt;br/&gt;
+                IsolationLevel.READ_UNCOMMITTED) {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            protected FetchSessionHandler sessionHandler(int id) {&lt;br/&gt;
+                final FetchSessionHandler handler = super.sessionHandler(id);&lt;br/&gt;
+                if (handler == null)&lt;br/&gt;
+                    return null;&lt;br/&gt;
+                else {&lt;br/&gt;
+                    return new FetchSessionHandler(new LogContext(), id) {&lt;br/&gt;
+                        @Override&lt;br/&gt;
+                        public Builder newBuilder() &lt;/p&gt;
{
+                            verifySessionPartitions();
+                            return handler.newBuilder();
+                        }
&lt;p&gt;+&lt;br/&gt;
+                        @Override&lt;br/&gt;
+                        public boolean handleResponse(FetchResponse response) &lt;/p&gt;
{
+                            verifySessionPartitions();
+                            return handler.handleResponse(response);
+                        }
&lt;p&gt;+&lt;br/&gt;
+                        @Override&lt;br/&gt;
+                        public void handleError(Throwable t) &lt;/p&gt;
{
+                            verifySessionPartitions();
+                            handler.handleError(t);
+                        }
&lt;p&gt;+&lt;br/&gt;
+                        // Verify that session partitions can be traversed safely.&lt;br/&gt;
+                        private void verifySessionPartitions() {&lt;br/&gt;
+                            try {&lt;br/&gt;
+                                Field field = FetchSessionHandler.class.getDeclaredField(&quot;sessionPartitions&quot;);&lt;br/&gt;
+                                field.setAccessible(true);&lt;br/&gt;
+                                LinkedHashMap&amp;lt;TopicPartition, FetchRequest.PartitionData&amp;gt; sessionPartitions =&lt;br/&gt;
+                                        (LinkedHashMap&amp;lt;TopicPartition, FetchRequest.PartitionData&amp;gt;) field.get(handler);&lt;br/&gt;
+                                for (Map.Entry&amp;lt;TopicPartition, FetchRequest.PartitionData&amp;gt; entry : sessionPartitions.entrySet()) &lt;/p&gt;
{
+                                    // If `sessionPartitions` are modified on another thread, Thread.yield will increase the
+                                    // possibility of ConcurrentModificationException if appropriate synchronization is not used.
+                                    Thread.yield();
+                                }
&lt;p&gt;+                            } catch (Exception e) &lt;/p&gt;
{
+                                throw new RuntimeException(e);
+                            }
&lt;p&gt;+                        }&lt;br/&gt;
+                    };&lt;br/&gt;
+                }&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+&lt;br/&gt;
+        subscriptions.assignFromUser(topicPartitions);&lt;br/&gt;
+        topicPartitions.forEach(tp -&amp;gt; subscriptions.seek(tp, 0L));&lt;br/&gt;
+&lt;br/&gt;
+        AtomicInteger fetchesRemaining = new AtomicInteger(1000);&lt;br/&gt;
+        executorService = Executors.newSingleThreadExecutor();&lt;br/&gt;
+        Future&amp;lt;?&amp;gt; future = executorService.submit(() -&amp;gt; {&lt;br/&gt;
+            while (fetchesRemaining.get() &amp;gt; 0) {&lt;br/&gt;
+                synchronized (consumerClient) {&lt;br/&gt;
+                    if (!client.requests().isEmpty()) {&lt;br/&gt;
+                        ClientRequest request = client.requests().peek();&lt;br/&gt;
+                        FetchRequest fetchRequest = (FetchRequest) request.requestBuilder().build();&lt;br/&gt;
+                        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; responseMap = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+                        for (Map.Entry&amp;lt;TopicPartition, FetchRequest.PartitionData&amp;gt; entry : fetchRequest.fetchData().entrySet()) &lt;/p&gt;
{
+                            TopicPartition tp = entry.getKey();
+                            long offset = entry.getValue().fetchOffset;
+                            responseMap.put(tp, new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE, offset + 2L, offset + 2,
+                                    0L, null, buildRecords(offset, 2, offset)));
+                        }
&lt;p&gt;+                        client.respondToRequest(request, new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, responseMap, 0, 123));&lt;br/&gt;
+                        consumerClient.poll(time.timer(0));&lt;br/&gt;
+                    }&lt;br/&gt;
+                }&lt;br/&gt;
+            }&lt;br/&gt;
+            return fetchesRemaining.get();&lt;br/&gt;
+        });&lt;br/&gt;
+        Map&amp;lt;TopicPartition, Long&amp;gt; nextFetchOffsets = topicPartitions.stream()&lt;br/&gt;
+                .collect(Collectors.toMap(Function.identity(), t -&amp;gt; 0L));&lt;br/&gt;
+        while (fetchesRemaining.get() &amp;gt; 0 &amp;amp;&amp;amp; !future.isDone()) {&lt;br/&gt;
+            if (fetcher.sendFetches() == 1) {&lt;br/&gt;
+                synchronized (consumerClient) &lt;/p&gt;
{
+                    consumerClient.poll(time.timer(0));
+                }
&lt;p&gt;+            }&lt;br/&gt;
+            if (fetcher.hasCompletedFetches()) {&lt;br/&gt;
+                Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; fetchedRecords = fetcher.fetchedRecords();&lt;br/&gt;
+                if (!fetchedRecords.isEmpty()) {&lt;br/&gt;
+                    fetchesRemaining.decrementAndGet();&lt;br/&gt;
+                    fetchedRecords.entrySet().forEach(entry -&amp;gt; &lt;/p&gt;
{
+                        TopicPartition tp = entry.getKey();
+                        List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records = entry.getValue();
+                        assertEquals(2, records.size());
+                        long nextOffset = nextFetchOffsets.get(tp);
+                        assertEquals(nextOffset, records.get(0).offset());
+                        assertEquals(nextOffset + 1, records.get(1).offset());
+                        nextFetchOffsets.put(tp, nextOffset + 2);
+                    }
&lt;p&gt;);&lt;br/&gt;
+                }&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+        assertEquals(0, future.get());&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private MemoryRecords buildRecords(long baseOffset, int count, long firstMessageId) &lt;/p&gt;
{
+        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, baseOffset);
+        for (int i = 0; i &amp;lt; count; i++)
+            builder.append(0L, &quot;key&quot;.getBytes(), (&quot;value-&quot; + (firstMessageId + i)).getBytes());
+        return builder.build();
+    }
&lt;p&gt;+&lt;br/&gt;
     private int appendTransactionalRecords(ByteBuffer buffer, long pid, long baseOffset, int baseSequence, SimpleRecord... records) {&lt;br/&gt;
         MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE,&lt;br/&gt;
                 TimestampType.CREATE_TIME, baseOffset, time.milliseconds(), pid, (short) 0, baseSequence, true,&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16675053" author="sachinu" created="Mon, 5 Nov 2018 12:17:14 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rsivaram&quot; class=&quot;user-hover&quot; rel=&quot;rsivaram&quot;&gt;rsivaram&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am getting this error also with Kafka 1.1.0.&#160; Is there any workaround that i can use till the fix is available in a public build?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16676909" author="rsivaram" created="Tue, 6 Nov 2018 15:45:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sachinu&quot; class=&quot;user-hover&quot; rel=&quot;sachinu&quot;&gt;sachinu&lt;/a&gt; The fix is available in 2.0.1 which is expected to be released very soon. The specific exception in this JIRA is in a code path that is used only when TRACE logging is enabled on the consumer. The fix in this JIRA makes the whole class thread-safe, but if you are seeing the exception without TRACE, can you include the stack trace, so that we can verify if the PR has fixed that too? Thanks.&lt;/p&gt;</comment>
                            <comment id="16681132" author="sachinu" created="Fri, 9 Nov 2018 09:28:18 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rsivaram&quot; class=&quot;user-hover&quot; rel=&quot;rsivaram&quot;&gt;rsivaram&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I am on &quot;1.1.0&quot; version and following is the stack trace. Is this fatal error? Or upon subsequent poll() or commitSync(), will heartbeat thread restart?:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2018-11-08 16:53:57,456 [kafka-coordinator-heartbeat-thread | ConsumerNginxRawCW] ERROR org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-1, groupId=ConsumerNginxRawCW] Heartbeat thread failed due to unexpected error
java.util.ConcurrentModificationException
at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:711)
at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:734)
at org.apache.kafka.clients.FetchSessionHandler.responseDataToLogString(FetchSessionHandler.java:362)
at org.apache.kafka.clients.FetchSessionHandler.handleResponse(FetchSessionHandler.java:423)
at org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:212)
at org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:202)
at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:563)
at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:390)
at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:293)
at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:300)
at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:948)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I&#160;have not enabled TRACE level logging. Following is the log4j2 config:&lt;/li&gt;
	&lt;li&gt;&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;lt;?xml version=&lt;span class=&quot;code-quote&quot;&gt;&quot;1.0&quot;&lt;/span&gt; encoding=&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;?&amp;gt;
&amp;lt;Configuration status=&lt;span class=&quot;code-quote&quot;&gt;&quot;error&quot;&lt;/span&gt; monitorInterval=&lt;span class=&quot;code-quote&quot;&gt;&quot;60&quot;&lt;/span&gt;&amp;gt;

   &amp;lt;ThresholdFilter level=&lt;span class=&quot;code-quote&quot;&gt;&quot;trace&quot;&lt;/span&gt; /&amp;gt;

   &amp;lt;Appenders&amp;gt;

      &amp;lt;RollingFile name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Appender&quot;&lt;/span&gt; fileName=&lt;span class=&quot;code-quote&quot;&gt;&quot;/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/log/worker-nginx-raw.log&quot;&lt;/span&gt;
         filePattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/log/worker-nginx-raw-%d{yyyy-MM-dd}-%i.log.gz&quot;&lt;/span&gt;&amp;gt;
         &amp;lt;PatternLayout&amp;gt;
            &amp;lt;pattern&amp;gt;%d [%t] %-5level %logger{36} - %msg%n&amp;lt;/pattern&amp;gt;
         &amp;lt;/PatternLayout&amp;gt;
         &amp;lt;Policies&amp;gt;
            &amp;lt;SizeBasedTriggeringPolicy size=&lt;span class=&quot;code-quote&quot;&gt;&quot;100 MB&quot;&lt;/span&gt; /&amp;gt;
            &amp;lt;TimeBasedTriggeringPolicy interval=&lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt; /&amp;gt;
         &amp;lt;/Policies&amp;gt;
      &amp;lt;/RollingFile&amp;gt;

   &amp;lt;/Appenders&amp;gt;

   &amp;lt;Loggers&amp;gt;

      &amp;lt;Logger name=&lt;span class=&quot;code-quote&quot;&gt;&quot;com.kafka-client&quot;&lt;/span&gt;  additivity=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&quot;&lt;/span&gt;&amp;gt;
         &amp;lt;AppenderRef ref=&lt;span class=&quot;code-quote&quot;&gt;&quot;Appender&quot;&lt;/span&gt; level=&lt;span class=&quot;code-quote&quot;&gt;&quot;info&quot;&lt;/span&gt; /&amp;gt;
      &amp;lt;/Logger&amp;gt;

      &amp;lt;Root level=&lt;span class=&quot;code-quote&quot;&gt;&quot;all&quot;&lt;/span&gt;&amp;gt;
         &amp;lt;AppenderRef ref=&lt;span class=&quot;code-quote&quot;&gt;&quot;Appender&quot;&lt;/span&gt; level=&lt;span class=&quot;code-quote&quot;&gt;&quot;error&quot;&lt;/span&gt; /&amp;gt;
      &amp;lt;/Root&amp;gt;

   &amp;lt;/Loggers&amp;gt;

&amp;lt;/Configuration&amp;gt;

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can any workaround be done to avoid this problem? Like listen for above exception from heartbeat thread and&#160;wakeup the&#160;thread&#160;doing the actual poll/commit then spawn another thread that reinitiates eveything including subscribe(...) etc?&#160;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16681611" author="rsivaram" created="Fri, 9 Nov 2018 15:45:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sachinu&quot; class=&quot;user-hover&quot; rel=&quot;sachinu&quot;&gt;sachinu&lt;/a&gt; Thanks for the stack trace. That should be fixed with the commit from this JIRA since we have made that code thread-safe. Upgrading the client to 2.0.1 would be the easiest approach. The RC has already been approved, so the release will be ready soon. Otherwise to work with 1.1.x, you will need to close the current consumer instance and start a new one as you described.&lt;/p&gt;</comment>
                            <comment id="16681632" author="ijuma" created="Fri, 9 Nov 2018 15:58:07 +0000"  >&lt;p&gt;2.0.1 is already in Maven.&lt;/p&gt;</comment>
                            <comment id="16683407" author="sachinu" created="Mon, 12 Nov 2018 08:35:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I checked at &lt;a href=&quot;https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients&lt;/a&gt;&#160;The latest is 2.0.0. Are you referring to another maven repo?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rsivaram&quot; class=&quot;user-hover&quot; rel=&quot;rsivaram&quot;&gt;rsivaram&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks for the valuable inputs.&lt;/p&gt;

&lt;p&gt;1) Will client and broker version mismatch be an issue? Can it introduce any performance issue too?&lt;/p&gt;

&lt;p&gt;2) In order to go with the discussed solution to work with 1.1.x,&#160;the issues i see are that we would be restarting another instance of consumer in the same process after heartbeat thread errs out. It may not be recommended as the heartbeat thread might have corrupted some global resources. Thoughts?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;-Sachin&lt;/p&gt;</comment>
                            <comment id="16683660" author="rsivaram" created="Mon, 12 Nov 2018 11:56:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sachinu&quot; class=&quot;user-hover&quot; rel=&quot;sachinu&quot;&gt;sachinu&lt;/a&gt; 2.0.1 clients are compatible with 1.1.x brokers, so there will be no compatibility issue. Also, since message format hasn&apos;t changed between these versions, there shouldn&apos;t be a performance issue either. &lt;/p&gt;

&lt;p&gt;To go with recreating consumers when the issue occurs, it should be safe if the old consumer is closed before the new one is created. This does result in consumer rebalance, so there is a performance impact whenever this is done. &lt;/p&gt;</comment>
                            <comment id="16689269" author="sachinu" created="Fri, 16 Nov 2018 10:30:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rsivaram&quot; class=&quot;user-hover&quot; rel=&quot;rsivaram&quot;&gt;rsivaram&lt;/a&gt; Thanks for the detailed answers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; I rechecked and I can see &quot;2.0.1&quot; kafka-client in maven repo now.&lt;/p&gt;

&lt;p&gt;Is &quot;1.1.2&quot; also scheduled to be released anytime soon? I was thinking of upgrading clients to version having only minor-version bumped up instead of major-version.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;-Sachin&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13194918">KAFKA-7565</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3wz3b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>