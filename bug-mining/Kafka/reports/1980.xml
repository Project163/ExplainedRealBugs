<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:12:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7103] Use bulkloading for RocksDBSegmentedBytesStore during init</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7103</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We should use bulk loading for recovering RocksDBWindowStore, same as RocksDBStore.&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13168456">KAFKA-7103</key>
            <summary>Use bulkloading for RocksDBSegmentedBytesStore during init</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="liquanpei">Liquan Pei</assignee>
                                    <reporter username="liquanpei">Liquan Pei</reporter>
                        <labels>
                    </labels>
                <created>Tue, 26 Jun 2018 20:25:59 +0000</created>
                <updated>Tue, 17 Jul 2018 21:05:22 +0000</updated>
                            <resolved>Tue, 17 Jul 2018 21:05:22 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>2.1.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                    <workratio workratioPercent="0"/>
                                    <progress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                    <timeoriginalestimate seconds="604800">168h</timeoriginalestimate>
                            <timeestimate seconds="604800">168h</timeestimate>
                                        <comments>
                            <comment id="16547108" author="githubbot" created="Tue, 17 Jul 2018 21:04:54 +0000"  >&lt;p&gt;guozhangwang closed pull request #5276: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7103&quot; title=&quot;Use bulkloading for RocksDBSegmentedBytesStore during init&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7103&quot;&gt;&lt;del&gt;KAFKA-7103&lt;/del&gt;&lt;/a&gt;: Use bulkloading for RocksDBSegmentedBytesStore during init&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5276&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5276&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java&lt;br/&gt;
index c5d15d6e5dc..5f1ec3781d8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java&lt;br/&gt;
@@ -16,26 +16,36 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.state.internals;&lt;/p&gt;

&lt;p&gt;+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
+import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
+import org.apache.kafka.streams.errors.ProcessorStateException;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractNotifyingBatchingRestoreCallback;&lt;br/&gt;
 import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
-import org.apache.kafka.streams.processor.StateRestoreCallback;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StateStore;&lt;br/&gt;
 import org.apache.kafka.streams.processor.internals.InternalProcessorContext;&lt;br/&gt;
 import org.apache.kafka.streams.processor.internals.ProcessorStateManager;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueIterator;&lt;br/&gt;
+import org.rocksdb.RocksDBException;&lt;br/&gt;
+import org.rocksdb.WriteBatch;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;/p&gt;

&lt;p&gt;+import java.util.Collection;&lt;br/&gt;
+import java.util.HashMap;&lt;br/&gt;
+import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
+import java.util.Set;&lt;/p&gt;

&lt;p&gt; class RocksDBSegmentedBytesStore implements SegmentedBytesStore {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final static Logger LOG = LoggerFactory.getLogger(RocksDBSegmentedBytesStore.class);&lt;br/&gt;
-&lt;br/&gt;
+    private static final Logger LOG = LoggerFactory.getLogger(RocksDBSegmentedBytesStore.class);&lt;br/&gt;
     private final String name;&lt;br/&gt;
     private final Segments segments;&lt;br/&gt;
     private final KeySchema keySchema;&lt;br/&gt;
     private InternalProcessorContext context;&lt;br/&gt;
     private volatile boolean open;&lt;br/&gt;
+    private Set&amp;lt;Segment&amp;gt; bulkLoadSegments;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     RocksDBSegmentedBytesStore(final String name,&lt;br/&gt;
                                final long retention,&lt;br/&gt;
@@ -131,15 +141,11 @@ public void init(ProcessorContext context, StateStore root) {&lt;/p&gt;

&lt;p&gt;         segments.openExisting(this.context);&lt;/p&gt;

&lt;p&gt;+        bulkLoadSegments = new HashSet&amp;lt;&amp;gt;(segments.allSegments());&lt;br/&gt;
+&lt;br/&gt;
         // register and possibly restore the state from the logs&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.register(root, new StateRestoreCallback() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void restore(byte[] key, byte[] value) 
{
-                put(Bytes.wrap(key), value);
-            }&lt;/li&gt;
	&lt;li&gt;});&lt;br/&gt;
+        context.register(root, new RocksDBSegmentsBatchingRestoreCallback());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;flush();&lt;br/&gt;
         open = true;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -164,4 +170,84 @@ public boolean isOpen() &lt;/p&gt;
{
         return open;
     }

&lt;p&gt;+    // Visible for testing&lt;br/&gt;
+    List&amp;lt;Segment&amp;gt; getSegments() &lt;/p&gt;
{
+        return segments.allSegments();
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Visible for testing&lt;br/&gt;
+    void restoreAllInternal(final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records) {&lt;br/&gt;
+        try {&lt;br/&gt;
+            final Map&amp;lt;Segment, WriteBatch&amp;gt; writeBatchMap = getWriteBatches(records);&lt;br/&gt;
+            for (final Map.Entry&amp;lt;Segment, WriteBatch&amp;gt; entry : writeBatchMap.entrySet()) &lt;/p&gt;
{
+                final Segment segment = entry.getKey();
+                final WriteBatch batch = entry.getValue();
+                segment.write(batch);
+            }
&lt;p&gt;+        } catch (final RocksDBException e) &lt;/p&gt;
{
+            throw new ProcessorStateException(&quot;Error restoring batch to store &quot; + this.name, e);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    // Visible for testing&lt;br/&gt;
+    Map&amp;lt;Segment, WriteBatch&amp;gt; getWriteBatches(final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records) {&lt;br/&gt;
+        final Map&amp;lt;Segment, WriteBatch&amp;gt; writeBatchMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final KeyValue&amp;lt;byte[], byte[]&amp;gt; record : records) {&lt;br/&gt;
+            final long segmentId = segments.segmentId(keySchema.segmentTimestamp(Bytes.wrap(record.key)));&lt;br/&gt;
+            final Segment segment = segments.getOrCreateSegmentIfLive(segmentId, context);&lt;br/&gt;
+            if (segment != null) {&lt;br/&gt;
+                // This handles the case that state store is moved to a new client and does not&lt;br/&gt;
+                // have the local RocksDB instance for the segment. In this case, toggleDBForBulkLoading&lt;br/&gt;
+                // will only close the database and open it again with bulk loading enabled.&lt;br/&gt;
+                if (!bulkLoadSegments.contains(segment)) &lt;/p&gt;
{
+                    segment.toggleDbForBulkLoading(true);
+                    // If the store does not exist yet, the getOrCreateSegmentIfLive will call openDB that
+                    // makes the open flag for the newly created store.
+                    // if the store does exist already, then toggleDbForBulkLoading will make sure that
+                    // the store is already open here.
+                    bulkLoadSegments = new HashSet&amp;lt;&amp;gt;(segments.allSegments());
+                }
&lt;p&gt;+                try {&lt;br/&gt;
+                    final WriteBatch batch = writeBatchMap.computeIfAbsent(segment, s -&amp;gt; new WriteBatch());&lt;br/&gt;
+                    if (record.value == null) &lt;/p&gt;
{
+                        batch.remove(record.key);
+                    }
&lt;p&gt; else &lt;/p&gt;
{
+                        batch.put(record.key, record.value);
+                    }
&lt;p&gt;+                } catch (final RocksDBException e) &lt;/p&gt;
{
+                    throw new ProcessorStateException(&quot;Error restoring batch to store &quot; + this.name, e);
+                }
&lt;p&gt;+            }&lt;br/&gt;
+        }&lt;br/&gt;
+        return writeBatchMap;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void toggleForBulkLoading(final boolean prepareForBulkload) {&lt;br/&gt;
+        for (final Segment segment: segments.allSegments()) &lt;/p&gt;
{
+            segment.toggleDbForBulkLoading(prepareForBulkload);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    private class RocksDBSegmentsBatchingRestoreCallback extends AbstractNotifyingBatchingRestoreCallback {&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void restoreAll(final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records) &lt;/p&gt;
{
+            restoreAllInternal(records);
+        }
&lt;p&gt;+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onRestoreStart(final TopicPartition topicPartition,&lt;br/&gt;
+                                   final String storeName,&lt;br/&gt;
+                                   final long startingOffset,&lt;br/&gt;
+                                   final long endingOffset) &lt;/p&gt;
{
+            toggleForBulkLoading(true);
+        }
&lt;p&gt;+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onRestoreEnd(final TopicPartition topicPartition,&lt;br/&gt;
+                                 final String storeName,&lt;br/&gt;
+                                 final long totalRestored) &lt;/p&gt;
{
+            toggleForBulkLoading(false);
+        }
&lt;p&gt;+    }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
index 17d03cc1da3..cb007474af3 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java&lt;br/&gt;
@@ -90,7 +90,7 @@&lt;br/&gt;
     private FlushOptions fOptions;&lt;/p&gt;

&lt;p&gt;     private volatile boolean prepareForBulkload = false;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private ProcessorContext internalProcessorContext;&lt;br/&gt;
+    ProcessorContext internalProcessorContext;&lt;br/&gt;
     // visible for testing&lt;br/&gt;
     volatile BatchingStateRestoreCallback batchingStateRestoreCallback = null;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -230,7 +230,7 @@ private void validateStoreOpen() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void toggleDbForBulkLoading(final boolean prepareForBulkload) {&lt;br/&gt;
+    void toggleDbForBulkLoading(final boolean prepareForBulkload) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (prepareForBulkload) &lt;/p&gt;
{
             // if the store is not empty, we need to compact to get around the num.levels check
@@ -276,7 +276,7 @@ public synchronized void put(final Bytes key,
         return originalValue;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void restoreAllInternal(final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records) {&lt;br/&gt;
+    void restoreAllInternal(final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records) {&lt;br/&gt;
         try (final WriteBatch batch = new WriteBatch()) {&lt;br/&gt;
             for (final KeyValue&amp;lt;byte[], byte[]&amp;gt; record : records) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                 if (record.value == null) {
@@ -285,7 +285,7 @@ private void restoreAllInternal(final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; recor
                     batch.put(record.key, record.value);
                 }             }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;db.write(wOptions, batch);&lt;br/&gt;
+            write(batch);&lt;br/&gt;
         } catch (final RocksDBException e) 
{
             throw new ProcessorStateException(&quot;Error restoring batch to store &quot; + this.name, e);
         }
&lt;p&gt;@@ -310,6 +310,10 @@ private void putInternal(final byte[] rawKey,&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    void write(final WriteBatch batch) throws RocksDBException &lt;/p&gt;
{
+        db.write(wOptions, batch);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public void putAll(final List&amp;lt;KeyValue&amp;lt;Bytes, byte[]&amp;gt;&amp;gt; entries) {&lt;br/&gt;
         try (final WriteBatch batch = new WriteBatch()) {&lt;br/&gt;
@@ -321,7 +325,7 @@ public void putAll(final List&amp;lt;KeyValue&amp;lt;Bytes, byte[]&amp;gt;&amp;gt; entries) &lt;/p&gt;
{
                     batch.put(entry.key.get(), entry.value);
                 }
&lt;p&gt;             }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;db.write(wOptions, batch);&lt;br/&gt;
+            write(batch);&lt;br/&gt;
         } catch (final RocksDBException e) 
{
             throw new ProcessorStateException(&quot;Error while batch writing to store &quot; + this.name, e);
         }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/Segment.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/Segment.java&lt;br/&gt;
index dbe470e7c15..d107812295e 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/state/internals/Segment.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/Segment.java&lt;br/&gt;
@@ -39,10 +39,12 @@ public int compareTo(Segment segment) 
{
         return Long.compare(id, segment.id);
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public void openDB(final ProcessorContext context) &lt;/p&gt;
{
         super.openDB(context);
         // skip the registering step
+        internalProcessorContext = context;
     }

&lt;p&gt;     @Override&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java&lt;br/&gt;
index 6b9e7a808ae..8e69ccbfee8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java&lt;br/&gt;
@@ -25,6 +25,7 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Window;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Windowed;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.internals.SessionWindow;&lt;br/&gt;
+import org.apache.kafka.streams.processor.StateRestoreListener;&lt;br/&gt;
 import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueIterator;&lt;br/&gt;
 import org.apache.kafka.streams.state.StateSerdes;&lt;br/&gt;
@@ -32,12 +33,14 @@&lt;br/&gt;
 import org.apache.kafka.test.NoOpRecordCollector;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.After;&lt;br/&gt;
+import org.junit.Assert;&lt;br/&gt;
 import org.junit.Before;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 import org.junit.runner.RunWith;&lt;br/&gt;
 import org.junit.runners.Parameterized;&lt;br/&gt;
 import org.junit.runners.Parameterized.Parameters;&lt;br/&gt;
 import org.junit.runners.Parameterized.Parameter;&lt;br/&gt;
+import org.rocksdb.WriteBatch;&lt;/p&gt;

&lt;p&gt; import static org.apache.kafka.streams.state.internals.WindowKeySchema.timeWindowForSize;&lt;/p&gt;

&lt;p&gt;@@ -46,10 +49,12 @@&lt;br/&gt;
 import java.text.SimpleDateFormat;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
+import java.util.Collection;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
 import java.util.Date;&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.SimpleTimeZone;&lt;/p&gt;

&lt;p&gt;@@ -291,6 +296,67 @@ public void shouldBeAbleToWriteToReInitializedStore() &lt;/p&gt;
{
         bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows[1])), serializeValue(100));
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldCreateWriteBatches() {&lt;br/&gt;
+        final String key = &quot;a&quot;;&lt;br/&gt;
+        final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        records.add(new KeyValue&amp;lt;&amp;gt;(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)).get(), serializeValue(50L)));&lt;br/&gt;
+        records.add(new KeyValue&amp;lt;&amp;gt;(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;)).get(), serializeValue(100L)));&lt;br/&gt;
+        final Map&amp;lt;Segment, WriteBatch&amp;gt; writeBatchMap = bytesStore.getWriteBatches(records);&lt;br/&gt;
+        assertEquals(2, writeBatchMap.size());&lt;br/&gt;
+        for (final WriteBatch batch: writeBatchMap.values()) &lt;/p&gt;
{
+            assertEquals(1, batch.count());
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRestoreToByteStore() {&lt;br/&gt;
+        // 0 segments initially.&lt;br/&gt;
+        assertEquals(0, bytesStore.getSegments().size());&lt;br/&gt;
+        final String key = &quot;a&quot;;&lt;br/&gt;
+        final Collection&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        records.add(new KeyValue&amp;lt;&amp;gt;(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)).get(), serializeValue(50L)));&lt;br/&gt;
+        records.add(new KeyValue&amp;lt;&amp;gt;(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;)).get(), serializeValue(100L)));&lt;br/&gt;
+        bytesStore.restoreAllInternal(records);&lt;br/&gt;
+&lt;br/&gt;
+        // 2 segments are created during restoration.&lt;br/&gt;
+        assertEquals(2, bytesStore.getSegments().size());&lt;br/&gt;
+&lt;br/&gt;
+        // Bulk loading is enabled during recovery.&lt;br/&gt;
+        for (final Segment segment: bytesStore.getSegments()) &lt;/p&gt;
{
+            Assert.assertThat(segment.getOptions().level0FileNumCompactionTrigger(), equalTo(1 &amp;lt;&amp;lt; 30));
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final List&amp;lt;KeyValue&amp;lt;Windowed&amp;lt;String&amp;gt;, Long&amp;gt;&amp;gt; expected = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        expected.add(new KeyValue&amp;lt;&amp;gt;(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;), 50L));&lt;br/&gt;
+        expected.add(new KeyValue&amp;lt;&amp;gt;(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;), 100L));&lt;br/&gt;
+&lt;br/&gt;
+        final List&amp;lt;KeyValue&amp;lt;Windowed&amp;lt;String&amp;gt;, Long&amp;gt;&amp;gt; results = toList(bytesStore.all());&lt;br/&gt;
+        assertEquals(expected, results);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRespectBulkLoadOptionsDuringInit() {&lt;br/&gt;
+        bytesStore.init(context, bytesStore);&lt;br/&gt;
+        final String key = &quot;a&quot;;&lt;br/&gt;
+        bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)), serializeValue(50L));&lt;br/&gt;
+        bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;)), serializeValue(100L));&lt;br/&gt;
+        assertEquals(2, bytesStore.getSegments().size());&lt;br/&gt;
+&lt;br/&gt;
+        final StateRestoreListener restoreListener = context.getRestoreListener(bytesStore.name());&lt;br/&gt;
+&lt;br/&gt;
+        restoreListener.onRestoreStart(null, bytesStore.name(), 0L, 0L);&lt;br/&gt;
+&lt;br/&gt;
+        for (final Segment segment: bytesStore.getSegments()) {+            Assert.assertThat(segment.getOptions().level0FileNumCompactionTrigger(), equalTo(1 &amp;lt;&amp;lt; 30));+        }
&lt;p&gt;+&lt;br/&gt;
+        restoreListener.onRestoreEnd(null, bytesStore.name(), 0L);&lt;br/&gt;
+        for (final Segment segment: bytesStore.getSegments()) &lt;/p&gt;
{
+            Assert.assertThat(segment.getOptions().level0FileNumCompactionTrigger(), equalTo(4));
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     private Set&amp;lt;String&amp;gt; segmentDirs() {&lt;br/&gt;
         File windowDir = new File(stateDir, storeName);&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 17 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3v9l3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>