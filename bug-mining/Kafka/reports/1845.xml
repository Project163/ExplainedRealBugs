<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 02:24:25 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6054] ERROR &quot;SubscriptionInfo - unable to decode subscription data: version=2&quot; when upgrading from 0.10.0.0 to 0.10.2.1</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6054</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;KIP: &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;We upgraded an app from kafka-streams 0.10.0.0 to 0.10.2.1. We did a rolling upgrade of the app, so that one point, there were both 0.10.0.0-based instances and 0.10.2.1-based instances running.&lt;/p&gt;

&lt;p&gt;We observed the following stack trace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2017-10-11 07:02:19.964 [StreamThread-3] ERROR o.a.k.s.p.i.a.SubscriptionInfo -
unable to decode subscription data: version=2
org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode
subscription data: version=2
        at org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.decode(SubscriptionInfo.java:113)
        at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.assign(StreamPartitionAssignor.java:235)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:260)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:404)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$900(AbstractCoordinator.java:81)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:358)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:340)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:679)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:658)
        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167)
        at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)
        at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:426)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:278)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:360)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:192)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:243)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.ensurePartitionAssignment(ConsumerCoordinator.java:345)
        at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:977)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:937)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:295)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
        
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I spoke with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; and he said this is a known issue that happens when you have both 0.10.0.0 instances and 0.10.2.1 instances running at the same time, because the internal version number of the protocol changed when adding Interactive Queries. Matthias asked me to file this JIRA&amp;gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13108666">KAFKA-6054</key>
            <summary>ERROR &quot;SubscriptionInfo - unable to decode subscription data: version=2&quot; when upgrading from 0.10.0.0 to 0.10.2.1</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mjsax">Matthias J. Sax</assignee>
                                    <reporter username="wushujames">James Cheng</reporter>
                        <labels>
                            <label>kip</label>
                    </labels>
                <created>Wed, 11 Oct 2017 16:40:24 +0000</created>
                <updated>Sat, 28 Sep 2019 06:54:04 +0000</updated>
                            <resolved>Thu, 31 May 2018 06:22:35 +0000</resolved>
                                    <version>0.10.2.1</version>
                                    <fixVersion>0.10.1.2</fixVersion>
                    <fixVersion>0.10.2.2</fixVersion>
                    <fixVersion>0.11.0.3</fixVersion>
                    <fixVersion>1.0.2</fixVersion>
                    <fixVersion>1.1.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16200571" author="wushujames" created="Wed, 11 Oct 2017 16:41:57 +0000"  >&lt;p&gt;Here is my conversation with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; from the Confluent Slack channel:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;James Cheng &lt;span class=&quot;error&quot;&gt;&amp;#91;9:16 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
Does this stack trace mean anything to anyone? It happened when we upgraded a kafka streams app from 0.10.0.0 to 0.10.2.1.&lt;br/&gt;
^ @mjsax, if you have any time to look. Thanks.&lt;/p&gt;


&lt;p&gt;Matthias J Sax &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;9:20 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
That makes sense. We bumped the internal version number when adding IQ feature &amp;#8211; thus, it seems you cannot mix instances for both version.&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;9:21&amp;#93;&lt;/span&gt; &lt;br/&gt;
Seems, we messed up the upgrade path :disappointed:&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;9:21&amp;#93;&lt;/span&gt; &lt;br/&gt;
If you can, you would need to stop all old instances, before starting with the new version.&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;9:21&amp;#93;&lt;/span&gt; &lt;br/&gt;
Can you also open a JIRA for this?&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;9:24&amp;#93;&lt;/span&gt; &lt;br/&gt;
Thus, rolling bounces to upgrade should actually work &amp;#8211; is this what you are doing?&lt;/p&gt;


&lt;p&gt;James Cheng &lt;span class=&quot;error&quot;&gt;&amp;#91;9:27 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
Yes, we&apos;re doing a rolling upgrade. We had (at one point, at least) both instances running.&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;9:27&amp;#93;&lt;/span&gt; &lt;br/&gt;
I imagine that if the 0.10.0.0 versions crashed, then restarted running 0.10.2.1, then they would be fine because they are all the same version at that point, right?&lt;/p&gt;


&lt;p&gt;Matthias J Sax &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;9:27 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
Yes.&lt;/p&gt;


&lt;p&gt;James Cheng &lt;span class=&quot;error&quot;&gt;&amp;#91;9:27 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
Cool, thanks.&lt;/p&gt;


&lt;p&gt;Matthias J Sax &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;9:28 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
Anyway. Please file a JIRA &amp;#8211; upgrading should always work without this error.&lt;/p&gt;


&lt;p&gt;James Cheng &lt;span class=&quot;error&quot;&gt;&amp;#91;9:29 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
I&apos;ll file the JIRA.&lt;/p&gt;



&lt;p&gt;Matthias J Sax &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;9:30 AM&amp;#93;&lt;/span&gt; &lt;br/&gt;
Thx.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="16381566" author="githubbot" created="Thu, 1 Mar 2018 06:08:37 +0000"  >&lt;p&gt;mjsax opened a new pull request #4630: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Code cleanup to prepare the actual fix for an upgrade path&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4630&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4630&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Small change in decoding version 1 metadata: don&apos;t upgrade to version 2 automatically&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16383259" author="mjsax" created="Fri, 2 Mar 2018 06:48:16 +0000"  >&lt;p&gt;We need to change the metadata version again for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3522&quot; title=&quot;Consider adding version information into rocksDB storage format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3522&quot;&gt;&lt;del&gt;KAFKA-3522&lt;/del&gt;&lt;/a&gt; and plan to piggyback a fix for this into this KIP.&lt;/p&gt;</comment>
                            <comment id="16383292" author="githubbot" created="Fri, 2 Mar 2018 07:39:25 +0000"  >&lt;p&gt;mjsax opened a new pull request #4636: &lt;span class=&quot;error&quot;&gt;&amp;#91;WIP&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix Kafka Streams upgrade path for v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4636&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4636&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Fixes the upgrade path from 0.10.0.x to 0.10.1.x+&lt;br/&gt;
   Contained in KIP-258&lt;br/&gt;
   Adds system tests for rolling bounce upgrades.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16386548" author="githubbot" created="Mon, 5 Mar 2018 18:56:45 +0000"  >&lt;p&gt;mjsax closed pull request #4630: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Code cleanup to prepare the actual fix for an upgrade path&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4630&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4630&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index 6b3626101bd..47becfc239b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -226,11 +226,11 @@&lt;br/&gt;
     public static final String DEFAULT_KEY_SERDE_CLASS_CONFIG = &quot;default.key.serde&quot;;&lt;br/&gt;
     private static final String DEFAULT_KEY_SERDE_CLASS_DOC = &quot; Default serializer / deserializer class for key that implements the &amp;lt;code&amp;gt;org.apache.kafka.common.serialization.Serde&amp;lt;/code&amp;gt; interface.&quot;;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/** 
{@code default timestamp.extractor}
&lt;p&gt; */&lt;br/&gt;
+    /** &lt;/p&gt;
{@code default.timestamp.extractor}
&lt;p&gt; */&lt;br/&gt;
     public static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG = &quot;default.timestamp.extractor&quot;;&lt;br/&gt;
     private static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_DOC = &quot;Default timestamp extractor class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.processor.TimestampExtractor&amp;lt;/code&amp;gt; interface.&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/** 
{@code default value.serde}
&lt;p&gt; */&lt;br/&gt;
+    /** &lt;/p&gt;
{@code default.value.serde}
&lt;p&gt; */&lt;br/&gt;
     public static final String DEFAULT_VALUE_SERDE_CLASS_CONFIG = &quot;default.value.serde&quot;;&lt;br/&gt;
     private static final String DEFAULT_VALUE_SERDE_CLASS_DOC = &quot;Default serializer / deserializer class for value that implements the &amp;lt;code&amp;gt;org.apache.kafka.common.serialization.Serde&amp;lt;/code&amp;gt; interface.&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
index 9aa0e94c8c1..71a84b2ca73 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
@@ -66,7 +66,8 @@&lt;br/&gt;
         public final TaskId taskId;&lt;br/&gt;
         public final TopicPartition partition;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignedPartition(final TaskId taskId, final TopicPartition partition) {&lt;br/&gt;
+        AssignedPartition(final TaskId taskId,&lt;br/&gt;
+                          final TopicPartition partition) 
{
             this.taskId = taskId;
             this.partition = partition;
         }
&lt;p&gt;@@ -77,11 +78,11 @@ public int compareTo(final AssignedPartition that) {&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public boolean equals(Object o) {&lt;br/&gt;
+        public boolean equals(final Object o) {&lt;br/&gt;
             if (!(o instanceof AssignedPartition)) 
{
                 return false;
             }&lt;/li&gt;
	&lt;li&gt;AssignedPartition other = (AssignedPartition) o;&lt;br/&gt;
+            final AssignedPartition other = (AssignedPartition) o;&lt;br/&gt;
             return compareTo(other) == 0;&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -104,8 +105,9 @@ public int hashCode() {&lt;br/&gt;
                 final String host = getHost(endPoint);&lt;br/&gt;
                 final Integer port = getPort(endPoint);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (host == null || port == null)&lt;br/&gt;
+                if (host == null || port == null) 
{
                     throw new ConfigException(String.format(&quot;Error parsing host address %s. Expected format host:port.&quot;, endPoint));
+                }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 hostInfo = new HostInfo(host, port);&lt;br/&gt;
             } else {&lt;br/&gt;
@@ -119,10 +121,11 @@ public int hashCode() &lt;/p&gt;
{
             state = new ClientState();
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void addConsumer(final String consumerMemberId, final SubscriptionInfo info) {&lt;br/&gt;
+        void addConsumer(final String consumerMemberId,&lt;br/&gt;
+                         final SubscriptionInfo info) 
{
             consumers.add(consumerMemberId);
-            state.addPreviousActiveTasks(info.prevTasks);
-            state.addPreviousStandbyTasks(info.standbyTasks);
+            state.addPreviousActiveTasks(info.prevTasks());
+            state.addPreviousStandbyTasks(info.standbyTasks());
             state.incrementCapacity();
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -157,8 +160,9 @@ public String toString() {&lt;/p&gt;

&lt;p&gt;     private static final Comparator&amp;lt;TopicPartition&amp;gt; PARTITION_COMPARATOR = new Comparator&amp;lt;TopicPartition&amp;gt;() {&lt;br/&gt;
         @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public int compare(TopicPartition p1, TopicPartition p2) {&lt;/li&gt;
	&lt;li&gt;int result = p1.topic().compareTo(p2.topic());&lt;br/&gt;
+        public int compare(final TopicPartition p1,&lt;br/&gt;
+                           final TopicPartition p2) {&lt;br/&gt;
+            final int result = p1.topic().compareTo(p2.topic());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             if (result != 0) {&lt;br/&gt;
                 return result;&lt;br/&gt;
@@ -194,15 +198,15 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;/p&gt;

&lt;p&gt;         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);&lt;br/&gt;
         if (o == null) &lt;/p&gt;
{
-            KafkaException ex = new KafkaException(&quot;TaskManager is not specified&quot;);
-            log.error(ex.getMessage(), ex);
-            throw ex;
+            final KafkaException fatalException = new KafkaException(&quot;TaskManager is not specified&quot;);
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
         }

&lt;p&gt;         if (!(o instanceof TaskManager)) &lt;/p&gt;
{
-            KafkaException ex = new KafkaException(String.format(&quot;%s is not an instance of %s&quot;, o.getClass().getName(), TaskManager.class.getName()));
-            log.error(ex.getMessage(), ex);
-            throw ex;
+            final KafkaException fatalException = new KafkaException(String.format(&quot;%s is not an instance of %s&quot;, o.getClass().getName(), TaskManager.class.getName()));
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
         }

&lt;p&gt;         taskManager = (TaskManager) o;&lt;br/&gt;
@@ -214,14 +218,14 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         final String userEndPoint = streamsConfig.getString(StreamsConfig.APPLICATION_SERVER_CONFIG);&lt;br/&gt;
         if (userEndPoint != null &amp;amp;&amp;amp; !userEndPoint.isEmpty()) {&lt;br/&gt;
             try &lt;/p&gt;
{
-                String host = getHost(userEndPoint);
-                Integer port = getPort(userEndPoint);
+                final String host = getHost(userEndPoint);
+                final Integer port = getPort(userEndPoint);
 
                 if (host == null || port == null)
                     throw new ConfigException(String.format(&quot;%s Config %s isn&apos;t in the correct format. Expected a host:port pair&quot; +
                                     &quot; but received %s&quot;,
                             logPrefix, StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint));
-            }
&lt;p&gt; catch (NumberFormatException nfe) &lt;/p&gt;
{
+            }
&lt;p&gt; catch (final NumberFormatException nfe) &lt;/p&gt;
{
                 throw new ConfigException(String.format(&quot;%s Invalid port supplied in %s for config %s&quot;,
                         logPrefix, userEndPoint, StreamsConfig.APPLICATION_SERVER_CONFIG));
             }
&lt;p&gt;@@ -240,7 +244,7 @@ public String name() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
+    public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // Adds the following information to subscription&lt;br/&gt;
         // 1. Client UUID (a unique id assigned to an instance of KafkaStreams)&lt;br/&gt;
         // 2. Task ids of previously running tasks&lt;br/&gt;
@@ -249,7 +253,11 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = taskManager.prevActiveTaskIds();&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks = taskManager.cachedTasksIds();&lt;br/&gt;
         standbyTasks.removeAll(previousActiveTasks);&lt;/li&gt;
	&lt;li&gt;final SubscriptionInfo data = new SubscriptionInfo(taskManager.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);&lt;br/&gt;
+        final SubscriptionInfo data = new SubscriptionInfo(&lt;br/&gt;
+            taskManager.processId(),&lt;br/&gt;
+            previousActiveTasks,&lt;br/&gt;
+            standbyTasks,&lt;br/&gt;
+            this.userEndPoint);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         taskManager.updateSubscriptionsFromMetadata(topics);&lt;/p&gt;

&lt;p&gt;@@ -277,22 +285,32 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;3. within each client, tasks are assigned to consumer clients in round-robin manner.&lt;br/&gt;
      */&lt;br/&gt;
     @Override&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Map&amp;lt;String, Assignment&amp;gt; assign(Cluster metadata, Map&amp;lt;String, Subscription&amp;gt; subscriptions) {&lt;br/&gt;
+    public Map&amp;lt;String, Assignment&amp;gt; assign(final Cluster metadata,&lt;br/&gt;
+                                          final Map&amp;lt;String, Subscription&amp;gt; subscriptions) {&lt;br/&gt;
         // construct the client metadata from the decoded subscription info&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;/li&gt;
	&lt;li&gt;String consumerId = entry.getKey();&lt;/li&gt;
	&lt;li&gt;Subscription subscription = entry.getValue();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+        final Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+        int minUserMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;br/&gt;
+            final String consumerId = entry.getKey();&lt;br/&gt;
+            final Subscription subscription = entry.getValue();&lt;br/&gt;
+&lt;br/&gt;
+            final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+            final int usedVersion = info.version();&lt;br/&gt;
+            if (usedVersion &amp;gt; SubscriptionInfo.LATEST_SUPPORTED_VERSION) 
{
+                throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
+                    + &quot;; latest supported version: &quot; + SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+            }
&lt;p&gt;+            if (usedVersion &amp;lt; minUserMetadataVersion) &lt;/p&gt;
{
+                minUserMetadataVersion = usedVersion;
+            }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             // create the new client metadata if necessary&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ClientMetadata clientMetadata = clientsMetadata.get(info.processId);&lt;br/&gt;
+            ClientMetadata clientMetadata = clientsMetadata.get(info.processId());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             if (clientMetadata == null) &lt;/p&gt;
{
-                clientMetadata = new ClientMetadata(info.userEndPoint);
-                clientsMetadata.put(info.processId, clientMetadata);
+                clientMetadata = new ClientMetadata(info.userEndPoint());
+                clientsMetadata.put(info.processId(), clientMetadata);
             }

&lt;p&gt;             // add the consumer to the client&lt;br/&gt;
@@ -309,8 +327,8 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         final Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups = taskManager.builder().topicGroups();&lt;/p&gt;

&lt;p&gt;         final Map&amp;lt;String, InternalTopicMetadata&amp;gt; repartitionTopicMetadata = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {&lt;/li&gt;
	&lt;li&gt;for (InternalTopicConfig topic: topicsInfo.repartitionSourceTopics.values()) {&lt;br/&gt;
+        for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (final InternalTopicConfig topic}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -319,13 +337,13 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         do {&lt;br/&gt;
             numPartitionsNeeded = false;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {&lt;/li&gt;
	&lt;li&gt;for (String topicName : topicsInfo.repartitionSourceTopics.keySet()) {&lt;br/&gt;
+            for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {&lt;br/&gt;
+                for (final String topicName : topicsInfo.repartitionSourceTopics.keySet()) {&lt;br/&gt;
                     int numPartitions = repartitionTopicMetadata.get(topicName).numPartitions;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     // try set the number of partitions for this repartition topic if it is not set yet&lt;br/&gt;
                     if (numPartitions == UNKNOWN) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (InternalTopologyBuilder.TopicsInfo otherTopicsInfo : topicGroups.values()) {&lt;br/&gt;
+                        for (final InternalTopologyBuilder.TopicsInfo otherTopicsInfo : topicGroups.values()) {&lt;br/&gt;
                             final Set&amp;lt;String&amp;gt; otherSinkTopics = otherTopicsInfo.sinkTopics;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                             if (otherSinkTopics.contains(topicName)) {&lt;br/&gt;
@@ -375,7 +393,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // augment the metadata with the newly computed number of partitions for all the&lt;br/&gt;
         // repartition source topics&lt;br/&gt;
         final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; allRepartitionTopicPartitions = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, InternalTopicMetadata&amp;gt; entry : repartitionTopicMetadata.entrySet()) {&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, InternalTopicMetadata&amp;gt; entry : repartitionTopicMetadata.entrySet()) {&lt;br/&gt;
             final String topic = entry.getKey();&lt;br/&gt;
             final int numPartitions = entry.getValue().numPartitions;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -395,7 +413,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // get the tasks as partition groups from the partition grouper&lt;br/&gt;
         final Set&amp;lt;String&amp;gt; allSourceTopics = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         final Map&amp;lt;Integer, Set&amp;lt;String&amp;gt;&amp;gt; sourceTopicsByGroup = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; entry : topicGroups.entrySet()) {&lt;br/&gt;
+        for (final Map.Entry&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; entry : topicGroups.entrySet()) 
{
             allSourceTopics.addAll(entry.getValue().sourceTopics);
             sourceTopicsByGroup.put(entry.getKey(), entry.getValue().sourceTopics);
         }
&lt;p&gt;@@ -405,9 +423,9 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // check if all partitions are assigned, and there are no duplicates of partitions in multiple tasks&lt;br/&gt;
         final Set&amp;lt;TopicPartition&amp;gt; allAssignedPartitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         final Map&amp;lt;Integer, Set&amp;lt;TaskId&amp;gt;&amp;gt; tasksByTopicGroup = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : partitionsForTask.entrySet()) {&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : partitionsForTask.entrySet()) {&lt;br/&gt;
             final Set&amp;lt;TopicPartition&amp;gt; partitions = entry.getValue();&lt;/li&gt;
	&lt;li&gt;for (TopicPartition partition : partitions) {&lt;br/&gt;
+            for (final TopicPartition partition : partitions) {&lt;br/&gt;
                 if (allAssignedPartitions.contains(partition)) {&lt;br/&gt;
                     log.warn(&quot;Partition {} is assigned to more than one tasks: {}&quot;, partition, partitionsForTask);&lt;br/&gt;
                 }&lt;br/&gt;
@@ -422,10 +440,10 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
             }&lt;br/&gt;
             ids.add(id);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;for (String topic : allSourceTopics) {&lt;br/&gt;
+        for (final String topic : allSourceTopics) {&lt;br/&gt;
             final List&amp;lt;PartitionInfo&amp;gt; partitionInfoList = fullMetadata.partitionsForTopic(topic);&lt;br/&gt;
             if (!partitionInfoList.isEmpty()) {&lt;/li&gt;
	&lt;li&gt;for (PartitionInfo partitionInfo : partitionInfoList) {&lt;br/&gt;
+                for (final PartitionInfo partitionInfo : partitionInfoList) {&lt;br/&gt;
                     final TopicPartition partition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());&lt;br/&gt;
                     if (!allAssignedPartitions.contains(partition)) {&lt;br/&gt;
                         log.warn(&quot;Partition {} is not assigned to any tasks: {}&quot;, partition, partitionsForTask);&lt;br/&gt;
@@ -438,15 +456,15 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add tasks to state change log topic subscribers&lt;br/&gt;
         final Map&amp;lt;String, InternalTopicMetadata&amp;gt; changelogTopicMetadata = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; entry : topicGroups.entrySet()) {&lt;br/&gt;
+        for (final Map.Entry&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; entry : topicGroups.entrySet()) {&lt;br/&gt;
             final int topicGroupId = entry.getKey();&lt;br/&gt;
             final Map&amp;lt;String, InternalTopicConfig&amp;gt; stateChangelogTopics = entry.getValue().stateChangelogTopics;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (InternalTopicConfig topicConfig : stateChangelogTopics.values()) {&lt;br/&gt;
+            for (final InternalTopicConfig topicConfig : stateChangelogTopics.values()) {&lt;br/&gt;
                 // the expected number of partitions is the max value of TaskId.partition + 1&lt;br/&gt;
                 int numPartitions = UNKNOWN;&lt;br/&gt;
                 if (tasksByTopicGroup.get(topicGroupId) != null) {&lt;/li&gt;
	&lt;li&gt;for (TaskId task : tasksByTopicGroup.get(topicGroupId)) {&lt;br/&gt;
+                    for (final TaskId task : tasksByTopicGroup.get(topicGroupId)) 
{
                         if (numPartitions &amp;lt; task.partition + 1)
                             numPartitions = task.partition + 1;
                     }
&lt;p&gt;@@ -468,7 +486,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // assign tasks to clients&lt;br/&gt;
         final Map&amp;lt;UUID, ClientState&amp;gt; states = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
+        for (final Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) 
{
             states.put(entry.getKey(), entry.getValue().state);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -484,25 +502,27 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;

&lt;p&gt;         // construct the global partition assignment per host map&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;/li&gt;
	&lt;li&gt;final HostInfo hostInfo = entry.getValue().hostInfo;&lt;br/&gt;
+        if (minUserMetadataVersion == 2) {&lt;br/&gt;
+            for (final Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
+                final HostInfo hostInfo = entry.getValue().hostInfo;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (hostInfo != null) {&lt;/li&gt;
	&lt;li&gt;final Set&amp;lt;TopicPartition&amp;gt; topicPartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;final ClientState state = entry.getValue().state;&lt;br/&gt;
+                if (hostInfo != null) {&lt;br/&gt;
+                    final Set&amp;lt;TopicPartition&amp;gt; topicPartitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+                    final ClientState state = entry.getValue().state;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (final TaskId id : state.activeTasks()) 
{
-                    topicPartitions.addAll(partitionsForTask.get(id));
-                }
&lt;p&gt;+                    for (final TaskId id : state.activeTasks()) &lt;/p&gt;
{
+                        topicPartitions.addAll(partitionsForTask.get(id));
+                    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;partitionsByHostState.put(hostInfo, topicPartitions);&lt;br/&gt;
+                    partitionsByHostState.put(hostInfo, topicPartitions);&lt;br/&gt;
+                }&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
         taskManager.setPartitionsByHostState(partitionsByHostState);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // within the client, distribute tasks to its owned consumers&lt;br/&gt;
         final Map&amp;lt;String, Assignment&amp;gt; assignment = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
+        for (final Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
             final Set&amp;lt;String&amp;gt; consumers = entry.getValue().consumers;&lt;br/&gt;
             final ClientState state = entry.getValue().state;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -511,7 +531,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;

&lt;p&gt;             int consumerTaskIndex = 0;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (String consumer : consumers) {&lt;br/&gt;
+            for (final String consumer : consumers) {&lt;br/&gt;
                 final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standby = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
                 final ArrayList&amp;lt;AssignedPartition&amp;gt; assignedPartitions = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -540,13 +560,15 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                 Collections.sort(assignedPartitions);&lt;br/&gt;
                 final List&amp;lt;TaskId&amp;gt; active = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
                 final List&amp;lt;TopicPartition&amp;gt; activePartitions = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (AssignedPartition partition : assignedPartitions) {&lt;br/&gt;
+                for (final AssignedPartition partition : assignedPartitions) 
{
                     active.add(partition.taskId);
                     activePartitions.add(partition.partition);
                 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 // finally, encode the assignment before sending back to coordinator&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));&lt;br/&gt;
+                assignment.put(consumer, new Assignment(&lt;br/&gt;
+                    activePartitions,&lt;br/&gt;
+                    new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -577,26 +599,54 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskAssignmentException if there is no task id for one of the partitions specified&lt;br/&gt;
      */&lt;br/&gt;
     @Override&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void onAssignment(Assignment assignment) {&lt;/li&gt;
	&lt;li&gt;List&amp;lt;TopicPartition&amp;gt; partitions = new ArrayList&amp;lt;&amp;gt;(assignment.partitions());&lt;br/&gt;
+    public void onAssignment(final Assignment assignment) {&lt;br/&gt;
+        final List&amp;lt;TopicPartition&amp;gt; partitions = new ArrayList&amp;lt;&amp;gt;(assignment.partitions());&lt;br/&gt;
         Collections.sort(partitions, PARTITION_COMPARATOR);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info = AssignmentInfo.decode(assignment.userData());&lt;br/&gt;
+        final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());&lt;br/&gt;
+        final int usedVersion = info.version();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        // version 1 field&lt;br/&gt;
+        final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        // version 2 fields&lt;br/&gt;
+        final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost;&lt;br/&gt;
+&lt;br/&gt;
+        switch (usedVersion) 
{
+            case 1:
+                processVersionOneAssignment(info, partitions, activeTasks);
+                partitionsByHost = Collections.emptyMap();
+                break;
+            case 2:
+                processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+                partitionsByHost = info.partitionsByHost();
+                break;
+            default:
+                throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
+                    + &quot;; latest supported version: &quot; + AssignmentInfo.LATEST_SUPPORTED_VERSION);
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));&lt;br/&gt;
+        taskManager.setPartitionsByHostState(partitionsByHost);&lt;br/&gt;
+        taskManager.setAssignmentMetadata(activeTasks, info.standbyTasks());&lt;br/&gt;
+        taskManager.updateSubscriptionsFromAssignment(partitions);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void processVersionOneAssignment(final AssignmentInfo info,&lt;br/&gt;
+                                             final List&amp;lt;TopicPartition&amp;gt; partitions,&lt;br/&gt;
+                                             final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks) {&lt;br/&gt;
         // the number of assigned partitions should be the same as number of active tasks, which&lt;br/&gt;
         // could be duplicated if one task has more than one assigned partitions&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (partitions.size() != info.activeTasks.size()) {&lt;br/&gt;
+        if (partitions.size() != info.activeTasks().size()) 
{
             throw new TaskAssignmentException(
-                    String.format(&quot;%sNumber of assigned partitions %d is not equal to the number of active taskIds %d&quot; +
-                            &quot;, assignmentInfo=%s&quot;, logPrefix, partitions.size(), info.activeTasks.size(), info.toString())
+                String.format(&quot;%sNumber of assigned partitions %d is not equal to the number of active taskIds %d&quot; +
+                    &quot;, assignmentInfo=%s&quot;, logPrefix, partitions.size(), info.activeTasks().size(), info.toString())
             );
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         for (int i = 0; i &amp;lt; partitions.size(); i++) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TopicPartition partition = partitions.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;/li&gt;
	&lt;li&gt;TaskId id = info.activeTasks.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
+            final TopicPartition partition = partitions.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
+            final TaskId id = info.activeTasks().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             Set&amp;lt;TopicPartition&amp;gt; assignedPartitions = activeTasks.get(id);&lt;br/&gt;
             if (assignedPartitions == null) {&lt;br/&gt;
@@ -605,23 +655,23 @@ public void onAssignment(Assignment assignment) {&lt;br/&gt;
             }&lt;br/&gt;
             assignedPartitions.add(partition);&lt;br/&gt;
         }&lt;br/&gt;
+    }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Set&amp;lt;TopicPartition&amp;gt; value : info.partitionsByHost.values()) {&lt;/li&gt;
	&lt;li&gt;for (TopicPartition topicPartition : value) {&lt;/li&gt;
	&lt;li&gt;topicToPartitionInfo.put(topicPartition, new PartitionInfo(topicPartition.topic(),&lt;/li&gt;
	&lt;li&gt;topicPartition.partition(),&lt;/li&gt;
	&lt;li&gt;null,&lt;/li&gt;
	&lt;li&gt;new Node&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;new Node&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
+    private void processVersionTwoAssignment(final AssignmentInfo info,&lt;br/&gt;
+                                             final List&amp;lt;TopicPartition&amp;gt; partitions,&lt;br/&gt;
+                                             final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks,&lt;br/&gt;
+                                             final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo) {&lt;br/&gt;
+        processVersionOneAssignment(info, partitions, activeTasks);&lt;br/&gt;
+&lt;br/&gt;
+        // process partitions by host&lt;br/&gt;
+        final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost = info.partitionsByHost();&lt;br/&gt;
+        for (final Set&amp;lt;TopicPartition&amp;gt; value : partitionsByHost.values()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (final TopicPartition topicPartition }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));&lt;/li&gt;
	&lt;li&gt;taskManager.setPartitionsByHostState(info.partitionsByHost);&lt;/li&gt;
	&lt;li&gt;taskManager.setAssignmentMetadata(activeTasks, info.standbyTasks);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;taskManager.updateSubscriptionsFromAssignment(partitions);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -658,10 +708,10 @@ private void prepareTopic(final Map&amp;lt;String, InternalTopicMetadata&amp;gt; topicPartitio&lt;br/&gt;
         log.debug(&quot;Completed validating internal topics in partition assignor.&quot;);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void ensureCopartitioning(Collection&amp;lt;Set&amp;lt;String&amp;gt;&amp;gt; copartitionGroups,&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, InternalTopicMetadata&amp;gt; allRepartitionTopicsNumPartitions,&lt;/li&gt;
	&lt;li&gt;Cluster metadata) {&lt;/li&gt;
	&lt;li&gt;for (Set&amp;lt;String&amp;gt; copartitionGroup : copartitionGroups) {&lt;br/&gt;
+    private void ensureCopartitioning(final Collection&amp;lt;Set&amp;lt;String&amp;gt;&amp;gt; copartitionGroups,&lt;br/&gt;
+                                      final Map&amp;lt;String, InternalTopicMetadata&amp;gt; allRepartitionTopicsNumPartitions,&lt;br/&gt;
+                                      final Cluster metadata) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        for (final Set&amp;lt;String&amp;gt; copartitionGroup }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -677,7 +727,7 @@ private void ensureCopartitioning(Collection&amp;lt;Set&amp;lt;String&amp;gt;&amp;gt; copartitionGroups,&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         private final Set&amp;lt;String&amp;gt; updatedTopicSubscriptions = new HashSet&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void updateTopics(Collection&amp;lt;String&amp;gt; topicNames) {&lt;br/&gt;
+        public void updateTopics(final Collection&amp;lt;String&amp;gt; topicNames) 
{
             updatedTopicSubscriptions.clear();
             updatedTopicSubscriptions.addAll(topicNames);
         }
&lt;p&gt;@@ -735,7 +785,7 @@ void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,&lt;br/&gt;
             // if all topics for this co-partition group is repartition topics,&lt;br/&gt;
             // then set the number of partitions to be the maximum of the number of partitions.&lt;br/&gt;
             if (numPartitions == UNKNOWN) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, InternalTopicMetadata&amp;gt; entry: allRepartitionTopicsNumPartitions.entrySet()) {&lt;br/&gt;
+                for (final Map.Entry&amp;lt;String, InternalTopicMetadata&amp;gt; entry: allRepartitionTopicsNumPartitions.entrySet()) {&lt;br/&gt;
                     if (copartitionGroup.contains(entry.getKey())) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                         final int partitions = entry.getValue().numPartitions;                         if (partitions &amp;gt; numPartitions) {
@@ -745,7 +795,7 @@ void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,
                 }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;             // enforce co-partitioning restrictions to repartition topics by updating their number of partitions&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, InternalTopicMetadata&amp;gt; entry : allRepartitionTopicsNumPartitions.entrySet()) {&lt;br/&gt;
+            for (final Map.Entry&amp;lt;String, InternalTopicMetadata&amp;gt; entry : allRepartitionTopicsNumPartitions.entrySet()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                 if (copartitionGroup.contains(entry.getKey())) {
                     entry.getValue().numPartitions = numPartitions;
                 }@@ -755,7 +805,7 @@ void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // following functions are for test only&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void setInternalTopicManager(InternalTopicManager internalTopicManager) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    void setInternalTopicManager(final InternalTopicManager internalTopicManager) {
         this.internalTopicManager = internalTopicManager;
     } }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index 8607472c281..c8df7498755 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -39,76 +39,123 @@&lt;br/&gt;
 public class AssignmentInfo {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(AssignmentInfo.class);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* A new field was added, partitionsByHost. CURRENT_VERSION&lt;/li&gt;
	&lt;li&gt;* is required so we can decode the previous version. For example, this may occur&lt;/li&gt;
	&lt;li&gt;* during a rolling upgrade&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private static final int CURRENT_VERSION = 2;&lt;/li&gt;
	&lt;li&gt;public final int version;&lt;/li&gt;
	&lt;li&gt;public final List&amp;lt;TaskId&amp;gt; activeTasks; // each element corresponds to a partition&lt;/li&gt;
	&lt;li&gt;public final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks;&lt;/li&gt;
	&lt;li&gt;public final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public AssignmentInfo(List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;/li&gt;
	&lt;li&gt;this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
+    public static final int LATEST_SUPPORTED_VERSION = 2;&lt;br/&gt;
+&lt;br/&gt;
+    private final int usedVersion;&lt;br/&gt;
+    private List&amp;lt;TaskId&amp;gt; activeTasks;&lt;br/&gt;
+    private Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks;&lt;br/&gt;
+    private Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost;&lt;br/&gt;
+&lt;br/&gt;
+    private AssignmentInfo(final int version) 
{
+        this.usedVersion = version;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public AssignmentInfo(final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
+                          final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+                          final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) &lt;/p&gt;
{
+        this(LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;/li&gt;
	&lt;li&gt;this.version = version;&lt;br/&gt;
+    public AssignmentInfo(final int version,&lt;br/&gt;
+                          final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
+                          final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+                          final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) 
{
+        this.usedVersion = version;
         this.activeTasks = activeTasks;
         this.standbyTasks = standbyTasks;
         this.partitionsByHost = hostState;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    public int version() &lt;/p&gt;
{
+        return usedVersion;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public List&amp;lt;TaskId&amp;gt; activeTasks() {
+        return activeTasks;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks() {
+        return standbyTasks;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost() {
+        return partitionsByHost;
+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * @throws TaskAssignmentException if method fails to encode the data, e.g., if there is an&lt;br/&gt;
      * IO exception during encoding&lt;br/&gt;
      */&lt;br/&gt;
     public ByteBuffer encode() {&lt;br/&gt;
-        ByteArrayOutputStream baos = new ByteArrayOutputStream();&lt;br/&gt;
-        DataOutputStream out = new DataOutputStream(baos);&lt;br/&gt;
-&lt;br/&gt;
-        try {&lt;br/&gt;
-            // Encode version&lt;br/&gt;
-            out.writeInt(version);&lt;br/&gt;
-            // Encode active tasks&lt;br/&gt;
-            out.writeInt(activeTasks.size());&lt;br/&gt;
-            for (TaskId id : activeTasks) {
-                id.writeTo(out);
-            }&lt;br/&gt;
-            // Encode standby tasks&lt;br/&gt;
-            out.writeInt(standbyTasks.size());&lt;br/&gt;
-            for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : standbyTasks.entrySet()) {
-                TaskId id = entry.getKey();
-                id.writeTo(out);
-
-                Set&amp;lt;TopicPartition&amp;gt; partitions = entry.getValue();
-                writeTopicPartitions(out, partitions);
-            }&lt;br/&gt;
-            out.writeInt(partitionsByHost.size());&lt;br/&gt;
-            for (Map.Entry&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : partitionsByHost&lt;br/&gt;
-                    .entrySet()) {&lt;br/&gt;
-                final HostInfo hostInfo = entry.getKey();&lt;br/&gt;
-                out.writeUTF(hostInfo.host());&lt;br/&gt;
-                out.writeInt(hostInfo.port());&lt;br/&gt;
-                writeTopicPartitions(out, entry.getValue());&lt;br/&gt;
+        final ByteArrayOutputStream baos = new ByteArrayOutputStream();&lt;br/&gt;
+&lt;br/&gt;
+        try (final DataOutputStream out = new DataOutputStream(baos)) {&lt;br/&gt;
+            switch (usedVersion) {
+                case 1:
+                    encodeVersionOne(out);
+                    break;
+                case 2:
+                    encodeVersionTwo(out);
+                    break;
+                default:
+                    throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
+                        + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);
             }&lt;br/&gt;
 &lt;br/&gt;
             out.flush();&lt;br/&gt;
             out.close();&lt;br/&gt;
 &lt;br/&gt;
             return ByteBuffer.wrap(baos.toByteArray());&lt;br/&gt;
-        } catch (IOException ex) {
+        } catch (final IOException ex) {
             throw new TaskAssignmentException(&quot;Failed to encode AssignmentInfo&quot;, ex);
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    private void writeTopicPartitions(DataOutputStream out, Set&amp;lt;TopicPartition&amp;gt; partitions) throws IOException {&lt;br/&gt;
+    private void encodeVersionOne(final DataOutputStream out) throws IOException {
+        out.writeInt(1); // version
+        encodeActiveAndStandbyTaskAssignment(out);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void encodeActiveAndStandbyTaskAssignment(final DataOutputStream out) throws IOException {&lt;br/&gt;
+        // encode active tasks&lt;br/&gt;
+        out.writeInt(activeTasks.size());&lt;br/&gt;
+        for (final TaskId id : activeTasks) {
+            id.writeTo(out);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        // encode standby tasks&lt;br/&gt;
+        out.writeInt(standbyTasks.size());&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : standbyTasks.entrySet()) {
+            final TaskId id = entry.getKey();
+            id.writeTo(out);
+
+            final Set&amp;lt;TopicPartition&amp;gt; partitions = entry.getValue();
+            writeTopicPartitions(out, partitions);
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void encodeVersionTwo(final DataOutputStream out) throws IOException {
+        out.writeInt(2); // version
+        encodeActiveAndStandbyTaskAssignment(out);
+        encodePartitionsByHost(out);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void encodePartitionsByHost(final DataOutputStream out) throws IOException {&lt;br/&gt;
+        // encode partitions by host&lt;br/&gt;
+        out.writeInt(partitionsByHost.size());&lt;br/&gt;
+        for (final Map.Entry&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : partitionsByHost.entrySet()) {
+            final HostInfo hostInfo = entry.getKey();
+            out.writeUTF(hostInfo.host());
+            out.writeInt(hostInfo.port());
+            writeTopicPartitions(out, entry.getValue());
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void writeTopicPartitions(final DataOutputStream out,&lt;br/&gt;
+                                      final Set&amp;lt;TopicPartition&amp;gt; partitions) throws IOException {&lt;br/&gt;
         out.writeInt(partitions.size());&lt;br/&gt;
-        for (TopicPartition partition : partitions) {&lt;br/&gt;
+        for (final TopicPartition partition : partitions) {
             out.writeUTF(partition.topic());
             out.writeInt(partition.partition());
         }&lt;br/&gt;
@@ -117,52 +164,69 @@ private void writeTopicPartitions(DataOutputStream out, Set&amp;lt;TopicPartition&amp;gt; part&lt;br/&gt;
     /**&lt;br/&gt;
      * @throws TaskAssignmentException if method fails to decode the data or if the data version is unknown&lt;br/&gt;
      */&lt;br/&gt;
-    public static AssignmentInfo decode(ByteBuffer data) {&lt;br/&gt;
+    public static AssignmentInfo decode(final ByteBuffer data) {&lt;br/&gt;
         // ensure we are at the beginning of the ByteBuffer&lt;br/&gt;
         data.rewind();&lt;br/&gt;
 &lt;br/&gt;
-        try (DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {&lt;br/&gt;
-            // Decode version&lt;br/&gt;
-            int version = in.readInt();&lt;br/&gt;
-            if (version &amp;lt; 0 || version &amp;gt; CURRENT_VERSION) {
-                TaskAssignmentException ex = new TaskAssignmentException(&quot;Unknown assignment data version: &quot; + version);
-                log.error(ex.getMessage(), ex);
-                throw ex;
-            }&lt;br/&gt;
+        try (final DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {&lt;br/&gt;
+            // decode used version&lt;br/&gt;
+            final int usedVersion = in.readInt();&lt;br/&gt;
+            final AssignmentInfo assignmentInfo = new AssignmentInfo(usedVersion);&lt;br/&gt;
 &lt;br/&gt;
-            // Decode active tasks&lt;br/&gt;
-            int count = in.readInt();&lt;br/&gt;
-            List&amp;lt;TaskId&amp;gt; activeTasks = new ArrayList&amp;lt;&amp;gt;(count);&lt;br/&gt;
-            for (int i = 0; i &amp;lt; count; i++) {
-                activeTasks.add(TaskId.readFrom(in));
-            }&lt;br/&gt;
-            // Decode standby tasks&lt;br/&gt;
-            count = in.readInt();&lt;br/&gt;
-            Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks = new HashMap&amp;lt;&amp;gt;(count);&lt;br/&gt;
-            for (int i = 0; i &amp;lt; count; i++) {&lt;br/&gt;
-                TaskId id = TaskId.readFrom(in);&lt;br/&gt;
-                standbyTasks.put(id, readTopicPartitions(in));&lt;br/&gt;
+            switch (usedVersion) {
+                case 1:
+                    decodeVersionOneData(assignmentInfo, in);
+                    break;
+                case 2:
+                    decodeVersionTwoData(assignmentInfo, in);
+                    break;
+                default:
+                    TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode subscription data: &quot; +
+                        &quot;used version: &quot; + usedVersion + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);
+                    log.error(fatalException.getMessage(), fatalException);
+                    throw fatalException;
             }&lt;br/&gt;
 &lt;br/&gt;
-            Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostStateToTopicPartitions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-            if (version == CURRENT_VERSION) {&lt;br/&gt;
-                int numEntries = in.readInt();&lt;br/&gt;
-                for (int i = 0; i &amp;lt; numEntries; i++) {
-                    HostInfo hostInfo = new HostInfo(in.readUTF(), in.readInt());
-                    hostStateToTopicPartitions.put(hostInfo, readTopicPartitions(in));
-                }&lt;br/&gt;
-            }&lt;br/&gt;
+            return assignmentInfo;&lt;br/&gt;
+        } catch (final IOException ex) {
+            throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static void decodeVersionOneData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
+                                             final DataInputStream in) throws IOException {&lt;br/&gt;
+        // decode active tasks&lt;br/&gt;
+        int count = in.readInt();&lt;br/&gt;
+        assignmentInfo.activeTasks = new ArrayList&amp;lt;&amp;gt;(count);&lt;br/&gt;
+        for (int i = 0; i &amp;lt; count; i++) {
+            assignmentInfo.activeTasks.add(TaskId.readFrom(in));
+        }&lt;br/&gt;
 &lt;br/&gt;
-            return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
+        // decode standby tasks&lt;br/&gt;
+        count = in.readInt();&lt;br/&gt;
+        assignmentInfo.standbyTasks = new HashMap&amp;lt;&amp;gt;(count);&lt;br/&gt;
+        for (int i = 0; i &amp;lt; count; i++) {
+            TaskId id = TaskId.readFrom(in);
+            assignmentInfo.standbyTasks.put(id, readTopicPartitions(in));
+        }&lt;br/&gt;
+    }&lt;br/&gt;
 &lt;br/&gt;
-        } catch (IOException ex) {&lt;br/&gt;
-            throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);&lt;br/&gt;
+    private static void decodeVersionTwoData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
+                                             final DataInputStream in) throws IOException {&lt;br/&gt;
+        decodeVersionOneData(assignmentInfo, in);&lt;br/&gt;
+&lt;br/&gt;
+        // decode partitions by host&lt;br/&gt;
+        assignmentInfo.partitionsByHost = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final int numEntries = in.readInt();&lt;br/&gt;
+        for (int i = 0; i &amp;lt; numEntries; i++) {
+            final HostInfo hostInfo = new HostInfo(in.readUTF(), in.readInt());
+            assignmentInfo.partitionsByHost.put(hostInfo, readTopicPartitions(in));
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    private static Set&amp;lt;TopicPartition&amp;gt; readTopicPartitions(DataInputStream in) throws IOException {&lt;br/&gt;
-        int numPartitions = in.readInt();&lt;br/&gt;
-        Set&amp;lt;TopicPartition&amp;gt; partitions = new HashSet&amp;lt;&amp;gt;(numPartitions);&lt;br/&gt;
+    private static Set&amp;lt;TopicPartition&amp;gt; readTopicPartitions(final DataInputStream in) throws IOException {&lt;br/&gt;
+        final int numPartitions = in.readInt();&lt;br/&gt;
+        final Set&amp;lt;TopicPartition&amp;gt; partitions = new HashSet&amp;lt;&amp;gt;(numPartitions);&lt;br/&gt;
         for (int j = 0; j &amp;lt; numPartitions; j++) {
             partitions.add(new TopicPartition(in.readUTF(), in.readInt()));
         }&lt;br/&gt;
@@ -171,14 +235,14 @@ public static AssignmentInfo decode(ByteBuffer data) {&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public int hashCode() {
-        return version ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
+        return usedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
     }&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
-    public boolean equals(Object o) {&lt;br/&gt;
+    public boolean equals(final Object o) {&lt;br/&gt;
         if (o instanceof AssignmentInfo) {&lt;br/&gt;
-            AssignmentInfo other = (AssignmentInfo) o;&lt;br/&gt;
-            return this.version == other.version &amp;amp;&amp;amp;&lt;br/&gt;
+            final AssignmentInfo other = (AssignmentInfo) o;&lt;br/&gt;
+            return this.usedVersion == other.usedVersion &amp;amp;&amp;amp;&lt;br/&gt;
                     this.activeTasks.equals(other.activeTasks) &amp;amp;&amp;amp;&lt;br/&gt;
                     this.standbyTasks.equals(other.standbyTasks) &amp;amp;&amp;amp;&lt;br/&gt;
                     this.partitionsByHost.equals(other.partitionsByHost);&lt;br/&gt;
@@ -189,7 +253,7 @@ public boolean equals(Object o) {&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() {
-        return &quot;[version=&quot; + version + &quot;, active tasks=&quot; + activeTasks.size() + &quot;, standby tasks=&quot; + standbyTasks.size() + &quot;]&quot;;
+        return &quot;[version=&quot; + usedVersion + &quot;, active tasks=&quot; + activeTasks.size() + &quot;, standby tasks=&quot; + standbyTasks.size() + &quot;]&quot;;
     }&lt;br/&gt;
 &lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index f583dbafc94..7fee90b5402 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -31,42 +31,96 @@&lt;br/&gt;
 &lt;br/&gt;
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;br/&gt;
 &lt;br/&gt;
-    private static final int CURRENT_VERSION = 2;&lt;br/&gt;
+    public static final int LATEST_SUPPORTED_VERSION = 2;&lt;br/&gt;
 &lt;br/&gt;
-    public final int version;&lt;br/&gt;
-    public final UUID processId;&lt;br/&gt;
-    public final Set&amp;lt;TaskId&amp;gt; prevTasks;&lt;br/&gt;
-    public final Set&amp;lt;TaskId&amp;gt; standbyTasks;&lt;br/&gt;
-    public final String userEndPoint;&lt;br/&gt;
+    private final int usedVersion;&lt;br/&gt;
+    private UUID processId;&lt;br/&gt;
+    private Set&amp;lt;TaskId&amp;gt; prevTasks;&lt;br/&gt;
+    private Set&amp;lt;TaskId&amp;gt; standbyTasks;&lt;br/&gt;
+    private String userEndPoint;&lt;br/&gt;
 &lt;br/&gt;
-    public SubscriptionInfo(UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
-        this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
+    private SubscriptionInfo(final int version) {
+        this.usedVersion = version;
     }&lt;br/&gt;
 &lt;br/&gt;
-    private SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
-        this.version = version;&lt;br/&gt;
+    public SubscriptionInfo(final UUID processId,&lt;br/&gt;
+                            final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
+                            final Set&amp;lt;TaskId&amp;gt; standbyTasks,&lt;br/&gt;
+                            final String userEndPoint) {
+        this(LATEST_SUPPORTED_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public SubscriptionInfo(final int version,&lt;br/&gt;
+                            final UUID processId,&lt;br/&gt;
+                            final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
+                            final Set&amp;lt;TaskId&amp;gt; standbyTasks,&lt;br/&gt;
+                            final String userEndPoint) {
+        this.usedVersion = version;
         this.processId = processId;
         this.prevTasks = prevTasks;
         this.standbyTasks = standbyTasks;
         this.userEndPoint = userEndPoint;
     }&lt;br/&gt;
 &lt;br/&gt;
+    public int version() {+        return usedVersion;+    }
&lt;p&gt;+&lt;br/&gt;
+    public UUID processId() &lt;/p&gt;
{
+        return processId;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public Set&amp;lt;TaskId&amp;gt; prevTasks() &lt;/p&gt;
{
+        return prevTasks;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public Set&amp;lt;TaskId&amp;gt; standbyTasks() &lt;/p&gt;
{
+        return standbyTasks;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public String userEndPoint() &lt;/p&gt;
{
+        return userEndPoint;
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskAssignmentException if method fails to encode the data&lt;br/&gt;
      */&lt;br/&gt;
     public ByteBuffer encode() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;byte[] endPointBytes;&lt;/li&gt;
	&lt;li&gt;if (userEndPoint == null) 
{
-            endPointBytes = new byte[0];
-        }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;endPointBytes = userEndPoint.getBytes(Charset.forName(&quot;UTF-8&quot;));&lt;br/&gt;
+        final ByteBuffer buf;&lt;br/&gt;
+&lt;br/&gt;
+        switch (usedVersion) 
{
+            case 1:
+                buf = encodeVersionOne();
+                break;
+            case 2:
+                buf = encodeVersionTwo(prepareUserEndPoint());
+                break;
+            default:
+                throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
+                    + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);
         }&lt;/li&gt;
	&lt;li&gt;ByteBuffer buf = ByteBuffer.allocate(4 /* version &lt;b&gt;/ + 16 /&lt;/b&gt; process id */ + 4 +&lt;/li&gt;
	&lt;li&gt;prevTasks.size() * 8 + 4 + standbyTasks.size() * 8&lt;/li&gt;
	&lt;li&gt;+ 4 /* length of bytes */ + endPointBytes.length&lt;/li&gt;
	&lt;li&gt;);&lt;/li&gt;
	&lt;li&gt;// version&lt;/li&gt;
	&lt;li&gt;buf.putInt(version);&lt;br/&gt;
+&lt;br/&gt;
+        buf.rewind();&lt;br/&gt;
+        return buf;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private ByteBuffer encodeVersionOne() 
{
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionOneByteLength());
+
+        buf.putInt(1); // version
+        encodeVersionOneData(buf);
+
+        return buf;
+    }
&lt;p&gt;+&lt;br/&gt;
+    private int getVersionOneByteLength() &lt;/p&gt;
{
+        return 4 + // version
+               16 + // client ID
+               4 + prevTasks.size() * 8 + // length + prev tasks
+               4 + standbyTasks.size() * 8; // length + standby tasks
+    }
&lt;p&gt;+&lt;br/&gt;
+    private void encodeVersionOneData(final ByteBuffer buf) {&lt;br/&gt;
         // encode client UUID&lt;br/&gt;
         buf.putLong(processId.getMostSignificantBits());&lt;br/&gt;
         buf.putLong(processId.getLeastSignificantBits());&lt;br/&gt;
@@ -80,60 +134,104 @@ public ByteBuffer encode() {&lt;br/&gt;
         for (TaskId id : standbyTasks) &lt;/p&gt;
{
             id.writeTo(buf);
         }&lt;/li&gt;
	&lt;li&gt;buf.putInt(endPointBytes.length);&lt;/li&gt;
	&lt;li&gt;buf.put(endPointBytes);&lt;/li&gt;
	&lt;li&gt;buf.rewind();&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private byte[] prepareUserEndPoint() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        if (userEndPoint == null) {
+            return new byte[0];
+        } else {
+            return userEndPoint.getBytes(Charset.forName(&quot;UTF-8&quot;));
+        }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+    private ByteBuffer encodeVersionTwo(final byte[] endPointBytes) &lt;/p&gt;
{
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionTwoByteLength(endPointBytes));
+
+        buf.putInt(2); // version
+        encodeVersionTwoData(buf, endPointBytes);
+
         return buf;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private int getVersionTwoByteLength(final byte[] endPointBytes) &lt;/p&gt;
{
+        return getVersionOneByteLength() +
+               4 + endPointBytes.length; // length + userEndPoint
+    }
&lt;p&gt;+&lt;br/&gt;
+    private void encodeVersionTwoData(final ByteBuffer buf,&lt;br/&gt;
+                                      final byte[] endPointBytes) {&lt;br/&gt;
+        encodeVersionOneData(buf);&lt;br/&gt;
+        if (endPointBytes != null) &lt;/p&gt;
{
+            buf.putInt(endPointBytes.length);
+            buf.put(endPointBytes);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskAssignmentException if method fails to decode the data&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static SubscriptionInfo decode(ByteBuffer data) {&lt;br/&gt;
+    public static SubscriptionInfo decode(final ByteBuffer data) {&lt;br/&gt;
         // ensure we are at the beginning of the ByteBuffer&lt;br/&gt;
         data.rewind();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Decode version&lt;/li&gt;
	&lt;li&gt;int version = data.getInt();&lt;/li&gt;
	&lt;li&gt;if (version == CURRENT_VERSION || version == 1) {&lt;/li&gt;
	&lt;li&gt;// Decode client UUID&lt;/li&gt;
	&lt;li&gt;UUID processId = new UUID(data.getLong(), data.getLong());&lt;/li&gt;
	&lt;li&gt;// Decode previously active tasks&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; prevTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;int numPrevs = data.getInt();&lt;/li&gt;
	&lt;li&gt;for (int i = 0; i &amp;lt; numPrevs; i++) 
{
-                TaskId id = TaskId.readFrom(data);
-                prevTasks.add(id);
-            }&lt;/li&gt;
	&lt;li&gt;// Decode previously cached tasks&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; standbyTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;int numCached = data.getInt();&lt;/li&gt;
	&lt;li&gt;for (int i = 0; i &amp;lt; numCached; i++) 
{
-                standbyTasks.add(TaskId.readFrom(data));
-            }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;String userEndPoint = null;&lt;/li&gt;
	&lt;li&gt;if (version == CURRENT_VERSION) {&lt;/li&gt;
	&lt;li&gt;int bytesLength = data.getInt();&lt;/li&gt;
	&lt;li&gt;if (bytesLength != 0) 
{
-                    byte[] bytes = new byte[bytesLength];
-                    data.get(bytes);
-                    userEndPoint = new String(bytes, Charset.forName(&quot;UTF-8&quot;));
-                }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;return new SubscriptionInfo(version, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
+        // decode used version&lt;br/&gt;
+        final int usedVersion = data.getInt();&lt;br/&gt;
+        final SubscriptionInfo subscriptionInfo = new SubscriptionInfo(usedVersion);&lt;br/&gt;
+&lt;br/&gt;
+        switch (usedVersion) 
{
+            case 1:
+                decodeVersionOneData(subscriptionInfo, data);
+                break;
+            case 2:
+                decodeVersionTwoData(subscriptionInfo, data);
+                break;
+            default:
+                TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode subscription data: &quot; +
+                    &quot;used version: &quot; + usedVersion + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);
+                log.error(fatalException.getMessage(), fatalException);
+                throw fatalException;
+        }
&lt;p&gt;+&lt;br/&gt;
+        return subscriptionInfo;&lt;br/&gt;
+    }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;TaskAssignmentException ex = new TaskAssignmentException(&quot;unable to decode subscription data: version=&quot; + version);&lt;/li&gt;
	&lt;li&gt;log.error(ex.getMessage(), ex);&lt;/li&gt;
	&lt;li&gt;throw ex;&lt;br/&gt;
+    private static void decodeVersionOneData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
+                                             final ByteBuffer data) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        // decode client UUID+        subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());++        // decode previously active tasks+        final int numPrevs = data.getInt();+        subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();+        for (int i = 0; i &amp;lt; numPrevs; i++) {
+            TaskId id = TaskId.readFrom(data);
+            subscriptionInfo.prevTasks.add(id);
+        }++        // decode previously cached tasks+        final int numCached = data.getInt();+        subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();+        for (int i = 0; i &amp;lt; numCached; i++) {
+            subscriptionInfo.standbyTasks.add(TaskId.readFrom(data));
+        }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+    private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
+                                             final ByteBuffer data) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        decodeVersionOneData(subscriptionInfo, data);++        // decode user end point (can be null)+        int bytesLength = data.getInt();+        if (bytesLength != 0) {
+            final byte[] bytes = new byte[bytesLength];
+            data.get(bytes);
+            subscriptionInfo.userEndPoint = new String(bytes, Charset.forName(&quot;UTF-8&quot;));
         }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
     public int hashCode() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int hashCode = version ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();&lt;br/&gt;
+        final int hashCode = usedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();&lt;br/&gt;
         if (userEndPoint == null) 
{
             return hashCode;
         }
&lt;p&gt;@@ -141,10 +239,10 @@ public int hashCode() {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public boolean equals(Object o) {&lt;br/&gt;
+    public boolean equals(final Object o) {&lt;br/&gt;
         if (o instanceof SubscriptionInfo) {&lt;/li&gt;
	&lt;li&gt;SubscriptionInfo other = (SubscriptionInfo) o;&lt;/li&gt;
	&lt;li&gt;return this.version == other.version &amp;amp;&amp;amp;&lt;br/&gt;
+            final SubscriptionInfo other = (SubscriptionInfo) o;&lt;br/&gt;
+            return this.usedVersion == other.usedVersion &amp;amp;&amp;amp;&lt;br/&gt;
                     this.processId.equals(other.processId) &amp;amp;&amp;amp;&lt;br/&gt;
                     this.prevTasks.equals(other.prevTasks) &amp;amp;&amp;amp;&lt;br/&gt;
                     this.standbyTasks.equals(other.standbyTasks) &amp;amp;&amp;amp;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
index 8b4e8957ed5..bb06c72d080 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
@@ -376,7 +376,6 @@ public boolean conditionMet() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
     @Test&lt;br/&gt;
     public void queryOnRebalance() throws InterruptedException {&lt;br/&gt;
         final int numThreads = STREAM_TWO_PARTITIONS;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
index bf3f1d1ac5e..b0c0d68287b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
@@ -239,17 +239,17 @@ public void testAssignBasic() throws Exception &lt;/p&gt;
{
 
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));
-        allActiveTasks.addAll(info10.activeTasks);
+        allActiveTasks.addAll(info10.activeTasks());
 
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));
-        allActiveTasks.addAll(info11.activeTasks);
+        allActiveTasks.addAll(info11.activeTasks());
 
         assertEquals(Utils.mkSet(task0, task1), allActiveTasks);
 
         // the third consumer
         AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(&quot;consumer20&quot;));
-        allActiveTasks.addAll(info20.activeTasks);
+        allActiveTasks.addAll(info20.activeTasks());
 
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));
@@ -317,13 +317,13 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
         final AssignmentInfo info10 = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());
 
         final List&amp;lt;TaskId&amp;gt; expectedInfo10TaskIds = Arrays.asList(taskIdA1, taskIdA3, taskIdB1, taskIdB3);
-        assertEquals(expectedInfo10TaskIds, info10.activeTasks);
+        assertEquals(expectedInfo10TaskIds, info10.activeTasks());
 
         // the second consumer
         final AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());
         final List&amp;lt;TaskId&amp;gt; expectedInfo11TaskIds = Arrays.asList(taskIdA0, taskIdA2, taskIdB0, taskIdB2);
 
-        assertEquals(expectedInfo11TaskIds, info11.activeTasks);
+        assertEquals(expectedInfo11TaskIds, info11.activeTasks());
     }

&lt;p&gt;     @Test&lt;br/&gt;
@@ -354,7 +354,7 @@ public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
         // check assignment info&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(&quot;topic1&quot;), assignments.get(&quot;consumer10&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        allActiveTasks.addAll(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -394,7 +394,7 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
         // check assignment info&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Collections.&amp;lt;String&amp;gt;emptySet(), assignments.get(&quot;consumer10&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        allActiveTasks.addAll(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, allActiveTasks.size());&lt;br/&gt;
         assertEquals(Collections.&amp;lt;TaskId&amp;gt;emptySet(), new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -407,7 +407,7 @@ public void testAssignEmptyMetadata() throws Exception {&lt;/p&gt;

&lt;p&gt;         // the first consumer&lt;br/&gt;
         info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        allActiveTasks.addAll(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -455,15 +455,15 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
         AssignmentInfo info;&lt;/p&gt;

&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info.activeTasks);&lt;br/&gt;
+        allActiveTasks.addAll(info.activeTasks());&lt;br/&gt;
         allPartitions.addAll(assignments.get(&quot;consumer10&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info.activeTasks);&lt;br/&gt;
+        allActiveTasks.addAll(info.activeTasks());&lt;br/&gt;
         allPartitions.addAll(assignments.get(&quot;consumer11&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer20&quot;).userData());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info.activeTasks);&lt;br/&gt;
+        allActiveTasks.addAll(info.activeTasks());&lt;br/&gt;
         allPartitions.addAll(assignments.get(&quot;consumer20&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(allTasks, allActiveTasks);&lt;br/&gt;
@@ -524,14 +524,14 @@ public void testAssignWithStates() throws Exception {&lt;br/&gt;
         AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());&lt;br/&gt;
         AssignmentInfo info20 = AssignmentInfo.decode(assignments.get(&quot;consumer20&quot;).userData());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(2, info10.activeTasks.size());&lt;/li&gt;
	&lt;li&gt;assertEquals(2, info11.activeTasks.size());&lt;/li&gt;
	&lt;li&gt;assertEquals(2, info20.activeTasks.size());&lt;br/&gt;
+        assertEquals(2, info10.activeTasks().size());&lt;br/&gt;
+        assertEquals(2, info11.activeTasks().size());&lt;br/&gt;
+        assertEquals(2, info20.activeTasks().size());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Set&amp;lt;TaskId&amp;gt; allTasks = new HashSet&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allTasks.addAll(info10.activeTasks);&lt;/li&gt;
	&lt;li&gt;allTasks.addAll(info11.activeTasks);&lt;/li&gt;
	&lt;li&gt;allTasks.addAll(info20.activeTasks);&lt;br/&gt;
+        allTasks.addAll(info10.activeTasks());&lt;br/&gt;
+        allTasks.addAll(info11.activeTasks());&lt;br/&gt;
+        allTasks.addAll(info20.activeTasks());&lt;br/&gt;
         assertEquals(new HashSet&amp;lt;&amp;gt;(tasks), allTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check tasks for state topics&lt;br/&gt;
@@ -603,15 +603,15 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;/p&gt;

&lt;p&gt;         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;/li&gt;
	&lt;li&gt;allStandbyTasks.addAll(info10.standbyTasks.keySet());&lt;br/&gt;
+        allActiveTasks.addAll(info10.activeTasks());&lt;br/&gt;
+        allStandbyTasks.addAll(info10.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info11.activeTasks);&lt;/li&gt;
	&lt;li&gt;allStandbyTasks.addAll(info11.standbyTasks.keySet());&lt;br/&gt;
+        allActiveTasks.addAll(info11.activeTasks());&lt;br/&gt;
+        allStandbyTasks.addAll(info11.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertNotEquals(&quot;same processId has same set of standby tasks&quot;, info11.standbyTasks.keySet(), info10.standbyTasks.keySet());&lt;br/&gt;
+        assertNotEquals(&quot;same processId has same set of standby tasks&quot;, info11.standbyTasks().keySet(), info10.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check active tasks assigned to the first client&lt;br/&gt;
         assertEquals(Utils.mkSet(task0, task1), new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -619,8 +619,8 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;/p&gt;

&lt;p&gt;         // the third consumer&lt;br/&gt;
         AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(&quot;consumer20&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allActiveTasks.addAll(info20.activeTasks);&lt;/li&gt;
	&lt;li&gt;allStandbyTasks.addAll(info20.standbyTasks.keySet());&lt;br/&gt;
+        allActiveTasks.addAll(info20.activeTasks());&lt;br/&gt;
+        allStandbyTasks.addAll(info20.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // all task ids are in the active tasks and also in the standby tasks&lt;/p&gt;

&lt;p&gt;@@ -847,7 +847,7 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception &lt;/p&gt;
{
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) userEndPoint));
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;input&quot;));
         final SubscriptionInfo subscriptionInfo = SubscriptionInfo.decode(subscription.userData());
-        assertEquals(&quot;localhost:8080&quot;, subscriptionInfo.userEndPoint);
+        assertEquals(&quot;localhost:8080&quot;, subscriptionInfo.userEndPoint());
     }

&lt;p&gt;     @Test&lt;br/&gt;
@@ -874,7 +874,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception &lt;/p&gt;
{
         final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);
         final PartitionAssignor.Assignment consumerAssignment = assignments.get(&quot;consumer1&quot;);
         final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumerAssignment.userData());
-        final Set&amp;lt;TopicPartition&amp;gt; topicPartitions = assignmentInfo.partitionsByHost.get(new HostInfo(&quot;localhost&quot;, 8080));
+        final Set&amp;lt;TopicPartition&amp;gt; topicPartitions = assignmentInfo.partitionsByHost().get(new HostInfo(&quot;localhost&quot;, 8080));
         assertEquals(Utils.mkSet(new TopicPartition(&quot;topic1&quot;, 0),
                 new TopicPartition(&quot;topic1&quot;, 1),
                 new TopicPartition(&quot;topic1&quot;, 2)), topicPartitions);
@@ -1072,8 +1072,8 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Except
         final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assign = partitionAssignor.assign(metadata, subscriptions);
         final PartitionAssignor.Assignment consumer1Assignment = assign.get(&quot;consumer1&quot;);
         final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumer1Assignment.userData());
-        final Set&amp;lt;TopicPartition&amp;gt; consumer1partitions = assignmentInfo.partitionsByHost.get(new HostInfo(&quot;localhost&quot;, 8080));
-        final Set&amp;lt;TopicPartition&amp;gt; consumer2Partitions = assignmentInfo.partitionsByHost.get(new HostInfo(&quot;other&quot;, 9090));
+        final Set&amp;lt;TopicPartition&amp;gt; consumer1partitions = assignmentInfo.partitionsByHost().get(new HostInfo(&quot;localhost&quot;, 8080));
+        final Set&amp;lt;TopicPartition&amp;gt; consumer2Partitions = assignmentInfo.partitionsByHost().get(new HostInfo(&quot;other&quot;, 9090));
         final HashSet&amp;lt;TopicPartition&amp;gt; allAssignedPartitions = new HashSet&amp;lt;&amp;gt;(consumer1partitions);
         allAssignedPartitions.addAll(consumer2Partitions);
         assertThat(consumer1partitions, not(allPartitions));
@@ -1095,6 +1095,37 @@ public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProvider
         partitionAssignor.configure(config);
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() throws Exception &lt;/p&gt;
{
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            &quot;consumer1&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            &quot;consumer2&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(),
+            Collections.&amp;lt;TaskId&amp;gt;emptySet(),
+            UUID.randomUUID(),
+            new InternalTopologyBuilder());
+        partitionAssignor.configure(configProps());
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertThat(assignment.size(), equalTo(2));
+        assertThat(AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version(), equalTo(1));
+        assertThat(AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version(), equalTo(1));
+    }
&lt;p&gt;+&lt;br/&gt;
     private PartitionAssignor.Assignment createAssignment(final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState) &lt;/p&gt;
{
         final AssignmentInfo info = new AssignmentInfo(Collections.&amp;lt;TaskId&amp;gt;emptyList(),
                                                        Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(),
@@ -1111,7 +1142,7 @@ private AssignmentInfo checkAssignment(Set&amp;lt;String&amp;gt; expectedTopics, PartitionAssi
         AssignmentInfo info = AssignmentInfo.decode(assignment.userData());
 
         // check if the number of assigned partitions == the size of active task id list
-        assertEquals(assignment.partitions().size(), info.activeTasks.size());
+        assertEquals(assignment.partitions().size(), info.activeTasks().size());
 
         // check if active tasks are consistent
         List&amp;lt;TaskId&amp;gt; activeTasks = new ArrayList&amp;lt;&amp;gt;();
@@ -1121,14 +1152,14 @@ private AssignmentInfo checkAssignment(Set&amp;lt;String&amp;gt; expectedTopics, PartitionAssi
             activeTasks.add(new TaskId(0, partition.partition()));
             activeTopics.add(partition.topic());
         }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(activeTasks, info.activeTasks);&lt;br/&gt;
+        assertEquals(activeTasks, info.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check if active partitions cover all topics&lt;br/&gt;
         assertEquals(expectedTopics, activeTopics);&lt;/p&gt;

&lt;p&gt;         // check if standby tasks are consistent&lt;br/&gt;
         Set&amp;lt;String&amp;gt; standbyTopics = new HashSet&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : info.standbyTasks.entrySet()) {&lt;br/&gt;
+        for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : info.standbyTasks().entrySet()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {             TaskId id = entry.getKey();             Set&amp;lt;TopicPartition&amp;gt; partitions = entry.getValue();             for (TopicPartition partition }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (info.standbyTasks.size() &amp;gt; 0) {&lt;br/&gt;
+        if (info.standbyTasks().size() &amp;gt; 0) 
{
             // check if standby partitions cover all topics
             assertEquals(expectedTopics, standbyTopics);
         }
&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index ec94ad81acd..726a5623cd5 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -33,6 +33,7 @@&lt;br/&gt;
 import java.util.Set;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static org.junit.Assert.assertEquals;&lt;br/&gt;
+import static org.junit.Assert.assertNull;&lt;/p&gt;

&lt;p&gt; public class AssignmentInfoTest {&lt;/p&gt;

&lt;p&gt;@@ -61,10 +62,10 @@ public void shouldDecodePreviousVersion() throws IOException &lt;/p&gt;
{
         standbyTasks.put(new TaskId(2, 0), Utils.mkSet(new TopicPartition(&quot;t3&quot;, 0), new TopicPartition(&quot;t3&quot;, 0)));
         final AssignmentInfo oldVersion = new AssignmentInfo(1, activeTasks, standbyTasks, null);
         final AssignmentInfo decoded = AssignmentInfo.decode(encodeV1(oldVersion));
-        assertEquals(oldVersion.activeTasks, decoded.activeTasks);
-        assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
-        assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn&apos;t in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(oldVersion.activeTasks(), decoded.activeTasks());
+        assertEquals(oldVersion.standbyTasks(), decoded.standbyTasks());
+        assertNull(decoded.partitionsByHost()); // should be null as wasn&apos;t in V1
+        assertEquals(1, decoded.version());
     }


&lt;p&gt;@@ -76,15 +77,15 @@ private ByteBuffer encodeV1(AssignmentInfo oldVersion) throws IOException {&lt;br/&gt;
         ByteArrayOutputStream baos = new ByteArrayOutputStream();&lt;br/&gt;
         DataOutputStream out = new DataOutputStream(baos);&lt;br/&gt;
         // Encode version&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;out.writeInt(oldVersion.version);&lt;br/&gt;
+        out.writeInt(oldVersion.version());&lt;br/&gt;
         // Encode active tasks&lt;/li&gt;
	&lt;li&gt;out.writeInt(oldVersion.activeTasks.size());&lt;/li&gt;
	&lt;li&gt;for (TaskId id : oldVersion.activeTasks) {&lt;br/&gt;
+        out.writeInt(oldVersion.activeTasks().size());&lt;br/&gt;
+        for (TaskId id : oldVersion.activeTasks()) 
{
             id.writeTo(out);
         }
&lt;p&gt;         // Encode standby tasks&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;out.writeInt(oldVersion.standbyTasks.size());&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : oldVersion.standbyTasks.entrySet()) {&lt;br/&gt;
+        out.writeInt(oldVersion.standbyTasks().size());&lt;br/&gt;
+        for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : oldVersion.standbyTasks().entrySet()) {&lt;br/&gt;
             TaskId id = entry.getKey();&lt;br/&gt;
             id.writeTo(out);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
index 9c011bb0cae..633285a2b4d 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
@@ -65,14 +65,12 @@ public void shouldBeBackwardCompatible() &lt;/p&gt;
{
 
         final ByteBuffer v1Encoding = encodePreviousVersion(processId, activeTasks, standbyTasks);
         final SubscriptionInfo decode = SubscriptionInfo.decode(v1Encoding);
-        assertEquals(activeTasks, decode.prevTasks);
-        assertEquals(standbyTasks, decode.standbyTasks);
-        assertEquals(processId, decode.processId);
-        assertNull(decode.userEndPoint);
-
+        assertEquals(activeTasks, decode.prevTasks());
+        assertEquals(standbyTasks, decode.standbyTasks());
+        assertEquals(processId, decode.processId());
+        assertNull(decode.userEndPoint());
     }

&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;/li&gt;
	&lt;li&gt;so it is impossible to test compatibility without having this&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16407503" author="githubbot" created="Wed, 21 Mar 2018 06:03:45 +0000"  >&lt;p&gt;mjsax opened a new pull request #4746: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4746&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4746&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16410019" author="githubbot" created="Thu, 22 Mar 2018 18:07:52 +0000"  >&lt;p&gt;mjsax opened a new pull request #4758: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4758&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4758&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16410797" author="githubbot" created="Fri, 23 Mar 2018 04:39:02 +0000"  >&lt;p&gt;mjsax opened a new pull request #4761:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4761&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4761&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16412266" author="githubbot" created="Fri, 23 Mar 2018 23:48:07 +0000"  >&lt;p&gt;mjsax opened a new pull request #4768:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4768&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4768&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16413272" author="githubbot" created="Mon, 26 Mar 2018 01:55:49 +0000"  >&lt;p&gt;mjsax opened a new pull request #4773:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4773&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4773&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16414777" author="githubbot" created="Mon, 26 Mar 2018 23:31:50 +0000"  >&lt;p&gt;mjsax closed pull request #4758: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4758&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4758&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh&lt;br/&gt;
index af10f61b5c4..a25868125e9 100755&lt;br/&gt;
&amp;#8212; a/bin/kafka-run-class.sh&lt;br/&gt;
+++ b/bin/kafka-run-class.sh&lt;br/&gt;
@@ -73,28 +73,50 @@ do&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/clients/build/libs/kafka-clients*.jar;&lt;br/&gt;
-do&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if should_include_file &quot;$file&quot;; then&lt;/li&gt;
	&lt;li&gt;CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;/li&gt;
	&lt;li&gt;fi&lt;br/&gt;
-done&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  clients_lib_dir=$(dirname $0)/../clients/build/libs&lt;br/&gt;
+  streams_lib_dir=$(dirname $0)/../streams/build/libs&lt;br/&gt;
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}&lt;br/&gt;
+else&lt;br/&gt;
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs&lt;br/&gt;
+  streams_lib_dir=$clients_lib_dir&lt;br/&gt;
+  rocksdb_lib_dir=$streams_lib_dir&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/build/libs/kafka-streams*.jar;&lt;br/&gt;
+for file in &quot;$clients_lib_dir&quot;/kafka-clients*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+for file in &quot;$streams_lib_dir&quot;/kafka-streams*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;&lt;del&gt;for file in &quot;$base_dir&quot;/streams/build/dependant-libs&lt;/del&gt;${SCALA_VERSION}/rocksdb*.jar;&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+else&lt;br/&gt;
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed &apos;s/\.//g&apos;`&lt;br/&gt;
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;(${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;br/&gt;
+for file in &quot;$rocksdb_lib_dir&quot;/rocksdb*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
 done&lt;br/&gt;
diff --git a/build.gradle b/build.gradle&lt;br/&gt;
index 20a184c437c..5e97f901cb6 100644&lt;br/&gt;
&amp;#8212; a/build.gradle&lt;br/&gt;
+++ b/build.gradle&lt;br/&gt;
@@ -770,6 +770,30 @@ project(&apos;:streams:examples&apos;) {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;+project(&apos;:streams:upgrade-system-tests-0100&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0100&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0100
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0101&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0101&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
 project(&apos;:log4j-appender&apos;) {&lt;br/&gt;
   archivesBaseName = &quot;kafka-log4j-appender&quot;&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
index 6094b547bb7..b80dfccf3d9 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
@@ -17,7 +17,9 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.security.authenticator;&lt;/p&gt;

&lt;p&gt;-import java.util.Map;&lt;br/&gt;
+import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
+import org.apache.kafka.common.network.Mode;&lt;br/&gt;
+import org.apache.kafka.common.security.auth.AuthCallbackHandler;&lt;/p&gt;

&lt;p&gt; import javax.security.auth.Subject;&lt;br/&gt;
 import javax.security.auth.callback.Callback;&lt;br/&gt;
@@ -26,10 +28,7 @@&lt;br/&gt;
 import javax.security.auth.callback.UnsupportedCallbackException;&lt;br/&gt;
 import javax.security.sasl.AuthorizeCallback;&lt;br/&gt;
 import javax.security.sasl.RealmCallback;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
-import org.apache.kafka.common.network.Mode;&lt;br/&gt;
-import org.apache.kafka.common.security.auth.AuthCallbackHandler;&lt;br/&gt;
+import java.util.Map;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Callback handler for Sasl clients. The callbacks required for the SASL mechanism&lt;br/&gt;
diff --git a/docs/streams.html b/docs/streams.html&lt;br/&gt;
index fe0e84ee3b7..d691e63a432 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/docs/streams.html&lt;br/&gt;
+++ b/docs/streams.html&lt;br/&gt;
@@ -807,21 +807,50 @@ &amp;lt;h2&amp;gt;&amp;lt;a id=&quot;streams_upgrade_and_api&quot; href=&quot;#streams_upgrade_and_api&quot;&amp;gt;Upgrade Guid&lt;br/&gt;
         See &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.2 API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
         &amp;lt;/p&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        &amp;lt;p&amp;gt;&lt;br/&gt;
+        Upgrading from 0.10.0.x to 0.10.2.x directly is also possible.&lt;br/&gt;
+        See &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; and &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt;&lt;br/&gt;
+        for a complete list of API changes.&lt;br/&gt;
+        Upgrading to 0.10.2.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, and offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;/p&amp;gt;&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from=&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.2.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+        &amp;lt;p&amp;gt; Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported) &amp;lt;/p&amp;gt;&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.2.0 or 0.10.2.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
         &amp;lt;p&amp;gt;&lt;br/&gt;
         If you want to upgrade from 0.10.0.x to 0.10.1, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1010_streams&quot;&amp;gt;Upgrade Section for 0.10.1&amp;lt;/a&amp;gt;.&lt;br/&gt;
         It highlights incompatible changes you need to consider to upgrade your code and application.&lt;br/&gt;
         See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
         &amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_01021&quot; href=&quot;#streams_api_changes_0102&quot;&amp;gt;Notable changes in 0.10.2.1&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;br/&gt;
+        &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_01022&quot; href=&quot;#streams_api_changes_0102&quot;&amp;gt;Notable changes in 0.10.2.2&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
+        &amp;lt;p&amp;gt;&lt;br/&gt;
+            Parameter updates in &amp;lt;code&amp;gt;StreamsConfig&amp;lt;/code&amp;gt;:&lt;br/&gt;
+        &amp;lt;/p&amp;gt;&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; New configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+        &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_01021&quot; href=&quot;#streams_api_changes_0102&quot;&amp;gt;Notable changes in 0.10.2.1&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
+        &amp;lt;p&amp;gt;&lt;br/&gt;
             Parameter updates in &amp;lt;code&amp;gt;StreamsConfig&amp;lt;/code&amp;gt;:&lt;br/&gt;
         &amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
             &amp;lt;li&amp;gt; of particular importance to improve the resiliency of a Kafka Streams application are two changes to default parameters of producer &amp;lt;code&amp;gt;retries&amp;lt;/code&amp;gt; and consumer &amp;lt;code&amp;gt;max.poll.interval.ms&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_0102&quot; href=&quot;#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_0102&quot; href=&quot;#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
         &amp;lt;p&amp;gt;&lt;br/&gt;
             New methods in &amp;lt;code&amp;gt;KafkaStreams&amp;lt;/code&amp;gt;:&lt;br/&gt;
         &amp;lt;/p&amp;gt;&lt;br/&gt;
diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index d7581fa8dac..77477628f4d 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -61,6 +61,11 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams&quot; href=&quot;#upgrade_1020_streams&quot;&amp;gt;Upgrading a 0.10.1&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10202_notable&quot; href=&quot;#upgrade_10202_notable&quot;&amp;gt;Notable changes in 0.10.2.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10201_notable&quot; href=&quot;#upgrade_10201_notable&quot;&amp;gt;Notable changes in 0.10.2.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
   &amp;lt;li&amp;gt; The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer &amp;lt;code&amp;gt;retries&amp;lt;/code&amp;gt; default value was changed from 0 to 10. The internal Kafka Streams consumer &amp;lt;code&amp;gt;max.poll.interval.ms&amp;lt;/code&amp;gt;  default value was changed from 300000 to &amp;lt;code&amp;gt;Integer.MAX_VALUE&amp;lt;/code&amp;gt;.&lt;br/&gt;
@@ -141,6 +146,23 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_streams&quot; href=&quot;#upgrade_1010_streams&quot;&amp;gt;Upgrading a 0.10.0&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.1 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, and offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.1.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.1.0 or 0.10.1.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_notable&quot; href=&quot;#upgrade_1010_notable&quot;&amp;gt;Notable changes in 0.10.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle&lt;br/&gt;
index 25faa90a78f..4084b12216d 100644&lt;br/&gt;
&amp;#8212; a/gradle/dependencies.gradle&lt;br/&gt;
+++ b/gradle/dependencies.gradle&lt;br/&gt;
@@ -31,6 +31,8 @@ versions += [&lt;br/&gt;
   jackson: &quot;2.8.5&quot;,&lt;br/&gt;
   jetty: &quot;9.2.22.v20170606&quot;,&lt;br/&gt;
   jersey: &quot;2.24&quot;,&lt;br/&gt;
+  kafka_0100: &quot;0.10.0.1&quot;,&lt;br/&gt;
+  kafka_0101: &quot;0.10.1.1&quot;,&lt;br/&gt;
   log4j: &quot;1.2.17&quot;,&lt;br/&gt;
   jopt: &quot;5.0.3&quot;,&lt;br/&gt;
   junit: &quot;4.12&quot;,&lt;br/&gt;
@@ -92,6 +94,8 @@ libs += [&lt;br/&gt;
   junit: &quot;junit:junit:$versions.junit&quot;,&lt;br/&gt;
   log4j: &quot;log4j:log4j:$versions.log4j&quot;,&lt;br/&gt;
   joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
+  kafkaStreams_0100: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0100&quot;,&lt;br/&gt;
+  kafkaStreams_0101: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0101&quot;,&lt;br/&gt;
   lz4: &quot;net.jpountz.lz4:lz4:$versions.lz4&quot;,&lt;br/&gt;
   metrics: &quot;com.yammer.metrics:metrics-core:$versions.metrics&quot;,&lt;br/&gt;
   powermock: &quot;org.powermock:powermock-module-junit4:$versions.powermock&quot;,&lt;br/&gt;
diff --git a/settings.gradle b/settings.gradle&lt;br/&gt;
index 29d38950a8a..576b40b9ce1 100644&lt;br/&gt;
&amp;#8212; a/settings.gradle&lt;br/&gt;
+++ b/settings.gradle&lt;br/&gt;
@@ -13,5 +13,6 @@&lt;br/&gt;
 // See the License for the specific language governing permissions and&lt;br/&gt;
 // limitations under the License.&lt;/p&gt;

&lt;p&gt;-include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
+include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;streams:upgrade-system-tests-0100&apos;,&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0101&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
         &apos;connect:api&apos;, &apos;connect:transforms&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index 0724571a0bc..3baa0785376 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -95,6 +95,16 @@&lt;br/&gt;
      */&lt;br/&gt;
     public static final String PRODUCER_PREFIX = &quot;producer.&quot;;&lt;/p&gt;

&lt;p&gt;+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.0.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
+    public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;br/&gt;
+    public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+        &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot; (for upgrading from 0.10.0.x).&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /** &lt;/p&gt;
{@code state.dir}
&lt;p&gt; */&lt;br/&gt;
     public static final String STATE_DIR_CONFIG = &quot;state.dir&quot;;&lt;br/&gt;
     private static final String STATE_DIR_DOC = &quot;Directory location for state store.&quot;;&lt;br/&gt;
@@ -383,7 +393,13 @@&lt;br/&gt;
                     40 * 1000,&lt;br/&gt;
                     atLeast(0),&lt;br/&gt;
                     ConfigDef.Importance.MEDIUM,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;REQUEST_TIMEOUT_MS_DOC);&lt;br/&gt;
+                    REQUEST_TIMEOUT_MS_DOC)&lt;br/&gt;
+            .define(UPGRADE_FROM_CONFIG,&lt;br/&gt;
+                    ConfigDef.Type.STRING,&lt;br/&gt;
+                    null,&lt;br/&gt;
+                    in(null, UPGRADE_FROM_0100),&lt;br/&gt;
+                    ConfigDef.Importance.LOW,&lt;br/&gt;
+                    UPGRADE_FROM_DOC);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // this is the list of configs for underlying clients&lt;br/&gt;
@@ -501,6 +517,7 @@ public StreamsConfig(final Map&amp;lt;?, ?&amp;gt; props) {&lt;br/&gt;
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + &quot;-consumer&quot;);&lt;/p&gt;

&lt;p&gt;         // add configs required for stream partition assignor&lt;br/&gt;
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));&lt;br/&gt;
         consumerProps.put(InternalConfig.STREAM_THREAD_INSTANCE, streamThread);&lt;br/&gt;
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));&lt;br/&gt;
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
index a50a81914cf..889d2ff6a32 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
@@ -14,7 +14,6 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-&lt;br/&gt;
 package org.apache.kafka.streams.processor.internals;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.clients.consumer.internals.PartitionAssignor;&lt;br/&gt;
@@ -155,6 +154,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {&lt;br/&gt;
     private String userEndPoint;&lt;br/&gt;
     private int numStandbyReplicas;&lt;/p&gt;

&lt;p&gt;+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
+&lt;br/&gt;
     private Cluster metadataWithInternalTopics;&lt;br/&gt;
     private Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState;&lt;/p&gt;

&lt;p&gt;@@ -182,6 +183,12 @@ void time(final Time time) {&lt;br/&gt;
     public void configure(Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         numStandbyReplicas = (Integer) configs.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);&lt;/p&gt;

&lt;p&gt;+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;br/&gt;
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) &lt;/p&gt;
{
+            log.info(&quot;Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x.&quot;);
+            userMetadataVersion = 1;
+        }
&lt;p&gt;+&lt;br/&gt;
         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);&lt;br/&gt;
         if (o == null) {&lt;br/&gt;
             KafkaException ex = new KafkaException(&quot;StreamThread is not specified&quot;);&lt;br/&gt;
@@ -241,7 +248,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; prevTasks = streamThread.prevTasks();&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; standbyTasks = streamThread.cachedTasks();&lt;br/&gt;
         standbyTasks.removeAll(prevTasks);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;SubscriptionInfo data = new SubscriptionInfo(streamThread.processId, prevTasks, standbyTasks, this.userEndPoint);&lt;br/&gt;
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, streamThread.processId, prevTasks, standbyTasks, this.userEndPoint);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (streamThread.builder.sourceTopicPattern() != null) {&lt;br/&gt;
             SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();&lt;br/&gt;
@@ -279,11 +286,16 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // construct the client metadata from the decoded subscription info&lt;br/&gt;
         Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;br/&gt;
             String consumerId = entry.getKey();&lt;br/&gt;
             Subscription subscription = entry.getValue();&lt;/p&gt;

&lt;p&gt;             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+            final int usedVersion = info.version;&lt;br/&gt;
+            if (usedVersion &amp;lt; minUserMetadataVersion) &lt;/p&gt;
{
+                minUserMetadataVersion = usedVersion;
+            }

&lt;p&gt;             // create the new client metadata if necessary&lt;br/&gt;
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);&lt;br/&gt;
@@ -539,7 +551,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                 }&lt;/p&gt;

&lt;p&gt;                 // finally, encode the assignment before sending back to coordinator&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));&lt;br/&gt;
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));&lt;br/&gt;
                 i++;&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index ddbd67d8d88..7a6bf14d918 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -14,7 +14,6 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-&lt;br/&gt;
 package org.apache.kafka.streams.processor.internals.assignment;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.record.ByteBufferInputStream;&lt;br/&gt;
@@ -56,7 +55,7 @@ public AssignmentInfo(List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;&lt;br/&gt;
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+    public AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
                              Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.activeTasks = activeTasks;&lt;br/&gt;
@@ -155,9 +154,7 @@ public static AssignmentInfo decode(ByteBuffer data) {&lt;br/&gt;
                 }&lt;br/&gt;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
         } catch (IOException ex) 
{
             throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);
         }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index c3481c05156..92c50a2a942 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -14,7 +14,6 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-&lt;br/&gt;
 package org.apache.kafka.streams.processor.internals.assignment;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.streams.errors.TaskAssignmentException;&lt;br/&gt;
@@ -32,7 +31,7 @@&lt;/p&gt;

&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int CURRENT_VERSION = 2;&lt;br/&gt;
+    public static final int CURRENT_VERSION = 2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public final int version;&lt;br/&gt;
     public final UUID processId;&lt;br/&gt;
@@ -44,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; stand&lt;br/&gt;
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
+    public SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.processId = processId;&lt;br/&gt;
         this.prevTasks = prevTasks;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java&lt;br/&gt;
index 50ab1175cef..832883a0268 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java&lt;br/&gt;
@@ -96,7 +96,7 @@ public boolean conditionMet() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testStateCloseAfterCreate() throws Exception {&lt;br/&gt;
+    public void testStateCloseAfterCreate() {&lt;br/&gt;
         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -160,7 +160,7 @@ public void testStateThreadClose() throws Exception &lt;/p&gt;
{
         // make sure we have the global state thread running too
         builder.globalTable(&quot;anyTopic&quot;, &quot;anyStore&quot;);
         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, numThreads);
-        final KafkaStreams streams = new KafkaStreams(builder, props);
+        new KafkaStreams(builder, props);
 
         testStateThreadCloseHelper(numThreads);
     }
&lt;p&gt;@@ -200,9 +200,8 @@ public boolean conditionMet() {&lt;/p&gt;

&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testInitializesAndDestroysMetricsReporters() throws Exception {&lt;br/&gt;
+    public void testInitializesAndDestroysMetricsReporters() {&lt;br/&gt;
         final int oldInitCount = MockMetricsReporter.INIT_COUNT.get();&lt;br/&gt;
         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;br/&gt;
@@ -217,7 +216,7 @@ public void testInitializesAndDestroysMetricsReporters() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCloseIsIdempotent() throws Exception {&lt;br/&gt;
+    public void testCloseIsIdempotent() {&lt;br/&gt;
         streams.close();&lt;br/&gt;
         final int closeCount = MockMetricsReporter.CLOSE_COUNT.get();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -227,7 +226,7 @@ public void testCloseIsIdempotent() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCannotStartOnceClosed() throws Exception {&lt;br/&gt;
+    public void testCannotStartOnceClosed() {&lt;br/&gt;
         streams.start();&lt;br/&gt;
         streams.close();&lt;br/&gt;
         try {&lt;br/&gt;
@@ -241,7 +240,7 @@ public void testCannotStartOnceClosed() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCannotStartTwice() throws Exception {&lt;br/&gt;
+    public void testCannotStartTwice() {&lt;br/&gt;
         streams.start();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         try {&lt;br/&gt;
@@ -267,10 +266,10 @@ public void testIllegalMetricsConfig() &lt;/p&gt;
{
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;appId&quot;);
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
         props.setProperty(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG, &quot;illegalConfig&quot;);
         final KStreamBuilder builder = new KStreamBuilder();
-        final KafkaStreams streams = new KafkaStreams(builder, props);
-
+        new KafkaStreams(builder, props);
     }

&lt;p&gt;     @Test&lt;br/&gt;
@@ -278,6 +277,7 @@ public void testLegalMetricsConfig() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;appId&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());&lt;br/&gt;
         props.setProperty(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG, Sensor.RecordingLevel.INFO.toString());&lt;br/&gt;
         final KStreamBuilder builder1 = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams1 = new KafkaStreams(builder1, props);&lt;br/&gt;
@@ -285,27 +285,26 @@ public void testLegalMetricsConfig() &lt;/p&gt;
{
 
         props.setProperty(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG, Sensor.RecordingLevel.DEBUG.toString());
         final KStreamBuilder builder2 = new KStreamBuilder();
-        final KafkaStreams streams2 = new KafkaStreams(builder2, props);
-
+        new KafkaStreams(builder2, props);
     }

&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetAllTasksWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetAllTasksWhenNotRunning() 
{
         streams.allMetadata();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetAllTasksWithStoreWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetAllTasksWithStoreWhenNotRunning() 
{
         streams.allMetadataForStore(&quot;store&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() 
{
         streams.metadataForKey(&quot;store&quot;, &quot;key&quot;, Serdes.String().serializer());
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() {&lt;br/&gt;
         streams.metadataForKey(&quot;store&quot;, &quot;key&quot;, new StreamPartitioner&amp;lt;String, Object&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public Integer partition(final String key, final Object value, final int numPartitions) {&lt;br/&gt;
@@ -321,6 +320,7 @@ public void shouldReturnFalseOnCloseWhenThreadsHaventTerminated() throws Excepti&lt;br/&gt;
             final Properties props = new Properties();&lt;br/&gt;
             props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;appId&quot;);&lt;br/&gt;
             props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+            props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;br/&gt;
             props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
@@ -366,16 +366,18 @@ private KafkaStreams createKafkaStreams() &lt;/p&gt;
{
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;appId&quot;);
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
 
         final KStreamBuilder builder = new KStreamBuilder();
         return new KafkaStreams(builder, props);
     }

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCleanup() throws Exception {&lt;br/&gt;
+    public void testCleanup() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testLocalCleanup&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;br/&gt;
@@ -391,6 +393,7 @@ public void testCannotCleanupWhileRunning() throws Exception &lt;/p&gt;
{
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testCannotCleanupWhileRunning&quot;);
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
 
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -448,6 +451,5 @@ public void onChange(final KafkaStreams.State newState, final KafkaStreams.State
                 streams.close();
             }
&lt;p&gt;         }&lt;br/&gt;
-&lt;br/&gt;
     }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
index 15cc1af8e5d..ab8701fa966 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
@@ -60,7 +60,7 @@ public void setUp() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetProducerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetProducerConfigs() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getProducerConfigs(&quot;client&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ProducerConfig.CLIENT_ID_CONFIG), &quot;client-producer&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ProducerConfig.LINGER_MS_CONFIG), &quot;100&quot;);&lt;br/&gt;
@@ -68,7 +68,7 @@ public void testGetProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetConsumerConfigs() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getConsumerConfigs(null, &quot;example-application&quot;, &quot;client&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), &quot;client-consumer&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG), &quot;example-application&quot;);&lt;br/&gt;
@@ -77,7 +77,7 @@ public void testGetConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetRestoreConsumerConfigs() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getRestoreConsumerConfigs(&quot;client&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), &quot;client-restore-consumer&quot;);&lt;br/&gt;
         assertNull(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG));&lt;br/&gt;
@@ -86,7 +86,7 @@ public void testGetRestoreConsumerConfigs() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void defaultSerdeShouldBeConfigured() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; serializerConfigs = new HashMap&amp;lt;String, Object&amp;gt;();&lt;br/&gt;
+        Map&amp;lt;String, Object&amp;gt; serializerConfigs = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         serializerConfigs.put(&quot;key.serializer.encoding&quot;, &quot;UTF8&quot;);&lt;br/&gt;
         serializerConfigs.put(&quot;value.serializer.encoding&quot;, &quot;UTF-16&quot;);&lt;br/&gt;
         Serializer&amp;lt;String&amp;gt; serializer = Serdes.String().serializer();&lt;br/&gt;
@@ -117,7 +117,7 @@ public void shouldSupportMultipleBootstrapServers() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedConsumerConfigs() {&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;earliest&quot;);&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -127,7 +127,7 @@ public void shouldSupportPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedRestoreConsumerConfigs() {&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;earliest&quot;);&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -137,7 +137,7 @@ public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() 
{
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = streamsConfig.getConsumerConfigs(null, &quot;groupId&quot;, &quot;clientId&quot;);
@@ -145,7 +145,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() thro
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() 
{
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = streamsConfig.getRestoreConsumerConfigs(&quot;clientId&quot;);
@@ -153,7 +153,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() {&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         props.put(producerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; producerConfigs = streamsConfig.getProducerConfigs(&quot;clientId&quot;);&lt;br/&gt;
@@ -162,7 +162,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() thro&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedProducerConfigs() {&lt;br/&gt;
         props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);&lt;br/&gt;
         props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -172,7 +172,7 @@ public void shouldSupportPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldBeSupportNonPrefixedConsumerConfigs() {&lt;br/&gt;
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;br/&gt;
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -182,7 +182,7 @@ public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() 
{
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -192,7 +192,7 @@ public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportNonPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportNonPrefixedProducerConfigs() {&lt;br/&gt;
         props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10);&lt;br/&gt;
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -201,24 +201,22 @@ public void shouldSupportNonPrefixedProducerConfigs() throws Exception 
{
         assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
     @Test(expected = StreamsException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() throws Exception {&lt;br/&gt;
+    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() 
{
         props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.keySerde();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = StreamsException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() throws Exception {&lt;br/&gt;
+    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() 
{
         props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.valueSerde();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultConsumerConfigs() {&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;latest&quot;);&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), &quot;10&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -228,7 +226,7 @@ public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultProducerConfigs() {&lt;br/&gt;
         props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), &quot;10000&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; producerConfigs = streamsConfig.getProducerConfigs(&quot;client&quot;);&lt;br/&gt;
@@ -236,7 +234,7 @@ public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() 
{
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;latest&quot;);
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), &quot;10&quot;);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -246,14 +244,14 @@ public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = ConfigException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() 
{
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), &quot;true&quot;);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getConsumerConfigs(null, &quot;a&quot;, &quot;b&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = ConfigException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() {&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), &quot;true&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         streamsConfig.getRestoreConsumerConfigs(&quot;client&quot;);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java&lt;br/&gt;
index 1d2a3e217e1..12568246e5d 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java&lt;br/&gt;
@@ -32,6 +32,7 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.ValueMapper;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.BeforeClass;&lt;br/&gt;
 import org.junit.ClassRule;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
@@ -112,6 +113,7 @@ public void shouldFanoutTheInput() throws Exception {&lt;br/&gt;
         final Properties streamsConfiguration = new Properties();&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;fanout-integration-test&quot;);&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        streamsConfiguration.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());&lt;br/&gt;
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, cacheSizeBytes);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index 6a8c7ff10e1..3e0d80a6b19 100644&lt;/li&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -10,6 +10,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.integration;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import kafka.utils.MockTime;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Deserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.IntegerSerializer;&lt;br/&gt;
@@ -44,8 +45,6 @@&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.concurrent.ExecutionException;&lt;/p&gt;

&lt;p&gt;-import kafka.utils.MockTime;&lt;br/&gt;
-&lt;br/&gt;
 import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
 import static org.hamcrest.core.Is.is;&lt;/p&gt;

&lt;p&gt;@@ -127,7 +126,7 @@ public void shouldReduce() throws Exception {&lt;br/&gt;
         List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;5);&lt;br/&gt;
+            5);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
@@ -177,7 +176,7 @@ public String apply(Windowed&amp;lt;String&amp;gt; windowedKey, String value) {&lt;br/&gt;
         List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; windowedOutput = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;10);&lt;br/&gt;
+            10);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;&lt;br/&gt;
             comparator =&lt;br/&gt;
@@ -229,7 +228,7 @@ public String apply(final Windowed&amp;lt;Integer&amp;gt; windowedKey, final Long value) {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new LongDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;5);&lt;br/&gt;
+            5);&lt;br/&gt;
         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public int compare(final KeyValue&amp;lt;String, Long&amp;gt; o1, final KeyValue&amp;lt;String, Long&amp;gt; o2) {&lt;br/&gt;
@@ -303,6 +302,4 @@ private void startStreams() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java&lt;br/&gt;
index bd5911d270b..13124f1cc5e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java&lt;br/&gt;
@@ -155,7 +155,7 @@ public void shouldReduce() throws Exception {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;10);&lt;br/&gt;
+            10);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
@@ -209,7 +209,7 @@ public String apply(final Windowed&amp;lt;String&amp;gt; windowedKey, final String value) {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; windowedOutput = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;15);&lt;br/&gt;
+            15);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;&lt;br/&gt;
             comparator =&lt;br/&gt;
@@ -263,7 +263,7 @@ public void shouldAggregate() throws Exception {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new IntegerDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;10);&lt;br/&gt;
+            10);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
@@ -313,7 +313,7 @@ public String apply(final Windowed&amp;lt;String&amp;gt; windowedKey, final Integer value) {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt; windowedMessages = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new IntegerDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;15);&lt;br/&gt;
+            15);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Comparator&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt;&lt;br/&gt;
             comparator =&lt;br/&gt;
@@ -364,7 +364,7 @@ public void shouldCount() throws Exception {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new LongDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;10);&lt;br/&gt;
+            10);&lt;br/&gt;
         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public int compare(final KeyValue&amp;lt;String, Long&amp;gt; o1, final KeyValue&amp;lt;String, Long&amp;gt; o2) {&lt;br/&gt;
@@ -406,7 +406,7 @@ public String apply(final Windowed&amp;lt;Integer&amp;gt; windowedKey, final Long value) {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;br/&gt;
             new LongDeserializer(),&lt;/li&gt;
	&lt;li&gt;10);&lt;br/&gt;
+            10);&lt;br/&gt;
         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public int compare(final KeyValue&amp;lt;String, Long&amp;gt; o1, final KeyValue&amp;lt;String, Long&amp;gt; o2) {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
index 64e8459b274..d09d505b5e7 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
@@ -129,7 +129,7 @@ public void createTopics() throws InterruptedException {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Before&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void before() throws IOException, InterruptedException {&lt;br/&gt;
+    public void before() throws Exception {&lt;br/&gt;
         testNo++;&lt;br/&gt;
         createTopics();&lt;br/&gt;
         streamsConfiguration = new Properties();&lt;br/&gt;
@@ -609,15 +609,13 @@ private void verifyCanGetByKey(final String[] keys,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param failIfKeyNotFound     if true, tests fails if an expected key is not found in store. If false,&lt;/li&gt;
	&lt;li&gt;the method merely inserts the new found key into the list of&lt;/li&gt;
	&lt;li&gt;expected keys.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws InterruptedException&lt;br/&gt;
      */&lt;br/&gt;
     private void verifyGreaterOrEqual(final String[] keys,&lt;br/&gt;
                                       final Map&amp;lt;String, Long&amp;gt; expectedWindowedCount,&lt;br/&gt;
                                       final Map&amp;lt;String, Long&amp;gt; expectedCount,&lt;br/&gt;
                                       final ReadOnlyWindowStore&amp;lt;String, Long&amp;gt; windowStore,&lt;br/&gt;
                                       final ReadOnlyKeyValueStore&amp;lt;String, Long&amp;gt; keyValueStore,&lt;/li&gt;
	&lt;li&gt;final boolean failIfKeyNotFound)&lt;/li&gt;
	&lt;li&gt;throws InterruptedException {&lt;br/&gt;
+                                      final boolean failIfKeyNotFound) {&lt;br/&gt;
         final Map&amp;lt;String, Long&amp;gt; windowState = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         final Map&amp;lt;String, Long&amp;gt; countState = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -744,5 +742,4 @@ public void run() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
index 6503038e5b3..e06ed739048 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
@@ -44,6 +44,7 @@&lt;br/&gt;
 import org.apache.kafka.test.MockProcessorSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockStateStoreSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockTimestampExtractor;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.Assert;&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt;@@ -111,6 +112,7 @@ private Properties configProps() {&lt;/p&gt;
             {
                 setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;stream-partition-assignor-test&quot;);
                 setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, userEndPoint);
+                setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
                 setProperty(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, &quot;3&quot;);
                 setProperty(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName());
             }
&lt;p&gt;@@ -119,7 +121,7 @@ private Properties configProps() {&lt;/p&gt;

&lt;p&gt;     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testSubscription() throws Exception {&lt;br/&gt;
+    public void testSubscription() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -159,7 +161,7 @@ public void testSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignBasic() throws Exception {&lt;br/&gt;
+    public void testAssignBasic() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -227,7 +229,7 @@ public void testAssignBasic() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
+    public void testAssignWithPartialTopology() {&lt;br/&gt;
         Properties props = configProps();&lt;br/&gt;
         props.put(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, SingleGroupPartitionGrouperStub.class);&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(props);&lt;br/&gt;
@@ -267,7 +269,7 @@ public void testAssignWithPartialTopology() throws Exception {&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
+    public void testAssignEmptyMetadata() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -324,7 +326,7 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
+    public void testAssignWithNewTasks() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addSource(&quot;source3&quot;, &quot;topic3&quot;);&lt;br/&gt;
@@ -381,7 +383,7 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStates() throws Exception {&lt;br/&gt;
+    public void testAssignWithStates() {&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
@@ -470,7 +472,7 @@ public void testAssignWithStates() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
+    public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         Properties props = configProps();&lt;br/&gt;
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(props);&lt;br/&gt;
@@ -543,7 +545,7 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testOnAssignment() throws Exception {&lt;br/&gt;
+    public void testOnAssignment() {&lt;br/&gt;
         TopicPartition t2p3 = new TopicPartition(&quot;topic2&quot;, 3);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         TopologyBuilder builder = new TopologyBuilder();&lt;br/&gt;
@@ -576,7 +578,7 @@ public void testOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopics() {&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addInternalTopic(&quot;topicX&quot;);&lt;br/&gt;
@@ -612,7 +614,7 @@ public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() 
{
         String applicationId = &quot;test&quot;;
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(&quot;topicX&quot;);
@@ -650,7 +652,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
+    public void shouldAddUserDefinedEndPointToSubscription() {&lt;br/&gt;
         final Properties properties = configProps();&lt;br/&gt;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, &quot;localhost:8080&quot;);&lt;br/&gt;
         final StreamsConfig config = new StreamsConfig(properties);&lt;br/&gt;
@@ -663,8 +665,8 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
         final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
         final String client1 = &quot;client1&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1, uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),&lt;/li&gt;
	&lt;li&gt;0);&lt;br/&gt;
+        final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1,&lt;br/&gt;
+            uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST), 0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         partitionAssignor.configure(config.getConsumerConfigs(streamThread, applicationId, client1));&lt;br/&gt;
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;input&quot;));&lt;br/&gt;
@@ -673,7 +675,80 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() 
{
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            &quot;consumer1&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            &quot;consumer2&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        StreamsConfig config = new StreamsConfig(configProps());
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            &quot;appId&quot;,
+            &quot;clientId&quot;,
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0);
+
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, &quot;test&quot;, &quot;clientId&quot;));
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscription() &lt;/p&gt;
{
+        final Properties properties = configProps();
+        properties.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        StreamsConfig config = new StreamsConfig(properties);
+
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);
+
+        String clientId = &quot;client-id&quot;;
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            &quot;appId&quot;,
+            &quot;clientId&quot;,
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0);
+
+        StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, &quot;test&quot;, clientId));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldMapUserEndPointToTopicPartitions() {&lt;br/&gt;
         final Properties properties = configProps();&lt;br/&gt;
         final String myEndPoint = &quot;localhost:8080&quot;;&lt;br/&gt;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);&lt;br/&gt;
@@ -711,7 +786,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() 
{
         final Properties properties = configProps();
         final String myEndPoint = &quot;localhost&quot;;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -736,7 +811,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() thr
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() 
{
         final Properties properties = configProps();
         final String myEndPoint = &quot;localhost:j87yhk&quot;;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -760,7 +835,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() th
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() 
{
         List&amp;lt;TopicPartition&amp;gt; topic = Collections.singletonList(new TopicPartition(&quot;topic&quot;, 0));
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState =
                 Collections.singletonMap(new HostInfo(&quot;localhost&quot;, 80),
@@ -773,7 +848,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSetClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldSetClusterMetadataOnAssignment() {&lt;br/&gt;
         final List&amp;lt;TopicPartition&amp;gt; topic = Collections.singletonList(new TopicPartition(&quot;topic&quot;, 0));&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState =&lt;br/&gt;
                 Collections.singletonMap(new HostInfo(&quot;localhost&quot;, 80),&lt;br/&gt;
@@ -793,7 +868,7 @@ public void shouldSetClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() throws Exception {&lt;br/&gt;
+    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() 
{
         final Cluster cluster = partitionAssignor.clusterMetadata();
         assertNotNull(cluster);
     }
&lt;p&gt;@@ -891,11 +966,11 @@ public Object apply(Object value1, Object value2) &lt;/p&gt;
{
             new TopicPartition(applicationId + &quot;-count-repartition&quot;, 1),
             new TopicPartition(applicationId + &quot;-count-repartition&quot;, 2)
         );
-        assertThat(new HashSet(assignment.get(client).partitions()), equalTo(new HashSet(expectedAssignment)));
+        assertThat(new HashSet&amp;lt;&amp;gt;(assignment.get(client).partitions()), equalTo(new HashSet&amp;lt;&amp;gt;(expectedAssignment)));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdatePartitionHostInfoMapOnAssignment() {&lt;br/&gt;
         final TopicPartition partitionOne = new TopicPartition(&quot;topic&quot;, 1);&lt;br/&gt;
         final TopicPartition partitionTwo = new TopicPartition(&quot;topic&quot;, 2);&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState = Collections.singletonMap(&lt;br/&gt;
@@ -912,7 +987,7 @@ public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdateClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdateClusterMetadataOnAssignment() {&lt;br/&gt;
         final TopicPartition topicOne = new TopicPartition(&quot;topic&quot;, 1);&lt;br/&gt;
         final TopicPartition topicTwo = new TopicPartition(&quot;topic2&quot;, 2);&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState = Collections.singletonMap(&lt;br/&gt;
@@ -928,7 +1003,7 @@ public void shouldUpdateClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {&lt;br/&gt;
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() 
{
         final Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);
         final StreamsConfig config = new StreamsConfig(props);
@@ -976,12 +1051,12 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Except
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = KafkaException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() throws Exception {&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() 
{
         partitionAssignor.configure(Collections.singletonMap(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = KafkaException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() throws Exception {&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() {&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; config = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);&lt;br/&gt;
         config.put(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE, &quot;i am not a stream thread&quot;);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index cfa0e61dc85..52c753d5201 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -65,10 +65,9 @@ public void shouldDecodePreviousVersion() throws Exception 
{
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn&apos;t in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;/li&gt;
	&lt;li&gt;so it is impossible to test compatibility without having this&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
index d1921261bf7..9f59b11cc14 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
@@ -115,7 +115,7 @@ public boolean test(String key, Integer value) {&lt;br/&gt;
             }&lt;br/&gt;
         });&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;data.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        data.process(SmokeTestUtil.&amp;lt;String, Integer&amp;gt;printProcessorSupplier(&quot;data&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // min&lt;br/&gt;
         KGroupedStream&amp;lt;String, Integer&amp;gt;&lt;br/&gt;
@@ -141,7 +141,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {&lt;br/&gt;
         ).to(stringSerde, intSerde, &quot;min&quot;);&lt;/p&gt;

&lt;p&gt;         KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(stringSerde, intSerde, &quot;min&quot;, &quot;minStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;min&quot;));&lt;br/&gt;
+        minTable.toStream().process(SmokeTestUtil.&amp;lt;String, Integer&amp;gt;printProcessorSupplier(&quot;min&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // max&lt;br/&gt;
         groupedData.aggregate(&lt;br/&gt;
@@ -163,7 +163,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {&lt;br/&gt;
         ).to(stringSerde, intSerde, &quot;max&quot;);&lt;/p&gt;

&lt;p&gt;         KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(stringSerde, intSerde, &quot;max&quot;, &quot;maxStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;max&quot;));&lt;br/&gt;
+        maxTable.toStream().process(SmokeTestUtil.&amp;lt;String, Integer&amp;gt;printProcessorSupplier(&quot;max&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // sum&lt;br/&gt;
         groupedData.aggregate(&lt;br/&gt;
@@ -186,7 +186,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {&lt;/p&gt;


&lt;p&gt;         KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(stringSerde, longSerde, &quot;sum&quot;, &quot;sumStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;sum&quot;));&lt;br/&gt;
+        sumTable.toStream().process(SmokeTestUtil.&amp;lt;String, Long&amp;gt;printProcessorSupplier(&quot;sum&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // cnt&lt;br/&gt;
         groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), &quot;uwin-cnt&quot;)&lt;br/&gt;
@@ -195,7 +195,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {&lt;br/&gt;
         ).to(stringSerde, longSerde, &quot;cnt&quot;);&lt;/p&gt;

&lt;p&gt;         KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(stringSerde, longSerde, &quot;cnt&quot;, &quot;cntStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;cnt&quot;));&lt;br/&gt;
+        cntTable.toStream().process(SmokeTestUtil.&amp;lt;String, Long&amp;gt;printProcessorSupplier(&quot;cnt&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // dif&lt;br/&gt;
         maxTable.join(minTable,&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
index c2cfd847ccd..a0c2933e6fe 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
@@ -77,7 +77,6 @@ int next() {&lt;br/&gt;
     // This main() is not used by the system test. It is intended to be used for local debugging.&lt;br/&gt;
     public static void main(String[] args) throws Exception {&lt;br/&gt;
         final String kafka = &quot;localhost:9092&quot;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final String zookeeper = &quot;localhost:2181&quot;;&lt;br/&gt;
         final File stateDir = TestUtils.tempDirectory();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final int numKeys = 20;&lt;br/&gt;
@@ -131,42 +130,50 @@ public void run() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(String kafka, final int numKeys, final int maxRecordsPerKey) throws Exception &lt;/p&gt;
{
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
&lt;p&gt;+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey,&lt;br/&gt;
+                                                     final boolean autoTerminate) throws Exception {&lt;br/&gt;
         final Properties producerProps = new Properties();&lt;br/&gt;
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;br/&gt;
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// the next 4 config values make sure that all records are produced with no loss and&lt;/li&gt;
	&lt;li&gt;// no duplicates&lt;br/&gt;
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates&lt;br/&gt;
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);&lt;br/&gt;
         producerProps.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;br/&gt;
+        final KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         int numRecordsProduced = 0;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         for (int i = 0; i &amp;lt; numKeys; i++) 
{
             data[i] = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data[i].key, new HashSet&amp;lt;Integer&amp;gt;());
         }&lt;/li&gt;
	&lt;li&gt;Random rand = new Random();&lt;br/&gt;
+        final Random rand = new Random();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int remaining = data.length;&lt;br/&gt;
+        int remaining = 1; // dummy value must be positive if &amp;lt;autoTerminate&amp;gt; is false&lt;br/&gt;
+        if (autoTerminate) 
{
+            remaining = data.length;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         while (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int index = rand.nextInt(remaining);&lt;/li&gt;
	&lt;li&gt;String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);&lt;br/&gt;
+            final String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
             int value = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.next();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (value &amp;lt; 0) {&lt;br/&gt;
+            if (autoTerminate &amp;amp;&amp;amp; value &amp;lt; 0) 
{
                 remaining--;
                 data[index] = data[remaining];
             }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;/li&gt;
	&lt;li&gt;new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;br/&gt;
+                final ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;br/&gt;
+                    new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 producer.send(record, new Callback() &lt;/p&gt;
{
                     @Override
@@ -178,11 +185,12 @@ public void onCompletion(final RecordMetadata metadata, final Exception exceptio
                     }
&lt;p&gt;                 });&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
                 numRecordsProduced++;&lt;br/&gt;
                 allData.get(key).add(value);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numRecordsProduced % 100 == 0)&lt;br/&gt;
+&lt;br/&gt;
+                if (numRecordsProduced % 100 == 0) 
{
                     System.out.println(numRecordsProduced + &quot; records produced&quot;);
+                }
&lt;p&gt;                 Utils.sleep(2);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
index 73fe27c4659..87ab60c12ba 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
@@ -33,8 +33,6 @@&lt;/p&gt;

&lt;p&gt; public class SmokeTestUtil {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public final static int WINDOW_SIZE = 100;&lt;/li&gt;
	&lt;li&gt;public final static long START_TIME = 60000L * 60 * 24 * 365 * 30;&lt;br/&gt;
     public final static int END = Integer.MAX_VALUE;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static ProcessorSupplier&amp;lt;Object, Object&amp;gt; printProcessorSupplier(final String topic) {&lt;br/&gt;
@@ -46,18 +44,15 @@&lt;br/&gt;
             public Processor&amp;lt;Object, Object&amp;gt; get() {&lt;br/&gt;
                 return new AbstractProcessor&amp;lt;Object, Object&amp;gt;() {&lt;br/&gt;
                     private int numRecordsProcessed = 0;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private ProcessorContext context;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;br/&gt;
                     public void init(ProcessorContext context) &lt;/p&gt;
{
                         System.out.println(&quot;initializing processor: topic=&quot; + topic + &quot; taskId=&quot; + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }

&lt;p&gt;                     @Override&lt;br/&gt;
                     public void process(Object key, Object value) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (printOffset) System.out.println(&quot;&amp;gt;&amp;gt;&amp;gt; &quot; + context.offset());&lt;br/&gt;
                         numRecordsProcessed++;&lt;br/&gt;
                         if (numRecordsProcessed % 100 == 0) {&lt;br/&gt;
                             System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=&quot; + topic);&lt;br/&gt;
@@ -65,12 +60,10 @@ public void process(Object key, Object value) {&lt;br/&gt;
                     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void punctuate(long timestamp) 
{
-                    }&lt;br/&gt;
+                    public void punctuate(long timestamp) {}&lt;br/&gt;
 &lt;br/&gt;
                     @Override&lt;br/&gt;
-                    public void close() {-                    }
&lt;p&gt;+                    public void close() {}&lt;br/&gt;
                 };&lt;br/&gt;
             }&lt;br/&gt;
         };&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
index 304cae7e0ad..aa1def1dd22 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
@@ -24,7 +24,7 @@&lt;br/&gt;
 public class StreamsSmokeTest {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  args ::= command kafka zookeeper stateDir&lt;br/&gt;
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;command := &quot;run&quot; | &quot;process&quot;&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;@param args&lt;br/&gt;
@@ -33,11 +33,13 @@ public static void main(String[] args) throws Exception {&lt;br/&gt;
         String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         String stateDir = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
         String command = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+        boolean disableAutoTerminate = args.length &amp;gt; 3;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.out.println(&quot;StreamsTest instance started&quot;);&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsSmokeTest)&quot;);&lt;br/&gt;
         System.out.println(&quot;command=&quot; + command);&lt;br/&gt;
         System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
         System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;disableAutoTerminate=&quot; + disableAutoTerminate);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         switch (command) {&lt;br/&gt;
             case &quot;standalone&quot;:&lt;br/&gt;
@@ -47,8 +49,12 @@ public static void main(String[] args) throws Exception {&lt;br/&gt;
                 // this starts the driver (data generation and result verification)&lt;br/&gt;
                 final int numKeys = 10;&lt;br/&gt;
                 final int maxRecordsPerKey = 500;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);&lt;/li&gt;
	&lt;li&gt;SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);&lt;br/&gt;
+                if (disableAutoTerminate) 
{
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
&lt;p&gt;                 break;&lt;br/&gt;
             case &quot;process&quot;:&lt;br/&gt;
                 // this starts a KafkaStreams client&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..17ff97ea083&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,73 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires two argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest trunk)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..7d3ed436881&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                System.out.println(&quot;closing Kafka Streams instance&quot;);+                System.out.flush();+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..604fbe71ad3&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,114 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        String upgradeFrom = args.length &amp;gt; 3 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.1)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index e7be9475f79..b7de568ad7a 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -20,6 +20,7 @@&lt;br/&gt;
 from ducktape.utils.util import wait_until&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1&lt;/p&gt;


&lt;p&gt; class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, &quot;tools-log4j.properties&quot;)&lt;br/&gt;
     PID_FILE = os.path.join(PERSISTENT_ROOT, &quot;streams.pid&quot;)&lt;/p&gt;

&lt;p&gt;+    CLEAN_NODE_ENABLED = True&lt;br/&gt;
+&lt;br/&gt;
     logs = {&lt;br/&gt;
         &quot;streams_log&quot;: {&lt;br/&gt;
             &quot;path&quot;: LOG_FILE,&lt;br/&gt;
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
         &quot;streams_stderr&quot;: &lt;/p&gt;
{
             &quot;path&quot;: STDERR_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None):&lt;br/&gt;
@@ -107,7 +218,8 @@ def wait_node(self, node, timeout_sec=None):&lt;/p&gt;

&lt;p&gt;     def clean_node(self, node):&lt;br/&gt;
         node.account.kill_process(&quot;streams&quot;, clean_shutdown=False, allow_fail=True)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if self.CLEAN_NODE_ENABLED:&lt;br/&gt;
+            node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def start_cmd(self, node):&lt;br/&gt;
         args = self.args.copy()&lt;br/&gt;
@@ -153,7 +265,28 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, command):&lt;br/&gt;
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
         super(StreamsSmokeTestDriverService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, kafka, &quot;run&quot;)&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def disable_auto_terminate(self):&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;disableAutoTerminate&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;disable_auto_terminate&amp;#39;&amp;#93;&lt;/span&gt; = self.DISABLE_AUTO_TERMINATE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;/p&gt;

&lt;p&gt;+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;/p&gt;

&lt;p&gt; class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
@@ -171,3 +304,41 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
                                                                 kafka,&lt;br/&gt;
                                                                 &quot;org.apache.kafka.streams.tests.BrokerCompatibilityTest&quot;,&lt;br/&gt;
                                                                 &quot;dummy&quot;)&lt;br/&gt;
+&lt;br/&gt;
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
+        super(StreamsUpgradeTestJobRunnerService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context,&lt;br/&gt;
+                                                                 kafka,&lt;br/&gt;
+                                                                 &quot;org.apache.kafka.streams.tests.StreamsUpgradeTest&quot;,&lt;br/&gt;
+                                                                 &quot;&quot;)&lt;br/&gt;
+        self.UPGRADE_FROM = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def set_version(self, kafka_streams_version):&lt;br/&gt;
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version&lt;br/&gt;
+&lt;br/&gt;
+    def set_upgrade_from(self, upgrade_from):&lt;br/&gt;
+        self.UPGRADE_FROM = upgrade_from&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
+        else:&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = &quot;&quot;&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;version&amp;#39;&amp;#93;&lt;/span&gt; = self.KAFKA_STREAMS_VERSION&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;upgrade_from&amp;#39;&amp;#93;&lt;/span&gt; = self.UPGRADE_FROM&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s &quot; \&lt;br/&gt;
+              &quot; %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..294e3544f79&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -0,0 +1,242 @@&lt;br/&gt;
+# Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+# contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+# this work for additional information regarding copyright ownership.&lt;br/&gt;
+# The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+# (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+# the License.  You may obtain a copy of the License at&lt;br/&gt;
+#&lt;br/&gt;
+#    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+#&lt;br/&gt;
+# Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+# See the License for the specific language governing permissions and&lt;br/&gt;
+# limitations under the License.&lt;br/&gt;
+&lt;br/&gt;
+from ducktape.mark import parametrize&lt;br/&gt;
+from kafkatest.tests.kafka_test import KafkaTest&lt;br/&gt;
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsUpgradeTestJobRunnerService&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, DEV_VERSION&lt;br/&gt;
+import random&lt;br/&gt;
+&lt;br/&gt;
+class StreamsUpgradeTest(KafkaTest):&lt;br/&gt;
+    &quot;&quot;&quot;&lt;br/&gt;
+    Test upgrading Kafka Streams (all version combination)&lt;br/&gt;
+    If metadata was changes, upgrade is more difficult&lt;br/&gt;
+    Metadata version was bumped in 0.10.1.0&lt;br/&gt;
+    &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
+        super(StreamsUpgradeTest, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, num_zk=1, num_brokers=1, topics={&lt;br/&gt;
+            &apos;echo&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+            &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;+        })&lt;br/&gt;
+&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+&lt;br/&gt;
+    def test_simple_upgrade(self):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version 0.10.1, and upgrades one-by-one to 0.10.2&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(str(LATEST_0_10_1))&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, str(DEV_VERSION), counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released&lt;br/&gt;
+    @parametrize(new_version=str(DEV_VERSION))&lt;br/&gt;
+    def test_metadata_upgrade(self, new_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(str(LATEST_0_10_0))&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # first rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;0.10.0&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # second rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    def start_all_nodes_with(self, version):&lt;br/&gt;
+        # start first with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor1, version)&lt;br/&gt;
+        node1 = self.processor1.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:&lt;br/&gt;
+                self.processor1.start()&lt;br/&gt;
+                log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                       timeout_sec=60,&lt;br/&gt;
+                                       err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node1.account))&lt;br/&gt;
+                monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start second with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor2, version)&lt;br/&gt;
+        node2 = self.processor2.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:&lt;br/&gt;
+                    self.processor2.start()&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node2.account))&lt;br/&gt;
+                    first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                             timeout_sec=60,&lt;br/&gt;
+                                             err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                    second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                              timeout_sec=60,&lt;br/&gt;
+                                              err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start third with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor3, version)&lt;br/&gt;
+        node3 = self.processor3.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:&lt;br/&gt;
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:&lt;br/&gt;
+                        self.processor3.start()&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node3.account))&lt;br/&gt;
+                        first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                 timeout_sec=60,&lt;br/&gt;
+                                                 err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                        second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+                        third_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node3.account))&lt;br/&gt;
+&lt;br/&gt;
+    @staticmethod&lt;br/&gt;
+    def prepare_for(processor, version):&lt;br/&gt;
+        processor.node.account.ssh(&quot;rm -rf &quot; + processor.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        processor.set_version(version)&lt;br/&gt;
+&lt;br/&gt;
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
+        first_other_processor = None&lt;br/&gt;
+        second_other_processor = None&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            if p != processor:&lt;br/&gt;
+                if first_other_processor is None:&lt;br/&gt;
+                    first_other_processor = p&lt;br/&gt;
+                else:&lt;br/&gt;
+                    second_other_processor = p&lt;br/&gt;
+&lt;br/&gt;
+        node = processor.node&lt;br/&gt;
+        first_other_node = first_other_processor.node&lt;br/&gt;
+        second_other_node = second_other_processor.node&lt;br/&gt;
+&lt;br/&gt;
+        # stop processor and wait for rebalance of others&lt;br/&gt;
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                processor.stop()&lt;br/&gt;
+                first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                timeout_sec=60,&lt;br/&gt;
+                                                err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+        node.account.ssh_capture(&quot;grep UPGRADE-TEST-CLIENT-CLOSED %s&quot; % processor.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if upgrade_from == &quot;&quot;:  # upgrade disabled &amp;#8211; second round of rolling bounces&lt;br/&gt;
+            roll_counter = &quot;.1-&quot;  # second round of rolling bounces&lt;br/&gt;
+        else:&lt;br/&gt;
+            roll_counter = &quot;.0-&quot;  # first  round of rolling boundes&lt;br/&gt;
+&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDOUT_FILE + &quot; &quot; + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDERR_FILE + &quot; &quot; + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.LOG_FILE + &quot; &quot; + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if new_version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(new_version)&lt;br/&gt;
+        processor.set_upgrade_from(upgrade_from)&lt;br/&gt;
+&lt;br/&gt;
+        grep_metadata_error = &quot;grep \&quot;org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\&quot; &quot;&lt;br/&gt;
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:&lt;br/&gt;
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                        processor.start()&lt;br/&gt;
+&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + new_version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + new_version + &quot; &quot; + str(node.account))&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                       timeout_sec=60,&lt;br/&gt;
+                                                       err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                        timeout_sec=60,&lt;br/&gt;
+                                                        err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node.account))&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index 7cd489d87ac..df956027509 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -61,6 +61,7 @@ def get_version(node=None):&lt;br/&gt;
         return DEV_BRANCH&lt;/p&gt;

&lt;p&gt; DEV_BRANCH = KafkaVersion(&quot;dev&quot;)&lt;br/&gt;
+DEV_VERSION = KafkaVersion(&quot;0.10.2.2-SNAPSHOT&quot;)&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;0.8.2.X versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;br/&gt;
diff --git a/vagrant/base.sh b/vagrant/base.sh&lt;br/&gt;
index 0bb0f30054b..70987c6c135 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/vagrant/base.sh&lt;br/&gt;
+++ b/vagrant/base.sh&lt;br/&gt;
@@ -52,6 +52,8 @@ get_kafka() {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;     kafka_dir=/opt/kafka-$version&lt;br/&gt;
     url=&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages-$version/kafka_2.10-$version.tgz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3-us-west-2.amazonaws.com/kafka-packages-$version/kafka_2.10-$version.tgz&lt;/a&gt;&lt;br/&gt;
+    # the .tgz above does not include the streams test jar hence we need to get it separately&lt;br/&gt;
+    url_streams_test=&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages/kafka-streams-$version-test.jar&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3-us-west-2.amazonaws.com/kafka-packages/kafka-streams-$version-test.jar&lt;/a&gt;&lt;br/&gt;
     if [ ! &lt;del&gt;d /opt/kafka&lt;/del&gt;$version ]; then&lt;br/&gt;
         pushd /tmp&lt;br/&gt;
         curl -O $url&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16414965" author="githubbot" created="Tue, 27 Mar 2018 03:32:23 +0000"  >&lt;p&gt;mjsax closed pull request #4761:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4761&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4761&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh&lt;br/&gt;
index fe6aefd7321..8e2ba91bf2b 100755&lt;br/&gt;
&amp;#8212; a/bin/kafka-run-class.sh&lt;br/&gt;
+++ b/bin/kafka-run-class.sh&lt;br/&gt;
@@ -73,28 +73,50 @@ do&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/clients/build/libs/kafka-clients*.jar;&lt;br/&gt;
-do&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if should_include_file &quot;$file&quot;; then&lt;/li&gt;
	&lt;li&gt;CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;/li&gt;
	&lt;li&gt;fi&lt;br/&gt;
-done&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  clients_lib_dir=$(dirname $0)/../clients/build/libs&lt;br/&gt;
+  streams_lib_dir=$(dirname $0)/../streams/build/libs&lt;br/&gt;
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}&lt;br/&gt;
+else&lt;br/&gt;
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs&lt;br/&gt;
+  streams_lib_dir=$clients_lib_dir&lt;br/&gt;
+  rocksdb_lib_dir=$streams_lib_dir&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/build/libs/kafka-streams*.jar;&lt;br/&gt;
+for file in &quot;$clients_lib_dir&quot;/kafka-clients*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+for file in &quot;$streams_lib_dir&quot;/kafka-streams*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;&lt;del&gt;for file in &quot;$base_dir&quot;/streams/build/dependant-libs&lt;/del&gt;${SCALA_VERSION}/rocksdb*.jar;&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+else&lt;br/&gt;
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed &apos;s/\.//g&apos;`&lt;br/&gt;
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;(${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;br/&gt;
+for file in &quot;$rocksdb_lib_dir&quot;/rocksdb*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
 done&lt;br/&gt;
diff --git a/build.gradle b/build.gradle&lt;br/&gt;
index ce4b4e44cb2..17f3e00358d 100644&lt;br/&gt;
&amp;#8212; a/build.gradle&lt;br/&gt;
+++ b/build.gradle&lt;br/&gt;
@@ -909,6 +909,42 @@ project(&apos;:streams:examples&apos;) {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;+project(&apos;:streams:upgrade-system-tests-0100&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0100&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0100
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0101&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0101&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0102&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0102&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0102
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
 project(&apos;:jmh-benchmarks&apos;) {&lt;/p&gt;

&lt;p&gt;   apply plugin: &apos;com.github.johnrengelman.shadow&apos;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
index 7111bad6054..7102414628a 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
@@ -16,7 +16,9 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.security.authenticator;&lt;/p&gt;

&lt;p&gt;-import java.util.Map;&lt;br/&gt;
+import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
+import org.apache.kafka.common.network.Mode;&lt;br/&gt;
+import org.apache.kafka.common.security.auth.AuthCallbackHandler;&lt;/p&gt;

&lt;p&gt; import javax.security.auth.Subject;&lt;br/&gt;
 import javax.security.auth.callback.Callback;&lt;br/&gt;
@@ -25,10 +27,7 @@&lt;br/&gt;
 import javax.security.auth.callback.UnsupportedCallbackException;&lt;br/&gt;
 import javax.security.sasl.AuthorizeCallback;&lt;br/&gt;
 import javax.security.sasl.RealmCallback;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
-import org.apache.kafka.common.network.Mode;&lt;br/&gt;
-import org.apache.kafka.common.security.auth.AuthCallbackHandler;&lt;br/&gt;
+import java.util.Map;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Callback handler for Sasl clients. The callbacks required for the SASL mechanism&lt;br/&gt;
diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html&lt;br/&gt;
index 7f2c9f6cf89..86d6d531a9c 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/docs/streams/upgrade-guide.html&lt;br/&gt;
+++ b/docs/streams/upgrade-guide.html&lt;br/&gt;
@@ -27,16 +27,33 @@ &amp;lt;h1&amp;gt;Upgrade Guide &amp;amp; API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.1.x to 0.10.2, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;br/&gt;
+        If you want to upgrade from 0.10.1.x to 0.11.0, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;br/&gt;
         It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.2 API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;0.10.2&amp;lt;/a&amp;gt; and &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;0.11.0&amp;lt;/a&amp;gt; API and semantical changes&lt;br/&gt;
+        that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.0.x to 0.10.1, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1010_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.1&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;/li&gt;
	&lt;li&gt;It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        Upgrading from 0.10.0.x to 0.11.0.x directly is also possible.&lt;br/&gt;
+        Note, that a brokers must be on version 0.10.1 or higher to run a Kafka Streams application version 0.10.1 or higher.&lt;br/&gt;
+        See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        and &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; for a complete list of API changes.&lt;br/&gt;
+        Upgrading to 0.11.0.3 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.11.0.3 &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;p&amp;gt; Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported) &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; restart all new (0.11.0.0, 0.11.0.1, or 0.11.0.2) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_0110&quot; href=&quot;#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;/p&gt;

&lt;p&gt;diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index 9f0dbdf55fb..06038753189 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -64,6 +64,12 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0&lt;br/&gt;
     before you switch to 0.11.0.&amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ol&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1103_notable&quot; href=&quot;#upgrade_1103_notable&quot;&amp;gt;Notable changes in 0.11.0.3&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_notable&quot; href=&quot;#upgrade_1100_notable&quot;&amp;gt;Notable changes in 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to&lt;br/&gt;
@@ -214,14 +220,41 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams&quot; href=&quot;#upgrade_1020_streams&quot;&amp;gt;Upgrading a 0.10.1&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams_from_0100&quot; href=&quot;#upgrade_1020_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.2 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.2.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.2.0 or 0.10.2.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10202_notable&quot; href=&quot;#upgrade_10202_notable&quot;&amp;gt;Notable changes in 0.10.2.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10201_notable&quot; href=&quot;#upgrade_10201_notable&quot;&amp;gt;Notable changes in 0.10.2.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
   &amp;lt;li&amp;gt; The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer &amp;lt;code&amp;gt;retries&amp;lt;/code&amp;gt; default value was changed from 0 to 10. The internal Kafka Streams consumer &amp;lt;code&amp;gt;max.poll.interval.ms&amp;lt;/code&amp;gt;  default value was changed from 300000 to &amp;lt;code&amp;gt;Integer.MAX_VALUE&amp;lt;/code&amp;gt;.&lt;br/&gt;
   &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_notable&quot; href=&quot;#upgrade_1020_notable&quot;&amp;gt;Notable changes in 0.10.2.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;The Java clients (producer and consumer) have acquired the ability to communicate with older brokers. Version 0.10.2 clients&lt;br/&gt;
@@ -294,6 +327,23 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_streams&quot; href=&quot;#upgrade_1010_streams&quot;&amp;gt;Upgrading a 0.10.0&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.1 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.1.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.1.0 or 0.10.1.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_notable&quot; href=&quot;#upgrade_1010_notable&quot;&amp;gt;Notable changes in 0.10.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle&lt;br/&gt;
index 5d145e17ed9..d881353703f 100644&lt;br/&gt;
&amp;#8212; a/gradle/dependencies.gradle&lt;br/&gt;
+++ b/gradle/dependencies.gradle&lt;br/&gt;
@@ -55,6 +55,9 @@ versions += [&lt;br/&gt;
   jackson: &quot;2.8.5&quot;,&lt;br/&gt;
   jetty: &quot;9.2.22.v20170606&quot;,&lt;br/&gt;
   jersey: &quot;2.24&quot;,&lt;br/&gt;
+  kafka_0100: &quot;0.10.0.1&quot;,&lt;br/&gt;
+  kafka_0101: &quot;0.10.1.1&quot;,&lt;br/&gt;
+  kafka_0102: &quot;0.10.2.1&quot;,&lt;br/&gt;
   log4j: &quot;1.2.17&quot;,&lt;br/&gt;
   jopt: &quot;5.0.3&quot;,&lt;br/&gt;
   junit: &quot;4.12&quot;,&lt;br/&gt;
@@ -96,6 +99,9 @@ libs += [&lt;br/&gt;
   junit: &quot;junit:junit:$versions.junit&quot;,&lt;br/&gt;
   log4j: &quot;log4j:log4j:$versions.log4j&quot;,&lt;br/&gt;
   joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
+  kafkaStreams_0100: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0100&quot;,&lt;br/&gt;
+  kafkaStreams_0101: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0101&quot;,&lt;br/&gt;
+  kafkaStreams_0102: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0102&quot;,&lt;br/&gt;
   lz4: &quot;net.jpountz.lz4:lz4:$versions.lz4&quot;,&lt;br/&gt;
   metrics: &quot;com.yammer.metrics:metrics-core:$versions.metrics&quot;,&lt;br/&gt;
   powermock: &quot;org.powermock:powermock-module-junit4:$versions.powermock&quot;,&lt;br/&gt;
diff --git a/settings.gradle b/settings.gradle&lt;br/&gt;
index f0fdf07128c..769046fe556 100644&lt;br/&gt;
&amp;#8212; a/settings.gradle&lt;br/&gt;
+++ b/settings.gradle&lt;br/&gt;
@@ -13,5 +13,6 @@&lt;br/&gt;
 // See the License for the specific language governing permissions and&lt;br/&gt;
 // limitations under the License.&lt;/p&gt;

&lt;p&gt;-include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
+include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;streams:upgrade-system-tests-0100&apos;,&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0101&apos;, &apos;streams:upgrade-system-tests-0102&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
         &apos;connect:api&apos;, &apos;connect:transforms&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;, &apos;jmh-benchmarks&apos;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index b411344aaac..d45b1357f0e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -105,6 +105,11 @@&lt;br/&gt;
      */&lt;br/&gt;
     public static final String PRODUCER_PREFIX = &quot;producer.&quot;;&lt;/p&gt;

&lt;p&gt;+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.0.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #PROCESSING_GUARANTEE_CONFIG &quot;processing.guarantee&quot;}
&lt;p&gt; for at-least-once processing guarantees.&lt;br/&gt;
      */&lt;br/&gt;
@@ -247,6 +252,11 @@&lt;br/&gt;
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = &quot;timestamp.extractor&quot;;&lt;br/&gt;
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = &quot;Timestamp extractor class that implements the &amp;lt;code&amp;gt;TimestampExtractor&amp;lt;/code&amp;gt; interface. This config is deprecated, use &amp;lt;code&amp;gt;&quot; + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + &quot;&amp;lt;/code&amp;gt; instead&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
+    public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;br/&gt;
+    public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+        &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot; (for upgrading from 0.10.0.x).&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code value.serde}&lt;/li&gt;
	&lt;li&gt;@deprecated Use 
{@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG}
&lt;p&gt; instead.&lt;br/&gt;
@@ -466,6 +476,12 @@&lt;br/&gt;
                     null,&lt;br/&gt;
                     Importance.LOW,&lt;br/&gt;
                     TIMESTAMP_EXTRACTOR_CLASS_DOC)&lt;br/&gt;
+            .define(UPGRADE_FROM_CONFIG,&lt;br/&gt;
+                    ConfigDef.Type.STRING,&lt;br/&gt;
+                    null,&lt;br/&gt;
+                    in(null, UPGRADE_FROM_0100),&lt;br/&gt;
+                    Importance.LOW,&lt;br/&gt;
+                    UPGRADE_FROM_DOC)&lt;br/&gt;
             .define(VALUE_SERDE_CLASS_CONFIG,&lt;br/&gt;
                     Type.CLASS,&lt;br/&gt;
                     null,&lt;br/&gt;
@@ -632,6 +648,7 @@ public StreamsConfig(final Map&amp;lt;?, ?&amp;gt; props) {&lt;br/&gt;
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + &quot;-consumer&quot;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add configs required for stream partition assignor&lt;br/&gt;
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));&lt;br/&gt;
         consumerProps.put(InternalConfig.STREAM_THREAD_INSTANCE, streamThread);&lt;br/&gt;
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));&lt;br/&gt;
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
index 0a1b2ab76cb..6e2bfa67bac 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
@@ -165,6 +165,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {&lt;br/&gt;
     private String userEndPoint;&lt;br/&gt;
     private int numStandbyReplicas;&lt;/p&gt;

&lt;p&gt;+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
+&lt;br/&gt;
     private Cluster metadataWithInternalTopics;&lt;br/&gt;
     private Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState;&lt;/p&gt;

&lt;p&gt;@@ -192,6 +194,12 @@ void time(final Time time) {&lt;br/&gt;
     public void configure(Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         numStandbyReplicas = (Integer) configs.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);&lt;/p&gt;

&lt;p&gt;+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;br/&gt;
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) &lt;/p&gt;
{
+            log.info(&quot;Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x.&quot;);
+            userMetadataVersion = 1;
+        }
&lt;p&gt;+&lt;br/&gt;
         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);&lt;br/&gt;
         if (o == null) {&lt;br/&gt;
             KafkaException ex = new KafkaException(&quot;StreamThread is not specified&quot;);&lt;br/&gt;
@@ -251,7 +259,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = streamThread.prevActiveTasks();&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; standbyTasks = streamThread.cachedTasks();&lt;br/&gt;
         standbyTasks.removeAll(previousActiveTasks);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;SubscriptionInfo data = new SubscriptionInfo(streamThread.processId, previousActiveTasks, standbyTasks, this.userEndPoint);&lt;br/&gt;
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, streamThread.processId, previousActiveTasks, standbyTasks, this.userEndPoint);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (streamThread.builder.sourceTopicPattern() != null &amp;amp;&amp;amp;&lt;br/&gt;
             !streamThread.builder.subscriptionUpdates().getUpdates().equals(topics)) {&lt;br/&gt;
@@ -295,11 +303,16 @@ private void updateSubscribedTopics(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // construct the client metadata from the decoded subscription info&lt;br/&gt;
         Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;br/&gt;
             String consumerId = entry.getKey();&lt;br/&gt;
             Subscription subscription = entry.getValue();&lt;/p&gt;

&lt;p&gt;             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+            final int usedVersion = info.version;&lt;br/&gt;
+            if (usedVersion &amp;lt; minUserMetadataVersion) &lt;/p&gt;
{
+                minUserMetadataVersion = usedVersion;
+            }

&lt;p&gt;             // create the new client metadata if necessary&lt;br/&gt;
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);&lt;br/&gt;
@@ -556,7 +569,7 @@ private void updateSubscribedTopics(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                 }&lt;/p&gt;

&lt;p&gt;                 // finally, encode the assignment before sending back to coordinator&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));&lt;br/&gt;
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));&lt;br/&gt;
                 i++;&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index 77fb58a113c..5409976d686 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -55,7 +55,7 @@ public AssignmentInfo(List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;&lt;br/&gt;
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+    public AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
                              Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.activeTasks = activeTasks;&lt;br/&gt;
@@ -154,9 +154,7 @@ public static AssignmentInfo decode(ByteBuffer data) {&lt;br/&gt;
                 }&lt;br/&gt;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
         } catch (IOException ex) 
{
             throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);
         }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index f583dbafc94..00227e799b8 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -31,7 +31,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int CURRENT_VERSION = 2;&lt;br/&gt;
+    public static final int CURRENT_VERSION = 2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public final int version;&lt;br/&gt;
     public final UUID processId;&lt;br/&gt;
@@ -43,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; stand&lt;br/&gt;
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
+    public SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.processId = processId;&lt;br/&gt;
         this.prevTasks = prevTasks;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
index 3bbd69ea4d0..9998283928c 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
@@ -82,7 +82,7 @@ public void shouldThrowExceptionIfBootstrapServersIsNotSet() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetProducerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetProducerConfigs() {&lt;br/&gt;
         final String clientId = &quot;client&quot;;&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getProducerConfigs(clientId);&lt;br/&gt;
         assertEquals(returnedProps.get(ProducerConfig.CLIENT_ID_CONFIG), clientId + &quot;-producer&quot;);&lt;br/&gt;
@@ -91,7 +91,7 @@ public void testGetProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetConsumerConfigs() {&lt;br/&gt;
         final String groupId = &quot;example-application&quot;;&lt;br/&gt;
         final String clientId = &quot;client&quot;;&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getConsumerConfigs(null, groupId, clientId);&lt;br/&gt;
@@ -102,7 +102,7 @@ public void testGetConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetRestoreConsumerConfigs() {&lt;br/&gt;
         final String clientId = &quot;client&quot;;&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getRestoreConsumerConfigs(clientId);&lt;br/&gt;
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), clientId + &quot;-restore-consumer&quot;);&lt;br/&gt;
@@ -143,7 +143,7 @@ public void shouldSupportMultipleBootstrapServers() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedConsumerConfigs() {&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;earliest&quot;);&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -153,7 +153,7 @@ public void shouldSupportPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedRestoreConsumerConfigs() {&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;earliest&quot;);&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -163,7 +163,7 @@ public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() 
{
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = streamsConfig.getConsumerConfigs(null, &quot;groupId&quot;, &quot;clientId&quot;);
@@ -171,7 +171,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() thro
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() 
{
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = streamsConfig.getRestoreConsumerConfigs(&quot;clientId&quot;);
@@ -179,7 +179,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() {&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         props.put(producerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; producerConfigs = streamsConfig.getProducerConfigs(&quot;clientId&quot;);&lt;br/&gt;
@@ -188,7 +188,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() thro&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedProducerConfigs() {&lt;br/&gt;
         props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);&lt;br/&gt;
         props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -198,7 +198,7 @@ public void shouldSupportPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldBeSupportNonPrefixedConsumerConfigs() {&lt;br/&gt;
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;br/&gt;
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -208,7 +208,7 @@ public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() 
{
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -218,7 +218,7 @@ public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportNonPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportNonPrefixedProducerConfigs() {&lt;br/&gt;
         props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10);&lt;br/&gt;
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -227,24 +227,22 @@ public void shouldSupportNonPrefixedProducerConfigs() throws Exception 
{
         assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
     @Test(expected = StreamsException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() throws Exception {&lt;br/&gt;
+    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() 
{
         props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.defaultKeySerde();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = StreamsException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() throws Exception {&lt;br/&gt;
+    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() 
{
         props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.defaultValueSerde();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultConsumerConfigs() {&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;latest&quot;);&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), &quot;10&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -254,7 +252,7 @@ public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultProducerConfigs() {&lt;br/&gt;
         props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), &quot;10000&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; producerConfigs = streamsConfig.getProducerConfigs(&quot;clientId&quot;);&lt;br/&gt;
@@ -262,7 +260,7 @@ public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() 
{
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;latest&quot;);
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), &quot;10&quot;);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -272,21 +270,21 @@ public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = ConfigException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() 
{
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), &quot;true&quot;);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getConsumerConfigs(null, &quot;a&quot;, &quot;b&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = ConfigException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() 
{
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), &quot;true&quot;);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getRestoreConsumerConfigs(&quot;client&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSetInternalLeaveGroupOnCloseConfigToFalseInConsumer() throws Exception {&lt;br/&gt;
+    public void shouldSetInternalLeaveGroupOnCloseConfigToFalseInConsumer() {&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = streamsConfig.getConsumerConfigs(null, &quot;groupId&quot;, &quot;clientId&quot;);&lt;br/&gt;
         assertThat(consumerConfigs.get(&quot;internal.leave.group.on.close&quot;), CoreMatchers.&amp;lt;Object&amp;gt;equalTo(false));&lt;br/&gt;
@@ -395,6 +393,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() 
{
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -429,6 +428,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() &lt;/p&gt;
{
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -442,6 +442,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -455,6 +456,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -468,6 +470,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index 372b89c048e..bb4b5758cfb 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -16,6 +16,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.integration;&lt;/p&gt;

&lt;p&gt;+import kafka.utils.MockTime;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Deserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.IntegerSerializer;&lt;br/&gt;
@@ -51,7 +52,6 @@&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.concurrent.ExecutionException;&lt;/p&gt;

&lt;p&gt;-import kafka.utils.MockTime;&lt;br/&gt;
 import org.junit.experimental.categories.Category;&lt;/p&gt;

&lt;p&gt; import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
@@ -315,6 +315,4 @@ private void startStreams() {&lt;/p&gt;

&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
index 98cd20a3527..a29380fecd0 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
@@ -135,7 +135,7 @@ public void setUp() {&lt;/p&gt;

&lt;p&gt;     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testSubscription() throws Exception {&lt;br/&gt;
+    public void testSubscription() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -187,7 +187,7 @@ public void testSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignBasic() throws Exception {&lt;br/&gt;
+    public void testAssignBasic() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -239,12 +239,10 @@ public void testAssignBasic() throws Exception {&lt;br/&gt;
         assertEquals(Utils.mkSet(t1p2, t2p2), new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer20&quot;).partitions()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the first consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -264,7 +262,7 @@ public void testAssignBasic() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
+    public void testAssignWithPartialTopology() {&lt;br/&gt;
         Properties props = configProps();&lt;br/&gt;
         props.put(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, SingleGroupPartitionGrouperStub.class);&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(props);&lt;br/&gt;
@@ -306,9 +304,8 @@ public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(&quot;topic1&quot;), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -316,7 +313,7 @@ public void testAssignWithPartialTopology() throws Exception {&lt;/p&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
+    public void testAssignEmptyMetadata() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -359,9 +356,8 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
             new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Collections.&amp;lt;String&amp;gt;emptySet(), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, allActiveTasks.size());&lt;br/&gt;
         assertEquals(Collections.&amp;lt;TaskId&amp;gt;emptySet(), new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -384,7 +380,7 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
+    public void testAssignWithNewTasks() {&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(&quot;source2&quot;, &quot;topic2&quot;);&lt;br/&gt;
         builder.addSource(&quot;source3&quot;, &quot;topic3&quot;);&lt;br/&gt;
@@ -430,13 +426,9 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match&lt;br/&gt;
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and&lt;br/&gt;
         // then later ones will be re-assigned to other hosts due to load balancing&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;AssignmentInfo info;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info.activeTasks);&lt;/li&gt;
	&lt;li&gt;allPartitions.addAll(assignments.get(&quot;consumer10&quot;).partitions());&lt;br/&gt;
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());&lt;br/&gt;
         allActiveTasks.addAll(info.activeTasks);&lt;br/&gt;
@@ -451,7 +443,7 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStates() throws Exception {&lt;br/&gt;
+    public void testAssignWithStates() {&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);&lt;br/&gt;
@@ -551,7 +543,7 @@ public void testAssignWithStates() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
+    public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         Properties props = configProps();&lt;br/&gt;
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(props);&lt;br/&gt;
@@ -600,13 +592,10 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;/li&gt;
	&lt;li&gt;allStandbyTasks.addAll(info10.standbyTasks.keySet());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;(info10.standbyTasks.keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -634,7 +623,7 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testOnAssignment() throws Exception {&lt;br/&gt;
+    public void testOnAssignment() {&lt;br/&gt;
         TopicPartition t2p3 = new TopicPartition(&quot;topic2&quot;, 3);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         TopologyBuilder builder = new TopologyBuilder();&lt;br/&gt;
@@ -677,7 +666,7 @@ public void testOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopics() {&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addInternalTopic(&quot;topicX&quot;);&lt;br/&gt;
@@ -722,7 +711,7 @@ public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() 
{
         String applicationId = &quot;test&quot;;
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(&quot;topicX&quot;);
@@ -760,7 +749,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
+    public void shouldAddUserDefinedEndPointToSubscription() {&lt;br/&gt;
         final Properties properties = configProps();&lt;br/&gt;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, &quot;localhost:8080&quot;);&lt;br/&gt;
         final StreamsConfig config = new StreamsConfig(properties);&lt;br/&gt;
@@ -773,8 +762,8 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
         final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
         final String client1 = &quot;client1&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1, uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),&lt;/li&gt;
	&lt;li&gt;0, stateDirectory);&lt;br/&gt;
+        final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1,&lt;br/&gt;
+            uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST), 0, stateDirectory);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         partitionAssignor.configure(config.getConsumerConfigs(streamThread, applicationId, client1));&lt;br/&gt;
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;input&quot;));&lt;br/&gt;
@@ -783,7 +772,82 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() 
{
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            &quot;consumer1&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            &quot;consumer2&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        StreamsConfig config = new StreamsConfig(configProps());
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            &quot;appId&quot;,
+            &quot;clientId&quot;,
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0,
+            stateDirectory);
+
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, &quot;test&quot;, &quot;clientId&quot;));
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscription() &lt;/p&gt;
{
+        final Properties properties = configProps();
+        properties.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        StreamsConfig config = new StreamsConfig(properties);
+
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);
+
+        String clientId = &quot;client-id&quot;;
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            &quot;appId&quot;,
+            &quot;clientId&quot;,
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0,
+            stateDirectory);
+
+        StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, &quot;test&quot;, clientId));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldMapUserEndPointToTopicPartitions() {&lt;br/&gt;
         final Properties properties = configProps();&lt;br/&gt;
         final String myEndPoint = &quot;localhost:8080&quot;;&lt;br/&gt;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);&lt;br/&gt;
@@ -831,7 +895,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() 
{
         final Properties properties = configProps();
         final String myEndPoint = &quot;localhost&quot;;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -865,7 +929,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() thr
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() 
{
         final Properties properties = configProps();
         final String myEndPoint = &quot;localhost:j87yhk&quot;;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -897,7 +961,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() th
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() 
{
         List&amp;lt;TopicPartition&amp;gt; topic = Collections.singletonList(new TopicPartition(&quot;topic&quot;, 0));
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState =
                 Collections.singletonMap(new HostInfo(&quot;localhost&quot;, 80),
@@ -910,7 +974,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSetClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldSetClusterMetadataOnAssignment() {&lt;br/&gt;
         final List&amp;lt;TopicPartition&amp;gt; topic = Collections.singletonList(new TopicPartition(&quot;topic&quot;, 0));&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState =&lt;br/&gt;
                 Collections.singletonMap(new HostInfo(&quot;localhost&quot;, 80),&lt;br/&gt;
@@ -930,7 +994,7 @@ public void shouldSetClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() throws Exception {&lt;br/&gt;
+    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() 
{
         final Cluster cluster = partitionAssignor.clusterMetadata();
         assertNotNull(cluster);
     }
&lt;p&gt;@@ -1039,11 +1103,11 @@ public Object apply(Object value1, Object value2) &lt;/p&gt;
{
             new TopicPartition(applicationId + &quot;-count-repartition&quot;, 1),
             new TopicPartition(applicationId + &quot;-count-repartition&quot;, 2)
         );
-        assertThat(new HashSet(assignment.get(client).partitions()), equalTo(new HashSet(expectedAssignment)));
+        assertThat(new HashSet&amp;lt;&amp;gt;(assignment.get(client).partitions()), equalTo(new HashSet&amp;lt;&amp;gt;(expectedAssignment)));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdatePartitionHostInfoMapOnAssignment() {&lt;br/&gt;
         final TopicPartition partitionOne = new TopicPartition(&quot;topic&quot;, 1);&lt;br/&gt;
         final TopicPartition partitionTwo = new TopicPartition(&quot;topic&quot;, 2);&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState = Collections.singletonMap(&lt;br/&gt;
@@ -1060,7 +1124,7 @@ public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdateClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdateClusterMetadataOnAssignment() {&lt;br/&gt;
         final TopicPartition topicOne = new TopicPartition(&quot;topic&quot;, 1);&lt;br/&gt;
         final TopicPartition topicTwo = new TopicPartition(&quot;topic2&quot;, 2);&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState = Collections.singletonMap(&lt;br/&gt;
@@ -1076,7 +1140,7 @@ public void shouldUpdateClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {&lt;br/&gt;
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() 
{
         final Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);
         final StreamsConfig config = new StreamsConfig(props);
@@ -1135,12 +1199,12 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Except
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = KafkaException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() throws Exception {&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() 
{
         partitionAssignor.configure(Collections.singletonMap(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = KafkaException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() throws Exception {&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() {&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; config = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);&lt;br/&gt;
         config.put(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE, &quot;i am not a stream thread&quot;);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index 9473a4027c4..361dde87776 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -64,10 +64,9 @@ public void shouldDecodePreviousVersion() throws Exception 
{
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn&apos;t in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;/li&gt;
	&lt;li&gt;so it is impossible to test compatibility without having this&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
index 11e1ae86fc2..303061541f3 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
@@ -77,7 +77,6 @@ int next() {&lt;br/&gt;
     // This main() is not used by the system test. It is intended to be used for local debugging.&lt;br/&gt;
     public static void main(String[] args) throws Exception {&lt;br/&gt;
         final String kafka = &quot;localhost:9092&quot;;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final String zookeeper = &quot;localhost:2181&quot;;&lt;br/&gt;
         final File stateDir = TestUtils.tempDirectory();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final int numKeys = 20;&lt;br/&gt;
@@ -131,42 +130,50 @@ public void run() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(String kafka, final int numKeys, final int maxRecordsPerKey) throws Exception &lt;/p&gt;
{
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
&lt;p&gt;+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey,&lt;br/&gt;
+                                                     final boolean autoTerminate) throws Exception {&lt;br/&gt;
         final Properties producerProps = new Properties();&lt;br/&gt;
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;br/&gt;
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// the next 4 config values make sure that all records are produced with no loss and&lt;/li&gt;
	&lt;li&gt;// no duplicates&lt;br/&gt;
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates&lt;br/&gt;
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);&lt;br/&gt;
         producerProps.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;br/&gt;
+        final KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         int numRecordsProduced = 0;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         for (int i = 0; i &amp;lt; numKeys; i++) 
{
             data[i] = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data[i].key, new HashSet&amp;lt;Integer&amp;gt;());
         }&lt;/li&gt;
	&lt;li&gt;Random rand = new Random();&lt;br/&gt;
+        final Random rand = new Random();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int remaining = data.length;&lt;br/&gt;
+        int remaining = 1; // dummy value must be positive if &amp;lt;autoTerminate&amp;gt; is false&lt;br/&gt;
+        if (autoTerminate) 
{
+            remaining = data.length;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         while (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int index = rand.nextInt(remaining);&lt;/li&gt;
	&lt;li&gt;String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);&lt;br/&gt;
+            final String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
             int value = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.next();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (value &amp;lt; 0) {&lt;br/&gt;
+            if (autoTerminate &amp;amp;&amp;amp; value &amp;lt; 0) 
{
                 remaining--;
                 data[index] = data[remaining];
             }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;/li&gt;
	&lt;li&gt;new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;br/&gt;
+                final ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;br/&gt;
+                    new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 producer.send(record, new Callback() &lt;/p&gt;
{
                     @Override
@@ -178,11 +185,12 @@ public void onCompletion(final RecordMetadata metadata, final Exception exceptio
                     }
&lt;p&gt;                 });&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
                 numRecordsProduced++;&lt;br/&gt;
                 allData.get(key).add(value);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numRecordsProduced % 100 == 0)&lt;br/&gt;
+&lt;br/&gt;
+                if (numRecordsProduced % 100 == 0) 
{
                     System.out.println(numRecordsProduced + &quot; records produced&quot;);
+                }
&lt;p&gt;                 Utils.sleep(2);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
index 150ec7d0c26..11845b4024c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
@@ -44,20 +44,15 @@&lt;br/&gt;
             public Processor&amp;lt;Object, Object&amp;gt; get() {&lt;br/&gt;
                 return new AbstractProcessor&amp;lt;Object, Object&amp;gt;() {&lt;br/&gt;
                     private int numRecordsProcessed = 0;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private ProcessorContext context;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;br/&gt;
                     public void init(final ProcessorContext context) &lt;/p&gt;
{
                         System.out.println(&quot;initializing processor: topic=&quot; + topic + &quot; taskId=&quot; + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }

&lt;p&gt;                     @Override&lt;br/&gt;
                     public void process(final Object key, final Object value) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (printOffset) 
{
-                            System.out.println(&quot;&amp;gt;&amp;gt;&amp;gt; &quot; + context.offset());
-                        }
&lt;p&gt;                         numRecordsProcessed++;&lt;br/&gt;
                         if (numRecordsProcessed % 100 == 0) {&lt;br/&gt;
                             System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=&quot; + topic);&lt;br/&gt;
@@ -65,10 +60,10 @@ public void process(final Object key, final Object value) {&lt;br/&gt;
                     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void punctuate(final long timestamp) { }&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void close() { }&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
                 };&lt;br/&gt;
             }&lt;br/&gt;
         };&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
index 244aa8eef6e..699aaeba287 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
@@ -23,7 +23,7 @@&lt;br/&gt;
 public class StreamsSmokeTest {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  args ::= command kafka zookeeper stateDir&lt;br/&gt;
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;command := &quot;run&quot; | &quot;process&quot;&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;@param args&lt;br/&gt;
@@ -32,11 +32,13 @@ public static void main(String[] args) throws Exception {&lt;br/&gt;
         String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         String stateDir = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
         String command = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+        boolean disableAutoTerminate = args.length &amp;gt; 3;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.out.println(&quot;StreamsTest instance started&quot;);&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsSmokeTest)&quot;);&lt;br/&gt;
         System.out.println(&quot;command=&quot; + command);&lt;br/&gt;
         System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
         System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;disableAutoTerminate=&quot; + disableAutoTerminate);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         switch (command) {&lt;br/&gt;
             case &quot;standalone&quot;:&lt;br/&gt;
@@ -46,8 +48,12 @@ public static void main(String[] args) throws Exception {&lt;br/&gt;
                 // this starts the driver (data generation and result verification)&lt;br/&gt;
                 final int numKeys = 10;&lt;br/&gt;
                 final int maxRecordsPerKey = 500;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);&lt;/li&gt;
	&lt;li&gt;SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);&lt;br/&gt;
+                if (disableAutoTerminate) 
{
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
&lt;p&gt;                 break;&lt;br/&gt;
             case &quot;process&quot;:&lt;br/&gt;
                 // this starts a KafkaStreams client&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..0ee47e416ff&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,73 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires two argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest trunk)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..72d7f5a7b04&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                System.out.println(&quot;closing Kafka Streams instance&quot;);+                System.out.flush();+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..eebd0fab83c&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,114 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 3 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.1)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..18240f04ff1&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,108 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.10.2.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.2)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            // TODO: because Kafka 0.10.2.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index e6f692b171d..eeb16816367 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -20,6 +20,7 @@&lt;br/&gt;
 from ducktape.utils.util import wait_until&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1&lt;/p&gt;


&lt;p&gt; class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, &quot;tools-log4j.properties&quot;)&lt;br/&gt;
     PID_FILE = os.path.join(PERSISTENT_ROOT, &quot;streams.pid&quot;)&lt;/p&gt;

&lt;p&gt;+    CLEAN_NODE_ENABLED = True&lt;br/&gt;
+&lt;br/&gt;
     logs = {&lt;br/&gt;
         &quot;streams_log&quot;: {&lt;br/&gt;
             &quot;path&quot;: LOG_FILE,&lt;br/&gt;
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
         &quot;streams_stderr&quot;: &lt;/p&gt;
{
             &quot;path&quot;: STDERR_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None):&lt;br/&gt;
@@ -107,7 +218,8 @@ def wait_node(self, node, timeout_sec=None):&lt;/p&gt;

&lt;p&gt;     def clean_node(self, node):&lt;br/&gt;
         node.account.kill_process(&quot;streams&quot;, clean_shutdown=False, allow_fail=True)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if self.CLEAN_NODE_ENABLED:&lt;br/&gt;
+            node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def start_cmd(self, node):&lt;br/&gt;
         args = self.args.copy()&lt;br/&gt;
@@ -163,7 +275,28 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, command):&lt;br/&gt;
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
         super(StreamsSmokeTestDriverService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, kafka, &quot;run&quot;)&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def disable_auto_terminate(self):&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;disableAutoTerminate&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;disable_auto_terminate&amp;#39;&amp;#93;&lt;/span&gt; = self.DISABLE_AUTO_TERMINATE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;/p&gt;

&lt;p&gt;+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;/p&gt;

&lt;p&gt; class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
@@ -206,3 +339,41 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, eosEnabled):&lt;br/&gt;
                                                                 kafka,&lt;br/&gt;
                                                                 &quot;org.apache.kafka.streams.tests.BrokerCompatibilityTest&quot;,&lt;br/&gt;
                                                                 eosEnabled)&lt;br/&gt;
+&lt;br/&gt;
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
+        super(StreamsUpgradeTestJobRunnerService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context,&lt;br/&gt;
+                                                                 kafka,&lt;br/&gt;
+                                                                 &quot;org.apache.kafka.streams.tests.StreamsUpgradeTest&quot;,&lt;br/&gt;
+                                                                 &quot;&quot;)&lt;br/&gt;
+        self.UPGRADE_FROM = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def set_version(self, kafka_streams_version):&lt;br/&gt;
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version&lt;br/&gt;
+&lt;br/&gt;
+    def set_upgrade_from(self, upgrade_from):&lt;br/&gt;
+        self.UPGRADE_FROM = upgrade_from&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
+        else:&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = &quot;&quot;&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;version&amp;#39;&amp;#93;&lt;/span&gt; = self.KAFKA_STREAMS_VERSION&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;upgrade_from&amp;#39;&amp;#93;&lt;/span&gt; = self.UPGRADE_FROM&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s &quot; \&lt;br/&gt;
+              &quot; %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..7aa2de67d53&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -0,0 +1,246 @@&lt;br/&gt;
+# Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+# contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+# this work for additional information regarding copyright ownership.&lt;br/&gt;
+# The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+# (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+# the License.  You may obtain a copy of the License at&lt;br/&gt;
+#&lt;br/&gt;
+#    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+#&lt;br/&gt;
+# Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+# See the License for the specific language governing permissions and&lt;br/&gt;
+# limitations under the License.&lt;br/&gt;
+&lt;br/&gt;
+from ducktape.mark import parametrize&lt;br/&gt;
+from kafkatest.tests.kafka_test import KafkaTest&lt;br/&gt;
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsUpgradeTestJobRunnerService&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, DEV_VERSION&lt;br/&gt;
+import random&lt;br/&gt;
+&lt;br/&gt;
+class StreamsUpgradeTest(KafkaTest):&lt;br/&gt;
+    &quot;&quot;&quot;&lt;br/&gt;
+    Test upgrading Kafka Streams (all version combination)&lt;br/&gt;
+    If metadata was changes, upgrade is more difficult&lt;br/&gt;
+    Metadata version was bumped in 0.10.1.0&lt;br/&gt;
+    &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
+        super(StreamsUpgradeTest, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, num_zk=1, num_brokers=1, topics={&lt;br/&gt;
+            &apos;echo&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+            &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;+        })&lt;br/&gt;
+&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+&lt;br/&gt;
+    @parametrize(old_version=str(LATEST_0_10_1), new_version=str(LATEST_0_10_2))&lt;br/&gt;
+    @parametrize(old_version=str(LATEST_0_10_1), new_version=str(DEV_VERSION))&lt;br/&gt;
+    @parametrize(old_version=str(LATEST_0_10_2), new_version=str(DEV_VERSION))&lt;br/&gt;
+    def test_simple_upgrade(self, old_version, new_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with &amp;lt;old_version&amp;gt;, and upgrades one-by-one to &amp;lt;new_verion&amp;gt;&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(old_version)&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released&lt;br/&gt;
+    @parametrize(new_version=str(DEV_VERSION))&lt;br/&gt;
+    def test_metadata_upgrade(self, new_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(str(LATEST_0_10_0))&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # first rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;0.10.0&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # second rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    def start_all_nodes_with(self, version):&lt;br/&gt;
+        # start first with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor1, version)&lt;br/&gt;
+        node1 = self.processor1.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:&lt;br/&gt;
+                self.processor1.start()&lt;br/&gt;
+                log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                       timeout_sec=60,&lt;br/&gt;
+                                       err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node1.account))&lt;br/&gt;
+                monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start second with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor2, version)&lt;br/&gt;
+        node2 = self.processor2.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:&lt;br/&gt;
+                    self.processor2.start()&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node2.account))&lt;br/&gt;
+                    first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                             timeout_sec=60,&lt;br/&gt;
+                                             err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                    second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                              timeout_sec=60,&lt;br/&gt;
+                                              err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start third with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor3, version)&lt;br/&gt;
+        node3 = self.processor3.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:&lt;br/&gt;
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:&lt;br/&gt;
+                        self.processor3.start()&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node3.account))&lt;br/&gt;
+                        first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                 timeout_sec=60,&lt;br/&gt;
+                                                 err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                        second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+                        third_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node3.account))&lt;br/&gt;
+&lt;br/&gt;
+    @staticmethod&lt;br/&gt;
+    def prepare_for(processor, version):&lt;br/&gt;
+        processor.node.account.ssh(&quot;rm -rf &quot; + processor.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        processor.set_version(version)&lt;br/&gt;
+&lt;br/&gt;
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
+        first_other_processor = None&lt;br/&gt;
+        second_other_processor = None&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            if p != processor:&lt;br/&gt;
+                if first_other_processor is None:&lt;br/&gt;
+                    first_other_processor = p&lt;br/&gt;
+                else:&lt;br/&gt;
+                    second_other_processor = p&lt;br/&gt;
+&lt;br/&gt;
+        node = processor.node&lt;br/&gt;
+        first_other_node = first_other_processor.node&lt;br/&gt;
+        second_other_node = second_other_processor.node&lt;br/&gt;
+&lt;br/&gt;
+        # stop processor and wait for rebalance of others&lt;br/&gt;
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                processor.stop()&lt;br/&gt;
+                first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                timeout_sec=60,&lt;br/&gt;
+                                                err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+        node.account.ssh_capture(&quot;grep UPGRADE-TEST-CLIENT-CLOSED %s&quot; % processor.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if upgrade_from == &quot;&quot;:  # upgrade disabled &amp;#8211; second round of rolling bounces&lt;br/&gt;
+            roll_counter = &quot;.1-&quot;  # second round of rolling bounces&lt;br/&gt;
+        else:&lt;br/&gt;
+            roll_counter = &quot;.0-&quot;  # first  round of rolling boundes&lt;br/&gt;
+&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDOUT_FILE + &quot; &quot; + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDERR_FILE + &quot; &quot; + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.LOG_FILE + &quot; &quot; + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if new_version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(new_version)&lt;br/&gt;
+        processor.set_upgrade_from(upgrade_from)&lt;br/&gt;
+&lt;br/&gt;
+        grep_metadata_error = &quot;grep \&quot;org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\&quot; &quot;&lt;br/&gt;
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:&lt;br/&gt;
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                        processor.start()&lt;br/&gt;
+&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + new_version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + new_version + &quot; &quot; + str(node.account))&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                       timeout_sec=60,&lt;br/&gt;
+                                                       err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                        timeout_sec=60,&lt;br/&gt;
+                                                        err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node.account))&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index f63a7c17ecd..94ba100bda7 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -61,6 +61,7 @@ def get_version(node=None):&lt;br/&gt;
         return DEV_BRANCH&lt;/p&gt;

&lt;p&gt; DEV_BRANCH = KafkaVersion(&quot;dev&quot;)&lt;br/&gt;
+DEV_VERSION = KafkaVersion(&quot;0.11.0.3-SNAPSHOT&quot;)&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;0.8.2.X versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;br/&gt;
@@ -91,5 +92,7 @@ def get_version(node=None):&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;0.11.0.0 versions&lt;br/&gt;
 V_0_11_0_0 = KafkaVersion(&quot;0.11.0.0&quot;)&lt;br/&gt;
-LATEST_0_11_0 = V_0_11_0_0&lt;br/&gt;
+V_0_11_0_1 = KafkaVersion(&quot;0.11.0.1&quot;)&lt;br/&gt;
+V_0_11_0_2 = KafkaVersion(&quot;0.11.0.2&quot;)&lt;br/&gt;
+LATEST_0_11_0 = V_0_11_0_2&lt;br/&gt;
 LATEST_0_11 = LATEST_0_11_0&lt;br/&gt;
diff --git a/vagrant/base.sh b/vagrant/base.sh&lt;br/&gt;
index 4c0add543aa..28b81ed05a8 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/vagrant/base.sh&lt;br/&gt;
+++ b/vagrant/base.sh&lt;br/&gt;
@@ -64,6 +64,8 @@ get_kafka() {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;     kafka_dir=/opt/kafka-$version&lt;br/&gt;
     url=&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages-$version/kafka_$scala_version-$version.tgz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3-us-west-2.amazonaws.com/kafka-packages-$version/kafka_$scala_version-$version.tgz&lt;/a&gt;&lt;br/&gt;
+    # the .tgz above does not include the streams test jar hence we need to get it separately&lt;br/&gt;
+    url_streams_test=&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages/kafka-streams-$version-test.jar&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3-us-west-2.amazonaws.com/kafka-packages/kafka-streams-$version-test.jar&lt;/a&gt;&lt;br/&gt;
     if [ ! &lt;del&gt;d /opt/kafka&lt;/del&gt;$version ]; then&lt;br/&gt;
         pushd /tmp&lt;br/&gt;
         curl -O $url&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16414976" author="githubbot" created="Tue, 27 Mar 2018 03:49:50 +0000"  >&lt;p&gt;mjsax opened a new pull request #4779: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4779&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4779&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16415012" author="githubbot" created="Tue, 27 Mar 2018 04:43:42 +0000"  >&lt;p&gt;mjsax closed pull request #4746: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4746&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4746&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh&lt;br/&gt;
index 1f5140b10c8..77123ff8093 100755&lt;br/&gt;
&amp;#8212; a/bin/kafka-run-class.sh&lt;br/&gt;
+++ b/bin/kafka-run-class.sh&lt;br/&gt;
@@ -73,28 +73,48 @@ do&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/clients/build/libs/kafka-clients*.jar;&lt;br/&gt;
-do&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if should_include_file &quot;$file&quot;; then&lt;/li&gt;
	&lt;li&gt;CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;/li&gt;
	&lt;li&gt;fi&lt;br/&gt;
-done&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  clients_lib_dir=$(dirname $0)/../clients/build/libs&lt;br/&gt;
+  streams_lib_dir=$(dirname $0)/../streams/build/libs&lt;br/&gt;
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}&lt;br/&gt;
+else&lt;br/&gt;
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs&lt;br/&gt;
+  streams_lib_dir=$clients_lib_dir&lt;br/&gt;
+  rocksdb_lib_dir=$streams_lib_dir&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/build/libs/kafka-streams*.jar;&lt;br/&gt;
+for file in &quot;$clients_lib_dir&quot;/kafka-clients*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+for file in &quot;$streams_lib_dir&quot;/kafka-streams*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;&lt;del&gt;for file in &quot;$base_dir&quot;/streams/build/dependant-libs&lt;/del&gt;${SCALA_VERSION}/rocksdb*.jar;&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+else&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/upgrade-system-tests-0100/build/libs/kafka-streams-upgrade-system-tests*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;br/&gt;
+for file in &quot;$rocksdb_lib_dir&quot;/rocksdb*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
 done&lt;br/&gt;
diff --git a/build.gradle b/build.gradle&lt;br/&gt;
index d221d965c5e..2a540211018 100644&lt;br/&gt;
&amp;#8212; a/build.gradle&lt;br/&gt;
+++ b/build.gradle&lt;br/&gt;
@@ -776,6 +776,19 @@ project(&apos;:streams:examples&apos;) {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;+project(&apos;:streams:upgrade-system-tests-0100&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0100&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0100
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
 project(&apos;:log4j-appender&apos;) {&lt;br/&gt;
   archivesBaseName = &quot;kafka-log4j-appender&quot;&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java&lt;br/&gt;
index 93b92bb52a3..dbbb9127199 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java&lt;br/&gt;
@@ -918,7 +918,7 @@ public void onFailure(RuntimeException e) {&lt;br/&gt;
                 log.error(&quot;Unexpected interrupt received in heartbeat thread for group {}&quot;, groupId, e);&lt;br/&gt;
                 this.failed.set(new RuntimeException(e));&lt;br/&gt;
             } catch (RuntimeException e) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.error(&quot;Heartbeat thread for group {} failed due to unexpected error&quot; , groupId, e);&lt;br/&gt;
+                log.error(&quot;Heartbeat thread for group {} failed due to unexpected error&quot;, groupId, e);&lt;br/&gt;
                 this.failed.set(e);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java b/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
index 212d701aaaa..74887483354 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
@@ -316,7 +316,7 @@ public int hashCode() {&lt;br/&gt;
             Field f = this.schema.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
             if (f.type() instanceof ArrayOf) {&lt;br/&gt;
                 if (this.get(f) != null) 
{
-                    Object[] arrayObject = (Object []) this.get(f);
+                    Object[] arrayObject = (Object[]) this.get(f);
                     for (Object arrayItem: arrayObject)
                         result = prime * result + arrayItem.hashCode();
                 }
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
index 8e0b8dbfa3d..b80dfccf3d9 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
@@ -17,7 +17,9 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.security.authenticator;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import java.util.Map;&lt;br/&gt;
+import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
+import org.apache.kafka.common.network.Mode;&lt;br/&gt;
+import org.apache.kafka.common.security.auth.AuthCallbackHandler;&lt;/p&gt;

&lt;p&gt; import javax.security.auth.Subject;&lt;br/&gt;
 import javax.security.auth.callback.Callback;&lt;br/&gt;
@@ -26,10 +28,7 @@&lt;br/&gt;
 import javax.security.auth.callback.UnsupportedCallbackException;&lt;br/&gt;
 import javax.security.sasl.AuthorizeCallback;&lt;br/&gt;
 import javax.security.sasl.RealmCallback;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
-import org.apache.kafka.common.network.Mode;&lt;br/&gt;
-import org.apache.kafka.common.security.auth.AuthCallbackHandler;&lt;br/&gt;
+import java.util.Map;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Callback handler for Sasl clients. The callbacks required for the SASL mechanism&lt;br/&gt;
@@ -59,7 +58,7 @@ public void handle(Callback[] callbacks) throws UnsupportedCallbackException 
{
                     nc.setName(nc.getDefaultName());
             }
&lt;p&gt; else if (callback instanceof PasswordCallback) {&lt;br/&gt;
                 if (!isKerberos &amp;amp;&amp;amp; subject != null &amp;amp;&amp;amp; !subject.getPrivateCredentials(String.class).isEmpty()) &lt;/p&gt;
{
-                    char [] password = subject.getPrivateCredentials(String.class).iterator().next().toCharArray();
+                    char[] password = subject.getPrivateCredentials(String.class).iterator().next().toCharArray();
                     ((PasswordCallback) callback).setPassword(password);
                 }
&lt;p&gt; else {&lt;br/&gt;
                     String errorMessage = &quot;Could not login: the client is being asked for a password, but the Kafka&quot; +&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/ProducerRecordTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/ProducerRecordTest.java&lt;br/&gt;
index a844bb08917..5186d05d492 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/producer/ProducerRecordTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/ProducerRecordTest.java&lt;br/&gt;
@@ -26,24 +26,24 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void testEqualsAndHashCode() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;String, Integer&amp;gt; producerRecord = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1 , &quot;key&quot;, 1);&lt;br/&gt;
+        ProducerRecord&amp;lt;String, Integer&amp;gt; producerRecord = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1, &quot;key&quot;, 1);&lt;br/&gt;
         assertEquals(producerRecord, producerRecord);&lt;br/&gt;
         assertEquals(producerRecord.hashCode(), producerRecord.hashCode());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;String, Integer&amp;gt; equalRecord = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1 , &quot;key&quot;, 1);&lt;br/&gt;
+        ProducerRecord&amp;lt;String, Integer&amp;gt; equalRecord = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1, &quot;key&quot;, 1);&lt;br/&gt;
         assertEquals(producerRecord, equalRecord);&lt;br/&gt;
         assertEquals(producerRecord.hashCode(), equalRecord.hashCode());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;String, Integer&amp;gt; topicMisMatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test-1&quot;, 1 , &quot;key&quot;, 1);&lt;br/&gt;
+        ProducerRecord&amp;lt;String, Integer&amp;gt; topicMisMatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test-1&quot;, 1, &quot;key&quot;, 1);&lt;br/&gt;
         assertFalse(producerRecord.equals(topicMisMatch));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;String, Integer&amp;gt; partitionMismatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 2 , &quot;key&quot;, 1);&lt;br/&gt;
+        ProducerRecord&amp;lt;String, Integer&amp;gt; partitionMismatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 2, &quot;key&quot;, 1);&lt;br/&gt;
         assertFalse(producerRecord.equals(partitionMismatch));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;String, Integer&amp;gt; keyMisMatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1 , &quot;key-1&quot;, 1);&lt;br/&gt;
+        ProducerRecord&amp;lt;String, Integer&amp;gt; keyMisMatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1, &quot;key-1&quot;, 1);&lt;br/&gt;
         assertFalse(producerRecord.equals(keyMisMatch));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;String, Integer&amp;gt; valueMisMatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1 , &quot;key&quot;, 2);&lt;br/&gt;
+        ProducerRecord&amp;lt;String, Integer&amp;gt; valueMisMatch = new ProducerRecord&amp;lt;&amp;gt;(&quot;test&quot;, 1, &quot;key&quot;, 2);&lt;br/&gt;
         assertFalse(producerRecord.equals(valueMisMatch));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         ProducerRecord&amp;lt;String, Integer&amp;gt; nullFieldsRecord = new ProducerRecord&amp;lt;&amp;gt;(&quot;topic&quot;, null, null, null, null);&lt;br/&gt;
diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index e6b9747d0f9..faa96c1aca6 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -55,6 +55,23 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10_1_breaking&quot; href=&quot;#upgrade_10_1_breaking&quot;&amp;gt;Potential breaki&lt;/p&gt;

&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_streams&quot; href=&quot;#upgrade_1010_streams&quot;&amp;gt;Streams API changes in 0.10.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.1.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.1.0 or 0.10.1.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Stream grouping and aggregation split into two methods:&lt;br/&gt;
         &amp;lt;ul&amp;gt;&lt;br/&gt;
             &amp;lt;li&amp;gt; old: KStream #aggregateByKey(), #reduceByKey(), and #countByKey() &amp;lt;/li&amp;gt;&lt;br/&gt;
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle&lt;br/&gt;
index 2ff459f1520..07944a9e42d 100644&lt;br/&gt;
&amp;#8212; a/gradle/dependencies.gradle&lt;br/&gt;
+++ b/gradle/dependencies.gradle&lt;br/&gt;
@@ -31,6 +31,7 @@ versions += [&lt;br/&gt;
   jackson: &quot;2.6.3&quot;,&lt;br/&gt;
   jetty: &quot;9.2.22.v20170606&quot;,&lt;br/&gt;
   jersey: &quot;2.22.2&quot;,&lt;br/&gt;
+  kafka_0100: &quot;0.10.0.1&quot;,&lt;br/&gt;
   log4j: &quot;1.2.17&quot;,&lt;br/&gt;
   jopt: &quot;4.9&quot;,&lt;br/&gt;
   junit: &quot;4.12&quot;,&lt;br/&gt;
@@ -91,6 +92,7 @@ libs += [&lt;br/&gt;
   junit: &quot;junit:junit:$versions.junit&quot;,&lt;br/&gt;
   log4j: &quot;log4j:log4j:$versions.log4j&quot;,&lt;br/&gt;
   joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
+  kafkaStreams_0100: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0100&quot;,&lt;br/&gt;
   lz4: &quot;net.jpountz.lz4:lz4:$versions.lz4&quot;,&lt;br/&gt;
   metrics: &quot;com.yammer.metrics:metrics-core:$versions.metrics&quot;,&lt;br/&gt;
   powermock: &quot;org.powermock:powermock-module-junit4:$versions.powermock&quot;,&lt;br/&gt;
diff --git a/gradle/rat.gradle b/gradle/rat.gradle&lt;br/&gt;
index d62b3722a4f..a51876c23ea 100644&lt;br/&gt;
&amp;#8212; a/gradle/rat.gradle&lt;br/&gt;
+++ b/gradle/rat.gradle&lt;br/&gt;
@@ -84,9 +84,15 @@ class RatTask extends DefaultTask {&lt;br/&gt;
     if (!reportDir.exists()) &lt;/p&gt;
{
       reportDir.mkdirs()
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;generateXmlReport(reportDir)&lt;/li&gt;
	&lt;li&gt;printUnknownFiles()&lt;/li&gt;
	&lt;li&gt;generateHtmlReport()&lt;br/&gt;
+    def origEncoding = System.getProperty(&quot;file.encoding&quot;)&lt;br/&gt;
+    try 
{
+      System.setProperty(&quot;file.encoding&quot;, &quot;UTF-8&quot;) //affects the output of the ant rat task
+      generateXmlReport(reportDir)
+      printUnknownFiles()
+      generateHtmlReport()
+    }
&lt;p&gt; finally &lt;/p&gt;
{
+      System.setProperty(&quot;file.encoding&quot;, origEncoding)
+    }
&lt;p&gt;   }&lt;br/&gt;
 }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -109,7 +115,7 @@ class RatPlugin implements Plugin&amp;lt;Project&amp;gt; &lt;/p&gt;
{
       mavenCentral()
     }
&lt;p&gt;     project.dependencies &lt;/p&gt;
{
-      rat &apos;org.apache.rat:apache-rat-tasks:0.11&apos;
+      rat &apos;org.apache.rat:apache-rat-tasks:0.12&apos;
     }
&lt;p&gt;   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/jenkins.sh b/jenkins.sh&lt;br/&gt;
new file mode 100755&lt;br/&gt;
index 00000000000..c21eb1d8500&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/jenkins.sh&lt;br/&gt;
@@ -0,0 +1,20 @@&lt;br/&gt;
+#!/bin/bash&lt;br/&gt;
+# Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+# contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+# this work for additional information regarding copyright ownership.&lt;br/&gt;
+# The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+# (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+# the License.  You may obtain a copy of the License at&lt;br/&gt;
+#&lt;br/&gt;
+#    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+#&lt;br/&gt;
+# Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+# See the License for the specific language governing permissions and&lt;br/&gt;
+# limitations under the License.&lt;br/&gt;
+&lt;br/&gt;
+# This script is used for verifying changes in Jenkins. In order to provide faster feedback, the tasks are ordered so&lt;br/&gt;
+# that faster tasks are executed in every module before slower tasks (if possible). For example, the unit tests for all&lt;br/&gt;
+# the modules are executed before the integration tests.&lt;br/&gt;
+./gradlew clean compileJava compileScala compileTestJava compileTestScala checkstyleMain checkstyleTest test rat --no-daemon -PxmlFindBugsReport=true -PtestLoggingEvents=started,passed,skipped,failed &quot;$@&quot;&lt;br/&gt;
diff --git a/settings.gradle b/settings.gradle&lt;br/&gt;
index d430c2fd919..f3a1b81ba65 100644&lt;br/&gt;
&amp;#8212; a/settings.gradle&lt;br/&gt;
+++ b/settings.gradle&lt;br/&gt;
@@ -13,5 +13,5 @@&lt;br/&gt;
 // See the License for the specific language governing permissions and&lt;br/&gt;
 // limitations under the License.&lt;/p&gt;

&lt;p&gt;-include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;log4j-appender&apos;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&apos;connect:api&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;&lt;br/&gt;
+include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;streams:upgrade-system-tests-0100&apos;,&lt;br/&gt;
+        &apos;log4j-appender&apos;, &apos;connect:api&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index 5ba438376f6..e33efefc38f 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -39,6 +39,7 @@&lt;br/&gt;
 import java.util.Set;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static org.apache.kafka.common.config.ConfigDef.Range.atLeast;&lt;br/&gt;
+import static org.apache.kafka.common.config.ConfigDef.ValidString.in;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Configuration for Kafka Streams. Documentation for these configurations can be found in the &amp;lt;a&lt;br/&gt;
@@ -54,6 +55,16 @@&lt;br/&gt;
     // Prefix used to isolate producer configs from consumer configs.&lt;br/&gt;
     public static final String PRODUCER_PREFIX = &quot;producer.&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.0.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
+    public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;br/&gt;
+    public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+        &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot; (for upgrading from 0.10.0.x).&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /** &amp;lt;code&amp;gt;state.dir&amp;lt;/code&amp;gt; */&lt;br/&gt;
     public static final String STATE_DIR_CONFIG = &quot;state.dir&quot;;&lt;br/&gt;
     private static final String STATE_DIR_DOC = &quot;Directory location for state store.&quot;;&lt;br/&gt;
@@ -257,14 +268,19 @@&lt;br/&gt;
                                         10 * 1024 * 1024L,&lt;br/&gt;
                                         atLeast(0),&lt;br/&gt;
                                         Importance.LOW,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;CACHE_MAX_BYTES_BUFFERING_DOC);&lt;br/&gt;
+                                        CACHE_MAX_BYTES_BUFFERING_DOC)&lt;br/&gt;
+                                .define(UPGRADE_FROM_CONFIG,&lt;br/&gt;
+                                        ConfigDef.Type.STRING,&lt;br/&gt;
+                                        null,&lt;br/&gt;
+                                        in(null, UPGRADE_FROM_0100),&lt;br/&gt;
+                                        ConfigDef.Importance.LOW,&lt;br/&gt;
+                                        UPGRADE_FROM_DOC);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // this is the list of configs for underlying clients&lt;br/&gt;
     // that streams prefer different default values&lt;br/&gt;
     private static final Map&amp;lt;String, Object&amp;gt; PRODUCER_DEFAULT_OVERRIDES;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static&lt;/li&gt;
	&lt;li&gt;{&lt;br/&gt;
+    static 
{
         Map&amp;lt;String, Object&amp;gt; tempProducerDefaultOverrides = new HashMap&amp;lt;&amp;gt;();
         tempProducerDefaultOverrides.put(ProducerConfig.LINGER_MS_CONFIG, &quot;100&quot;);
 
@@ -272,8 +288,7 @@
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Map&amp;lt;String, Object&amp;gt; CONSUMER_DEFAULT_OVERRIDES;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static&lt;/li&gt;
	&lt;li&gt;{&lt;br/&gt;
+    static 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         Map&amp;lt;String, Object&amp;gt; tempConsumerDefaultOverrides = new HashMap&amp;lt;&amp;gt;();         tempConsumerDefaultOverrides.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, &amp;quot;1000&amp;quot;);         tempConsumerDefaultOverrides.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &amp;quot;earliest&amp;quot;);@@ -342,6 +357,7 @@ public StreamsConfig(Map&amp;lt;?, ?&amp;gt; props) {
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + &quot;-consumer&quot;);
 
         // add configs required for stream partition assignor
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));
         consumerProps.put(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE, streamThread);
         consumerProps.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));
         consumerProps.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java b/streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java
index 81f1f63078f..bfca83a39b4 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java
@@ -676,7 +676,7 @@ private void connectProcessorAndStateStore(String processorName, String stateSto
         }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Set&amp;lt;String&amp;gt; findSourceTopicsForProcessorParents(String [] parents) {&lt;br/&gt;
+    private Set&amp;lt;String&amp;gt; findSourceTopicsForProcessorParents(String[] parents) {&lt;br/&gt;
         final Set&amp;lt;String&amp;gt; sourceTopics = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         for (String parent : parents) {&lt;br/&gt;
             NodeFactory nodeFactory = nodeFactories.get(parent);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
index dcba5437bf5..e6c407fdbae 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
@@ -14,7 +14,6 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-&lt;br/&gt;
 package org.apache.kafka.streams.processor.internals;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.clients.consumer.internals.PartitionAssignor;&lt;br/&gt;
@@ -66,7 +65,7 @@&lt;br/&gt;
         public final TaskId taskId;&lt;br/&gt;
         public final TopicPartition partition;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public AssignedPartition(TaskId taskId, TopicPartition partition) {&lt;br/&gt;
+        AssignedPartition(final TaskId taskId, final TopicPartition partition) 
{
             this.taskId = taskId;
             this.partition = partition;
         }
&lt;p&gt;@@ -92,6 +91,7 @@ public int compare(TopicPartition p1, TopicPartition p2) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private StreamThread streamThread;&lt;/p&gt;

&lt;p&gt;+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
     private int numStandbyReplicas;&lt;br/&gt;
     private Map&amp;lt;Integer, TopologyBuilder.TopicsInfo&amp;gt; topicGroups;&lt;br/&gt;
     private Map&amp;lt;TopicPartition, Set&amp;lt;TaskId&amp;gt;&amp;gt; partitionToTaskIds;&lt;br/&gt;
@@ -111,6 +111,11 @@ public int compare(TopicPartition p1, TopicPartition p2) {&lt;br/&gt;
     public void configure(Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         numStandbyReplicas = (Integer) configs.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);&lt;/p&gt;

&lt;p&gt;+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;br/&gt;
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) &lt;/p&gt;
{
+            log.info(&quot;Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x.&quot;);
+            userMetadataVersion = 1;
+        }

&lt;p&gt;         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);&lt;br/&gt;
         if (o == null) {&lt;br/&gt;
@@ -174,7 +179,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; prevTasks = streamThread.prevTasks();&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; standbyTasks = streamThread.cachedTasks();&lt;br/&gt;
         standbyTasks.removeAll(prevTasks);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;SubscriptionInfo data = new SubscriptionInfo(streamThread.processId, prevTasks, standbyTasks, this.userEndPointConfig);&lt;br/&gt;
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, streamThread.processId, prevTasks, standbyTasks, this.userEndPointConfig);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (streamThread.builder.sourceTopicPattern() != null) {&lt;br/&gt;
             SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();&lt;br/&gt;
@@ -265,12 +270,16 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         Map&amp;lt;UUID, ClientState&amp;lt;TaskId&amp;gt;&amp;gt; states = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         Map&amp;lt;UUID, HostInfo&amp;gt; consumerEndPointMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         // decode subscription info&lt;br/&gt;
+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;br/&gt;
             String consumerId = entry.getKey();&lt;br/&gt;
             Subscription subscription = entry.getValue();&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+            final int usedVersion = info.version;&lt;br/&gt;
+            if (usedVersion &amp;lt; minUserMetadataVersion) &lt;/p&gt;
{
+                minUserMetadataVersion = usedVersion;
+            }
&lt;p&gt;             if (info.userEndPoint != null) {&lt;br/&gt;
                 final String[] hostPort = info.userEndPoint.split(&quot;:&quot;);&lt;br/&gt;
                 consumerEndPointMap.put(info.processId, new HostInfo(hostPort&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, Integer.valueOf(hostPort&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;)));&lt;br/&gt;
@@ -460,6 +469,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;


&lt;p&gt;                 assignmentSuppliers.add(new AssignmentSupplier(consumer,&lt;br/&gt;
+                                                               minUserMetadataVersion,&lt;br/&gt;
                                                                active,&lt;br/&gt;
                                                                standby,&lt;br/&gt;
                                                                endPointMap,&lt;br/&gt;
@@ -483,17 +493,20 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;

&lt;p&gt;     class AssignmentSupplier {&lt;br/&gt;
         private final String consumer;&lt;br/&gt;
+        private final int metadataVersion;&lt;br/&gt;
         private final List&amp;lt;TaskId&amp;gt; active;&lt;br/&gt;
         private final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standby;&lt;br/&gt;
         private final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; endPointMap;&lt;br/&gt;
         private final List&amp;lt;TopicPartition&amp;gt; activePartitions;&lt;/p&gt;

&lt;p&gt;         AssignmentSupplier(final String consumer,&lt;br/&gt;
+                           final int metadataVersion,&lt;br/&gt;
                            final List&amp;lt;TaskId&amp;gt; active,&lt;br/&gt;
                            final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standby,&lt;br/&gt;
                            final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; endPointMap,&lt;br/&gt;
                            final List&amp;lt;TopicPartition&amp;gt; activePartitions) {&lt;br/&gt;
             this.consumer = consumer;&lt;br/&gt;
+            this.metadataVersion = metadataVersion;&lt;br/&gt;
             this.active = active;&lt;br/&gt;
             this.standby = standby;&lt;br/&gt;
             this.endPointMap = endPointMap;&lt;br/&gt;
@@ -501,7 +514,8 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         Assignment get() &lt;/p&gt;
{
-            return new Assignment(activePartitions, new AssignmentInfo(active,
+            return new Assignment(activePartitions, new AssignmentInfo(metadataVersion,
+                                                                       active,
                                                                        standby,
                                                                        endPointMap).encode());
         }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index 6569f8587bc..ce9aa6309ca 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -14,7 +14,6 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-&lt;br/&gt;
 package org.apache.kafka.streams.processor.internals.assignment;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.record.ByteBufferInputStream;&lt;br/&gt;
@@ -56,7 +55,7 @@ public AssignmentInfo(List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;&lt;br/&gt;
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+    public AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
                              Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.activeTasks = activeTasks;&lt;br/&gt;
@@ -155,9 +154,7 @@ public static AssignmentInfo decode(ByteBuffer data) {&lt;br/&gt;
                 }&lt;br/&gt;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
         } catch (IOException ex) 
{
             throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);
         }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index c3481c05156..92c50a2a942 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -14,7 +14,6 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-&lt;br/&gt;
 package org.apache.kafka.streams.processor.internals.assignment;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.streams.errors.TaskAssignmentException;&lt;br/&gt;
@@ -32,7 +31,7 @@&lt;/p&gt;

&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int CURRENT_VERSION = 2;&lt;br/&gt;
+    public static final int CURRENT_VERSION = 2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public final int version;&lt;br/&gt;
     public final UUID processId;&lt;br/&gt;
@@ -44,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; stand&lt;br/&gt;
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
+    public SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.processId = processId;&lt;br/&gt;
         this.prevTasks = prevTasks;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java&lt;br/&gt;
index 35b88db583b..e4ba9cdf143 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java&lt;br/&gt;
@@ -22,6 +22,7 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StreamPartitioner;&lt;br/&gt;
 import org.apache.kafka.test.MockMetricsReporter;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.Assert;&lt;br/&gt;
 import org.junit.ClassRule;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
@@ -39,11 +40,12 @@&lt;br/&gt;
     public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(NUM_BROKERS);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testStartAndClose() throws Exception {&lt;br/&gt;
+    public void testStartAndClose() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testStartAndClose&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
         props.setProperty(StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final int oldInitCount = MockMetricsReporter.INIT_COUNT.get();&lt;br/&gt;
         final int oldCloseCount = MockMetricsReporter.CLOSE_COUNT.get();&lt;br/&gt;
@@ -62,11 +64,12 @@ public void testStartAndClose() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCloseIsIdempotent() throws Exception {&lt;br/&gt;
+    public void testCloseIsIdempotent() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testCloseIsIdempotent&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
         props.setProperty(StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;br/&gt;
@@ -79,10 +82,11 @@ public void testCloseIsIdempotent() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCannotStartOnceClosed() throws Exception {&lt;br/&gt;
+    public void testCannotStartOnceClosed() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testCannotStartOnceClosed&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;br/&gt;
@@ -99,10 +103,11 @@ public void testCannotStartOnceClosed() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCannotStartTwice() throws Exception {&lt;br/&gt;
+    public void testCannotStartTwice() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testCannotStartTwice&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;br/&gt;
@@ -119,25 +124,25 @@ public void testCannotStartTwice() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetAllTasksWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetAllTasksWhenNotRunning() 
{
         final KafkaStreams streams = createKafkaStreams();
         streams.allMetadata();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetAllTasksWithStoreWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetAllTasksWithStoreWhenNotRunning() 
{
         final KafkaStreams streams = createKafkaStreams();
         streams.allMetadataForStore(&quot;store&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() 
{
         final KafkaStreams streams = createKafkaStreams();
         streams.metadataForKey(&quot;store&quot;, &quot;key&quot;, Serdes.String().serializer());
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() throws Exception {&lt;br/&gt;
+    public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() {&lt;br/&gt;
         final KafkaStreams streams = createKafkaStreams();&lt;br/&gt;
         streams.metadataForKey(&quot;store&quot;, &quot;key&quot;, new StreamPartitioner&amp;lt;String, Object&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
@@ -152,16 +157,18 @@ private KafkaStreams createKafkaStreams() 
{
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;appId&quot;);
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
 
         final KStreamBuilder builder = new KStreamBuilder();
         return new KafkaStreams(builder, props);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCleanup() throws Exception {&lt;br/&gt;
+    public void testCleanup() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testLocalCleanup&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;br/&gt;
@@ -173,10 +180,11 @@ public void testCleanup() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test(expected = IllegalStateException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testCannotCleanupWhileRunning() throws Exception {&lt;br/&gt;
+    public void testCannotCleanupWhileRunning() {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;testCannotCleanupWhileRunning&quot;);&lt;br/&gt;
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
         final KafkaStreams streams = new KafkaStreams(builder, props);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
index f03bed95c1a..9d401487d6c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
@@ -58,7 +58,7 @@ public void setUp() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetProducerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetProducerConfigs() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getProducerConfigs(&quot;client&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ProducerConfig.CLIENT_ID_CONFIG), &quot;client-producer&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ProducerConfig.LINGER_MS_CONFIG), &quot;100&quot;);&lt;br/&gt;
@@ -66,7 +66,7 @@ public void testGetProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetConsumerConfigs() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getConsumerConfigs(null, &quot;example-application&quot;, &quot;client&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), &quot;client-consumer&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG), &quot;example-application&quot;);&lt;br/&gt;
@@ -75,7 +75,7 @@ public void testGetConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testGetRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void testGetRestoreConsumerConfigs() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; returnedProps = streamsConfig.getRestoreConsumerConfigs(&quot;client&quot;);&lt;br/&gt;
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), &quot;client-restore-consumer&quot;);&lt;br/&gt;
         assertNull(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG));&lt;br/&gt;
@@ -84,7 +84,7 @@ public void testGetRestoreConsumerConfigs() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void defaultSerdeShouldBeConfigured() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; serializerConfigs = new HashMap&amp;lt;String, Object&amp;gt;();&lt;br/&gt;
+        Map&amp;lt;String, Object&amp;gt; serializerConfigs = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         serializerConfigs.put(&quot;key.serializer.encoding&quot;, &quot;UTF8&quot;);&lt;br/&gt;
         serializerConfigs.put(&quot;value.serializer.encoding&quot;, &quot;UTF-16&quot;);&lt;br/&gt;
         Serializer&amp;lt;String&amp;gt; serializer = Serdes.String().serializer();&lt;br/&gt;
@@ -115,7 +115,7 @@ public void shouldSupportMultipleBootstrapServers() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedConsumerConfigs() {&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;earliest&quot;);&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -125,7 +125,7 @@ public void shouldSupportPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedRestoreConsumerConfigs() {&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;earliest&quot;);&lt;br/&gt;
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -135,7 +135,7 @@ public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() 
{
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = streamsConfig.getConsumerConfigs(null, &quot;groupId&quot;, &quot;clientId&quot;);
@@ -143,7 +143,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() thro
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() 
{
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = streamsConfig.getRestoreConsumerConfigs(&quot;clientId&quot;);
@@ -151,7 +151,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() {&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         props.put(producerPrefix(&quot;interceptor.statsd.host&quot;), &quot;host&quot;);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; producerConfigs = streamsConfig.getProducerConfigs(&quot;clientId&quot;);&lt;br/&gt;
@@ -160,7 +160,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() thro&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportPrefixedProducerConfigs() {&lt;br/&gt;
         props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);&lt;br/&gt;
         props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -170,7 +170,7 @@ public void shouldSupportPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldBeSupportNonPrefixedConsumerConfigs() {&lt;br/&gt;
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;br/&gt;
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -180,7 +180,7 @@ public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() 
{
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -190,7 +190,7 @@ public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSupportNonPrefixedProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldSupportNonPrefixedProducerConfigs() {&lt;br/&gt;
         props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10);&lt;br/&gt;
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -199,24 +199,22 @@ public void shouldSupportNonPrefixedProducerConfigs() throws Exception 
{
         assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
     @Test(expected = StreamsException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() throws Exception {&lt;br/&gt;
+    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() 
{
         props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.keySerde();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = StreamsException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() throws Exception {&lt;br/&gt;
+    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() 
{
         props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.valueSerde();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultConsumerConfigs() {&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;latest&quot;);&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), &quot;10&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -226,7 +224,7 @@ public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultProducerConfigs() {&lt;br/&gt;
         props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), &quot;10000&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; producerConfigs = streamsConfig.getProducerConfigs(&quot;client&quot;);&lt;br/&gt;
@@ -234,7 +232,7 @@ public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throws Exception {&lt;br/&gt;
+    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() 
{
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), &quot;latest&quot;);
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), &quot;10&quot;);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -244,14 +242,14 @@ public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = ConfigException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() 
{
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), &quot;true&quot;);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getConsumerConfigs(null, &quot;a&quot;, &quot;b&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = ConfigException.class)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() {&lt;br/&gt;
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), &quot;true&quot;);&lt;br/&gt;
         final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
         streamsConfig.getRestoreConsumerConfigs(&quot;client&quot;);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java&lt;br/&gt;
index a5fb0763187..88098dc679b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java&lt;br/&gt;
@@ -32,6 +32,7 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.ValueMapper;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.BeforeClass;&lt;br/&gt;
 import org.junit.ClassRule;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
@@ -79,13 +80,12 @@&lt;br/&gt;
     private static final String OUTPUT_TOPIC_C = &quot;C&quot;;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @BeforeClass&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static void startKafkaCluster() throws Exception {&lt;br/&gt;
+    public static void startKafkaCluster() 
{
         CLUSTER.createTopic(INPUT_TOPIC_A);
         CLUSTER.createTopic(OUTPUT_TOPIC_B);
         CLUSTER.createTopic(OUTPUT_TOPIC_C);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
     @Parameter&lt;br/&gt;
     public long cacheSizeBytes;&lt;/p&gt;

&lt;p&gt;@@ -117,6 +117,7 @@ public void shouldFanoutTheInput() throws Exception {&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());&lt;br/&gt;
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, cacheSizeBytes);&lt;br/&gt;
+        streamsConfiguration.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;/p&gt;

&lt;p&gt;         final KStream&amp;lt;byte[], String&amp;gt; stream1 = builder.stream(INPUT_TOPIC_A);&lt;br/&gt;
         final KStream&amp;lt;byte[], String&amp;gt; stream2 = stream1.mapValues(&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index ab08dbe0f59..eeb455bcc1e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -122,8 +122,8 @@ public void shouldReduce() throws Exception {&lt;/p&gt;

&lt;p&gt;         List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new StringDeserializer()&lt;/li&gt;
	&lt;li&gt;, 5);&lt;br/&gt;
+            new StringDeserializer(),&lt;br/&gt;
+            5);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
@@ -172,8 +172,8 @@ public String apply(Windowed&amp;lt;String&amp;gt; windowedKey, String value) {&lt;/p&gt;

&lt;p&gt;         List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; windowedOutput = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new StringDeserializer()&lt;/li&gt;
	&lt;li&gt;, 10);&lt;br/&gt;
+            new StringDeserializer(),&lt;br/&gt;
+            10);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;&lt;br/&gt;
             comparator =&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java&lt;br/&gt;
index e5560c1b62f..383a79363a8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java&lt;br/&gt;
@@ -97,11 +97,10 @@ public void before() {&lt;br/&gt;
         streamsConfiguration = new Properties();&lt;br/&gt;
         final String applicationId = &quot;kgrouped-stream-test-&quot; + testNo;&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;streamsConfiguration&lt;/li&gt;
	&lt;li&gt;.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, CLUSTER.zKConnectString());&lt;br/&gt;
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;/li&gt;
	&lt;li&gt;streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());&lt;br/&gt;
+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1);&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, cacheSizeBytes);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -155,8 +154,8 @@ public void shouldReduce() throws Exception {&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new StringDeserializer()&lt;/li&gt;
	&lt;li&gt;, 10);&lt;br/&gt;
+            new StringDeserializer(),&lt;br/&gt;
+            10);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
@@ -209,8 +208,8 @@ public String apply(final Windowed&amp;lt;String&amp;gt; windowedKey, final String value) {&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; windowedOutput = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new StringDeserializer()&lt;/li&gt;
	&lt;li&gt;, 15);&lt;br/&gt;
+            new StringDeserializer(),&lt;br/&gt;
+            15);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;&lt;br/&gt;
             comparator =&lt;br/&gt;
@@ -263,8 +262,8 @@ public void shouldAggregate() throws Exception {&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new IntegerDeserializer()&lt;/li&gt;
	&lt;li&gt;, 10);&lt;br/&gt;
+            new IntegerDeserializer(),&lt;br/&gt;
+            10);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
@@ -313,8 +312,8 @@ public String apply(final Windowed&amp;lt;String&amp;gt; windowedKey, final Integer value) {&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt; windowedMessages = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new IntegerDeserializer()&lt;/li&gt;
	&lt;li&gt;, 15);&lt;br/&gt;
+            new IntegerDeserializer(),&lt;br/&gt;
+            15);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Comparator&amp;lt;KeyValue&amp;lt;String, Integer&amp;gt;&amp;gt;&lt;br/&gt;
             comparator =&lt;br/&gt;
@@ -364,8 +363,8 @@ public void shouldCount() throws Exception {&lt;/p&gt;

&lt;p&gt;         final List&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new LongDeserializer()&lt;/li&gt;
	&lt;li&gt;, 10);&lt;br/&gt;
+            new LongDeserializer(),&lt;br/&gt;
+            10);&lt;br/&gt;
         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public int compare(final KeyValue&amp;lt;String, Long&amp;gt; o1, final KeyValue&amp;lt;String, Long&amp;gt; o2) {&lt;br/&gt;
@@ -406,8 +405,8 @@ public String apply(final Windowed&amp;lt;Integer&amp;gt; windowedKey, final Long value) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
             new StringDeserializer(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new LongDeserializer()&lt;/li&gt;
	&lt;li&gt;, 10);&lt;br/&gt;
+            new LongDeserializer(),&lt;br/&gt;
+            10);&lt;br/&gt;
         Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public int compare(final KeyValue&amp;lt;String, Long&amp;gt; o1, final KeyValue&amp;lt;String, Long&amp;gt; o2) {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationTest.java&lt;br/&gt;
index 5f85536efe3..7848d1b3f2b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationTest.java&lt;br/&gt;
@@ -260,7 +260,7 @@ private Properties prepareTest() throws Exception {&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, APP_ID + testNo);&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, CLUSTER.zKConnectString());&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());&lt;br/&gt;
+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.Long().getClass());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 8);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
index e46a016447d..8a1e13a1925 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
@@ -37,6 +37,7 @@&lt;br/&gt;
 import org.apache.kafka.test.MockProcessorSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockStateStoreSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockTimestampExtractor;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.Assert;&lt;br/&gt;
 import org.junit.Test;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -99,13 +100,14 @@ private Properties configProps() &lt;/p&gt;
{
                 setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:2171&quot;);
                 setProperty(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, &quot;3&quot;);
                 setProperty(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName());
+                setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
             }
&lt;p&gt;         };&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testSubscription() throws Exception {&lt;br/&gt;
+    public void testSubscription() {&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(configProps());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         TopologyBuilder builder = new TopologyBuilder();&lt;br/&gt;
@@ -148,7 +150,7 @@ public void testSubscription() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignBasic() throws Exception {&lt;br/&gt;
+    public void testAssignBasic() {&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(configProps());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         TopologyBuilder builder = new TopologyBuilder();&lt;br/&gt;
@@ -215,7 +217,7 @@ public void testAssignBasic() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
+    public void testAssignWithNewTasks() {&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(configProps());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         TopologyBuilder builder = new TopologyBuilder();&lt;br/&gt;
@@ -274,7 +276,7 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStates() throws Exception {&lt;br/&gt;
+    public void testAssignWithStates() {&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(configProps());&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         TopologyBuilder builder = new TopologyBuilder();&lt;br/&gt;
@@ -334,7 +336,7 @@ public void testAssignWithStates() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
+    public void testAssignWithStandbyReplicas() 
{
         Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);
         StreamsConfig config = new StreamsConfig(props);
@@ -449,7 +451,7 @@ private AssignmentInfo checkAssignment(PartitionAssignor.Assignment assignment)
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testOnAssignment() throws Exception {&lt;br/&gt;
+    public void testOnAssignment() {&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(configProps());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         TopicPartition t2p3 = new TopicPartition(&quot;topic2&quot;, 3);&lt;br/&gt;
@@ -482,7 +484,7 @@ public void testOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopics() {&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(configProps());&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         TopologyBuilder builder = new TopologyBuilder();&lt;br/&gt;
@@ -521,7 +523,7 @@ public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() 
{
         StreamsConfig config = new StreamsConfig(configProps());
         String applicationId = &quot;test&quot;;
         TopologyBuilder builder = new TopologyBuilder();
@@ -555,7 +557,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
         subscriptions.put(&quot;consumer10&quot;,
                           new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));
 
-        Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);
+        partitionAssignor.assign(metadata, subscriptions);
 
         // check prepared internal topics
         assertEquals(2, internalTopicManager.readyTopics.size());
@@ -563,7 +565,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
+    public void shouldAddUserDefinedEndPointToSubscription() {&lt;br/&gt;
         final Properties properties = configProps();&lt;br/&gt;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, &quot;localhost:8080&quot;);&lt;br/&gt;
         final StreamsConfig config = new StreamsConfig(properties);&lt;br/&gt;
@@ -589,7 +591,70 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() 
{
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            &quot;consumer1&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            &quot;consumer2&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        StreamsConfig config = new StreamsConfig(configProps());
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            new MockClientSupplier(),
+            &quot;applicationId&quot;,
+            &quot;clientId&quot;,
+            UUID.randomUUID(),
+            new Metrics(),
+            new SystemTime(),
+            new StreamsMetadataState(builder));
+
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, &quot;test&quot;, &quot;clientId&quot;));
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscription() &lt;/p&gt;
{
+        final Properties properties = configProps();
+        properties.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        final StreamsConfig config = new StreamsConfig(properties);
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        builder.addSource(&quot;source1&quot;, &quot;topic1&quot;);
+
+        final String clientId = &quot;client-id&quot;;
+        final StreamThread thread = new StreamThread(builder, config, new MockClientSupplier(), &quot;test&quot;, clientId,
+            UUID.randomUUID(), new Metrics(), new SystemTime(), new StreamsMetadataState(builder));
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        partitionAssignor.configure(config.getConsumerConfigs(thread, &quot;test&quot;, clientId));
+
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldMapUserEndPointToTopicPartitions() {&lt;br/&gt;
         final Properties properties = configProps();&lt;br/&gt;
         final String myEndPoint = &quot;localhost:8080&quot;;&lt;br/&gt;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);&lt;br/&gt;
@@ -628,7 +693,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() 
{
         final Properties properties = configProps();
         final String myEndPoint = &quot;localhost&quot;;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -655,7 +720,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() thr
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() 
{
         final Properties properties = configProps();
         final String myEndPoint = &quot;localhost:j87yhk&quot;;
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -682,7 +747,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() th
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() 
{
         final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
         List&amp;lt;TopicPartition&amp;gt; topic = Arrays.asList(new TopicPartition(&quot;topic&quot;, 0));
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState =
@@ -696,7 +761,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSetClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldSetClusterMetadataOnAssignment() {&lt;br/&gt;
         final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;TopicPartition&amp;gt; topic = Arrays.asList(new TopicPartition(&quot;topic&quot;, 0));&lt;br/&gt;
@@ -718,7 +783,7 @@ public void shouldSetClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() throws Exception {&lt;br/&gt;
+    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() {&lt;br/&gt;
         final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();&lt;br/&gt;
         final Cluster cluster = partitionAssignor.clusterMetadata();&lt;br/&gt;
         assertNotNull(cluster);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
index 2f252e9fa60..2ea57388f2c 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
@@ -17,11 +17,6 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; package org.apache.kafka.streams.processor.internals;&lt;/p&gt;

&lt;p&gt;-import static org.junit.Assert.assertEquals;&lt;br/&gt;
-import static org.junit.Assert.assertFalse;&lt;br/&gt;
-import static org.junit.Assert.assertSame;&lt;br/&gt;
-import static org.junit.Assert.assertTrue;&lt;br/&gt;
-&lt;br/&gt;
 import org.apache.kafka.clients.consumer.Consumer;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.internals.PartitionAssignor;&lt;br/&gt;
@@ -40,6 +35,7 @@&lt;br/&gt;
 import org.apache.kafka.test.MockClientSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockProcessorSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockTimestampExtractor;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt; import java.io.File;&lt;br/&gt;
@@ -56,6 +52,11 @@&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;/p&gt;

&lt;p&gt;+import static org.junit.Assert.assertEquals;&lt;br/&gt;
+import static org.junit.Assert.assertFalse;&lt;br/&gt;
+import static org.junit.Assert.assertSame;&lt;br/&gt;
+import static org.junit.Assert.assertTrue;&lt;br/&gt;
+&lt;br/&gt;
 public class StreamThreadTest {&lt;/p&gt;

&lt;p&gt;     private final String clientId = &quot;clientId&quot;;&lt;br/&gt;
@@ -117,6 +118,7 @@ private Properties configProps() &lt;/p&gt;
{
                 setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:2171&quot;);
                 setProperty(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, &quot;3&quot;);
                 setProperty(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName());
+                setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
             }
&lt;p&gt;         };&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index ce94a23a2c9..6c94c18991a 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -65,10 +65,9 @@ public void shouldDecodePreviousVersion() throws Exception &lt;/p&gt;
{
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHostState.size()); // should be empty as wasn&apos;t in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }

&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;/li&gt;
	&lt;li&gt;so it is impossible to test compatibility without having this&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestClient.java&lt;br/&gt;
index f920c515c0e..63ad01d8609 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestClient.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestClient.java&lt;br/&gt;
@@ -105,7 +105,7 @@ public boolean test(String key, Integer value) {&lt;br/&gt;
             }&lt;br/&gt;
         });&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;data.process(SmokeTestUtil.&amp;lt;Integer&amp;gt;printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        data.process(SmokeTestUtil.&amp;lt;String, Integer&amp;gt;printProcessorSupplier(&quot;data&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // min&lt;br/&gt;
         KGroupedStream&amp;lt;String, Integer&amp;gt;&lt;br/&gt;
@@ -131,7 +131,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {&lt;br/&gt;
         ).to(stringSerde, intSerde, &quot;min&quot;);&lt;/p&gt;

&lt;p&gt;         KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(stringSerde, intSerde, &quot;min&quot;, &quot;minStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;minTable.toStream().process(SmokeTestUtil.&amp;lt;Integer&amp;gt;printProcessorSupplier(&quot;min&quot;));&lt;br/&gt;
+        minTable.toStream().process(SmokeTestUtil.&amp;lt;String, Integer&amp;gt;printProcessorSupplier(&quot;min&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // max&lt;br/&gt;
         groupedData.aggregate(&lt;br/&gt;
@@ -153,7 +153,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {&lt;br/&gt;
         ).to(stringSerde, intSerde, &quot;max&quot;);&lt;/p&gt;

&lt;p&gt;         KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(stringSerde, intSerde, &quot;max&quot;, &quot;maxStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;maxTable.toStream().process(SmokeTestUtil.&amp;lt;Integer&amp;gt;printProcessorSupplier(&quot;max&quot;));&lt;br/&gt;
+        maxTable.toStream().process(SmokeTestUtil.&amp;lt;String, Integer&amp;gt;printProcessorSupplier(&quot;max&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // sum&lt;br/&gt;
         groupedData.aggregate(&lt;br/&gt;
@@ -176,7 +176,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {&lt;/p&gt;


&lt;p&gt;         KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(stringSerde, longSerde, &quot;sum&quot;, &quot;sumStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sumTable.toStream().process(SmokeTestUtil.&amp;lt;Long&amp;gt;printProcessorSupplier(&quot;sum&quot;));&lt;br/&gt;
+        sumTable.toStream().process(SmokeTestUtil.&amp;lt;String, Long&amp;gt;printProcessorSupplier(&quot;sum&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // cnt&lt;br/&gt;
         groupedData.count(UnlimitedWindows.of(), &quot;uwin-cnt&quot;)&lt;br/&gt;
@@ -185,7 +185,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {&lt;br/&gt;
         ).to(stringSerde, longSerde, &quot;cnt&quot;);&lt;/p&gt;

&lt;p&gt;         KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(stringSerde, longSerde, &quot;cnt&quot;, &quot;cntStoreName&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cntTable.toStream().process(SmokeTestUtil.&amp;lt;Long&amp;gt;printProcessorSupplier(&quot;cnt&quot;));&lt;br/&gt;
+        cntTable.toStream().process(SmokeTestUtil.&amp;lt;String, Long&amp;gt;printProcessorSupplier(&quot;cnt&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // dif&lt;br/&gt;
         maxTable.join(minTable,&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java b/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java&lt;br/&gt;
index f9d30d50e36..f103355ac9e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java&lt;br/&gt;
@@ -125,38 +125,48 @@ public void run() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(String kafka, final int numKeys, final int maxRecordsPerKey) throws Exception &lt;/p&gt;
{
-        Properties props = new Properties();
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey,&lt;br/&gt;
+                                                     final boolean autoTerminate) throws Exception {&lt;br/&gt;
+        final Properties props = new Properties();&lt;br/&gt;
         props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;br/&gt;
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(props);&lt;br/&gt;
+        final KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(props);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         int numRecordsProduced = 0;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         for (int i = 0; i &amp;lt; numKeys; i++) 
{
             data[i] = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data[i].key, new HashSet&amp;lt;Integer&amp;gt;());
         }&lt;/li&gt;
	&lt;li&gt;Random rand = new Random();&lt;br/&gt;
+        final Random rand = new Random();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int remaining = data.length;&lt;br/&gt;
+        int remaining = 1; // dummy value must be positive if &amp;lt;autoTerminate&amp;gt; is false&lt;br/&gt;
+        if (autoTerminate) 
{
+            remaining = data.length;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         while (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int index = rand.nextInt(remaining);&lt;/li&gt;
	&lt;li&gt;String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);&lt;br/&gt;
+            final String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
             int value = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.next();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (value &amp;lt; 0) {&lt;br/&gt;
+            if (autoTerminate &amp;amp;&amp;amp; value &amp;lt; 0) 
{
                 remaining--;
                 data[index] = data[remaining];
                 value = END;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;br/&gt;
+            final ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;br/&gt;
                     new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             producer.send(record);&lt;br/&gt;
@@ -165,8 +175,9 @@ public void run() {&lt;br/&gt;
                 numRecordsProduced++;&lt;br/&gt;
                 allData.get(key).add(value);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numRecordsProduced % 100 == 0)&lt;br/&gt;
+                if (numRecordsProduced % 100 == 0) 
{
                     System.out.println(numRecordsProduced + &quot; records produced&quot;);
+                }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 Thread.sleep(10);&lt;br/&gt;
             }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestUtil.java b/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestUtil.java&lt;br/&gt;
index 7ff738f1d95..d9ad745e169 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestUtil.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestUtil.java&lt;br/&gt;
@@ -37,27 +37,20 @@&lt;br/&gt;
     public final static long START_TIME = 60000L * 60 * 24 * 365 * 30;&lt;br/&gt;
     public final static int END = Integer.MAX_VALUE;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static &amp;lt;T&amp;gt; ProcessorSupplier&amp;lt;String, T&amp;gt; printProcessorSupplier(final String topic) 
{
-        return printProcessorSupplier(topic, false);
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static &amp;lt;T&amp;gt; ProcessorSupplier&amp;lt;String, T&amp;gt; printProcessorSupplier(final String topic, final boolean printOffset) {&lt;/li&gt;
	&lt;li&gt;return new ProcessorSupplier&amp;lt;String, T&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;public Processor&amp;lt;String, T&amp;gt; get() {&lt;/li&gt;
	&lt;li&gt;return new AbstractProcessor&amp;lt;String, T&amp;gt;() {&lt;br/&gt;
+    public static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier(final String topic) {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
                     private int numRecordsProcessed = 0;&lt;/li&gt;
	&lt;li&gt;private ProcessorContext context;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;br/&gt;
                     public void init(ProcessorContext context) &lt;/p&gt;
{
                         System.out.println(&quot;initializing processor: topic=&quot; + topic + &quot; taskId=&quot; + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }

&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void process(String key, T value) {&lt;/li&gt;
	&lt;li&gt;if (printOffset) System.out.println(&quot;&amp;gt;&amp;gt;&amp;gt; &quot; + context.offset());&lt;br/&gt;
+                    public void process(K key, V value) {&lt;br/&gt;
                         numRecordsProcessed++;&lt;br/&gt;
                         if (numRecordsProcessed % 100 == 0) {&lt;br/&gt;
                             System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=&quot; + topic);&lt;br/&gt;
@@ -65,12 +58,10 @@ public void process(String key, T value) {&lt;br/&gt;
                     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void punctuate(long timestamp) 
{
-                    }&lt;br/&gt;
+                    public void punctuate(long timestamp) {}&lt;br/&gt;
 &lt;br/&gt;
                     @Override&lt;br/&gt;
-                    public void close() {-                    }
&lt;p&gt;+                    public void close() {}&lt;br/&gt;
                 };&lt;br/&gt;
             }&lt;br/&gt;
         };&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/smoketest/StreamsSmokeTest.java b/streams/src/test/java/org/apache/kafka/streams/smoketest/StreamsSmokeTest.java&lt;br/&gt;
index c26544e4fc5..3328ae5e480 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/smoketest/StreamsSmokeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/smoketest/StreamsSmokeTest.java&lt;br/&gt;
@@ -24,7 +24,7 @@&lt;br/&gt;
 public class StreamsSmokeTest {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  args ::= command kafka zookeeper stateDir&lt;br/&gt;
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;command := &quot;run&quot; | &quot;process&quot;&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;@param args&lt;br/&gt;
@@ -34,12 +34,14 @@ public static void main(String[] args) throws Exception {&lt;br/&gt;
         String kafka = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
         String zookeeper = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
         String stateDir = args.length &amp;gt; 3 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+        boolean disableAutoTerminate = args.length &amp;gt; 4;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.out.println(&quot;StreamsSmokeTest instance started&quot;);&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsSmokeTest)&quot;);&lt;br/&gt;
         System.out.println(&quot;command=&quot; + command);&lt;br/&gt;
         System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
         System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
         System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;disableAutoTerminate=&quot; + disableAutoTerminate);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         switch (command) {&lt;br/&gt;
             case &quot;standalone&quot;:&lt;br/&gt;
@@ -49,8 +51,12 @@ public static void main(String[] args) throws Exception {&lt;br/&gt;
                 // this starts the driver (data generation and result verification)&lt;br/&gt;
                 final int numKeys = 10;&lt;br/&gt;
                 final int maxRecordsPerKey = 500;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);&lt;/li&gt;
	&lt;li&gt;SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);&lt;br/&gt;
+                if (disableAutoTerminate) 
{
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
&lt;p&gt;                 break;&lt;br/&gt;
             case &quot;process&quot;:&lt;br/&gt;
                 // this starts a KafkaStreams client&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..e771bead6e8&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,78 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.smoketest.SmokeTestUtil;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 3 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest trunk)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java b/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java&lt;br/&gt;
index 83a9092fa20..1bedd870032 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java&lt;br/&gt;
@@ -345,7 +345,7 @@ public synchronized long position(TopicPartition partition) {
             // consumer.subscribe(new TopicPartition(topicName, 1));
             // Set up the partition that matches the ID (which is what ProcessorStateManager expects) ...
             List&amp;lt;PartitionInfo&amp;gt; partitionInfos = new ArrayList&amp;lt;&amp;gt;();
-            partitionInfos.add(new PartitionInfo(topicName , id.partition, null, null, null));
+            partitionInfos.add(new PartitionInfo(topicName, id.partition, null, null, null));
             consumer.updatePartitions(topicName, partitionInfos);
             consumer.updateEndOffsets(Collections.singletonMap(new TopicPartition(topicName, id.partition), 0L));
         }&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..7d3ed436881&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                System.out.println(&quot;closing Kafka Streams instance&quot;);+                System.out.flush();+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index a63810e2bd4..b857bd53224 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -33,6 +33,8 @@ class StreamsSmokeTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, &quot;tools-log4j.properties&quot;)&lt;br/&gt;
     PID_FILE = os.path.join(PERSISTENT_ROOT, &quot;streams.pid&quot;)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    CLEAN_NODE_ENABLED = True&lt;br/&gt;
+&lt;br/&gt;
     logs = {&lt;br/&gt;
         &quot;streams_log&quot;: {&lt;br/&gt;
             &quot;path&quot;: LOG_FILE,&lt;br/&gt;
@@ -43,6 +45,114 @@ class StreamsSmokeTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
         &quot;streams_stderr&quot;: &lt;/p&gt;
{
             &quot;path&quot;: STDERR_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, context, kafka, command):&lt;br/&gt;
@@ -95,7 +205,8 @@ def wait(self):&lt;/p&gt;

&lt;p&gt;     def clean_node(self, node):&lt;br/&gt;
         node.account.kill_process(&quot;streams&quot;, clean_shutdown=False, allow_fail=True)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if self.CLEAN_NODE_ENABLED:&lt;br/&gt;
+            node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def start_cmd(self, node):&lt;br/&gt;
         args = self.args.copy()&lt;br/&gt;
@@ -120,10 +231,10 @@ def start_node(self, node):&lt;/p&gt;

&lt;p&gt;         node.account.create_file(self.LOG4J_CONFIG_FILE, self.render(&apos;tools_log4j.properties&apos;, log_file=self.LOG_FILE))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.logger.info(&quot;Starting StreamsSmokeTest process on &quot; + str(node.account))&lt;br/&gt;
+        self.logger.info(&quot;Starting StreamsTest process on &quot; + str(node.account))&lt;br/&gt;
         with node.account.monitor_log(self.STDOUT_FILE) as monitor:&lt;br/&gt;
             node.account.ssh(self.start_cmd(node))&lt;/li&gt;
	&lt;li&gt;monitor.wait_until(&apos;StreamsSmokeTest instance started&apos;, timeout_sec=15, err_msg=&quot;Never saw message indicating StreamsSmokeTest finished startup on &quot; + str(node.account))&lt;br/&gt;
+            monitor.wait_until(&apos;StreamsTest instance started&apos;, timeout_sec=15, err_msg=&quot;Never saw message indicating StreamsTest finished startup on &quot; + str(node.account))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if len(self.pids(node)) == 0:&lt;br/&gt;
             raise RuntimeError(&quot;No process ids recorded&quot;)&lt;br/&gt;
@@ -132,8 +243,62 @@ def start_node(self, node):&lt;br/&gt;
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, context, kafka):&lt;br/&gt;
         super(StreamsSmokeTestDriverService, self)._&lt;em&gt;init&lt;/em&gt;_(context, kafka, &quot;run&quot;)&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;&quot;&lt;/p&gt;

&lt;p&gt;+    def disable_auto_terminate(self):&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;disableAutoTerminate&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;disable_auto_terminate&amp;#39;&amp;#93;&lt;/span&gt; = self.DISABLE_AUTO_TERMINATE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true %(kafka_run_class)s org.apache.kafka.streams.smoketest.StreamsSmokeTest &quot; \&lt;br/&gt;
+              &quot; %(command)s %(kafka)s %(zk)s %(state_dir)s %(disable_auto_terminate)s &quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;/p&gt;

&lt;p&gt; class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, context, kafka):&lt;br/&gt;
         super(StreamsSmokeTestJobRunnerService, self)._&lt;em&gt;init&lt;/em&gt;_(context, kafka, &quot;process&quot;)&lt;br/&gt;
+&lt;br/&gt;
+class StreamsUpgradeTestJobRunnerService(StreamsSmokeTestBaseService):&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, context, kafka):&lt;br/&gt;
+        super(StreamsUpgradeTestJobRunnerService, self)._&lt;em&gt;init&lt;/em&gt;_(context, kafka, &quot;&quot;)&lt;br/&gt;
+        self.UPGRADE_FROM = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def set_version(self, kafka_streams_version):&lt;br/&gt;
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version&lt;br/&gt;
+&lt;br/&gt;
+    def set_upgrade_from(self, upgrade_from):&lt;br/&gt;
+        self.UPGRADE_FROM = upgrade_from&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;version&amp;#39;&amp;#93;&lt;/span&gt; = self.KAFKA_STREAMS_VERSION&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;upgrade_from&amp;#39;&amp;#93;&lt;/span&gt; = self.UPGRADE_FROM&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot; INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s &quot; \&lt;br/&gt;
+              &quot; %(kafka_run_class)s org.apache.kafka.streams.tests.StreamsUpgradeTest &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(zk)s %(state_dir)s %(upgrade_from)s &quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..8266e073d13&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -0,0 +1,200 @@&lt;br/&gt;
+# Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+# contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+# this work for additional information regarding copyright ownership.&lt;br/&gt;
+# The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+# (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+# the License.  You may obtain a copy of the License at&lt;br/&gt;
+#&lt;br/&gt;
+#    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+#&lt;br/&gt;
+# Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+# See the License for the specific language governing permissions and&lt;br/&gt;
+# limitations under the License.&lt;br/&gt;
+&lt;br/&gt;
+from kafkatest.tests.kafka_test import KafkaTest&lt;br/&gt;
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsUpgradeTestJobRunnerService&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, TRUNK_VERSION&lt;br/&gt;
+import random&lt;br/&gt;
+&lt;br/&gt;
+class StreamsUpgradeTest(KafkaTest):&lt;br/&gt;
+    &quot;&quot;&quot;&lt;br/&gt;
+    Test upgrading Kafka Streams from 0.10.0.x to 0.10.1.x (ie, TRUNK)&lt;br/&gt;
+    &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
+        super(StreamsUpgradeTest, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, num_zk=1, num_brokers=1, topics={&lt;br/&gt;
+            &apos;echo&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+            &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;+        })&lt;br/&gt;
+&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)&lt;br/&gt;
+&lt;br/&gt;
+    def test_upgrade(self):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to 0.10.1&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with_0100()&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # first rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;0.10.0&quot;, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # second rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    def start_all_nodes_with_0100(self):&lt;br/&gt;
+        # start first with 0.10.0&lt;br/&gt;
+        self.prepare_for_0100(self.processor1)&lt;br/&gt;
+        node1 = self.processor1.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:&lt;br/&gt;
+                self.processor1.start()&lt;br/&gt;
+                log_monitor.wait_until(&quot;Kafka version : 0.10.0.1&quot;,&lt;br/&gt;
+                                       timeout_sec=60,&lt;br/&gt;
+                                       err_msg=&quot;Could not detect Kafka Streams version 0.10.0.1&quot; + str(node1.account))&lt;br/&gt;
+                monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start second with 0.10.0&lt;br/&gt;
+        self.prepare_for_0100(self.processor2)&lt;br/&gt;
+        node2 = self.processor2.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:&lt;br/&gt;
+                    self.processor2.start()&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Kafka version : 0.10.0.1&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect Kafka Streams version 0.10.0.1&quot; + str(node2.account))&lt;br/&gt;
+                    first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                             timeout_sec=60,&lt;br/&gt;
+                                             err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                    second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                              timeout_sec=60,&lt;br/&gt;
+                                              err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start third with 0.10.0&lt;br/&gt;
+        self.prepare_for_0100(self.processor3)&lt;br/&gt;
+        node3 = self.processor3.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:&lt;br/&gt;
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:&lt;br/&gt;
+                        self.processor3.start()&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : 0.10.0.1&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version 0.10.0.1&quot; + str(node3.account))&lt;br/&gt;
+                        first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                 timeout_sec=60,&lt;br/&gt;
+                                                 err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                        second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+                        third_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node3.account))&lt;br/&gt;
+&lt;br/&gt;
+    @staticmethod&lt;br/&gt;
+    def prepare_for_0100(processor):&lt;br/&gt;
+        processor.node.account.ssh(&quot;rm -rf &quot; + processor.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        processor.set_version(str(LATEST_0_10_0))&lt;br/&gt;
+&lt;br/&gt;
+    def do_rolling_bounce(self, processor, upgrade_from, counter):&lt;br/&gt;
+        first_other_processor = None&lt;br/&gt;
+        second_other_processor = None&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            if p != processor:&lt;br/&gt;
+                if first_other_processor is None:&lt;br/&gt;
+                    first_other_processor = p&lt;br/&gt;
+                else:&lt;br/&gt;
+                    second_other_processor = p&lt;br/&gt;
+&lt;br/&gt;
+        node = processor.node&lt;br/&gt;
+        first_other_node = first_other_processor.node&lt;br/&gt;
+        second_other_node = second_other_processor.node&lt;br/&gt;
+&lt;br/&gt;
+        # stop processor and wait for rebalance of others&lt;br/&gt;
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                processor.stop()&lt;br/&gt;
+                first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                timeout_sec=60,&lt;br/&gt;
+                                                err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+        node.account.ssh_capture(&quot;grep UPGRADE-TEST-CLIENT-CLOSED %s&quot; % processor.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if upgrade_from == &quot;&quot;:  # upgrade disabled &amp;#8211; second round of rolling bounces&lt;br/&gt;
+            roll_counter = &quot;.1-&quot;  # second round of rolling bounces&lt;br/&gt;
+        else:&lt;br/&gt;
+            roll_counter = &quot;.0-&quot;  # first  round of rolling boundes&lt;br/&gt;
+&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDOUT_FILE + &quot; &quot; + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDERR_FILE + &quot; &quot; + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.LOG_FILE + &quot; &quot; + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        processor.set_upgrade_from(upgrade_from)&lt;br/&gt;
+&lt;br/&gt;
+        grep_metadata_error = &quot;grep \&quot;org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\&quot; &quot;&lt;br/&gt;
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:&lt;br/&gt;
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                        processor.start()&lt;br/&gt;
+&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + str(TRUNK_VERSION),&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + str(TRUNK_VERSION) + &quot; &quot; + str(node.account))&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                       timeout_sec=60,&lt;br/&gt;
+                                                       err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                        timeout_sec=60,&lt;br/&gt;
+                                                        err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node.account))&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index 239a9f45d37..ebf5ecf6a51 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -64,6 +64,7 @@ def get_version(node=None):&lt;br/&gt;
         return TRUNK&lt;/p&gt;

&lt;p&gt; TRUNK = KafkaVersion(&quot;trunk&quot;)&lt;br/&gt;
+TRUNK_VERSION = KafkaVersion(&quot;0.10.1.2-SNAPSHOT&quot;)&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;0.8.2.X versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;br/&gt;
@@ -78,7 +79,11 @@ def get_version(node=None):&lt;/li&gt;
	&lt;li&gt;0.10.0.X versions&lt;br/&gt;
 V_0_10_0_0 = KafkaVersion(&quot;0.10.0.0&quot;)&lt;br/&gt;
 V_0_10_0_1 = KafkaVersion(&quot;0.10.0.1&quot;)
	&lt;ol&gt;
		&lt;li&gt;Adding 0.10.0 as the next version will be 0.10.1.x&lt;br/&gt;
 LATEST_0_10_0 = V_0_10_0_1&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;-LATEST_0_10 = LATEST_0_10_0&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
+# 0.10.1.X versions&lt;br/&gt;
+V_0_10_1_0 = KafkaVersion(&quot;0.10.1.0&quot;)&lt;br/&gt;
+V_0_10_1_1 = KafkaVersion(&quot;0.10.1.1&quot;)&lt;br/&gt;
+LATEST_0_10_1 = V_0_10_1_1&lt;br/&gt;
+&lt;br/&gt;
+LATEST_0_10 = LATEST_0_10_1&lt;br/&gt;
diff --git a/vagrant/base.sh b/vagrant/base.sh&lt;br/&gt;
index 88878dcc2ed..b8cde3a48cc 100755&lt;br/&gt;
&amp;#8212; a/vagrant/base.sh&lt;br/&gt;
+++ b/vagrant/base.sh&lt;br/&gt;
@@ -52,6 +52,8 @@ get_kafka() {&lt;/p&gt;

&lt;p&gt;     kafka_dir=/opt/kafka-$version&lt;br/&gt;
     url=&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages-$version/kafka_2.10-$version.tgz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3-us-west-2.amazonaws.com/kafka-packages-$version/kafka_2.10-$version.tgz&lt;/a&gt;&lt;br/&gt;
+    # the .tgz above does not include the streams test jar hence we need to get it separately&lt;br/&gt;
+    url_streams_test=&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages/kafka-streams-$version-test.jar&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3-us-west-2.amazonaws.com/kafka-packages/kafka-streams-$version-test.jar&lt;/a&gt;&lt;br/&gt;
     if [ ! &lt;del&gt;d /opt/kafka&lt;/del&gt;$version ]; then&lt;br/&gt;
         pushd /tmp&lt;br/&gt;
         curl -O $url&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16423359" author="githubbot" created="Tue, 3 Apr 2018 01:38:48 +0000"  >&lt;p&gt;mjsax closed pull request #4768:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4768&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4768&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh&lt;br/&gt;
index fc89f25d9d2..81b423afa1e 100755&lt;br/&gt;
&amp;#8212; a/bin/kafka-run-class.sh&lt;br/&gt;
+++ b/bin/kafka-run-class.sh&lt;br/&gt;
@@ -73,28 +73,50 @@ do&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/clients/build/libs/kafka-clients*.jar;&lt;br/&gt;
-do&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if should_include_file &quot;$file&quot;; then&lt;/li&gt;
	&lt;li&gt;CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;/li&gt;
	&lt;li&gt;fi&lt;br/&gt;
-done&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  clients_lib_dir=$(dirname $0)/../clients/build/libs&lt;br/&gt;
+  streams_lib_dir=$(dirname $0)/../streams/build/libs&lt;br/&gt;
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}&lt;br/&gt;
+else&lt;br/&gt;
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs&lt;br/&gt;
+  streams_lib_dir=$clients_lib_dir&lt;br/&gt;
+  rocksdb_lib_dir=$streams_lib_dir&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/build/libs/kafka-streams*.jar;&lt;br/&gt;
+for file in &quot;$clients_lib_dir&quot;/kafka-clients*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+for file in &quot;$streams_lib_dir&quot;/kafka-streams*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;&lt;del&gt;for file in &quot;$base_dir&quot;/streams/build/dependant-libs&lt;/del&gt;${SCALA_VERSION}/rocksdb*.jar;&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+else&lt;br/&gt;
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed &apos;s/\.//g&apos;`&lt;br/&gt;
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;(${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;br/&gt;
+for file in &quot;$rocksdb_lib_dir&quot;/rocksdb*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
 done&lt;br/&gt;
diff --git a/build.gradle b/build.gradle&lt;br/&gt;
index 7a3b4119f81..727bdeae8a5 100644&lt;br/&gt;
&amp;#8212; a/build.gradle&lt;br/&gt;
+++ b/build.gradle&lt;br/&gt;
@@ -961,6 +961,54 @@ project(&apos;:streams:examples&apos;) {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;+project(&apos;:streams:upgrade-system-tests-0100&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0100&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0100
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0101&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0101&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0102&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0102&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0102
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0110&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0110&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0110
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
 project(&apos;:jmh-benchmarks&apos;) {&lt;/p&gt;

&lt;p&gt;   apply plugin: &apos;com.github.johnrengelman.shadow&apos;&lt;br/&gt;
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml&lt;br/&gt;
index acff3f625bc..bc7a2bd5779 100644&lt;br/&gt;
&amp;#8212; a/checkstyle/suppressions.xml&lt;br/&gt;
+++ b/checkstyle/suppressions.xml&lt;br/&gt;
@@ -181,7 +181,7 @@&lt;br/&gt;
               files=&quot;SmokeTestDriver.java&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;     &amp;lt;suppress checks=&quot;NPathComplexity&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;files=&quot;KStreamKStreamJoinTest.java&quot;/&amp;gt;&lt;br/&gt;
+              files=&quot;KStreamKStreamJoinTest.java|SmokeTestDriver.java&quot;/&amp;gt;&lt;br/&gt;
     &amp;lt;suppress checks=&quot;NPathComplexity&quot;&lt;br/&gt;
               files=&quot;KStreamKStreamLeftJoinTest.java&quot;/&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
index 4756387bba8..4853f92ed6a 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
@@ -16,7 +16,8 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.security.authenticator;&lt;/p&gt;

&lt;p&gt;-import java.util.Map;&lt;br/&gt;
+import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
+import org.apache.kafka.common.network.Mode;&lt;/p&gt;

&lt;p&gt; import javax.security.auth.Subject;&lt;br/&gt;
 import javax.security.auth.callback.Callback;&lt;br/&gt;
@@ -25,9 +26,7 @@&lt;br/&gt;
 import javax.security.auth.callback.UnsupportedCallbackException;&lt;br/&gt;
 import javax.security.sasl.AuthorizeCallback;&lt;br/&gt;
 import javax.security.sasl.RealmCallback;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
-import org.apache.kafka.common.network.Mode;&lt;br/&gt;
+import java.util.Map;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Callback handler for Sasl clients. The callbacks required for the SASL mechanism&lt;br/&gt;
diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html&lt;br/&gt;
index 87d34f127a7..58cb91c7e62 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/docs/streams/upgrade-guide.html&lt;br/&gt;
+++ b/docs/streams/upgrade-guide.html&lt;br/&gt;
@@ -34,28 +34,46 @@ &amp;lt;h1&amp;gt;Upgrade Guide &amp;amp; API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
     &amp;lt;/div&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.11.0.x to 1.0.0 you don&apos;t need to do any code changes as the public API is fully backward compatible.&lt;br/&gt;
+        If you want to upgrade from 0.10.2.x or 0.11.0.x to 1.0.x you don&apos;t need to do any code changes as the public API is fully backward compatible.&lt;br/&gt;
         However, some public APIs were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 1.0.0 API and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;1.0&amp;lt;/a&amp;gt; and &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;0.11.0&amp;lt;/a&amp;gt; API&lt;br/&gt;
+        and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        Additionally, Streams API 1.0.x requires broker on-disk message format version 0.10 or higher; thus, you need to make sure that the message&lt;br/&gt;
+        format is configured correctly before you upgrade your Kafka Streams application.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.2.x to 0.11.0 you don&apos;t need to do any code changes as the public API is fully backward compatible.&lt;/li&gt;
	&lt;li&gt;However, some configuration parameters were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.11.0 API and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        If you want to upgrade from 0.10.1.x to 1.0.x see the Upgrade Sections for &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1100_streams&quot;&amp;gt;&amp;lt;b&amp;gt;0.11.0&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_100_streams&quot;&amp;gt;&amp;lt;b&amp;gt;1.0&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;br/&gt;
+        Note, that a brokers on-disk message format must be on version 0.10 or higher to run a Kafka Streams application version 1.0 or higher.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;0.10.2&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        and &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;1.0&amp;lt;/a&amp;gt; API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.1.x to 0.10.2, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;/li&gt;
	&lt;li&gt;It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.2 API and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;/li&gt;
	&lt;li&gt;&amp;lt;/p&amp;gt;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;If you want to upgrade from 0.10.0.x to 0.10.1, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1010_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.1&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;/li&gt;
	&lt;li&gt;It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        Upgrading from 0.10.0.x to 1.0.x directly is also possible.&lt;br/&gt;
+        Note, that a brokers must be on version 0.10.1 or higher and on-disk message format must be on version 0.10 or higher&lt;br/&gt;
+        to run a Kafka Streams application version 1.0 or higher.&lt;br/&gt;
+        See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;, and &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0&amp;lt;/a&amp;gt;&lt;br/&gt;
+        for a complete list of API changes.&lt;br/&gt;
+        Upgrading to 1.0.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 1.0.2&amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;p&amp;gt; Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported) &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; restart all new (1.0.0 or 1.0.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_100&quot; href=&quot;#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;/p&gt;

&lt;p&gt;diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index caf8e1cb276..33b94d4299c 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -129,17 +129,77 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_new_protocols&quot; href=&quot;#upgrade_100_new_protocols&quot;&amp;gt;New Prot&lt;br/&gt;
          be used if the SaslHandshake request version is greater than 0. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;-&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams&quot; href=&quot;#upgrade_100_streams&quot;&amp;gt;Upgrading a 1.0.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams&quot; href=&quot;#upgrade_100_streams&quot;&amp;gt;Upgrading a 0.11.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.11.0 to 1.0.0 does not require a broker upgrade.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).&lt;/li&gt;
	&lt;li&gt;However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. &amp;lt;/li&amp;gt;&lt;br/&gt;
+         A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).&lt;br/&gt;
+         However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;/li&gt;
	&lt;li&gt;We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0102&quot; href=&quot;#upgrade_100_streams_from_0102&quot;&amp;gt;Upgrading a 0.10.2 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.2 to 1.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0101&quot; href=&quot;#upgrade_1100_streams_from_0101&quot;&amp;gt;Upgrading a 0.10.1 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.1 to 1.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the &amp;lt;code&amp;gt;TimestampExtractor&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you register custom metrics, you will need to update this code, because the &amp;lt;code&amp;gt;StreamsMetric&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0100&quot; href=&quot;#upgrade_100_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 1.0 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 1.0 application can only connect to 0.1, 0.11.0, 0.10.2, or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 1.0.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.11.0.3 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (1.0.0 or 1.0.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_102_notable&quot; href=&quot;#upgrade_102_notable&quot;&amp;gt;Notable changes in 1.0.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
 &amp;lt;p&amp;gt;Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below,&lt;br/&gt;
   you guarantee no downtime during the upgrade. However, please review the &amp;lt;a href=&quot;#upgrade_1100_notable&quot;&amp;gt;notable changes in 0.11.0.0&amp;lt;/a&amp;gt; before upgrading.&lt;br/&gt;
@@ -188,11 +248,55 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams&quot; href=&quot;#upgrade_1100_streams&quot;&amp;gt;Upgrading a 0.10.2 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.2 to 0.11.0 does not require a broker upgrade.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams_from_0101&quot; href=&quot;#upgrade_1100_streams_from_0101&quot;&amp;gt;Upgrading a 0.10.1 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.1 to 0.11.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the &amp;lt;code&amp;gt;TimestampExtractor&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you register custom metrics, you will need to update this code, because the &amp;lt;code&amp;gt;StreamsMetric&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams_from_0100&quot; href=&quot;#upgrade_1100_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.11.0 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.11.0 application can only connect to 0.11.0, 0.10.2, or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.11.0.3 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.11.0.3 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.11.0.0 , 0.11.0.1, or 0.11.0.2) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1103_notable&quot; href=&quot;#upgrade_1103_notable&quot;&amp;gt;Notable changes in 0.11.0.3&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_notable&quot; href=&quot;#upgrade_1100_notable&quot;&amp;gt;Notable changes in 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to&lt;br/&gt;
@@ -343,6 +447,35 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams&quot; href=&quot;#upgrade_1020_streams&quot;&amp;gt;Upgrading a 0.10.1&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams_from_0100&quot; href=&quot;#upgrade_1020_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.2 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.2.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.2.0 or 0.10.2.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10202_notable&quot; href=&quot;#upgrade_10202_notable&quot;&amp;gt;Notable changes in 0.10.2.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10201_notable&quot; href=&quot;#upgrade_10201_notable&quot;&amp;gt;Notable changes in 0.10.2.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
   &amp;lt;li&amp;gt; The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer &amp;lt;code&amp;gt;retries&amp;lt;/code&amp;gt; default value was changed from 0 to 10. The internal Kafka Streams consumer &amp;lt;code&amp;gt;max.poll.interval.ms&amp;lt;/code&amp;gt;  default value was changed from 300000 to &amp;lt;code&amp;gt;Integer.MAX_VALUE&amp;lt;/code&amp;gt;.&lt;br/&gt;
@@ -421,6 +554,23 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_streams&quot; href=&quot;#upgrade_1010_streams&quot;&amp;gt;Upgrading a 0.10.0&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.1 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.1.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.1.0 or 0.10.1.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_notable&quot; href=&quot;#upgrade_1010_notable&quot;&amp;gt;Notable changes in 0.10.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle&lt;br/&gt;
index cfb0b9bb39c..f7027cf9de4 100644&lt;br/&gt;
&amp;#8212; a/gradle/dependencies.gradle&lt;br/&gt;
+++ b/gradle/dependencies.gradle&lt;br/&gt;
@@ -59,6 +59,10 @@ versions += [&lt;br/&gt;
   log4j: &quot;1.2.17&quot;,&lt;br/&gt;
   jopt: &quot;5.0.4&quot;,&lt;br/&gt;
   junit: &quot;4.12&quot;,&lt;br/&gt;
+  kafka_0100: &quot;0.10.0.1&quot;,&lt;br/&gt;
+  kafka_0101: &quot;0.10.1.1&quot;,&lt;br/&gt;
+  kafka_0102: &quot;0.10.2.1&quot;,&lt;br/&gt;
+  kafka_0110: &quot;0.11.0.2&quot;,&lt;br/&gt;
   lz4: &quot;1.4&quot;,&lt;br/&gt;
   metrics: &quot;2.2.0&quot;,&lt;br/&gt;
   // PowerMock 1.x doesn&apos;t support Java 9, so use PowerMock 2.0.0 beta&lt;br/&gt;
@@ -95,11 +99,15 @@ libs += [&lt;br/&gt;
   jettyServlets: &quot;org.eclipse.jetty:jetty-servlets:$versions.jetty&quot;,&lt;br/&gt;
   jerseyContainerServlet: &quot;org.glassfish.jersey.containers:jersey-container-servlet:$versions.jersey&quot;,&lt;br/&gt;
   jmhCore: &quot;org.openjdk.jmh:jmh-core:$versions.jmh&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;jmhGeneratorAnnProcess: &quot;org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh&quot;,&lt;br/&gt;
   jmhCoreBenchmarks: &quot;org.openjdk.jmh:jmh-core-benchmarks:$versions.jmh&quot;,&lt;br/&gt;
+  jmhGeneratorAnnProcess: &quot;org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh&quot;,&lt;br/&gt;
+  joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
   junit: &quot;junit:junit:$versions.junit&quot;,&lt;br/&gt;
+  kafkaStreams_0100: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0100&quot;,&lt;br/&gt;
+  kafkaStreams_0101: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0101&quot;,&lt;br/&gt;
+  kafkaStreams_0102: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0102&quot;,&lt;br/&gt;
+  kafkaStreams_0110: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0110&quot;,&lt;br/&gt;
   log4j: &quot;log4j:log4j:$versions.log4j&quot;,&lt;/li&gt;
	&lt;li&gt;joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
   lz4: &quot;org.lz4:lz4-java:$versions.lz4&quot;,&lt;br/&gt;
   metrics: &quot;com.yammer.metrics:metrics-core:$versions.metrics&quot;,&lt;br/&gt;
   powermockJunit4: &quot;org.powermock:powermock-module-junit4:$versions.powermock&quot;,&lt;br/&gt;
diff --git a/settings.gradle b/settings.gradle&lt;br/&gt;
index f0fdf07128c..2820287b11e 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/settings.gradle&lt;br/&gt;
+++ b/settings.gradle&lt;br/&gt;
@@ -13,5 +13,7 @@&lt;br/&gt;
 // See the License for the specific language governing permissions and&lt;br/&gt;
 // limitations under the License.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
+include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:examples&apos;, &apos;streams:upgrade-system-tests-0100&apos;,&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0101&apos;, &apos;streams:upgrade-system-tests-0102&apos;, &apos;streams:upgrade-system-tests-0110&apos;,&lt;br/&gt;
+        &apos;log4j-appender&apos;,&lt;br/&gt;
         &apos;connect:api&apos;, &apos;connect:transforms&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;, &apos;jmh-benchmarks&apos;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index 42e65df92a6..7510e381d39 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -129,6 +129,11 @@&lt;br/&gt;
      */&lt;br/&gt;
     public static final String PRODUCER_PREFIX = &quot;producer.&quot;;&lt;/p&gt;

&lt;p&gt;+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.0.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #PROCESSING_GUARANTEE_CONFIG &quot;processing.guarantee&quot;}
&lt;p&gt; for at-least-once processing guarantees.&lt;br/&gt;
      */&lt;br/&gt;
@@ -280,6 +285,11 @@&lt;br/&gt;
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = &quot;timestamp.extractor&quot;;&lt;br/&gt;
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = &quot;Timestamp extractor class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.processor.TimestampExtractor&amp;lt;/code&amp;gt; interface. This config is deprecated, use &amp;lt;code&amp;gt;&quot; + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + &quot;&amp;lt;/code&amp;gt; instead&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
+    public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;br/&gt;
+    public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+        &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot; (for upgrading from 0.10.0.x).&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code value.serde}&lt;/li&gt;
	&lt;li&gt;@deprecated Use 
{@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG}
&lt;p&gt; instead.&lt;br/&gt;
@@ -491,6 +501,12 @@&lt;br/&gt;
                     10 * 60 * 1000,&lt;br/&gt;
                     Importance.LOW,&lt;br/&gt;
                     STATE_CLEANUP_DELAY_MS_DOC)&lt;br/&gt;
+            .define(UPGRADE_FROM_CONFIG,&lt;br/&gt;
+                    ConfigDef.Type.STRING,&lt;br/&gt;
+                    null,&lt;br/&gt;
+                    in(null, UPGRADE_FROM_0100),&lt;br/&gt;
+                    Importance.LOW,&lt;br/&gt;
+                    UPGRADE_FROM_DOC)&lt;br/&gt;
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,&lt;br/&gt;
                     Type.LONG,&lt;br/&gt;
                     24 * 60 * 60 * 1000,&lt;br/&gt;
@@ -712,6 +728,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;br/&gt;
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + &quot;-consumer&quot;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add configs required for stream partition assignor&lt;br/&gt;
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));&lt;br/&gt;
         consumerProps.put(InternalConfig.STREAM_THREAD_INSTANCE, streamThread);&lt;br/&gt;
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));&lt;br/&gt;
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
index c4ca2078fb7..ab134be8d30 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
@@ -175,6 +175,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {&lt;br/&gt;
     private String userEndPoint;&lt;br/&gt;
     private int numStandbyReplicas;&lt;/p&gt;

&lt;p&gt;+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
+&lt;br/&gt;
     private Cluster metadataWithInternalTopics;&lt;br/&gt;
     private Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState;&lt;/p&gt;

&lt;p&gt;@@ -205,7 +207,13 @@ public void configure(Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         // Setting the logger with the passed in client thread name&lt;br/&gt;
         logPrefix = String.format(&quot;stream-thread &lt;span class=&quot;error&quot;&gt;&amp;#91;%s&amp;#93;&lt;/span&gt; &quot;, configs.get(CommonClientConfigs.CLIENT_ID_CONFIG));&lt;br/&gt;
         final LogContext logContext = new LogContext(logPrefix);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.log = logContext.logger(getClass());&lt;br/&gt;
+        log = logContext.logger(getClass());&lt;br/&gt;
+&lt;br/&gt;
+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;br/&gt;
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) 
{
+            log.info(&quot;Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x.&quot;);
+            userMetadataVersion = 1;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);&lt;br/&gt;
         if (o == null) {&lt;br/&gt;
@@ -266,7 +274,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = threadDataProvider.prevActiveTasks();&lt;br/&gt;
         Set&amp;lt;TaskId&amp;gt; standbyTasks = threadDataProvider.cachedTasks();&lt;br/&gt;
         standbyTasks.removeAll(previousActiveTasks);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;SubscriptionInfo data = new SubscriptionInfo(threadDataProvider.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);&lt;br/&gt;
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, threadDataProvider.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (threadDataProvider.builder().sourceTopicPattern() != null &amp;amp;&amp;amp;&lt;br/&gt;
             !threadDataProvider.builder().subscriptionUpdates().getUpdates().equals(topics)) {&lt;br/&gt;
@@ -309,11 +317,16 @@ private void updateSubscribedTopics(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // construct the client metadata from the decoded subscription info&lt;br/&gt;
         Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;br/&gt;
             String consumerId = entry.getKey();&lt;br/&gt;
             Subscription subscription = entry.getValue();&lt;/p&gt;

&lt;p&gt;             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+            final int usedVersion = info.version;&lt;br/&gt;
+            if (usedVersion &amp;lt; minUserMetadataVersion) &lt;/p&gt;
{
+                minUserMetadataVersion = usedVersion;
+            }

&lt;p&gt;             // create the new client metadata if necessary&lt;br/&gt;
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);&lt;br/&gt;
@@ -572,7 +585,7 @@ private void updateSubscribedTopics(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                 }&lt;/p&gt;

&lt;p&gt;                 // finally, encode the assignment before sending back to coordinator&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));&lt;br/&gt;
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index 8607472c281..42abb4cac0b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -55,7 +55,7 @@ public AssignmentInfo(List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;&lt;br/&gt;
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+    public AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
                              Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.activeTasks = activeTasks;&lt;br/&gt;
@@ -153,8 +153,7 @@ public static AssignmentInfo decode(ByteBuffer data) {&lt;br/&gt;
                 }&lt;br/&gt;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
-&lt;br/&gt;
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
         } catch (IOException ex) 
{
             throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);
         }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index f583dbafc94..00227e799b8 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -31,7 +31,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int CURRENT_VERSION = 2;&lt;br/&gt;
+    public static final int CURRENT_VERSION = 2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public final int version;&lt;br/&gt;
     public final UUID processId;&lt;br/&gt;
@@ -43,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; stand&lt;br/&gt;
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
+    public SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.processId = processId;&lt;br/&gt;
         this.prevTasks = prevTasks;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
index b217bf9b5bf..30ab6922754 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
@@ -423,6 +423,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() 
{
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -457,6 +458,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() &lt;/p&gt;
{
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -470,6 +472,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -483,6 +486,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -496,6 +500,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -518,9 +523,7 @@ public void configure(final Map configs, final boolean isKey) {&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void close() 
{
-
-        }
&lt;p&gt;+        public void close() {}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public Serializer serializer() {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index 4c12bb93544..44e139a28bd 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -313,6 +313,4 @@ private void startStreams() {&lt;/p&gt;

&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
index 918b6aa5bd4..d34bea3eeb2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
@@ -140,7 +140,7 @@ private void mockThreadDataProvider(final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
                                         final Set&amp;lt;TaskId&amp;gt; cachedTasks,&lt;br/&gt;
                                         final UUID processId,&lt;br/&gt;
                                         final PartitionGrouper partitionGrouper,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final InternalTopologyBuilder builder) throws NoSuchFieldException, IllegalAccessException {&lt;br/&gt;
+                                        final InternalTopologyBuilder builder) {&lt;br/&gt;
         EasyMock.expect(threadDataProvider.name()).andReturn(&quot;name&quot;).anyTimes();&lt;br/&gt;
         EasyMock.expect(threadDataProvider.prevActiveTasks()).andReturn(prevTasks).anyTimes();&lt;br/&gt;
         EasyMock.expect(threadDataProvider.cachedTasks()).andReturn(cachedTasks).anyTimes();&lt;br/&gt;
@@ -179,7 +179,7 @@ public void shouldInterleaveTasksByGroupId() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testSubscription() throws Exception {&lt;br/&gt;
+    public void testSubscription() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -208,7 +208,7 @@ public void testSubscription() throws Exception {&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignBasic() throws Exception {&lt;br/&gt;
+    public void testAssignBasic() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -248,11 +248,9 @@ public void testAssignBasic() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -272,7 +270,7 @@ public void testAssignBasic() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws Exception {&lt;br/&gt;
+    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() 
{
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;);
@@ -340,7 +338,7 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
+    public void testAssignWithPartialTopology() {&lt;br/&gt;
         Properties props = configProps();&lt;br/&gt;
         props.put(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, SingleGroupPartitionGrouperStub.class);&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(props);&lt;br/&gt;
@@ -369,9 +367,8 @@ public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(&quot;topic1&quot;), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -379,7 +376,7 @@ public void testAssignWithPartialTopology() throws Exception {&lt;/p&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
+    public void testAssignEmptyMetadata() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -409,9 +406,8 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
             new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Collections.&amp;lt;String&amp;gt;emptySet(), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, allActiveTasks.size());&lt;br/&gt;
         assertEquals(Collections.&amp;lt;TaskId&amp;gt;emptySet(), new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -434,7 +430,7 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
+    public void testAssignWithNewTasks() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source3&quot;, null, null, null, &quot;topic3&quot;);&lt;br/&gt;
@@ -466,13 +462,9 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match&lt;br/&gt;
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and&lt;br/&gt;
         // then later ones will be re-assigned to other hosts due to load balancing&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;AssignmentInfo info;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info.activeTasks);&lt;/li&gt;
	&lt;li&gt;allPartitions.addAll(assignments.get(&quot;consumer10&quot;).partitions());&lt;br/&gt;
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());&lt;br/&gt;
         allActiveTasks.addAll(info.activeTasks);&lt;br/&gt;
@@ -487,7 +479,7 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStates() throws Exception {&lt;br/&gt;
+    public void testAssignWithStates() {&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
@@ -576,7 +568,7 @@ public void testAssignWithStates() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
+    public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         Properties props = configProps();&lt;br/&gt;
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);&lt;br/&gt;
         StreamsConfig config = new StreamsConfig(props);&lt;br/&gt;
@@ -613,13 +605,10 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;/li&gt;
	&lt;li&gt;allStandbyTasks.addAll(info10.standbyTasks.keySet());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;(info10.standbyTasks.keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -647,7 +636,7 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testOnAssignment() throws Exception {&lt;br/&gt;
+    public void testOnAssignment() {&lt;br/&gt;
         TopicPartition t2p3 = new TopicPartition(&quot;topic2&quot;, 3);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
@@ -675,7 +664,7 @@ public void testOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopics() {&lt;br/&gt;
         String applicationId = &quot;test&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addInternalTopic(&quot;topicX&quot;);&lt;br/&gt;
@@ -706,7 +695,7 @@ public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() 
{
         String applicationId = &quot;test&quot;;
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(&quot;topicX&quot;);
@@ -741,9 +730,8 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;/li&gt;
	&lt;li&gt;final String applicationId = &quot;application-id&quot;;&lt;/li&gt;
	&lt;li&gt;builder.setApplicationId(applicationId);&lt;br/&gt;
+    public void shouldAddUserDefinedEndPointToSubscription() {&lt;br/&gt;
+        builder.setApplicationId(&quot;application-id&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source&quot;, null, null, null, &quot;input&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source&quot;);&lt;br/&gt;
         builder.addSink(&quot;sink&quot;, &quot;output&quot;, null, null, null, &quot;processor&quot;);&lt;br/&gt;
@@ -752,7 +740,8 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
         mockThreadDataProvider(Collections.&amp;lt;TaskId&amp;gt;emptySet(),&lt;br/&gt;
                                Collections.&amp;lt;TaskId&amp;gt;emptySet(),&lt;br/&gt;
                                uuid1,&lt;/li&gt;
	&lt;li&gt;defaultPartitionGrouper, builder);&lt;br/&gt;
+                               defaultPartitionGrouper,&lt;br/&gt;
+                               builder);&lt;br/&gt;
         configurePartitionAssignor(0, userEndPoint);&lt;br/&gt;
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;input&quot;));&lt;br/&gt;
         final SubscriptionInfo subscriptionInfo = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
@@ -760,7 +749,59 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() 
{
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            &quot;consumer1&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            &quot;consumer2&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        mockThreadDataProvider(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            defaultPartitionGrouper,
+            builder);
+        configurePartitionAssignor(0, null);
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscription() &lt;/p&gt;
{
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+
+        mockThreadDataProvider(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            defaultPartitionGrouper,
+            builder);
+
+        configurationMap.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        configurePartitionAssignor(0, null);
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldMapUserEndPointToTopicPartitions() {&lt;br/&gt;
         final String applicationId = &quot;application-id&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
@@ -790,7 +831,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {&lt;br/&gt;
         final String myEndPoint = &quot;localhost&quot;;&lt;br/&gt;
         final String applicationId = &quot;application-id&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
@@ -821,7 +862,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() 
{
         List&amp;lt;TopicPartition&amp;gt; topic = Collections.singletonList(new TopicPartition(&quot;topic&quot;, 0));
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState =
                 Collections.singletonMap(new HostInfo(&quot;localhost&quot;, 80),
@@ -837,7 +878,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldSetClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldSetClusterMetadataOnAssignment() {&lt;br/&gt;
         final List&amp;lt;TopicPartition&amp;gt; topic = Collections.singletonList(new TopicPartition(&quot;topic&quot;, 0));&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState =&lt;br/&gt;
                 Collections.singletonMap(new HostInfo(&quot;localhost&quot;, 80),&lt;br/&gt;
@@ -865,7 +906,7 @@ public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() throws Exception {&lt;br/&gt;
+    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() {&lt;br/&gt;
         final String applicationId = &quot;application-id&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
@@ -970,7 +1011,7 @@ public Object apply(final Object value1, final Object value2) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdatePartitionHostInfoMapOnAssignment() {&lt;br/&gt;
         final TopicPartition partitionOne = new TopicPartition(&quot;topic&quot;, 1);&lt;br/&gt;
         final TopicPartition partitionTwo = new TopicPartition(&quot;topic&quot;, 2);&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState = Collections.singletonMap(&lt;br/&gt;
@@ -993,7 +1034,7 @@ public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdateClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdateClusterMetadataOnAssignment() {&lt;br/&gt;
         final TopicPartition topicOne = new TopicPartition(&quot;topic&quot;, 1);&lt;br/&gt;
         final TopicPartition topicTwo = new TopicPartition(&quot;topic2&quot;, 2);&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState = Collections.singletonMap(&lt;br/&gt;
@@ -1015,7 +1056,7 @@ public void shouldUpdateClusterMetadataOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {&lt;br/&gt;
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {&lt;br/&gt;
         final String applicationId = &quot;appId&quot;;&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index ec94ad81acd..8032f7da865 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -64,10 +64,9 @@ public void shouldDecodePreviousVersion() throws IOException &lt;/p&gt;
{
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn&apos;t in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }

&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;/li&gt;
	&lt;li&gt;so it is impossible to test compatibility without having this&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
index 887f763e8fa..6902bd6d376 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
+import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
 import org.apache.kafka.streams.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
@@ -30,10 +31,13 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KTable;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Materialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Predicate;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.Produced;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Serialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.TimeWindows;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.ValueJoiner;&lt;br/&gt;
+import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
 import org.apache.kafka.streams.state.Stores;&lt;br/&gt;
+import org.apache.kafka.streams.state.WindowStore;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.io.File;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
@@ -47,7 +51,7 @@&lt;br/&gt;
     private Thread thread;&lt;br/&gt;
     private boolean uncaughtException = false;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public SmokeTestClient(File stateDir, String kafka) {&lt;br/&gt;
+    public SmokeTestClient(final File stateDir, final String kafka) {&lt;br/&gt;
         super();&lt;br/&gt;
         this.stateDir = stateDir;&lt;br/&gt;
         this.kafka = kafka;&lt;br/&gt;
@@ -57,7 +61,7 @@ public void start() {&lt;br/&gt;
         streams = createKafkaStreams(stateDir, kafka);&lt;br/&gt;
         streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public void uncaughtException(Thread t, Throwable e) {&lt;br/&gt;
+            public void uncaughtException(final Thread t, final Throwable e) {&lt;br/&gt;
                 System.out.println(&quot;SMOKE-TEST-CLIENT-EXCEPTION&quot;);&lt;br/&gt;
                 uncaughtException = true;&lt;br/&gt;
                 e.printStackTrace();&lt;br/&gt;
@@ -94,7 +98,7 @@ public void close() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static KafkaStreams createKafkaStreams(File stateDir, String kafka) {&lt;br/&gt;
+    private static Properties getStreamsConfig(final File stateDir, final String kafka) {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         props.put(StreamsConfig.STATE_DIR_CONFIG, stateDir.toString());&lt;br/&gt;
@@ -109,25 +113,29 @@ private static KafkaStreams createKafkaStreams(File stateDir, String kafka) 
{
         props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);
         //TODO remove this config or set to smaller value when KIP-91 is merged
         props.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 60000);
+        return props;
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
	&lt;li&gt;Consumed&amp;lt;String, Integer&amp;gt; stringIntConsumed = Consumed.with(stringSerde, intSerde);&lt;/li&gt;
	&lt;li&gt;KStream&amp;lt;String, Integer&amp;gt; source = builder.stream(&quot;data&quot;, stringIntConsumed);&lt;/li&gt;
	&lt;li&gt;source.to(stringSerde, intSerde, &quot;echo&quot;);&lt;/li&gt;
	&lt;li&gt;KStream&amp;lt;String, Integer&amp;gt; data = source.filter(new Predicate&amp;lt;String, Integer&amp;gt;() {&lt;br/&gt;
+    private static KafkaStreams createKafkaStreams(final File stateDir, final String kafka) {&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final Consumed&amp;lt;String, Integer&amp;gt; stringIntConsumed = Consumed.with(stringSerde, intSerde);&lt;br/&gt;
+        final KStream&amp;lt;String, Integer&amp;gt; source = builder.stream(&quot;data&quot;, stringIntConsumed);&lt;br/&gt;
+        source.to(&quot;echo&quot;, Produced.with(stringSerde, intSerde));&lt;br/&gt;
+        final KStream&amp;lt;String, Integer&amp;gt; data = source.filter(new Predicate&amp;lt;String, Integer&amp;gt;() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public boolean test(String key, Integer value) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            public boolean test(final String key, final Integer value) {
                 return value == null || value != END;
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
         data.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // min&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KGroupedStream&amp;lt;String, Integer&amp;gt;&lt;/li&gt;
	&lt;li&gt;groupedData =&lt;br/&gt;
+        final KGroupedStream&amp;lt;String, Integer&amp;gt; groupedData =&lt;br/&gt;
             data.groupByKey(Serialized.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(1)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
                     public Integer apply() {&lt;br/&gt;
                         return Integer.MAX_VALUE;&lt;br/&gt;
@@ -135,21 +143,24 @@ public Integer apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Integer&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Integer apply(String aggKey, Integer value, Integer aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value &amp;lt; aggregate) ? value : aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(1)),&lt;/li&gt;
	&lt;li&gt;intSerde, &quot;uwin-min&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Integer&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;min&quot;);&lt;br/&gt;
+                Materialized.&amp;lt;String, Integer, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-min&quot;).withValueSerde(intSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Integer&amp;gt;())&lt;br/&gt;
+            .to(&quot;min&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(&quot;min&quot;, stringIntConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(&lt;br/&gt;
+            &quot;min&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, intSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;minStoreName&quot;));&lt;br/&gt;
         minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;min&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // max&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
                     public Integer apply() {&lt;br/&gt;
                         return Integer.MIN_VALUE;&lt;br/&gt;
@@ -157,21 +168,24 @@ public Integer apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Integer&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Integer apply(String aggKey, Integer value, Integer aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value &amp;gt; aggregate) ? value : aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(2)),&lt;/li&gt;
	&lt;li&gt;intSerde, &quot;uwin-max&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Integer&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;max&quot;);&lt;br/&gt;
+                Materialized.&amp;lt;String, Integer, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-max&quot;).withValueSerde(intSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Integer&amp;gt;())&lt;br/&gt;
+            .to(&quot;max&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(&quot;max&quot;, stringIntConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(&lt;br/&gt;
+            &quot;max&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, intSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;maxStoreName&quot;));&lt;br/&gt;
         maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;max&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // sum&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Long&amp;gt;() {&lt;br/&gt;
                     public Long apply() {&lt;br/&gt;
                         return 0L;&lt;br/&gt;
@@ -179,70 +193,74 @@ public Long apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Long&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Long apply(String aggKey, Integer value, Long aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Long apply(final String aggKey, final Integer value, final Long aggregate) {
                         return (long) value + aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(2)),&lt;/li&gt;
	&lt;li&gt;longSerde, &quot;win-sum&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Long&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;sum&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Consumed&amp;lt;String, Long&amp;gt; stringLongConsumed = Consumed.with(stringSerde, longSerde);&lt;/li&gt;
	&lt;li&gt;KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(&quot;sum&quot;, stringLongConsumed);&lt;br/&gt;
+                Materialized.&amp;lt;String, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;win-sum&quot;).withValueSerde(longSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Long&amp;gt;())&lt;br/&gt;
+            .to(&quot;sum&quot;, Produced.with(stringSerde, longSerde));&lt;br/&gt;
+&lt;br/&gt;
+        final Consumed&amp;lt;String, Long&amp;gt; stringLongConsumed = Consumed.with(stringSerde, longSerde);&lt;br/&gt;
+        final KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(&quot;sum&quot;, stringLongConsumed);&lt;br/&gt;
         sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;sum&quot;));&lt;br/&gt;
+&lt;br/&gt;
         // cnt&lt;/li&gt;
	&lt;li&gt;groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), &quot;uwin-cnt&quot;)&lt;/li&gt;
	&lt;li&gt;.toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Long&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;cnt&quot;);&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .count(Materialized.&amp;lt;String, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-cnt&quot;))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Long&amp;gt;())&lt;br/&gt;
+            .to(&quot;cnt&quot;, Produced.with(stringSerde, longSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(&quot;cnt&quot;, stringLongConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(&lt;br/&gt;
+            &quot;cnt&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, longSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;cntStoreName&quot;));&lt;br/&gt;
         cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;cnt&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // dif&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;maxTable.join(minTable,&lt;br/&gt;
+        maxTable&lt;br/&gt;
+            .join(&lt;br/&gt;
+                minTable,&lt;br/&gt;
                 new ValueJoiner&amp;lt;Integer, Integer, Integer&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;public Integer apply(Integer value1, Integer value2) {&lt;br/&gt;
+                    public Integer apply(final Integer value1, final Integer value2) 
{
                         return value1 - value2;
                     }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;dif&quot;);&lt;br/&gt;
+                })&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;dif&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // avg&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sumTable.join(&lt;br/&gt;
+        sumTable&lt;br/&gt;
+            .join(&lt;br/&gt;
                 cntTable,&lt;br/&gt;
                 new ValueJoiner&amp;lt;Long, Long, Double&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;public Double apply(Long value1, Long value2) {&lt;br/&gt;
+                    public Double apply(final Long value1, final Long value2) 
{
                         return (double) value1 / (double) value2;
                     }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, doubleSerde, &quot;avg&quot;);&lt;br/&gt;
+                })&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;avg&quot;, Produced.with(stringSerde, doubleSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // test repartition&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Agg agg = new Agg();&lt;/li&gt;
	&lt;li&gt;cntTable.groupBy(agg.selector(),&lt;/li&gt;
	&lt;li&gt;Serialized.with(stringSerde, longSerde)&lt;/li&gt;
	&lt;li&gt;).aggregate(agg.init(),&lt;/li&gt;
	&lt;li&gt;agg.adder(),&lt;/li&gt;
	&lt;li&gt;agg.remover(),&lt;/li&gt;
	&lt;li&gt;Materialized.&amp;lt;String, Long&amp;gt;as(Stores.inMemoryKeyValueStore(&quot;cntByCnt&quot;))&lt;/li&gt;
	&lt;li&gt;.withKeySerde(Serdes.String())&lt;/li&gt;
	&lt;li&gt;.withValueSerde(Serdes.Long())&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;tagg&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final KafkaStreams streamsClient = new KafkaStreams(builder.build(), props);&lt;br/&gt;
+        final Agg agg = new Agg();&lt;br/&gt;
+        cntTable.groupBy(agg.selector(), Serialized.with(stringSerde, longSerde))&lt;br/&gt;
+            .aggregate(agg.init(), agg.adder(), agg.remover(),&lt;br/&gt;
+                Materialized.&amp;lt;String, Long&amp;gt;as(Stores.inMemoryKeyValueStore(&quot;cntByCnt&quot;))&lt;br/&gt;
+                    .withKeySerde(Serdes.String())&lt;br/&gt;
+                    .withValueSerde(Serdes.Long()))&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;tagg&quot;, Produced.with(stringSerde, longSerde));&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), getStreamsConfig(stateDir, kafka));&lt;br/&gt;
         streamsClient.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public void uncaughtException(Thread t, Throwable e) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(&quot;FATAL: An unexpected exception is encountered on thread &quot; + t + &quot;: &quot; + e);
-                
                 streamsClient.close(30, TimeUnit.SECONDS);
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return streamsClient;&lt;br/&gt;
     }&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
index fdc9326427c..1e661baad4a 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
@@ -130,53 +130,65 @@ public void run() &lt;/p&gt;
{
         System.out.println(&quot;shutdown&quot;);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(String kafka, final int numKeys, final int maxRecordsPerKey) {&lt;br/&gt;
+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey) 
{
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey,&lt;br/&gt;
+                                                     final boolean autoTerminate) {&lt;br/&gt;
         final Properties producerProps = new Properties();&lt;br/&gt;
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;br/&gt;
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// the next 4 config values make sure that all records are produced with no loss and&lt;/li&gt;
	&lt;li&gt;// no duplicates&lt;br/&gt;
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates&lt;br/&gt;
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);&lt;br/&gt;
         producerProps.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 45000);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;br/&gt;
+        final KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         int numRecordsProduced = 0;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         for (int i = 0; i &amp;lt; numKeys; i++) 
{
             data[i] = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data[i].key, new HashSet&amp;lt;Integer&amp;gt;());
         }&lt;/li&gt;
	&lt;li&gt;Random rand = new Random();&lt;br/&gt;
+        final Random rand = new Random();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int remaining = data.length;&lt;br/&gt;
+        int remaining = 1; // dummy value must be positive if &amp;lt;autoTerminate&amp;gt; is false&lt;br/&gt;
+        if (autoTerminate) 
{
+            remaining = data.length;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; needRetry = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;         while (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int index = rand.nextInt(remaining);&lt;/li&gt;
	&lt;li&gt;String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);&lt;br/&gt;
+            final String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
             int value = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.next();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (value &amp;lt; 0) {&lt;br/&gt;
+            if (autoTerminate &amp;amp;&amp;amp; value &amp;lt; 0) 
{
                 remaining--;
                 data[index] = data[remaining];
             }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;/li&gt;
	&lt;li&gt;new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;br/&gt;
+                final ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;br/&gt;
+                    new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 producer.send(record, new TestCallback(record, needRetry));&lt;/p&gt;

&lt;p&gt;                 numRecordsProduced++;&lt;br/&gt;
                 allData.get(key).add(value);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numRecordsProduced % 100 == 0)&lt;br/&gt;
+                if (numRecordsProduced % 100 == 0) 
{
                     System.out.println(numRecordsProduced + &quot; records produced&quot;);
+                }
&lt;p&gt;                 Utils.sleep(2);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
index dc4c91b4097..87ca82918a9 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
@@ -44,20 +44,15 @@&lt;br/&gt;
             public Processor&amp;lt;Object, Object&amp;gt; get() {&lt;br/&gt;
                 return new AbstractProcessor&amp;lt;Object, Object&amp;gt;() {&lt;br/&gt;
                     private int numRecordsProcessed = 0;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;private ProcessorContext context;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;br/&gt;
                     public void init(final ProcessorContext context) &lt;/p&gt;
{
                         System.out.println(&quot;initializing processor: topic=&quot; + topic + &quot; taskId=&quot; + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }

&lt;p&gt;                     @Override&lt;br/&gt;
                     public void process(final Object key, final Object value) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (printOffset) 
{
-                            System.out.println(&quot;&amp;gt;&amp;gt;&amp;gt; &quot; + context.offset());
-                        }
&lt;p&gt;                         numRecordsProcessed++;&lt;br/&gt;
                         if (numRecordsProcessed % 100 == 0) {&lt;br/&gt;
                             System.out.println(System.currentTimeMillis());&lt;br/&gt;
@@ -66,19 +61,19 @@ public void process(final Object key, final Object value) {&lt;br/&gt;
                     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void punctuate(final long timestamp) { }&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void close() { }&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
                 };&lt;br/&gt;
             }&lt;br/&gt;
         };&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final class Unwindow&amp;lt;K, V&amp;gt; implements KeyValueMapper&amp;lt;Windowed&amp;lt;K&amp;gt;, V, KeyValue&amp;lt;K, V&amp;gt;&amp;gt; {&lt;br/&gt;
+    public static final class Unwindow&amp;lt;K, V&amp;gt; implements KeyValueMapper&amp;lt;Windowed&amp;lt;K&amp;gt;, V, K&amp;gt; {&lt;br/&gt;
         @Override&lt;/li&gt;
	&lt;li&gt;public KeyValue&amp;lt;K, V&amp;gt; apply(final Windowed&amp;lt;K&amp;gt; winKey, final V value) {&lt;/li&gt;
	&lt;li&gt;return new KeyValue&amp;lt;&amp;gt;(winKey.key(), value);&lt;br/&gt;
+        public K apply(final Windowed&amp;lt;K&amp;gt; winKey, final V value) 
{
+            return winKey.key();
         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
index 64597bdcda6..c0e345ff2b6 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
@@ -23,7 +23,7 @@&lt;br/&gt;
 public class StreamsSmokeTest {&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  args ::= command kafka zookeeper stateDir&lt;br/&gt;
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;command := &quot;run&quot; | &quot;process&quot;&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;@param args&lt;br/&gt;
@@ -32,11 +32,13 @@ public static void main(String[] args) throws InterruptedException {&lt;br/&gt;
         String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         String stateDir = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
         String command = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+        boolean disableAutoTerminate = args.length &amp;gt; 3;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.out.println(&quot;StreamsTest instance started&quot;);&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsSmokeTest)&quot;);&lt;br/&gt;
         System.out.println(&quot;command=&quot; + command);&lt;br/&gt;
         System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
         System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;disableAutoTerminate=&quot; + disableAutoTerminate);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         switch (command) {&lt;br/&gt;
             case &quot;standalone&quot;:&lt;br/&gt;
@@ -46,8 +48,12 @@ public static void main(String[] args) throws InterruptedException {&lt;br/&gt;
                 // this starts the driver (data generation and result verification)&lt;br/&gt;
                 final int numKeys = 10;&lt;br/&gt;
                 final int maxRecordsPerKey = 500;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);&lt;/li&gt;
	&lt;li&gt;SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);&lt;br/&gt;
+                if (disableAutoTerminate) 
{
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
&lt;p&gt;                 break;&lt;br/&gt;
             case &quot;process&quot;:&lt;br/&gt;
                 // this starts a KafkaStreams client&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..5486374b62c&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,72 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires two argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest trunk)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..72d7f5a7b04&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                System.out.println(&quot;closing Kafka Streams instance&quot;);+                System.out.flush();+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..eebd0fab83c&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,114 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 3 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.1)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..18240f04ff1&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,108 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.10.2.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.2)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            // TODO: because Kafka 0.10.2.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..779021d8672&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,108 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.11.0.3 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.11.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.11.0.3 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/tests/docker/Dockerfile b/tests/docker/Dockerfile&lt;br/&gt;
index 3ca99938fb6..6fe05ffed6d 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/docker/Dockerfile&lt;br/&gt;
+++ b/tests/docker/Dockerfile&lt;br/&gt;
@@ -40,13 +40,13 @@ COPY ./ssh-config /root/.ssh/config&lt;br/&gt;
 RUN ssh-keygen -q -t rsa -N &apos;&apos; -f /root/.ssh/id_rsa &amp;amp;&amp;amp; cp -f /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;Install binary test dependencies.&lt;br/&gt;
-ENV MIRROR=&quot;http://mirrors.ocf.berkeley.edu/apache/&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.8.2.2&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.8.2.2&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.9.0.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.9.0.1&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.10.0.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}kafka/0.10.0.1/kafka_2.11-0.10.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.0.1&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.10.1.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}kafka/0.10.1.1/kafka_2.11-0.10.1.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.1.1&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.10.2.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.2.1&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.11.0.0&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.11.0.0&quot;&lt;br/&gt;
+ENV MIRROR=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.8.2.2&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}/kafka_2.10-0.8.2.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.8.2.2&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.9.0.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}/kafka_2.11-0.9.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.9.0.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.10.0.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}/kafka_2.11-0.10.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.0.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.10.1.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}/kafka_2.11-0.10.1.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.1.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.10.2.1&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}/kafka_2.11-0.10.2.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.2.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.11.0.2&quot; &amp;amp;&amp;amp; curl -s &quot;${MIRROR}/kafka_2.11-0.11.0.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.11.0.2&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;Set up the ducker user.&lt;br/&gt;
 RUN useradd -ms /bin/bash ducker &amp;amp;&amp;amp; mkdir -p /home/ducker/ &amp;amp;&amp;amp; rsync -aiq /root/.ssh/ /home/ducker/.ssh &amp;amp;&amp;amp; chown -R ducker /home/ducker/ /mnt/ &amp;amp;&amp;amp; echo &apos;ducker ALL=(ALL) NOPASSWD: ALL&apos; &amp;gt;&amp;gt; /etc/sudoers&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index f3f7348bf4e..f60074c7c6b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -20,6 +20,7 @@&lt;br/&gt;
 from ducktape.utils.util import wait_until&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1&lt;/p&gt;


&lt;p&gt; class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, &quot;tools-log4j.properties&quot;)&lt;br/&gt;
     PID_FILE = os.path.join(PERSISTENT_ROOT, &quot;streams.pid&quot;)&lt;/p&gt;

&lt;p&gt;+    CLEAN_NODE_ENABLED = True&lt;br/&gt;
+&lt;br/&gt;
     logs = {&lt;br/&gt;
         &quot;streams_log&quot;: {&lt;br/&gt;
             &quot;path&quot;: LOG_FILE,&lt;br/&gt;
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
         &quot;streams_stderr&quot;: &lt;/p&gt;
{
             &quot;path&quot;: STDERR_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None, user_test_args3=None):&lt;br/&gt;
@@ -108,7 +219,8 @@ def wait_node(self, node, timeout_sec=None):&lt;/p&gt;

&lt;p&gt;     def clean_node(self, node):&lt;br/&gt;
         node.account.kill_process(&quot;streams&quot;, clean_shutdown=False, allow_fail=True)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if self.CLEAN_NODE_ENABLED:&lt;br/&gt;
+            node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def start_cmd(self, node):&lt;br/&gt;
         args = self.args.copy()&lt;br/&gt;
@@ -170,7 +282,28 @@ def clean_node(self, node):&lt;br/&gt;
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
         super(StreamsSmokeTestDriverService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, kafka, &quot;run&quot;)&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def disable_auto_terminate(self):&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;disableAutoTerminate&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;disable_auto_terminate&amp;#39;&amp;#93;&lt;/span&gt; = self.DISABLE_AUTO_TERMINATE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;/p&gt;

&lt;p&gt;+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;/p&gt;

&lt;p&gt; class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
@@ -211,3 +344,41 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, eosEnabled):&lt;br/&gt;
                                                                 kafka,&lt;br/&gt;
                                                                 &quot;org.apache.kafka.streams.tests.BrokerCompatibilityTest&quot;,&lt;br/&gt;
                                                                 eosEnabled)&lt;br/&gt;
+&lt;br/&gt;
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
+        super(StreamsUpgradeTestJobRunnerService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context,&lt;br/&gt;
+                                                                 kafka,&lt;br/&gt;
+                                                                 &quot;org.apache.kafka.streams.tests.StreamsUpgradeTest&quot;,&lt;br/&gt;
+                                                                 &quot;&quot;)&lt;br/&gt;
+        self.UPGRADE_FROM = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def set_version(self, kafka_streams_version):&lt;br/&gt;
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version&lt;br/&gt;
+&lt;br/&gt;
+    def set_upgrade_from(self, upgrade_from):&lt;br/&gt;
+        self.UPGRADE_FROM = upgrade_from&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
+        else:&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = &quot;&quot;&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;version&amp;#39;&amp;#93;&lt;/span&gt; = self.KAFKA_STREAMS_VERSION&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;upgrade_from&amp;#39;&amp;#93;&lt;/span&gt; = self.UPGRADE_FROM&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s &quot; \&lt;br/&gt;
+              &quot; %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
index 81b7ffe7047..e97bbb7fcfd 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -15,21 +15,48 @@&lt;/p&gt;

&lt;p&gt; from ducktape.mark.resource import cluster&lt;br/&gt;
 from ducktape.tests.test import Test&lt;br/&gt;
-from ducktape.mark import parametrize, ignore&lt;br/&gt;
+from ducktape.mark import ignore, matrix, parametrize&lt;br/&gt;
 from kafkatest.services.kafka import KafkaService&lt;br/&gt;
 from kafkatest.services.zookeeper import ZookeeperService&lt;br/&gt;
-from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService&lt;br/&gt;
-from kafkatest.version import LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, DEV_BRANCH, KafkaVersion&lt;br/&gt;
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService, StreamsUpgradeTestJobRunnerService&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, DEV_BRANCH, DEV_VERSION, KafkaVersion&lt;br/&gt;
+import random&lt;br/&gt;
 import time&lt;/p&gt;

&lt;p&gt;+broker_upgrade_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(DEV_BRANCH)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+simple_upgrade_versions_metadata_version_2 = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(DEV_VERSION)&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; class StreamsUpgradeTest(Test):&lt;br/&gt;
     &quot;&quot;&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Tests rolling upgrades and downgrades of the Kafka Streams library.&lt;br/&gt;
+    Test upgrading Kafka Streams (all version combination)&lt;br/&gt;
+    If metadata was changes, upgrade is more difficult&lt;br/&gt;
+    Metadata version was bumped in 0.10.1.0&lt;br/&gt;
     &quot;&quot;&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
         super(StreamsUpgradeTest, self)._&lt;em&gt;init&lt;/em&gt;_(test_context)&lt;br/&gt;
+        self.topics = {&lt;br/&gt;
+            &apos;echo&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+            &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+    def perform_broker_upgrade(self, to_version):&lt;br/&gt;
+        self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;br/&gt;
+        for node in self.kafka.nodes:&lt;br/&gt;
+            self.kafka.stop_node(node)&lt;br/&gt;
+            node.version = KafkaVersion(to_version)&lt;br/&gt;
+            self.kafka.start_node(node)&lt;br/&gt;
+&lt;br/&gt;
+    @cluster(num_nodes=6)&lt;br/&gt;
+    @matrix(from_version=broker_upgrade_versions, to_version=broker_upgrade_versions)&lt;br/&gt;
+    def test_upgrade_downgrade_brokers(self, from_version, to_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Start a smoke test client then perform rolling upgrades on the broker. &lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        if from_version == to_version:&lt;br/&gt;
+            return&lt;br/&gt;
+&lt;br/&gt;
         self.replication = 3&lt;br/&gt;
         self.partitions = 1&lt;br/&gt;
         self.isr = 2&lt;br/&gt;
@@ -55,45 +82,7 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
             &apos;tagg&apos; : { &apos;partitions&apos;: self.partitions, &apos;replication-factor&apos;: self.replication,&lt;br/&gt;
                        &apos;configs&apos;: &lt;/p&gt;
{&quot;min.insync.replicas&quot;: self.isr}
&lt;p&gt; }&lt;br/&gt;
         }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;def perform_streams_upgrade(self, to_version):&lt;/li&gt;
	&lt;li&gt;self.logger.info(&quot;First pass bounce - rolling streams upgrade&quot;)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;# get the node running the streams app&lt;/li&gt;
	&lt;li&gt;node = self.processor1.node&lt;/li&gt;
	&lt;li&gt;self.processor1.stop()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;# change it&apos;s version. This will automatically make it pick up a different&lt;/li&gt;
	&lt;li&gt;# JAR when it starts again&lt;/li&gt;
	&lt;li&gt;node.version = KafkaVersion(to_version)&lt;/li&gt;
	&lt;li&gt;self.processor1.start()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;def perform_broker_upgrade(self, to_version):&lt;/li&gt;
	&lt;li&gt;self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;/li&gt;
	&lt;li&gt;for node in self.kafka.nodes:&lt;/li&gt;
	&lt;li&gt;self.kafka.stop_node(node)&lt;/li&gt;
	&lt;li&gt;node.version = KafkaVersion(to_version)&lt;/li&gt;
	&lt;li&gt;self.kafka.start_node(node)&lt;br/&gt;
-&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@cluster(num_nodes=6)&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_1), to_version=str(DEV_BRANCH))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_1), to_version=str(LATEST_0_11_0))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_2), to_version=str(LATEST_0_11_0))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_11_0), to_version=str(LATEST_0_10_2))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(DEV_BRANCH), to_version=str(LATEST_0_10_2))&lt;/li&gt;
	&lt;li&gt;def test_upgrade_downgrade_streams(self, from_version, to_version):&lt;/li&gt;
	&lt;li&gt;&quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;Start a smoke test client, then abort (kill -9) and restart it a few times.&lt;/li&gt;
	&lt;li&gt;Ensure that all records are delivered.&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Note, that just like tests/core/upgrade_test.py, a prerequisite for this test to succeed&lt;/li&gt;
	&lt;li&gt;if the inclusion of all parametrized versions of kafka in kafka/vagrant/base.sh&lt;/li&gt;
	&lt;li&gt;(search for get_kafka()). For streams in particular, that means that someone has manually&lt;/li&gt;
	&lt;li&gt;copies the kafka-stream-$version-test.jar in the right S3 bucket as shown in base.sh.&lt;/li&gt;
	&lt;li&gt;&quot;&quot;&quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;Setup phase&lt;br/&gt;
         self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
         self.zk.start()&lt;br/&gt;
@@ -108,13 +97,12 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
         self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
-&lt;/p&gt;

&lt;p&gt;         self.driver.start()&lt;br/&gt;
         self.processor1.start()&lt;br/&gt;
         time.sleep(15)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.perform_streams_upgrade(to_version)&lt;br/&gt;
+        self.perform_broker_upgrade(to_version)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         time.sleep(15)&lt;br/&gt;
         self.driver.wait()&lt;br/&gt;
@@ -126,42 +114,239 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):&lt;br/&gt;
         node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
         self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;/p&gt;

&lt;p&gt;+    @matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)&lt;br/&gt;
+    def test_simple_upgrade_downgrade(self, from_version, to_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with &amp;lt;old_version&amp;gt;, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
+        &quot;&quot;&quot;&lt;/p&gt;

&lt;p&gt;+        if from_version == to_version:&lt;br/&gt;
+            return&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@cluster(num_nodes=6)&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))&lt;/li&gt;
	&lt;li&gt;def test_upgrade_brokers(self, from_version, to_version):&lt;br/&gt;
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
+        self.zk.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)&lt;br/&gt;
+        self.kafka.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(from_version)&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # upgrade one-by-one via rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, to_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released&lt;br/&gt;
+    @parametrize(new_version=str(DEV_VERSION))&lt;br/&gt;
+    def test_metadata_upgrade(self, new_version):&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;Start a smoke test client then perform rolling upgrades on the broker.&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;# Setup phase&lt;br/&gt;
+&lt;br/&gt;
         self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
         self.zk.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# number of nodes needs to be &amp;gt;= 3 for the smoke test&lt;/li&gt;
	&lt;li&gt;self.kafka = KafkaService(self.test_context, num_nodes=3,&lt;/li&gt;
	&lt;li&gt;zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)&lt;br/&gt;
         self.kafka.start()&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;# allow some time for topics to be created&lt;/li&gt;
	&lt;li&gt;time.sleep(10)&lt;/li&gt;
	&lt;li&gt;&lt;p&gt;+&lt;br/&gt;
         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;br/&gt;
         self.driver.start()&lt;/li&gt;
	&lt;li&gt;self.processor1.start()&lt;/li&gt;
	&lt;li&gt;time.sleep(15)&lt;br/&gt;
+        self.start_all_nodes_with(str(LATEST_0_10_0))&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.perform_broker_upgrade(to_version)&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;time.sleep(15)&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # first rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;0.10.0&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # second rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
         self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
         self.driver.stop()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.processor1.stop()&lt;br/&gt;
+    def start_all_nodes_with(self, version):&lt;br/&gt;
+        # start first with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor1, version)&lt;br/&gt;
+        node1 = self.processor1.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:&lt;br/&gt;
+                self.processor1.start()&lt;br/&gt;
+                log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                       timeout_sec=60,&lt;br/&gt;
+                                       err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node1.account))&lt;br/&gt;
+                monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node = self.driver.node&lt;/li&gt;
	&lt;li&gt;node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;/li&gt;
	&lt;li&gt;self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+        # start second with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor2, version)&lt;br/&gt;
+        node2 = self.processor2.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:&lt;br/&gt;
+                    self.processor2.start()&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node2.account))&lt;br/&gt;
+                    first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                             timeout_sec=60,&lt;br/&gt;
+                                             err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                    second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                              timeout_sec=60,&lt;br/&gt;
+                                              err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start third with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor3, version)&lt;br/&gt;
+        node3 = self.processor3.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:&lt;br/&gt;
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:&lt;br/&gt;
+                        self.processor3.start()&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node3.account))&lt;br/&gt;
+                        first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                 timeout_sec=60,&lt;br/&gt;
+                                                 err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                        second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+                        third_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node3.account))&lt;br/&gt;
+&lt;br/&gt;
+    @staticmethod&lt;br/&gt;
+    def prepare_for(processor, version):&lt;br/&gt;
+        processor.node.account.ssh(&quot;rm -rf &quot; + processor.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(version)&lt;br/&gt;
+&lt;br/&gt;
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
+        first_other_processor = None&lt;br/&gt;
+        second_other_processor = None&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            if p != processor:&lt;br/&gt;
+                if first_other_processor is None:&lt;br/&gt;
+                    first_other_processor = p&lt;br/&gt;
+                else:&lt;br/&gt;
+                    second_other_processor = p&lt;br/&gt;
+&lt;br/&gt;
+        node = processor.node&lt;br/&gt;
+        first_other_node = first_other_processor.node&lt;br/&gt;
+        second_other_node = second_other_processor.node&lt;br/&gt;
+&lt;br/&gt;
+        # stop processor and wait for rebalance of others&lt;br/&gt;
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                processor.stop()&lt;br/&gt;
+                first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                timeout_sec=60,&lt;br/&gt;
+                                                err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+        node.account.ssh_capture(&quot;grep UPGRADE-TEST-CLIENT-CLOSED %s&quot; % processor.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if upgrade_from == &quot;&quot;:  # upgrade disabled &amp;#8211; second round of rolling bounces&lt;br/&gt;
+            roll_counter = &quot;.1-&quot;  # second round of rolling bounces&lt;br/&gt;
+        else:&lt;br/&gt;
+            roll_counter = &quot;.0-&quot;  # first  round of rolling boundes&lt;br/&gt;
+&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDOUT_FILE + &quot; &quot; + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDERR_FILE + &quot; &quot; + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.LOG_FILE + &quot; &quot; + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if new_version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(new_version)&lt;br/&gt;
+        processor.set_upgrade_from(upgrade_from)&lt;br/&gt;
+&lt;br/&gt;
+        grep_metadata_error = &quot;grep \&quot;org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\&quot; &quot;&lt;br/&gt;
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:&lt;br/&gt;
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                        processor.start()&lt;br/&gt;
+&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + new_version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + new_version + &quot; &quot; + str(node.account))&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                       timeout_sec=60,&lt;br/&gt;
+                                                       err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                        timeout_sec=60,&lt;br/&gt;
+                                                        err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node.account))&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index 8a9373c1413..b49a3ef2fcc 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -61,6 +61,7 @@ def get_version(node=None):&lt;br/&gt;
         return DEV_BRANCH&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; DEV_BRANCH = KafkaVersion(&quot;dev&quot;)&lt;br/&gt;
+DEV_VERSION = KafkaVersion(&quot;1.0.2-SNAPSHOT&quot;)&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;0.8.2.X versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;br/&gt;
@@ -91,7 +92,9 @@ def get_version(node=None):&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;0.11.0.0 versions&lt;br/&gt;
 V_0_11_0_0 = KafkaVersion(&quot;0.11.0.0&quot;)&lt;br/&gt;
-LATEST_0_11_0 = V_0_11_0_0&lt;br/&gt;
+V_0_11_0_1 = KafkaVersion(&quot;0.11.0.1&quot;)&lt;br/&gt;
+V_0_11_0_2 = KafkaVersion(&quot;0.11.0.2&quot;)&lt;br/&gt;
+LATEST_0_11_0 = V_0_11_0_2&lt;br/&gt;
 LATEST_0_11 = LATEST_0_11_0&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;1.0.0 versions&lt;br/&gt;
diff --git a/vagrant/base.sh b/vagrant/base.sh&lt;br/&gt;
index 2ebebf9adaa..86ceefe6897 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/vagrant/base.sh&lt;br/&gt;
+++ b/vagrant/base.sh&lt;br/&gt;
@@ -93,8 +93,8 @@ get_kafka 0.10.1.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.10.1.1&lt;br/&gt;
 get_kafka 0.10.2.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.10.2.1&lt;br/&gt;
-get_kafka 0.11.0.0 2.11&lt;br/&gt;
-chmod a+rw /opt/kafka-0.11.0.0&lt;br/&gt;
+get_kafka 0.11.0.2 2.11&lt;br/&gt;
+chmod a+rw /opt/kafka-0.11.0.2&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;



&lt;ol&gt;
	&lt;li&gt;For EC2 nodes, we want to use /mnt, which should have the local disk. On local&lt;/li&gt;
&lt;/ol&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16427788" author="githubbot" created="Fri, 6 Apr 2018 00:14:23 +0000"  >&lt;p&gt;mjsax closed pull request #4773:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4773&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4773&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh&lt;br/&gt;
index bb786da43ca..4dd092323b0 100755&lt;br/&gt;
&amp;#8212; a/bin/kafka-run-class.sh&lt;br/&gt;
+++ b/bin/kafka-run-class.sh&lt;br/&gt;
@@ -69,28 +69,50 @@ do&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/clients/build/libs/kafka-clients*.jar;&lt;br/&gt;
-do&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if should_include_file &quot;$file&quot;; then&lt;/li&gt;
	&lt;li&gt;CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;/li&gt;
	&lt;li&gt;fi&lt;br/&gt;
-done&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  clients_lib_dir=$(dirname $0)/../clients/build/libs&lt;br/&gt;
+  streams_lib_dir=$(dirname $0)/../streams/build/libs&lt;br/&gt;
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}&lt;br/&gt;
+else&lt;br/&gt;
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs&lt;br/&gt;
+  streams_lib_dir=$clients_lib_dir&lt;br/&gt;
+  rocksdb_lib_dir=$streams_lib_dir&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/build/libs/kafka-streams*.jar;&lt;br/&gt;
+for file in &quot;$clients_lib_dir&quot;/kafka-clients*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+for file in &quot;$streams_lib_dir&quot;/kafka-streams*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;&lt;del&gt;for file in &quot;$base_dir&quot;/streams/build/dependant-libs&lt;/del&gt;${SCALA_VERSION}/rocksdb*.jar;&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+else&lt;br/&gt;
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed &apos;s/\.//g&apos;`&lt;br/&gt;
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;(${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;br/&gt;
+for file in &quot;$rocksdb_lib_dir&quot;/rocksdb*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
 done&lt;br/&gt;
diff --git a/build.gradle b/build.gradle&lt;br/&gt;
index 5e4c35643c2..f4d1fb3e090 100644&lt;br/&gt;
&amp;#8212; a/build.gradle&lt;br/&gt;
+++ b/build.gradle&lt;br/&gt;
@@ -1019,6 +1019,66 @@ project(&apos;:streams:examples&apos;) {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;+project(&apos;:streams:upgrade-system-tests-0100&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0100&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0100
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0101&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0101&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0102&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0102&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0102
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0110&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0110&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0110
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-10&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-10&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_10
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
 project(&apos;:jmh-benchmarks&apos;) {&lt;/p&gt;

&lt;p&gt;   apply plugin: &apos;com.github.johnrengelman.shadow&apos;&lt;br/&gt;
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml&lt;br/&gt;
index f23805e665f..1ec535f6f32 100644&lt;br/&gt;
&amp;#8212; a/checkstyle/suppressions.xml&lt;br/&gt;
+++ b/checkstyle/suppressions.xml&lt;br/&gt;
@@ -189,7 +189,7 @@&lt;br/&gt;
               files=&quot;SmokeTestDriver.java&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;     &amp;lt;suppress checks=&quot;NPathComplexity&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;files=&quot;KStreamKStreamJoinTest.java&quot;/&amp;gt;&lt;br/&gt;
+              files=&quot;KStreamKStreamJoinTest.java|SmokeTestDriver.java&quot;/&amp;gt;&lt;br/&gt;
     &amp;lt;suppress checks=&quot;NPathComplexity&quot;&lt;br/&gt;
               files=&quot;KStreamKStreamLeftJoinTest.java&quot;/&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
index 31c51c22ca3..4c17e596b32 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java&lt;br/&gt;
@@ -16,7 +16,9 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.security.authenticator;&lt;/p&gt;

&lt;p&gt;-import java.util.Map;&lt;br/&gt;
+import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
+import org.apache.kafka.common.network.Mode;&lt;br/&gt;
+import org.apache.kafka.common.security.scram.ScramExtensionsCallback;&lt;/p&gt;

&lt;p&gt; import javax.security.auth.Subject;&lt;br/&gt;
 import javax.security.auth.callback.Callback;&lt;br/&gt;
@@ -25,10 +27,7 @@&lt;br/&gt;
 import javax.security.auth.callback.UnsupportedCallbackException;&lt;br/&gt;
 import javax.security.sasl.AuthorizeCallback;&lt;br/&gt;
 import javax.security.sasl.RealmCallback;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.kafka.common.config.SaslConfigs;&lt;br/&gt;
-import org.apache.kafka.common.network.Mode;&lt;br/&gt;
-import org.apache.kafka.common.security.scram.ScramExtensionsCallback;&lt;br/&gt;
+import java.util.Map;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Callback handler for Sasl clients. The callbacks required for the SASL mechanism&lt;br/&gt;
diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html&lt;br/&gt;
index b9b4a4e48cf..09557fa2c63 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/docs/streams/upgrade-guide.html&lt;br/&gt;
+++ b/docs/streams/upgrade-guide.html&lt;br/&gt;
@@ -34,7 +34,7 @@ &amp;lt;h1&amp;gt;Upgrade Guide and API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
     &amp;lt;/div&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 1.0.x to 1.1.0 and you have customized window store implementations on the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface&lt;br/&gt;
+        If you want to upgrade from 1.0.x to 1.1.x and you have customized window store implementations on the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface&lt;br/&gt;
         you&apos;d need to update your code to incorporate the newly added public APIs.&lt;br/&gt;
         Otherwise, if you are using Java 7 you don&apos;t need to make any code changes as the public API is fully backward compatible;&lt;br/&gt;
         but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.&lt;br/&gt;
@@ -43,28 +43,52 @@ &amp;lt;h1&amp;gt;Upgrade Guide and API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.11.0.x to 1.0.0 you don&apos;t need to make any code changes as the public API is fully backward compatible.&lt;br/&gt;
+        If you want to upgrade from 0.10.2.x or 0.11.0.x to 1.1.x and you have customized window store implementations on the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface&lt;br/&gt;
+        you&apos;d need to update your code to incorporate the newly added public APIs.&lt;br/&gt;
+        Otherwise, if you are using Java 7 you don&apos;t need to do any code changes as the public API is fully backward compatible;&lt;br/&gt;
+        but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.&lt;br/&gt;
         However, some public APIs were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;below&amp;lt;/a&amp;gt; for a complete list of 1.0.0 API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;/li&gt;
	&lt;li&gt;&amp;lt;/p&amp;gt;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;If you want to upgrade from 0.10.2.x to 0.11.0 you don&apos;t need to make any code changes as the public API is fully backward compatible.&lt;/li&gt;
	&lt;li&gt;However, some configuration parameters were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;below&amp;lt;/a&amp;gt; for a complete list of 0.11.0 API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_110&quot;&amp;gt;1.1&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;1.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        and &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;0.11.0&amp;lt;/a&amp;gt; API&lt;br/&gt;
+        and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        Additionally, Streams API 1.1.x requires broker on-disk message format version 0.10 or higher; thus, you need to make sure that the message&lt;br/&gt;
+        format is configured correctly before you upgrade your Kafka Streams application.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.1.x to 0.10.2, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;/li&gt;
	&lt;li&gt;It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;below&amp;lt;/a&amp;gt; for a complete list of 0.10.2 API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;br/&gt;
+        If you want to upgrade from 0.10.1.x to 1.1.x see the Upgrade Sections for &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1100_streams&quot;&amp;gt;&amp;lt;b&amp;gt;0.11.0&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_100_streams&quot;&amp;gt;&amp;lt;b&amp;gt;1.0&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_110_streams&quot;&amp;gt;&amp;lt;b&amp;gt;1.1&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;br/&gt;
+        Note, that a brokers on-disk message format must be on version 0.10 or higher to run a Kafka Streams application version 1.1 or higher.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;0.10.2&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;1.0&amp;lt;/a&amp;gt;, and &amp;lt;a href=&quot;#streams_api_changes_110&quot;&amp;gt;1.1&amp;lt;/a&amp;gt; API and semantical changes&lt;br/&gt;
+        that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.0.x to 0.10.1, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1010_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.1&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;/li&gt;
	&lt;li&gt;It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        Upgrading from 0.10.0.x to 1.1.x directly is also possible.&lt;br/&gt;
+        Note, that a brokers must be on version 0.10.1 or higher and on-disk message format must be on version 0.10 or higher&lt;br/&gt;
+        to run a Kafka Streams application version 1.1 or higher.&lt;br/&gt;
+        See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;, and &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0&amp;lt;/a&amp;gt;&lt;br/&gt;
+        for a complete list of API changes.&lt;br/&gt;
+        Upgrading to 1.1.1 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 1.1.1&amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your newly deployed 1.1.1 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;p&amp;gt; Upgrading from 0.10.0.x to 1.1.0 requires an offline upgrade (rolling bounce upgrade is not supported) &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; restart all new (1.1.0) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_110&quot; href=&quot;#streams_api_changes_110&quot;&amp;gt;Streams API changes in 1.1.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
     &amp;lt;p&amp;gt;&lt;br/&gt;
diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index 3ac293d8498..26c4779a133 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -63,6 +63,12 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_1_1_0&quot; href=&quot;#upgrade_1_1_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.1&lt;br/&gt;
         Hot-swaping the jar-file only might not work.&amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ol&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_111_notable&quot; href=&quot;#upgrade_111_notable&quot;&amp;gt;Notable changes in 1.1.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_110_notable&quot; href=&quot;#upgrade_110_notable&quot;&amp;gt;Notable changes in 1.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;The kafka artifact in Maven no longer depends on log4j or slf4j-log4j12. Similarly to the kafka-clients artifact, users&lt;br/&gt;
@@ -134,6 +140,12 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_1_0_0&quot; href=&quot;#upgrade_1_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.1&lt;br/&gt;
         Similarly for the message format version.&amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ol&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_102_notable&quot; href=&quot;#upgrade_102_notable&quot;&amp;gt;Notable changes in 1.0.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_101_notable&quot; href=&quot;#upgrade_101_notable&quot;&amp;gt;Notable changes in 1.0.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;Restored binary compatibility of AdminClient&apos;s Options classes (e.g. CreateTopicsOptions, DeleteTopicsOptions, etc.) with&lt;br/&gt;
@@ -199,17 +211,71 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_new_protocols&quot; href=&quot;#upgrade_100_new_protocols&quot;&amp;gt;New Prot&lt;br/&gt;
          be used if the SaslHandshake request version is greater than 0. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;-&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams&quot; href=&quot;#upgrade_100_streams&quot;&amp;gt;Upgrading a 1.0.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams&quot; href=&quot;#upgrade_100_streams&quot;&amp;gt;Upgrading a 0.11.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.11.0 to 1.0.0 does not require a broker upgrade.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).&lt;/li&gt;
	&lt;li&gt;However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. &amp;lt;/li&amp;gt;&lt;br/&gt;
+         A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).&lt;br/&gt;
+         However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;/li&gt;
	&lt;li&gt;We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0102&quot; href=&quot;#upgrade_100_streams_from_0102&quot;&amp;gt;Upgrading a 0.10.2 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.2 to 1.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0101&quot; href=&quot;#upgrade_1100_streams_from_0101&quot;&amp;gt;Upgrading a 0.10.1 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.1 to 1.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the &amp;lt;code&amp;gt;TimestampExtractor&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you register custom metrics, you will need to update this code, because the &amp;lt;code&amp;gt;StreamsMetric&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0100&quot; href=&quot;#upgrade_100_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 1.0 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 1.0 application can only connect to 0.1, 0.11.0, 0.10.2, or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 1.0.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.11.0.3 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (1.0.0 or 1.0.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
 &amp;lt;p&amp;gt;Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below,&lt;br/&gt;
   you guarantee no downtime during the upgrade. However, please review the &amp;lt;a href=&quot;#upgrade_1100_notable&quot;&amp;gt;notable changes in 0.11.0.0&amp;lt;/a&amp;gt; before upgrading.&lt;br/&gt;
@@ -258,11 +324,55 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams&quot; href=&quot;#upgrade_1100_streams&quot;&amp;gt;Upgrading a 0.10.2 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.2 to 0.11.0 does not require a broker upgrade.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams_from_0101&quot; href=&quot;#upgrade_1100_streams_from_0101&quot;&amp;gt;Upgrading a 0.10.1 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.1 to 0.11.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the &amp;lt;code&amp;gt;TimestampExtractor&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you register custom metrics, you will need to update this code, because the &amp;lt;code&amp;gt;StreamsMetric&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams_from_0100&quot; href=&quot;#upgrade_1100_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.11.0 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.11.0 application can only connect to 0.11.0, 0.10.2, or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.11.0.3 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.11.0.3 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.11.0.0 , 0.11.0.1, or 0.11.0.2) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1103_notable&quot; href=&quot;#upgrade_1103_notable&quot;&amp;gt;Notable changes in 0.11.0.3&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_notable&quot; href=&quot;#upgrade_1100_notable&quot;&amp;gt;Notable changes in 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to&lt;br/&gt;
@@ -413,6 +523,35 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams&quot; href=&quot;#upgrade_1020_streams&quot;&amp;gt;Upgrading a 0.10.1&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams_from_0100&quot; href=&quot;#upgrade_1020_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.2 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.2.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.2.0 or 0.10.2.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10202_notable&quot; href=&quot;#upgrade_10202_notable&quot;&amp;gt;Notable changes in 0.10.2.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10201_notable&quot; href=&quot;#upgrade_10201_notable&quot;&amp;gt;Notable changes in 0.10.2.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
   &amp;lt;li&amp;gt; The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer &amp;lt;code&amp;gt;retries&amp;lt;/code&amp;gt; default value was changed from 0 to 10. The internal Kafka Streams consumer &amp;lt;code&amp;gt;max.poll.interval.ms&amp;lt;/code&amp;gt;  default value was changed from 300000 to &amp;lt;code&amp;gt;Integer.MAX_VALUE&amp;lt;/code&amp;gt;.&lt;br/&gt;
@@ -491,6 +630,23 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_streams&quot; href=&quot;#upgrade_1010_streams&quot;&amp;gt;Upgrading a 0.10.0&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.1 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.1.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.1.0 or 0.10.1.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_notable&quot; href=&quot;#upgrade_1010_notable&quot;&amp;gt;Notable changes in 0.10.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
@@ -534,14 +690,17 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_new_protocols&quot; href=&quot;#upgrade_1010_new_protocols&quot;&amp;gt;New Pr&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_10&quot; href=&quot;#upgrade_10&quot;&amp;gt;Upgrading from 0.8.x or 0.9.x to 0.10.0.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
+&amp;lt;p&amp;gt;&lt;br/&gt;
 0.10.0.0 has &amp;lt;a href=&quot;#upgrade_10_breaking&quot;&amp;gt;potential breaking changes&amp;lt;/a&amp;gt; (please review before upgrading) and possible &amp;lt;a href=&quot;#upgrade_10_performance_impact&quot;&amp;gt;  performance impact following the upgrade&amp;lt;/a&amp;gt;. By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade.&lt;br/&gt;
 &amp;lt;br&amp;gt;&lt;br/&gt;
 Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.&lt;br/&gt;
-&amp;lt;p/&amp;gt;&lt;br/&gt;
+&amp;lt;/p&amp;gt;&lt;br/&gt;
+&amp;lt;p&amp;gt;&lt;br/&gt;
 &amp;lt;b&amp;gt;Notes to clients with version 0.9.0.0: &amp;lt;/b&amp;gt;Due to a bug introduced in 0.9.0.0,&lt;br/&gt;
 clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not&lt;br/&gt;
 work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 &amp;lt;b&amp;gt;before&amp;lt;/b&amp;gt; brokers are upgraded to&lt;br/&gt;
 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.&lt;br/&gt;
+&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;For a rolling upgrade:&amp;lt;/b&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle&lt;br/&gt;
index b7a03dcd752..d82ec4b3d6a 100644&lt;br/&gt;
&amp;#8212; a/gradle/dependencies.gradle&lt;br/&gt;
+++ b/gradle/dependencies.gradle&lt;br/&gt;
@@ -60,6 +60,11 @@ versions += [&lt;br/&gt;
   scalaLogging: &quot;3.7.2&quot;,&lt;br/&gt;
   jopt: &quot;5.0.4&quot;,&lt;br/&gt;
   junit: &quot;4.12&quot;,&lt;br/&gt;
+  kafka_0100: &quot;0.10.0.1&quot;,&lt;br/&gt;
+  kafka_0101: &quot;0.10.1.1&quot;,&lt;br/&gt;
+  kafka_0102: &quot;0.10.2.1&quot;,&lt;br/&gt;
+  kafka_0110: &quot;0.11.0.2&quot;,&lt;br/&gt;
+  kafka_10: &quot;1.0.1&quot;,&lt;br/&gt;
   lz4: &quot;1.4&quot;,&lt;br/&gt;
   metrics: &quot;2.2.0&quot;,&lt;br/&gt;
   // PowerMock 1.x doesn&apos;t support Java 9, so use PowerMock 2.0.0 beta&lt;br/&gt;
@@ -97,12 +102,16 @@ libs += [&lt;br/&gt;
   jettyServlets: &quot;org.eclipse.jetty:jetty-servlets:$versions.jetty&quot;,&lt;br/&gt;
   jerseyContainerServlet: &quot;org.glassfish.jersey.containers:jersey-container-servlet:$versions.jersey&quot;,&lt;br/&gt;
   jmhCore: &quot;org.openjdk.jmh:jmh-core:$versions.jmh&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;jmhGeneratorAnnProcess: &quot;org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh&quot;,&lt;br/&gt;
   jmhCoreBenchmarks: &quot;org.openjdk.jmh:jmh-core-benchmarks:$versions.jmh&quot;,&lt;br/&gt;
+  jmhGeneratorAnnProcess: &quot;org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh&quot;,&lt;br/&gt;
+  joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
   junit: &quot;junit:junit:$versions.junit&quot;,&lt;br/&gt;
+  kafkaStreams_0100: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0100&quot;,&lt;br/&gt;
+  kafkaStreams_0101: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0101&quot;,&lt;br/&gt;
+  kafkaStreams_0102: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0102&quot;,&lt;br/&gt;
+  kafkaStreams_0110: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0110&quot;,&lt;br/&gt;
+  kafkaStreams_10: &quot;org.apache.kafka:kafka-streams:$versions.kafka_10&quot;,&lt;br/&gt;
   log4j: &quot;log4j:log4j:$versions.log4j&quot;,&lt;/li&gt;
	&lt;li&gt;scalaLogging: &quot;com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging&quot;,&lt;/li&gt;
	&lt;li&gt;joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
   lz4: &quot;org.lz4:lz4-java:$versions.lz4&quot;,&lt;br/&gt;
   metrics: &quot;com.yammer.metrics:metrics-core:$versions.metrics&quot;,&lt;br/&gt;
   powermockJunit4: &quot;org.powermock:powermock-module-junit4:$versions.powermock&quot;,&lt;br/&gt;
@@ -110,6 +119,7 @@ libs += [&lt;br/&gt;
   reflections: &quot;org.reflections:reflections:$versions.reflections&quot;,&lt;br/&gt;
   rocksDBJni: &quot;org.rocksdb:rocksdbjni:$versions.rocksDB&quot;,&lt;br/&gt;
   scalaLibrary: &quot;org.scala-lang:scala-library:$versions.scala&quot;,&lt;br/&gt;
+  scalaLogging: &quot;com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging&quot;,&lt;br/&gt;
   scalaReflect: &quot;org.scala-lang:scala-reflect:$versions.scala&quot;,&lt;br/&gt;
   scalatest: &quot;org.scalatest:scalatest_$versions.baseScala:$versions.scalatest&quot;,&lt;br/&gt;
   scoveragePlugin: &quot;org.scoverage:scalac-scoverage-plugin_$versions.baseScala:$versions.scoverage&quot;,&lt;br/&gt;
diff --git a/settings.gradle b/settings.gradle&lt;br/&gt;
index e599d01215c..03136849fd5 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/settings.gradle&lt;br/&gt;
+++ b/settings.gradle&lt;br/&gt;
@@ -13,5 +13,7 @@&lt;br/&gt;
 // See the License for the specific language governing permissions and&lt;br/&gt;
 // limitations under the License.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:test-utils&apos;, &apos;streams:examples&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
+include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:test-utils&apos;, &apos;streams:examples&apos;,&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0100&apos;, &apos;streams:upgrade-system-tests-0101&apos;, &apos;streams:upgrade-system-tests-0102&apos;,&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0110&apos;, &apos;streams:upgrade-system-tests-10&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
         &apos;connect:api&apos;, &apos;connect:transforms&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;, &apos;jmh-benchmarks&apos;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index ec0a1a8385b..c1c50a64cbe 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -167,6 +167,11 @@&lt;br/&gt;
      */&lt;br/&gt;
     public static final String ADMIN_CLIENT_PREFIX = &quot;admin.&quot;;&lt;/p&gt;

&lt;p&gt;+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.0.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #PROCESSING_GUARANTEE_CONFIG &quot;processing.guarantee&quot;}
&lt;p&gt; for at-least-once processing guarantees.&lt;br/&gt;
      */&lt;br/&gt;
@@ -326,6 +331,11 @@&lt;br/&gt;
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = &quot;timestamp.extractor&quot;;&lt;br/&gt;
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = &quot;Timestamp extractor class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.processor.TimestampExtractor&amp;lt;/code&amp;gt; interface. This config is deprecated, use &amp;lt;code&amp;gt;&quot; + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + &quot;&amp;lt;/code&amp;gt; instead&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
+    public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;br/&gt;
+    public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+        &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot; (for upgrading from 0.10.0.x).&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code value.serde}&lt;/li&gt;
	&lt;li&gt;@deprecated Use 
{@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG}
&lt;p&gt; instead.&lt;br/&gt;
@@ -548,6 +558,12 @@&lt;br/&gt;
                     10 * 60 * 1000,&lt;br/&gt;
                     Importance.LOW,&lt;br/&gt;
                     STATE_CLEANUP_DELAY_MS_DOC)&lt;br/&gt;
+            .define(UPGRADE_FROM_CONFIG,&lt;br/&gt;
+                    ConfigDef.Type.STRING,&lt;br/&gt;
+                    null,&lt;br/&gt;
+                    in(null, UPGRADE_FROM_0100),&lt;br/&gt;
+                    Importance.LOW,&lt;br/&gt;
+                    UPGRADE_FROM_DOC)&lt;br/&gt;
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,&lt;br/&gt;
                     Type.LONG,&lt;br/&gt;
                     24 * 60 * 60 * 1000,&lt;br/&gt;
@@ -779,6 +795,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;br/&gt;
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + &quot;-consumer&quot;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add configs required for stream partition assignor&lt;br/&gt;
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));&lt;br/&gt;
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));&lt;br/&gt;
         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));&lt;br/&gt;
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
index 2a08308a2fd..1bd03768153 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java&lt;br/&gt;
@@ -174,6 +174,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {&lt;br/&gt;
     private TaskManager taskManager;&lt;br/&gt;
     private PartitionGrouper partitionGrouper;&lt;/p&gt;

&lt;p&gt;+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
+&lt;br/&gt;
     private InternalTopicManager internalTopicManager;&lt;br/&gt;
     private CopartitionedTopicsValidator copartitionedTopicsValidator;&lt;/p&gt;

&lt;p&gt;@@ -192,6 +194,12 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         final LogContext logContext = new LogContext(logPrefix);&lt;br/&gt;
         log = logContext.logger(getClass());&lt;/p&gt;

&lt;p&gt;+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;br/&gt;
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) &lt;/p&gt;
{
+            log.info(&quot;Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x.&quot;);
+            userMetadataVersion = 1;
+        }
&lt;p&gt;+&lt;br/&gt;
         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);&lt;br/&gt;
         if (o == null) {&lt;br/&gt;
             KafkaException ex = new KafkaException(&quot;TaskManager is not specified&quot;);&lt;br/&gt;
@@ -249,7 +257,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = taskManager.prevActiveTaskIds();&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks = taskManager.cachedTasksIds();&lt;br/&gt;
         standbyTasks.removeAll(previousActiveTasks);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final SubscriptionInfo data = new SubscriptionInfo(taskManager.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);&lt;br/&gt;
+        final SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, taskManager.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         taskManager.updateSubscriptionsFromMetadata(topics);&lt;/p&gt;

&lt;p&gt;@@ -281,11 +289,16 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         // construct the client metadata from the decoded subscription info&lt;br/&gt;
         Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;br/&gt;
             String consumerId = entry.getKey();&lt;br/&gt;
             Subscription subscription = entry.getValue();&lt;/p&gt;

&lt;p&gt;             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+            final int usedVersion = info.version;&lt;br/&gt;
+            if (usedVersion &amp;lt; minUserMetadataVersion) &lt;/p&gt;
{
+                minUserMetadataVersion = usedVersion;
+            }

&lt;p&gt;             // create the new client metadata if necessary&lt;br/&gt;
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);&lt;br/&gt;
@@ -546,7 +559,7 @@ public Subscription subscription(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                 }&lt;/p&gt;

&lt;p&gt;                 // finally, encode the assignment before sending back to coordinator&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));&lt;br/&gt;
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index 8607472c281..42abb4cac0b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -55,7 +55,7 @@ public AssignmentInfo(List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;&lt;br/&gt;
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+    public AssignmentInfo(int version, List&amp;lt;TaskId&amp;gt; activeTasks, Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
                              Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.activeTasks = activeTasks;&lt;br/&gt;
@@ -153,8 +153,7 @@ public static AssignmentInfo decode(ByteBuffer data) {&lt;br/&gt;
                 }&lt;br/&gt;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
-&lt;br/&gt;
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);&lt;br/&gt;
         } catch (IOException ex) 
{
             throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);
         }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index f583dbafc94..00227e799b8 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -31,7 +31,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int CURRENT_VERSION = 2;&lt;br/&gt;
+    public static final int CURRENT_VERSION = 2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public final int version;&lt;br/&gt;
     public final UUID processId;&lt;br/&gt;
@@ -43,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; stand&lt;br/&gt;
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
+    public SubscriptionInfo(int version, UUID processId, Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, String userEndPoint) {&lt;br/&gt;
         this.version = version;&lt;br/&gt;
         this.processId = processId;&lt;br/&gt;
         this.prevTasks = prevTasks;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
index cc072d5508d..a873e4c4920 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
@@ -472,6 +472,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() 
{
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -506,6 +507,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() &lt;/p&gt;
{
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -519,6 +521,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -532,6 +535,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -545,6 +549,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -567,9 +572,7 @@ public void configure(final Map configs, final boolean isKey) {&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void close() 
{
-
-        }
&lt;p&gt;+        public void close() {}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public Serializer serializer() {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index 4c12bb93544..44e139a28bd 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -313,6 +313,4 @@ private void startStreams() {&lt;/p&gt;

&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
index 02ab803735a..e4b07ba3852 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignorTest.java&lt;br/&gt;
@@ -131,7 +131,7 @@ private void configurePartitionAssignor(final Map&amp;lt;String, Object&amp;gt; props) {&lt;br/&gt;
     private void mockTaskManager(final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
                                  final Set&amp;lt;TaskId&amp;gt; cachedTasks,&lt;br/&gt;
                                  final UUID processId,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final InternalTopologyBuilder builder) throws NoSuchFieldException, IllegalAccessException {&lt;br/&gt;
+                                 final InternalTopologyBuilder builder) {&lt;br/&gt;
         EasyMock.expect(taskManager.builder()).andReturn(builder).anyTimes();&lt;br/&gt;
         EasyMock.expect(taskManager.prevActiveTaskIds()).andReturn(prevTasks).anyTimes();&lt;br/&gt;
         EasyMock.expect(taskManager.cachedTasksIds()).andReturn(cachedTasks).anyTimes();&lt;br/&gt;
@@ -167,7 +167,7 @@ public void shouldInterleaveTasksByGroupId() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testSubscription() throws Exception {&lt;br/&gt;
+    public void testSubscription() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -195,7 +195,7 @@ public void testSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignBasic() throws Exception {&lt;br/&gt;
+    public void testAssignBasic() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -235,11 +235,9 @@ public void testAssignBasic() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -259,7 +257,7 @@ public void testAssignBasic() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws Exception {&lt;br/&gt;
+    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() 
{
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;);
@@ -327,7 +325,7 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
+    public void testAssignWithPartialTopology() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor1&quot;, new MockProcessorSupplier(), &quot;source1&quot;);&lt;br/&gt;
         builder.addStateStore(new MockStateStoreSupplier(&quot;store1&quot;, false), &quot;processor1&quot;);&lt;br/&gt;
@@ -352,9 +350,8 @@ public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(&quot;topic1&quot;), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -362,7 +359,7 @@ public void testAssignWithPartialTopology() throws Exception {&lt;/p&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
+    public void testAssignEmptyMetadata() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -392,9 +389,8 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
             new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Collections.&amp;lt;String&amp;gt;emptySet(), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, allActiveTasks.size());&lt;br/&gt;
         assertEquals(Collections.&amp;lt;TaskId&amp;gt;emptySet(), new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -417,7 +413,7 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
+    public void testAssignWithNewTasks() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source3&quot;, null, null, null, &quot;topic3&quot;);&lt;br/&gt;
@@ -450,13 +446,9 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match&lt;br/&gt;
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and&lt;br/&gt;
         // then later ones will be re-assigned to other hosts due to load balancing&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;AssignmentInfo info;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info.activeTasks);&lt;/li&gt;
	&lt;li&gt;allPartitions.addAll(assignments.get(&quot;consumer10&quot;).partitions());&lt;br/&gt;
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());&lt;br/&gt;
         allActiveTasks.addAll(info.activeTasks);&lt;br/&gt;
@@ -471,7 +463,7 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStates() throws Exception {&lt;br/&gt;
+    public void testAssignWithStates() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
@@ -542,7 +534,10 @@ public void testAssignWithStates() throws Exception 
{
         assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, &quot;store3&quot;, tasks, topicGroups));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Set&amp;lt;TaskId&amp;gt; tasksForState(String applicationId, String storeName, List&amp;lt;TaskId&amp;gt; tasks, Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups) {&lt;br/&gt;
+    private Set&amp;lt;TaskId&amp;gt; tasksForState(final String applicationId,&lt;br/&gt;
+                                      final String storeName,&lt;br/&gt;
+                                      final List&amp;lt;TaskId&amp;gt; tasks,&lt;br/&gt;
+                                      final Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups) {&lt;br/&gt;
         final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, storeName);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Set&amp;lt;TaskId&amp;gt; ids = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -560,7 +555,7 @@ public void testAssignWithStates() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
+    public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; props = configProps();&lt;br/&gt;
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);&lt;br/&gt;
         StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -598,13 +593,10 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks);&lt;/li&gt;
	&lt;li&gt;allStandbyTasks.addAll(info10.standbyTasks.keySet());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks);&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;(info10.standbyTasks.keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -632,7 +624,7 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testOnAssignment() throws Exception {&lt;br/&gt;
+    public void testOnAssignment() {&lt;br/&gt;
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;TaskId&amp;gt; activeTaskList = Utils.mkList(task0, task3);&lt;br/&gt;
@@ -667,7 +659,7 @@ public void testOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopics() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addInternalTopic(&quot;topicX&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
@@ -697,7 +689,7 @@ public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() 
{
         String applicationId = &quot;test&quot;;
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(&quot;topicX&quot;);
@@ -732,7 +724,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldGenerateTasksForAllCreatedPartitions() throws Exception {&lt;br/&gt;
+    public void shouldGenerateTasksForAllCreatedPartitions() {&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);&lt;br/&gt;
@@ -832,7 +824,7 @@ public Object apply(final Object value1, final Object value2) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
+    public void shouldAddUserDefinedEndPointToSubscription() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source&quot;, null, null, null, &quot;input&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source&quot;);&lt;br/&gt;
@@ -851,7 +843,56 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() 
{
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            &quot;consumer1&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            &quot;consumer2&quot;,
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(&quot;topic1&quot;),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());
+
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscription() &lt;/p&gt;
{
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, (Object) StreamsConfig.UPGRADE_FROM_0100));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldMapUserEndPointToTopicPartitions() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source&quot;);&lt;br/&gt;
@@ -881,7 +922,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), UUID.randomUUID(), builder);&lt;br/&gt;
@@ -908,7 +949,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() throws Exception {&lt;br/&gt;
+    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() {&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);&lt;br/&gt;
@@ -1010,7 +1051,7 @@ public Object apply(final Object value1, final Object value2) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() 
{
         final TopicPartition partitionOne = new TopicPartition(&quot;topic&quot;, 1);
         final TopicPartition partitionTwo = new TopicPartition(&quot;topic&quot;, 2);
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState = Collections.singletonMap(
@@ -1028,7 +1069,7 @@ public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exceptio
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {&lt;br/&gt;
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index ec94ad81acd..8032f7da865 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -64,10 +64,9 @@ public void shouldDecodePreviousVersion() throws IOException &lt;/p&gt;
{
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn&apos;t in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }

&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;/li&gt;
	&lt;li&gt;so it is impossible to test compatibility without having this&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
index 727c4217595..9304cec5986 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
+import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
 import org.apache.kafka.streams.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
@@ -30,10 +31,13 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KTable;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Materialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Predicate;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.Produced;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Serialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.TimeWindows;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.ValueJoiner;&lt;br/&gt;
+import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
 import org.apache.kafka.streams.state.Stores;&lt;br/&gt;
+import org.apache.kafka.streams.state.WindowStore;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.io.File;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
@@ -47,7 +51,7 @@&lt;br/&gt;
     private Thread thread;&lt;br/&gt;
     private boolean uncaughtException = false;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public SmokeTestClient(File stateDir, String kafka) {&lt;br/&gt;
+    public SmokeTestClient(final File stateDir, final String kafka) {&lt;br/&gt;
         super();&lt;br/&gt;
         this.stateDir = stateDir;&lt;br/&gt;
         this.kafka = kafka;&lt;br/&gt;
@@ -57,7 +61,7 @@ public void start() {&lt;br/&gt;
         streams = createKafkaStreams(stateDir, kafka);&lt;br/&gt;
         streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public void uncaughtException(Thread t, Throwable e) {&lt;br/&gt;
+            public void uncaughtException(final Thread t, final Throwable e) {&lt;br/&gt;
                 System.out.println(&quot;SMOKE-TEST-CLIENT-EXCEPTION&quot;);&lt;br/&gt;
                 uncaughtException = true;&lt;br/&gt;
                 e.printStackTrace();&lt;br/&gt;
@@ -94,7 +98,7 @@ public void close() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static KafkaStreams createKafkaStreams(File stateDir, String kafka) {&lt;br/&gt;
+    private static Properties getStreamsConfig(final File stateDir, final String kafka) {&lt;br/&gt;
         final Properties props = new Properties();&lt;br/&gt;
         props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         props.put(StreamsConfig.STATE_DIR_CONFIG, stateDir.toString());&lt;br/&gt;
@@ -109,25 +113,29 @@ private static KafkaStreams createKafkaStreams(File stateDir, String kafka) 
{
         props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);
         //TODO remove this config or set to smaller value when KIP-91 is merged
         props.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 80000);
+        return props;
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
	&lt;li&gt;Consumed&amp;lt;String, Integer&amp;gt; stringIntConsumed = Consumed.with(stringSerde, intSerde);&lt;/li&gt;
	&lt;li&gt;KStream&amp;lt;String, Integer&amp;gt; source = builder.stream(&quot;data&quot;, stringIntConsumed);&lt;/li&gt;
	&lt;li&gt;source.to(stringSerde, intSerde, &quot;echo&quot;);&lt;/li&gt;
	&lt;li&gt;KStream&amp;lt;String, Integer&amp;gt; data = source.filter(new Predicate&amp;lt;String, Integer&amp;gt;() {&lt;br/&gt;
+    private static KafkaStreams createKafkaStreams(final File stateDir, final String kafka) {&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final Consumed&amp;lt;String, Integer&amp;gt; stringIntConsumed = Consumed.with(stringSerde, intSerde);&lt;br/&gt;
+        final KStream&amp;lt;String, Integer&amp;gt; source = builder.stream(&quot;data&quot;, stringIntConsumed);&lt;br/&gt;
+        source.to(&quot;echo&quot;, Produced.with(stringSerde, intSerde));&lt;br/&gt;
+        final KStream&amp;lt;String, Integer&amp;gt; data = source.filter(new Predicate&amp;lt;String, Integer&amp;gt;() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public boolean test(String key, Integer value) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            public boolean test(final String key, final Integer value) {
                 return value == null || value != END;
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
         data.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // min&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KGroupedStream&amp;lt;String, Integer&amp;gt;&lt;/li&gt;
	&lt;li&gt;groupedData =&lt;br/&gt;
+        final KGroupedStream&amp;lt;String, Integer&amp;gt; groupedData =&lt;br/&gt;
             data.groupByKey(Serialized.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(1)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
                     public Integer apply() {&lt;br/&gt;
                         return Integer.MAX_VALUE;&lt;br/&gt;
@@ -135,21 +143,24 @@ public Integer apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Integer&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Integer apply(String aggKey, Integer value, Integer aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value &amp;lt; aggregate) ? value : aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(1)),&lt;/li&gt;
	&lt;li&gt;intSerde, &quot;uwin-min&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Integer&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;min&quot;);&lt;br/&gt;
+                Materialized.&amp;lt;String, Integer, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-min&quot;).withValueSerde(intSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Integer&amp;gt;())&lt;br/&gt;
+            .to(&quot;min&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(&quot;min&quot;, stringIntConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(&lt;br/&gt;
+            &quot;min&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, intSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;minStoreName&quot;));&lt;br/&gt;
         minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;min&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // max&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
                     public Integer apply() {&lt;br/&gt;
                         return Integer.MIN_VALUE;&lt;br/&gt;
@@ -157,21 +168,24 @@ public Integer apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Integer&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Integer apply(String aggKey, Integer value, Integer aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value &amp;gt; aggregate) ? value : aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(2)),&lt;/li&gt;
	&lt;li&gt;intSerde, &quot;uwin-max&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Integer&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;max&quot;);&lt;br/&gt;
+                Materialized.&amp;lt;String, Integer, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-max&quot;).withValueSerde(intSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Integer&amp;gt;())&lt;br/&gt;
+            .to(&quot;max&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(&quot;max&quot;, stringIntConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(&lt;br/&gt;
+            &quot;max&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, intSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;maxStoreName&quot;));&lt;br/&gt;
         maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;max&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // sum&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Long&amp;gt;() {&lt;br/&gt;
                     public Long apply() {&lt;br/&gt;
                         return 0L;&lt;br/&gt;
@@ -179,70 +193,74 @@ public Long apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Long&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Long apply(String aggKey, Integer value, Long aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Long apply(final String aggKey, final Integer value, final Long aggregate) {
                         return (long) value + aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(2)),&lt;/li&gt;
	&lt;li&gt;longSerde, &quot;win-sum&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Long&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;sum&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Consumed&amp;lt;String, Long&amp;gt; stringLongConsumed = Consumed.with(stringSerde, longSerde);&lt;/li&gt;
	&lt;li&gt;KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(&quot;sum&quot;, stringLongConsumed);&lt;br/&gt;
+                Materialized.&amp;lt;String, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;win-sum&quot;).withValueSerde(longSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Long&amp;gt;())&lt;br/&gt;
+            .to(&quot;sum&quot;, Produced.with(stringSerde, longSerde));&lt;br/&gt;
+&lt;br/&gt;
+        final Consumed&amp;lt;String, Long&amp;gt; stringLongConsumed = Consumed.with(stringSerde, longSerde);&lt;br/&gt;
+        final KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(&quot;sum&quot;, stringLongConsumed);&lt;br/&gt;
         sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;sum&quot;));&lt;br/&gt;
+&lt;br/&gt;
         // cnt&lt;/li&gt;
	&lt;li&gt;groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), &quot;uwin-cnt&quot;)&lt;/li&gt;
	&lt;li&gt;.toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Long&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;cnt&quot;);&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .count(Materialized.&amp;lt;String, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-cnt&quot;))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Long&amp;gt;())&lt;br/&gt;
+            .to(&quot;cnt&quot;, Produced.with(stringSerde, longSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(&quot;cnt&quot;, stringLongConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(&lt;br/&gt;
+            &quot;cnt&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, longSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;cntStoreName&quot;));&lt;br/&gt;
         cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;cnt&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // dif&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;maxTable.join(minTable,&lt;br/&gt;
+        maxTable&lt;br/&gt;
+            .join(&lt;br/&gt;
+                minTable,&lt;br/&gt;
                 new ValueJoiner&amp;lt;Integer, Integer, Integer&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;public Integer apply(Integer value1, Integer value2) {&lt;br/&gt;
+                    public Integer apply(final Integer value1, final Integer value2) 
{
                         return value1 - value2;
                     }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;dif&quot;);&lt;br/&gt;
+                })&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;dif&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // avg&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sumTable.join(&lt;br/&gt;
+        sumTable&lt;br/&gt;
+            .join(&lt;br/&gt;
                 cntTable,&lt;br/&gt;
                 new ValueJoiner&amp;lt;Long, Long, Double&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;public Double apply(Long value1, Long value2) {&lt;br/&gt;
+                    public Double apply(final Long value1, final Long value2) 
{
                         return (double) value1 / (double) value2;
                     }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, doubleSerde, &quot;avg&quot;);&lt;br/&gt;
+                })&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;avg&quot;, Produced.with(stringSerde, doubleSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // test repartition&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Agg agg = new Agg();&lt;/li&gt;
	&lt;li&gt;cntTable.groupBy(agg.selector(),&lt;/li&gt;
	&lt;li&gt;Serialized.with(stringSerde, longSerde)&lt;/li&gt;
	&lt;li&gt;).aggregate(agg.init(),&lt;/li&gt;
	&lt;li&gt;agg.adder(),&lt;/li&gt;
	&lt;li&gt;agg.remover(),&lt;/li&gt;
	&lt;li&gt;Materialized.&amp;lt;String, Long&amp;gt;as(Stores.inMemoryKeyValueStore(&quot;cntByCnt&quot;))&lt;/li&gt;
	&lt;li&gt;.withKeySerde(Serdes.String())&lt;/li&gt;
	&lt;li&gt;.withValueSerde(Serdes.Long())&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;tagg&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final KafkaStreams streamsClient = new KafkaStreams(builder.build(), props);&lt;br/&gt;
+        final Agg agg = new Agg();&lt;br/&gt;
+        cntTable.groupBy(agg.selector(), Serialized.with(stringSerde, longSerde))&lt;br/&gt;
+            .aggregate(agg.init(), agg.adder(), agg.remover(),&lt;br/&gt;
+                Materialized.&amp;lt;String, Long&amp;gt;as(Stores.inMemoryKeyValueStore(&quot;cntByCnt&quot;))&lt;br/&gt;
+                    .withKeySerde(Serdes.String())&lt;br/&gt;
+                    .withValueSerde(Serdes.Long()))&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;tagg&quot;, Produced.with(stringSerde, longSerde));&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), getStreamsConfig(stateDir, kafka));&lt;br/&gt;
         streamsClient.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public void uncaughtException(Thread t, Throwable e) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(&quot;FATAL: An unexpected exception is encountered on thread &quot; + t + &quot;: &quot; + e);
-                
                 streamsClient.close(30, TimeUnit.SECONDS);
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return streamsClient;&lt;br/&gt;
     }&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
index 882e9c0a585..cba5fba6bd8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
@@ -130,53 +130,65 @@ public void run() &lt;/p&gt;
{
         System.out.println(&quot;shutdown&quot;);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(String kafka, final int numKeys, final int maxRecordsPerKey) {&lt;br/&gt;
+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey) 
{
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey,&lt;br/&gt;
+                                                     final boolean autoTerminate) {&lt;br/&gt;
         final Properties producerProps = new Properties();&lt;br/&gt;
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;br/&gt;
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// the next 4 config values make sure that all records are produced with no loss and&lt;/li&gt;
	&lt;li&gt;// no duplicates&lt;br/&gt;
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates&lt;br/&gt;
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);&lt;br/&gt;
         producerProps.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 80000);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;br/&gt;
+        final KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         int numRecordsProduced = 0;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         for (int i = 0; i &amp;lt; numKeys; i++) 
{
             data[i] = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data[i].key, new HashSet&amp;lt;Integer&amp;gt;());
         }&lt;/li&gt;
	&lt;li&gt;Random rand = new Random();&lt;br/&gt;
+        final Random rand = new Random();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int remaining = data.length;&lt;br/&gt;
+        int remaining = 1; // dummy value must be positive if &amp;lt;autoTerminate&amp;gt; is false&lt;br/&gt;
+        if (autoTerminate) 
{
+            remaining = data.length;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; needRetry = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;         while (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int index = rand.nextInt(remaining);&lt;/li&gt;
	&lt;li&gt;String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);&lt;br/&gt;
+            final String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
             int value = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.next();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (value &amp;lt; 0) {&lt;br/&gt;
+            if (autoTerminate &amp;amp;&amp;amp; value &amp;lt; 0) 
{
                 remaining--;
                 data[index] = data[remaining];
             }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;/li&gt;
	&lt;li&gt;new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;br/&gt;
+                final ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;br/&gt;
+                    new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 producer.send(record, new TestCallback(record, needRetry));&lt;/p&gt;

&lt;p&gt;                 numRecordsProduced++;&lt;br/&gt;
                 allData.get(key).add(value);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numRecordsProduced % 100 == 0)&lt;br/&gt;
+                if (numRecordsProduced % 100 == 0) 
{
                     System.out.println(numRecordsProduced + &quot; records produced&quot;);
+                }
&lt;p&gt;                 Utils.sleep(2);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
index dc4c91b4097..87ca82918a9 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
@@ -44,20 +44,15 @@&lt;br/&gt;
             public Processor&amp;lt;Object, Object&amp;gt; get() {&lt;br/&gt;
                 return new AbstractProcessor&amp;lt;Object, Object&amp;gt;() {&lt;br/&gt;
                     private int numRecordsProcessed = 0;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;private ProcessorContext context;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;br/&gt;
                     public void init(final ProcessorContext context) &lt;/p&gt;
{
                         System.out.println(&quot;initializing processor: topic=&quot; + topic + &quot; taskId=&quot; + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }

&lt;p&gt;                     @Override&lt;br/&gt;
                     public void process(final Object key, final Object value) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (printOffset) 
{
-                            System.out.println(&quot;&amp;gt;&amp;gt;&amp;gt; &quot; + context.offset());
-                        }
&lt;p&gt;                         numRecordsProcessed++;&lt;br/&gt;
                         if (numRecordsProcessed % 100 == 0) {&lt;br/&gt;
                             System.out.println(System.currentTimeMillis());&lt;br/&gt;
@@ -66,19 +61,19 @@ public void process(final Object key, final Object value) {&lt;br/&gt;
                     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void punctuate(final long timestamp) { }&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void close() { }&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
                 };&lt;br/&gt;
             }&lt;br/&gt;
         };&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final class Unwindow&amp;lt;K, V&amp;gt; implements KeyValueMapper&amp;lt;Windowed&amp;lt;K&amp;gt;, V, KeyValue&amp;lt;K, V&amp;gt;&amp;gt; {&lt;br/&gt;
+    public static final class Unwindow&amp;lt;K, V&amp;gt; implements KeyValueMapper&amp;lt;Windowed&amp;lt;K&amp;gt;, V, K&amp;gt; {&lt;br/&gt;
         @Override&lt;/li&gt;
	&lt;li&gt;public KeyValue&amp;lt;K, V&amp;gt; apply(final Windowed&amp;lt;K&amp;gt; winKey, final V value) {&lt;/li&gt;
	&lt;li&gt;return new KeyValue&amp;lt;&amp;gt;(winKey.key(), value);&lt;br/&gt;
+        public K apply(final Windowed&amp;lt;K&amp;gt; winKey, final V value) 
{
+            return winKey.key();
         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
index 64597bdcda6..c0e345ff2b6 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
@@ -23,7 +23,7 @@&lt;br/&gt;
 public class StreamsSmokeTest {&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  args ::= command kafka zookeeper stateDir&lt;br/&gt;
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;command := &quot;run&quot; | &quot;process&quot;&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;@param args&lt;br/&gt;
@@ -32,11 +32,13 @@ public static void main(String[] args) throws InterruptedException {&lt;br/&gt;
         String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         String stateDir = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
         String command = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+        boolean disableAutoTerminate = args.length &amp;gt; 3;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.out.println(&quot;StreamsTest instance started&quot;);&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsSmokeTest)&quot;);&lt;br/&gt;
         System.out.println(&quot;command=&quot; + command);&lt;br/&gt;
         System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
         System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;disableAutoTerminate=&quot; + disableAutoTerminate);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         switch (command) {&lt;br/&gt;
             case &quot;standalone&quot;:&lt;br/&gt;
@@ -46,8 +48,12 @@ public static void main(String[] args) throws InterruptedException {&lt;br/&gt;
                 // this starts the driver (data generation and result verification)&lt;br/&gt;
                 final int numKeys = 10;&lt;br/&gt;
                 final int maxRecordsPerKey = 500;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);&lt;/li&gt;
	&lt;li&gt;SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);&lt;br/&gt;
+                if (disableAutoTerminate) 
{
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
&lt;p&gt;                 break;&lt;br/&gt;
             case &quot;process&quot;:&lt;br/&gt;
                 // this starts a KafkaStreams client&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..5486374b62c&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,72 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires two argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest trunk)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..72d7f5a7b04&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                System.out.println(&quot;closing Kafka Streams instance&quot;);+                System.out.flush();+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..eebd0fab83c&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,114 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 3 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.1)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..18240f04ff1&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,108 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.10.2.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.2)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) &lt;/p&gt;
{
+            // TODO: because Kafka 0.10.2.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }
&lt;p&gt;+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..779021d8672&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,108 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 0.11.0.3 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.11.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.11.0.3 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..4d008c25136&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,108 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be run executed, as long as Kafka 1.0.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) {&lt;br/&gt;
+        if (args.length &amp;lt; 2) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, state-dir, [upgradeFrom: optional]) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String stateDir = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String upgradeFrom = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v1.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;stateDir=&quot; + stateDir);&lt;br/&gt;
+        System.out.println(&quot;upgradeFrom=&quot; + upgradeFrom);&lt;br/&gt;
+&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 1.0.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(&quot;upgrade.from&quot;, upgradeFrom);
+        }&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/tests/docker/Dockerfile b/tests/docker/Dockerfile&lt;br/&gt;
index 149e3911bc6..b6c499756a8 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/docker/Dockerfile&lt;br/&gt;
+++ b/tests/docker/Dockerfile&lt;br/&gt;
@@ -39,13 +39,15 @@ COPY ./ssh-config /root/.ssh/config&lt;br/&gt;
 RUN ssh-keygen -q -t rsa -N &apos;&apos; -f /root/.ssh/id_rsa &amp;amp;&amp;amp; cp -f /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;Install binary test dependencies.&lt;br/&gt;
+# we use the same versions as in vagrant/base.sh&lt;br/&gt;
 ARG KAFKA_MIRROR=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.8.2.2&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.10-0.8.2.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.8.2.2&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.8.2.2&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.8.2.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.8.2.2&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.9.0.1&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.9.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.9.0.1&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.10.0.1&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.0.1&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.10.1.1&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.1.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.1.1&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.10.2.1&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.2.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.2.1&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.11.0.0&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.11.0.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.11.0.0&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.11.0.2&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.11.0.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.11.0.2&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-1.0.1&quot; &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-1.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-1.0.1&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;The version of Kibosh to use for testing.&lt;/li&gt;
	&lt;li&gt;If you update this, also update vagrant/base.sy&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index 54e26bfbc2b..0ed3a4290fb 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -20,6 +20,7 @@&lt;br/&gt;
 from ducktape.utils.util import wait_until&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1&lt;/p&gt;


&lt;p&gt; class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, &quot;tools-log4j.properties&quot;)&lt;br/&gt;
     PID_FILE = os.path.join(PERSISTENT_ROOT, &quot;streams.pid&quot;)&lt;/p&gt;

&lt;p&gt;+    CLEAN_NODE_ENABLED = True&lt;br/&gt;
+&lt;br/&gt;
     logs = {&lt;br/&gt;
         &quot;streams_log&quot;: {&lt;br/&gt;
             &quot;path&quot;: LOG_FILE,&lt;br/&gt;
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):&lt;br/&gt;
         &quot;streams_stderr&quot;: &lt;/p&gt;
{
             &quot;path&quot;: STDERR_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None, user_test_args3=None):&lt;br/&gt;
@@ -108,7 +219,8 @@ def wait_node(self, node, timeout_sec=None):&lt;/p&gt;

&lt;p&gt;     def clean_node(self, node):&lt;br/&gt;
         node.account.kill_process(&quot;streams&quot;, clean_shutdown=False, allow_fail=True)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if self.CLEAN_NODE_ENABLED:&lt;br/&gt;
+            node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def start_cmd(self, node):&lt;br/&gt;
         args = self.args.copy()&lt;br/&gt;
@@ -170,7 +282,28 @@ def clean_node(self, node):&lt;br/&gt;
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
         super(StreamsSmokeTestDriverService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, kafka, &quot;run&quot;)&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def disable_auto_terminate(self):&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;disableAutoTerminate&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;disable_auto_terminate&amp;#39;&amp;#93;&lt;/span&gt; = self.DISABLE_AUTO_TERMINATE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;/p&gt;

&lt;p&gt;+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;/p&gt;

&lt;p&gt; class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
@@ -218,3 +351,40 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, configs):&lt;br/&gt;
                                                                  kafka,&lt;br/&gt;
                                                                  &quot;org.apache.kafka.streams.tests.StreamsBrokerDownResilienceTest&quot;,&lt;br/&gt;
                                                                  configs)&lt;br/&gt;
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
+        super(StreamsUpgradeTestJobRunnerService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context,&lt;br/&gt;
+                                                                 kafka,&lt;br/&gt;
+                                                                 &quot;org.apache.kafka.streams.tests.StreamsUpgradeTest&quot;,&lt;br/&gt;
+                                                                 &quot;&quot;)&lt;br/&gt;
+        self.UPGRADE_FROM = &quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def set_version(self, kafka_streams_version):&lt;br/&gt;
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version&lt;br/&gt;
+&lt;br/&gt;
+    def set_upgrade_from(self, upgrade_from):&lt;br/&gt;
+        self.UPGRADE_FROM = upgrade_from&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
+        else:&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = &quot;&quot;&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;state_dir&amp;#39;&amp;#93;&lt;/span&gt; = self.PERSISTENT_ROOT&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;version&amp;#39;&amp;#93;&lt;/span&gt; = self.KAFKA_STREAMS_VERSION&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;upgrade_from&amp;#39;&amp;#93;&lt;/span&gt; = self.UPGRADE_FROM&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s &quot; \&lt;br/&gt;
+              &quot; %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
index 81b7ffe7047..77833a92be4 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -15,21 +15,48 @@&lt;/p&gt;

&lt;p&gt; from ducktape.mark.resource import cluster&lt;br/&gt;
 from ducktape.tests.test import Test&lt;br/&gt;
-from ducktape.mark import parametrize, ignore&lt;br/&gt;
+from ducktape.mark import ignore, matrix, parametrize&lt;br/&gt;
 from kafkatest.services.kafka import KafkaService&lt;br/&gt;
 from kafkatest.services.zookeeper import ZookeeperService&lt;br/&gt;
-from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService&lt;br/&gt;
-from kafkatest.version import LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, DEV_BRANCH, KafkaVersion&lt;br/&gt;
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService, StreamsUpgradeTestJobRunnerService&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, LATEST_1_0, DEV_BRANCH, DEV_VERSION, KafkaVersion&lt;br/&gt;
+import random&lt;br/&gt;
 import time&lt;/p&gt;

&lt;p&gt;+broker_upgrade_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(DEV_BRANCH)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+simple_upgrade_versions_metadata_version_2 = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(DEV_VERSION)&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; class StreamsUpgradeTest(Test):&lt;br/&gt;
     &quot;&quot;&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Tests rolling upgrades and downgrades of the Kafka Streams library.&lt;br/&gt;
+    Test upgrading Kafka Streams (all version combination)&lt;br/&gt;
+    If metadata was changes, upgrade is more difficult&lt;br/&gt;
+    Metadata version was bumped in 0.10.1.0&lt;br/&gt;
     &quot;&quot;&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
         super(StreamsUpgradeTest, self)._&lt;em&gt;init&lt;/em&gt;_(test_context)&lt;br/&gt;
+        self.topics = {&lt;br/&gt;
+            &apos;echo&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+            &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+    def perform_broker_upgrade(self, to_version):&lt;br/&gt;
+        self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;br/&gt;
+        for node in self.kafka.nodes:&lt;br/&gt;
+            self.kafka.stop_node(node)&lt;br/&gt;
+            node.version = KafkaVersion(to_version)&lt;br/&gt;
+            self.kafka.start_node(node)&lt;br/&gt;
+&lt;br/&gt;
+    @cluster(num_nodes=6)&lt;br/&gt;
+    @matrix(from_version=broker_upgrade_versions, to_version=broker_upgrade_versions)&lt;br/&gt;
+    def test_upgrade_downgrade_brokers(self, from_version, to_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Start a smoke test client then perform rolling upgrades on the broker. &lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        if from_version == to_version:&lt;br/&gt;
+            return&lt;br/&gt;
+&lt;br/&gt;
         self.replication = 3&lt;br/&gt;
         self.partitions = 1&lt;br/&gt;
         self.isr = 2&lt;br/&gt;
@@ -55,45 +82,7 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
             &apos;tagg&apos; : { &apos;partitions&apos;: self.partitions, &apos;replication-factor&apos;: self.replication,&lt;br/&gt;
                        &apos;configs&apos;: &lt;/p&gt;
{&quot;min.insync.replicas&quot;: self.isr}
&lt;p&gt; }&lt;br/&gt;
         }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;def perform_streams_upgrade(self, to_version):&lt;/li&gt;
	&lt;li&gt;self.logger.info(&quot;First pass bounce - rolling streams upgrade&quot;)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;# get the node running the streams app&lt;/li&gt;
	&lt;li&gt;node = self.processor1.node&lt;/li&gt;
	&lt;li&gt;self.processor1.stop()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;# change it&apos;s version. This will automatically make it pick up a different&lt;/li&gt;
	&lt;li&gt;# JAR when it starts again&lt;/li&gt;
	&lt;li&gt;node.version = KafkaVersion(to_version)&lt;/li&gt;
	&lt;li&gt;self.processor1.start()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;def perform_broker_upgrade(self, to_version):&lt;/li&gt;
	&lt;li&gt;self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;/li&gt;
	&lt;li&gt;for node in self.kafka.nodes:&lt;/li&gt;
	&lt;li&gt;self.kafka.stop_node(node)&lt;/li&gt;
	&lt;li&gt;node.version = KafkaVersion(to_version)&lt;/li&gt;
	&lt;li&gt;self.kafka.start_node(node)&lt;br/&gt;
-&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@cluster(num_nodes=6)&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_1), to_version=str(DEV_BRANCH))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_1), to_version=str(LATEST_0_11_0))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_2), to_version=str(LATEST_0_11_0))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_11_0), to_version=str(LATEST_0_10_2))&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(DEV_BRANCH), to_version=str(LATEST_0_10_2))&lt;/li&gt;
	&lt;li&gt;def test_upgrade_downgrade_streams(self, from_version, to_version):&lt;/li&gt;
	&lt;li&gt;&quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;Start a smoke test client, then abort (kill -9) and restart it a few times.&lt;/li&gt;
	&lt;li&gt;Ensure that all records are delivered.&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Note, that just like tests/core/upgrade_test.py, a prerequisite for this test to succeed&lt;/li&gt;
	&lt;li&gt;if the inclusion of all parametrized versions of kafka in kafka/vagrant/base.sh&lt;/li&gt;
	&lt;li&gt;(search for get_kafka()). For streams in particular, that means that someone has manually&lt;/li&gt;
	&lt;li&gt;copies the kafka-stream-$version-test.jar in the right S3 bucket as shown in base.sh.&lt;/li&gt;
	&lt;li&gt;&quot;&quot;&quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;Setup phase&lt;br/&gt;
         self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
         self.zk.start()&lt;br/&gt;
@@ -108,13 +97,12 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
         self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
-&lt;/p&gt;

&lt;p&gt;         self.driver.start()&lt;br/&gt;
         self.processor1.start()&lt;br/&gt;
         time.sleep(15)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.perform_streams_upgrade(to_version)&lt;br/&gt;
+        self.perform_broker_upgrade(to_version)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         time.sleep(15)&lt;br/&gt;
         self.driver.wait()&lt;br/&gt;
@@ -126,42 +114,241 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):&lt;br/&gt;
         node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
         self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;/p&gt;

&lt;p&gt;+    @matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)&lt;br/&gt;
+    def test_simple_upgrade_downgrade(self, from_version, to_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with &amp;lt;old_version&amp;gt;, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
+        &quot;&quot;&quot;&lt;/p&gt;

&lt;p&gt;+        if from_version == to_version:&lt;br/&gt;
+            return&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@cluster(num_nodes=6)&lt;/li&gt;
	&lt;li&gt;@parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))&lt;/li&gt;
	&lt;li&gt;def test_upgrade_brokers(self, from_version, to_version):&lt;br/&gt;
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
+        self.zk.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)&lt;br/&gt;
+        self.kafka.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(from_version)&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # upgrade one-by-one via rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, to_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_1_0)) we cannot run this test until Kafka 1.0.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_1_1)) we cannot run this test until Kafka 1.1.1 is released&lt;br/&gt;
+    @parametrize(new_version=str(DEV_VERSION))&lt;br/&gt;
+    def test_metadata_upgrade(self, new_version):&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;Start a smoke test client then perform rolling upgrades on the broker.&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;# Setup phase&lt;br/&gt;
+&lt;br/&gt;
         self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
         self.zk.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# number of nodes needs to be &amp;gt;= 3 for the smoke test&lt;/li&gt;
	&lt;li&gt;self.kafka = KafkaService(self.test_context, num_nodes=3,&lt;/li&gt;
	&lt;li&gt;zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)&lt;br/&gt;
         self.kafka.start()&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;# allow some time for topics to be created&lt;/li&gt;
	&lt;li&gt;time.sleep(10)&lt;/li&gt;
	&lt;li&gt;&lt;p&gt;+&lt;br/&gt;
         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;br/&gt;
         self.driver.start()&lt;/li&gt;
	&lt;li&gt;self.processor1.start()&lt;/li&gt;
	&lt;li&gt;time.sleep(15)&lt;br/&gt;
+        self.start_all_nodes_with(str(LATEST_0_10_0))&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.perform_broker_upgrade(to_version)&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;time.sleep(15)&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # first rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;0.10.0&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # second rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
         self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
         self.driver.stop()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.processor1.stop()&lt;br/&gt;
+    def start_all_nodes_with(self, version):&lt;br/&gt;
+        # start first with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor1, version)&lt;br/&gt;
+        node1 = self.processor1.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:&lt;br/&gt;
+                self.processor1.start()&lt;br/&gt;
+                log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                       timeout_sec=60,&lt;br/&gt;
+                                       err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node1.account))&lt;br/&gt;
+                monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node = self.driver.node&lt;/li&gt;
	&lt;li&gt;node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;/li&gt;
	&lt;li&gt;self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+        # start second with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor2, version)&lt;br/&gt;
+        node2 = self.processor2.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:&lt;br/&gt;
+                    self.processor2.start()&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node2.account))&lt;br/&gt;
+                    first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                             timeout_sec=60,&lt;br/&gt;
+                                             err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                    second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                              timeout_sec=60,&lt;br/&gt;
+                                              err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start third with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor3, version)&lt;br/&gt;
+        node3 = self.processor3.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:&lt;br/&gt;
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:&lt;br/&gt;
+                        self.processor3.start()&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node3.account))&lt;br/&gt;
+                        first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                 timeout_sec=60,&lt;br/&gt;
+                                                 err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                        second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+                        third_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node3.account))&lt;br/&gt;
+&lt;br/&gt;
+    @staticmethod&lt;br/&gt;
+    def prepare_for(processor, version):&lt;br/&gt;
+        processor.node.account.ssh(&quot;rm -rf &quot; + processor.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(version)&lt;br/&gt;
+&lt;br/&gt;
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
+        first_other_processor = None&lt;br/&gt;
+        second_other_processor = None&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            if p != processor:&lt;br/&gt;
+                if first_other_processor is None:&lt;br/&gt;
+                    first_other_processor = p&lt;br/&gt;
+                else:&lt;br/&gt;
+                    second_other_processor = p&lt;br/&gt;
+&lt;br/&gt;
+        node = processor.node&lt;br/&gt;
+        first_other_node = first_other_processor.node&lt;br/&gt;
+        second_other_node = second_other_processor.node&lt;br/&gt;
+&lt;br/&gt;
+        # stop processor and wait for rebalance of others&lt;br/&gt;
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                processor.stop()&lt;br/&gt;
+                first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                timeout_sec=60,&lt;br/&gt;
+                                                err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+        node.account.ssh_capture(&quot;grep UPGRADE-TEST-CLIENT-CLOSED %s&quot; % processor.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if upgrade_from == &quot;&quot;:  # upgrade disabled &amp;#8211; second round of rolling bounces&lt;br/&gt;
+            roll_counter = &quot;.1-&quot;  # second round of rolling bounces&lt;br/&gt;
+        else:&lt;br/&gt;
+            roll_counter = &quot;.0-&quot;  # first  round of rolling boundes&lt;br/&gt;
+&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDOUT_FILE + &quot; &quot; + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDERR_FILE + &quot; &quot; + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.LOG_FILE + &quot; &quot; + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+        if new_version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(new_version)&lt;br/&gt;
+        processor.set_upgrade_from(upgrade_from)&lt;br/&gt;
+&lt;br/&gt;
+        grep_metadata_error = &quot;grep \&quot;org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\&quot; &quot;&lt;br/&gt;
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:&lt;br/&gt;
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                        processor.start()&lt;br/&gt;
+&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + new_version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + new_version + &quot; &quot; + str(node.account))&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                       timeout_sec=60,&lt;br/&gt;
+                                                       err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                        timeout_sec=60,&lt;br/&gt;
+                                                        err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                        monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node.account))&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index f63a7c17ecd..ee3f8b57ef0 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -61,6 +61,7 @@ def get_version(node=None):&lt;br/&gt;
         return DEV_BRANCH&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; DEV_BRANCH = KafkaVersion(&quot;dev&quot;)&lt;br/&gt;
+DEV_VERSION = KafkaVersion(&quot;1.1.1-SNAPSHOT&quot;)&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;0.8.2.X versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;br/&gt;
@@ -89,7 +90,14 @@ def get_version(node=None):&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; LATEST_0_10 = LATEST_0_10_2&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;0.11.0.0 versions&lt;br/&gt;
+# 0.11.0.x versions&lt;br/&gt;
 V_0_11_0_0 = KafkaVersion(&quot;0.11.0.0&quot;)&lt;br/&gt;
-LATEST_0_11_0 = V_0_11_0_0&lt;br/&gt;
+V_0_11_0_1 = KafkaVersion(&quot;0.11.0.1&quot;)&lt;br/&gt;
+V_0_11_0_2 = KafkaVersion(&quot;0.11.0.2&quot;)&lt;br/&gt;
+LATEST_0_11_0 = V_0_11_0_2&lt;br/&gt;
 LATEST_0_11 = LATEST_0_11_0&lt;br/&gt;
+&lt;br/&gt;
+# 1.0.x versions&lt;br/&gt;
+V_1_0_0 = KafkaVersion(&quot;1.0.0&quot;)&lt;br/&gt;
+V_1_0_1 = KafkaVersion(&quot;1.0.1&quot;)&lt;br/&gt;
+LATEST_1_0 = V_1_0_1&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
diff --git a/vagrant/base.sh b/vagrant/base.sh&lt;br/&gt;
index 4b5540652fa..c520d49971f 100755
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/vagrant/base.sh&lt;br/&gt;
+++ b/vagrant/base.sh&lt;br/&gt;
@@ -99,8 +99,10 @@ popd&lt;br/&gt;
 popd&lt;br/&gt;
 popd&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Test multiple Scala versions&lt;br/&gt;
-get_kafka 0.8.2.2 2.10&lt;br/&gt;
+# Test multiple Kafka versions&lt;br/&gt;
+# we want to use the latest Scala version per Kafka version&lt;br/&gt;
+# however, we cannot pull in Scala 2.12 builds atm, because Scala 2.12 requires Java 8, but we use Java 7 to run the system tests&lt;br/&gt;
+get_kafka 0.8.2.2 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.8.2.2&lt;br/&gt;
 get_kafka 0.9.0.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.9.0.1&lt;br/&gt;
@@ -110,8 +112,10 @@ get_kafka 0.10.1.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.10.1.1&lt;br/&gt;
 get_kafka 0.10.2.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.10.2.1&lt;br/&gt;
-get_kafka 0.11.0.0 2.11&lt;br/&gt;
-chmod a+rw /opt/kafka-0.11.0.0&lt;br/&gt;
+get_kafka 0.11.0.2 2.11&lt;br/&gt;
+chmod a+rw /opt/kafka-0.11.0.2&lt;br/&gt;
+get_kafka 1.0.1 2.11&lt;br/&gt;
+chmod a+rw /opt/kafka-1.0.1&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;ol&gt;
	&lt;li&gt;For EC2 nodes, we want to use /mnt, which should have the local disk. On local&lt;/li&gt;
&lt;/ol&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16429149" author="githubbot" created="Sat, 7 Apr 2018 00:00:56 +0000"  >&lt;p&gt;mjsax closed pull request #4779: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Fix upgrade path from Kafka Streams v0.10.0&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4779&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4779&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh&lt;br/&gt;
index bb786da43ca..4dd092323b0 100755&lt;br/&gt;
&amp;#8212; a/bin/kafka-run-class.sh&lt;br/&gt;
+++ b/bin/kafka-run-class.sh&lt;br/&gt;
@@ -69,28 +69,50 @@ do&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/clients/build/libs/kafka-clients*.jar;&lt;br/&gt;
-do&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if should_include_file &quot;$file&quot;; then&lt;/li&gt;
	&lt;li&gt;CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;/li&gt;
	&lt;li&gt;fi&lt;br/&gt;
-done&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  clients_lib_dir=$(dirname $0)/../clients/build/libs&lt;br/&gt;
+  streams_lib_dir=$(dirname $0)/../streams/build/libs&lt;br/&gt;
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}&lt;br/&gt;
+else&lt;br/&gt;
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs&lt;br/&gt;
+  streams_lib_dir=$clients_lib_dir&lt;br/&gt;
+  rocksdb_lib_dir=$streams_lib_dir&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/build/libs/kafka-streams*.jar;&lt;br/&gt;
+for file in &quot;$clients_lib_dir&quot;/kafka-clients*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;-for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+for file in &quot;$streams_lib_dir&quot;/kafka-streams*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   if should_include_file &quot;$file&quot;; then&lt;br/&gt;
     CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
   fi&lt;br/&gt;
 done&lt;/p&gt;

&lt;p&gt;&lt;del&gt;for file in &quot;$base_dir&quot;/streams/build/dependant-libs&lt;/del&gt;${SCALA_VERSION}/rocksdb*.jar;&lt;br/&gt;
+if [ -z &quot;$UPGRADE_KAFKA_STREAMS_TEST_VERSION&quot; ]; then&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/examples/build/libs/kafka-streams-examples*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+else&lt;br/&gt;
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed &apos;s/\.//g&apos;`&lt;br/&gt;
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;(${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number&lt;br/&gt;
+  for file in &quot;$base_dir&quot;/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;&lt;br/&gt;
+  do&lt;br/&gt;
+    if should_include_file &quot;$file&quot;; then&lt;br/&gt;
+      CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  done&lt;br/&gt;
+fi&lt;br/&gt;
+&lt;br/&gt;
+for file in &quot;$rocksdb_lib_dir&quot;/rocksdb*.jar;&lt;br/&gt;
 do&lt;br/&gt;
   CLASSPATH=&quot;$CLASSPATH&quot;:&quot;$file&quot;&lt;br/&gt;
 done&lt;br/&gt;
diff --git a/build.gradle b/build.gradle&lt;br/&gt;
index 33fa7a750f2..f8daf2fdddc 100644&lt;br/&gt;
&amp;#8212; a/build.gradle&lt;br/&gt;
+++ b/build.gradle&lt;br/&gt;
@@ -1027,6 +1027,66 @@ project(&apos;:streams:examples&apos;) {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;+project(&apos;:streams:upgrade-system-tests-0100&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0100&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0100
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0101&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0101&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0102&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0102&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_0102
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-0110&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-0110&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies {
+    testCompile libs.kafkaStreams_0110
+  }&lt;br/&gt;
+&lt;br/&gt;
+  systemTestLibs {+    dependsOn testJar+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+project(&apos;:streams:upgrade-system-tests-10&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-10&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_10
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
 project(&apos;:jmh-benchmarks&apos;) {&lt;/p&gt;

&lt;p&gt;   apply plugin: &apos;com.github.johnrengelman.shadow&apos;&lt;br/&gt;
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml&lt;br/&gt;
index e3bf15102bd..0fec810a95b 100644&lt;br/&gt;
&amp;#8212; a/checkstyle/suppressions.xml&lt;br/&gt;
+++ b/checkstyle/suppressions.xml&lt;br/&gt;
@@ -189,7 +189,7 @@&lt;br/&gt;
               files=&quot;SmokeTestDriver.java&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;     &amp;lt;suppress checks=&quot;NPathComplexity&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;files=&quot;KStreamKStreamJoinTest.java&quot;/&amp;gt;&lt;br/&gt;
+              files=&quot;KStreamKStreamJoinTest.java|SmokeTestDriver.java&quot;/&amp;gt;&lt;br/&gt;
     &amp;lt;suppress checks=&quot;NPathComplexity&quot;&lt;br/&gt;
               files=&quot;KStreamKStreamLeftJoinTest.java&quot;/&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html&lt;br/&gt;
index d21d505671a..fdc0af1de1f 100644&lt;br/&gt;
&amp;#8212; a/docs/streams/upgrade-guide.html&lt;br/&gt;
+++ b/docs/streams/upgrade-guide.html&lt;br/&gt;
@@ -40,37 +40,64 @@ &amp;lt;h1&amp;gt;Upgrade Guide and API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 1.0.x to 1.1.0 and you have customized window store implementations on the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface&lt;br/&gt;
+        If you want to upgrade from 1.0.x to 1.2.0 and you have customized window store implementations on the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface&lt;br/&gt;
         you&apos;d need to update your code to incorporate the newly added public APIs.&lt;br/&gt;
         Otherwise, if you are using Java 7 you don&apos;t need to make any code changes as the public API is fully backward compatible;&lt;br/&gt;
         but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.&lt;br/&gt;
         Hot-swaping the jar-file only might not work for this case.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_110&quot;&amp;gt;below&amp;lt;/a&amp;gt; for a complete list of 1.1.0 API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_120&quot;&amp;gt;1.2.0&amp;lt;/a&amp;gt; and &amp;lt;a href=&quot;#streams_api_changes_110&quot;&amp;gt;1.1.0&amp;lt;/a&amp;gt;&lt;br/&gt;
+        API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.11.0.x to 1.0.0 you don&apos;t need to make any code changes as the public API is fully backward compatible.&lt;br/&gt;
+        If you want to upgrade from 0.10.2.x or 0.11.0.x to 1.2.x and you have customized window store implementations on the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface&lt;br/&gt;
+        you&apos;d need to update your code to incorporate the newly added public APIs.&lt;br/&gt;
+        Otherwise, if you are using Java 7 you don&apos;t need to do any code changes as the public API is fully backward compatible;&lt;br/&gt;
+        but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.&lt;br/&gt;
         However, some public APIs were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;below&amp;lt;/a&amp;gt; for a complete list of 1.0.0 API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;/li&gt;
	&lt;li&gt;&amp;lt;/p&amp;gt;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;If you want to upgrade from 0.10.2.x to 0.11.0 you don&apos;t need to make any code changes as the public API is fully backward compatible.&lt;/li&gt;
	&lt;li&gt;However, some configuration parameters were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;below&amp;lt;/a&amp;gt; for a complete list of 0.11.0 API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_120&quot;&amp;gt;1.2&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_110&quot;&amp;gt;1.1&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;1.0&amp;lt;/a&amp;gt;, and &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;0.11.0&amp;lt;/a&amp;gt; API&lt;br/&gt;
+        and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        Additionally, Streams API 1.1.x requires broker on-disk message format version 0.10 or higher; thus, you need to make sure that the message&lt;br/&gt;
+        format is configured correctly before you upgrade your Kafka Streams application.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.1.x to 0.10.2, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;/li&gt;
	&lt;li&gt;It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;below&amp;lt;/a&amp;gt; for a complete list of 0.10.2 API and semantic changes that allow you to advance your application and/or simplify your code base.&lt;br/&gt;
+        If you want to upgrade from 0.10.1.x to 1.2.x see the Upgrade Sections for &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1020_streams&quot;&amp;gt;&amp;lt;b&amp;gt;0.10.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1100_streams&quot;&amp;gt;&amp;lt;b&amp;gt;0.11.0&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_100_streams&quot;&amp;gt;&amp;lt;b&amp;gt;1.0&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_100_streams&quot;&amp;gt;&amp;lt;b&amp;gt;1.0&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+        &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_110_streams&quot;&amp;gt;&amp;lt;b&amp;gt;1.2&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;br/&gt;
+        Note, that a brokers on-disk message format must be on version 0.10 or higher to run a Kafka Streams application version 1.2 or higher.&lt;br/&gt;
+        See below a complete list of &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;0.10.2&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;1.0&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_110&quot;&amp;gt;1.1&amp;lt;/a&amp;gt;, and &amp;lt;a href=&quot;#streams_api_changes_120&quot;&amp;gt;1.2&amp;lt;/a&amp;gt;&lt;br/&gt;
+        API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If you want to upgrade from 0.10.0.x to 0.10.1, see the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/#upgrade_1010_streams&quot;&amp;gt;&amp;lt;b&amp;gt;Upgrade Section for 0.10.1&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;.&lt;/li&gt;
	&lt;li&gt;It highlights incompatible changes you need to consider to upgrade your code and application.&lt;/li&gt;
	&lt;li&gt;See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;below&amp;lt;/a&amp;gt; a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.&lt;br/&gt;
+        Upgrading from 0.10.0.x to 1.2.0 directly is also possible.&lt;br/&gt;
+        Note, that a brokers must be on version 0.10.1 or higher and on-disk message format must be on version 0.10 or higher&lt;br/&gt;
+        to run a Kafka Streams application version 1.2 or higher.&lt;br/&gt;
+        See &amp;lt;a href=&quot;#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;,&lt;br/&gt;
+        &amp;lt;a href=&quot;#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;, &amp;lt;a href=&quot;#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+        &amp;lt;a href=&quot;#streams_api_changes_110&quot;&amp;gt;Streams API changes in 1.1&amp;lt;/a&amp;gt;, and &amp;lt;a href=&quot;#streams_api_changes_120&quot;&amp;gt;Streams API changes in 1.2&amp;lt;/a&amp;gt;&lt;br/&gt;
+        for a complete list of API changes.&lt;br/&gt;
+        Upgrading to 1.2.0 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 1.2.0&amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; prepare your newly deployed 1.2.0 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;p&amp;gt; Upgrading from 0.10.0.x to 1.2.0 in offline mode: &amp;lt;/p&amp;gt;&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; restart all new (1.2.0) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     &amp;lt;!-- TODO: verify release verion and update `id` and `href` attributes (also at other places that link to this headline) --&amp;gt;&lt;br/&gt;
     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_120&quot; href=&quot;#streams_api_changes_120&quot;&amp;gt;Streams API changes in 1.2.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index 0c5f5fd3247..95f2c418c3b 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -68,6 +68,7 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_120_notable&quot; href=&quot;#upgrade_120_notable&quot;&amp;gt;Notable changes in 1&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;&amp;lt;a href=&quot;https://cwiki.apache.org/confluence/x/oYtjB&quot;&amp;gt;KIP-186&amp;lt;/a&amp;gt; increases the default offset retention time from 1 day to 7 days. This makes it less likely to &quot;lose&quot; offsets in an application that commits infrequently. It also increases the active set of offsets and therefore can increase memory usage on the broker. Note that the console consumer currently enables offset commit by default and can be the source of a large number of offsets which this change will now preserve for 7 days instead of 1. You can preserve the existing behavior by setting the broker config &amp;lt;code&amp;gt;offsets.retention.minutes&amp;lt;/code&amp;gt; to 1440.&amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;&amp;lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5674&quot;&amp;gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5674&quot; title=&quot;max.connections.per.ip minimum value to be zero to allow IP address blocking&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5674&quot;&gt;&lt;del&gt;KAFKA-5674&lt;/del&gt;&lt;/a&gt;&amp;lt;/a&amp;gt; extends the lower interval of &amp;lt;code&amp;gt;max.connections.per.ip minimum&amp;lt;/code&amp;gt; to zero and therefore allows IP-based filtering of inbound connections.&amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from older version. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_120_new_protocols&quot; href=&quot;#upgrade_120_new_protocols&quot;&amp;gt;New Protocol Versions&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
@@ -76,7 +77,7 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_120_new_protocols&quot; href=&quot;#upgrade_120_new_protocols&quot;&amp;gt;New Prot&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_120_streams&quot; href=&quot;#upgrade_120_streams&quot;&amp;gt;Upgrading a 1.2.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 1.1.0 to 1.2.0 does not require a broker upgrade.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A Kafka Streams 1.2.0 application can connect to 1.2, 1.1, 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+         A Kafka Streams 1.2.0 application can connect to 1.2, 1.1, 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_120&quot;&amp;gt;Streams API changes in 1.2.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -125,6 +126,14 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_1_1_0&quot; href=&quot;#upgrade_1_1_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.1&lt;br/&gt;
         Hot-swaping the jar-file only might not work.&amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ol&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;!-- TODO add if 1.1.1 gets release&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_111_notable&quot; href=&quot;#upgrade_111_notable&quot;&amp;gt;Notable changes in 1.1.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+--&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_110_notable&quot; href=&quot;#upgrade_110_notable&quot;&amp;gt;Notable changes in 1.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;The kafka artifact in Maven no longer depends on log4j or slf4j-log4j12. Similarly to the kafka-clients artifact, users&lt;br/&gt;
@@ -196,6 +205,14 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_1_0_0&quot; href=&quot;#upgrade_1_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.1&lt;br/&gt;
         Similarly for the message format version.&amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ol&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;!-- TODO add if 1.0.2 gets release&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_102_notable&quot; href=&quot;#upgrade_102_notable&quot;&amp;gt;Notable changes in 1.0.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+--&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_101_notable&quot; href=&quot;#upgrade_101_notable&quot;&amp;gt;Notable changes in 1.0.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;Restored binary compatibility of AdminClient&apos;s Options classes (e.g. CreateTopicsOptions, DeleteTopicsOptions, etc.) with&lt;br/&gt;
@@ -261,17 +278,74 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_new_protocols&quot; href=&quot;#upgrade_100_new_protocols&quot;&amp;gt;New Prot&lt;br/&gt;
          be used if the SaslHandshake request version is greater than 0. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;-&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams&quot; href=&quot;#upgrade_100_streams&quot;&amp;gt;Upgrading a 1.0.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams&quot; href=&quot;#upgrade_100_streams&quot;&amp;gt;Upgrading a 0.11.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.11.0 to 1.0.0 does not require a broker upgrade.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).&lt;/li&gt;
	&lt;li&gt;However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. &amp;lt;/li&amp;gt;&lt;br/&gt;
+         A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).&lt;br/&gt;
+         However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;/li&gt;
	&lt;li&gt;We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0102&quot; href=&quot;#upgrade_100_streams_from_0102&quot;&amp;gt;Upgrading a 0.10.2 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.2 to 1.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0101&quot; href=&quot;#upgrade_1100_streams_from_0101&quot;&amp;gt;Upgrading a 0.10.1 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.1 to 1.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are a few public APIs including &amp;lt;code&amp;gt;ProcessorContext#schedule()&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;Processor#punctuate()&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;TopologyBuilder&amp;lt;/code&amp;gt; are being deprecated by new APIs.&lt;br/&gt;
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the &amp;lt;code&amp;gt;TimestampExtractor&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you register custom metrics, you will need to update this code, because the &amp;lt;code&amp;gt;StreamsMetric&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_100_streams_from_0100&quot; href=&quot;#upgrade_100_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 1.0 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 1.0 application can only connect to 0.1, 0.11.0, 0.10.2, or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_100&quot;&amp;gt;Streams API changes in 1.0.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;!-- TODO add if 1.0.2 gets release&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 1.0.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.11.0.3 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    --&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (1.0.0 or 1.0.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
 &amp;lt;p&amp;gt;Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below,&lt;br/&gt;
   you guarantee no downtime during the upgrade. However, please review the &amp;lt;a href=&quot;#upgrade_1100_notable&quot;&amp;gt;notable changes in 0.11.0.0&amp;lt;/a&amp;gt; before upgrading.&lt;br/&gt;
@@ -291,7 +365,7 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0&lt;br/&gt;
         &amp;lt;ul&amp;gt;&lt;br/&gt;
             &amp;lt;li&amp;gt;inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1 or 0.10.2).&amp;lt;/li&amp;gt;&lt;br/&gt;
             &amp;lt;li&amp;gt;log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION  (See &amp;lt;a href=&quot;#upgrade_10_performance_impact&quot;&amp;gt;potential performance impact&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;following the upgrade&amp;lt;/a&amp;gt; for the details on what this configuration does.)&amp;lt;/li&amp;gt;&lt;br/&gt;
+        following the upgrade&amp;lt;/a&amp;gt; for the details on what this configuration does.)&amp;lt;/li&amp;gt;&lt;br/&gt;
         &amp;lt;/ul&amp;gt;&lt;br/&gt;
     &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. &amp;lt;/li&amp;gt;&lt;br/&gt;
@@ -320,11 +394,59 @@ &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_11_0_0&quot; href=&quot;#upgrade_11_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams&quot; href=&quot;#upgrade_1100_streams&quot;&amp;gt;Upgrading a 0.10.2 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.2 to 0.11.0 does not require a broker upgrade.&lt;/li&gt;
	&lt;li&gt;A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams_from_0101&quot; href=&quot;#upgrade_1100_streams_from_0101&quot;&amp;gt;Upgrading a 0.10.1 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.1 to 0.11.0 does not require a broker upgrade.&lt;br/&gt;
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you specify customized &amp;lt;code&amp;gt;key.serde&amp;lt;/code&amp;gt;, &amp;lt;code&amp;gt;value.serde&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;timestamp.extractor&amp;lt;/code&amp;gt; in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the &amp;lt;code&amp;gt;TimestampExtractor&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you register custom metrics, you will need to update this code, because the &amp;lt;code&amp;gt;StreamsMetric&amp;lt;/code&amp;gt; interface was changed. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt; and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_streams_from_0100&quot; href=&quot;#upgrade_1100_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.11.0 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.11.0 application can only connect to 0.11.0, 0.10.2, or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0110&quot;&amp;gt;Streams API changes in 0.11.0&amp;lt;/a&amp;gt;,&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt;, and&lt;br/&gt;
+         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;!-- TODO add if 0.11.0.3 gets release&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.11.0.3 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+        (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.11.0.3 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    --&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.11.0.0 , 0.11.0.1, or 0.11.0.2) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;!-- TODO add if 0.11.0.3 gets release&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1103_notable&quot; href=&quot;#upgrade_1103_notable&quot;&amp;gt;Notable changes in 0.11.0.3&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+&amp;lt;li&amp;gt; New Kafka Streams configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;li&amp;gt; See the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide.html&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams upgrade guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; for details about this new config.&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+--&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1100_notable&quot; href=&quot;#upgrade_1100_notable&quot;&amp;gt;Notable changes in 0.11.0.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt;Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to&lt;br/&gt;
@@ -475,6 +597,39 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams&quot; href=&quot;#upgrade_1020_streams&quot;&amp;gt;Upgrading a 0.10.1&lt;br/&gt;
     &amp;lt;li&amp;gt; See &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details. &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1020_streams_from_0100&quot; href=&quot;#upgrade_1020_streams_from_0100&quot;&amp;gt;Upgrading a 0.10.0 Kafka Streams Application&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.2 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams#streams_api_changes_0102&quot;&amp;gt;Streams API changes in 0.10.2&amp;lt;/a&amp;gt; for more details).&lt;br/&gt;
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;!-- TODO add if 0.10.2.2 gets release&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.2.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    --&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt; restart all new (0.10.2.0 or 0.10.2.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;!-- TODO add if 0.10.2.2 gets release&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10202_notable&quot; href=&quot;#upgrade_10202_notable&quot;&amp;gt;Notable changes in 0.10.2.2&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+&amp;lt;li&amp;gt; New configuration parameter &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; added that allows rolling bounce upgrade from version 0.10.0.x &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+--&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
 &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_10201_notable&quot; href=&quot;#upgrade_10201_notable&quot;&amp;gt;Notable changes in 0.10.2.1&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
   &amp;lt;li&amp;gt; The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer &amp;lt;code&amp;gt;retries&amp;lt;/code&amp;gt; default value was changed from 0 to 10. The internal Kafka Streams consumer &amp;lt;code&amp;gt;max.poll.interval.ms&amp;lt;/code&amp;gt;  default value was changed from 300000 to &amp;lt;code&amp;gt;Integer.MAX_VALUE&amp;lt;/code&amp;gt;.&lt;br/&gt;
@@ -552,7 +707,26 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_streams&quot; href=&quot;#upgrade_1010_streams&quot;&amp;gt;Upgrading a 0.10.0&lt;br/&gt;
 &amp;lt;ul&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; Upgrading your Streams application from 0.10.0 to 0.10.1 does require a &amp;lt;a href=&quot;#upgrade_10_1&quot;&amp;gt;broker upgrade&amp;lt;/a&amp;gt; because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. &amp;lt;/li&amp;gt;&lt;br/&gt;
     &amp;lt;li&amp;gt; There are couple of API changes, that are not backward compatible (cf. &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/upgrade-guide#streams_api_changes_0101&quot;&amp;gt;Streams API changes in 0.10.1&amp;lt;/a&amp;gt; for more details).&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+     Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;!-- TODO add if 0.10.1.2 gets release&lt;br/&gt;
+        &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config &amp;lt;code&amp;gt;upgrade.from=&quot;0.10.0&quot;&amp;lt;/code&amp;gt; set for first upgrade phase&lt;br/&gt;
+         (cf. &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade&quot;&amp;gt;KIP-268&amp;lt;/a&amp;gt;).&lt;br/&gt;
+         As an alternative, an offline upgrade is also possible.&lt;br/&gt;
+            &amp;lt;ul&amp;gt;&lt;br/&gt;
+                &amp;lt;li&amp;gt; prepare your application instances for a rolling bounce and make sure that config &amp;lt;code&amp;gt;upgrade.from&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot;0.10.0&quot;&amp;lt;/code&amp;gt; for new version 0.10.1.2 &amp;lt;/li&amp;gt;&lt;br/&gt;
+                &amp;lt;li&amp;gt; bounce each instance of your application once &amp;lt;/li&amp;gt;&lt;br/&gt;
+                &amp;lt;li&amp;gt; prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config &amp;lt;code&amp;gt;upgrade.mode&amp;lt;/code&amp;gt; &amp;lt;/li&amp;gt;&lt;br/&gt;
+                &amp;lt;li&amp;gt; bounce each instance of your application once more to complete the upgrade &amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;/ul&amp;gt;&lt;br/&gt;
+        &amp;lt;/li&amp;gt;&lt;br/&gt;
+        --&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; stop all old (0.10.0.x) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; update your code and swap old code and jar file with new code and new jar file &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; restart all new (0.10.1.0 or 0.10.1.1) application instances &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_notable&quot; href=&quot;#upgrade_1010_notable&quot;&amp;gt;Notable changes in 0.10.1.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
@@ -596,14 +770,17 @@ &amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_1010_new_protocols&quot; href=&quot;#upgrade_1010_new_protocols&quot;&amp;gt;New Pr&lt;br/&gt;
 &amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_10&quot; href=&quot;#upgrade_10&quot;&amp;gt;Upgrading from 0.8.x or 0.9.x to 0.10.0.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
+&amp;lt;p&amp;gt;&lt;br/&gt;
 0.10.0.0 has &amp;lt;a href=&quot;#upgrade_10_breaking&quot;&amp;gt;potential breaking changes&amp;lt;/a&amp;gt; (please review before upgrading) and possible &amp;lt;a href=&quot;#upgrade_10_performance_impact&quot;&amp;gt;  performance impact following the upgrade&amp;lt;/a&amp;gt;. By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade.&lt;br/&gt;
 &amp;lt;br&amp;gt;&lt;br/&gt;
 Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.&lt;br/&gt;
-&amp;lt;p/&amp;gt;&lt;br/&gt;
+&amp;lt;/p&amp;gt;&lt;br/&gt;
+&amp;lt;p&amp;gt;&lt;br/&gt;
 &amp;lt;b&amp;gt;Notes to clients with version 0.9.0.0: &amp;lt;/b&amp;gt;Due to a bug introduced in 0.9.0.0,&lt;br/&gt;
 clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not&lt;br/&gt;
 work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 &amp;lt;b&amp;gt;before&amp;lt;/b&amp;gt; brokers are upgraded to&lt;br/&gt;
 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.&lt;br/&gt;
+&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;For a rolling upgrade:&amp;lt;/b&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle&lt;br/&gt;
index 32c0040d0ff..effe763ac45 100644&lt;br/&gt;
&amp;#8212; a/gradle/dependencies.gradle&lt;br/&gt;
+++ b/gradle/dependencies.gradle&lt;br/&gt;
@@ -62,6 +62,11 @@ versions += [&lt;br/&gt;
   jaxb: &quot;2.3.0&quot;,&lt;br/&gt;
   jopt: &quot;5.0.4&quot;,&lt;br/&gt;
   junit: &quot;4.12&quot;,&lt;br/&gt;
+  kafka_0100: &quot;0.10.0.1&quot;,&lt;br/&gt;
+  kafka_0101: &quot;0.10.1.1&quot;,&lt;br/&gt;
+  kafka_0102: &quot;0.10.2.1&quot;,&lt;br/&gt;
+  kafka_0110: &quot;0.11.0.2&quot;,&lt;br/&gt;
+  kafka_10: &quot;1.0.1&quot;,&lt;br/&gt;
   lz4: &quot;1.4.1&quot;,&lt;br/&gt;
   metrics: &quot;2.2.0&quot;,&lt;br/&gt;
   // PowerMock 1.x doesn&apos;t support Java 9, so use PowerMock 2.0.0 beta&lt;br/&gt;
@@ -101,12 +106,16 @@ libs += [&lt;br/&gt;
   jettyServlets: &quot;org.eclipse.jetty:jetty-servlets:$versions.jetty&quot;,&lt;br/&gt;
   jerseyContainerServlet: &quot;org.glassfish.jersey.containers:jersey-container-servlet:$versions.jersey&quot;,&lt;br/&gt;
   jmhCore: &quot;org.openjdk.jmh:jmh-core:$versions.jmh&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;jmhGeneratorAnnProcess: &quot;org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh&quot;,&lt;br/&gt;
   jmhCoreBenchmarks: &quot;org.openjdk.jmh:jmh-core-benchmarks:$versions.jmh&quot;,&lt;br/&gt;
+  jmhGeneratorAnnProcess: &quot;org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh&quot;,&lt;br/&gt;
+  joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
   junit: &quot;junit:junit:$versions.junit&quot;,&lt;br/&gt;
+  kafkaStreams_0100: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0100&quot;,&lt;br/&gt;
+  kafkaStreams_0101: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0101&quot;,&lt;br/&gt;
+  kafkaStreams_0102: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0102&quot;,&lt;br/&gt;
+  kafkaStreams_0110: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0110&quot;,&lt;br/&gt;
+  kafkaStreams_10: &quot;org.apache.kafka:kafka-streams:$versions.kafka_10&quot;,&lt;br/&gt;
   log4j: &quot;log4j:log4j:$versions.log4j&quot;,&lt;/li&gt;
	&lt;li&gt;scalaLogging: &quot;com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging&quot;,&lt;/li&gt;
	&lt;li&gt;joptSimple: &quot;net.sf.jopt-simple:jopt-simple:$versions.jopt&quot;,&lt;br/&gt;
   lz4: &quot;org.lz4:lz4-java:$versions.lz4&quot;,&lt;br/&gt;
   metrics: &quot;com.yammer.metrics:metrics-core:$versions.metrics&quot;,&lt;br/&gt;
   powermockJunit4: &quot;org.powermock:powermock-module-junit4:$versions.powermock&quot;,&lt;br/&gt;
@@ -114,6 +123,7 @@ libs += [&lt;br/&gt;
   reflections: &quot;org.reflections:reflections:$versions.reflections&quot;,&lt;br/&gt;
   rocksDBJni: &quot;org.rocksdb:rocksdbjni:$versions.rocksDB&quot;,&lt;br/&gt;
   scalaLibrary: &quot;org.scala-lang:scala-library:$versions.scala&quot;,&lt;br/&gt;
+  scalaLogging: &quot;com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging&quot;,&lt;br/&gt;
   scalaReflect: &quot;org.scala-lang:scala-reflect:$versions.scala&quot;,&lt;br/&gt;
   scalatest: &quot;org.scalatest:scalatest_$versions.baseScala:$versions.scalatest&quot;,&lt;br/&gt;
   scoveragePlugin: &quot;org.scoverage:scalac-scoverage-plugin_$versions.baseScala:$versions.scoverage&quot;,&lt;br/&gt;
diff --git a/settings.gradle b/settings.gradle&lt;br/&gt;
index e599d01215c..03136849fd5 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/settings.gradle&lt;br/&gt;
+++ b/settings.gradle&lt;br/&gt;
@@ -13,5 +13,7 @@&lt;br/&gt;
 // See the License for the specific language governing permissions and&lt;br/&gt;
 // limitations under the License.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:test-utils&apos;, &apos;streams:examples&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
+include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:test-utils&apos;, &apos;streams:examples&apos;,&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0100&apos;, &apos;streams:upgrade-system-tests-0101&apos;, &apos;streams:upgrade-system-tests-0102&apos;,&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0110&apos;, &apos;streams:upgrade-system-tests-10&apos;, &apos;log4j-appender&apos;,&lt;br/&gt;
         &apos;connect:api&apos;, &apos;connect:transforms&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;, &apos;jmh-benchmarks&apos;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index 0a525169fa6..819bebd43b6 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -167,6 +167,11 @@&lt;br/&gt;
      */&lt;br/&gt;
     public static final String ADMIN_CLIENT_PREFIX = &quot;admin.&quot;;&lt;/p&gt;

&lt;p&gt;+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.0.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #PROCESSING_GUARANTEE_CONFIG &quot;processing.guarantee&quot;}
&lt;p&gt; for at-least-once processing guarantees.&lt;br/&gt;
      */&lt;br/&gt;
@@ -340,6 +345,11 @@&lt;br/&gt;
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = &quot;timestamp.extractor&quot;;&lt;br/&gt;
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = &quot;Timestamp extractor class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.processor.TimestampExtractor&amp;lt;/code&amp;gt; interface. This config is deprecated, use &amp;lt;code&amp;gt;&quot; + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + &quot;&amp;lt;/code&amp;gt; instead&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
+    public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;br/&gt;
+    public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+        &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot; (for upgrading from 0.10.0.x).&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code value.serde}&lt;/li&gt;
	&lt;li&gt;@deprecated Use 
{@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG}
&lt;p&gt; instead.&lt;br/&gt;
@@ -562,6 +572,12 @@&lt;br/&gt;
                     10 * 60 * 1000L,&lt;br/&gt;
                     Importance.LOW,&lt;br/&gt;
                     STATE_CLEANUP_DELAY_MS_DOC)&lt;br/&gt;
+            .define(UPGRADE_FROM_CONFIG,&lt;br/&gt;
+                    ConfigDef.Type.STRING,&lt;br/&gt;
+                    null,&lt;br/&gt;
+                    in(null, UPGRADE_FROM_0100),&lt;br/&gt;
+                    Importance.LOW,&lt;br/&gt;
+                    UPGRADE_FROM_DOC)&lt;br/&gt;
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,&lt;br/&gt;
                     Type.LONG,&lt;br/&gt;
                     24 * 60 * 60 * 1000L,&lt;br/&gt;
@@ -793,6 +809,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;br/&gt;
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + &quot;-consumer&quot;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add configs required for stream partition assignor&lt;br/&gt;
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));&lt;br/&gt;
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));&lt;br/&gt;
         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));&lt;br/&gt;
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
index 0edbe2f523d..97771e56879 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
@@ -179,6 +179,8 @@ public int compare(final TopicPartition p1,&lt;br/&gt;
     private TaskManager taskManager;&lt;br/&gt;
     private PartitionGrouper partitionGrouper;&lt;/p&gt;

&lt;p&gt;+    private int userMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;&lt;br/&gt;
+&lt;br/&gt;
     private InternalTopicManager internalTopicManager;&lt;br/&gt;
     private CopartitionedTopicsValidator copartitionedTopicsValidator;&lt;/p&gt;

&lt;p&gt;@@ -197,6 +199,12 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         final LogContext logContext = new LogContext(logPrefix);&lt;br/&gt;
         log = logContext.logger(getClass());&lt;/p&gt;

&lt;p&gt;+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;br/&gt;
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) &lt;/p&gt;
{
+            log.info(&quot;Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x.&quot;);
+            userMetadataVersion = 1;
+        }
&lt;p&gt;+&lt;br/&gt;
         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);&lt;br/&gt;
         if (o == null) {&lt;br/&gt;
             final KafkaException fatalException = new KafkaException(&quot;TaskManager is not specified&quot;);&lt;br/&gt;
@@ -255,6 +263,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks = taskManager.cachedTasksIds();&lt;br/&gt;
         standbyTasks.removeAll(previousActiveTasks);&lt;br/&gt;
         final SubscriptionInfo data = new SubscriptionInfo(&lt;br/&gt;
+            userMetadataVersion,&lt;br/&gt;
             taskManager.processId(),&lt;br/&gt;
             previousActiveTasks,&lt;br/&gt;
             standbyTasks,&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
index 0309659042d..87f70759220 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java&lt;br/&gt;
@@ -472,6 +472,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() &lt;/p&gt;
{
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -506,6 +507,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() &lt;/p&gt;
{
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -519,6 +521,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -532,6 +535,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -545,6 +549,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldSpecifyCorrectValueSerdeClassOnError() {&lt;br/&gt;
         final Properties props = minimalStreamsConfig();&lt;br/&gt;
@@ -567,9 +572,7 @@ public void configure(final Map configs, final boolean isKey) {&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void close() 
{
-
-        }
&lt;p&gt;+        public void close() {}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public Serializer serializer() {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index 4c12bb93544..44e139a28bd 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -313,6 +313,4 @@ private void startStreams() {&lt;/p&gt;

&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
index b0c0d68287b..e9ed9682066 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
@@ -131,7 +131,7 @@ private void configurePartitionAssignor(final Map&amp;lt;String, Object&amp;gt; props) {&lt;br/&gt;
     private void mockTaskManager(final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
                                  final Set&amp;lt;TaskId&amp;gt; cachedTasks,&lt;br/&gt;
                                  final UUID processId,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final InternalTopologyBuilder builder) throws NoSuchFieldException, IllegalAccessException {&lt;br/&gt;
+                                 final InternalTopologyBuilder builder) {&lt;br/&gt;
         EasyMock.expect(taskManager.builder()).andReturn(builder).anyTimes();&lt;br/&gt;
         EasyMock.expect(taskManager.prevActiveTaskIds()).andReturn(prevTasks).anyTimes();&lt;br/&gt;
         EasyMock.expect(taskManager.cachedTasksIds()).andReturn(cachedTasks).anyTimes();&lt;br/&gt;
@@ -167,7 +167,7 @@ public void shouldInterleaveTasksByGroupId() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testSubscription() throws Exception {&lt;br/&gt;
+    public void testSubscription() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -195,7 +195,7 @@ public void testSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignBasic() throws Exception {&lt;br/&gt;
+    public void testAssignBasic() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -235,11 +235,9 @@ public void testAssignBasic() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -259,7 +257,7 @@ public void testAssignBasic() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws Exception {&lt;br/&gt;
+    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() 
{
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;);
@@ -327,7 +325,7 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
+    public void testAssignWithPartialTopology() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor1&quot;, new MockProcessorSupplier(), &quot;source1&quot;);&lt;br/&gt;
         builder.addStateStore(new MockStateStoreSupplier(&quot;store1&quot;, false), &quot;processor1&quot;);&lt;br/&gt;
@@ -352,9 +350,8 @@ public void testAssignWithPartialTopology() throws Exception {&lt;br/&gt;
         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(&quot;topic1&quot;), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -362,7 +359,7 @@ public void testAssignWithPartialTopology() throws Exception {&lt;/p&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
+    public void testAssignEmptyMetadata() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;br/&gt;
@@ -392,9 +389,8 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
             new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Collections.&amp;lt;String&amp;gt;emptySet(), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, allActiveTasks.size());&lt;br/&gt;
         assertEquals(Collections.&amp;lt;TaskId&amp;gt;emptySet(), new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -417,7 +413,7 @@ public void testAssignEmptyMetadata() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
+    public void testAssignWithNewTasks() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source3&quot;, null, null, null, &quot;topic3&quot;);&lt;br/&gt;
@@ -450,13 +446,9 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match&lt;br/&gt;
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and&lt;br/&gt;
         // then later ones will be re-assigned to other hosts due to load balancing&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;AssignmentInfo info;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info.activeTasks());&lt;/li&gt;
	&lt;li&gt;allPartitions.addAll(assignments.get(&quot;consumer10&quot;).partitions());&lt;br/&gt;
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info.activeTasks());&lt;br/&gt;
+        Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());&lt;br/&gt;
         allActiveTasks.addAll(info.activeTasks());&lt;br/&gt;
@@ -471,7 +463,7 @@ public void testAssignWithNewTasks() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStates() throws Exception {&lt;br/&gt;
+    public void testAssignWithStates() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
@@ -542,7 +534,10 @@ public void testAssignWithStates() throws Exception 
{
         assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, &quot;store3&quot;, tasks, topicGroups));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Set&amp;lt;TaskId&amp;gt; tasksForState(String applicationId, String storeName, List&amp;lt;TaskId&amp;gt; tasks, Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups) {&lt;br/&gt;
+    private Set&amp;lt;TaskId&amp;gt; tasksForState(final String applicationId,&lt;br/&gt;
+                                      final String storeName,&lt;br/&gt;
+                                      final List&amp;lt;TaskId&amp;gt; tasks,&lt;br/&gt;
+                                      final Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups) {&lt;br/&gt;
         final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, storeName);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Set&amp;lt;TaskId&amp;gt; ids = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -560,7 +555,7 @@ public void testAssignWithStates() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
+    public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         Map&amp;lt;String, Object&amp;gt; props = configProps();&lt;br/&gt;
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);&lt;br/&gt;
         StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
@@ -598,13 +593,10 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         // the first consumer&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;allActiveTasks.addAll(info10.activeTasks());&lt;/li&gt;
	&lt;li&gt;allStandbyTasks.addAll(info10.standbyTasks().keySet());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;br/&gt;
+        Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;(info10.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;br/&gt;
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
@@ -632,7 +624,7 @@ public void testAssignWithStandbyReplicas() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testOnAssignment() throws Exception {&lt;br/&gt;
+    public void testOnAssignment() {&lt;br/&gt;
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;TaskId&amp;gt; activeTaskList = Utils.mkList(task0, task3);&lt;br/&gt;
@@ -667,7 +659,7 @@ public void testOnAssignment() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopics() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addInternalTopic(&quot;topicX&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
@@ -697,7 +689,7 @@ public void testAssignWithInternalTopics() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {&lt;br/&gt;
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() 
{
         String applicationId = &quot;test&quot;;
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(&quot;topicX&quot;);
@@ -732,7 +724,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldGenerateTasksForAllCreatedPartitions() throws Exception {&lt;br/&gt;
+    public void shouldGenerateTasksForAllCreatedPartitions() {&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);&lt;br/&gt;
@@ -832,7 +824,7 @@ public Object apply(final Object value1, final Object value2) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
+    public void shouldAddUserDefinedEndPointToSubscription() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source&quot;, null, null, null, &quot;input&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source&quot;);&lt;br/&gt;
@@ -851,7 +843,7 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
+    public void shouldMapUserEndPointToTopicPartitions() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addSource(null, &quot;source&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source&quot;);&lt;br/&gt;
@@ -881,7 +873,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {&lt;br/&gt;
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), UUID.randomUUID(), builder);&lt;br/&gt;
@@ -908,7 +900,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() throws Exception {&lt;br/&gt;
+    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() {&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);&lt;br/&gt;
@@ -1010,7 +1002,7 @@ public Object apply(final Object value1, final Object value2) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exception {&lt;br/&gt;
+    public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() 
{
         final TopicPartition partitionOne = new TopicPartition(&quot;topic&quot;, 1);
         final TopicPartition partitionTwo = new TopicPartition(&quot;topic&quot;, 2);
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState = Collections.singletonMap(
@@ -1028,7 +1020,7 @@ public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exceptio
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {&lt;br/&gt;
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() 
{
         final StreamsBuilder builder = new StreamsBuilder();
 
         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);
@@ -1096,7 +1088,7 @@ public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProvider
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() throws Exception {&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() 
{
         final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
         final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
         subscriptions.put(
@@ -1114,10 +1106,11 @@ public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(
             )
         );
 
-        mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(),
-            Collections.&amp;lt;TaskId&amp;gt;emptySet(),
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
             UUID.randomUUID(),
-            new InternalTopologyBuilder());
+            builder);
         partitionAssignor.configure(configProps());
         final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
 
@@ -1126,6 +1119,22 @@ public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(
         assertThat(AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version(), equalTo(1));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscription() &lt;/p&gt;
{
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, (Object) StreamsConfig.UPGRADE_FROM_0100));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+
+        assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(1));
+    }
&lt;p&gt;+&lt;br/&gt;
     private PartitionAssignor.Assignment createAssignment(final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState) {&lt;br/&gt;
         final AssignmentInfo info = new AssignmentInfo(Collections.&amp;lt;TaskId&amp;gt;emptyList(),&lt;br/&gt;
                                                        Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(),&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index 726a5623cd5..c1020a98ba9 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -68,7 +68,6 @@ public void shouldDecodePreviousVersion() throws IOException &lt;/p&gt;
{
         assertEquals(1, decoded.version());
     }

&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;/li&gt;
	&lt;li&gt;so it is impossible to test compatibility without having this&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
index e2493b2c5a0..2936c63d8f8 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
+import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
 import org.apache.kafka.streams.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
@@ -30,10 +31,13 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KTable;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Materialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Predicate;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.Produced;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Serialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.TimeWindows;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.ValueJoiner;&lt;br/&gt;
+import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
 import org.apache.kafka.streams.state.Stores;&lt;br/&gt;
+import org.apache.kafka.streams.state.WindowStore;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.util.Properties;&lt;br/&gt;
 import java.util.concurrent.TimeUnit;&lt;br/&gt;
@@ -56,7 +60,7 @@ public void start() {&lt;br/&gt;
         streams = createKafkaStreams(streamsProperties, kafka);&lt;br/&gt;
         streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {&lt;br/&gt;
             @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void uncaughtException(Thread t, Throwable e) {&lt;br/&gt;
+            public void uncaughtException(final Thread t, final Throwable e) {&lt;br/&gt;
                 System.out.println(&quot;SMOKE-TEST-CLIENT-EXCEPTION&quot;);&lt;br/&gt;
                 uncaughtException = true;&lt;br/&gt;
                 e.printStackTrace();&lt;br/&gt;
@@ -93,38 +97,45 @@ public void close() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static KafkaStreams createKafkaStreams(final Properties props, final String kafka) {&lt;/li&gt;
	&lt;li&gt;props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;SmokeTest&quot;);&lt;/li&gt;
	&lt;li&gt;props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;/li&gt;
	&lt;li&gt;props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);&lt;/li&gt;
	&lt;li&gt;props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);&lt;/li&gt;
	&lt;li&gt;props.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);&lt;/li&gt;
	&lt;li&gt;props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;/li&gt;
	&lt;li&gt;props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);&lt;/li&gt;
	&lt;li&gt;props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;/li&gt;
	&lt;li&gt;props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);&lt;/li&gt;
	&lt;li&gt;props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);&lt;br/&gt;
+    private static Properties getStreamsConfig(final Properties props, final String kafka) 
{
+        final Properties config = new Properties(props);
+        config.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;SmokeTest&quot;);
+        config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);
+        config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);
+        config.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);
+        config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
+        config.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
+        config.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);
         //TODO remove this config or set to smaller value when KIP-91 is merged
-        props.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 80000);
+        config.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 80000);
+
+        config.putAll(props);
+        return config;
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
	&lt;li&gt;Consumed&amp;lt;String, Integer&amp;gt; stringIntConsumed = Consumed.with(stringSerde, intSerde);&lt;/li&gt;
	&lt;li&gt;KStream&amp;lt;String, Integer&amp;gt; source = builder.stream(&quot;data&quot;, stringIntConsumed);&lt;/li&gt;
	&lt;li&gt;source.to(stringSerde, intSerde, &quot;echo&quot;);&lt;/li&gt;
	&lt;li&gt;KStream&amp;lt;String, Integer&amp;gt; data = source.filter(new Predicate&amp;lt;String, Integer&amp;gt;() {&lt;br/&gt;
+    private static KafkaStreams createKafkaStreams(final Properties props, final String kafka) {&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final Consumed&amp;lt;String, Integer&amp;gt; stringIntConsumed = Consumed.with(stringSerde, intSerde);&lt;br/&gt;
+        final KStream&amp;lt;String, Integer&amp;gt; source = builder.stream(&quot;data&quot;, stringIntConsumed);&lt;br/&gt;
+        source.to(&quot;echo&quot;, Produced.with(stringSerde, intSerde));&lt;br/&gt;
+        final KStream&amp;lt;String, Integer&amp;gt; data = source.filter(new Predicate&amp;lt;String, Integer&amp;gt;() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public boolean test(String key, Integer value) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            public boolean test(final String key, final Integer value) {
                 return value == null || value != END;
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
         data.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // min&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KGroupedStream&amp;lt;String, Integer&amp;gt;&lt;/li&gt;
	&lt;li&gt;groupedData =&lt;br/&gt;
+        final KGroupedStream&amp;lt;String, Integer&amp;gt; groupedData =&lt;br/&gt;
             data.groupByKey(Serialized.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(1)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
                     public Integer apply() {&lt;br/&gt;
                         return Integer.MAX_VALUE;&lt;br/&gt;
@@ -132,21 +143,24 @@ public Integer apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Integer&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Integer apply(String aggKey, Integer value, Integer aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value &amp;lt; aggregate) ? value : aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(1)),&lt;/li&gt;
	&lt;li&gt;intSerde, &quot;uwin-min&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Integer&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;min&quot;);&lt;br/&gt;
+                Materialized.&amp;lt;String, Integer, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-min&quot;).withValueSerde(intSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Integer&amp;gt;())&lt;br/&gt;
+            .to(&quot;min&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(&quot;min&quot;, stringIntConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Integer&amp;gt; minTable = builder.table(&lt;br/&gt;
+            &quot;min&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, intSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;minStoreName&quot;));&lt;br/&gt;
         minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;min&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // max&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
                     public Integer apply() {&lt;br/&gt;
                         return Integer.MIN_VALUE;&lt;br/&gt;
@@ -154,21 +168,24 @@ public Integer apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Integer&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Integer apply(String aggKey, Integer value, Integer aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value &amp;gt; aggregate) ? value : aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(2)),&lt;/li&gt;
	&lt;li&gt;intSerde, &quot;uwin-max&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Integer&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;max&quot;);&lt;br/&gt;
+                Materialized.&amp;lt;String, Integer, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-max&quot;).withValueSerde(intSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Integer&amp;gt;())&lt;br/&gt;
+            .to(&quot;max&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(&quot;max&quot;, stringIntConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Integer&amp;gt; maxTable = builder.table(&lt;br/&gt;
+            &quot;max&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, intSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;maxStoreName&quot;));&lt;br/&gt;
         maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;max&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // sum&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;groupedData.aggregate(&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .aggregate(&lt;br/&gt;
                 new Initializer&amp;lt;Long&amp;gt;() {&lt;br/&gt;
                     public Long apply() {&lt;br/&gt;
                         return 0L;&lt;br/&gt;
@@ -176,70 +193,74 @@ public Long apply() {&lt;br/&gt;
                 },&lt;br/&gt;
                 new Aggregator&amp;lt;String, Integer, Long&amp;gt;() {&lt;br/&gt;
                     @Override&lt;/li&gt;
	&lt;li&gt;public Long apply(String aggKey, Integer value, Long aggregate) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    public Long apply(final String aggKey, final Integer value, final Long aggregate) {
                         return (long) value + aggregate;
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;TimeWindows.of(TimeUnit.DAYS.toMillis(2)),&lt;/li&gt;
	&lt;li&gt;longSerde, &quot;win-sum&quot;&lt;/li&gt;
	&lt;li&gt;).toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Long&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;sum&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Consumed&amp;lt;String, Long&amp;gt; stringLongConsumed = Consumed.with(stringSerde, longSerde);&lt;/li&gt;
	&lt;li&gt;KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(&quot;sum&quot;, stringLongConsumed);&lt;br/&gt;
+                Materialized.&amp;lt;String, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;win-sum&quot;).withValueSerde(longSerde))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Long&amp;gt;())&lt;br/&gt;
+            .to(&quot;sum&quot;, Produced.with(stringSerde, longSerde));&lt;br/&gt;
+&lt;br/&gt;
+        final Consumed&amp;lt;String, Long&amp;gt; stringLongConsumed = Consumed.with(stringSerde, longSerde);&lt;br/&gt;
+        final KTable&amp;lt;String, Long&amp;gt; sumTable = builder.table(&quot;sum&quot;, stringLongConsumed);&lt;br/&gt;
         sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;sum&quot;));&lt;br/&gt;
+&lt;br/&gt;
         // cnt&lt;/li&gt;
	&lt;li&gt;groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), &quot;uwin-cnt&quot;)&lt;/li&gt;
	&lt;li&gt;.toStream().map(&lt;/li&gt;
	&lt;li&gt;new Unwindow&amp;lt;String, Long&amp;gt;()&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;cnt&quot;);&lt;br/&gt;
+        groupedData&lt;br/&gt;
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))&lt;br/&gt;
+            .count(Materialized.&amp;lt;String, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;uwin-cnt&quot;))&lt;br/&gt;
+            .toStream(new Unwindow&amp;lt;String, Long&amp;gt;())&lt;br/&gt;
+            .to(&quot;cnt&quot;, Produced.with(stringSerde, longSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(&quot;cnt&quot;, stringLongConsumed);&lt;br/&gt;
+        final KTable&amp;lt;String, Long&amp;gt; cntTable = builder.table(&lt;br/&gt;
+            &quot;cnt&quot;,&lt;br/&gt;
+            Consumed.with(stringSerde, longSerde),&lt;br/&gt;
+            Materialized.&amp;lt;String, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;cntStoreName&quot;));&lt;br/&gt;
         cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(&quot;cnt&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // dif&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;maxTable.join(minTable,&lt;br/&gt;
+        maxTable&lt;br/&gt;
+            .join(&lt;br/&gt;
+                minTable,&lt;br/&gt;
                 new ValueJoiner&amp;lt;Integer, Integer, Integer&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;public Integer apply(Integer value1, Integer value2) {&lt;br/&gt;
+                    public Integer apply(final Integer value1, final Integer value2) 
{
                         return value1 - value2;
                     }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, intSerde, &quot;dif&quot;);&lt;br/&gt;
+                })&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;dif&quot;, Produced.with(stringSerde, intSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // avg&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sumTable.join(&lt;br/&gt;
+        sumTable&lt;br/&gt;
+            .join(&lt;br/&gt;
                 cntTable,&lt;br/&gt;
                 new ValueJoiner&amp;lt;Long, Long, Double&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;public Double apply(Long value1, Long value2) {&lt;br/&gt;
+                    public Double apply(final Long value1, final Long value2) 
{
                         return (double) value1 / (double) value2;
                     }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, doubleSerde, &quot;avg&quot;);&lt;br/&gt;
+                })&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;avg&quot;, Produced.with(stringSerde, doubleSerde));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // test repartition&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Agg agg = new Agg();&lt;/li&gt;
	&lt;li&gt;cntTable.groupBy(agg.selector(),&lt;/li&gt;
	&lt;li&gt;Serialized.with(stringSerde, longSerde)&lt;/li&gt;
	&lt;li&gt;).aggregate(agg.init(),&lt;/li&gt;
	&lt;li&gt;agg.adder(),&lt;/li&gt;
	&lt;li&gt;agg.remover(),&lt;/li&gt;
	&lt;li&gt;Materialized.&amp;lt;String, Long&amp;gt;as(Stores.inMemoryKeyValueStore(&quot;cntByCnt&quot;))&lt;/li&gt;
	&lt;li&gt;.withKeySerde(Serdes.String())&lt;/li&gt;
	&lt;li&gt;.withValueSerde(Serdes.Long())&lt;/li&gt;
	&lt;li&gt;).to(stringSerde, longSerde, &quot;tagg&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final KafkaStreams streamsClient = new KafkaStreams(builder.build(), props);&lt;br/&gt;
+        final Agg agg = new Agg();&lt;br/&gt;
+        cntTable.groupBy(agg.selector(), Serialized.with(stringSerde, longSerde))&lt;br/&gt;
+            .aggregate(agg.init(), agg.adder(), agg.remover(),&lt;br/&gt;
+                Materialized.&amp;lt;String, Long&amp;gt;as(Stores.inMemoryKeyValueStore(&quot;cntByCnt&quot;))&lt;br/&gt;
+                    .withKeySerde(Serdes.String())&lt;br/&gt;
+                    .withValueSerde(Serdes.Long()))&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(&quot;tagg&quot;, Produced.with(stringSerde, longSerde));&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), getStreamsConfig(props, kafka));&lt;br/&gt;
         streamsClient.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {&lt;br/&gt;
             @Override&lt;/li&gt;
	&lt;li&gt;public void uncaughtException(Thread t, Throwable e) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(&quot;FATAL: An unexpected exception is encountered on thread &quot; + t + &quot;: &quot; + e);
-                
                 streamsClient.close(30, TimeUnit.SECONDS);
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return streamsClient;&lt;br/&gt;
     }&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
index a3f520a82db..fc7a26ee7a2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java&lt;br/&gt;
@@ -136,53 +136,65 @@ public void run() &lt;/p&gt;
{
         System.out.println(&quot;shutdown&quot;);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(String kafka, final int numKeys, final int maxRecordsPerKey) {&lt;br/&gt;
+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey) 
{
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public static Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; generate(final String kafka,&lt;br/&gt;
+                                                     final int numKeys,&lt;br/&gt;
+                                                     final int maxRecordsPerKey,&lt;br/&gt;
+                                                     final boolean autoTerminate) {&lt;br/&gt;
         final Properties producerProps = new Properties();&lt;br/&gt;
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;SmokeTest&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;br/&gt;
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// the next 4 config values make sure that all records are produced with no loss and&lt;/li&gt;
	&lt;li&gt;// no duplicates&lt;br/&gt;
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates&lt;br/&gt;
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);&lt;br/&gt;
         producerProps.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);&lt;br/&gt;
         producerProps.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 80000);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;br/&gt;
+        final KafkaProducer&amp;lt;byte[], byte[]&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         int numRecordsProduced = 0;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final ValueList[] data = new ValueList&lt;span class=&quot;error&quot;&gt;&amp;#91;numKeys&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         for (int i = 0; i &amp;lt; numKeys; i++) 
{
             data[i] = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data[i].key, new HashSet&amp;lt;Integer&amp;gt;());
         }&lt;/li&gt;
	&lt;li&gt;Random rand = new Random();&lt;br/&gt;
+        final Random rand = new Random();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int remaining = data.length;&lt;br/&gt;
+        int remaining = 1; // dummy value must be positive if &amp;lt;autoTerminate&amp;gt; is false&lt;br/&gt;
+        if (autoTerminate) 
{
+            remaining = data.length;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; needRetry = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;         while (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int index = rand.nextInt(remaining);&lt;/li&gt;
	&lt;li&gt;String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);&lt;br/&gt;
+            final String key = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.key;&lt;br/&gt;
             int value = data&lt;span class=&quot;error&quot;&gt;&amp;#91;index&amp;#93;&lt;/span&gt;.next();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (value &amp;lt; 0) {&lt;br/&gt;
+            if (autoTerminate &amp;amp;&amp;amp; value &amp;lt; 0) 
{
                 remaining--;
                 data[index] = data[remaining];
             }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;/li&gt;
	&lt;li&gt;new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;br/&gt;
+                final ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record =&lt;br/&gt;
+                    new ProducerRecord&amp;lt;&amp;gt;(&quot;data&quot;, stringSerde.serializer().serialize(&quot;&quot;, key), intSerde.serializer().serialize(&quot;&quot;, value));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 producer.send(record, new TestCallback(record, needRetry));&lt;/p&gt;

&lt;p&gt;                 numRecordsProduced++;&lt;br/&gt;
                 allData.get(key).add(value);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numRecordsProduced % 100 == 0)&lt;br/&gt;
+                if (numRecordsProduced % 100 == 0) 
{
                     System.out.println(numRecordsProduced + &quot; records produced&quot;);
+                }
&lt;p&gt;                 Utils.sleep(2);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
index dc4c91b4097..87ca82918a9 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java&lt;br/&gt;
@@ -44,20 +44,15 @@&lt;br/&gt;
             public Processor&amp;lt;Object, Object&amp;gt; get() {&lt;br/&gt;
                 return new AbstractProcessor&amp;lt;Object, Object&amp;gt;() {&lt;br/&gt;
                     private int numRecordsProcessed = 0;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;private ProcessorContext context;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;br/&gt;
                     public void init(final ProcessorContext context) &lt;/p&gt;
{
                         System.out.println(&quot;initializing processor: topic=&quot; + topic + &quot; taskId=&quot; + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }

&lt;p&gt;                     @Override&lt;br/&gt;
                     public void process(final Object key, final Object value) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (printOffset) 
{
-                            System.out.println(&quot;&amp;gt;&amp;gt;&amp;gt; &quot; + context.offset());
-                        }
&lt;p&gt;                         numRecordsProcessed++;&lt;br/&gt;
                         if (numRecordsProcessed % 100 == 0) {&lt;br/&gt;
                             System.out.println(System.currentTimeMillis());&lt;br/&gt;
@@ -66,19 +61,19 @@ public void process(final Object key, final Object value) {&lt;br/&gt;
                     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void punctuate(final long timestamp) { }&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void close() { }&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
                 };&lt;br/&gt;
             }&lt;br/&gt;
         };&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final class Unwindow&amp;lt;K, V&amp;gt; implements KeyValueMapper&amp;lt;Windowed&amp;lt;K&amp;gt;, V, KeyValue&amp;lt;K, V&amp;gt;&amp;gt; {&lt;br/&gt;
+    public static final class Unwindow&amp;lt;K, V&amp;gt; implements KeyValueMapper&amp;lt;Windowed&amp;lt;K&amp;gt;, V, K&amp;gt; {&lt;br/&gt;
         @Override&lt;/li&gt;
	&lt;li&gt;public KeyValue&amp;lt;K, V&amp;gt; apply(final Windowed&amp;lt;K&amp;gt; winKey, final V value) {&lt;/li&gt;
	&lt;li&gt;return new KeyValue&amp;lt;&amp;gt;(winKey.key(), value);&lt;br/&gt;
+        public K apply(final Windowed&amp;lt;K&amp;gt; winKey, final V value) 
{
+            return winKey.key();
         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
index 27aba29bba6..41c3f6c58f0 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
@@ -18,7 +18,6 @@&lt;/p&gt;

&lt;p&gt; import org.apache.kafka.common.utils.Utils;&lt;/p&gt;

&lt;p&gt;-import java.io.IOException;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
@@ -26,22 +25,24 @@&lt;br/&gt;
 public class StreamsSmokeTest {&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  args ::= kafka propFileName command&lt;br/&gt;
+     *  args ::= kafka propFileName command disableAutoTerminate&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;command := &quot;run&quot; | &quot;process&quot;&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;@param args&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static void main(final String[] args) throws InterruptedException, IOException {&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
         final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         final String propFileName = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
         final String command = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+        final boolean disableAutoTerminate = args.length &amp;gt; 3;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Properties streamsProperties = Utils.loadProps(propFileName);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.out.println(&quot;StreamsTest instance started&quot;);&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsSmokeTest)&quot;);&lt;br/&gt;
         System.out.println(&quot;command=&quot; + command);&lt;br/&gt;
         System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
         System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+        System.out.println(&quot;disableAutoTerminate=&quot; + disableAutoTerminate);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         switch (command) {&lt;br/&gt;
             case &quot;standalone&quot;:&lt;br/&gt;
@@ -51,8 +52,12 @@ public static void main(final String[] args) throws InterruptedException, IOExce&lt;br/&gt;
                 // this starts the driver (data generation and result verification)&lt;br/&gt;
                 final int numKeys = 10;&lt;br/&gt;
                 final int maxRecordsPerKey = 500;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);&lt;/li&gt;
	&lt;li&gt;SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);&lt;br/&gt;
+                if (disableAutoTerminate) 
{
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    Map&amp;lt;String, Set&amp;lt;Integer&amp;gt;&amp;gt; allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
&lt;p&gt;                 break;&lt;br/&gt;
             case &quot;process&quot;:&lt;br/&gt;
                 // this starts a KafkaStreams client&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..69eea0b37c0&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,69 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest trunk)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(&quot;data&quot;));&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        config.putAll(streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..32f96a01d99&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,107 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        config.putAll(streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                System.out.println(&quot;closing Kafka Streams instance&quot;);+                System.out.flush();+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..dcb05ca5d72&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,110 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be executed, as long as Kafka 0.10.1.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
+        if (args.length &amp;lt; 3) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] + &quot; &quot; : &quot;&quot;)
+                + (args.length &amp;gt; 1 ? args[1] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String zookeeper = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 2 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.1)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;zookeeper=&quot; + zookeeper);&lt;br/&gt;
+        System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        config.putAll(streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                System.out.println(&quot;closing Kafka Streams instance&quot;);
+                System.out.flush();
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..fb4a4091610&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be executed, as long as Kafka 0.10.2.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.10.2)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        config.putAll(streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..b1aad5dea9d&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be executed, as long as Kafka 0.11.0.3 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
+        if (args.length &amp;lt; 2) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v0.11.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        config.putAll(streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder, config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                streams.close();+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);+                System.out.flush();+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }&lt;br/&gt;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..dc72f2dcc9d&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be executed, as long as Kafka 1.0.2 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
+        if (args.length &amp;lt; 2) {
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }&lt;br/&gt;
+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v1.0)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        config.putAll(streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }&lt;br/&gt;
+        });&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) {
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) {&lt;br/&gt;
+                        numRecordsProcessed++;&lt;br/&gt;
+                        if (numRecordsProcessed % 100 == 0) {+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/tests/docker/Dockerfile b/tests/docker/Dockerfile&lt;br/&gt;
index 57ca2423e3f..25da8db59ea 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/docker/Dockerfile&lt;br/&gt;
+++ b/tests/docker/Dockerfile&lt;br/&gt;
@@ -39,24 +39,36 @@ COPY ./ssh-config /root/.ssh/config&lt;br/&gt;
 RUN ssh-keygen -q -t rsa -N &apos;&apos; -f /root/.ssh/id_rsa &amp;amp;&amp;amp; cp -f /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;Install binary test dependencies.&lt;br/&gt;
+# we use the same versions as in vagrant/base.sh&lt;br/&gt;
 ARG KAFKA_MIRROR=&quot;https://s3-us-west-2.amazonaws.com/kafka-packages&quot;&lt;br/&gt;
-RUN mkdir -p &quot;/opt/kafka-0.8.2.2&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.8.2.2 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.10-0.8.2.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.8.2.2&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.8.2.2&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.8.2.2 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.8.2.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.8.2.2&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.9.0.1&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.9.0.1 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.9.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.9.0.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.10.0.0&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.10.0.0 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.0.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.0.0&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.10.0.1&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.10.0.1 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.0.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.10.1.0&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.10.1.0 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.1.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.1.0&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.10.1.1&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.10.1.1 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.1.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.1.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.10.2.0&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.10.2.0 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.2.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.2.0&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.10.2.1&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.10.2.1 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.10.2.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.10.2.1&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.11.0.0&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.11.0.0 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.11.0.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.11.0.0&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-0.11.0.1&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.11.0.1 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.11.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.11.0.1&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-0.11.0.2&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-0.11.0.2 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-0.11.0.2.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-0.11.0.2&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-1.0.0&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-1.0.0 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-1.0.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-1.0.0&quot;&lt;br/&gt;
 RUN mkdir -p &quot;/opt/kafka-1.0.1&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-1.0.1 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-1.0.1.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-1.0.1&quot;&lt;br/&gt;
+RUN mkdir -p &quot;/opt/kafka-1.1.0&quot; &amp;amp;&amp;amp; chmod a+rw /opt/kafka-1.1.0 &amp;amp;&amp;amp; curl -s &quot;$KAFKA_MIRROR/kafka_2.11-1.1.0.tgz&quot; | tar xz --strip-components=1 -C &quot;/opt/kafka-1.1.0&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;Streams test dependencies&lt;br/&gt;
-RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.1.1-test.jar&quot; -o /opt/kafka-0.10.1.1/libs/kafka-streams-0.10.1.1-test.jar &amp;amp;&amp;amp; \&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.2.1-test.jar&quot; -o /opt/kafka-0.10.2.1/libs/kafka-streams-0.10.2.1-test.jar &amp;amp;&amp;amp; \&lt;/li&gt;
	&lt;li&gt;curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.11.0.0-test.jar&quot; -o /opt/kafka-0.11.0.0/libs/kafka-streams-0.11.0.0-test.jar &amp;amp;&amp;amp; \&lt;/li&gt;
	&lt;li&gt;curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.11.0.2-test.jar&quot; -o /opt/kafka-0.11.0.2/libs/kafka-streams-0.11.0.2-test.jar &amp;amp;&amp;amp; \&lt;/li&gt;
	&lt;li&gt;curl -s &quot;$KAFKA_MIRROR/kafka-streams-1.0.0-test.jar&quot; -o /opt/kafka-1.0.0/libs/kafka-streams-1.0.0-test.jar &amp;amp;&amp;amp; \&lt;/li&gt;
	&lt;li&gt;curl -s &quot;$KAFKA_MIRROR/kafka-streams-1.0.1-test.jar&quot; -o /opt/kafka-1.0.1/libs/kafka-streams-1.0.1-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.0.0-test.jar&quot; -o /opt/kafka-0.10.0.0/libs/kafka-streams-0.10.0.0-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.0.1-test.jar&quot; -o /opt/kafka-0.10.0.1/libs/kafka-streams-0.10.0.1-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.1.0-test.jar&quot; -o /opt/kafka-0.10.1.0/libs/kafka-streams-0.10.1.0-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.1.1-test.jar&quot; -o /opt/kafka-0.10.1.1/libs/kafka-streams-0.10.1.1-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.2.0-test.jar&quot; -o /opt/kafka-0.10.2.0/libs/kafka-streams-0.10.2.0-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.10.2.1-test.jar&quot; -o /opt/kafka-0.10.2.1/libs/kafka-streams-0.10.2.1-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.11.0.0-test.jar&quot; -o /opt/kafka-0.11.0.0/libs/kafka-streams-0.11.0.0-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.11.0.1-test.jar&quot; -o /opt/kafka-0.11.0.1/libs/kafka-streams-0.11.0.1-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-0.11.0.2-test.jar&quot; -o /opt/kafka-0.11.0.2/libs/kafka-streams-0.11.0.2-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-1.0.0-test.jar&quot; -o /opt/kafka-1.0.0/libs/kafka-streams-1.0.0-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-1.0.1-test.jar&quot; -o /opt/kafka-1.0.1/libs/kafka-streams-1.0.1-test.jar&lt;br/&gt;
+RUN curl -s &quot;$KAFKA_MIRROR/kafka-streams-1.1.0-test.jar&quot; -o /opt/kafka-1.1.0/libs/kafka-streams-1.1.0-test.jar&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;The version of Kibosh to use for testing.&lt;/li&gt;
	&lt;li&gt;If you update this, also update vagrant/base.sy&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index d9b475e191b..a5be816c737 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -21,6 +21,7 @@&lt;br/&gt;
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin&lt;br/&gt;
 from kafkatest.services.monitor.jmx import JmxMixin&lt;br/&gt;
 from kafkatest.services.kafka import KafkaConfig&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; STATE_DIR = &quot;state.dir&quot;&lt;/p&gt;

&lt;p&gt;@@ -39,6 +40,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):&lt;br/&gt;
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, &quot;tools-log4j.properties&quot;)&lt;br/&gt;
     PID_FILE = os.path.join(PERSISTENT_ROOT, &quot;streams.pid&quot;)&lt;/p&gt;

&lt;p&gt;+    CLEAN_NODE_ENABLED = True&lt;br/&gt;
+&lt;br/&gt;
     logs = {&lt;br/&gt;
         &quot;streams_log&quot;: {&lt;br/&gt;
             &quot;path&quot;: LOG_FILE,&lt;br/&gt;
@@ -49,6 +52,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):&lt;br/&gt;
         &quot;streams_stderr&quot;: &lt;/p&gt;
{
             &quot;path&quot;: STDERR_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.0-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.0-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-4&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-4&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-5&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-5&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1-6&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1-6&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
         &quot;jmx_log&quot;: &lt;/p&gt;
{
             &quot;path&quot;: JMX_LOG_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
@@ -120,7 +231,8 @@ def wait_node(self, node, timeout_sec=None):&lt;/p&gt;

&lt;p&gt;     def clean_node(self, node):&lt;br/&gt;
         node.account.kill_process(&quot;streams&quot;, clean_shutdown=False, allow_fail=True)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if self.CLEAN_NODE_ENABLED:&lt;br/&gt;
+            node.account.ssh(&quot;rm -rf &quot; + self.PERSISTENT_ROOT, allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def start_cmd(self, node):&lt;br/&gt;
         args = self.args.copy()&lt;br/&gt;
@@ -141,13 +253,13 @@ def start_cmd(self, node):&lt;/p&gt;

&lt;p&gt;         return cmd&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def prop_file(self, node):&lt;br/&gt;
+    def prop_file(self):&lt;br/&gt;
         cfg = KafkaConfig(**
{STATE_DIR: self.PERSISTENT_ROOT}
&lt;p&gt;)&lt;br/&gt;
         return cfg.render()&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def start_node(self, node):&lt;br/&gt;
         node.account.mkdirs(self.PERSISTENT_ROOT)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;prop_file = self.prop_file(node)&lt;br/&gt;
+        prop_file = self.prop_file()&lt;br/&gt;
         node.account.create_file(self.CONFIG_FILE, prop_file)&lt;br/&gt;
         node.account.create_file(self.LOG4J_CONFIG_FILE, self.render(&apos;tools_log4j.properties&apos;, log_file=self.LOG_FILE))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -189,7 +301,28 @@ def clean_node(self, node):&lt;br/&gt;
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
         super(StreamsSmokeTestDriverService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context, kafka, &quot;run&quot;)&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;&quot;&lt;/p&gt;

&lt;p&gt;+    def disable_auto_terminate(self):&lt;br/&gt;
+        self.DISABLE_AUTO_TERMINATE = &quot;disableAutoTerminate&quot;&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;config_file&amp;#39;&amp;#93;&lt;/span&gt; = self.CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;disable_auto_terminate&amp;#39;&amp;#93;&lt;/span&gt; = self.DISABLE_AUTO_TERMINATE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s &quot; \&lt;br/&gt;
+              &quot; %(kafka)s %(config_file)s %(user_test_args)s %(disable_auto_terminate)s&quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;/p&gt;

&lt;p&gt; class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):&lt;br/&gt;
     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
@@ -273,3 +406,48 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka, configs):&lt;br/&gt;
                                                                         kafka,&lt;br/&gt;
                                                                         &quot;org.apache.kafka.streams.tests.StreamsRepeatingIntegerKeyProducer&quot;,&lt;br/&gt;
                                                                         configs)&lt;br/&gt;
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):&lt;br/&gt;
+    def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
+        super(StreamsUpgradeTestJobRunnerService, self)._&lt;em&gt;init&lt;/em&gt;_(test_context,&lt;br/&gt;
+                                                                 kafka,&lt;br/&gt;
+                                                                 &quot;org.apache.kafka.streams.tests.StreamsUpgradeTest&quot;,&lt;br/&gt;
+                                                                 &quot;&quot;)&lt;br/&gt;
+        self.UPGRADE_FROM = None&lt;br/&gt;
+&lt;br/&gt;
+    def set_version(self, kafka_streams_version):&lt;br/&gt;
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version&lt;br/&gt;
+&lt;br/&gt;
+    def set_upgrade_from(self, upgrade_from):&lt;br/&gt;
+        self.UPGRADE_FROM = upgrade_from&lt;br/&gt;
+&lt;br/&gt;
+    def prop_file(self):&lt;br/&gt;
+        properties = &lt;/p&gt;
{STATE_DIR: self.PERSISTENT_ROOT}
&lt;p&gt;+        if self.UPGRADE_FROM is not None:&lt;br/&gt;
+            properties&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;upgrade.from&amp;#39;&amp;#93;&lt;/span&gt; = self.UPGRADE_FROM&lt;br/&gt;
+&lt;br/&gt;
+        cfg = KafkaConfig(**properties)&lt;br/&gt;
+        return cfg.render()&lt;br/&gt;
+&lt;br/&gt;
+    def start_cmd(self, node):&lt;br/&gt;
+        args = self.args.copy()&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
+        else:&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = &quot;&quot;&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;config_file&amp;#39;&amp;#93;&lt;/span&gt; = self.CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stdout&amp;#39;&amp;#93;&lt;/span&gt; = self.STDOUT_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;stderr&amp;#39;&amp;#93;&lt;/span&gt; = self.STDERR_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;pidfile&amp;#39;&amp;#93;&lt;/span&gt; = self.PID_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;log4j&amp;#39;&amp;#93;&lt;/span&gt; = self.LOG4J_CONFIG_FILE&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;version&amp;#39;&amp;#93;&lt;/span&gt; = self.KAFKA_STREAMS_VERSION&lt;br/&gt;
+        args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka_run_class&amp;#39;&amp;#93;&lt;/span&gt; = self.path.script(&quot;kafka-run-class.sh&quot;, node)&lt;br/&gt;
+&lt;br/&gt;
+        cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
+              &quot;INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s &quot; \&lt;br/&gt;
+              &quot; %(kafka_run_class)s %(streams_class_name)s  %(kafka)s %(zk)s %(config_file)s &quot; \&lt;br/&gt;
+              &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;br/&gt;
+&lt;br/&gt;
+        self.logger.info(&quot;Executing: &quot; + cmd)&lt;br/&gt;
+&lt;br/&gt;
+        return cmd&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
index 3b38ff6f0f6..fa79d571f36 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -13,25 +13,50 @@&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;-import time&lt;br/&gt;
-from ducktape.mark import ignore&lt;br/&gt;
-from ducktape.mark import matrix&lt;br/&gt;
+from ducktape.mark import ignore, matrix, parametrize&lt;br/&gt;
 from ducktape.mark.resource import cluster&lt;br/&gt;
 from ducktape.tests.test import Test&lt;br/&gt;
 from kafkatest.services.kafka import KafkaService&lt;br/&gt;
-from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService&lt;br/&gt;
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService, StreamsUpgradeTestJobRunnerService&lt;br/&gt;
 from kafkatest.services.zookeeper import ZookeeperService&lt;br/&gt;
-from kafkatest.version import LATEST_0_10_2, LATEST_0_11, LATEST_1_0, DEV_BRANCH, KafkaVersion&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, LATEST_1_0, LATEST_1_1, DEV_BRANCH, DEV_VERSION, KafkaVersion&lt;br/&gt;
+import random&lt;br/&gt;
+import time&lt;/p&gt;

&lt;p&gt;-upgrade_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_2), str(LATEST_0_11), str(LATEST_1_0), str(DEV_BRANCH)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+broker_upgrade_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1), str(DEV_BRANCH)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+simple_upgrade_versions_metadata_version_2 = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(DEV_VERSION)&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; class StreamsUpgradeTest(Test):&lt;br/&gt;
     &quot;&quot;&quot;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Tests rolling upgrades and downgrades of the Kafka Streams library.&lt;br/&gt;
+    Test upgrading Kafka Streams (all version combination)&lt;br/&gt;
+    If metadata was changes, upgrade is more difficult&lt;br/&gt;
+    Metadata version was bumped in 0.10.1.0&lt;br/&gt;
     &quot;&quot;&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
         super(StreamsUpgradeTest, self)._&lt;em&gt;init&lt;/em&gt;_(test_context)&lt;br/&gt;
+        self.topics = {&lt;br/&gt;
+            &apos;echo&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+            &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+    def perform_broker_upgrade(self, to_version):&lt;br/&gt;
+        self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;br/&gt;
+        for node in self.kafka.nodes:&lt;br/&gt;
+            self.kafka.stop_node(node)&lt;br/&gt;
+            node.version = KafkaVersion(to_version)&lt;br/&gt;
+            self.kafka.start_node(node)&lt;br/&gt;
+&lt;br/&gt;
+    @cluster(num_nodes=6)&lt;br/&gt;
+    @matrix(from_version=broker_upgrade_versions, to_version=broker_upgrade_versions)&lt;br/&gt;
+    def test_upgrade_downgrade_brokers(self, from_version, to_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Start a smoke test client then perform rolling upgrades on the broker.&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        if from_version == to_version:&lt;br/&gt;
+            return&lt;br/&gt;
+&lt;br/&gt;
         self.replication = 3&lt;br/&gt;
         self.partitions = 1&lt;br/&gt;
         self.isr = 2&lt;br/&gt;
@@ -58,112 +83,272 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
                        &apos;configs&apos;: &lt;/p&gt;
{&quot;min.insync.replicas&quot;: self.isr}
&lt;p&gt; }&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;+        # Setup phase&lt;br/&gt;
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
+        self.zk.start()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def perform_streams_upgrade(self, to_version):&lt;/li&gt;
	&lt;li&gt;self.logger.info(&quot;First pass bounce - rolling streams upgrade&quot;)&lt;br/&gt;
+        # number of nodes needs to be &amp;gt;= 3 for the smoke test&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=3,&lt;br/&gt;
+                                  zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)&lt;br/&gt;
+        self.kafka.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# get the node running the streams app&lt;/li&gt;
	&lt;li&gt;node = self.processor1.node&lt;/li&gt;
	&lt;li&gt;self.processor1.stop()&lt;br/&gt;
+        # allow some time for topics to be created&lt;br/&gt;
+        time.sleep(10)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# change it&apos;s version. This will automatically make it pick up a different&lt;/li&gt;
	&lt;li&gt;# JAR when it starts again&lt;/li&gt;
	&lt;li&gt;node.version = KafkaVersion(to_version)&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        &lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
         self.processor1.start()&lt;br/&gt;
+        time.sleep(15)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def perform_broker_upgrade(self, to_version):&lt;/li&gt;
	&lt;li&gt;self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;/li&gt;
	&lt;li&gt;for node in self.kafka.nodes:&lt;/li&gt;
	&lt;li&gt;self.kafka.stop_node(node)&lt;/li&gt;
	&lt;li&gt;node.version = KafkaVersion(to_version)&lt;/li&gt;
	&lt;li&gt;self.kafka.start_node(node)&lt;br/&gt;
+        self.perform_broker_upgrade(to_version)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@ignore&lt;/li&gt;
	&lt;li&gt;@cluster(num_nodes=6)&lt;/li&gt;
	&lt;li&gt;@matrix(from_version=upgrade_versions, to_version=upgrade_versions)&lt;/li&gt;
	&lt;li&gt;def test_upgrade_downgrade_streams(self, from_version, to_version):&lt;/li&gt;
	&lt;li&gt;&quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;Start a smoke test client, then abort (kill -9) and restart it a few times.&lt;/li&gt;
	&lt;li&gt;Ensure that all records are delivered.&lt;br/&gt;
+        time.sleep(15)&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+        self.processor1.stop()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Note, that just like tests/core/upgrade_test.py, a prerequisite for this test to succeed&lt;/li&gt;
	&lt;li&gt;if the inclusion of all parametrized versions of kafka in kafka/vagrant/base.sh&lt;/li&gt;
	&lt;li&gt;(search for get_kafka()). For streams in particular, that means that someone has manually&lt;/li&gt;
	&lt;li&gt;copies the kafka-stream-$version-test.jar in the right S3 bucket as shown in base.sh.&lt;br/&gt;
+        node = self.driver.node&lt;br/&gt;
+        node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+        self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+    @matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)&lt;br/&gt;
+    def test_simple_upgrade_downgrade(self, from_version, to_version):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with &amp;lt;old_version&amp;gt;, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;if from_version != to_version:&lt;/li&gt;
	&lt;li&gt;# Setup phase&lt;/li&gt;
	&lt;li&gt;self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;/li&gt;
	&lt;li&gt;self.zk.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# number of nodes needs to be &amp;gt;= 3 for the smoke test&lt;/li&gt;
	&lt;li&gt;self.kafka = KafkaService(self.test_context, num_nodes=3,&lt;/li&gt;
	&lt;li&gt;zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)&lt;/li&gt;
	&lt;li&gt;self.kafka.start()&lt;br/&gt;
+        if from_version == to_version:&lt;br/&gt;
+            return&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# allow some time for topics to be created&lt;/li&gt;
	&lt;li&gt;time.sleep(10)&lt;br/&gt;
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
+        self.zk.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;/li&gt;
	&lt;li&gt;self.driver.node.version = KafkaVersion(from_version)&lt;/li&gt;
	&lt;li&gt;self.driver.start()&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)&lt;br/&gt;
+        self.kafka.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)&lt;/li&gt;
	&lt;li&gt;self.processor1.node.version = KafkaVersion(from_version)&lt;/li&gt;
	&lt;li&gt;self.processor1.start()&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;time.sleep(15)&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(from_version)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.perform_streams_upgrade(to_version)&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;time.sleep(15)&lt;/li&gt;
	&lt;li&gt;self.driver.wait()&lt;/li&gt;
	&lt;li&gt;self.driver.stop()&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.processor1.stop()&lt;br/&gt;
+        # upgrade one-by-one via rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, None, to_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.driver.node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;/li&gt;
	&lt;li&gt;self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@ignore&lt;/li&gt;
	&lt;li&gt;@cluster(num_nodes=6)&lt;/li&gt;
	&lt;li&gt;@matrix(from_version=upgrade_versions, to_version=upgrade_versions)&lt;/li&gt;
	&lt;li&gt;def test_upgrade_brokers(self, from_version, to_version):&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_1_0)) we cannot run this test until Kafka 1.0.2 is released&lt;br/&gt;
+    #@parametrize(new_version=str(LATEST_1_1)) we cannot run this test until Kafka 1.1.1 is released&lt;br/&gt;
+    @parametrize(new_version=str(DEV_VERSION))&lt;br/&gt;
+    def test_metadata_upgrade(self, new_version):&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;Start a smoke test client then perform rolling upgrades on the broker.&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;if from_version != to_version:&lt;/li&gt;
	&lt;li&gt;# Setup phase&lt;/li&gt;
	&lt;li&gt;self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;/li&gt;
	&lt;li&gt;self.zk.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# number of nodes needs to be &amp;gt;= 3 for the smoke test&lt;/li&gt;
	&lt;li&gt;self.kafka = KafkaService(self.test_context, num_nodes=3,&lt;/li&gt;
	&lt;li&gt;zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)&lt;/li&gt;
	&lt;li&gt;self.kafka.start()&lt;br/&gt;
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
+        self.zk.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)&lt;br/&gt;
+        self.kafka.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(str(LATEST_0_10_0))&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+&lt;br/&gt;
+        # first rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            self.do_rolling_bounce(p, &quot;0.10.0&quot;, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # second rolling bounce&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.do_rolling_bounce(p, None, new_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    def start_all_nodes_with(self, version):&lt;br/&gt;
+        # start first with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor1, version)&lt;br/&gt;
+        node1 = self.processor1.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:&lt;br/&gt;
+                self.processor1.start()&lt;br/&gt;
+                log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                       timeout_sec=60,&lt;br/&gt;
+                                       err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node1.account))&lt;br/&gt;
+                monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start second with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor2, version)&lt;br/&gt;
+        node2 = self.processor2.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:&lt;br/&gt;
+                    self.processor2.start()&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node2.account))&lt;br/&gt;
+                    first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                             timeout_sec=60,&lt;br/&gt;
+                                             err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                    second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                              timeout_sec=60,&lt;br/&gt;
+                                              err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+&lt;br/&gt;
+        # start third with &amp;lt;version&amp;gt;&lt;br/&gt;
+        self.prepare_for(self.processor3, version)&lt;br/&gt;
+        node3 = self.processor3.node&lt;br/&gt;
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:&lt;br/&gt;
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:&lt;br/&gt;
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:&lt;br/&gt;
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:&lt;br/&gt;
+                        self.processor3.start()&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + version + &quot; &quot; + str(node3.account))&lt;br/&gt;
+                        first_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                 timeout_sec=60,&lt;br/&gt;
+                                                 err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node1.account))&lt;br/&gt;
+                        second_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node2.account))&lt;br/&gt;
+                        third_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                  timeout_sec=60,&lt;br/&gt;
+                                                  err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node3.account))&lt;br/&gt;
+&lt;br/&gt;
+    @staticmethod&lt;br/&gt;
+    def prepare_for(processor, version):&lt;br/&gt;
+        processor.node.account.ssh(&quot;rm -rf &quot; + processor.PERSISTENT_ROOT, allow_fail=False)&lt;br/&gt;
+        if version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(version)&lt;br/&gt;
+&lt;br/&gt;
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
+        first_other_processor = None&lt;br/&gt;
+        second_other_processor = None&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            if p != processor:&lt;br/&gt;
+                if first_other_processor is None:&lt;br/&gt;
+                    first_other_processor = p&lt;br/&gt;
+                else:&lt;br/&gt;
+                    second_other_processor = p&lt;br/&gt;
+&lt;br/&gt;
+        node = processor.node&lt;br/&gt;
+        first_other_node = first_other_processor.node&lt;br/&gt;
+        second_other_node = second_other_processor.node&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# allow some time for topics to be created&lt;/li&gt;
	&lt;li&gt;time.sleep(10)&lt;br/&gt;
+        # stop processor and wait for rebalance of others&lt;br/&gt;
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                processor.stop()&lt;br/&gt;
+                first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                timeout_sec=60,&lt;br/&gt;
+                                                err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+        node.account.ssh_capture(&quot;grep UPGRADE-TEST-CLIENT-CLOSED %s&quot; % processor.STDOUT_FILE, allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# use the current (dev) version driver&lt;/li&gt;
	&lt;li&gt;self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;/li&gt;
	&lt;li&gt;self.driver.node.version = KafkaVersion(from_version)&lt;/li&gt;
	&lt;li&gt;self.driver.start()&lt;br/&gt;
+        if upgrade_from is None:  # upgrade disabled &amp;#8211; second round of rolling bounces&lt;br/&gt;
+            roll_counter = &quot;.1-&quot;  # second round of rolling bounces&lt;br/&gt;
+        else:&lt;br/&gt;
+            roll_counter = &quot;.0-&quot;  # first  round of rolling boundes&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)&lt;/li&gt;
	&lt;li&gt;self.processor1.node.version = KafkaVersion(from_version)&lt;/li&gt;
	&lt;li&gt;self.processor1.start()&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDOUT_FILE + &quot; &quot; + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.STDERR_FILE + &quot; &quot; + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)&lt;br/&gt;
+        node.account.ssh(&quot;mv &quot; + processor.LOG_FILE + &quot; &quot; + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;time.sleep(15)&lt;br/&gt;
+        if new_version == str(DEV_VERSION):&lt;br/&gt;
+            processor.set_version(&quot;&quot;)  # set to TRUNK&lt;br/&gt;
+        else:&lt;br/&gt;
+            processor.set_version(new_version)&lt;br/&gt;
+        processor.set_upgrade_from(upgrade_from)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.perform_broker_upgrade(to_version)&lt;br/&gt;
+        grep_metadata_error = &quot;grep \&quot;org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\&quot; &quot;&lt;br/&gt;
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:&lt;br/&gt;
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:&lt;br/&gt;
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:&lt;br/&gt;
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:&lt;br/&gt;
+                        processor.start()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;time.sleep(15)&lt;/li&gt;
	&lt;li&gt;self.driver.wait()&lt;/li&gt;
	&lt;li&gt;self.driver.stop()&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Kafka version : &quot; + new_version,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect Kafka Streams version &quot; + new_version + &quot; &quot; + str(node.account))&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                       timeout_sec=60,&lt;br/&gt;
+                                                       err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.processor1.stop()&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                                        timeout_sec=60,&lt;br/&gt;
+                                                        err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))&lt;br/&gt;
+                        if len(found) &amp;gt; 0:&lt;br/&gt;
+                            raise Exception(&quot;Kafka Streams failed with &apos;unable to decode subscription data: version=2&apos;&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.driver.node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;/li&gt;
	&lt;li&gt;self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+                        monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node.account))&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index b7071e79dae..66e5fcf18aa 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -61,6 +61,7 @@ def get_version(node=None):&lt;br/&gt;
         return DEV_BRANCH&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; DEV_BRANCH = KafkaVersion(&quot;dev&quot;)&lt;br/&gt;
+DEV_VERSION = KafkaVersion(&quot;1.2.0-SNAPSHOT&quot;)&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;0.8.2.X versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;br/&gt;
@@ -89,7 +90,7 @@ def get_version(node=None):&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; LATEST_0_10 = LATEST_0_10_2&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;0.11.0.0 versions&lt;br/&gt;
+# 0.11.0.x versions&lt;br/&gt;
 V_0_11_0_0 = KafkaVersion(&quot;0.11.0.0&quot;)&lt;br/&gt;
 V_0_11_0_1 = KafkaVersion(&quot;0.11.0.1&quot;)&lt;br/&gt;
 V_0_11_0_2 = KafkaVersion(&quot;0.11.0.2&quot;)&lt;br/&gt;
diff --git a/vagrant/base.sh b/vagrant/base.sh&lt;br/&gt;
index bfc34967351..f5c03cca4fd 100755
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/vagrant/base.sh&lt;br/&gt;
+++ b/vagrant/base.sh&lt;br/&gt;
@@ -99,8 +99,10 @@ popd&lt;br/&gt;
 popd&lt;br/&gt;
 popd&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Test multiple Scala versions&lt;br/&gt;
-get_kafka 0.8.2.2 2.10&lt;br/&gt;
+# Test multiple Kafka versions&lt;br/&gt;
+# we want to use the latest Scala version per Kafka version&lt;br/&gt;
+# however, we cannot pull in Scala 2.12 builds atm, because Scala 2.12 requires Java 8, but we use Java 7 to run the system tests&lt;br/&gt;
+get_kafka 0.8.2.2 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.8.2.2&lt;br/&gt;
 get_kafka 0.9.0.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.9.0.1&lt;br/&gt;
@@ -112,10 +114,10 @@ get_kafka 0.10.2.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.10.2.1&lt;br/&gt;
 get_kafka 0.11.0.2 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-0.11.0.2&lt;br/&gt;
-get_kafka 1.0.0 2.11&lt;br/&gt;
-chmod a+rw /opt/kafka-1.0.0&lt;br/&gt;
 get_kafka 1.0.1 2.11&lt;br/&gt;
 chmod a+rw /opt/kafka-1.0.1&lt;br/&gt;
+get_kafka 1.1.0 2.11&lt;br/&gt;
+chmod a+rw /opt/kafka-1.1.0&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;ol&gt;
	&lt;li&gt;For EC2 nodes, we want to use /mnt, which should have the local disk. On local&lt;/li&gt;
&lt;/ol&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16439502" author="githubbot" created="Mon, 16 Apr 2018 14:24:05 +0000"  >&lt;p&gt;mjsax opened a new pull request #4880:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Update Kafka Streams metadata to version 3&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4880&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4880&lt;/a&gt;&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;adds Streams upgrade tests for 1.1 release&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16442058" author="githubbot" created="Wed, 18 Apr 2018 07:38:30 +0000"  >&lt;p&gt;mjsax closed pull request #4880:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Update Kafka Streams metadata to version 3&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4880&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4880&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/build.gradle b/build.gradle&lt;br/&gt;
index f8daf2fdddc..5b0e6496c2e 100644&lt;br/&gt;
&amp;#8212; a/build.gradle&lt;br/&gt;
+++ b/build.gradle&lt;br/&gt;
@@ -1087,6 +1087,18 @@ project(&apos;:streams:upgrade-system-tests-10&apos;) {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;+project(&apos;:streams:upgrade-system-tests-11&apos;) {&lt;br/&gt;
+  archivesBaseName = &quot;kafka-streams-upgrade-system-tests-11&quot;&lt;br/&gt;
+&lt;br/&gt;
+  dependencies &lt;/p&gt;
{
+    testCompile libs.kafkaStreams_11
+  }
&lt;p&gt;+&lt;br/&gt;
+  systemTestLibs &lt;/p&gt;
{
+    dependsOn testJar
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
 project(&apos;:jmh-benchmarks&apos;) {&lt;/p&gt;

&lt;p&gt;   apply plugin: &apos;com.github.johnrengelman.shadow&apos;&lt;br/&gt;
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle&lt;br/&gt;
index effe763ac45..a6ef5dddeec 100644&lt;br/&gt;
&amp;#8212; a/gradle/dependencies.gradle&lt;br/&gt;
+++ b/gradle/dependencies.gradle&lt;br/&gt;
@@ -67,6 +67,7 @@ versions += [&lt;br/&gt;
   kafka_0102: &quot;0.10.2.1&quot;,&lt;br/&gt;
   kafka_0110: &quot;0.11.0.2&quot;,&lt;br/&gt;
   kafka_10: &quot;1.0.1&quot;,&lt;br/&gt;
+  kafka_11: &quot;1.1.0&quot;,&lt;br/&gt;
   lz4: &quot;1.4.1&quot;,&lt;br/&gt;
   metrics: &quot;2.2.0&quot;,&lt;br/&gt;
   // PowerMock 1.x doesn&apos;t support Java 9, so use PowerMock 2.0.0 beta&lt;br/&gt;
@@ -115,6 +116,7 @@ libs += [&lt;br/&gt;
   kafkaStreams_0102: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0102&quot;,&lt;br/&gt;
   kafkaStreams_0110: &quot;org.apache.kafka:kafka-streams:$versions.kafka_0110&quot;,&lt;br/&gt;
   kafkaStreams_10: &quot;org.apache.kafka:kafka-streams:$versions.kafka_10&quot;,&lt;br/&gt;
+  kafkaStreams_11: &quot;org.apache.kafka:kafka-streams:$versions.kafka_11&quot;,&lt;br/&gt;
   log4j: &quot;log4j:log4j:$versions.log4j&quot;,&lt;br/&gt;
   lz4: &quot;org.lz4:lz4-java:$versions.lz4&quot;,&lt;br/&gt;
   metrics: &quot;com.yammer.metrics:metrics-core:$versions.metrics&quot;,&lt;br/&gt;
diff --git a/settings.gradle b/settings.gradle&lt;br/&gt;
index 03136849fd5..2a7977cfc93 100644&lt;br/&gt;
&amp;#8212; a/settings.gradle&lt;br/&gt;
+++ b/settings.gradle&lt;br/&gt;
@@ -15,5 +15,6 @@&lt;/p&gt;

&lt;p&gt; include &apos;core&apos;, &apos;examples&apos;, &apos;clients&apos;, &apos;tools&apos;, &apos;streams&apos;, &apos;streams:test-utils&apos;, &apos;streams:examples&apos;,&lt;br/&gt;
         &apos;streams:upgrade-system-tests-0100&apos;, &apos;streams:upgrade-system-tests-0101&apos;, &apos;streams:upgrade-system-tests-0102&apos;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&apos;streams:upgrade-system-tests-0110&apos;, &apos;streams:upgrade-system-tests-10&apos;, &apos;log4j-appender&apos;,&lt;/li&gt;
	&lt;li&gt;&apos;connect:api&apos;, &apos;connect:transforms&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;, &apos;jmh-benchmarks&apos;&lt;br/&gt;
+        &apos;streams:upgrade-system-tests-0110&apos;, &apos;streams:upgrade-system-tests-10&apos;, &apos;streams:upgrade-system-tests-11&apos;,&lt;br/&gt;
+        &apos;log4j-appender&apos;, &apos;connect:api&apos;, &apos;connect:transforms&apos;, &apos;connect:runtime&apos;, &apos;connect:json&apos;, &apos;connect:file&apos;,&lt;br/&gt;
+        &apos;jmh-benchmarks&apos;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index 819bebd43b6..65b1da6dede 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -172,6 +172,31 @@&lt;br/&gt;
      */&lt;br/&gt;
     public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;} for upgrading an application from version {@code 0.10.1.x}.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0101 = &quot;0.10.1&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.2.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0102 = &quot;0.10.2&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;} for upgrading an application from version {@code 0.11.0.x}.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_0110 = &quot;0.11.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 1.0.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_10 = &quot;1.0&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Config value for parameter &lt;/p&gt;
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 1.1.x}
&lt;p&gt;.&lt;br/&gt;
+     */&lt;br/&gt;
+    public static final String UPGRADE_FROM_11 = &quot;1.1&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #PROCESSING_GUARANTEE_CONFIG &quot;processing.guarantee&quot;}
&lt;p&gt; for at-least-once processing guarantees.&lt;br/&gt;
      */&lt;br/&gt;
@@ -347,8 +372,9 @@&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
     public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot; (for upgrading from 0.10.0.x).&quot;;&lt;br/&gt;
+    public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from versions 0.10.0/0.10.1/0.10.2/0.11.0/1.0/1.1 to version 1.2 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+        &quot;When upgrading from 1.2 to a newer version it is not required to specify this config.&quot; +&lt;br/&gt;
+        &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_0101 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_0102 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_0110 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_10 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_11 + &quot;\&quot; (for upgrading from the corresponding old version).&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code value.serde}
&lt;p&gt;@@ -364,7 +390,7 @@&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code zookeeper.connect}&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @deprecated Kakfa Streams does not use Zookeeper anymore and this parameter will be ignored.&lt;br/&gt;
+     * @deprecated Kafka Streams does not use Zookeeper anymore and this parameter will be ignored.&lt;br/&gt;
      */&lt;br/&gt;
     @Deprecated&lt;br/&gt;
     public static final String ZOOKEEPER_CONNECT_CONFIG = &quot;zookeeper.connect&quot;;&lt;br/&gt;
@@ -575,7 +601,7 @@&lt;br/&gt;
             .define(UPGRADE_FROM_CONFIG,&lt;br/&gt;
                     ConfigDef.Type.STRING,&lt;br/&gt;
                     null,&lt;/li&gt;
	&lt;li&gt;in(null, UPGRADE_FROM_0100),&lt;br/&gt;
+                    in(null, UPGRADE_FROM_0100, UPGRADE_FROM_0101, UPGRADE_FROM_0102, UPGRADE_FROM_0110, UPGRADE_FROM_10, UPGRADE_FROM_11),&lt;br/&gt;
                     Importance.LOW,&lt;br/&gt;
                     UPGRADE_FROM_DOC)&lt;br/&gt;
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
index 97771e56879..c81105ef821 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
@@ -199,10 +199,24 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
         final LogContext logContext = new LogContext(logPrefix);&lt;br/&gt;
         log = logContext.logger(getClass());&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;/li&gt;
	&lt;li&gt;if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {&lt;/li&gt;
	&lt;li&gt;log.info(&quot;Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x.&quot;);&lt;/li&gt;
	&lt;li&gt;userMetadataVersion = 1;&lt;br/&gt;
+        final String upgradeFrom = streamsConfig.getString(StreamsConfig.UPGRADE_FROM_CONFIG);&lt;br/&gt;
+        if (upgradeFrom != null) {&lt;br/&gt;
+            switch (upgradeFrom) {&lt;br/&gt;
+                case StreamsConfig.UPGRADE_FROM_0100:&lt;br/&gt;
+                    log.info(&quot;Downgrading metadata version from {} to 1 for upgrade from 0.10.0.x.&quot;, SubscriptionInfo.LATEST_SUPPORTED_VERSION);&lt;br/&gt;
+                    userMetadataVersion = 1;&lt;br/&gt;
+                    break;&lt;br/&gt;
+                case StreamsConfig.UPGRADE_FROM_0101:&lt;br/&gt;
+                case StreamsConfig.UPGRADE_FROM_0102:&lt;br/&gt;
+                case StreamsConfig.UPGRADE_FROM_0110:&lt;br/&gt;
+                case StreamsConfig.UPGRADE_FROM_10:&lt;br/&gt;
+                case StreamsConfig.UPGRADE_FROM_11:&lt;br/&gt;
+                    log.info(&quot;Downgrading metadata version from {} to 2 for upgrade from &quot; + upgradeFrom + &quot;.x.&quot;, SubscriptionInfo.LATEST_SUPPORTED_VERSION);&lt;br/&gt;
+                    userMetadataVersion = 2;&lt;br/&gt;
+                    break;&lt;br/&gt;
+                default:&lt;br/&gt;
+                    throw new IllegalArgumentException(&quot;Unknown configuration value for parameter &apos;upgrade.from&apos;: &quot; + upgradeFrom);&lt;br/&gt;
+            }&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);&lt;br/&gt;
@@ -512,7 +526,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;/p&gt;

&lt;p&gt;         // construct the global partition assignment per host map&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (minUserMetadataVersion == 2) {&lt;br/&gt;
+        if (minUserMetadataVersion == 2 || minUserMetadataVersion == 3) {&lt;br/&gt;
             for (final Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
                 final HostInfo hostInfo = entry.getValue().hostInfo;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -631,6 +645,10 @@ public void onAssignment(final Assignment assignment) &lt;/p&gt;
{
                 processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
                 partitionsByHost = info.partitionsByHost();
                 break;
+            case 3:
+                processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+                partitionsByHost = info.partitionsByHost();
+                break;
             default:
                 throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
                     + &quot;; latest supported version: &quot; + AssignmentInfo.LATEST_SUPPORTED_VERSION);
@@ -684,6 +702,13 @@ private void processVersionTwoAssignment(final AssignmentInfo info,
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;+    private void processVersionThreeAssignment(final AssignmentInfo info,&lt;br/&gt;
+                                               final List&amp;lt;TopicPartition&amp;gt; partitions,&lt;br/&gt;
+                                               final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks,&lt;br/&gt;
+                                               final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo) &lt;/p&gt;
{
+        processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Internal helper function that creates a Kafka topic&lt;br/&gt;
      *&lt;br/&gt;
@@ -818,4 +843,5 @@ void validate(final Set&amp;lt;String&amp;gt; copartitionGroup,&lt;br/&gt;
     void setInternalTopicManager(final InternalTopicManager internalTopicManager) 
{
         this.internalTopicManager = internalTopicManager;
     }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index c8df7498755..3c5cee2bfc3 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -16,8 +16,8 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.processor.internals.assignment;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import org.apache.kafka.common.utils.ByteBufferInputStream;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.utils.ByteBufferInputStream;&lt;br/&gt;
 import org.apache.kafka.streams.errors.TaskAssignmentException;&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
 import org.apache.kafka.streams.state.HostInfo;&lt;br/&gt;
@@ -30,6 +30,7 @@&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
@@ -40,15 +41,20 @@&lt;/p&gt;

&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(AssignmentInfo.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final int LATEST_SUPPORTED_VERSION = 2;&lt;br/&gt;
+    public static final int LATEST_SUPPORTED_VERSION = 3;&lt;br/&gt;
+    public static final int UNKNOWN = -1;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final int usedVersion;&lt;br/&gt;
+    private final int latestSupportedVersion;&lt;br/&gt;
     private List&amp;lt;TaskId&amp;gt; activeTasks;&lt;br/&gt;
     private Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks;&lt;br/&gt;
     private Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private AssignmentInfo(final int version) {&lt;br/&gt;
+    // used for decoding; don&apos;t apply version checks&lt;br/&gt;
+    private AssignmentInfo(final int version,&lt;br/&gt;
+                           final int latestSupportedVersion) 
{
         this.usedVersion = version;
+        this.latestSupportedVersion = latestSupportedVersion;
     }&lt;br/&gt;
 &lt;br/&gt;
     public AssignmentInfo(final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
@@ -57,11 +63,33 @@ public AssignmentInfo(final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
         this(LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
+    public AssignmentInfo() {
+        this(LATEST_SUPPORTED_VERSION,
+            Collections.&amp;lt;TaskId&amp;gt;emptyList(),
+            Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(),
+            Collections.&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap());
+    }&lt;br/&gt;
+&lt;br/&gt;
     public AssignmentInfo(final int version,&lt;br/&gt;
                           final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
                           final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
                           final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
+        this(version, LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);&lt;br/&gt;
+&lt;br/&gt;
+        if (version &amp;lt; 1 || version &amp;gt; LATEST_SUPPORTED_VERSION) {
+            throw new IllegalArgumentException(&quot;version must be between 1 and &quot; + LATEST_SUPPORTED_VERSION
+                + &quot;; was: &quot; + version);
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // for testing only; don&apos;t apply version checks&lt;br/&gt;
+    AssignmentInfo(final int version,&lt;br/&gt;
+                   final int latestSupportedVersion,&lt;br/&gt;
+                   final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
+                   final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks,&lt;br/&gt;
+                   final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; hostState) {&lt;br/&gt;
         this.usedVersion = version;&lt;br/&gt;
+        this.latestSupportedVersion = latestSupportedVersion;&lt;br/&gt;
         this.activeTasks = activeTasks;&lt;br/&gt;
         this.standbyTasks = standbyTasks;&lt;br/&gt;
         this.partitionsByHost = hostState;&lt;br/&gt;
@@ -71,6 +99,10 @@ public int version() {
         return usedVersion;
     }&lt;br/&gt;
 &lt;br/&gt;
+    public int latestSupportedVersion() {
+        return latestSupportedVersion;
+    }&lt;br/&gt;
+&lt;br/&gt;
     public List&amp;lt;TaskId&amp;gt; activeTasks() {
         return activeTasks;
     }&lt;br/&gt;
@@ -98,6 +130,9 @@ public ByteBuffer encode() {
                 case 2:
                     encodeVersionTwo(out);
                     break;
+                case 3:
+                    encodeVersionThree(out);
+                    break;
                 default:
                     throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
                         + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);
@@ -161,6 +196,13 @@ private void writeTopicPartitions(final DataOutputStream out,
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
+    private void encodeVersionThree(final DataOutputStream out) throws IOException {
+        out.writeInt(3);
+        out.writeInt(LATEST_SUPPORTED_VERSION);
+        encodeActiveAndStandbyTaskAssignment(out);
+        encodePartitionsByHost(out);
+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * @throws TaskAssignmentException if method fails to decode the data or if the data version is unknown&lt;br/&gt;
      */&lt;br/&gt;
@@ -169,19 +211,25 @@ public static AssignmentInfo decode(final ByteBuffer data) {&lt;br/&gt;
         data.rewind();&lt;br/&gt;
 &lt;br/&gt;
         try (final DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {&lt;br/&gt;
-            // decode used version&lt;br/&gt;
-            final int usedVersion = in.readInt();&lt;br/&gt;
-            final AssignmentInfo assignmentInfo = new AssignmentInfo(usedVersion);&lt;br/&gt;
+            final AssignmentInfo assignmentInfo;&lt;br/&gt;
 &lt;br/&gt;
+            final int usedVersion = in.readInt();&lt;br/&gt;
             switch (usedVersion) {&lt;br/&gt;
                 case 1:&lt;br/&gt;
+                    assignmentInfo = new AssignmentInfo(usedVersion, UNKNOWN);&lt;br/&gt;
                     decodeVersionOneData(assignmentInfo, in);&lt;br/&gt;
                     break;&lt;br/&gt;
                 case 2:&lt;br/&gt;
+                    assignmentInfo = new AssignmentInfo(usedVersion, UNKNOWN);&lt;br/&gt;
                     decodeVersionTwoData(assignmentInfo, in);&lt;br/&gt;
                     break;&lt;br/&gt;
+                case 3:&lt;br/&gt;
+                    final int latestSupportedVersion = in.readInt();&lt;br/&gt;
+                    assignmentInfo = new AssignmentInfo(usedVersion, latestSupportedVersion);&lt;br/&gt;
+                    decodeVersionThreeData(assignmentInfo, in);&lt;br/&gt;
+                    break;&lt;br/&gt;
                 default:&lt;br/&gt;
-                    TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode subscription data: &quot; +&lt;br/&gt;
+                    TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode assignment data: &quot; +&lt;br/&gt;
                         &quot;used version: &quot; + usedVersion + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);&lt;br/&gt;
                     log.error(fatalException.getMessage(), fatalException);&lt;br/&gt;
                     throw fatalException;&lt;br/&gt;
@@ -195,15 +243,23 @@ public static AssignmentInfo decode(final ByteBuffer data) {&lt;br/&gt;
 &lt;br/&gt;
     private static void decodeVersionOneData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
                                              final DataInputStream in) throws IOException {
-        // decode active tasks
-        int count = in.readInt();
+        decodeActiveTasks(assignmentInfo, in);
+        decodeStandbyTasks(assignmentInfo, in);
+        assignmentInfo.partitionsByHost = new HashMap&amp;lt;&amp;gt;();
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static void decodeActiveTasks(final AssignmentInfo assignmentInfo,&lt;br/&gt;
+                                          final DataInputStream in) throws IOException {&lt;br/&gt;
+        final int count = in.readInt();&lt;br/&gt;
         assignmentInfo.activeTasks = new ArrayList&amp;lt;&amp;gt;(count);&lt;br/&gt;
         for (int i = 0; i &amp;lt; count; i++) {
             assignmentInfo.activeTasks.add(TaskId.readFrom(in));
         }&lt;br/&gt;
+    }&lt;br/&gt;
 &lt;br/&gt;
-        // decode standby tasks&lt;br/&gt;
-        count = in.readInt();&lt;br/&gt;
+    private static void decodeStandbyTasks(final AssignmentInfo assignmentInfo,&lt;br/&gt;
+                                           final DataInputStream in) throws IOException {&lt;br/&gt;
+        final int count = in.readInt();&lt;br/&gt;
         assignmentInfo.standbyTasks = new HashMap&amp;lt;&amp;gt;(count);&lt;br/&gt;
         for (int i = 0; i &amp;lt; count; i++) {&lt;br/&gt;
             TaskId id = TaskId.readFrom(in);&lt;br/&gt;
@@ -213,9 +269,13 @@ private static void decodeVersionOneData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
 &lt;br/&gt;
     private static void decodeVersionTwoData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
                                              final DataInputStream in) throws IOException {
-        decodeVersionOneData(assignmentInfo, in);
+        decodeActiveTasks(assignmentInfo, in);
+        decodeStandbyTasks(assignmentInfo, in);
+        decodeGlobalAssignmentData(assignmentInfo, in);
+    }&lt;br/&gt;
 &lt;br/&gt;
-        // decode partitions by host&lt;br/&gt;
+    private static void decodeGlobalAssignmentData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
+                                                   final DataInputStream in) throws IOException {&lt;br/&gt;
         assignmentInfo.partitionsByHost = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         final int numEntries = in.readInt();&lt;br/&gt;
         for (int i = 0; i &amp;lt; numEntries; i++) {
@@ -233,19 +293,27 @@ private static void decodeVersionTwoData(final AssignmentInfo assignmentInfo,
         return partitions;
     }&lt;br/&gt;
 &lt;br/&gt;
+    private static void decodeVersionThreeData(final AssignmentInfo assignmentInfo,&lt;br/&gt;
+                                               final DataInputStream in) throws IOException {
+        decodeActiveTasks(assignmentInfo, in);
+        decodeStandbyTasks(assignmentInfo, in);
+        decodeGlobalAssignmentData(assignmentInfo, in);
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Override&lt;br/&gt;
     public int hashCode() {
-        return usedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
+        return usedVersion ^ latestSupportedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
     }&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public boolean equals(final Object o) {&lt;br/&gt;
         if (o instanceof AssignmentInfo) {
             final AssignmentInfo other = (AssignmentInfo) o;
-            return this.usedVersion == other.usedVersion &amp;amp;&amp;amp;
-                    this.activeTasks.equals(other.activeTasks) &amp;amp;&amp;amp;
-                    this.standbyTasks.equals(other.standbyTasks) &amp;amp;&amp;amp;
-                    this.partitionsByHost.equals(other.partitionsByHost);
+            return usedVersion == other.usedVersion &amp;amp;&amp;amp;
+                    latestSupportedVersion == other.latestSupportedVersion &amp;amp;&amp;amp;
+                    activeTasks.equals(other.activeTasks) &amp;amp;&amp;amp;
+                    standbyTasks.equals(other.standbyTasks) &amp;amp;&amp;amp;
+                    partitionsByHost.equals(other.partitionsByHost);
         } else {
             return false;
         }&lt;br/&gt;
@@ -253,7 +321,11 @@ public boolean equals(final Object o) {&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() {
-        return &quot;[version=&quot; + usedVersion + &quot;, active tasks=&quot; + activeTasks.size() + &quot;, standby tasks=&quot; + standbyTasks.size() + &quot;]&quot;;
+        return &quot;[version=&quot; + usedVersion
+            + &quot;, supported version=&quot; + latestSupportedVersion
+            + &quot;, active tasks=&quot; + activeTasks
+            + &quot;, standby tasks=&quot; + standbyTasks
+            + &quot;, global assignment=&quot; + partitionsByHost + &quot;]&quot;;
     }&lt;br/&gt;
 &lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index 7fee90b5402..be709472441 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -23,6 +23,7 @@&lt;br/&gt;
 &lt;br/&gt;
 import java.nio.ByteBuffer;&lt;br/&gt;
 import java.nio.charset.Charset;&lt;br/&gt;
+import java.util.Collection;&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
@@ -31,16 +32,21 @@&lt;br/&gt;
 &lt;br/&gt;
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;br/&gt;
 &lt;br/&gt;
-    public static final int LATEST_SUPPORTED_VERSION = 2;&lt;br/&gt;
+    public static final int LATEST_SUPPORTED_VERSION = 3;&lt;br/&gt;
+    public static final int UNKNOWN = -1;&lt;br/&gt;
 &lt;br/&gt;
     private final int usedVersion;&lt;br/&gt;
+    private final int latestSupportedVersion;&lt;br/&gt;
     private UUID processId;&lt;br/&gt;
     private Set&amp;lt;TaskId&amp;gt; prevTasks;&lt;br/&gt;
     private Set&amp;lt;TaskId&amp;gt; standbyTasks;&lt;br/&gt;
     private String userEndPoint;&lt;br/&gt;
 &lt;br/&gt;
-    private SubscriptionInfo(final int version) {&lt;br/&gt;
+    // used for decoding; don&apos;t apply version checks&lt;br/&gt;
+    private SubscriptionInfo(final int version,&lt;br/&gt;
+                             final int latestSupportedVersion) {         this.usedVersion = version;+        this.latestSupportedVersion = latestSupportedVersion;     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public SubscriptionInfo(final UUID processId,&lt;br/&gt;
@@ -55,7 +61,23 @@ public SubscriptionInfo(final int version,&lt;br/&gt;
                             final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
                             final Set&amp;lt;TaskId&amp;gt; standbyTasks,&lt;br/&gt;
                             final String userEndPoint) {&lt;br/&gt;
+        this(version, LATEST_SUPPORTED_VERSION, processId, prevTasks, standbyTasks, userEndPoint);&lt;br/&gt;
+&lt;br/&gt;
+        if (version &amp;lt; 1 || version &amp;gt; LATEST_SUPPORTED_VERSION) &lt;/p&gt;
{
+            throw new IllegalArgumentException(&quot;version must be between 1 and &quot; + LATEST_SUPPORTED_VERSION
+                + &quot;; was: &quot; + version);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    // for testing only; don&apos;t apply version checks&lt;br/&gt;
+    protected SubscriptionInfo(final int version,&lt;br/&gt;
+                               final int latestSupportedVersion,&lt;br/&gt;
+                               final UUID processId,&lt;br/&gt;
+                               final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
+                               final Set&amp;lt;TaskId&amp;gt; standbyTasks,&lt;br/&gt;
+                               final String userEndPoint) {&lt;br/&gt;
         this.usedVersion = version;&lt;br/&gt;
+        this.latestSupportedVersion = latestSupportedVersion;&lt;br/&gt;
         this.processId = processId;&lt;br/&gt;
         this.prevTasks = prevTasks;&lt;br/&gt;
         this.standbyTasks = standbyTasks;&lt;br/&gt;
@@ -66,6 +88,10 @@ public int version() &lt;/p&gt;
{
         return usedVersion;
     }

&lt;p&gt;+    public int latestSupportedVersion() &lt;/p&gt;
{
+        return latestSupportedVersion;
+    }
&lt;p&gt;+&lt;br/&gt;
     public UUID processId() &lt;/p&gt;
{
         return processId;
     }
&lt;p&gt;@@ -93,7 +119,10 @@ public ByteBuffer encode() {&lt;br/&gt;
                 buf = encodeVersionOne();&lt;br/&gt;
                 break;&lt;br/&gt;
             case 2:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;buf = encodeVersionTwo(prepareUserEndPoint());&lt;br/&gt;
+                buf = encodeVersionTwo();&lt;br/&gt;
+                break;&lt;br/&gt;
+            case 3:&lt;br/&gt;
+                buf = encodeVersionThree();&lt;br/&gt;
                 break;&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion&lt;br/&gt;
@@ -108,7 +137,9 @@ private ByteBuffer encodeVersionOne() 
{
         final ByteBuffer buf = ByteBuffer.allocate(getVersionOneByteLength());
 
         buf.putInt(1); // version
-        encodeVersionOneData(buf);
+        encodeClientUUID(buf);
+        encodeTasks(buf, prevTasks);
+        encodeTasks(buf, standbyTasks);
 
         return buf;
     }
&lt;p&gt;@@ -120,18 +151,15 @@ private int getVersionOneByteLength() &lt;/p&gt;
{
                4 + standbyTasks.size() * 8; // length + standby tasks
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void encodeVersionOneData(final ByteBuffer buf) {&lt;/li&gt;
	&lt;li&gt;// encode client UUID&lt;br/&gt;
+    private void encodeClientUUID(final ByteBuffer buf) {&lt;br/&gt;
         buf.putLong(processId.getMostSignificantBits());&lt;br/&gt;
         buf.putLong(processId.getLeastSignificantBits());&lt;/li&gt;
	&lt;li&gt;// encode ids of previously running tasks&lt;/li&gt;
	&lt;li&gt;buf.putInt(prevTasks.size());&lt;/li&gt;
	&lt;li&gt;for (TaskId id : prevTasks) 
{
-            id.writeTo(buf);
-        }&lt;br/&gt;
-        // encode ids of cached tasks&lt;br/&gt;
-        buf.putInt(standbyTasks.size());&lt;br/&gt;
-        for (TaskId id : standbyTasks) {
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void encodeTasks(final ByteBuffer buf,&lt;br/&gt;
+                             final Collection&amp;lt;TaskId&amp;gt; taskIds) {&lt;br/&gt;
+        buf.putInt(taskIds.size());&lt;br/&gt;
+        for (TaskId id : taskIds) {
             id.writeTo(buf);
         }&lt;br/&gt;
     }&lt;br/&gt;
@@ -144,52 +172,87 @@ private void encodeVersionOneData(final ByteBuffer buf) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    private ByteBuffer encodeVersionTwo(final byte[] endPointBytes) {&lt;br/&gt;
+    private ByteBuffer encodeVersionTwo() {
+        final byte[] endPointBytes = prepareUserEndPoint();
+
         final ByteBuffer buf = ByteBuffer.allocate(getVersionTwoByteLength(endPointBytes));
 
         buf.putInt(2); // version
-        encodeVersionTwoData(buf, endPointBytes);
+        encodeClientUUID(buf);
+        encodeTasks(buf, prevTasks);
+        encodeTasks(buf, standbyTasks);
+        encodeUserEndPoint(buf, endPointBytes);
 
         return buf;
     }&lt;br/&gt;
 &lt;br/&gt;
     private int getVersionTwoByteLength(final byte[] endPointBytes) {
-        return getVersionOneByteLength() +
+        return 4 + // version
+               16 + // client ID
+               4 + prevTasks.size() * 8 + // length + prev tasks
+               4 + standbyTasks.size() * 8 + // length + standby tasks
                4 + endPointBytes.length; // length + userEndPoint
     }&lt;br/&gt;
 &lt;br/&gt;
-    private void encodeVersionTwoData(final ByteBuffer buf,&lt;br/&gt;
-                                      final byte[] endPointBytes) {&lt;br/&gt;
-        encodeVersionOneData(buf);&lt;br/&gt;
+    private void encodeUserEndPoint(final ByteBuffer buf,&lt;br/&gt;
+                                    final byte[] endPointBytes) {&lt;br/&gt;
         if (endPointBytes != null) {
             buf.putInt(endPointBytes.length);
             buf.put(endPointBytes);
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
+    private ByteBuffer encodeVersionThree() {
+        final byte[] endPointBytes = prepareUserEndPoint();
+
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeByteLength(endPointBytes));
+
+        buf.putInt(3); // used version
+        buf.putInt(LATEST_SUPPORTED_VERSION); // supported version
+        encodeClientUUID(buf);
+        encodeTasks(buf, prevTasks);
+        encodeTasks(buf, standbyTasks);
+        encodeUserEndPoint(buf, endPointBytes);
+
+        return buf;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private int getVersionThreeByteLength(final byte[] endPointBytes) {
+        return 4 + // used version
+               4 + // latest supported version version
+               16 + // client ID
+               4 + prevTasks.size() * 8 + // length + prev tasks
+               4 + standbyTasks.size() * 8 + // length + standby tasks
+               4 + endPointBytes.length; // length + userEndPoint
+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * @throws TaskAssignmentException if method fails to decode the data&lt;br/&gt;
      */&lt;br/&gt;
     public static SubscriptionInfo decode(final ByteBuffer data) {&lt;br/&gt;
+        final SubscriptionInfo subscriptionInfo;&lt;br/&gt;
+&lt;br/&gt;
         // ensure we are at the beginning of the ByteBuffer&lt;br/&gt;
         data.rewind();&lt;br/&gt;
 &lt;br/&gt;
-        // decode used version&lt;br/&gt;
         final int usedVersion = data.getInt();&lt;br/&gt;
-        final SubscriptionInfo subscriptionInfo = new SubscriptionInfo(usedVersion);&lt;br/&gt;
-&lt;br/&gt;
         switch (usedVersion) {&lt;br/&gt;
             case 1:&lt;br/&gt;
+                subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);&lt;br/&gt;
                 decodeVersionOneData(subscriptionInfo, data);&lt;br/&gt;
                 break;&lt;br/&gt;
             case 2:&lt;br/&gt;
+                subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);&lt;br/&gt;
                 decodeVersionTwoData(subscriptionInfo, data);&lt;br/&gt;
                 break;&lt;br/&gt;
+            case 3:&lt;br/&gt;
+                final int latestSupportedVersion = data.getInt();&lt;br/&gt;
+                subscriptionInfo = new SubscriptionInfo(usedVersion, latestSupportedVersion);&lt;br/&gt;
+                decodeVersionThreeData(subscriptionInfo, data);&lt;br/&gt;
+                break;&lt;br/&gt;
             default:&lt;br/&gt;
-                TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode subscription data: &quot; +&lt;br/&gt;
-                    &quot;used version: &quot; + usedVersion + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);&lt;br/&gt;
-                log.error(fatalException.getMessage(), fatalException);&lt;br/&gt;
-                throw fatalException;&lt;br/&gt;
+                subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);&lt;br/&gt;
+                log.info(&quot;Unable to decode subscription data: used version: {}; latest supported version: {}&quot;, usedVersion, LATEST_SUPPORTED_VERSION);&lt;br/&gt;
         }&lt;br/&gt;
 &lt;br/&gt;
         return subscriptionInfo;&lt;br/&gt;
@@ -197,30 +260,43 @@ public static SubscriptionInfo decode(final ByteBuffer data) {&lt;br/&gt;
 &lt;br/&gt;
     private static void decodeVersionOneData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
                                              final ByteBuffer data) {&lt;br/&gt;
-        // decode client UUID&lt;br/&gt;
-        subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());&lt;br/&gt;
+        decodeClientUUID(subscriptionInfo, data);&lt;br/&gt;
 &lt;br/&gt;
-        // decode previously active tasks&lt;br/&gt;
-        final int numPrevs = data.getInt();&lt;br/&gt;
         subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-        for (int i = 0; i &amp;lt; numPrevs; i++) {
-            TaskId id = TaskId.readFrom(data);
-            subscriptionInfo.prevTasks.add(id);
-        }&lt;br/&gt;
+        decodeTasks(subscriptionInfo.prevTasks, data);&lt;br/&gt;
 &lt;br/&gt;
-        // decode previously cached tasks&lt;br/&gt;
-        final int numCached = data.getInt();&lt;br/&gt;
         subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-        for (int i = 0; i &amp;lt; numCached; i++) {
-            subscriptionInfo.standbyTasks.add(TaskId.readFrom(data));
+        decodeTasks(subscriptionInfo.standbyTasks, data);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static void decodeClientUUID(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
+                                         final ByteBuffer data) {
+        subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static void decodeTasks(final Collection&amp;lt;TaskId&amp;gt; taskIds,&lt;br/&gt;
+                                    final ByteBuffer data) {&lt;br/&gt;
+        final int numPrevs = data.getInt();&lt;br/&gt;
+        for (int i = 0; i &amp;lt; numPrevs; i++) {
+            taskIds.add(TaskId.readFrom(data));
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
                                              final ByteBuffer data) {
-        decodeVersionOneData(subscriptionInfo, data);
+        decodeClientUUID(subscriptionInfo, data);
+
+        subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();
+        decodeTasks(subscriptionInfo.prevTasks, data);
 
-        // decode user end point (can be null)
+        subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();
+        decodeTasks(subscriptionInfo.standbyTasks, data);
+
+        decodeUserEndPoint(subscriptionInfo, data);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static void decodeUserEndPoint(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
+                                           final ByteBuffer data) {&lt;br/&gt;
         int bytesLength = data.getInt();&lt;br/&gt;
         if (bytesLength != 0) {
             final byte[] bytes = new byte[bytesLength];
@@ -229,9 +305,21 @@ private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    @Override&lt;br/&gt;
+    private static void decodeVersionThreeData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
+                                               final ByteBuffer data) {
+        decodeClientUUID(subscriptionInfo, data);
+
+        subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();
+        decodeTasks(subscriptionInfo.prevTasks, data);
+
+        subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();
+        decodeTasks(subscriptionInfo.standbyTasks, data);
+
+        decodeUserEndPoint(subscriptionInfo, data);
+    }&lt;br/&gt;
+&lt;br/&gt;
     public int hashCode() {&lt;br/&gt;
-        final int hashCode = usedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();&lt;br/&gt;
+        final int hashCode = usedVersion ^ latestSupportedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();&lt;br/&gt;
         if (userEndPoint == null) {
             return hashCode;
         }&lt;br/&gt;
@@ -243,6 +331,7 @@ public boolean equals(final Object o) {&lt;br/&gt;
         if (o instanceof SubscriptionInfo) {&lt;br/&gt;
             final SubscriptionInfo other = (SubscriptionInfo) o;&lt;br/&gt;
             return this.usedVersion == other.usedVersion &amp;amp;&amp;amp;&lt;br/&gt;
+                    this.latestSupportedVersion == other.latestSupportedVersion &amp;amp;&amp;amp;&lt;br/&gt;
                     this.processId.equals(other.processId) &amp;amp;&amp;amp;&lt;br/&gt;
                     this.prevTasks.equals(other.prevTasks) &amp;amp;&amp;amp;&lt;br/&gt;
                     this.standbyTasks.equals(other.standbyTasks) &amp;amp;&amp;amp;&lt;br/&gt;
@@ -252,4 +341,13 @@ public boolean equals(final Object o) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
+    @Override&lt;br/&gt;
+    public String toString() {
+        return &quot;[version=&quot; + usedVersion
+            + &quot;, supported version=&quot; + latestSupportedVersion
+            + &quot;, process ID=&quot; + processId
+            + &quot;, prev tasks=&quot; + prevTasks
+            + &quot;, standby tasks=&quot; + standbyTasks
+            + &quot;, user endpoint=&quot; + userEndPoint + &quot;]&quot;;
+    }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
index e9ed9682066..4e04b4985ed 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
@@ -46,7 +46,6 @@&lt;br/&gt;
 import org.apache.kafka.test.MockStateStoreSupplier;&lt;br/&gt;
 import org.easymock.Capture;&lt;br/&gt;
 import org.easymock.EasyMock;&lt;br/&gt;
-import org.junit.Assert;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 &lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
@@ -64,6 +63,7 @@&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertNotEquals;&lt;br/&gt;
 import static org.junit.Assert.assertThat;&lt;br/&gt;
+import static org.junit.Assert.fail;&lt;br/&gt;
 &lt;br/&gt;
 public class StreamsPartitionAssignorTest {&lt;br/&gt;
 &lt;br/&gt;
@@ -867,9 +867,12 @@ public void shouldMapUserEndPointToTopicPartitions() {
         final PartitionAssignor.Assignment consumerAssignment = assignments.get(&quot;consumer1&quot;);
         final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumerAssignment.userData());
         final Set&amp;lt;TopicPartition&amp;gt; topicPartitions = assignmentInfo.partitionsByHost().get(new HostInfo(&quot;localhost&quot;, 8080));
-        assertEquals(Utils.mkSet(new TopicPartition(&quot;topic1&quot;, 0),
+        assertEquals(
+            Utils.mkSet(
+                new TopicPartition(&quot;topic1&quot;, 0),
                 new TopicPartition(&quot;topic1&quot;, 1),
-                new TopicPartition(&quot;topic1&quot;, 2)), topicPartitions);
+                new TopicPartition(&quot;topic1&quot;, 2)),
+            topicPartitions);
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -881,7 +884,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {&lt;br/&gt;
 &lt;br/&gt;
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) &quot;localhost&quot;));
-            Assert.fail(&quot;expected to an exception due to invalid config&quot;);
+            fail(&quot;expected to an exception due to invalid config&quot;);
         } catch (ConfigException e) {
             // pass
         }&lt;br/&gt;
@@ -893,7 +896,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {&lt;br/&gt;
 &lt;br/&gt;
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) &quot;localhost:j87yhk&quot;));
-            Assert.fail(&quot;expected to an exception due to invalid config&quot;);
+            fail(&quot;expected to an exception due to invalid config&quot;);
         } catch (ConfigException e) {             // pass         }&lt;br/&gt;
@@ -1088,21 +1091,36 @@ public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProvider&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
-    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersionsV1V2() {
+        shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(1, 2);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersionsV1V3() {
+        shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(1, 3);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersionsV2V3() {
+        shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(2, 3);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(final int smallestVersion,&lt;br/&gt;
+                                                                                     final int otherVersion) {
         final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();
         final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
         subscriptions.put(
             &quot;consumer1&quot;,
             new PartitionAssignor.Subscription(
                 Collections.singletonList(&quot;topic1&quot;),
-                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+                new SubscriptionInfo(smallestVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
             )
         );
         subscriptions.put(
             &quot;consumer2&quot;,
             new PartitionAssignor.Subscription(
                 Collections.singletonList(&quot;topic1&quot;),
-                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+                new SubscriptionInfo(otherVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
             )
         );
 
@@ -1115,12 +1133,12 @@ public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(
         final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);
 
         assertThat(assignment.size(), equalTo(2));
-        assertThat(AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version(), equalTo(1));
-        assertThat(AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version(), equalTo(1));
+        assertThat(AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()).version(), equalTo(smallestVersion));
+        assertThat(AssignmentInfo.decode(assignment.get(&quot;consumer2&quot;).userData()).version(), equalTo(smallestVersion));
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
-    public void shouldDownGradeSubscription() {&lt;br/&gt;
+    public void shouldDownGradeSubscriptionToVersion1() {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();&lt;br/&gt;
 &lt;br/&gt;
         mockTaskManager(&lt;br/&gt;
@@ -1135,6 +1153,46 @@ public void shouldDownGradeSubscription() {
         assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(1));
     }&lt;br/&gt;
 &lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscriptionToVersion2For0101() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0101);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscriptionToVersion2For0102() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0102);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscriptionToVersion2For0110() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0110);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscriptionToVersion2For10() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_10);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldDownGradeSubscriptionToVersion2For11() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_11);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void shouldDownGradeSubscriptionToVersion2(final Object upgradeFromValue) {
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFromValue));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+
+        assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(2));
+    }&lt;br/&gt;
+&lt;br/&gt;
     private PartitionAssignor.Assignment createAssignment(final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState) {&lt;br/&gt;
         final AssignmentInfo info = new AssignmentInfo(Collections.&amp;lt;TaskId&amp;gt;emptyList(),&lt;br/&gt;
                                                        Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(),&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
index c1020a98ba9..c7382e7671c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java&lt;br/&gt;
@@ -22,85 +22,70 @@&lt;br/&gt;
 import org.apache.kafka.streams.state.HostInfo;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 &lt;br/&gt;
-import java.io.ByteArrayOutputStream;&lt;br/&gt;
-import java.io.DataOutputStream;&lt;br/&gt;
-import java.io.IOException;&lt;br/&gt;
-import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 &lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
-import static org.junit.Assert.assertNull;&lt;br/&gt;
 &lt;br/&gt;
 public class AssignmentInfoTest {&lt;br/&gt;
+    private final List&amp;lt;TaskId&amp;gt; activeTasks = Arrays.asList(&lt;br/&gt;
+        new TaskId(0, 0),&lt;br/&gt;
+        new TaskId(0, 0),&lt;br/&gt;
+        new TaskId(0, 1), new TaskId(1, 0));&lt;br/&gt;
+    private final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks = new HashMap&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;() {&lt;br/&gt;
+        {
+            put(new TaskId(1, 1),
+                Utils.mkSet(new TopicPartition(&quot;t1&quot;, 1), new TopicPartition(&quot;t2&quot;, 1)));
+            put(new TaskId(2, 0),
+                Utils.mkSet(new TopicPartition(&quot;t3&quot;, 0), new TopicPartition(&quot;t3&quot;, 0)));
+        }&lt;br/&gt;
+    };&lt;br/&gt;
+    private final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; globalAssignment = new HashMap&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;() {&lt;br/&gt;
+        {
+            put(new HostInfo(&quot;localhost&quot;, 80),
+                Utils.mkSet(new TopicPartition(&quot;t1&quot;, 1), new TopicPartition(&quot;t3&quot;, 3)));
+        }&lt;br/&gt;
+    };&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
-    public void testEncodeDecode() {&lt;br/&gt;
-        List&amp;lt;TaskId&amp;gt; activeTasks =&lt;br/&gt;
-                Arrays.asList(new TaskId(0, 0), new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0));&lt;br/&gt;
-        Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
-        standbyTasks.put(new TaskId(1, 1), Utils.mkSet(new TopicPartition(&quot;t1&quot;, 1), new TopicPartition(&quot;t2&quot;, 1)));&lt;br/&gt;
-        standbyTasks.put(new TaskId(2, 0), Utils.mkSet(new TopicPartition(&quot;t3&quot;, 0), new TopicPartition(&quot;t3&quot;, 0)));&lt;br/&gt;
+    public void shouldUseLatestSupportedVersionByDefault() {
+        final AssignmentInfo info = new AssignmentInfo(activeTasks, standbyTasks, globalAssignment);
+        assertEquals(AssignmentInfo.LATEST_SUPPORTED_VERSION, info.version());
+    }&lt;br/&gt;
 &lt;br/&gt;
-        AssignmentInfo info = new AssignmentInfo(activeTasks, standbyTasks, new HashMap&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;());&lt;br/&gt;
-        AssignmentInfo decoded = AssignmentInfo.decode(info.encode());&lt;br/&gt;
+    @Test(expected = IllegalArgumentException.class)&lt;br/&gt;
+    public void shouldThrowForUnknownVersion1() {
+        new AssignmentInfo(0, activeTasks, standbyTasks, globalAssignment);
+    }&lt;br/&gt;
 &lt;br/&gt;
-        assertEquals(info, decoded);&lt;br/&gt;
+    @Test(expected = IllegalArgumentException.class)&lt;br/&gt;
+    public void shouldThrowForUnknownVersion2() {
+        new AssignmentInfo(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1, activeTasks, standbyTasks, globalAssignment);
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
-    public void shouldDecodePreviousVersion() throws IOException {&lt;br/&gt;
-        List&amp;lt;TaskId&amp;gt; activeTasks =&lt;br/&gt;
-                Arrays.asList(new TaskId(0, 0), new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0));&lt;br/&gt;
-        Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
-        standbyTasks.put(new TaskId(1, 1), Utils.mkSet(new TopicPartition(&quot;t1&quot;, 1), new TopicPartition(&quot;t2&quot;, 1)));&lt;br/&gt;
-        standbyTasks.put(new TaskId(2, 0), Utils.mkSet(new TopicPartition(&quot;t3&quot;, 0), new TopicPartition(&quot;t3&quot;, 0)));&lt;br/&gt;
-        final AssignmentInfo oldVersion = new AssignmentInfo(1, activeTasks, standbyTasks, null);&lt;br/&gt;
-        final AssignmentInfo decoded = AssignmentInfo.decode(encodeV1(oldVersion));&lt;br/&gt;
-        assertEquals(oldVersion.activeTasks(), decoded.activeTasks());&lt;br/&gt;
-        assertEquals(oldVersion.standbyTasks(), decoded.standbyTasks());&lt;br/&gt;
-        assertNull(decoded.partitionsByHost()); // should be null as wasn&apos;t in V1&lt;br/&gt;
-        assertEquals(1, decoded.version());&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion1() {
+        final AssignmentInfo info = new AssignmentInfo(1, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(1, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, Collections.&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap());
+        assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
     }&lt;br/&gt;
 &lt;br/&gt;
-    /**&lt;br/&gt;
-     * This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;br/&gt;
-     * so it is impossible to test compatibility without having this&lt;br/&gt;
-     */&lt;br/&gt;
-    private ByteBuffer encodeV1(AssignmentInfo oldVersion) throws IOException {&lt;br/&gt;
-        ByteArrayOutputStream baos = new ByteArrayOutputStream();&lt;br/&gt;
-        DataOutputStream out = new DataOutputStream(baos);&lt;br/&gt;
-        // Encode version&lt;br/&gt;
-        out.writeInt(oldVersion.version());&lt;br/&gt;
-        // Encode active tasks&lt;br/&gt;
-        out.writeInt(oldVersion.activeTasks().size());&lt;br/&gt;
-        for (TaskId id : oldVersion.activeTasks()) {
-            id.writeTo(out);
-        }&lt;br/&gt;
-        // Encode standby tasks&lt;br/&gt;
-        out.writeInt(oldVersion.standbyTasks().size());&lt;br/&gt;
-        for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : oldVersion.standbyTasks().entrySet()) {&lt;br/&gt;
-            TaskId id = entry.getKey();&lt;br/&gt;
-            id.writeTo(out);&lt;br/&gt;
-&lt;br/&gt;
-            Set&amp;lt;TopicPartition&amp;gt; partitions = entry.getValue();&lt;br/&gt;
-            out.writeInt(partitions.size());&lt;br/&gt;
-            for (TopicPartition partition : partitions) {
-                out.writeUTF(partition.topic());
-                out.writeInt(partition.partition());
-            }&lt;br/&gt;
-        }&lt;br/&gt;
-&lt;br/&gt;
-        out.flush();&lt;br/&gt;
-        out.close();&lt;br/&gt;
-&lt;br/&gt;
-        return ByteBuffer.wrap(baos.toByteArray());&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion2() {
+        final AssignmentInfo info = new AssignmentInfo(2, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(2, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, globalAssignment);
+        assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
+    }&lt;br/&gt;
 &lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion3() {
+        final AssignmentInfo info = new AssignmentInfo(3, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(3, AssignmentInfo.LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, globalAssignment);
+        assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
     }&lt;br/&gt;
 &lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
index 633285a2b4d..e98b8ce0727 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
@@ -19,81 +19,60 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 &lt;br/&gt;
-import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
-import java.util.Collections;&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
 &lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
-import static org.junit.Assert.assertNull;&lt;br/&gt;
 &lt;br/&gt;
 public class SubscriptionInfoTest {&lt;br/&gt;
+    private final UUID processId = UUID.randomUUID();&lt;br/&gt;
+    private final Set&amp;lt;TaskId&amp;gt; activeTasks = new HashSet&amp;lt;&amp;gt;(Arrays.asList(&lt;br/&gt;
+        new TaskId(0, 0),&lt;br/&gt;
+        new TaskId(0, 1),&lt;br/&gt;
+        new TaskId(1, 0)));&lt;br/&gt;
+    private final Set&amp;lt;TaskId&amp;gt; standbyTasks = new HashSet&amp;lt;&amp;gt;(Arrays.asList(&lt;br/&gt;
+        new TaskId(1, 1),&lt;br/&gt;
+        new TaskId(2, 0)));&lt;br/&gt;
 &lt;br/&gt;
-    @Test&lt;br/&gt;
-    public void testEncodeDecode() {&lt;br/&gt;
-        UUID processId = UUID.randomUUID();&lt;br/&gt;
+    private final static String IGNORED_USER_ENDPOINT = &quot;ignoredUserEndpoint:80&quot;;&lt;br/&gt;
 &lt;br/&gt;
-        Set&amp;lt;TaskId&amp;gt; activeTasks =&lt;br/&gt;
-                new HashSet&amp;lt;&amp;gt;(Arrays.asList(new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0)));&lt;br/&gt;
-        Set&amp;lt;TaskId&amp;gt; standbyTasks =&lt;br/&gt;
-                new HashSet&amp;lt;&amp;gt;(Arrays.asList(new TaskId(1, 1), new TaskId(2, 0)));&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldUseLatestSupportedVersionByDefault() {
+        final SubscriptionInfo info = new SubscriptionInfo(processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+        assertEquals(SubscriptionInfo.LATEST_SUPPORTED_VERSION, info.version());
+    }&lt;br/&gt;
 &lt;br/&gt;
-        SubscriptionInfo info = new SubscriptionInfo(processId, activeTasks, standbyTasks, null);&lt;br/&gt;
-        SubscriptionInfo decoded = SubscriptionInfo.decode(info.encode());&lt;br/&gt;
+    @Test(expected = IllegalArgumentException.class)&lt;br/&gt;
+    public void shouldThrowForUnknownVersion1() {
+        new SubscriptionInfo(0, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+    }&lt;br/&gt;
 &lt;br/&gt;
-        assertEquals(info, decoded);&lt;br/&gt;
+    @Test(expected = IllegalArgumentException.class)&lt;br/&gt;
+    public void shouldThrowForUnknownVersion2() {
+        new SubscriptionInfo(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
-    public void shouldEncodeDecodeWithUserEndPoint() {&lt;br/&gt;
-        SubscriptionInfo original = new SubscriptionInfo(UUID.randomUUID(),&lt;br/&gt;
-                Collections.singleton(new TaskId(0, 0)), Collections.&amp;lt;TaskId&amp;gt;emptySet(), &quot;localhost:80&quot;);&lt;br/&gt;
-        SubscriptionInfo decoded = SubscriptionInfo.decode(original.encode());&lt;br/&gt;
-        assertEquals(original, decoded);&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion1() {
+        final SubscriptionInfo info = new SubscriptionInfo(1, processId, activeTasks, standbyTasks, IGNORED_USER_ENDPOINT);
+        final SubscriptionInfo expectedInfo = new SubscriptionInfo(1, SubscriptionInfo.UNKNOWN, processId, activeTasks, standbyTasks, null);
+        assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
-    public void shouldBeBackwardCompatible() {&lt;br/&gt;
-        UUID processId = UUID.randomUUID();&lt;br/&gt;
-&lt;br/&gt;
-        Set&amp;lt;TaskId&amp;gt; activeTasks =&lt;br/&gt;
-                new HashSet&amp;lt;&amp;gt;(Arrays.asList(new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0)));&lt;br/&gt;
-        Set&amp;lt;TaskId&amp;gt; standbyTasks =&lt;br/&gt;
-                new HashSet&amp;lt;&amp;gt;(Arrays.asList(new TaskId(1, 1), new TaskId(2, 0)));&lt;br/&gt;
-&lt;br/&gt;
-        final ByteBuffer v1Encoding = encodePreviousVersion(processId, activeTasks, standbyTasks);&lt;br/&gt;
-        final SubscriptionInfo decode = SubscriptionInfo.decode(v1Encoding);&lt;br/&gt;
-        assertEquals(activeTasks, decode.prevTasks());&lt;br/&gt;
-        assertEquals(standbyTasks, decode.standbyTasks());&lt;br/&gt;
-        assertEquals(processId, decode.processId());&lt;br/&gt;
-        assertNull(decode.userEndPoint());&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion2() {
+        final SubscriptionInfo info = new SubscriptionInfo(2, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+        final SubscriptionInfo expectedInfo = new SubscriptionInfo(2, SubscriptionInfo.UNKNOWN, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+        assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }&lt;br/&gt;
 &lt;br/&gt;
-    /**&lt;br/&gt;
-     * This is a clone of what the V1 encoding did. The encode method has changed for V2&lt;br/&gt;
-     * so it is impossible to test compatibility without having this&lt;br/&gt;
-     */&lt;br/&gt;
-    private ByteBuffer encodePreviousVersion(UUID processId,  Set&amp;lt;TaskId&amp;gt; prevTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks) {&lt;br/&gt;
-        ByteBuffer buf = ByteBuffer.allocate(4 /* version &lt;b&gt;/ + 16 /&lt;/b&gt; process id */ + 4 + prevTasks.size() * 8 + 4 + standbyTasks.size() * 8);&lt;br/&gt;
-        // version&lt;br/&gt;
-        buf.putInt(1);&lt;br/&gt;
-        // encode client UUID&lt;br/&gt;
-        buf.putLong(processId.getMostSignificantBits());&lt;br/&gt;
-        buf.putLong(processId.getLeastSignificantBits());&lt;br/&gt;
-        // encode ids of previously running tasks&lt;br/&gt;
-        buf.putInt(prevTasks.size());&lt;br/&gt;
-        for (TaskId id : prevTasks) {-            id.writeTo(buf);-        }&lt;/li&gt;
	&lt;li&gt;// encode ids of cached tasks&lt;/li&gt;
	&lt;li&gt;buf.putInt(standbyTasks.size());&lt;/li&gt;
	&lt;li&gt;for (TaskId id : standbyTasks) 
{
-            id.writeTo(buf);
-        }&lt;/li&gt;
	&lt;li&gt;buf.rewind();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;return buf;&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldEncodeAndDecodeVersion3() 
{
+        final SubscriptionInfo info = new SubscriptionInfo(3, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+        final SubscriptionInfo expectedInfo = new SubscriptionInfo(3, SubscriptionInfo.LATEST_SUPPORTED_VERSION, processId, activeTasks, standbyTasks, &quot;localhost:80&quot;);
+        assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/upgrade-system-tests-11/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/upgrade-system-tests-11/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..a8796cb056a&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/streams/upgrade-system-tests-11/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.tests;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.processor.AbstractProcessor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.Processor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+&lt;br/&gt;
+public class StreamsUpgradeTest {&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * This test cannot be executed, as long as Kafka 1.1.1 is not released&lt;br/&gt;
+     */&lt;br/&gt;
+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    public static void main(final String[] args) throws Exception {&lt;br/&gt;
+        if (args.length &amp;lt; 2) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;
+                + (args.length &amp;gt; 0 ? args[0] : &quot;&quot;));
+        }
&lt;p&gt;+        final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+&lt;br/&gt;
+        final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
+&lt;br/&gt;
+        System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest v1.1)&quot;);&lt;br/&gt;
+        System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
+        System.out.println(&quot;props=&quot; + streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+        final KStream dataStream = builder.stream(&quot;data&quot;);&lt;br/&gt;
+        dataStream.process(printProcessorSupplier());&lt;br/&gt;
+        dataStream.to(&quot;echo&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Properties config = new Properties();&lt;br/&gt;
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;br/&gt;
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+        config.putAll(streamsProperties);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);&lt;br/&gt;
+        streams.start();&lt;br/&gt;
+&lt;br/&gt;
+        Runtime.getRuntime().addShutdownHook(new Thread() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            @Override+            public void run() {
+                streams.close();
+                System.out.println(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;);
+                System.out.flush();
+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static &amp;lt;K, V&amp;gt; ProcessorSupplier&amp;lt;K, V&amp;gt; printProcessorSupplier() {&lt;br/&gt;
+        return new ProcessorSupplier&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+            public Processor&amp;lt;K, V&amp;gt; get() {&lt;br/&gt;
+                return new AbstractProcessor&amp;lt;K, V&amp;gt;() {&lt;br/&gt;
+                    private int numRecordsProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void init(final ProcessorContext context) &lt;/p&gt;
{
+                        System.out.println(&quot;initializing processor: topic=data taskId=&quot; + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void process(final K key, final V value) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        numRecordsProcessed++;+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(&quot;processed &quot; + numRecordsProcessed + &quot; records from topic=data&quot;);
+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void punctuate(final long timestamp) {}&lt;br/&gt;
+&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void close() {}&lt;br/&gt;
+                };&lt;br/&gt;
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index a5be816c737..e0e445de22a 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -413,6 +413,7 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context, kafka):&lt;br/&gt;
                                                                  &quot;org.apache.kafka.streams.tests.StreamsUpgradeTest&quot;,&lt;br/&gt;
                                                                  &quot;&quot;)&lt;br/&gt;
         self.UPGRADE_FROM = None&lt;br/&gt;
+        self.UPGRADE_TO = None&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def set_version(self, kafka_streams_version):&lt;br/&gt;
         self.KAFKA_STREAMS_VERSION = kafka_streams_version&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
index fa79d571f36..8b7d7712459 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -23,8 +23,16 @@&lt;br/&gt;
 import random&lt;br/&gt;
 import time&lt;/p&gt;

&lt;p&gt;+# broker 0.10.0 is not compatible with newer Kafka Streams versions&lt;br/&gt;
 broker_upgrade_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1), str(DEV_BRANCH)&amp;#93;&lt;/span&gt;&lt;br/&gt;
-simple_upgrade_versions_metadata_version_2 = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(DEV_VERSION)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+metadata_1_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_0)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+metadata_2_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+# we can add the following versions to `backward_compatible_metadata_2_versions` after the corresponding&lt;br/&gt;
+# bug-fix release 0.10.1.2, 0.10.2.2, 0.11.0.3, 1.0.2, and 1.1.1 are available:&lt;br/&gt;
+# str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1)&lt;br/&gt;
+backward_compatible_metadata_2_versions = []&lt;br/&gt;
+metadata_3_versions = &lt;span class=&quot;error&quot;&gt;&amp;#91;str(DEV_VERSION)&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; class StreamsUpgradeTest(Test):&lt;br/&gt;
     &quot;&quot;&quot;&lt;br/&gt;
@@ -39,6 +47,7 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
             &apos;echo&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
             &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
         }&lt;br/&gt;
+        self.leader = None&lt;/p&gt;

&lt;p&gt;     def perform_broker_upgrade(self, to_version):&lt;br/&gt;
         self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;br/&gt;
@@ -114,7 +123,7 @@ def test_upgrade_downgrade_brokers(self, from_version, to_version):&lt;br/&gt;
         node.account.ssh(&quot;grep ALL-RECORDS-DELIVERED %s&quot; % self.driver.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
         self.processor1.node.account.ssh_capture(&quot;grep SMOKE-TEST-CLIENT-CLOSED %s&quot; % self.processor1.STDOUT_FILE, allow_fail=False)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)&lt;br/&gt;
+    @matrix(from_version=metadata_2_versions, to_version=metadata_2_versions)&lt;br/&gt;
     def test_simple_upgrade_downgrade(self, from_version, to_version):&lt;br/&gt;
         &quot;&quot;&quot;&lt;br/&gt;
         Starts 3 KafkaStreams instances with &amp;lt;old_version&amp;gt;, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
@@ -165,15 +174,12 @@ def test_simple_upgrade_downgrade(self, from_version, to_version):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         self.driver.stop()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;#@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released&lt;/li&gt;
	&lt;li&gt;#@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released&lt;/li&gt;
	&lt;li&gt;#@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released&lt;/li&gt;
	&lt;li&gt;#@parametrize(new_version=str(LATEST_1_0)) we cannot run this test until Kafka 1.0.2 is released&lt;/li&gt;
	&lt;li&gt;#@parametrize(new_version=str(LATEST_1_1)) we cannot run this test until Kafka 1.1.1 is released&lt;/li&gt;
	&lt;li&gt;@parametrize(new_version=str(DEV_VERSION))&lt;/li&gt;
	&lt;li&gt;def test_metadata_upgrade(self, new_version):&lt;br/&gt;
+    #@matrix(from_version=metadata_1_versions, to_version=backward_compatible_metadata_2_versions)&lt;br/&gt;
+    @matrix(from_version=metadata_1_versions, to_version=metadata_3_versions)&lt;br/&gt;
+    @matrix(from_version=metadata_2_versions, to_version=metadata_3_versions)&lt;br/&gt;
+    def test_metadata_upgrade(self, from_version, to_version):&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
	&lt;li&gt;Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to &amp;lt;new_version&amp;gt;&lt;br/&gt;
+        Starts 3 KafkaStreams instances with version &amp;lt;from_version&amp;gt; and upgrades one-by-one to &amp;lt;to_version&amp;gt;&lt;br/&gt;
         &quot;&quot;&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
@@ -189,7 +195,7 @@ def test_metadata_upgrade(self, new_version):&lt;br/&gt;
         self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;/p&gt;

&lt;p&gt;         self.driver.start()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.start_all_nodes_with(str(LATEST_0_10_0))&lt;br/&gt;
+        self.start_all_nodes_with(from_version)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;@@ -200,13 +206,13 @@ def test_metadata_upgrade(self, new_version):&lt;br/&gt;
         random.shuffle(self.processors)&lt;br/&gt;
         for p in self.processors:&lt;br/&gt;
             p.CLEAN_NODE_ENABLED = False&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.do_rolling_bounce(p, &quot;0.10.0&quot;, new_version, counter)&lt;br/&gt;
+            self.do_rolling_bounce(p, from_version&lt;span class=&quot;error&quot;&gt;&amp;#91;:-2&amp;#93;&lt;/span&gt;, to_version, counter)&lt;br/&gt;
             counter = counter + 1&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;second rolling bounce&lt;br/&gt;
         random.shuffle(self.processors)&lt;br/&gt;
         for p in self.processors:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.do_rolling_bounce(p, None, new_version, counter)&lt;br/&gt;
+            self.do_rolling_bounce(p, None, to_version, counter)&lt;br/&gt;
             counter = counter + 1&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;shutdown&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index 66e5fcf18aa..7823efac1d4 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -63,17 +63,17 @@ def get_version(node=None):&lt;br/&gt;
 DEV_BRANCH = KafkaVersion(&quot;dev&quot;)&lt;br/&gt;
 DEV_VERSION = KafkaVersion(&quot;1.2.0-SNAPSHOT&quot;)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;0.8.2.X versions&lt;br/&gt;
+# 0.8.2.x versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;br/&gt;
 V_0_8_2_2 = KafkaVersion(&quot;0.8.2.2&quot;)&lt;br/&gt;
 LATEST_0_8_2 = V_0_8_2_2&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;0.9.0.X versions&lt;br/&gt;
+# 0.9.0.x versions&lt;br/&gt;
 V_0_9_0_0 = KafkaVersion(&quot;0.9.0.0&quot;)&lt;br/&gt;
 V_0_9_0_1 = KafkaVersion(&quot;0.9.0.1&quot;)&lt;br/&gt;
 LATEST_0_9 = V_0_9_0_1&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;0.10.0.X versions&lt;br/&gt;
+# 0.10.0.x versions&lt;br/&gt;
 V_0_10_0_0 = KafkaVersion(&quot;0.10.0.0&quot;)&lt;br/&gt;
 V_0_10_0_1 = KafkaVersion(&quot;0.10.0.1&quot;)&lt;br/&gt;
 LATEST_0_10_0 = V_0_10_0_1&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16496133" author="githubbot" created="Thu, 31 May 2018 05:39:45 +0000"  >&lt;p&gt;mjsax closed pull request #4636: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6054&quot; title=&quot;ERROR &amp;quot;SubscriptionInfo - unable to decode subscription data: version=2&amp;quot; when upgrading from 0.10.0.0 to 0.10.2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6054&quot;&gt;&lt;del&gt;KAFKA-6054&lt;/del&gt;&lt;/a&gt;: Add &apos;version probing&apos; to Kafka Streams rebalance&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4636&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4636&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index 18dc891682d..bc549960de4 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -145,6 +145,7 @@&lt;br/&gt;
      */&lt;br/&gt;
     // TODO: currently we cannot get the full topic configurations and hence cannot allow topic configs without the prefix,&lt;br/&gt;
     //       this can be lifted once kafka.log.LogConfig is completely deprecated by org.apache.kafka.common.config.TopicConfig&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String TOPIC_PREFIX = &quot;topic.&quot;;&lt;/p&gt;

&lt;p&gt;     /**&lt;br/&gt;
@@ -152,6 +153,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;It is recommended to use 
{@link #consumerPrefix(String)}
&lt;p&gt; to add this prefix to &lt;/p&gt;
{@link ConsumerConfig consumer
      * properties}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String CONSUMER_PREFIX = &quot;consumer.&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -161,6 +163,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;2. consumer.&lt;span class=&quot;error&quot;&gt;&amp;#91;config-name&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;3. &lt;span class=&quot;error&quot;&gt;&amp;#91;config-name&amp;#93;&lt;/span&gt;&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String MAIN_CONSUMER_PREFIX = &quot;main.consumer.&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -170,6 +173,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;2. consumer.&lt;span class=&quot;error&quot;&gt;&amp;#91;config-name&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;3. &lt;span class=&quot;error&quot;&gt;&amp;#91;config-name&amp;#93;&lt;/span&gt;&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String RESTORE_CONSUMER_PREFIX = &quot;restore.consumer.&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -179,6 +183,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;2. consumer.&lt;span class=&quot;error&quot;&gt;&amp;#91;config-name&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;3. &lt;span class=&quot;error&quot;&gt;&amp;#91;config-name&amp;#93;&lt;/span&gt;&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String GLOBAL_CONSUMER_PREFIX = &quot;global.consumer.&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -186,6 +191,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;It is recommended to use 
{@link #producerPrefix(String)}
&lt;p&gt; to add this prefix to &lt;/p&gt;
{@link ProducerConfig producer
      * properties}.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String PRODUCER_PREFIX = &quot;producer.&quot;;&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
@@ -193,202 +199,250 @@&lt;br/&gt;
      * It is recommended to use {@link #adminClientPrefix(String)} to add this prefix to {@link ProducerConfig producer      * properties}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String ADMIN_CLIENT_PREFIX = &quot;admin.&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;} for upgrading an application from version {@code 0.10.0.x}.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String UPGRADE_FROM_0100 = &quot;0.10.0&quot;;&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.10.1.x}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String UPGRADE_FROM_0101 = &quot;0.10.1&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;} for upgrading an application from version {@code 0.10.2.x}.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String UPGRADE_FROM_0102 = &quot;0.10.2&quot;;&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 0.11.0.x}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String UPGRADE_FROM_0110 = &quot;0.11.0&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;} for upgrading an application from version {@code 1.0.x}.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String UPGRADE_FROM_10 = &quot;1.0&quot;;&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG &quot;upgrade.from&quot;}
&lt;p&gt; for upgrading an application from version &lt;/p&gt;
{@code 1.1.x}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String UPGRADE_FROM_11 = &quot;1.1&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Config value for parameter 
{@link #PROCESSING_GUARANTEE_CONFIG &quot;processing.guarantee&quot;} for at-least-once processing guarantees.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String AT_LEAST_ONCE = &quot;at_least_once&quot;;&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG &quot;processing.guarantee&quot;}
&lt;p&gt; for exactly-once processing guarantees.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String EXACTLY_ONCE = &quot;exactly_once&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /** &lt;/p&gt;
{@code application.id}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String APPLICATION_ID_CONFIG = &quot;application.id&quot;;&lt;br/&gt;
     private static final String APPLICATION_ID_DOC = &quot;An identifier for the stream processing application. Must be unique within the Kafka cluster. It is used as 1) the default client-id prefix, 2) the group-id for membership management, 3) the changelog topic prefix.&quot;;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
{@code user.endpoint}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String APPLICATION_SERVER_CONFIG = &quot;application.server&quot;;&lt;br/&gt;
     private static final String APPLICATION_SERVER_DOC = &quot;A host:port pair pointing to an embedded user defined endpoint that can be used for discovering the locations of state stores within a single KafkaStreams application&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code bootstrap.servers}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String BOOTSTRAP_SERVERS_CONFIG = CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code buffered.records.per.partition}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String BUFFERED_RECORDS_PER_PARTITION_CONFIG = &quot;buffered.records.per.partition&quot;;&lt;br/&gt;
     private static final String BUFFERED_RECORDS_PER_PARTITION_DOC = &quot;The maximum number of records to buffer per partition.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code cache.max.bytes.buffering}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String CACHE_MAX_BYTES_BUFFERING_CONFIG = &quot;cache.max.bytes.buffering&quot;;&lt;br/&gt;
     private static final String CACHE_MAX_BYTES_BUFFERING_DOC = &quot;Maximum number of memory bytes to be used for buffering across all threads&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code client.id}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String CLIENT_ID_CONFIG = CommonClientConfigs.CLIENT_ID_CONFIG;&lt;br/&gt;
     private static final String CLIENT_ID_DOC = &quot;An ID prefix string used for the client IDs of internal consumer, producer and restore-consumer,&quot; +&lt;br/&gt;
         &quot; with pattern &apos;&amp;lt;client.id&amp;gt;&lt;del&gt;StreamThread&lt;/del&gt;&amp;lt;threadSequenceNumber&amp;gt;-&amp;lt;consumer|producer|restore-consumer&amp;gt;&apos;.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code commit.interval.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String COMMIT_INTERVAL_MS_CONFIG = &quot;commit.interval.ms&quot;;&lt;br/&gt;
     private static final String COMMIT_INTERVAL_MS_DOC = &quot;The frequency with which to save the position of the processor.&quot; +&lt;br/&gt;
         &quot; (Note, if &apos;processing.guarantee&apos; is set to &apos;&quot; + EXACTLY_ONCE + &quot;&apos;, the default value is &quot; + EOS_DEFAULT_COMMIT_INTERVAL_MS + &quot;,&quot; +&lt;br/&gt;
         &quot; otherwise the default value is &quot; + DEFAULT_COMMIT_INTERVAL_MS + &quot;.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code connections.max.idle.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = CommonClientConfigs.CONNECTIONS_MAX_IDLE_MS_CONFIG;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code default.deserialization.exception.handler}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG = &quot;default.deserialization.exception.handler&quot;;&lt;br/&gt;
     private static final String DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_DOC = &quot;Exception handling class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.errors.DeserializationExceptionHandler&amp;lt;/code&amp;gt; interface.&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code default.production.exception.handler}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG = &quot;default.production.exception.handler&quot;;&lt;br/&gt;
     private static final String DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_DOC = &quot;Exception handling class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.errors.ProductionExceptionHandler&amp;lt;/code&amp;gt; interface.&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code default.windowed.key.serde.inner}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS = &quot;default.windowed.key.serde.inner&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;{@code default.windowed.value.serde.inner}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS = &quot;default.windowed.value.serde.inner&quot;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /** &lt;/p&gt;
{@code default key.serde}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String DEFAULT_KEY_SERDE_CLASS_CONFIG = &quot;default.key.serde&quot;;&lt;br/&gt;
     private static final String DEFAULT_KEY_SERDE_CLASS_DOC = &quot; Default serializer / deserializer class for key that implements the &amp;lt;code&amp;gt;org.apache.kafka.common.serialization.Serde&amp;lt;/code&amp;gt; interface. &quot;&lt;br/&gt;
             + &quot;Note when windowed serde class is used, one needs to set the inner serde class that implements the &amp;lt;code&amp;gt;org.apache.kafka.common.serialization.Serde&amp;lt;/code&amp;gt; interface via &apos;&quot;&lt;br/&gt;
             + DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS + &quot;&apos; or &apos;&quot; + DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS + &quot;&apos; as well&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code default value.serde}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String DEFAULT_VALUE_SERDE_CLASS_CONFIG = &quot;default.value.serde&quot;;&lt;br/&gt;
     private static final String DEFAULT_VALUE_SERDE_CLASS_DOC = &quot;Default serializer / deserializer class for value that implements the &amp;lt;code&amp;gt;org.apache.kafka.common.serialization.Serde&amp;lt;/code&amp;gt; interface. &quot;&lt;br/&gt;
             + &quot;Note when windowed serde class is used, one needs to set the inner serde class that implements the &amp;lt;code&amp;gt;org.apache.kafka.common.serialization.Serde&amp;lt;/code&amp;gt; interface via &apos;&quot;&lt;br/&gt;
             + DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS + &quot;&apos; or &apos;&quot; + DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS + &quot;&apos; as well&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code default.timestamp.extractor}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG = &quot;default.timestamp.extractor&quot;;&lt;br/&gt;
     private static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_DOC = &quot;Default timestamp extractor class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.processor.TimestampExtractor&amp;lt;/code&amp;gt; interface.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code metadata.max.age.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String METADATA_MAX_AGE_CONFIG = CommonClientConfigs.METADATA_MAX_AGE_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code metrics.num.samples}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String METRICS_NUM_SAMPLES_CONFIG = CommonClientConfigs.METRICS_NUM_SAMPLES_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code metrics.record.level}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String METRICS_RECORDING_LEVEL_CONFIG = CommonClientConfigs.METRICS_RECORDING_LEVEL_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code metric.reporters}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String METRIC_REPORTER_CLASSES_CONFIG = CommonClientConfigs.METRIC_REPORTER_CLASSES_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code metrics.sample.window.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code num.standby.replicas}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String NUM_STANDBY_REPLICAS_CONFIG = &quot;num.standby.replicas&quot;;&lt;br/&gt;
     private static final String NUM_STANDBY_REPLICAS_DOC = &quot;The number of standby replicas for each task.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code num.stream.threads}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String NUM_STREAM_THREADS_CONFIG = &quot;num.stream.threads&quot;;&lt;br/&gt;
     private static final String NUM_STREAM_THREADS_DOC = &quot;The number of threads to execute stream processing.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code partition.grouper}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String PARTITION_GROUPER_CLASS_CONFIG = &quot;partition.grouper&quot;;&lt;br/&gt;
     private static final String PARTITION_GROUPER_CLASS_DOC = &quot;Partition grouper class that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.processor.PartitionGrouper&amp;lt;/code&amp;gt; interface.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code poll.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String POLL_MS_CONFIG = &quot;poll.ms&quot;;&lt;br/&gt;
     private static final String POLL_MS_DOC = &quot;The amount of time in milliseconds to block waiting for input.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code processing.guarantee}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String PROCESSING_GUARANTEE_CONFIG = &quot;processing.guarantee&quot;;&lt;br/&gt;
     private static final String PROCESSING_GUARANTEE_DOC = &quot;The processing guarantee that should be used. Possible values are &amp;lt;code&amp;gt;&quot; + AT_LEAST_ONCE + &quot;&amp;lt;/code&amp;gt; (default) and &amp;lt;code&amp;gt;&quot; + EXACTLY_ONCE + &quot;&amp;lt;/code&amp;gt;. &quot; +&lt;br/&gt;
         &quot;Note that exactly-once processing requires a cluster of at least three brokers by default what is the recommended setting for production; for development you can change this, by adjusting broker setting `transaction.state.log.replication.factor`.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code receive.buffer.bytes}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String RECEIVE_BUFFER_CONFIG = CommonClientConfigs.RECEIVE_BUFFER_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code reconnect.backoff.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String RECONNECT_BACKOFF_MS_CONFIG = CommonClientConfigs.RECONNECT_BACKOFF_MS_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code reconnect.backoff.max}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = CommonClientConfigs.RECONNECT_BACKOFF_MAX_MS_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code replication.factor}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String REPLICATION_FACTOR_CONFIG = &quot;replication.factor&quot;;&lt;br/&gt;
     private static final String REPLICATION_FACTOR_DOC = &quot;The replication factor for change log topics and repartition topics created by the stream processing application.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code request.timeout.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String REQUEST_TIMEOUT_MS_CONFIG = CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code retries}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String RETRIES_CONFIG = CommonClientConfigs.RETRIES_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code retry.backoff.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String RETRY_BACKOFF_MS_CONFIG = CommonClientConfigs.RETRY_BACKOFF_MS_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code rocksdb.config.setter}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String ROCKSDB_CONFIG_SETTER_CLASS_CONFIG = &quot;rocksdb.config.setter&quot;;&lt;br/&gt;
     private static final String ROCKSDB_CONFIG_SETTER_CLASS_DOC = &quot;A Rocks DB config setter class or class name that implements the &amp;lt;code&amp;gt;org.apache.kafka.streams.state.RocksDBConfigSetter&amp;lt;/code&amp;gt; interface&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code security.protocol}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String SECURITY_PROTOCOL_CONFIG = CommonClientConfigs.SECURITY_PROTOCOL_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code send.buffer.bytes}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String SEND_BUFFER_CONFIG = CommonClientConfigs.SEND_BUFFER_CONFIG;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code state.cleanup.delay}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String STATE_CLEANUP_DELAY_MS_CONFIG = &quot;state.cleanup.delay.ms&quot;;&lt;br/&gt;
     private static final String STATE_CLEANUP_DELAY_MS_DOC = &quot;The amount of time in milliseconds to wait before deleting state when a partition has migrated. Only state directories that have not been modified for at least state.cleanup.delay.ms will be removed&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code state.dir}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String STATE_DIR_CONFIG = &quot;state.dir&quot;;&lt;br/&gt;
     private static final String STATE_DIR_DOC = &quot;Directory location for state store.&quot;;&lt;/p&gt;

&lt;p&gt;     /** &lt;/p&gt;
{@code upgrade.from}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String UPGRADE_FROM_CONFIG = &quot;upgrade.from&quot;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from versions 0.10.0/0.10.1/0.10.2/0.11.0/1.0/1.1 to version 1.2 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
+    private static final String UPGRADE_FROM_DOC = &quot;Allows upgrading from versions 0.10.0/0.10.1/0.10.2/0.11.0/1.0/1.1 to version 1.2 (or newer) in a backward compatible way. &quot; +&lt;br/&gt;
         &quot;When upgrading from 1.2 to a newer version it is not required to specify this config.&quot; +&lt;br/&gt;
         &quot;Default is null. Accepted values are \&quot;&quot; + UPGRADE_FROM_0100 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_0101 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_0102 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_0110 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_10 + &quot;\&quot;, \&quot;&quot; + UPGRADE_FROM_11 + &quot;\&quot; (for upgrading from the corresponding old version).&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /** &lt;/p&gt;
{@code windowstore.changelog.additional.retention.ms}
&lt;p&gt; */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG = &quot;windowstore.changelog.additional.retention.ms&quot;;&lt;br/&gt;
     private static final String WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_DOC = &quot;Added to a windows maintainMs to ensure data is not deleted from the log prematurely. Allows for clock drift. Default is 1 day&quot;;&lt;/p&gt;

&lt;p&gt;@@ -653,6 +707,7 @@&lt;/p&gt;

&lt;p&gt;     public static class InternalConfig &lt;/p&gt;
{
         public static final String TASK_MANAGER_FOR_PARTITION_ASSIGNOR = &quot;__task.manager.instance__&quot;;
+        public static final String VERSION_PROBING_FLAG = &quot;__version.probing.flag__&quot;;
     }

&lt;p&gt;     /**&lt;br/&gt;
@@ -662,6 +717,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param consumerProp the consumer property to be masked&lt;/li&gt;
	&lt;li&gt;@return 
{@link #CONSUMER_PREFIX}
&lt;p&gt; + &lt;/p&gt;
{@code consumerProp}&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static String consumerPrefix(final String consumerProp) {
         return CONSUMER_PREFIX + consumerProp;
     }&lt;br/&gt;
@@ -673,6 +729,7 @@ public static String consumerPrefix(final String consumerProp) {&lt;br/&gt;
      * @param consumerProp the consumer property to be masked&lt;br/&gt;
      * @return {@link #MAIN_CONSUMER_PREFIX} + {@code consumerProp}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static String mainConsumerPrefix(final String consumerProp) &lt;/p&gt;
{
         return MAIN_CONSUMER_PREFIX + consumerProp;
     }
&lt;p&gt;@@ -684,6 +741,7 @@ public static String mainConsumerPrefix(final String consumerProp) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@param consumerProp the consumer property to be masked&lt;/li&gt;
	&lt;li&gt;@return 
{@link #RESTORE_CONSUMER_PREFIX}
&lt;p&gt; + &lt;/p&gt;
{@code consumerProp}&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static String restoreConsumerPrefix(final String consumerProp) {
         return RESTORE_CONSUMER_PREFIX + consumerProp;
     }&lt;br/&gt;
@@ -695,6 +753,7 @@ public static String restoreConsumerPrefix(final String consumerProp) {&lt;br/&gt;
      * @param consumerProp the consumer property to be masked&lt;br/&gt;
      * @return {@link #GLOBAL_CONSUMER_PREFIX} + {@code consumerProp}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static String globalConsumerPrefix(final String consumerProp) &lt;/p&gt;
{
         return GLOBAL_CONSUMER_PREFIX + consumerProp;
     }
&lt;p&gt;@@ -706,6 +765,7 @@ public static String globalConsumerPrefix(final String consumerProp) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@param producerProp the producer property to be masked&lt;/li&gt;
	&lt;li&gt;@return PRODUCER_PREFIX + 
{@code producerProp}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static String producerPrefix(final String producerProp) &lt;/p&gt;
{
         return PRODUCER_PREFIX + producerProp;
     }
&lt;p&gt;@@ -717,6 +777,7 @@ public static String producerPrefix(final String producerProp) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@param adminClientProp the admin client property to be masked&lt;/li&gt;
	&lt;li&gt;@return ADMIN_CLIENT_PREFIX + 
{@code adminClientProp}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static String adminClientPrefix(final String adminClientProp) &lt;/p&gt;
{
         return ADMIN_CLIENT_PREFIX + adminClientProp;
     }
&lt;p&gt;@@ -728,6 +789,7 @@ public static String adminClientPrefix(final String adminClientProp) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@param topicProp the topic property to be masked&lt;/li&gt;
	&lt;li&gt;@return TOPIC_PREFIX + 
{@code topicProp}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static String topicPrefix(final String topicProp) &lt;/p&gt;
{
         return TOPIC_PREFIX + topicProp;
     }
&lt;p&gt;@@ -737,6 +799,7 @@ public static String topicPrefix(final String topicProp) {&lt;br/&gt;
      *&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@return a copy of the config definition&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;unused&quot;)&lt;br/&gt;
     public static ConfigDef configDef() 
{
         return new ConfigDef(CONFIG);
     }
&lt;p&gt;@@ -788,8 +851,8 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;br/&gt;
         // consumer/producer configurations, log a warning and remove the user defined value from the Map.&lt;br/&gt;
         // Thus the default values for these consumer/producer configurations that are suitable for&lt;br/&gt;
         // Streams will be used instead.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Object maxInflightRequests = clientProvidedProps.get(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);&lt;/li&gt;
	&lt;li&gt;if (eosEnabled &amp;amp;&amp;amp; maxInflightRequests != null &amp;amp;&amp;amp; 5 &amp;lt; (int) maxInflightRequests) {&lt;br/&gt;
+        final Object maxInFlightRequests = clientProvidedProps.get(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);&lt;br/&gt;
+        if (eosEnabled &amp;amp;&amp;amp; maxInFlightRequests != null &amp;amp;&amp;amp; 5 &amp;lt; (int) maxInFlightRequests) 
{
             throw new ConfigException(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION + &quot; can&apos;t exceed 5 when using the idempotent producer&quot;);
         }
&lt;p&gt;         for (final String config: nonConfigurableConfigs) {&lt;br/&gt;
@@ -831,8 +894,9 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param groupId      consumer groupId&lt;/li&gt;
	&lt;li&gt;@param clientId     clientId&lt;/li&gt;
	&lt;li&gt;@return Map of the consumer configuration.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @Deprecated use 
{@link StreamsConfig#getMainConsumerConfigs(String, String)}&lt;br/&gt;
+     * @deprecated use {@link StreamsConfig#getMainConsumerConfigs(String, String)}
&lt;p&gt;      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     @Deprecated&lt;br/&gt;
     public Map&amp;lt;String, Object&amp;gt; getConsumerConfigs(final String groupId,&lt;br/&gt;
                                                   final String clientId) {&lt;br/&gt;
@@ -853,13 +917,14 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param clientId     clientId&lt;/li&gt;
	&lt;li&gt;@return Map of the consumer configuration.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public Map&amp;lt;String, Object&amp;gt; getMainConsumerConfigs(final String groupId,&lt;br/&gt;
                                                       final String clientId) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; consumerProps = getCommonConsumerConfigs();&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; consumerProps = getCommonConsumerConfigs();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // Get main consumer override configs&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; mainConsumerProps = originalsWithPrefix(MAIN_CONSUMER_PREFIX);&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, Object&amp;gt; entry: mainConsumerProps.entrySet()) {&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; mainConsumerProps = originalsWithPrefix(MAIN_CONSUMER_PREFIX);&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, Object&amp;gt; entry: mainConsumerProps.entrySet()) 
{
             consumerProps.put(entry.getKey(), entry.getValue());
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -919,12 +984,13 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param clientId clientId&lt;/li&gt;
	&lt;li&gt;@return Map of the restore consumer configuration.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public Map&amp;lt;String, Object&amp;gt; getRestoreConsumerConfigs(final String clientId) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; baseConsumerProps = getCommonConsumerConfigs();&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; baseConsumerProps = getCommonConsumerConfigs();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // Get restore consumer override configs&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; restoreConsumerProps = originalsWithPrefix(RESTORE_CONSUMER_PREFIX);&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, Object&amp;gt; entry: restoreConsumerProps.entrySet()) {&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; restoreConsumerProps = originalsWithPrefix(RESTORE_CONSUMER_PREFIX);&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, Object&amp;gt; entry: restoreConsumerProps.entrySet()) 
{
             baseConsumerProps.put(entry.getKey(), entry.getValue());
         }&lt;br/&gt;
 &lt;br/&gt;
@@ -950,12 +1016,13 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;br/&gt;
      * @param clientId clientId&lt;br/&gt;
      * @return Map of the global consumer configuration.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public Map&amp;lt;String, Object&amp;gt; getGlobalConsumerConfigs(final String clientId) {&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; baseConsumerProps = getCommonConsumerConfigs();&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; baseConsumerProps = getCommonConsumerConfigs();&lt;br/&gt;
 &lt;br/&gt;
         // Get global consumer override configs&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; globalConsumerProps = originalsWithPrefix(GLOBAL_CONSUMER_PREFIX);&lt;br/&gt;
-        for (Map.Entry&amp;lt;String, Object&amp;gt; entry: globalConsumerProps.entrySet()) {&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; globalConsumerProps = originalsWithPrefix(GLOBAL_CONSUMER_PREFIX);&lt;br/&gt;
+        for (final Map.Entry&amp;lt;String, Object&amp;gt; entry: globalConsumerProps.entrySet()) {             baseConsumerProps.put(entry.getKey(), entry.getValue());         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -977,6 +1044,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param clientId clientId&lt;/li&gt;
	&lt;li&gt;@return Map of the producer configuration.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public Map&amp;lt;String, Object&amp;gt; getProducerConfigs(final String clientId) {&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; clientProvidedProps = getClientPropsWithPrefix(PRODUCER_PREFIX, ProducerConfig.configNames());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -999,6 +1067,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param clientId clientId&lt;/li&gt;
	&lt;li&gt;@return Map of the admin client configuration.&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public Map&amp;lt;String, Object&amp;gt; getAdminConfigs(final String clientId) {&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; clientProvidedProps = getClientPropsWithPrefix(ADMIN_CLIENT_PREFIX, AdminClientConfig.configNames());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1045,10 +1114,11 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map&amp;lt;String, Obje&lt;br/&gt;
      *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return an configured instance of key Serde class&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public Serde defaultKeySerde() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object keySerdeConfigSetting = get(DEFAULT_KEY_SERDE_CLASS_CONFIG);&lt;br/&gt;
+        final Object keySerdeConfigSetting = get(DEFAULT_KEY_SERDE_CLASS_CONFIG);&lt;br/&gt;
         try 
{
-            Serde&amp;lt;?&amp;gt; serde = getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);
+            final Serde&amp;lt;?&amp;gt; serde = getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);
             serde.configure(originals(), true);
             return serde;
         }
&lt;p&gt; catch (final Exception e) {&lt;br/&gt;
@@ -1063,10 +1133,11 @@ public Serde defaultKeySerde() {&lt;br/&gt;
      *&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return an configured instance of value Serde class&lt;br/&gt;
      */&lt;br/&gt;
+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public Serde defaultValueSerde() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object valueSerdeConfigSetting = get(DEFAULT_VALUE_SERDE_CLASS_CONFIG);&lt;br/&gt;
+        final Object valueSerdeConfigSetting = get(DEFAULT_VALUE_SERDE_CLASS_CONFIG);&lt;br/&gt;
         try 
{
-            Serde&amp;lt;?&amp;gt; serde = getConfiguredInstance(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serde.class);
+            final Serde&amp;lt;?&amp;gt; serde = getConfiguredInstance(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serde.class);
             serde.configure(originals(), false);
             return serde;
         }
&lt;p&gt; catch (final Exception e) {&lt;br/&gt;
@@ -1075,14 +1146,17 @@ public Serde defaultValueSerde() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public TimestampExtractor defaultTimestampExtractor() &lt;/p&gt;
{
         return getConfiguredInstance(DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, TimestampExtractor.class);
     }

&lt;p&gt;+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public DeserializationExceptionHandler defaultDeserializationExceptionHandler() &lt;/p&gt;
{
         return getConfiguredInstance(DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, DeserializationExceptionHandler.class);
     }

&lt;p&gt;+    @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public ProductionExceptionHandler defaultProductionExceptionHandler() &lt;/p&gt;
{
         return getConfiguredInstance(DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, ProductionExceptionHandler.class);
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
index 6f4f454bfc4..079d405cb50 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
@@ -40,15 +40,15 @@&lt;br/&gt;
     private final Logger log;&lt;br/&gt;
     private final String taskTypeName;&lt;br/&gt;
     private final TaskAction&amp;lt;T&amp;gt; commitAction;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Map&amp;lt;TaskId, T&amp;gt; created = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Map&amp;lt;TaskId, T&amp;gt; suspended = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Map&amp;lt;TaskId, T&amp;gt; restoring = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Set&amp;lt;TopicPartition&amp;gt; restoredPartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Set&amp;lt;TaskId&amp;gt; previousActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Map&amp;lt;TaskId, T&amp;gt; created = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Map&amp;lt;TaskId, T&amp;gt; suspended = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Map&amp;lt;TaskId, T&amp;gt; restoring = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Set&amp;lt;TopicPartition&amp;gt; restoredPartitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
     // IQ may access this map.&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;TaskId, T&amp;gt; running = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;private Map&amp;lt;TopicPartition, T&amp;gt; runningByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, T&amp;gt; restoringByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    final Map&amp;lt;TaskId, T&amp;gt; running = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final Map&amp;lt;TopicPartition, T&amp;gt; runningByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    final Map&amp;lt;TopicPartition, T&amp;gt; restoringByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     AssignedTasks(final LogContext logContext,&lt;br/&gt;
                   final String taskTypeName) {&lt;br/&gt;
@@ -176,7 +176,7 @@ private RuntimeException closeNonRunningTasks(final Collection&amp;lt;T&amp;gt; tasks) {&lt;/p&gt;

&lt;p&gt;     private RuntimeException suspendTasks(final Collection&amp;lt;T&amp;gt; tasks) {&lt;br/&gt;
         final AtomicReference&amp;lt;RuntimeException&amp;gt; firstException = new AtomicReference&amp;lt;&amp;gt;(null);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Iterator&amp;lt;T&amp;gt; it = tasks.iterator(); it.hasNext(); ) {&lt;br/&gt;
+        for (final Iterator&amp;lt;T&amp;gt; it = tasks.iterator(); it.hasNext(); ) {&lt;br/&gt;
             final T task = it.next();&lt;br/&gt;
             try {&lt;br/&gt;
                 task.suspend();&lt;br/&gt;
@@ -249,10 +249,10 @@ boolean maybeResumeSuspendedTask(final TaskId taskId, final Set&amp;lt;TopicPartition&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private void addToRestoring(final T task) {&lt;br/&gt;
         restoring.put(task.id(), task);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (TopicPartition topicPartition : task.partitions()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        for (final TopicPartition topicPartition }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -264,10 +264,10 @@ private void transitionToRunning(final T task) {&lt;br/&gt;
         log.debug(&quot;transitioning {} {} to running&quot;, taskTypeName, task.id());&lt;br/&gt;
         running.put(task.id(), task);&lt;br/&gt;
         task.initializeTopology();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;for (TopicPartition topicPartition : task.partitions()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        for (final TopicPartition topicPartition }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -356,7 +356,7 @@ int commit() {&lt;br/&gt;
     void applyToRunningTasks(final TaskAction&amp;lt;T&amp;gt; action) {&lt;br/&gt;
         RuntimeException firstException = null;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Iterator&amp;lt;T&amp;gt; it = running().iterator(); it.hasNext(); ) {&lt;br/&gt;
+        for (final Iterator&amp;lt;T&amp;gt; it = running().iterator(); it.hasNext(); ) {&lt;br/&gt;
             final T task = it.next();&lt;br/&gt;
             try {&lt;br/&gt;
                 action.apply(task);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index 3080d2e1583..e72c4a5de94 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -62,6 +62,7 @@&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
 import java.util.concurrent.TimeUnit;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicBoolean;&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicInteger;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static java.util.Collections.singleton;&lt;br/&gt;
@@ -69,7 +70,7 @@&lt;br/&gt;
 public class StreamThread extends Thread {&lt;/p&gt;

&lt;p&gt;     private final static int UNLIMITED_RECORDS = -1;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final AtomicInteger STREAM_THREAD_ID_SEQUENCE = new AtomicInteger(1);&lt;br/&gt;
+    private final static AtomicInteger STREAM_THREAD_ID_SEQUENCE = new AtomicInteger(1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Stream thread states are the possible states that a stream thread can be in.&lt;br/&gt;
@@ -264,7 +265,9 @@ public void onPartitionsAssigned(final Collection&amp;lt;TopicPartition&amp;gt; assignment) {&lt;br/&gt;
                 if (streamThread.setState(State.PARTITIONS_ASSIGNED) == null) 
{
                     return;
                 }&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;taskManager.createTasks(assignment);&lt;br/&gt;
+                if (!streamThread.versionProbingFlag.get()) 
{
+                    taskManager.createTasks(assignment);
+                }
&lt;p&gt;             } catch (final Throwable t) {&lt;br/&gt;
                 log.error(&lt;br/&gt;
                     &quot;Error caught during partition assignment, &quot; +&lt;br/&gt;
@@ -298,7 +301,11 @@ public void onPartitionsRevoked(final Collection&amp;lt;TopicPartition&amp;gt; assignment) {&lt;br/&gt;
                 final long start = time.milliseconds();&lt;br/&gt;
                 try {&lt;br/&gt;
                     // suspend active tasks&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;taskManager.suspendTasksAndState();&lt;br/&gt;
+                    if (streamThread.versionProbingFlag.get()) 
{
+                        streamThread.versionProbingFlag.set(false);
+                    }
&lt;p&gt; else &lt;/p&gt;
{
+                        taskManager.suspendTasksAndState();
+                    }
&lt;p&gt;                 } catch (final Throwable t) {&lt;br/&gt;
                     log.error(&lt;br/&gt;
                         &quot;Error caught during partition revocation, &quot; +&lt;br/&gt;
@@ -555,6 +562,7 @@ StandbyTask createTask(final Consumer&amp;lt;byte[], byte[]&amp;gt; consumer,&lt;br/&gt;
     private final String logPrefix;&lt;br/&gt;
     private final TaskManager taskManager;&lt;br/&gt;
     private final StreamsMetricsThreadImpl streamsMetrics;&lt;br/&gt;
+    private final AtomicBoolean versionProbingFlag;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private long lastCommitMs;&lt;br/&gt;
     private long timerStartedMs;&lt;br/&gt;
@@ -647,6 +655,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,&lt;br/&gt;
         final String applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);&lt;br/&gt;
         final Map&amp;lt;String, Object&amp;gt; consumerConfigs = config.getMainConsumerConfigs(applicationId, threadClientId);&lt;br/&gt;
         consumerConfigs.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);&lt;br/&gt;
+        final AtomicBoolean versionProbingFlag = new AtomicBoolean();&lt;br/&gt;
+        consumerConfigs.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, versionProbingFlag);&lt;br/&gt;
         String originalReset = null;&lt;br/&gt;
         if (!builder.latestResetTopicsPattern().pattern().equals(&quot;&quot;) || !builder.earliestResetTopicsPattern().pattern().equals(&quot;&quot;)) &lt;/p&gt;
{
             originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);
@@ -666,7 +676,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,
             streamsMetrics,
             builder,
             threadClientId,
-            logContext);
+            logContext,
+            versionProbingFlag);
     }

&lt;p&gt;     public StreamThread(final Time time,&lt;br/&gt;
@@ -679,7 +690,8 @@ public StreamThread(final Time time,&lt;br/&gt;
                         final StreamsMetricsThreadImpl streamsMetrics,&lt;br/&gt;
                         final InternalTopologyBuilder builder,&lt;br/&gt;
                         final String threadClientId,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final LogContext logContext) {&lt;br/&gt;
+                        final LogContext logContext,&lt;br/&gt;
+                        final AtomicBoolean versionProbingFlag) {&lt;br/&gt;
         super(threadClientId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         this.stateLock = new Object();&lt;br/&gt;
@@ -696,6 +708,7 @@ public StreamThread(final Time time,&lt;br/&gt;
         this.restoreConsumer = restoreConsumer;&lt;br/&gt;
         this.consumer = consumer;&lt;br/&gt;
         this.originalReset = originalReset;&lt;br/&gt;
+        this.versionProbingFlag = versionProbingFlag;&lt;/p&gt;

&lt;p&gt;         this.pollTimeMs = config.getLong(StreamsConfig.POLL_MS_CONFIG);&lt;br/&gt;
         this.commitTimeMs = config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG);&lt;br/&gt;
@@ -750,19 +763,26 @@ private void runLoop() {&lt;br/&gt;
         while (isRunning()) {&lt;br/&gt;
             try {&lt;br/&gt;
                 recordsProcessedBeforeCommit = runOnce(recordsProcessedBeforeCommit);&lt;br/&gt;
+                if (versionProbingFlag.get()) &lt;/p&gt;
{
+                    log.info(&quot;Version probing detected. Triggering new rebalance.&quot;);
+                    enforceRebalance();
+                }
&lt;p&gt;             } catch (final TaskMigratedException ignoreAndRejoinGroup) {&lt;br/&gt;
                 log.warn(&quot;Detected task {} that got migrated to another thread. &quot; +&lt;br/&gt;
                         &quot;This implies that this thread missed a rebalance and dropped out of the consumer group. &quot; +&lt;br/&gt;
                         &quot;Will try to rejoin the consumer group. Below is the detailed description of the task:\n{}&quot;,&lt;br/&gt;
                     ignoreAndRejoinGroup.migratedTask().id(), ignoreAndRejoinGroup.migratedTask().toString(&quot;&amp;gt;&quot;));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// re-subscribe to enforce a rebalance in the next poll call&lt;/li&gt;
	&lt;li&gt;consumer.unsubscribe();&lt;/li&gt;
	&lt;li&gt;consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);&lt;br/&gt;
+                enforceRebalance();&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private void enforceRebalance() &lt;/p&gt;
{
+        consumer.unsubscribe();
+        consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws IllegalStateException If store gets registered after initialized is already finished&lt;/li&gt;
	&lt;li&gt;@throws StreamsException      If the store&apos;s change log does not contain the partition&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
index e1464e6b72c..db94ac0c852 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java&lt;br/&gt;
@@ -51,6 +51,7 @@&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicBoolean;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static org.apache.kafka.common.utils.Utils.getHost;&lt;br/&gt;
 import static org.apache.kafka.common.utils.Utils.getPort;&lt;br/&gt;
@@ -59,6 +60,12 @@&lt;/p&gt;

&lt;p&gt;     private final static int UNKNOWN = -1;&lt;br/&gt;
     public final static int NOT_AVAILABLE = -2;&lt;br/&gt;
+    private final static int VERSION_ONE = 1;&lt;br/&gt;
+    private final static int VERSION_TWO = 2;&lt;br/&gt;
+    private final static int VERSION_THREE = 3;&lt;br/&gt;
+    private final static int EARLIEST_PROBEABLE_VERSION = VERSION_THREE;&lt;br/&gt;
+    private int minReceivedMetadataVersion = UNKNOWN;&lt;br/&gt;
+    protected Set&amp;lt;Integer&amp;gt; supportedVersions = new HashSet&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;     private Logger log;&lt;br/&gt;
     private String logPrefix;&lt;br/&gt;
@@ -159,7 +166,7 @@ public String toString() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Comparator&amp;lt;TopicPartition&amp;gt; PARTITION_COMPARATOR = new Comparator&amp;lt;TopicPartition&amp;gt;() {&lt;br/&gt;
+    protected static final Comparator&amp;lt;TopicPartition&amp;gt; PARTITION_COMPARATOR = new Comparator&amp;lt;TopicPartition&amp;gt;() {&lt;br/&gt;
         @Override&lt;br/&gt;
         public int compare(final TopicPartition p1,&lt;br/&gt;
                            final TopicPartition p2) {&lt;br/&gt;
@@ -178,12 +185,21 @@ public int compare(final TopicPartition p1,&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private TaskManager taskManager;&lt;br/&gt;
     private PartitionGrouper partitionGrouper;&lt;br/&gt;
+    private AtomicBoolean versionProbingFlag;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private int userMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;&lt;br/&gt;
+    protected int usedSubscriptionMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private InternalTopicManager internalTopicManager;&lt;br/&gt;
     private CopartitionedTopicsValidator copartitionedTopicsValidator;&lt;/p&gt;

&lt;p&gt;+    protected String userEndPoint() &lt;/p&gt;
{
+        return userEndPoint;
+    }
&lt;p&gt;+&lt;br/&gt;
+    protected TaskManager taskManger() &lt;/p&gt;
{
+        return taskManager;
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;We need to have the PartitionAssignor and its StreamThread to be mutually accessible&lt;/li&gt;
	&lt;li&gt;since the former needs later&apos;s cached metadata while sending subscriptions,&lt;br/&gt;
@@ -204,15 +220,15 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;br/&gt;
             switch (upgradeFrom) {&lt;br/&gt;
                 case StreamsConfig.UPGRADE_FROM_0100:&lt;br/&gt;
                     log.info(&quot;Downgrading metadata version from {} to 1 for upgrade from 0.10.0.x.&quot;, SubscriptionInfo.LATEST_SUPPORTED_VERSION);&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;userMetadataVersion = 1;&lt;br/&gt;
+                    usedSubscriptionMetadataVersion = VERSION_ONE;&lt;br/&gt;
                     break;&lt;br/&gt;
                 case StreamsConfig.UPGRADE_FROM_0101:&lt;br/&gt;
                 case StreamsConfig.UPGRADE_FROM_0102:&lt;br/&gt;
                 case StreamsConfig.UPGRADE_FROM_0110:&lt;br/&gt;
                 case StreamsConfig.UPGRADE_FROM_10:&lt;br/&gt;
                 case StreamsConfig.UPGRADE_FROM_11:&lt;/li&gt;
	&lt;li&gt;log.info(&quot;Downgrading metadata version from {} to 2 for upgrade from &quot; + upgradeFrom + &quot;.x.&quot;, SubscriptionInfo.LATEST_SUPPORTED_VERSION);&lt;/li&gt;
	&lt;li&gt;userMetadataVersion = 2;&lt;br/&gt;
+                    log.info(&quot;Downgrading metadata version from {} to 2 for upgrade from {}.x.&quot;, SubscriptionInfo.LATEST_SUPPORTED_VERSION, upgradeFrom);&lt;br/&gt;
+                    usedSubscriptionMetadataVersion = VERSION_TWO;&lt;br/&gt;
                     break;&lt;br/&gt;
                 default:&lt;br/&gt;
                     throw new IllegalArgumentException(&quot;Unknown configuration value for parameter &apos;upgrade.from&apos;: &quot; + upgradeFrom);&lt;br/&gt;
@@ -234,6 +250,21 @@ public void configure(final Map&amp;lt;String, ?&amp;gt; configs) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         taskManager = (TaskManager) o;&lt;/p&gt;

&lt;p&gt;+        final Object o2 = configs.get(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG);&lt;br/&gt;
+        if (o2 == null) &lt;/p&gt;
{
+            final KafkaException fatalException = new KafkaException(&quot;VersionProbingFlag is not specified&quot;);
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
+        }
&lt;p&gt;+&lt;br/&gt;
+        if (!(o2 instanceof AtomicBoolean)) &lt;/p&gt;
{
+            final KafkaException fatalException = new KafkaException(String.format(&quot;%s is not an instance of %s&quot;, o2.getClass().getName(), AtomicBoolean.class.getName()));
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
+        }
&lt;p&gt;+&lt;br/&gt;
+        versionProbingFlag = (AtomicBoolean) o2;&lt;br/&gt;
+&lt;br/&gt;
         numStandbyReplicas = streamsConfig.getInt(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);&lt;/p&gt;

&lt;p&gt;         partitionGrouper = streamsConfig.getConfiguredInstance(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, PartitionGrouper.class);&lt;br/&gt;
@@ -277,7 +308,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks = taskManager.cachedTasksIds();&lt;br/&gt;
         standbyTasks.removeAll(previousActiveTasks);&lt;br/&gt;
         final SubscriptionInfo data = new SubscriptionInfo(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;userMetadataVersion,&lt;br/&gt;
+            usedSubscriptionMetadataVersion,&lt;br/&gt;
             taskManager.processId(),&lt;br/&gt;
             previousActiveTasks,&lt;br/&gt;
             standbyTasks,&lt;br/&gt;
@@ -313,20 +344,25 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                                           final Map&amp;lt;String, Subscription&amp;gt; subscriptions) {&lt;br/&gt;
         // construct the client metadata from the decoded subscription info&lt;br/&gt;
         final Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Set&amp;lt;String&amp;gt; futureConsumers = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int minUserMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;&lt;br/&gt;
+        minReceivedMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;&lt;br/&gt;
+        supportedVersions.clear();&lt;br/&gt;
+        int futureMetadataVersion = UNKNOWN;&lt;br/&gt;
         for (final Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) {&lt;br/&gt;
             final String consumerId = entry.getKey();&lt;br/&gt;
             final Subscription subscription = entry.getValue();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
             final int usedVersion = info.version();&lt;br/&gt;
+            supportedVersions.add(info.latestSupportedVersion());&lt;br/&gt;
             if (usedVersion &amp;gt; SubscriptionInfo.LATEST_SUPPORTED_VERSION) &lt;/p&gt;
{
-                throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
-                    + &quot;; latest supported version: &quot; + SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+                futureMetadataVersion = usedVersion;
+                futureConsumers.add(consumerId);
+                continue;
             }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (usedVersion &amp;lt; minUserMetadataVersion) {&lt;/li&gt;
	&lt;li&gt;minUserMetadataVersion = usedVersion;&lt;br/&gt;
+            if (usedVersion &amp;lt; minReceivedMetadataVersion) 
{
+                minReceivedMetadataVersion = usedVersion;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             // create the new client metadata if necessary&lt;br/&gt;
@@ -341,6 +377,27 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) &lt;/p&gt;
{
             clientMetadata.addConsumer(consumerId, info);
         }

&lt;p&gt;+        final boolean versionProbing;&lt;br/&gt;
+        if (futureMetadataVersion != UNKNOWN) {&lt;br/&gt;
+            if (minReceivedMetadataVersion &amp;gt;= EARLIEST_PROBEABLE_VERSION) {&lt;br/&gt;
+                log.info(&quot;Received a future (version probing) subscription (version: {}). Sending empty assignment back (with supported version {}).&quot;,&lt;br/&gt;
+                    futureMetadataVersion,&lt;br/&gt;
+                    SubscriptionInfo.LATEST_SUPPORTED_VERSION);&lt;br/&gt;
+                versionProbing = true;&lt;br/&gt;
+            } else &lt;/p&gt;
{
+                throw new IllegalStateException(&quot;Received a future (version probing) subscription (version: &quot; + futureMetadataVersion
+                    + &quot;) and an incompatible pre Kafka 2.0 subscription (version: &quot; + minReceivedMetadataVersion + &quot;) at the same time.&quot;);
+            }
&lt;p&gt;+        } else &lt;/p&gt;
{
+            versionProbing = false;
+        }
&lt;p&gt;+&lt;br/&gt;
+        if (minReceivedMetadataVersion &amp;lt; SubscriptionInfo.LATEST_SUPPORTED_VERSION) {&lt;br/&gt;
+            log.info(&quot;Downgrading metadata to version {}. Latest supported version is {}.&quot;,&lt;br/&gt;
+                minReceivedMetadataVersion,&lt;br/&gt;
+                SubscriptionInfo.LATEST_SUPPORTED_VERSION);&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
         log.debug(&quot;Constructed client metadata {} from the member subscriptions.&quot;, clientsMetadata);&lt;/p&gt;

&lt;p&gt;         // ---------------- Step Zero ---------------- //&lt;br/&gt;
@@ -457,12 +514,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
             allAssignedPartitions.addAll(partitions);&lt;/p&gt;

&lt;p&gt;             final TaskId id = entry.getKey();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; ids = tasksByTopicGroup.get(id.topicGroupId);&lt;/li&gt;
	&lt;li&gt;if (ids == null) 
{
-                ids = new HashSet&amp;lt;&amp;gt;();
-                tasksByTopicGroup.put(id.topicGroupId, ids);
-            }&lt;/li&gt;
	&lt;li&gt;ids.add(id);&lt;br/&gt;
+            tasksByTopicGroup.computeIfAbsent(id.topicGroupId, k -&amp;gt; new HashSet&amp;lt;&amp;gt;()).add(id);&lt;br/&gt;
         }&lt;br/&gt;
         for (final String topic : allSourceTopics) {&lt;br/&gt;
             final List&amp;lt;PartitionInfo&amp;gt; partitionInfoList = fullMetadata.partitionsForTopic(topic);&lt;br/&gt;
@@ -530,7 +582,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // construct the global partition assignment per host map&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (minUserMetadataVersion == 2 || minUserMetadataVersion == 3) {&lt;br/&gt;
+        if (minReceivedMetadataVersion == 2 || minReceivedMetadataVersion == 3) {&lt;br/&gt;
             for (final Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
                 final HostInfo hostInfo = entry.getValue().hostInfo;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -548,8 +600,23 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         }&lt;br/&gt;
         taskManager.setPartitionsByHostState(partitionsByHostState);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// within the client, distribute tasks to its owned consumers&lt;br/&gt;
+        final Map&amp;lt;String, Assignment&amp;gt; assignment;&lt;br/&gt;
+        if (versionProbing) 
{
+            assignment = versionProbingAssignment(clientsMetadata, partitionsForTask, partitionsByHostState, futureConsumers, minReceivedMetadataVersion);
+        }
&lt;p&gt; else &lt;/p&gt;
{
+            assignment = computeNewAssignment(clientsMetadata, partitionsForTask, partitionsByHostState, minReceivedMetadataVersion);
+        }
&lt;p&gt;+&lt;br/&gt;
+        return assignment;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private Map&amp;lt;String, Assignment&amp;gt; computeNewAssignment(final Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata,&lt;br/&gt;
+                                                         final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsForTask,&lt;br/&gt;
+                                                         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState,&lt;br/&gt;
+                                                         final int minUserMetadataVersion) {&lt;br/&gt;
         final Map&amp;lt;String, Assignment&amp;gt; assignment = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+        // within the client, distribute tasks to its owned consumers&lt;br/&gt;
         for (final Map.Entry&amp;lt;UUID, ClientMetadata&amp;gt; entry : clientsMetadata.entrySet()) {&lt;br/&gt;
             final Set&amp;lt;String&amp;gt; consumers = entry.getValue().consumers;&lt;br/&gt;
             final ClientState state = entry.getValue().state;&lt;br/&gt;
@@ -574,12 +641,7 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
                 if (!state.standbyTasks().isEmpty()) {&lt;br/&gt;
                     final List&amp;lt;TaskId&amp;gt; assignedStandbyList = interleavedStandby.get(consumerTaskIndex);&lt;br/&gt;
                     for (final TaskId taskId : assignedStandbyList) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; standbyPartitions = standby.get(taskId);&lt;/li&gt;
	&lt;li&gt;if (standbyPartitions == null) 
{
-                            standbyPartitions = new HashSet&amp;lt;&amp;gt;();
-                            standby.put(taskId, standbyPartitions);
-                        }&lt;/li&gt;
	&lt;li&gt;standbyPartitions.addAll(partitionsForTask.get(taskId));&lt;br/&gt;
+                        standby.computeIfAbsent(taskId, k -&amp;gt; new HashSet&amp;lt;&amp;gt;()).addAll(partitionsForTask.get(taskId));&lt;br/&gt;
                     }&lt;br/&gt;
                 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -603,13 +665,63 @@ public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) &lt;/p&gt;
{
         return assignment;
     }

&lt;p&gt;+    private Map&amp;lt;String, Assignment&amp;gt; versionProbingAssignment(final Map&amp;lt;UUID, ClientMetadata&amp;gt; clientsMetadata,&lt;br/&gt;
+                                                             final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsForTask,&lt;br/&gt;
+                                                             final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState,&lt;br/&gt;
+                                                             final Set&amp;lt;String&amp;gt; futureConsumers,&lt;br/&gt;
+                                                             final int minUserMetadataVersion) {&lt;br/&gt;
+        final Map&amp;lt;String, Assignment&amp;gt; assignment = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+        // assign previously assigned tasks to &quot;old consumers&quot;&lt;br/&gt;
+        for (final ClientMetadata clientMetadata : clientsMetadata.values()) {&lt;br/&gt;
+            for (final String consumerId : clientMetadata.consumers) {&lt;br/&gt;
+&lt;br/&gt;
+                if (futureConsumers.contains(consumerId)) &lt;/p&gt;
{
+                    continue;
+                }
&lt;p&gt;+&lt;br/&gt;
+                final List&amp;lt;TaskId&amp;gt; activeTasks = new ArrayList&amp;lt;&amp;gt;(clientMetadata.state.prevActiveTasks());&lt;br/&gt;
+&lt;br/&gt;
+                final List&amp;lt;TopicPartition&amp;gt; assignedPartitions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+                for (final TaskId taskId : activeTasks) &lt;/p&gt;
{
+                    assignedPartitions.addAll(partitionsForTask.get(taskId));
+                }
&lt;p&gt;+&lt;br/&gt;
+                final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+                for (final TaskId taskId : clientMetadata.state.prevStandbyTasks()) &lt;/p&gt;
{
+                    standbyTasks.put(taskId, partitionsForTask.get(taskId));
+                }
&lt;p&gt;+&lt;br/&gt;
+                assignment.put(consumerId, new Assignment(&lt;br/&gt;
+                    assignedPartitions,&lt;br/&gt;
+                    new AssignmentInfo(&lt;br/&gt;
+                        minUserMetadataVersion,&lt;br/&gt;
+                        activeTasks,&lt;br/&gt;
+                        standbyTasks,&lt;br/&gt;
+                        partitionsByHostState)&lt;br/&gt;
+                        .encode()&lt;br/&gt;
+                ));&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        // add empty assignment for &quot;future version&quot; clients (ie, empty version probing response)&lt;br/&gt;
+        for (final String consumerId : futureConsumers) &lt;/p&gt;
{
+            assignment.put(consumerId, new Assignment(
+                Collections.emptyList(),
+                new AssignmentInfo().encode()
+            ));
+        }
&lt;p&gt;+&lt;br/&gt;
+        return assignment;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     // visible for testing&lt;br/&gt;
     List&amp;lt;List&amp;lt;TaskId&amp;gt;&amp;gt; interleaveTasksByGroupId(final Collection&amp;lt;TaskId&amp;gt; taskIds, final int numberThreads) {&lt;br/&gt;
         final LinkedList&amp;lt;TaskId&amp;gt; sortedTasks = new LinkedList&amp;lt;&amp;gt;(taskIds);&lt;br/&gt;
         Collections.sort(sortedTasks);&lt;br/&gt;
         final List&amp;lt;List&amp;lt;TaskId&amp;gt;&amp;gt; taskIdsForConsumerAssignment = new ArrayList&amp;lt;&amp;gt;(numberThreads);&lt;br/&gt;
         for (int i = 0; i &amp;lt; numberThreads; i++) &lt;/p&gt;
{
-            taskIdsForConsumerAssignment.add(new ArrayList&amp;lt;TaskId&amp;gt;());
+            taskIdsForConsumerAssignment.add(new ArrayList&amp;lt;&amp;gt;());
         }
&lt;p&gt;         while (!sortedTasks.isEmpty()) {&lt;br/&gt;
             for (final List&amp;lt;TaskId&amp;gt; taskIdList : taskIdsForConsumerAssignment) {&lt;br/&gt;
@@ -632,7 +744,35 @@ public void onAssignment(final Assignment assignment) {&lt;br/&gt;
         Collections.sort(partitions, PARTITION_COMPARATOR);&lt;/p&gt;

&lt;p&gt;         final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int usedVersion = info.version();&lt;br/&gt;
+        final int receivedAssignmentMetadataVersion = info.version();&lt;br/&gt;
+        final int leaderSupportedVersion = info.latestSupportedVersion();&lt;br/&gt;
+&lt;br/&gt;
+        if (receivedAssignmentMetadataVersion &amp;gt; usedSubscriptionMetadataVersion) 
{
+            throw new IllegalStateException(&quot;Sent a version &quot; + usedSubscriptionMetadataVersion
+                + &quot; subscription but got an assignment with higher version &quot; + receivedAssignmentMetadataVersion + &quot;.&quot;);
+        }
&lt;p&gt;+&lt;br/&gt;
+        if (receivedAssignmentMetadataVersion &amp;lt; usedSubscriptionMetadataVersion&lt;br/&gt;
+            &amp;amp;&amp;amp; receivedAssignmentMetadataVersion &amp;gt;= EARLIEST_PROBEABLE_VERSION) {&lt;br/&gt;
+&lt;br/&gt;
+            if (receivedAssignmentMetadataVersion == leaderSupportedVersion) {&lt;br/&gt;
+                log.info(&quot;Sent a version {} subscription and got version {} assignment back (successful version probing). &quot; +&lt;br/&gt;
+                        &quot;Downgrading subscription metadata to received version and trigger new rebalance.&quot;,&lt;br/&gt;
+                    usedSubscriptionMetadataVersion,&lt;br/&gt;
+                    receivedAssignmentMetadataVersion);&lt;br/&gt;
+                usedSubscriptionMetadataVersion = receivedAssignmentMetadataVersion;&lt;br/&gt;
+            } else {&lt;br/&gt;
+                log.info(&quot;Sent a version {} subscription and got version {} assignment back (successful version probing). &quot; +&lt;br/&gt;
+                    &quot;Setting subscription metadata to leaders supported version {} and trigger new rebalance.&quot;,&lt;br/&gt;
+                    usedSubscriptionMetadataVersion,&lt;br/&gt;
+                    receivedAssignmentMetadataVersion,&lt;br/&gt;
+                    leaderSupportedVersion);&lt;br/&gt;
+                usedSubscriptionMetadataVersion = leaderSupportedVersion;&lt;br/&gt;
+            }&lt;br/&gt;
+&lt;br/&gt;
+            versionProbingFlag.set(true);&lt;br/&gt;
+            return;&lt;br/&gt;
+        }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // version 1 field&lt;br/&gt;
         final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -640,22 +780,29 @@ public void onAssignment(final Assignment assignment) {&lt;br/&gt;
         final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;switch (usedVersion) {&lt;/li&gt;
	&lt;li&gt;case 1:&lt;br/&gt;
+        switch (receivedAssignmentMetadataVersion) {&lt;br/&gt;
+            case VERSION_ONE:&lt;br/&gt;
                 processVersionOneAssignment(info, partitions, activeTasks);&lt;br/&gt;
                 partitionsByHost = Collections.emptyMap();&lt;br/&gt;
                 break;&lt;/li&gt;
	&lt;li&gt;case 2:&lt;br/&gt;
+            case VERSION_TWO:&lt;br/&gt;
                 processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);&lt;br/&gt;
                 partitionsByHost = info.partitionsByHost();&lt;br/&gt;
                 break;&lt;/li&gt;
	&lt;li&gt;case 3:&lt;br/&gt;
+            case VERSION_THREE:&lt;br/&gt;
+                if (leaderSupportedVersion &amp;gt; usedSubscriptionMetadataVersion) {&lt;br/&gt;
+                    log.info(&quot;Sent a version {} subscription and group leader&apos;s latest supported version is {}. &quot; +&lt;br/&gt;
+                        &quot;Upgrading subscription metadata version to {} for next rebalance.&quot;,&lt;br/&gt;
+                        usedSubscriptionMetadataVersion,&lt;br/&gt;
+                        leaderSupportedVersion,&lt;br/&gt;
+                        leaderSupportedVersion);&lt;br/&gt;
+                    usedSubscriptionMetadataVersion = leaderSupportedVersion;&lt;br/&gt;
+                }&lt;br/&gt;
                 processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);&lt;br/&gt;
                 partitionsByHost = info.partitionsByHost();&lt;br/&gt;
                 break;&lt;br/&gt;
             default:&lt;/li&gt;
	&lt;li&gt;throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion&lt;/li&gt;
	&lt;li&gt;+ &quot;; latest supported version: &quot; + AssignmentInfo.LATEST_SUPPORTED_VERSION);&lt;br/&gt;
+                throw new IllegalStateException(&quot;This code should never be reached. Please file a bug report at &lt;a href=&quot;https://issues.apache.org/jira/projects/KAFKA/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/projects/KAFKA/&lt;/a&gt;&quot;);&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));&lt;br/&gt;
@@ -679,13 +826,7 @@ private void processVersionOneAssignment(final AssignmentInfo info,&lt;br/&gt;
         for (int i = 0; i &amp;lt; partitions.size(); i++) {&lt;br/&gt;
             final TopicPartition partition = partitions.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
             final TaskId id = info.activeTasks().get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; assignedPartitions = activeTasks.get(id);&lt;/li&gt;
	&lt;li&gt;if (assignedPartitions == null) 
{
-                assignedPartitions = new HashSet&amp;lt;&amp;gt;();
-                activeTasks.put(id, assignedPartitions);
-            }&lt;/li&gt;
	&lt;li&gt;assignedPartitions.add(partition);&lt;br/&gt;
+            activeTasks.computeIfAbsent(id, k -&amp;gt; new HashSet&amp;lt;&amp;gt;()).add(partition);&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -713,6 +854,14 @@ private void processVersionThreeAssignment(final AssignmentInfo info,&lt;br/&gt;
         processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    // for testing&lt;br/&gt;
+    protected void processLatestVersionAssignment(final AssignmentInfo info,&lt;br/&gt;
+                                                  final List&amp;lt;TopicPartition&amp;gt; partitions,&lt;br/&gt;
+                                                  final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks,&lt;br/&gt;
+                                                  final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo) &lt;/p&gt;
{
+        processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Internal helper function that creates a Kafka topic&lt;br/&gt;
      *&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
index 63224dbcde7..6e6e4ca7c5c 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
@@ -42,7 +42,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static java.util.Collections.singleton;&lt;/p&gt;

&lt;p&gt;-class TaskManager {&lt;br/&gt;
+public class TaskManager {&lt;br/&gt;
     // initialize the task list&lt;br/&gt;
     // activeTasks needs to be concurrent as it can be accessed&lt;br/&gt;
     // by QueryableState&lt;br/&gt;
@@ -187,14 +187,14 @@ private void addStandbyTasks() &lt;/p&gt;
{
         return standby.allAssignedTaskIds();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; prevActiveTaskIds() {&lt;br/&gt;
+    public Set&amp;lt;TaskId&amp;gt; prevActiveTaskIds() 
{
         return active.previousTaskIds();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Returns ids of tasks whose states are kept on the local storage.&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; cachedTasksIds() {&lt;br/&gt;
+    public Set&amp;lt;TaskId&amp;gt; cachedTasksIds() {&lt;br/&gt;
         // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:&lt;br/&gt;
         // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.&lt;br/&gt;
         // 2) the client has just got some tasks migrated out of itself to other clients while these task states&lt;br/&gt;
@@ -221,7 +221,7 @@ private void addStandbyTasks() 
{
         return tasks;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID processId() {&lt;br/&gt;
+    public UUID processId() 
{
         return processId;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -356,21 +356,21 @@ private void assignStandbyPartitions() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void setClusterMetadata(final Cluster cluster) {&lt;br/&gt;
+    public void setClusterMetadata(final Cluster cluster) 
{
         this.cluster = cluster;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void setPartitionsByHostState(final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState) {&lt;br/&gt;
+    public void setPartitionsByHostState(final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHostState) 
{
         this.streamsMetadataState.onChange(partitionsByHostState, cluster);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void setAssignmentMetadata(final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks,&lt;br/&gt;
+    public void setAssignmentMetadata(final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks,&lt;br/&gt;
                                final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks) 
{
         this.assignedActiveTasks = activeTasks;
         this.assignedStandbyTasks = standbyTasks;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void updateSubscriptionsFromAssignment(List&amp;lt;TopicPartition&amp;gt; partitions) {&lt;br/&gt;
+    public void updateSubscriptionsFromAssignment(List&amp;lt;TopicPartition&amp;gt; partitions) {&lt;br/&gt;
         if (builder().sourceTopicPattern() != null) {&lt;br/&gt;
             final Set&amp;lt;String&amp;gt; assignedTopics = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
             for (final TopicPartition topicPartition : partitions) {&lt;br/&gt;
@@ -385,7 +385,7 @@ void updateSubscriptionsFromAssignment(List&amp;lt;TopicPartition&amp;gt; partitions) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void updateSubscriptionsFromMetadata(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
+    public void updateSubscriptionsFromMetadata(Set&amp;lt;String&amp;gt; topics) {&lt;br/&gt;
         if (builder().sourceTopicPattern() != null) {&lt;br/&gt;
             final Collection&amp;lt;String&amp;gt; existingTopics = builder().subscriptionUpdates().getUpdates();&lt;br/&gt;
             if (!existingTopics.equals(topics)) {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
index 3c5cee2bfc3..c577830e3e2 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java&lt;br/&gt;
@@ -42,7 +42,7 @@&lt;br/&gt;
     private static final Logger log = LoggerFactory.getLogger(AssignmentInfo.class);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static final int LATEST_SUPPORTED_VERSION = 3;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final int UNKNOWN = -1;&lt;br/&gt;
+    static final int UNKNOWN = -1;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final int usedVersion;&lt;br/&gt;
     private final int latestSupportedVersion;&lt;br/&gt;
@@ -65,9 +65,9 @@ public AssignmentInfo(final List&amp;lt;TaskId&amp;gt; activeTasks,&lt;/p&gt;

&lt;p&gt;     public AssignmentInfo() &lt;/p&gt;
{
         this(LATEST_SUPPORTED_VERSION,
-            Collections.&amp;lt;TaskId&amp;gt;emptyList(),
-            Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(),
-            Collections.&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap());
+            Collections.emptyList(),
+            Collections.emptyMap(),
+            Collections.emptyMap());
     }

&lt;p&gt;     public AssignmentInfo(final int version,&lt;br/&gt;
@@ -229,7 +229,7 @@ public static AssignmentInfo decode(final ByteBuffer data) {&lt;br/&gt;
                     decodeVersionThreeData(assignmentInfo, in);&lt;br/&gt;
                     break;&lt;br/&gt;
                 default:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode assignment data: &quot; +&lt;br/&gt;
+                    final TaskAssignmentException fatalException = new TaskAssignmentException(&quot;Unable to decode assignment data: &quot; +&lt;br/&gt;
                         &quot;used version: &quot; + usedVersion + &quot;; latest supported version: &quot; + LATEST_SUPPORTED_VERSION);&lt;br/&gt;
                     log.error(fatalException.getMessage(), fatalException);&lt;br/&gt;
                     throw fatalException;&lt;br/&gt;
@@ -262,7 +262,7 @@ private static void decodeStandbyTasks(final AssignmentInfo assignmentInfo,&lt;br/&gt;
         final int count = in.readInt();&lt;br/&gt;
         assignmentInfo.standbyTasks = new HashMap&amp;lt;&amp;gt;(count);&lt;br/&gt;
         for (int i = 0; i &amp;lt; count; i++) 
{
-            TaskId id = TaskId.readFrom(in);
+            final TaskId id = TaskId.readFrom(in);
             assignmentInfo.standbyTasks.put(id, readTopicPartitions(in));
         }
&lt;p&gt;     }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java&lt;br/&gt;
index 15ee849bffc..66e655fa837 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java&lt;br/&gt;
@@ -26,6 +26,7 @@&lt;br/&gt;
     private final Set&amp;lt;TaskId&amp;gt; standbyTasks;&lt;br/&gt;
     private final Set&amp;lt;TaskId&amp;gt; assignedTasks;&lt;br/&gt;
     private final Set&amp;lt;TaskId&amp;gt; prevActiveTasks;&lt;br/&gt;
+    private final Set&amp;lt;TaskId&amp;gt; prevStandbyTasks;&lt;br/&gt;
     private final Set&amp;lt;TaskId&amp;gt; prevAssignedTasks;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private int capacity;&lt;br/&gt;
@@ -36,21 +37,34 @@ public ClientState() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     ClientState(final int capacity) &lt;/p&gt;
{
-        this(new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), capacity);
+        this(new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), new HashSet&amp;lt;TaskId&amp;gt;(), capacity);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private ClientState(Set&amp;lt;TaskId&amp;gt; activeTasks, Set&amp;lt;TaskId&amp;gt; standbyTasks, Set&amp;lt;TaskId&amp;gt; assignedTasks, Set&amp;lt;TaskId&amp;gt; prevActiveTasks, Set&amp;lt;TaskId&amp;gt; prevAssignedTasks, int capacity) {&lt;br/&gt;
+    private ClientState(final Set&amp;lt;TaskId&amp;gt; activeTasks,&lt;br/&gt;
+                        final Set&amp;lt;TaskId&amp;gt; standbyTasks,&lt;br/&gt;
+                        final Set&amp;lt;TaskId&amp;gt; assignedTasks,&lt;br/&gt;
+                        final Set&amp;lt;TaskId&amp;gt; prevActiveTasks,&lt;br/&gt;
+                        final Set&amp;lt;TaskId&amp;gt; prevStandbyTasks,&lt;br/&gt;
+                        final Set&amp;lt;TaskId&amp;gt; prevAssignedTasks,&lt;br/&gt;
+                        final int capacity) 
{
         this.activeTasks = activeTasks;
         this.standbyTasks = standbyTasks;
         this.assignedTasks = assignedTasks;
         this.prevActiveTasks = prevActiveTasks;
+        this.prevStandbyTasks = prevStandbyTasks;
         this.prevAssignedTasks = prevAssignedTasks;
         this.capacity = capacity;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public ClientState copy() &lt;/p&gt;
{
-        return new ClientState(new HashSet&amp;lt;&amp;gt;(activeTasks), new HashSet&amp;lt;&amp;gt;(standbyTasks), new HashSet&amp;lt;&amp;gt;(assignedTasks),
-                new HashSet&amp;lt;&amp;gt;(prevActiveTasks), new HashSet&amp;lt;&amp;gt;(prevAssignedTasks), capacity);
+        return new ClientState(
+            new HashSet&amp;lt;&amp;gt;(activeTasks),
+            new HashSet&amp;lt;&amp;gt;(standbyTasks),
+            new HashSet&amp;lt;&amp;gt;(assignedTasks),
+            new HashSet&amp;lt;&amp;gt;(prevActiveTasks),
+            new HashSet&amp;lt;&amp;gt;(prevStandbyTasks),
+            new HashSet&amp;lt;&amp;gt;(prevAssignedTasks),
+            capacity);
     }

&lt;p&gt;     public void assign(final TaskId taskId, final boolean active) {&lt;br/&gt;
@@ -71,6 +85,14 @@ public void assign(final TaskId taskId, final boolean active) &lt;/p&gt;
{
         return standbyTasks;
     }

&lt;p&gt;+    public Set&amp;lt;TaskId&amp;gt; prevActiveTasks() &lt;/p&gt;
{
+        return prevActiveTasks;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public Set&amp;lt;TaskId&amp;gt; prevStandbyTasks() &lt;/p&gt;
{
+        return prevStandbyTasks;
+    }
&lt;p&gt;+&lt;br/&gt;
     public int assignedTaskCount() &lt;/p&gt;
{
         return assignedTasks.size();
     }
&lt;p&gt;@@ -89,6 +111,7 @@ public void addPreviousActiveTasks(final Set&amp;lt;TaskId&amp;gt; prevTasks) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public void addPreviousStandbyTasks(final Set&amp;lt;TaskId&amp;gt; standbyTasks) &lt;/p&gt;
{
+        prevStandbyTasks.addAll(standbyTasks);
         prevAssignedTasks.addAll(standbyTasks);
     }

&lt;p&gt;@@ -98,6 +121,7 @@ public String toString() {&lt;br/&gt;
                 &quot;) standbyTasks: (&quot; + standbyTasks +&lt;br/&gt;
                 &quot;) assignedTasks: (&quot; + assignedTasks +&lt;br/&gt;
                 &quot;) prevActiveTasks: (&quot; + prevActiveTasks +&lt;br/&gt;
+                &quot;) prevStandbyTasks: (&quot; + prevStandbyTasks +&lt;br/&gt;
                 &quot;) prevAssignedTasks: (&quot; + prevAssignedTasks +&lt;br/&gt;
                 &quot;) capacity: &quot; + capacity +&lt;br/&gt;
                 &quot;]&quot;;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
index be709472441..4ebc95674b0 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java&lt;br/&gt;
@@ -33,7 +33,7 @@&lt;br/&gt;
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);&lt;/p&gt;

&lt;p&gt;     public static final int LATEST_SUPPORTED_VERSION = 3;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final int UNKNOWN = -1;&lt;br/&gt;
+    static final int UNKNOWN = -1;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final int usedVersion;&lt;br/&gt;
     private final int latestSupportedVersion;&lt;br/&gt;
@@ -151,20 +151,20 @@ private int getVersionOneByteLength() &lt;/p&gt;
{
                4 + standbyTasks.size() * 8; // length + standby tasks
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void encodeClientUUID(final ByteBuffer buf) {&lt;br/&gt;
+    protected void encodeClientUUID(final ByteBuffer buf) 
{
         buf.putLong(processId.getMostSignificantBits());
         buf.putLong(processId.getLeastSignificantBits());
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void encodeTasks(final ByteBuffer buf,&lt;/li&gt;
	&lt;li&gt;final Collection&amp;lt;TaskId&amp;gt; taskIds) {&lt;br/&gt;
+    protected void encodeTasks(final ByteBuffer buf,&lt;br/&gt;
+                               final Collection&amp;lt;TaskId&amp;gt; taskIds) {&lt;br/&gt;
         buf.putInt(taskIds.size());&lt;/li&gt;
	&lt;li&gt;for (TaskId id : taskIds) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        for (final TaskId id }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private byte[] prepareUserEndPoint() {&lt;br/&gt;
+    protected byte[] prepareUserEndPoint() {&lt;br/&gt;
         if (userEndPoint == null) 
{
             return new byte[0];
         }
&lt;p&gt; else {&lt;br/&gt;
@@ -194,8 +194,8 @@ private int getVersionTwoByteLength(final byte[] endPointBytes) &lt;/p&gt;
{
                4 + endPointBytes.length; // length + userEndPoint
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void encodeUserEndPoint(final ByteBuffer buf,&lt;/li&gt;
	&lt;li&gt;final byte[] endPointBytes) {&lt;br/&gt;
+    protected void encodeUserEndPoint(final ByteBuffer buf,&lt;br/&gt;
+                                      final byte[] endPointBytes) {&lt;br/&gt;
         if (endPointBytes != null) {&lt;br/&gt;
             buf.putInt(endPointBytes.length);&lt;br/&gt;
             buf.put(endPointBytes);&lt;br/&gt;
@@ -217,7 +217,7 @@ private ByteBuffer encodeVersionThree() 
{
         return buf;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private int getVersionThreeByteLength(final byte[] endPointBytes) {&lt;br/&gt;
+    protected int getVersionThreeByteLength(final byte[] endPointBytes) {&lt;br/&gt;
         return 4 + // used version&lt;br/&gt;
                4 + // latest supported version version&lt;br/&gt;
                16 + // client ID&lt;br/&gt;
@@ -236,6 +236,7 @@ public static SubscriptionInfo decode(final ByteBuffer data) {&lt;br/&gt;
         data.rewind();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final int usedVersion = data.getInt();&lt;br/&gt;
+        final int latestSupportedVersion;&lt;br/&gt;
         switch (usedVersion) {&lt;br/&gt;
             case 1:&lt;br/&gt;
                 subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);&lt;br/&gt;
@@ -246,12 +247,13 @@ public static SubscriptionInfo decode(final ByteBuffer data) {&lt;br/&gt;
                 decodeVersionTwoData(subscriptionInfo, data);&lt;br/&gt;
                 break;&lt;br/&gt;
             case 3:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int latestSupportedVersion = data.getInt();&lt;br/&gt;
+                latestSupportedVersion = data.getInt();&lt;br/&gt;
                 subscriptionInfo = new SubscriptionInfo(usedVersion, latestSupportedVersion);&lt;br/&gt;
                 decodeVersionThreeData(subscriptionInfo, data);&lt;br/&gt;
                 break;&lt;br/&gt;
             default:&lt;/li&gt;
	&lt;li&gt;subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);&lt;br/&gt;
+                latestSupportedVersion = data.getInt();&lt;br/&gt;
+                subscriptionInfo = new SubscriptionInfo(usedVersion, latestSupportedVersion);&lt;br/&gt;
                 log.info(&quot;Unable to decode subscription data: used version: {}; latest supported version: {}&quot;, usedVersion, LATEST_SUPPORTED_VERSION);&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -261,12 +263,7 @@ public static SubscriptionInfo decode(final ByteBuffer data) {&lt;br/&gt;
     private static void decodeVersionOneData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
                                              final ByteBuffer data) &lt;/p&gt;
{
         decodeClientUUID(subscriptionInfo, data);
-
-        subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();
-        decodeTasks(subscriptionInfo.prevTasks, data);
-
-        subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();
-        decodeTasks(subscriptionInfo.standbyTasks, data);
+        decodeTasks(subscriptionInfo, data);
     }

&lt;p&gt;     private static void decodeClientUUID(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
@@ -274,30 +271,31 @@ private static void decodeClientUUID(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
         subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static void decodeTasks(final Collection&amp;lt;TaskId&amp;gt; taskIds,&lt;br/&gt;
+    private static void decodeTasks(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
                                     final ByteBuffer data) {&lt;/li&gt;
	&lt;li&gt;final int numPrevs = data.getInt();&lt;/li&gt;
	&lt;li&gt;for (int i = 0; i &amp;lt; numPrevs; i++) {&lt;/li&gt;
	&lt;li&gt;taskIds.add(TaskId.readFrom(data));&lt;br/&gt;
+        subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        final int numPrevTasks = data.getInt();&lt;br/&gt;
+        for (int i = 0; i &amp;lt; numPrevTasks; i++) 
{
+            subscriptionInfo.prevTasks.add(TaskId.readFrom(data));
+        }
&lt;p&gt;+&lt;br/&gt;
+        subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        final int numStandbyTasks = data.getInt();&lt;br/&gt;
+        for (int i = 0; i &amp;lt; numStandbyTasks; i++) &lt;/p&gt;
{
+            subscriptionInfo.standbyTasks.add(TaskId.readFrom(data));
         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
                                              final ByteBuffer data) &lt;/p&gt;
{
         decodeClientUUID(subscriptionInfo, data);
-
-        subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();
-        decodeTasks(subscriptionInfo.prevTasks, data);
-
-        subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();
-        decodeTasks(subscriptionInfo.standbyTasks, data);
-
+        decodeTasks(subscriptionInfo, data);
         decodeUserEndPoint(subscriptionInfo, data);
     }&lt;br/&gt;
 &lt;br/&gt;
     private static void decodeUserEndPoint(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
                                            final ByteBuffer data) {&lt;br/&gt;
-        int bytesLength = data.getInt();&lt;br/&gt;
+        final int bytesLength = data.getInt();&lt;br/&gt;
         if (bytesLength != 0) {&lt;br/&gt;
             final byte[] bytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;bytesLength&amp;#93;&lt;/span&gt;;&lt;br/&gt;
             data.get(bytes);&lt;br/&gt;
@@ -308,16 +306,11 @@ private static void decodeUserEndPoint(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
     private static void decodeVersionThreeData(final SubscriptionInfo subscriptionInfo,&lt;br/&gt;
                                                final ByteBuffer data) {         decodeClientUUID(subscriptionInfo, data);--        subscriptionInfo.prevTasks = new HashSet&amp;lt;&amp;gt;();-        decodeTasks(subscriptionInfo.prevTasks, data);--        subscriptionInfo.standbyTasks = new HashSet&amp;lt;&amp;gt;();-        decodeTasks(subscriptionInfo.standbyTasks, data);-+        decodeTasks(subscriptionInfo, data);         decodeUserEndPoint(subscriptionInfo, data);     }

&lt;p&gt;+    @Override&lt;br/&gt;
     public int hashCode() {&lt;br/&gt;
         final int hashCode = usedVersion ^ latestSupportedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();&lt;br/&gt;
         if (userEndPoint == null) {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
index 749d618bac5..f3dce521998 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
@@ -78,6 +78,7 @@&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicBoolean;&lt;/p&gt;

&lt;p&gt; import static java.util.Collections.singletonList;&lt;br/&gt;
 import static org.apache.kafka.common.utils.Utils.mkEntry;&lt;br/&gt;
@@ -298,7 +299,8 @@ public void shouldNotCommitBeforeTheCommitInterval() {&lt;br/&gt;
             streamsMetrics,&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new LogContext(&quot;&quot;)&lt;br/&gt;
+            new LogContext(&quot;&quot;),&lt;br/&gt;
+            new AtomicBoolean()&lt;br/&gt;
         );&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
         mockTime.sleep(commitInterval - 10L);&lt;br/&gt;
@@ -331,7 +333,9 @@ public void shouldNotCauseExceptionIfNothingCommitted() {&lt;br/&gt;
             streamsMetrics,&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;/li&gt;
	&lt;li&gt;new LogContext(&quot;&quot;));&lt;br/&gt;
+            new LogContext(&quot;&quot;),&lt;br/&gt;
+            new AtomicBoolean()&lt;br/&gt;
+        );&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
         mockTime.sleep(commitInterval - 10L);&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
@@ -364,7 +368,9 @@ public void shouldCommitAfterTheCommitInterval() {&lt;br/&gt;
             streamsMetrics,&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;/li&gt;
	&lt;li&gt;new LogContext(&quot;&quot;));&lt;br/&gt;
+            new LogContext(&quot;&quot;),&lt;br/&gt;
+            new AtomicBoolean()&lt;br/&gt;
+        );&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
         mockTime.sleep(commitInterval + 1);&lt;br/&gt;
         thread.maybeCommit(mockTime.milliseconds());&lt;br/&gt;
@@ -511,7 +517,8 @@ public void shouldShutdownTaskManagerOnClose() {&lt;br/&gt;
             streamsMetrics,&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;/li&gt;
	&lt;li&gt;new LogContext(&quot;&quot;)&lt;br/&gt;
+            new LogContext(&quot;&quot;),&lt;br/&gt;
+            new AtomicBoolean()&lt;br/&gt;
         );&lt;br/&gt;
         thread.setStateListener(&lt;br/&gt;
             new StreamThread.StateListener() {&lt;br/&gt;
@@ -547,7 +554,8 @@ public void shouldShutdownTaskManagerOnCloseWithoutStart() {&lt;br/&gt;
             streamsMetrics,&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;/li&gt;
	&lt;li&gt;new LogContext(&quot;&quot;)&lt;br/&gt;
+            new LogContext(&quot;&quot;),&lt;br/&gt;
+            new AtomicBoolean()&lt;br/&gt;
         );&lt;br/&gt;
         thread.shutdown();&lt;br/&gt;
         EasyMock.verify(taskManager);&lt;br/&gt;
@@ -574,7 +582,9 @@ public void shouldOnlyShutdownOnce() {&lt;br/&gt;
             streamsMetrics,&lt;br/&gt;
             internalTopologyBuilder,&lt;br/&gt;
             clientId,&lt;/li&gt;
	&lt;li&gt;new LogContext(&quot;&quot;));&lt;br/&gt;
+            new LogContext(&quot;&quot;),&lt;br/&gt;
+            new AtomicBoolean()&lt;br/&gt;
+        );&lt;br/&gt;
         thread.shutdown();&lt;br/&gt;
         // Execute the run method. Verification of the mock will check that shutdown was only done once&lt;br/&gt;
         thread.run();&lt;br/&gt;
@@ -1255,7 +1265,7 @@ private void assertThreadMetadataHasEmptyTasksWithState(final ThreadMetadata met&lt;br/&gt;
     @Test&lt;br/&gt;
     // TODO: Need to add a test case covering EOS when we create a mock taskManager class&lt;br/&gt;
     public void producerMetricsVerificationWithoutEOS() {&lt;/li&gt;
	&lt;li&gt;final MockProducer&amp;lt;byte[], byte[]&amp;gt; producer = new MockProducer();&lt;br/&gt;
+        final MockProducer&amp;lt;byte[], byte[]&amp;gt; producer = new MockProducer&amp;lt;&amp;gt;();&lt;br/&gt;
         final Consumer&amp;lt;byte[], byte[]&amp;gt; consumer = EasyMock.createNiceMock(Consumer.class);&lt;br/&gt;
         final TaskManager taskManager = mockTaskManagerCommit(consumer, 1, 0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1271,7 +1281,8 @@ public void producerMetricsVerificationWithoutEOS() {&lt;br/&gt;
                 streamsMetrics,&lt;br/&gt;
                 internalTopologyBuilder,&lt;br/&gt;
                 clientId,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new LogContext(&quot;&quot;));&lt;br/&gt;
+                new LogContext(&quot;&quot;),&lt;br/&gt;
+                new AtomicBoolean());&lt;br/&gt;
         final MetricName testMetricName = new MetricName(&quot;test_metric&quot;, &quot;&quot;, &quot;&quot;, new HashMap&amp;lt;String, String&amp;gt;());&lt;br/&gt;
         final Metric testMetric = new KafkaMetric(&lt;br/&gt;
                 new Object(),&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
index 37b03fa3418..a32d193a171 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
@@ -48,6 +48,7 @@&lt;br/&gt;
 import org.easymock.EasyMock;&lt;br/&gt;
 import org.junit.Test;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
@@ -57,6 +58,7 @@&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicBoolean;&lt;/p&gt;

&lt;p&gt; import static org.hamcrest.CoreMatchers.equalTo;&lt;br/&gt;
 import static org.hamcrest.CoreMatchers.not;&lt;br/&gt;
@@ -98,7 +100,8 @@&lt;br/&gt;
     private final Cluster metadata = new Cluster(&lt;br/&gt;
         &quot;cluster&quot;,&lt;br/&gt;
         Collections.singletonList(Node.noNode()),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;infos, Collections.&amp;lt;String&amp;gt;emptySet(),&lt;br/&gt;
+        infos,&lt;br/&gt;
+        Collections.&amp;lt;String&amp;gt;emptySet(),&lt;br/&gt;
         Collections.&amp;lt;String&amp;gt;emptySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final TaskId task0 = new TaskId(0, 0);&lt;br/&gt;
@@ -115,15 +118,16 @@&lt;br/&gt;
     private final TaskManager taskManager = EasyMock.createNiceMock(TaskManager.class);&lt;/p&gt;

&lt;p&gt;     private Map&amp;lt;String, Object&amp;gt; configProps() &lt;/p&gt;
{
-        Map&amp;lt;String, Object&amp;gt; configurationMap = new HashMap&amp;lt;&amp;gt;();
+        final Map&amp;lt;String, Object&amp;gt; configurationMap = new HashMap&amp;lt;&amp;gt;();
         configurationMap.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
         configurationMap.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, userEndPoint);
         configurationMap.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);
+        configurationMap.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, new AtomicBoolean());
         return configurationMap;
     }

&lt;p&gt;     private void configurePartitionAssignor(final Map&amp;lt;String, Object&amp;gt; props) &lt;/p&gt;
{
-        Map&amp;lt;String, Object&amp;gt; configurationMap = configProps();
+        final Map&amp;lt;String, Object&amp;gt; configurationMap = configProps();
         configurationMap.putAll(props);
         partitionAssignor.configure(configurationMap);
     }
&lt;p&gt;@@ -158,7 +162,7 @@ public void shouldInterleaveTasksByGroupId() {&lt;br/&gt;
         final List&amp;lt;TaskId&amp;gt; expectedSubList3 = Arrays.asList(taskIdA2, taskIdB1, taskIdC1);&lt;br/&gt;
         final List&amp;lt;List&amp;lt;TaskId&amp;gt;&amp;gt; embeddedList = Arrays.asList(expectedSubList1, expectedSubList2, expectedSubList3);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;TaskId&amp;gt; tasks = Arrays.asList(taskIdC0, taskIdC1, taskIdB0, taskIdB1, taskIdB2, taskIdA0, taskIdA1, taskIdA2, taskIdA3);&lt;br/&gt;
+        final List&amp;lt;TaskId&amp;gt; tasks = Arrays.asList(taskIdC0, taskIdC1, taskIdB0, taskIdB1, taskIdB2, taskIdA0, taskIdA1, taskIdA2, taskIdA3);&lt;br/&gt;
         Collections.shuffle(tasks);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;List&amp;lt;TaskId&amp;gt;&amp;gt; interleavedTaskIds = partitionAssignor.interleaveTasksByGroupId(tasks, 3);&lt;br/&gt;
@@ -182,15 +186,15 @@ public void testSubscription() &lt;/p&gt;
{
         mockTaskManager(prevTasks, cachedTasks, processId, builder);
 
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());
-        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;, &quot;topic2&quot;));
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;, &quot;topic2&quot;));
 
         Collections.sort(subscription.topics());
         assertEquals(Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;), subscription.topics());
 
-        Set&amp;lt;TaskId&amp;gt; standbyTasks = new HashSet&amp;lt;&amp;gt;(cachedTasks);
+        final Set&amp;lt;TaskId&amp;gt; standbyTasks = new HashSet&amp;lt;&amp;gt;(cachedTasks);
         standbyTasks.removeAll(prevTasks);
 
-        SubscriptionInfo info = new SubscriptionInfo(processId, prevTasks, standbyTasks, null);
+        final SubscriptionInfo info = new SubscriptionInfo(processId, prevTasks, standbyTasks, null);
         assertEquals(info.encode(), subscription.userData());
     }

&lt;p&gt;@@ -199,8 +203,8 @@ public void testAssignBasic() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Set&amp;lt;TaskId&amp;gt; prevTasks10 = Utils.mkSet(task0);&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; prevTasks11 = Utils.mkSet(task1);&lt;br/&gt;
@@ -209,15 +213,15 @@ public void testAssignBasic() {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks11 = Utils.mkSet(task2);&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks20 = Utils.mkSet(task0);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;/li&gt;
	&lt;li&gt;UUID uuid2 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid2 = UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockTaskManager(prevTasks10, standbyTasks10, uuid1, builder);&lt;br/&gt;
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/p&gt;

&lt;p&gt;         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks10, standbyTasks10, userEndPoint).encode()));&lt;br/&gt;
         subscriptions.put(&quot;consumer11&quot;,&lt;br/&gt;
@@ -226,7 +230,7 @@ public void testAssignBasic() {&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, prevTasks20, standbyTasks20, userEndPoint).encode()));&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assigned partitions&lt;br/&gt;
         assertEquals(Utils.mkSet(Utils.mkSet(t1p0, t2p0), Utils.mkSet(t1p1, t2p1)),&lt;br/&gt;
@@ -236,17 +240,17 @@ public void testAssignBasic() {&lt;br/&gt;
         // check assignment info&lt;/p&gt;

&lt;p&gt;         // the first consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;br/&gt;
+        final AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
+        final AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
         allActiveTasks.addAll(info11.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(Utils.mkSet(task0, task1), allActiveTasks);&lt;/p&gt;

&lt;p&gt;         // the third consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(&quot;consumer20&quot;));&lt;br/&gt;
+        final AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(&quot;consumer20&quot;));&lt;br/&gt;
         allActiveTasks.addAll(info20.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
@@ -277,7 +281,8 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() {&lt;br/&gt;
         final Cluster localMetadata = new Cluster(&lt;br/&gt;
             &quot;cluster&quot;,&lt;br/&gt;
             Collections.singletonList(Node.noNode()),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;localInfos, Collections.&amp;lt;String&amp;gt;emptySet(),&lt;br/&gt;
+            localInfos,&lt;br/&gt;
+            Collections.&amp;lt;String&amp;gt;emptySet(),&lt;br/&gt;
             Collections.&amp;lt;String&amp;gt;emptySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;br/&gt;
@@ -332,26 +337,26 @@ public void testAssignWithPartialTopology() {&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor2&quot;, new MockProcessorSupplier(), &quot;source2&quot;);&lt;br/&gt;
         builder.addStateStore(new MockStoreBuilder(&quot;store2&quot;, false), &quot;processor2&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), uuid1, builder);&lt;br/&gt;
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, (Object) SingleGroupPartitionGrouperStub.class));&lt;/p&gt;

&lt;p&gt;         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
             new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), userEndPoint).encode()));&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;         // will throw exception if it fails&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assignment info&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info10 = checkAssignment(Utils.mkSet(&quot;topic1&quot;), assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;br/&gt;
+        final AssignmentInfo info10 = checkAssignment(Utils.mkSet(&quot;topic1&quot;), assignments.get(&quot;consumer10&quot;));&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(3, allActiveTasks.size());&lt;br/&gt;
         assertEquals(allTasks, new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -363,8 +368,8 @@ public void testAssignEmptyMetadata() {&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Set&amp;lt;TaskId&amp;gt; prevTasks10 = Utils.mkSet(task0);&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks10 = Utils.mkSet(task1);&lt;br/&gt;
@@ -372,12 +377,12 @@ public void testAssignEmptyMetadata() {&lt;br/&gt;
             Collections.&amp;lt;PartitionInfo&amp;gt;emptySet(),&lt;br/&gt;
             Collections.&amp;lt;String&amp;gt;emptySet(),&lt;br/&gt;
             Collections.&amp;lt;String&amp;gt;emptySet());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockTaskManager(prevTasks10, standbyTasks10, uuid1, builder);&lt;br/&gt;
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
             new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks10, standbyTasks10, userEndPoint).encode()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -390,7 +395,7 @@ public void testAssignEmptyMetadata() {&lt;/p&gt;

&lt;p&gt;         // check assignment info&lt;br/&gt;
         AssignmentInfo info10 = checkAssignment(Collections.&amp;lt;String&amp;gt;emptySet(), assignments.get(&quot;consumer10&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, allActiveTasks.size());&lt;br/&gt;
         assertEquals(Collections.&amp;lt;TaskId&amp;gt;emptySet(), new HashSet&amp;lt;&amp;gt;(allActiveTasks));&lt;br/&gt;
@@ -418,22 +423,22 @@ public void testAssignWithNewTasks() {&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source3&quot;, null, null, null, &quot;topic3&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;, &quot;source3&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;, &quot;topic3&quot;);&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2, task3);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;, &quot;topic3&quot;);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2, task3);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // assuming that previous tasks do not have topic3&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; prevTasks10 = Utils.mkSet(task0);&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; prevTasks11 = Utils.mkSet(task1);&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; prevTasks20 = Utils.mkSet(task2);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;/li&gt;
	&lt;li&gt;UUID uuid2 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid2 = UUID.randomUUID();&lt;br/&gt;
         mockTaskManager(prevTasks10, Collections.&amp;lt;TaskId&amp;gt;emptySet(), uuid1, builder);&lt;br/&gt;
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks10, Collections.&amp;lt;TaskId&amp;gt;emptySet(), userEndPoint).encode()));&lt;br/&gt;
         subscriptions.put(&quot;consumer11&quot;,&lt;br/&gt;
@@ -441,14 +446,14 @@ public void testAssignWithNewTasks() {&lt;br/&gt;
         subscriptions.put(&quot;consumer20&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, prevTasks20, Collections.&amp;lt;TaskId&amp;gt;emptySet(), userEndPoint).encode()));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match&lt;br/&gt;
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and&lt;br/&gt;
         // then later ones will be re-assigned to other hosts due to load balancing&lt;br/&gt;
         AssignmentInfo info = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info.activeTasks());&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions());&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info.activeTasks());&lt;br/&gt;
+        final Set&amp;lt;TopicPartition&amp;gt; allPartitions = new HashSet&amp;lt;&amp;gt;(assignments.get(&quot;consumer10&quot;).partitions());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         info = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());&lt;br/&gt;
         allActiveTasks.addAll(info.activeTasks());&lt;br/&gt;
@@ -475,18 +480,18 @@ public void testAssignWithStates() {&lt;br/&gt;
         builder.addStateStore(new MockStoreBuilder(&quot;store2&quot;, false), &quot;processor-2&quot;);&lt;br/&gt;
         builder.addStateStore(new MockStoreBuilder(&quot;store3&quot;, false), &quot;processor-2&quot;);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TaskId task00 = new TaskId(0, 0);&lt;/li&gt;
	&lt;li&gt;TaskId task01 = new TaskId(0, 1);&lt;/li&gt;
	&lt;li&gt;TaskId task02 = new TaskId(0, 2);&lt;/li&gt;
	&lt;li&gt;TaskId task10 = new TaskId(1, 0);&lt;/li&gt;
	&lt;li&gt;TaskId task11 = new TaskId(1, 1);&lt;/li&gt;
	&lt;li&gt;TaskId task12 = new TaskId(1, 2);&lt;/li&gt;
	&lt;li&gt;List&amp;lt;TaskId&amp;gt; tasks = Utils.mkList(task00, task01, task02, task10, task11, task12);&lt;br/&gt;
+        final TaskId task00 = new TaskId(0, 0);&lt;br/&gt;
+        final TaskId task01 = new TaskId(0, 1);&lt;br/&gt;
+        final TaskId task02 = new TaskId(0, 2);&lt;br/&gt;
+        final TaskId task10 = new TaskId(1, 0);&lt;br/&gt;
+        final TaskId task11 = new TaskId(1, 1);&lt;br/&gt;
+        final TaskId task12 = new TaskId(1, 2);&lt;br/&gt;
+        final List&amp;lt;TaskId&amp;gt; tasks = Utils.mkList(task00, task01, task02, task10, task11, task12);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;/li&gt;
	&lt;li&gt;UUID uuid2 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid2 = UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockTaskManager(&lt;br/&gt;
             Collections.&amp;lt;TaskId&amp;gt;emptySet(),&lt;br/&gt;
@@ -497,7 +502,7 @@ public void testAssignWithStates() {&lt;/p&gt;

&lt;p&gt;         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), userEndPoint).encode()));&lt;br/&gt;
         subscriptions.put(&quot;consumer11&quot;,&lt;br/&gt;
@@ -505,47 +510,46 @@ public void testAssignWithStates() 
{
         subscriptions.put(&quot;consumer20&quot;,
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), userEndPoint).encode()));
 
-        Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);
 
         // check assigned partition size: since there is no previous task and there are two sub-topologies the assignment is random so we cannot check exact match
         assertEquals(2, assignments.get(&quot;consumer10&quot;).partitions().size());
         assertEquals(2, assignments.get(&quot;consumer11&quot;).partitions().size());
         assertEquals(2, assignments.get(&quot;consumer20&quot;).partitions().size());
 
-        AssignmentInfo info10 = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());
-        AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());
-        AssignmentInfo info20 = AssignmentInfo.decode(assignments.get(&quot;consumer20&quot;).userData());
+        final AssignmentInfo info10 = AssignmentInfo.decode(assignments.get(&quot;consumer10&quot;).userData());
+        final AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(&quot;consumer11&quot;).userData());
+        final AssignmentInfo info20 = AssignmentInfo.decode(assignments.get(&quot;consumer20&quot;).userData());
 
         assertEquals(2, info10.activeTasks().size());
         assertEquals(2, info11.activeTasks().size());
         assertEquals(2, info20.activeTasks().size());
 
-        Set&amp;lt;TaskId&amp;gt; allTasks = new HashSet&amp;lt;&amp;gt;();
+        final Set&amp;lt;TaskId&amp;gt; allTasks = new HashSet&amp;lt;&amp;gt;();
         allTasks.addAll(info10.activeTasks());
         allTasks.addAll(info11.activeTasks());
         allTasks.addAll(info20.activeTasks());
         assertEquals(new HashSet&amp;lt;&amp;gt;(tasks), allTasks);
 
         // check tasks for state topics
-        Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups = builder.topicGroups();
+        final Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups = builder.topicGroups();
 
-        assertEquals(Utils.mkSet(task00, task01, task02), tasksForState(applicationId, &quot;store1&quot;, tasks, topicGroups));
-        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, &quot;store2&quot;, tasks, topicGroups));
-        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, &quot;store3&quot;, tasks, topicGroups));
+        assertEquals(Utils.mkSet(task00, task01, task02), tasksForState(&quot;store1&quot;, tasks, topicGroups));
+        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(&quot;store2&quot;, tasks, topicGroups));
+        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(&quot;store3&quot;, tasks, topicGroups));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Set&amp;lt;TaskId&amp;gt; tasksForState(final String applicationId,&lt;/li&gt;
	&lt;li&gt;final String storeName,&lt;br/&gt;
+    private Set&amp;lt;TaskId&amp;gt; tasksForState(final String storeName,&lt;br/&gt;
                                       final List&amp;lt;TaskId&amp;gt; tasks,&lt;br/&gt;
                                       final Map&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; topicGroups) {&lt;br/&gt;
         final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, storeName);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; ids = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; entry : topicGroups.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;String&amp;gt; stateChangelogTopics = entry.getValue().stateChangelogTopics.keySet();&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; ids = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;Integer, InternalTopologyBuilder.TopicsInfo&amp;gt; entry : topicGroups.entrySet()) {&lt;br/&gt;
+            final Set&amp;lt;String&amp;gt; stateChangelogTopics = entry.getValue().stateChangelogTopics.keySet();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             if (stateChangelogTopics.contains(changelogTopic)) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (TaskId id : tasks) {&lt;br/&gt;
+                for (final TaskId id : tasks) 
{
                     if (id.topicGroupId == entry.getKey())
                         ids.add(id);
                 }
&lt;p&gt;@@ -556,15 +560,15 @@ public void testAssignWithStates() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void testAssignWithStandbyReplicas() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; props = configProps();&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; props = configProps();&lt;br/&gt;
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, &quot;1&quot;);&lt;/li&gt;
	&lt;li&gt;StreamsConfig streamsConfig = new StreamsConfig(props);&lt;br/&gt;
+        final StreamsConfig streamsConfig = new StreamsConfig(props);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topic2&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor&quot;, new MockProcessorSupplier(), &quot;source1&quot;, &quot;source2&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;topic2&quot;);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;         final Set&amp;lt;TaskId&amp;gt; prevTasks00 = Utils.mkSet(task0);&lt;br/&gt;
@@ -574,8 +578,8 @@ public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks02 = Utils.mkSet(task2);&lt;br/&gt;
         final Set&amp;lt;TaskId&amp;gt; standbyTasks00 = Utils.mkSet(task0);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;/li&gt;
	&lt;li&gt;UUID uuid2 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid2 = UUID.randomUUID();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockTaskManager(prevTasks00, standbyTasks01, uuid1, builder);&lt;/p&gt;

&lt;p&gt;@@ -583,7 +587,7 @@ public void testAssignWithStandbyReplicas() {&lt;/p&gt;

&lt;p&gt;         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks00, standbyTasks01, userEndPoint).encode()));&lt;br/&gt;
         subscriptions.put(&quot;consumer11&quot;,&lt;br/&gt;
@@ -591,15 +595,15 @@ public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         subscriptions.put(&quot;consumer20&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, prevTasks02, standbyTasks00, &quot;any:9097&quot;).encode()));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignments = partitionAssignor.assign(metadata, subscriptions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the first consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;(info10.standbyTasks().keySet());&lt;br/&gt;
+        final AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(&quot;consumer10&quot;));&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allActiveTasks = new HashSet&amp;lt;&amp;gt;(info10.activeTasks());&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allStandbyTasks = new HashSet&amp;lt;&amp;gt;(info10.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // the second consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
+        final AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(&quot;consumer11&quot;));&lt;br/&gt;
         allActiveTasks.addAll(info11.activeTasks());&lt;br/&gt;
         allStandbyTasks.addAll(info11.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -610,7 +614,7 @@ public void testAssignWithStandbyReplicas() {&lt;br/&gt;
         assertEquals(Utils.mkSet(task2), new HashSet&amp;lt;&amp;gt;(allStandbyTasks));&lt;/p&gt;

&lt;p&gt;         // the third consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(&quot;consumer20&quot;));&lt;br/&gt;
+        final AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(&quot;consumer20&quot;));&lt;br/&gt;
         allActiveTasks.addAll(info20.activeTasks());&lt;br/&gt;
         allStandbyTasks.addAll(info20.standbyTasks().keySet());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -641,7 +645,7 @@ public void testOnAssignment() {&lt;br/&gt;
         final AssignmentInfo info = new AssignmentInfo(activeTaskList, standbyTasks, hostState);&lt;br/&gt;
         final PartitionAssignor.Assignment assignment = new PartitionAssignor.Assignment(Utils.mkList(t3p0, t3p3), info.encode());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Capture&amp;lt;Cluster&amp;gt; capturedCluster = EasyMock.newCapture();&lt;br/&gt;
+        final Capture&amp;lt;Cluster&amp;gt; capturedCluster = EasyMock.newCapture();&lt;br/&gt;
         taskManager.setPartitionsByHostState(hostState);&lt;br/&gt;
         EasyMock.expectLastCall();&lt;br/&gt;
         taskManager.setAssignmentMetadata(activeTasks, standbyTasks);&lt;br/&gt;
@@ -667,17 +671,17 @@ public void testAssignWithInternalTopics() {&lt;br/&gt;
         builder.addSink(&quot;sink1&quot;, &quot;topicX&quot;, null, null, null, &quot;processor1&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source2&quot;, null, null, null, &quot;topicX&quot;);&lt;br/&gt;
         builder.addProcessor(&quot;processor2&quot;, new MockProcessorSupplier(), &quot;source2&quot;);&lt;/li&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, applicationId + &quot;-topicX&quot;);&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, applicationId + &quot;-topicX&quot;);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
         mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), uuid1, builder);&lt;br/&gt;
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/li&gt;
	&lt;li&gt;MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);&lt;br/&gt;
+        final MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);&lt;br/&gt;
         partitionAssignor.setInternalTopicManager(internalTopicManager);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -690,7 +694,7 @@ public void testAssignWithInternalTopics() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String applicationId = &quot;test&quot;;&lt;br/&gt;
+        final String applicationId = &quot;test&quot;;&lt;br/&gt;
         builder.setApplicationId(applicationId);&lt;br/&gt;
         builder.addInternalTopic(&quot;topicX&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
@@ -701,18 +705,18 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {&lt;br/&gt;
         builder.addProcessor(&quot;processor2&quot;, new MockProcessorSupplier(), &quot;source2&quot;);&lt;br/&gt;
         builder.addSink(&quot;sink2&quot;, &quot;topicZ&quot;, null, null, null, &quot;processor2&quot;);&lt;br/&gt;
         builder.addSource(null, &quot;source3&quot;, null, null, null, &quot;topicZ&quot;);&lt;/li&gt;
	&lt;li&gt;List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;test-topicX&quot;, &quot;test-topicZ&quot;);&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; topics = Utils.mkList(&quot;topic1&quot;, &quot;test-topicX&quot;, &quot;test-topicZ&quot;);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
+        final UUID uuid1 = UUID.randomUUID();&lt;br/&gt;
         mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(), Collections.&amp;lt;TaskId&amp;gt;emptySet(), uuid1, builder);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);&lt;br/&gt;
+        final MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);&lt;br/&gt;
         partitionAssignor.setInternalTopicManager(internalTopicManager);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();&lt;br/&gt;
         subscriptions.put(&quot;consumer10&quot;,&lt;br/&gt;
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -731,7 +735,7 @@ public void shouldGenerateTasksForAllCreatedPartitions() {&lt;br/&gt;
         internalTopologyBuilder.setApplicationId(applicationId);&lt;/p&gt;

&lt;p&gt;         // KStream with 3 partitions&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KStream&amp;lt;Object, Object&amp;gt; stream1 = builder&lt;br/&gt;
+        final KStream&amp;lt;Object, Object&amp;gt; stream1 = builder&lt;br/&gt;
             .stream(&quot;topic1&quot;)&lt;br/&gt;
             // force creation of internal repartition topic&lt;br/&gt;
             .map(new KeyValueMapper&amp;lt;Object, Object, KeyValue&amp;lt;Object, Object&amp;gt;&amp;gt;() {&lt;br/&gt;
@@ -742,7 +746,7 @@ public void shouldGenerateTasksForAllCreatedPartitions() {&lt;br/&gt;
             });&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // KTable with 4 partitions&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KTable&amp;lt;Object, Long&amp;gt; table1 = builder&lt;br/&gt;
+        final KTable&amp;lt;Object, Long&amp;gt; table1 = builder&lt;br/&gt;
             .table(&quot;topic3&quot;)&lt;br/&gt;
             // force creation of internal repartition topic&lt;br/&gt;
             .groupBy(new KeyValueMapper&amp;lt;Object, Object, KeyValue&amp;lt;Object, Object&amp;gt;&amp;gt;() {&lt;br/&gt;
@@ -884,7 +888,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) &quot;localhost&quot;));
             fail(&quot;expected to an exception due to invalid config&quot;);
-        } catch (ConfigException e) {
+        } catch (final ConfigException e) {
             // pass
         }&lt;br/&gt;
     }&lt;br/&gt;
@@ -896,7 +900,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {&lt;br/&gt;
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) &quot;localhost:j87yhk&quot;));
             fail(&quot;expected to an exception due to invalid config&quot;);
-        } catch (ConfigException e) {+        } catch (final ConfigException e) {
             // pass
         }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -908,7 +912,7 @@ public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTas&lt;br/&gt;
         final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());&lt;br/&gt;
         internalTopologyBuilder.setApplicationId(applicationId);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KStream&amp;lt;Object, Object&amp;gt; stream1 = builder&lt;br/&gt;
+        final KStream&amp;lt;Object, Object&amp;gt; stream1 = builder&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             // Task 1 (should get created):&lt;br/&gt;
             .stream(&quot;topic1&quot;)&lt;br/&gt;
@@ -964,10 +968,11 @@ public Object apply(final Object value1, final Object value2) {&lt;br/&gt;
         final UUID uuid = UUID.randomUUID();&lt;br/&gt;
         final String client = &quot;client1&quot;;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;mockTaskManager(Collections.&amp;lt;TaskId&amp;gt;emptySet(),&lt;/li&gt;
	&lt;li&gt;Collections.&amp;lt;TaskId&amp;gt;emptySet(),&lt;/li&gt;
	&lt;li&gt;UUID.randomUUID(),&lt;/li&gt;
	&lt;li&gt;internalTopologyBuilder);&lt;br/&gt;
+        mockTaskManager(&lt;br/&gt;
+            Collections.&amp;lt;TaskId&amp;gt;emptySet(),&lt;br/&gt;
+            Collections.&amp;lt;TaskId&amp;gt;emptySet(),&lt;br/&gt;
+            UUID.randomUUID(),&lt;br/&gt;
+            internalTopologyBuilder);&lt;br/&gt;
         configurePartitionAssignor(Collections.&amp;lt;String, Object&amp;gt;emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final MockInternalTopicManager mockInternalTopicManager = new MockInternalTopicManager(&lt;br/&gt;
@@ -1037,7 +1042,7 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {&lt;br/&gt;
             uuid,&lt;br/&gt;
             internalTopologyBuilder);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; props = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; props = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);&lt;br/&gt;
         props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint);&lt;br/&gt;
         configurePartitionAssignor(props);&lt;br/&gt;
@@ -1075,18 +1080,58 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() 
{
         assertThat(allAssignedPartitions, equalTo(allPartitions));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = KafkaException.class)&lt;/li&gt;
	&lt;li&gt;public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() {&lt;/li&gt;
	&lt;li&gt;partitionAssignor.configure(Collections.singletonMap(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1));&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfTaskMangerNotConfigured() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        final Map&amp;lt;String, Object&amp;gt; config = configProps();+        config.remove(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);++        try {
+            partitionAssignor.configure(config);
+            fail(&quot;Should have thrown KafkaException&quot;);
+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(), equalTo(&quot;TaskManager is not specified&quot;));
+        }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    @Test(expected = KafkaException.class)&lt;br/&gt;
-    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProviderInstance() {&lt;br/&gt;
-        final Map&amp;lt;String, Object&amp;gt; config = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-        config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);&lt;br/&gt;
-        config.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, &quot;i am not a stream thread&quot;);&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfTaskMangerConfigIsNotTaskManagerInstance() {&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; config = configProps();&lt;br/&gt;
+        config.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, &quot;i am not a task manager&quot;);&lt;br/&gt;
 &lt;br/&gt;
-        partitionAssignor.configure(config);&lt;br/&gt;
+        try {+            partitionAssignor.configure(config);+            fail(&quot;Should have thrown KafkaException&quot;);+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(),
+                equalTo(&quot;java.lang.String is not an instance of org.apache.kafka.streams.processor.internals.TaskManager&quot;));
+        }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldThrowKafkaExceptionVersionProbingFlagNotConfigured() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        final Map&amp;lt;String, Object&amp;gt; config = configProps();+        config.remove(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG);++        try {
+            partitionAssignor.configure(config);
+            fail(&quot;Should have thrown KafkaException&quot;);
+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(), equalTo(&quot;VersionProbingFlag is not specified&quot;));
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldThrowKafkaExceptionIfVersionProbingFlagConfigIsNotAtomicBoolean() {&lt;br/&gt;
+        final Map&amp;lt;String, Object&amp;gt; config = configProps();&lt;br/&gt;
+        config.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, &quot;i am not an AtomicBoolean&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        try {+            partitionAssignor.configure(config);+            fail(&quot;Should have thrown KafkaException&quot;);+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(),
+                equalTo(&quot;java.lang.String is not an instance of java.util.concurrent.atomic.AtomicBoolean&quot;));
+        }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
@@ -1147,7 +1192,7 @@ public void shouldDownGradeSubscriptionToVersion1() &lt;/p&gt;
{
             builder);
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, (Object) StreamsConfig.UPGRADE_FROM_0100));
 
-        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));
 
         assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(1));
     }
&lt;p&gt;@@ -1187,11 +1232,114 @@ private void shouldDownGradeSubscriptionToVersion2(final Object upgradeFromValue&lt;br/&gt;
             builder);&lt;br/&gt;
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFromValue));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));&lt;br/&gt;
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(&quot;topic1&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(2));&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldReturnUnchangedAssignmentForOldInstancesAndEmptyAssignmentForFutureInstances() {&lt;br/&gt;
+        builder.addSource(null, &quot;source1&quot;, null, null, null, &quot;topic1&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; allTasks = Utils.mkSet(task0, task1, task2);&lt;br/&gt;
+&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; activeTasks = Utils.mkSet(task0, task1);&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; standbyTasks = Utils.mkSet(task2);&lt;br/&gt;
+        final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTaskMap = new HashMap&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;() {&lt;br/&gt;
+            &lt;/p&gt;
{
+                put(task2, Collections.singleton(t1p2));
+            }
&lt;p&gt;+        };&lt;br/&gt;
+&lt;br/&gt;
+        subscriptions.put(&lt;br/&gt;
+            &quot;consumer1&quot;,&lt;br/&gt;
+            new PartitionAssignor.Subscription(&lt;br/&gt;
+                Collections.singletonList(&quot;topic1&quot;),&lt;br/&gt;
+                new SubscriptionInfo(UUID.randomUUID(), activeTasks, standbyTasks, null).encode()&lt;br/&gt;
+            )&lt;br/&gt;
+        );&lt;br/&gt;
+        subscriptions.put(&lt;br/&gt;
+            &quot;future-consumer&quot;,&lt;br/&gt;
+            new PartitionAssignor.Subscription(&lt;br/&gt;
+                Collections.singletonList(&quot;topic1&quot;),&lt;br/&gt;
+                encodeFutureSubscription()&lt;br/&gt;
+            )&lt;br/&gt;
+        );&lt;br/&gt;
+&lt;br/&gt;
+        mockTaskManager(&lt;br/&gt;
+            allTasks,&lt;br/&gt;
+            allTasks,&lt;br/&gt;
+            UUID.randomUUID(),&lt;br/&gt;
+            builder);&lt;br/&gt;
+        partitionAssignor.configure(configProps());&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Assignment&amp;gt; assignment = partitionAssignor.assign(metadata, subscriptions);&lt;br/&gt;
+&lt;br/&gt;
+        assertThat(assignment.size(), equalTo(2));&lt;br/&gt;
+        assertThat(&lt;br/&gt;
+            AssignmentInfo.decode(assignment.get(&quot;consumer1&quot;).userData()),&lt;br/&gt;
+            equalTo(new AssignmentInfo(&lt;br/&gt;
+                new ArrayList&amp;lt;&amp;gt;(activeTasks),&lt;br/&gt;
+                standbyTaskMap,&lt;br/&gt;
+                Collections.&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap()&lt;br/&gt;
+            )));&lt;br/&gt;
+        assertThat(assignment.get(&quot;consumer1&quot;).partitions(), equalTo(Utils.mkList(t1p0, t1p1)));&lt;br/&gt;
+&lt;br/&gt;
+        assertThat(AssignmentInfo.decode(assignment.get(&quot;future-consumer&quot;).userData()), equalTo(new AssignmentInfo()));&lt;br/&gt;
+        assertThat(assignment.get(&quot;future-consumer&quot;).partitions().size(), equalTo(0));&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldThrowIfV1SubscriptionAndFutureSubscriptionIsMixed() &lt;/p&gt;
{
+        shouldThrowIfPreVersionProbingSubscriptionAndFutureSubscriptionIsMixed(1);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldThrowIfV2SubscriptionAndFutureSubscriptionIsMixed() &lt;/p&gt;
{
+        shouldThrowIfPreVersionProbingSubscriptionAndFutureSubscriptionIsMixed(2);
+    }
&lt;p&gt;+&lt;br/&gt;
+    private ByteBuffer encodeFutureSubscription() &lt;/p&gt;
{
+        final ByteBuffer buf = ByteBuffer.allocate(4 /* used version */
+                                                   + 4 /* supported version */);
+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);
+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);
+        return buf;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void shouldThrowIfPreVersionProbingSubscriptionAndFutureSubscriptionIsMixed(final int oldVersion) {&lt;br/&gt;
+        final Map&amp;lt;String, PartitionAssignor.Subscription&amp;gt; subscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Set&amp;lt;TaskId&amp;gt; emptyTasks = Collections.emptySet();&lt;br/&gt;
+        subscriptions.put(&lt;br/&gt;
+            &quot;consumer1&quot;,&lt;br/&gt;
+            new PartitionAssignor.Subscription(&lt;br/&gt;
+                Collections.singletonList(&quot;topic1&quot;),&lt;br/&gt;
+                new SubscriptionInfo(oldVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()&lt;br/&gt;
+            )&lt;br/&gt;
+        );&lt;br/&gt;
+        subscriptions.put(&lt;br/&gt;
+            &quot;future-consumer&quot;,&lt;br/&gt;
+            new PartitionAssignor.Subscription(&lt;br/&gt;
+                Collections.singletonList(&quot;topic1&quot;),&lt;br/&gt;
+                encodeFutureSubscription()&lt;br/&gt;
+            )&lt;br/&gt;
+        );&lt;br/&gt;
+&lt;br/&gt;
+        mockTaskManager(&lt;br/&gt;
+            emptyTasks,&lt;br/&gt;
+            emptyTasks,&lt;br/&gt;
+            UUID.randomUUID(),&lt;br/&gt;
+            builder);&lt;br/&gt;
+        partitionAssignor.configure(configProps());&lt;br/&gt;
+&lt;br/&gt;
+        try {
+            partitionAssignor.assign(metadata, subscriptions);
+            fail(&quot;Should have thrown IllegalStateException&quot;);
+        } catch (final IllegalStateException expected) {
+            // pass
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     private PartitionAssignor.Assignment createAssignment(final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; firstHostState) {
         final AssignmentInfo info = new AssignmentInfo(Collections.&amp;lt;TaskId&amp;gt;emptyList(),
                                                        Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(),
@@ -1201,19 +1349,20 @@ private void shouldDownGradeSubscriptionToVersion2(final Object upgradeFromValue
                 Collections.&amp;lt;TopicPartition&amp;gt;emptyList(), info.encode());
     }&lt;br/&gt;
 &lt;br/&gt;
-    private AssignmentInfo checkAssignment(Set&amp;lt;String&amp;gt; expectedTopics, PartitionAssignor.Assignment assignment) {&lt;br/&gt;
+    private AssignmentInfo checkAssignment(final Set&amp;lt;String&amp;gt; expectedTopics,&lt;br/&gt;
+                                           final PartitionAssignor.Assignment assignment) {&lt;br/&gt;
 &lt;br/&gt;
         // This assumed 1) DefaultPartitionGrouper is used, and 2) there is an only one topic group.&lt;br/&gt;
 &lt;br/&gt;
-        AssignmentInfo info = AssignmentInfo.decode(assignment.userData());&lt;br/&gt;
+        final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());&lt;br/&gt;
 &lt;br/&gt;
         // check if the number of assigned partitions == the size of active task id list&lt;br/&gt;
         assertEquals(assignment.partitions().size(), info.activeTasks().size());&lt;br/&gt;
 &lt;br/&gt;
         // check if active tasks are consistent&lt;br/&gt;
-        List&amp;lt;TaskId&amp;gt; activeTasks = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
-        Set&amp;lt;String&amp;gt; activeTopics = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-        for (TopicPartition partition : assignment.partitions()) {&lt;br/&gt;
+        final List&amp;lt;TaskId&amp;gt; activeTasks = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        final Set&amp;lt;String&amp;gt; activeTopics = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final TopicPartition partition : assignment.partitions()) {&lt;br/&gt;
             // since default grouper, taskid.partition == partition.partition()&lt;br/&gt;
             activeTasks.add(new TaskId(0, partition.partition()));&lt;br/&gt;
             activeTopics.add(partition.topic());&lt;br/&gt;
@@ -1224,11 +1373,11 @@ private AssignmentInfo checkAssignment(Set&amp;lt;String&amp;gt; expectedTopics, PartitionAssi&lt;br/&gt;
         assertEquals(expectedTopics, activeTopics);&lt;br/&gt;
 &lt;br/&gt;
         // check if standby tasks are consistent&lt;br/&gt;
-        Set&amp;lt;String&amp;gt; standbyTopics = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
-        for (Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : info.standbyTasks().entrySet()) {&lt;br/&gt;
-            TaskId id = entry.getKey();&lt;br/&gt;
-            Set&amp;lt;TopicPartition&amp;gt; partitions = entry.getValue();&lt;br/&gt;
-            for (TopicPartition partition : partitions) {&lt;br/&gt;
+        final Set&amp;lt;String&amp;gt; standbyTopics = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; entry : info.standbyTasks().entrySet()) {&lt;br/&gt;
+            final TaskId id = entry.getKey();&lt;br/&gt;
+            final Set&amp;lt;TopicPartition&amp;gt; partitions = entry.getValue();&lt;br/&gt;
+            for (final TopicPartition partition : partitions) {&lt;br/&gt;
                 // since default grouper, taskid.partition == partition.partition()&lt;br/&gt;
                 assertEquals(id.partition, partition.partition());&lt;br/&gt;
 &lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
index e98b8ce0727..0611bfc4d5c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 &lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
@@ -75,4 +76,19 @@ public void shouldEncodeAndDecodeVersion3() {
         assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }&lt;br/&gt;
 &lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldAllowToDecodeFutureSupportedVersion() {
+        final SubscriptionInfo info = SubscriptionInfo.decode(encodeFutureVersion());
+        assertEquals(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1, info.version());
+        assertEquals(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1, info.latestSupportedVersion());
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private ByteBuffer encodeFutureVersion() {+        final ByteBuffer buf = ByteBuffer.allocate(4 /* used version */+                                                   + 4 /* supported version */);+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);+        return buf;+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
index 2409bd59643..8c807800869 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java&lt;br/&gt;
@@ -40,7 +40,7 @@ public static void main(final String[] args) throws InterruptedException, IOExce&lt;/p&gt;

&lt;p&gt;         final String propFileName = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         final String command = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final boolean disableAutoTerminate = args.length &amp;gt; 3;&lt;br/&gt;
+        final boolean disableAutoTerminate = args.length &amp;gt; 2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Properties streamsProperties = Utils.loadProps(propFileName);&lt;br/&gt;
         final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
index 69eea0b37c0..1b01a7300a1 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java&lt;br/&gt;
@@ -16,29 +16,57 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.tests;&lt;/p&gt;

&lt;p&gt;+import org.apache.kafka.clients.consumer.Consumer;&lt;br/&gt;
+import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
+import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
+import org.apache.kafka.clients.consumer.internals.PartitionAssignor;&lt;br/&gt;
+import org.apache.kafka.common.Cluster;&lt;br/&gt;
+import org.apache.kafka.common.PartitionInfo;&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.serialization.ByteArrayDeserializer;&lt;br/&gt;
+import org.apache.kafka.common.utils.ByteBufferInputStream;&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaClientSupplier;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.errors.TaskAssignmentException;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
+import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.TaskManager;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.assignment.AssignmentInfo;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo;&lt;br/&gt;
+import org.apache.kafka.streams.state.HostInfo;&lt;/p&gt;

&lt;p&gt;+import java.io.ByteArrayOutputStream;&lt;br/&gt;
+import java.io.DataInputStream;&lt;br/&gt;
+import java.io.DataOutputStream;&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
+import java.nio.BufferUnderflowException;&lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
+import java.util.HashMap;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
+import java.util.Set;&lt;br/&gt;
+import java.util.UUID;&lt;/p&gt;

&lt;p&gt; public class StreamsUpgradeTest {&lt;/p&gt;

&lt;p&gt;     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     public static void main(final String[] args) throws Exception {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (args.length &amp;lt; 2) {&lt;/li&gt;
	&lt;li&gt;System.err.println(&quot;StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only &quot; + args.length + &quot; provided: &quot;&lt;/li&gt;
	&lt;li&gt;+ (args.length &amp;gt; 0 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; : &quot;&quot;));&lt;br/&gt;
+        if (args.length &amp;lt; 1) 
{
+            System.err.println(&quot;StreamsUpgradeTest requires one argument (properties-file) but no provided: &quot;);
         }&lt;/li&gt;
	&lt;li&gt;final String kafka = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;final String propFileName = args.length &amp;gt; 1 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; : null;&lt;br/&gt;
+        final String propFileName = args.length &amp;gt; 0 ? args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; : null;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Properties streamsProperties = Utils.loadProps(propFileName);&lt;/p&gt;

&lt;p&gt;         System.out.println(&quot;StreamsTest instance started (StreamsUpgradeTest trunk)&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.out.println(&quot;kafka=&quot; + kafka);&lt;br/&gt;
         System.out.println(&quot;props=&quot; + streamsProperties);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
@@ -48,11 +76,18 @@ public static void main(final String[] args) throws Exception {&lt;/p&gt;

&lt;p&gt;         final Properties config = new Properties();&lt;br/&gt;
         config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;StreamsUpgradeTest&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);&lt;br/&gt;
         config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);&lt;br/&gt;
+&lt;br/&gt;
+        final KafkaClientSupplier kafkaClientSupplier;&lt;br/&gt;
+        if (streamsProperties.containsKey(&quot;test.future.metadata&quot;)) 
{
+            streamsProperties.remove(&quot;test.future.metadata&quot;);
+            kafkaClientSupplier = new FutureKafkaClientSupplier();
+        }
&lt;p&gt; else &lt;/p&gt;
{
+            kafkaClientSupplier = new DefaultKafkaClientSupplier();
+        }
&lt;p&gt;         config.putAll(streamsProperties);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final KafkaStreams streams = new KafkaStreams(builder.build(), config);&lt;br/&gt;
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config, kafkaClientSupplier);&lt;br/&gt;
         streams.start();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Runtime.getRuntime().addShutdownHook(new Thread() {&lt;br/&gt;
@@ -66,4 +101,237 @@ public void run() {&lt;br/&gt;
             }&lt;br/&gt;
         });&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
+    private static class FutureKafkaClientSupplier extends DefaultKafkaClientSupplier {&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public Consumer&amp;lt;byte[], byte[]&amp;gt; getConsumer(final Map&amp;lt;String, Object&amp;gt; config) &lt;/p&gt;
{
+            config.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, FutureStreamsPartitionAssignor.class.getName());
+            return new KafkaConsumer&amp;lt;&amp;gt;(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    public static class FutureStreamsPartitionAssignor extends StreamsPartitionAssignor {&lt;br/&gt;
+&lt;br/&gt;
+        public FutureStreamsPartitionAssignor() &lt;/p&gt;
{
+            usedSubscriptionMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1;
+        }
&lt;p&gt;+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public Subscription subscription(final Set&amp;lt;String&amp;gt; topics) &lt;/p&gt;
{
+            // Adds the following information to subscription
+            // 1. Client UUID (a unique id assigned to an instance of KafkaStreams)
+            // 2. Task ids of previously running tasks
+            // 3. Task ids of valid local states on the client&apos;s state directory.
+
+            final TaskManager taskManager = taskManger();
+            final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = taskManager.prevActiveTaskIds();
+            final Set&amp;lt;TaskId&amp;gt; standbyTasks = taskManager.cachedTasksIds();
+            standbyTasks.removeAll(previousActiveTasks);
+            final FutureSubscriptionInfo data = new FutureSubscriptionInfo(
+                usedSubscriptionMetadataVersion,
+                SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1,
+                taskManager.processId(),
+                previousActiveTasks,
+                standbyTasks,
+                userEndPoint());
+
+            taskManager.updateSubscriptionsFromMetadata(topics);
+
+            return new Subscription(new ArrayList&amp;lt;&amp;gt;(topics), data.encode());
+        }
&lt;p&gt;+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onAssignment(final PartitionAssignor.Assignment assignment) {&lt;br/&gt;
+            try &lt;/p&gt;
{
+                super.onAssignment(assignment);
+                return;
+            }
&lt;p&gt; catch (final TaskAssignmentException cannotProcessFutureVersion) &lt;/p&gt;
{
+                // continue
+            }
&lt;p&gt;+&lt;br/&gt;
+            final ByteBuffer data = assignment.userData();&lt;br/&gt;
+            data.rewind();&lt;br/&gt;
+&lt;br/&gt;
+            final int usedVersion;&lt;br/&gt;
+            try (final DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) &lt;/p&gt;
{
+                usedVersion = in.readInt();
+            }
&lt;p&gt; catch (final IOException ex) &lt;/p&gt;
{
+                throw new TaskAssignmentException(&quot;Failed to decode AssignmentInfo&quot;, ex);
+            }
&lt;p&gt;+&lt;br/&gt;
+            if (usedVersion &amp;gt; AssignmentInfo.LATEST_SUPPORTED_VERSION + 1) &lt;/p&gt;
{
+                throw new IllegalStateException(&quot;Unknown metadata version: &quot; + usedVersion
+                    + &quot;; latest supported version: &quot; + AssignmentInfo.LATEST_SUPPORTED_VERSION + 1);
+            }
&lt;p&gt;+&lt;br/&gt;
+            final AssignmentInfo info = AssignmentInfo.decode(&lt;br/&gt;
+                assignment.userData().putInt(0, AssignmentInfo.LATEST_SUPPORTED_VERSION));&lt;br/&gt;
+&lt;br/&gt;
+            final List&amp;lt;TopicPartition&amp;gt; partitions = new ArrayList&amp;lt;&amp;gt;(assignment.partitions());&lt;br/&gt;
+            Collections.sort(partitions, PARTITION_COMPARATOR);&lt;br/&gt;
+&lt;br/&gt;
+            // version 1 field&lt;br/&gt;
+            final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+            // version 2 fields&lt;br/&gt;
+            final Map&amp;lt;TopicPartition, PartitionInfo&amp;gt; topicToPartitionInfo = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+            final Map&amp;lt;HostInfo, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; partitionsByHost;&lt;br/&gt;
+&lt;br/&gt;
+            processLatestVersionAssignment(info, partitions, activeTasks, topicToPartitionInfo);&lt;br/&gt;
+            partitionsByHost = info.partitionsByHost();&lt;br/&gt;
+&lt;br/&gt;
+            final TaskManager taskManager = taskManger();&lt;br/&gt;
+            taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));&lt;br/&gt;
+            taskManager.setPartitionsByHostState(partitionsByHost);&lt;br/&gt;
+            taskManager.setAssignmentMetadata(activeTasks, info.standbyTasks());&lt;br/&gt;
+            taskManager.updateSubscriptionsFromAssignment(partitions);&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public Map&amp;lt;String, Assignment&amp;gt; assign(final Cluster metadata,&lt;br/&gt;
+                                              final Map&amp;lt;String, Subscription&amp;gt; subscriptions) {&lt;br/&gt;
+            Map&amp;lt;String, Assignment&amp;gt; assignment = null;&lt;br/&gt;
+&lt;br/&gt;
+            final Map&amp;lt;String, Subscription&amp;gt; downgradedSubscriptions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+            for (final Subscription subscription : subscriptions.values()) {&lt;br/&gt;
+                final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());&lt;br/&gt;
+                if (info.version() &amp;lt; SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1) &lt;/p&gt;
{
+                    assignment = super.assign(metadata, subscriptions);
+                    break;
+                }
&lt;p&gt;+            }&lt;br/&gt;
+&lt;br/&gt;
+            boolean bumpUsedVersion = false;&lt;br/&gt;
+            final boolean bumpSupportedVersion;&lt;br/&gt;
+            if (assignment != null) &lt;/p&gt;
{
+                bumpSupportedVersion = supportedVersions.size() == 1 &amp;amp;&amp;amp; supportedVersions.iterator().next() == SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1;
+            }
&lt;p&gt; else {&lt;br/&gt;
+                for (final Map.Entry&amp;lt;String, Subscription&amp;gt; entry : subscriptions.entrySet()) &lt;/p&gt;
{
+                    final Subscription subscription = entry.getValue();
+
+                    final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData()
+                        .putInt(0, SubscriptionInfo.LATEST_SUPPORTED_VERSION)
+                        .putInt(4, SubscriptionInfo.LATEST_SUPPORTED_VERSION));
+
+                    downgradedSubscriptions.put(
+                        entry.getKey(),
+                        new Subscription(
+                            subscription.topics(),
+                            new SubscriptionInfo(
+                                info.processId(),
+                                info.prevTasks(),
+                                info.standbyTasks(),
+                                info.userEndPoint())
+                                .encode()));
+                }
&lt;p&gt;+                assignment = super.assign(metadata, downgradedSubscriptions);&lt;br/&gt;
+                bumpUsedVersion = true;&lt;br/&gt;
+                bumpSupportedVersion = true;&lt;br/&gt;
+            }&lt;br/&gt;
+&lt;br/&gt;
+            final Map&amp;lt;String, Assignment&amp;gt; newAssignment = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+            for (final Map.Entry&amp;lt;String, Assignment&amp;gt; entry : assignment.entrySet()) &lt;/p&gt;
{
+                final Assignment singleAssignment = entry.getValue();
+                newAssignment.put(
+                    entry.getKey(),
+                    new Assignment(
+                        singleAssignment.partitions(),
+                        new FutureAssignmentInfo(
+                            bumpUsedVersion,
+                            bumpSupportedVersion,
+                            singleAssignment.userData())
+                            .encode()));
+            }
&lt;p&gt;+&lt;br/&gt;
+            return newAssignment;&lt;br/&gt;
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static class FutureSubscriptionInfo extends SubscriptionInfo {&lt;br/&gt;
+        // for testing only; don&apos;t apply version checks&lt;br/&gt;
+        FutureSubscriptionInfo(final int version,&lt;br/&gt;
+                               final int latestSupportedVersion,&lt;br/&gt;
+                               final UUID processId,&lt;br/&gt;
+                               final Set&amp;lt;TaskId&amp;gt; prevTasks,&lt;br/&gt;
+                               final Set&amp;lt;TaskId&amp;gt; standbyTasks,&lt;br/&gt;
+                               final String userEndPoint) &lt;/p&gt;
{
+            super(version, latestSupportedVersion, processId, prevTasks, standbyTasks, userEndPoint);
+        }
&lt;p&gt;+&lt;br/&gt;
+        public ByteBuffer encode() {&lt;br/&gt;
+            if (version() &amp;lt;= SubscriptionInfo.LATEST_SUPPORTED_VERSION) &lt;/p&gt;
{
+                final ByteBuffer buf = super.encode();
+                // super.encode() always encodes `LATEST_SUPPORTED_VERSION` as &quot;latest supported version&quot;
+                // need to update to future version
+                buf.putInt(4, latestSupportedVersion());
+                return buf;
+            }
&lt;p&gt;+&lt;br/&gt;
+            final ByteBuffer buf = encodeFutureVersion();&lt;br/&gt;
+            buf.rewind();&lt;br/&gt;
+            return buf;&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        private ByteBuffer encodeFutureVersion() &lt;/p&gt;
{
+            final byte[] endPointBytes = prepareUserEndPoint();
+
+            final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeByteLength(endPointBytes));
+
+            buf.putInt(LATEST_SUPPORTED_VERSION + 1); // used version
+            buf.putInt(LATEST_SUPPORTED_VERSION + 1); // supported version
+            encodeClientUUID(buf);
+            encodeTasks(buf, prevTasks());
+            encodeTasks(buf, standbyTasks());
+            encodeUserEndPoint(buf, endPointBytes);
+
+            return buf;
+        }
&lt;p&gt;+&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private static class FutureAssignmentInfo extends AssignmentInfo {&lt;br/&gt;
+        private final boolean bumpUsedVersion;&lt;br/&gt;
+        private final boolean bumpSupportedVersion;&lt;br/&gt;
+        final ByteBuffer originalUserMetadata;&lt;br/&gt;
+&lt;br/&gt;
+        private FutureAssignmentInfo(final boolean bumpUsedVersion,&lt;br/&gt;
+                                     final boolean bumpSupportedVersion,&lt;br/&gt;
+                                     final ByteBuffer bytes) &lt;/p&gt;
{
+            this.bumpUsedVersion = bumpUsedVersion;
+            this.bumpSupportedVersion = bumpSupportedVersion;
+            originalUserMetadata = bytes;
+        }
&lt;p&gt;+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public ByteBuffer encode() {&lt;br/&gt;
+            final ByteArrayOutputStream baos = new ByteArrayOutputStream();&lt;br/&gt;
+&lt;br/&gt;
+            originalUserMetadata.rewind();&lt;br/&gt;
+&lt;br/&gt;
+            try (final DataOutputStream out = new DataOutputStream(baos)) {&lt;br/&gt;
+                if (bumpUsedVersion) &lt;/p&gt;
{
+                    originalUserMetadata.getInt(); // discard original used version
+                    out.writeInt(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    out.writeInt(originalUserMetadata.getInt());
+                }
&lt;p&gt;+                if (bumpSupportedVersion) &lt;/p&gt;
{
+                    originalUserMetadata.getInt(); // discard original supported version
+                    out.writeInt(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1);
+                }
&lt;p&gt;+&lt;br/&gt;
+                try {&lt;br/&gt;
+                    while (true) &lt;/p&gt;
{
+                        out.write(originalUserMetadata.get());
+                    }
&lt;p&gt;+                } catch (final BufferUnderflowException expectedWhenAllDataCopied) { }&lt;br/&gt;
+&lt;br/&gt;
+                out.flush();&lt;br/&gt;
+                out.close();&lt;br/&gt;
+&lt;br/&gt;
+                return ByteBuffer.wrap(baos.toByteArray());&lt;br/&gt;
+            } catch (final IOException ex) &lt;/p&gt;
{
+                throw new TaskAssignmentException(&quot;Failed to encode AssignmentInfo&quot;, ex);
+            }
&lt;p&gt;+        }&lt;br/&gt;
+    }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py&lt;br/&gt;
index f268ab8de59..1d8ed270cc5 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/services/streams.py&lt;br/&gt;
+++ b/tests/kafkatest/services/streams.py&lt;br/&gt;
@@ -21,7 +21,7 @@&lt;br/&gt;
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin&lt;br/&gt;
 from kafkatest.services.kafka import KafkaConfig&lt;br/&gt;
 from kafkatest.services.monitor.jmx import JmxMixin&lt;br/&gt;
-from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1&lt;br/&gt;
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, LATEST_1_0, LATEST_1_1&lt;/p&gt;

&lt;p&gt; STATE_DIR = &quot;state.dir&quot;&lt;/p&gt;

&lt;p&gt;@@ -52,6 +52,33 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):&lt;br/&gt;
         &quot;streams_stderr&quot;: &lt;/p&gt;
{
             &quot;path&quot;: STDERR_FILE,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.1&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.1&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.2&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.2&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_log.3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: LOG_FILE + &quot;.3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stdout.3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDOUT_FILE + &quot;.3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
+        &quot;streams_stderr.3&quot;: &lt;/p&gt;
{
+            &quot;path&quot;: STDERR_FILE + &quot;.3&quot;,
+            &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
         &quot;streams_log.0-1&quot;: &lt;/p&gt;
{
             &quot;path&quot;: LOG_FILE + &quot;.0-1&quot;,
             &quot;collect_default&quot;: True}
&lt;p&gt;,&lt;br/&gt;
@@ -412,17 +439,26 @@ def set_version(self, kafka_streams_version):&lt;br/&gt;
     def set_upgrade_from(self, upgrade_from):&lt;br/&gt;
         self.UPGRADE_FROM = upgrade_from&lt;/p&gt;

&lt;p&gt;+    def set_upgrade_to(self, upgrade_to):&lt;br/&gt;
+        self.UPGRADE_TO = upgrade_to&lt;br/&gt;
+&lt;br/&gt;
     def prop_file(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;properties = 
{STATE_DIR: self.PERSISTENT_ROOT}
&lt;p&gt;+        properties = &lt;/p&gt;
{streams_property.STATE_DIR: self.PERSISTENT_ROOT,
+                      streams_property.KAFKA_SERVERS: self.kafka.bootstrap_servers()}
&lt;p&gt;         if self.UPGRADE_FROM is not None:&lt;br/&gt;
             properties&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;upgrade.from&amp;#39;&amp;#93;&lt;/span&gt; = self.UPGRADE_FROM&lt;br/&gt;
+        if self.UPGRADE_TO == &quot;future_version&quot;:&lt;br/&gt;
+            properties&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;test.future.metadata&amp;#39;&amp;#93;&lt;/span&gt; = &quot;any_value&quot;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         cfg = KafkaConfig(**properties)&lt;br/&gt;
         return cfg.render()&lt;/p&gt;

&lt;p&gt;     def start_cmd(self, node):&lt;br/&gt;
         args = self.args.copy()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        if self.KAFKA_STREAMS_VERSION in &lt;span class=&quot;error&quot;&gt;&amp;#91;str(LATEST_0_10_0), str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1)&amp;#93;&lt;/span&gt;:&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.bootstrap_servers()&lt;br/&gt;
+        else:&lt;br/&gt;
+            args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;kafka&amp;#39;&amp;#93;&lt;/span&gt; = &quot;&quot;&lt;br/&gt;
         if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):&lt;br/&gt;
             args&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;zk&amp;#39;&amp;#93;&lt;/span&gt; = self.kafka.zk.connect_setting()&lt;br/&gt;
         else:&lt;br/&gt;
@@ -437,7 +473,7 @@ def start_cmd(self, node):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         cmd = &quot;( export KAFKA_LOG4J_OPTS=\&quot;-Dlog4j.configuration=&lt;a href=&quot;file:%(log4j)s&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:%(log4j)s\&lt;/a&gt;&quot;; &quot; \&lt;br/&gt;
               &quot;INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s &quot; \&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot; %(kafka_run_class)s %(streams_class_name)s  %(kafka)s %(zk)s %(config_file)s &quot; \&lt;br/&gt;
+              &quot; %(kafka_run_class)s %(streams_class_name)s %(kafka)s %(zk)s %(config_file)s &quot; \&lt;br/&gt;
               &quot; &amp;amp; echo $! &amp;gt;&amp;amp;3 ) 1&amp;gt;&amp;gt; %(stdout)s 2&amp;gt;&amp;gt; %(stderr)s 3&amp;gt; %(pidfile)s&quot; % args&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         self.logger.info(&quot;Executing: &quot; + cmd)&lt;br/&gt;
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
index debe85fd7e2..41134672e98 100644&lt;br/&gt;
&amp;#8212; a/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py&lt;br/&gt;
@@ -48,6 +48,7 @@ def _&lt;em&gt;init&lt;/em&gt;_(self, test_context):&lt;br/&gt;
             &apos;data&apos; : &lt;/p&gt;
{ &apos;partitions&apos;: 5 }
&lt;p&gt;,&lt;br/&gt;
         }&lt;br/&gt;
         self.leader = None&lt;br/&gt;
+        self.leader_counter = {}&lt;/p&gt;

&lt;p&gt;     def perform_broker_upgrade(self, to_version):&lt;br/&gt;
         self.logger.info(&quot;First pass bounce - rolling broker upgrade&quot;)&lt;br/&gt;
@@ -158,7 +159,7 @@ def test_simple_upgrade_downgrade(self, from_version, to_version):&lt;br/&gt;
         random.shuffle(self.processors)&lt;br/&gt;
         for p in self.processors:&lt;br/&gt;
             p.CLEAN_NODE_ENABLED = False&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.do_rolling_bounce(p, None, to_version, counter)&lt;br/&gt;
+            self.do_stop_start_bounce(p, None, to_version, counter)&lt;br/&gt;
             counter = counter + 1&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;shutdown&lt;br/&gt;
@@ -176,8 +177,7 @@ def test_simple_upgrade_downgrade(self, from_version, to_version):&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;         self.driver.stop()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;#@matrix(from_version=metadata_1_versions, to_version=backward_compatible_metadata_2_versions)&lt;/li&gt;
	&lt;li&gt;@ignore&lt;br/&gt;
+    @matrix(from_version=metadata_1_versions, to_version=backward_compatible_metadata_2_versions)&lt;br/&gt;
     @matrix(from_version=metadata_1_versions, to_version=metadata_3_versions)&lt;br/&gt;
     @matrix(from_version=metadata_2_versions, to_version=metadata_3_versions)&lt;br/&gt;
     def test_metadata_upgrade(self, from_version, to_version):&lt;br/&gt;
@@ -209,13 +209,70 @@ def test_metadata_upgrade(self, from_version, to_version):&lt;br/&gt;
         random.shuffle(self.processors)&lt;br/&gt;
         for p in self.processors:&lt;br/&gt;
             p.CLEAN_NODE_ENABLED = False&lt;/li&gt;
	&lt;li&gt;self.do_rolling_bounce(p, from_version&lt;span class=&quot;error&quot;&gt;&amp;#91;:-2&amp;#93;&lt;/span&gt;, to_version, counter)&lt;br/&gt;
+            self.do_stop_start_bounce(p, from_version&lt;span class=&quot;error&quot;&gt;&amp;#91;:-2&amp;#93;&lt;/span&gt;, to_version, counter)&lt;br/&gt;
             counter = counter + 1&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;second rolling bounce&lt;br/&gt;
         random.shuffle(self.processors)&lt;br/&gt;
         for p in self.processors:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;self.do_rolling_bounce(p, None, to_version, counter)&lt;br/&gt;
+            self.do_stop_start_bounce(p, None, to_version, counter)&lt;br/&gt;
+            counter = counter + 1&lt;br/&gt;
+&lt;br/&gt;
+        # shutdown&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+        self.driver.wait()&lt;br/&gt;
+&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            node = p.node&lt;br/&gt;
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:&lt;br/&gt;
+                p.stop()&lt;br/&gt;
+                monitor.wait_until(&quot;UPGRADE-TEST-CLIENT-CLOSED&quot;,&lt;br/&gt;
+                                   timeout_sec=60,&lt;br/&gt;
+                                   err_msg=&quot;Never saw output &apos;UPGRADE-TEST-CLIENT-CLOSED&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.stop()&lt;br/&gt;
+&lt;br/&gt;
+    def test_version_probing_upgrade(self):&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+        Starts 3 KafkaStreams instances, and upgrades one-by-one to &quot;future version&quot;&lt;br/&gt;
+        &quot;&quot;&quot;&lt;br/&gt;
+&lt;br/&gt;
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)&lt;br/&gt;
+        self.zk.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)&lt;br/&gt;
+        self.kafka.start()&lt;br/&gt;
+&lt;br/&gt;
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)&lt;br/&gt;
+        self.driver.disable_auto_terminate()&lt;br/&gt;
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)&lt;br/&gt;
+&lt;br/&gt;
+        self.driver.start()&lt;br/&gt;
+        self.start_all_nodes_with(&quot;&quot;) # run with TRUNK&lt;br/&gt;
+&lt;br/&gt;
+        self.processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        self.old_processors = &lt;span class=&quot;error&quot;&gt;&amp;#91;self.processor1, self.processor2, self.processor3&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        self.upgraded_processors = []&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt; = 2&lt;br/&gt;
+&lt;br/&gt;
+        self.update_leader()&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt; = 0&lt;br/&gt;
+        self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;self.leader&amp;#93;&lt;/span&gt; = 3&lt;br/&gt;
+&lt;br/&gt;
+        counter = 1&lt;br/&gt;
+        current_generation = 3&lt;br/&gt;
+&lt;br/&gt;
+        random.seed()&lt;br/&gt;
+        random.shuffle(self.processors)&lt;br/&gt;
+&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            p.CLEAN_NODE_ENABLED = False&lt;br/&gt;
+            current_generation = self.do_rolling_bounce(p, counter, current_generation)&lt;br/&gt;
             counter = counter + 1&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;shutdown&lt;br/&gt;
@@ -233,6 +290,27 @@ def test_metadata_upgrade(self, from_version, to_version):&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;         self.driver.stop()&lt;/p&gt;

&lt;p&gt;+    def update_leader(self):&lt;br/&gt;
+        self.leader = None&lt;br/&gt;
+        retries = 10&lt;br/&gt;
+        while retries &amp;gt; 0:&lt;br/&gt;
+            for p in self.processors:&lt;br/&gt;
+                found = list(p.node.account.ssh_capture(&quot;grep \&quot;Finished assignment for group\&quot; %s&quot; % p.LOG_FILE, allow_fail=True))&lt;br/&gt;
+                if len(found) == self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt; + 1:&lt;br/&gt;
+                    if self.leader is not None:&lt;br/&gt;
+                        raise Exception(&quot;Could not uniquely identify leader&quot;)&lt;br/&gt;
+                    self.leader = p&lt;br/&gt;
+                    self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt; = self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt; + 1&lt;br/&gt;
+&lt;br/&gt;
+            if self.leader is None:&lt;br/&gt;
+                retries = retries - 1&lt;br/&gt;
+                time.sleep(5)&lt;br/&gt;
+            else:&lt;br/&gt;
+                break&lt;br/&gt;
+&lt;br/&gt;
+        if self.leader is None:&lt;br/&gt;
+            raise Exception(&quot;Could not identify leader&quot;)&lt;br/&gt;
+&lt;br/&gt;
     def start_all_nodes_with(self, version):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;start first with &amp;lt;version&amp;gt;&lt;br/&gt;
         self.prepare_for(self.processor1, version)&lt;br/&gt;
@@ -293,7 +371,7 @@ def prepare_for(processor, version):&lt;br/&gt;
         else:&lt;br/&gt;
             processor.set_version(version)&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
+    def do_stop_start_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
         first_other_processor = None&lt;br/&gt;
         second_other_processor = None&lt;br/&gt;
         for p in self.processors:&lt;br/&gt;
@@ -361,3 +439,120 @@ def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):&lt;br/&gt;
                         monitor.wait_until(&quot;processed 100 records from topic&quot;,&lt;br/&gt;
                                            timeout_sec=60,&lt;br/&gt;
                                            err_msg=&quot;Never saw output &apos;processed 100 records from topic&apos; on&quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+    def do_rolling_bounce(self, processor, counter, current_generation):&lt;br/&gt;
+        first_other_processor = None&lt;br/&gt;
+        second_other_processor = None&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            if p != processor:&lt;br/&gt;
+                if first_other_processor is None:&lt;br/&gt;
+                    first_other_processor = p&lt;br/&gt;
+                else:&lt;br/&gt;
+                    second_other_processor = p&lt;br/&gt;
+&lt;br/&gt;
+        node = processor.node&lt;br/&gt;
+        first_other_node = first_other_processor.node&lt;br/&gt;
+        second_other_node = second_other_processor.node&lt;br/&gt;
+&lt;br/&gt;
+        with first_other_node.account.monitor_log(first_other_processor.LOG_FILE) as first_other_monitor:&lt;br/&gt;
+            with second_other_node.account.monitor_log(second_other_processor.LOG_FILE) as second_other_monitor:&lt;br/&gt;
+                # stop processor&lt;br/&gt;
+                processor.stop()&lt;br/&gt;
+                node.account.ssh_capture(&quot;grep UPGRADE-TEST-CLIENT-CLOSED %s&quot; % processor.STDOUT_FILE, allow_fail=False)&lt;br/&gt;
+&lt;br/&gt;
+                node.account.ssh(&quot;mv &quot; + processor.STDOUT_FILE + &quot; &quot; + processor.STDOUT_FILE + &quot;.&quot; + str(counter), allow_fail=False)&lt;br/&gt;
+                node.account.ssh(&quot;mv &quot; + processor.STDERR_FILE + &quot; &quot; + processor.STDERR_FILE + &quot;.&quot; + str(counter), allow_fail=False)&lt;br/&gt;
+                node.account.ssh(&quot;mv &quot; + processor.LOG_FILE + &quot; &quot; + processor.LOG_FILE + &quot;.&quot; + str(counter), allow_fail=False)&lt;br/&gt;
+                self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;processor&amp;#93;&lt;/span&gt; = 0&lt;br/&gt;
+&lt;br/&gt;
+                with node.account.monitor_log(processor.LOG_FILE) as log_monitor:&lt;br/&gt;
+                    processor.set_upgrade_to(&quot;future_version&quot;)&lt;br/&gt;
+                    processor.start()&lt;br/&gt;
+                    self.old_processors.remove(processor)&lt;br/&gt;
+                    self.upgraded_processors.append(processor)&lt;br/&gt;
+&lt;br/&gt;
+                    current_generation = current_generation + 1&lt;br/&gt;
+&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Kafka version : &quot; + str(DEV_VERSION),&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect Kafka Streams version &quot; + str(DEV_VERSION) + &quot; in &quot; + str(node.account))&lt;br/&gt;
+                    log_monitor.offset = 5&lt;br/&gt;
+                    log_monitor.wait_until(&quot;partition\.assignment\.strategy = [org\.apache\.kafka\.streams\.tests\.StreamsUpgradeTest$FutureStreamsPartitionAssignor]&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect FutureStreamsPartitionAssignor in &quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Successfully joined group with generation &quot; + str(current_generation),&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Never saw output &apos;Successfully joined group with generation &quot; + str(current_generation) + &quot;&apos; on&quot; + str(node.account))&lt;br/&gt;
+                    first_other_monitor.wait_until(&quot;Successfully joined group with generation &quot; + str(current_generation),&lt;br/&gt;
+                                                   timeout_sec=60,&lt;br/&gt;
+                                                   err_msg=&quot;Never saw output &apos;Successfully joined group with generation &quot; + str(current_generation) + &quot;&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                    second_other_monitor.wait_until(&quot;Successfully joined group with generation &quot; + str(current_generation),&lt;br/&gt;
+                                                    timeout_sec=60,&lt;br/&gt;
+                                                    err_msg=&quot;Never saw output &apos;Successfully joined group with generation &quot; + str(current_generation) + &quot;&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+&lt;br/&gt;
+                    if processor == self.leader:&lt;br/&gt;
+                        self.update_leader()&lt;br/&gt;
+                    else:&lt;br/&gt;
+                        self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;self.leader&amp;#93;&lt;/span&gt; = self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;self.leader&amp;#93;&lt;/span&gt; + 1&lt;br/&gt;
+&lt;br/&gt;
+                    if processor == self.leader:&lt;br/&gt;
+                        leader_monitor = log_monitor&lt;br/&gt;
+                    elif first_other_processor == self.leader:&lt;br/&gt;
+                        leader_monitor = first_other_monitor&lt;br/&gt;
+                    elif second_other_processor == self.leader:&lt;br/&gt;
+                        leader_monitor = second_other_monitor&lt;br/&gt;
+                    else:&lt;br/&gt;
+                        raise Exception(&quot;Could not identify leader.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+                    monitors = {}&lt;br/&gt;
+                    monitors&lt;span class=&quot;error&quot;&gt;&amp;#91;processor&amp;#93;&lt;/span&gt; = log_monitor&lt;br/&gt;
+                    monitors&lt;span class=&quot;error&quot;&gt;&amp;#91;first_other_processor&amp;#93;&lt;/span&gt; = first_other_monitor&lt;br/&gt;
+                    monitors&lt;span class=&quot;error&quot;&gt;&amp;#91;second_other_processor&amp;#93;&lt;/span&gt; = second_other_monitor&lt;br/&gt;
+&lt;br/&gt;
+                    leader_monitor.wait_until(&quot;Received a future (version probing) subscription (version: 4). Sending empty assignment back (with supported version 3).&quot;,&lt;br/&gt;
+                                              timeout_sec=60,&lt;br/&gt;
+                                              err_msg=&quot;Could not detect &apos;version probing&apos; attempt at leader &quot; + str(self.leader.node.account))&lt;br/&gt;
+&lt;br/&gt;
+                    if len(self.old_processors) &amp;gt; 0:&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Sent a version 4 subscription and got version 3 assignment back (successful version probing). Downgrading subscription metadata to received version and trigger new rebalance.&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect &apos;successful version probing&apos; at upgrading node &quot; + str(node.account))&lt;br/&gt;
+                    else:&lt;br/&gt;
+                        log_monitor.wait_until(&quot;Sent a version 4 subscription and got version 3 assignment back (successful version probing). Setting subscription metadata to leaders supported version 4 and trigger new rebalance.&quot;,&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Could not detect &apos;successful version probing with upgraded leader&apos; at upgrading node &quot; + str(node.account))&lt;br/&gt;
+                        first_other_monitor.wait_until(&quot;Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance.&quot;,&lt;br/&gt;
+                                                       timeout_sec=60,&lt;br/&gt;
+                                                       err_msg=&quot;Never saw output &apos;Upgrade metadata to version 4&apos; on&quot; + str(first_other_node.account))&lt;br/&gt;
+                        second_other_monitor.wait_until(&quot;Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance.&quot;,&lt;br/&gt;
+                                                        timeout_sec=60,&lt;br/&gt;
+                                                        err_msg=&quot;Never saw output &apos;Upgrade metadata to version 4&apos; on&quot; + str(second_other_node.account))&lt;br/&gt;
+&lt;br/&gt;
+                    log_monitor.wait_until(&quot;Version probing detected. Triggering new rebalance.&quot;,&lt;br/&gt;
+                                           timeout_sec=60,&lt;br/&gt;
+                                           err_msg=&quot;Could not detect &apos;Triggering new rebalance&apos; at upgrading node &quot; + str(node.account))&lt;br/&gt;
+&lt;br/&gt;
+                    # version probing should trigger second rebalance&lt;br/&gt;
+                    current_generation = current_generation + 1&lt;br/&gt;
+&lt;br/&gt;
+                    for p in self.processors:&lt;br/&gt;
+                        monitors&lt;span class=&quot;error&quot;&gt;&amp;#91;p&amp;#93;&lt;/span&gt;.wait_until(&quot;Successfully joined group with generation &quot; + str(current_generation),&lt;br/&gt;
+                                               timeout_sec=60,&lt;br/&gt;
+                                               err_msg=&quot;Never saw output &apos;Successfully joined group with generation &quot; + str(current_generation) + &quot;&apos; on&quot; + str(p.node.account))&lt;br/&gt;
+&lt;br/&gt;
+                    if processor == self.leader:&lt;br/&gt;
+                        self.update_leader()&lt;br/&gt;
+                    else:&lt;br/&gt;
+                        self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;self.leader&amp;#93;&lt;/span&gt; = self.leader_counter&lt;span class=&quot;error&quot;&gt;&amp;#91;self.leader&amp;#93;&lt;/span&gt; + 1&lt;br/&gt;
+&lt;br/&gt;
+                    if self.leader in self.old_processors or len(self.old_processors) &amp;gt; 0:&lt;br/&gt;
+                        self.verify_metadata_no_upgraded_yet()&lt;br/&gt;
+&lt;br/&gt;
+        return current_generation&lt;br/&gt;
+&lt;br/&gt;
+    def verify_metadata_no_upgraded_yet(self):&lt;br/&gt;
+        for p in self.processors:&lt;br/&gt;
+            found = list(p.node.account.ssh_capture(&quot;grep \&quot;Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance.\&quot; &quot; + p.LOG_FILE, allow_fail=True))&lt;br/&gt;
+            if len(found) &amp;gt; 0:&lt;br/&gt;
+                raise Exception(&quot;Kafka Streams failed with &apos;group member upgraded to metadata 4 too early&apos;&quot;)&lt;br/&gt;
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py&lt;br/&gt;
index 7823efac1d4..0ed29a34968 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/tests/kafkatest/version.py&lt;br/&gt;
+++ b/tests/kafkatest/version.py&lt;br/&gt;
@@ -61,7 +61,7 @@ def get_version(node=None):&lt;br/&gt;
         return DEV_BRANCH&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; DEV_BRANCH = KafkaVersion(&quot;dev&quot;)&lt;br/&gt;
-DEV_VERSION = KafkaVersion(&quot;1.2.0-SNAPSHOT&quot;)&lt;br/&gt;
+DEV_VERSION = KafkaVersion(&quot;2.0.0-SNAPSHOT&quot;)&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;0.8.2.x versions&lt;br/&gt;
 V_0_8_2_1 = KafkaVersion(&quot;0.8.2.1&quot;)&lt;/li&gt;
&lt;/ol&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16939878" author="sarzhynskyi" created="Sat, 28 Sep 2019 06:31:56 +0000"  >&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;we got error &lt;b&gt;Kafka Streams error &#8220;TaskAssignmentException: unable to decode subscription data: version=4&#8221;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;During deployment with only changed Kafka-Streams version from&#160;&lt;tt&gt;1.1.1&lt;/tt&gt;&#160;to&#160;&lt;tt&gt;2.x.x&lt;/tt&gt;&#160;(without changing&#160;&lt;em&gt;&lt;tt&gt;application.id&lt;/tt&gt;&lt;/em&gt;), we got exceptions on app node with older Kafka-Streams version and, as a result, Kafka streams changed state to error and closed, meanwhile app node with new Kafka-Streams version consumes messages fine.&lt;/p&gt;

&lt;p&gt;If we upgrade from&#160;&lt;tt&gt;1.1.1&lt;/tt&gt;&#160;to&#160;&lt;tt&gt;2.0.0&lt;/tt&gt;, got error&#160;&lt;em&gt;&lt;tt&gt;unable to decode subscription data: version=3&lt;/tt&gt;&lt;/em&gt;; if from&#160;&lt;tt&gt;1.1.1&lt;/tt&gt;&#160;to&#160;&lt;tt&gt;2.3.0&lt;/tt&gt;:&#160;&lt;em&gt;&lt;tt&gt;unable to decode subscription data: version=4&lt;/tt&gt;&lt;/em&gt;. It might be really painful during canary deployment, e.g. we have 3 app nodes with previous Kafka-Streams version, and when we add one more node with a new version, all existing 3 nodes will be in error state. Issue is reproducible in 100% cases and not depend on the number of app instances with previous Kafka Streams version (an error occurred for both cases with either one or three app nodes having Kafka Streams 1.1.1, during deployment time the first app node with new Kafka Streams version).&lt;/p&gt;

&lt;p&gt;Error stack trace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
TaskAssignmentException: unable to decode subscription data: version=4
    at org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.decode(SubscriptionInfo.java:128)
    at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.assign(StreamPartitionAssignor.java:297)
    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:358)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:520)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1100(AbstractCoordinator.java:93)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:472)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:455)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:822)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:802)
    at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
    at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
    at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:563)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:390)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:293)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:364)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:316)
    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:290)
    at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1149)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1115)
    at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:831)
    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:788)
    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749)
    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:719)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Issue is reproducible on both Kafka broker versions&#160;&lt;tt&gt;1.1.0&lt;/tt&gt;&#160;and&#160;&lt;tt&gt;2.1.1&lt;/tt&gt;, even with the simple Kafka-Streams DSL example:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160;Properties props = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Properties();
 props.put(&lt;span class=&quot;code-quote&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;localhost:9092&quot;&lt;/span&gt;);
 props.put(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.key.serde&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.kafka.common.serialization.Serdes$StringSerde&quot;&lt;/span&gt;);
 props.put(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.value.serde&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.kafka.common.serialization.Serdes$StringSerde&quot;&lt;/span&gt;);
 props.put(&lt;span class=&quot;code-quote&quot;&gt;&quot;application.id&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;xxx&quot;&lt;/span&gt;);
StreamsBuilder streamsBuilder = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamsBuilder();
 streamsBuilder.&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;stream(&lt;span class=&quot;code-quote&quot;&gt;&quot;source&quot;&lt;/span&gt;)
 .mapValues(value -&amp;gt; value + value)
 .to(&lt;span class=&quot;code-quote&quot;&gt;&quot;destination&quot;&lt;/span&gt;);
 KafkaStreams kafkaStreams = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; KafkaStreams(streamsBuilder.build(), props);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Seems it&apos;s&#160;a bug of Kafka Streams.&lt;/p&gt;

&lt;p&gt;As I see, the problem is inside the implementation of Kafka Streams&#160;&lt;tt&gt;1.1.1&lt;/tt&gt;&#160;&lt;em&gt;&lt;tt&gt;org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo&lt;/tt&gt;&lt;/em&gt;&#160;. With new version&#160;&lt;tt&gt;2.3.0&lt;/tt&gt;,&#160;&lt;tt&gt;decode&lt;/tt&gt;&#160;implementation already take into consideration that we might receive newer&#160;&lt;em&gt;&lt;tt&gt;latestSupportedVersion&lt;/tt&gt;&lt;/em&gt;, but still seems new version will not decode appropriately.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12956686">KAFKA-3522</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 7 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3l59z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>