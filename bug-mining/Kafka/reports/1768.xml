<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:05:43 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6269] KTable state restore fails after rebalance</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6269</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;I have the following kafka streams topology:&lt;/p&gt;

&lt;p&gt;entity-B -&amp;gt; map step -&amp;gt; entity-B-exists (with state store)&lt;br/&gt;
entity-A   -&amp;gt; map step -&amp;gt; entity-A-exists (with state store)&lt;/p&gt;

&lt;p&gt;(entity-B-exists, entity-A-exists) -&amp;gt; outer join with state store.&lt;/p&gt;

&lt;p&gt;The topology building code looks like this (some data type, serde, valuemapper, and joiner code omitted):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
def buildTable[V](builder: StreamsBuilder,
                          sourceTopic: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
                          existsTopic: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
                          valueSerde: Serde[V],
                          valueMapper: ValueMapper[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, V]): KTable[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, V] = {

  val stream: KStream[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;] = builder.stream[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;](sourceTopic)
  val transformed: KStream[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, V] = stream.mapValues(valueMapper)
  transformed.to(existsTopic, Produced.`with`(Serdes.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;(), valueSerde))

  val inMemoryStoreName = s&lt;span class=&quot;code-quote&quot;&gt;&quot;$existsTopic-persisted&quot;&lt;/span&gt;

  val materialized = Materialized.as(Stores.inMemoryKeyValueStore(inMemoryStoreName))
      .withKeySerde(Serdes.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;())
      .withValueSerde(valueSerde)
      .withLoggingDisabled()

  builder.table(existsTopic, materialized)
}


val builder = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamsBuilder
val mapToEmptyString: ValueMapper[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;] = (value: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;) =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (value != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &quot;&quot; &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;

val entitiesB: KTable[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, EntityBInfo] =
  buildTable(builder,
             &lt;span class=&quot;code-quote&quot;&gt;&quot;entity-B&quot;&lt;/span&gt;,
             &lt;span class=&quot;code-quote&quot;&gt;&quot;entity-B-exists&quot;&lt;/span&gt;,
             EntityBInfoSerde,
             ListingImagesToEntityBInfo)

val entitiesA: KTable[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;] =
  buildTable(builder, &lt;span class=&quot;code-quote&quot;&gt;&quot;entity-A&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;entity-A-exists&quot;&lt;/span&gt;, Serdes.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;(), mapToEmptyString)

val joiner: ValueJoiner[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, EntityBInfo, EntityDiff] = (a, b) =&amp;gt; EntityDiff.fromJoin(a, b)

val materialized = Materialized.as(Stores.inMemoryKeyValueStore(&lt;span class=&quot;code-quote&quot;&gt;&quot;entity-A-joined-with-entity-B&quot;&lt;/span&gt;))
  .withKeySerde(Serdes.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;())
  .withValueSerde(EntityDiffSerde)
  .withLoggingEnabled(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; java.util.HashMap[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]())

val joined: KTable[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, EntityDiff] = entitiesA.outerJoin(entitiesB, joiner, materialized)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We run 4 processor machines with 30 stream threads each; each topic has 30 partitions so that there is a total of 4 x 30 = 120 partitions to consume. The initial launch of the processor works fine, but when killing one processor and letting him re-join the stream threads leads to some faulty behaviour.&lt;/p&gt;

&lt;p&gt;Fist, the total number of assigned partitions over all processor machines is larger than 120 (sometimes 157, sometimes just 132), so the partition / task assignment seems to assign the same job to different stream threads.&lt;/p&gt;

&lt;p&gt;The processor machines trying to re-join the consumer group fail constantly with the error message of &apos;Detected a task that got migrated to another thread.&apos; We gave the processor half an hour to recover; usually, rebuilding the KTable states take around 20 seconds (with Kafka 0.11.0.1).&lt;/p&gt;

&lt;p&gt;Here are the details of the errors we see:&lt;/p&gt;

&lt;p&gt;stream-thread &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka-processor-6-StreamThread-9&amp;#93;&lt;/span&gt; Detected a task that got migrated to another thread. This implies that this thread missed a rebalance and dropped out of the consumer group. Trying to rejoin the consumer group now.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.kafka.streams.errors.TaskMigratedException: Log end offset of entity-B-exists-0 should not change &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; restoring: old end offset 4750539, current offset 4751388
&amp;gt; StreamsTask taskId: 1_0
&amp;gt; &amp;gt; 	ProcessorTopology:
&amp;gt; 		KSTREAM-SOURCE-0000000008:
&amp;gt; 			topics:		[entity-A-exists]
&amp;gt; 			children:	[KTABLE-SOURCE-0000000009]
&amp;gt; 		KTABLE-SOURCE-0000000009:
&amp;gt; 			states:		[entity-A-exists-persisted]
&amp;gt; 			children:	[KTABLE-JOINTHIS-0000000011]
&amp;gt; 		KTABLE-JOINTHIS-0000000011:
&amp;gt; 			states:		[entity-B-exists-persisted]
&amp;gt; 			children:	[KTABLE-MERGE-0000000010]
&amp;gt; 		KTABLE-MERGE-0000000010:
&amp;gt; 			states:		[entity-A-joined-with-entity-B]
&amp;gt; 		KSTREAM-SOURCE-0000000003:
&amp;gt; 			topics:		[entity-B-exists]
&amp;gt; 			children:	[KTABLE-SOURCE-0000000004]
&amp;gt; 		KTABLE-SOURCE-0000000004:
&amp;gt; 			states:		[entity-B-exists-persisted]
&amp;gt; 			children:	[KTABLE-JOINOTHER-0000000012]
&amp;gt; 		KTABLE-JOINOTHER-0000000012:
&amp;gt; 			states:		[entity-A-exists-persisted]
&amp;gt; 			children:	[KTABLE-MERGE-0000000010]
&amp;gt; 		KTABLE-MERGE-0000000010:
&amp;gt; 			states:		[entity-A-joined-with-entity-B]
&amp;gt; Partitions [entity-A-exists-0, entity-B-exists-0]

	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restorePartition(StoreChangelogReader.java:242)
	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:83)
	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:263)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:803)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:774)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That one surprises me: the KTable state store entity-B-exists-persisted is rebuilt from entity-B-exists that of course can change while the rebuild is happening, since it the topic entity-B-exists is fed by another stream thread.&lt;/p&gt;

&lt;p&gt;Another one, very similar:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.kafka.streams.errors.TaskMigratedException: Log end offset of entity-A-exists-24 should not change &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; restoring: old end offset 6483978, current offset 6485108
&amp;gt; StreamsTask taskId: 1_24
&amp;gt; &amp;gt; 	ProcessorTopology:
&amp;gt; 		KSTREAM-SOURCE-0000000008:
&amp;gt; 			topics:		[entity-A-exists]
&amp;gt; 			children:	[KTABLE-SOURCE-0000000009]
&amp;gt; 		KTABLE-SOURCE-0000000009:
&amp;gt; 			states:		[entity-A-exists-persisted]
&amp;gt; 			children:	[KTABLE-JOINTHIS-0000000011]
&amp;gt; 		KTABLE-JOINTHIS-0000000011:
&amp;gt; 			states:		[entity-B-exists-persisted]
&amp;gt; 			children:	[KTABLE-MERGE-0000000010]
&amp;gt; 		KTABLE-MERGE-0000000010:
&amp;gt; 			states:		[entity-A-joined-with-entity-B]
&amp;gt; 		KSTREAM-SOURCE-0000000003:
&amp;gt; 			topics:		[entity-B-exists]
&amp;gt; 			children:	[KTABLE-SOURCE-0000000004]
&amp;gt; 		KTABLE-SOURCE-0000000004:
&amp;gt; 			states:		[entity-B-exists-persisted]
&amp;gt; 			children:	[KTABLE-JOINOTHER-0000000012]
&amp;gt; 		KTABLE-JOINOTHER-0000000012:
&amp;gt; 			states:		[entity-A-exists-persisted]
&amp;gt; 			children:	[KTABLE-MERGE-0000000010]
&amp;gt; 		KTABLE-MERGE-0000000010:
&amp;gt; 			states:		[entity-A-joined-with-entity-B]
&amp;gt; Partitions [entity-A-exists-24, entity-B-exists-24]

	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restorePartition(StoreChangelogReader.java:242)
	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:83)
	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:263)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:803)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:774)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again, the topic entity-A-exists is fed by another stream thread.&lt;/p&gt;

&lt;p&gt;We saw around 60000 such errors per minute, as the stream threads continuously try to recover and fail.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13120514">KAFKA-6269</key>
            <summary>KTable state restore fails after rebalance</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bbejeck">Bill Bejeck</assignee>
                                    <reporter username="andreas-schroeder">Andreas Schroeder</reporter>
                        <labels>
                    </labels>
                <created>Thu, 23 Nov 2017 15:55:09 +0000</created>
                <updated>Tue, 16 Jan 2018 23:29:09 +0000</updated>
                            <resolved>Tue, 2 Jan 2018 18:24:17 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.0.1</fixVersion>
                    <fixVersion>1.1.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="16264921" author="mjsax" created="Fri, 24 Nov 2017 04:30:20 +0000"  >&lt;p&gt;Thanks for reporting this. If you get the exception &lt;tt&gt;Log end offset of entity-B-exists-0 should not change while restoring: old end offset 4750539, current offset 4751388&lt;/tt&gt;, this means that a thread started to recover a state from a changelog topic and the endOffset moved during the process &amp;#8211; this should never happen (only if the task was migrated to another thread and this other thread writes into the changelog topic). If the state is not migrated, the thread that restores would be the only one that is &quot;allowed&quot; to write into the changelog; but as long it restores it does not write.&lt;/p&gt;

&lt;p&gt;What I could think of is a bug, that relates to an optimization we do: As you read KTables from source topics, there is no additional changelog topic as the source topic can be used to recreate the state (the error message shows that the source topic is used for store recovery). Thus, a upstream processor that write to the source topic can of course append data to this topic &amp;#8211; we actually have a guard for this case, but we change some code here, maybe introducing a bug that this guard does not work properly anymore: thus, the restoring thread &quot;thinks&quot; it reads a changelog topic (while it does read a source topic) for recovery and fails even if it shouldn&apos;t. I&apos;ll try to reproduce this locally and cycle back to you.&lt;/p&gt;</comment>
                            <comment id="16264926" author="mjsax" created="Fri, 24 Nov 2017 05:12:59 +0000"  >&lt;p&gt;Had a look into the code, and I think I understand the issue. The guard is not working as expected (what was my suspicion).&lt;/p&gt;

&lt;p&gt;The only workaround I can think of atm, would be to read the table source topics as &lt;tt&gt;KStream&lt;/tt&gt; and do a dummy aggregation that only keeps the latest value to transform the &lt;tt&gt;KStream&lt;/tt&gt; into a &lt;tt&gt;KTable&lt;/tt&gt; &amp;#8211; this will create a proper changelog topic and avoid hitting this issue.&lt;/p&gt;

&lt;p&gt;For your special case, as you write the topic with &lt;tt&gt;to()&lt;/tt&gt; you could also replace the &lt;tt&gt;to()&lt;/tt&gt; with a &lt;tt&gt;groupByKey().aggregate()&lt;/tt&gt; and you don&apos;t need the &lt;tt&gt;entitiy-X-exists&lt;/tt&gt; topics anymore. This should result in a no-overhead solution for you, as there will be no repartitioning topic as you only call a &lt;tt&gt;mapValues&lt;/tt&gt; before the &lt;tt&gt;groupBy().aggregation()&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;It would be great if you could confirm if this workaround works. Thx.&lt;/p&gt;</comment>
                            <comment id="16266157" author="guozhang" created="Sun, 26 Nov 2017 19:54:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; What bug have you found that relates to the guard not effective?&lt;/p&gt;</comment>
                            <comment id="16266321" author="mjsax" created="Mon, 27 Nov 2017 02:49:46 +0000"  >&lt;p&gt;In the PR linked below, we changed the &lt;tt&gt;StoreChangelogReader&lt;/tt&gt;. The change in the PR was a fix for regular restore case if EOS is enabled. Because of commit markers in the topic, using the record offset as &quot;end offset&quot; to check if restore is finished is not save and might end up in infinite loop (can dig out the JIRA if you wish). Thus, we return &lt;tt&gt;restoreConsumer.position()&lt;/tt&gt; instead. However, this is wrong for the source-KTable case when the guard fires (originally, we set &lt;tt&gt;offset = record.offset&lt;/tt&gt; if guard fires, but remove this line). Thus, even if we stop restore correctly, we return the wrong offset that is larger then expected end-offset and thus we get the exception.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/commit/eaabb6cd0173c4f6854eb5da39194a7e3fc0162c#diff-46ed6d177221c8778965ecb1b6657be3R264&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/commit/eaabb6cd0173c4f6854eb5da39194a7e3fc0162c#diff-46ed6d177221c8778965ecb1b6657be3R264&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I was thinking about a potential fix. A straight forward fix would not work: returning eagerly if the guard condition is met, and return &lt;tt&gt;record.offset()&lt;/tt&gt; would yield the EOS issue again. But I think, we could return the offset of the next record (the first one that we do not restore anymore), or &lt;tt&gt;restoreConsumer.position()&lt;/tt&gt; if no next record exist (ie, if nobody actually added records to the source-KTable topic.&lt;/p&gt;

&lt;p&gt;WDYT?&lt;/p&gt;</comment>
                            <comment id="16266491" author="andreas-schroeder" created="Mon, 27 Nov 2017 08:18:22 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt;, thanks for the swift response and your workaround proposal. I tried to make that workaround work, but it breaks some of my integration tests. I&apos;m still investigating my tests to determine if this workaround is feasible. I&apos;ll keep you posted.&lt;/p&gt;</comment>
                            <comment id="16269058" author="andreas-schroeder" created="Tue, 28 Nov 2017 17:09:54 +0000"  >&lt;p&gt;Hi again &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt;, I&apos;m back with some news (finally): The issue we are having is that Records with null value are ignored. So deletes won&apos;t propagate to the outer join, so that our business logic doesn&apos;t work any more.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;http://apache.mirror.digionline.de/kafka/1.0.0/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html#aggregate(org.apache.kafka.streams.kstream.Initializer,%20org.apache.kafka.streams.kstream.Aggregator,%20org.apache.kafka.streams.kstream.Materialized)&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KGroupedStream API docs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Any other ideas? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16271399" author="guozhang" created="Wed, 29 Nov 2017 19:28:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andreas-schroeder&quot; class=&quot;user-hover&quot; rel=&quot;andreas-schroeder&quot;&gt;andreas-schroeder&lt;/a&gt; We have spotted the regression bug introduced in 1.0.0 and we are working on a fix that would be included in the coming 1.0.1 release. But the bug-fix release may only come in early December, and I&apos;m wondering how much this issue is blocking your team&apos;s progress. Would you be OK with compiling from &lt;tt&gt;1.0&lt;/tt&gt; branch directly once we have the bug fix in?&lt;/p&gt;</comment>
                            <comment id="16272013" author="mjsax" created="Thu, 30 Nov 2017 01:55:26 +0000"  >&lt;p&gt;About the workaround. You could mask &lt;tt&gt;null&lt;/tt&gt; with a dummy value: &lt;tt&gt;stream.mapValue(/* replace null with custom NULL */).groupByKey.aggregate()&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="16272972" author="andreas-schroeder" created="Thu, 30 Nov 2017 17:16:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; it&apos;s okay for my team to wait or the 1.0.1 release. Until then, we&apos;ll stick to the 0.11.0.1 version we are currently using. The reason to migrate to 1.0.0 was that we are experiencing some unfair task assignment across our stream processor nodes, which leads to some nodes crashing (and immediately being recreated). So our current system runs and we can wait for 1.0.1 Thanks however for giving suggestions on how to proceed! I&apos;ll try &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt;&apos;s suggestion on hiding the null value &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;</comment>
                            <comment id="16280993" author="githubbot" created="Wed, 6 Dec 2017 21:59:15 +0000"  >&lt;p&gt;GitHub user bbejeck opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/4300&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4300&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6269&quot; title=&quot;KTable state restore fails after rebalance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6269&quot;&gt;&lt;del&gt;KAFKA-6269&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;WIP&amp;#93;&lt;/span&gt; DO NOT MERGE attempts to recreate original error&lt;/p&gt;

&lt;p&gt;    *More detailed description of your change,&lt;br/&gt;
    if necessary. The PR title and PR message become&lt;br/&gt;
    the squashed commit message, so use a separate&lt;br/&gt;
    comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;    *Summary of testing strategy (including rationale)&lt;br/&gt;
    for the feature or bug fix. Unit and/or integration&lt;br/&gt;
    tests are expected for any behaviour change and&lt;br/&gt;
    system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/bbejeck/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/bbejeck/kafka&lt;/a&gt; KAFKA_6269_ktable_restore_fails_after_rebalance&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/4300.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4300.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4300&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 107a213af4f1106857bbc0b60dabb0bf5ecd3cc0&lt;br/&gt;
Author: Bill Bejeck &amp;lt;bill@confluent.io&amp;gt;&lt;br/&gt;
Date:   2017-12-06T21:57:58Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6269&quot; title=&quot;KTable state restore fails after rebalance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6269&quot;&gt;&lt;del&gt;KAFKA-6269&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;WIP&amp;#93;&lt;/span&gt; attempts to recreate original error&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16300940" author="githubbot" created="Fri, 22 Dec 2017 04:09:04 +0000"  >&lt;p&gt;mjsax commented on a change in pull request #4300: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6269&quot; title=&quot;KTable state restore fails after rebalance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6269&quot;&gt;&lt;del&gt;KAFKA-6269&lt;/del&gt;&lt;/a&gt;: KTable restore fails after rebalance&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4300#discussion_r158426654&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4300#discussion_r158426654&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -385,6 +379,130 @@ public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestore&lt;br/&gt;
         } catch (final TaskMigratedException expected) &lt;/p&gt;
{ /* ignore */ }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForChangelogTopicEOSEnabled() {&lt;br/&gt;
+        final int totalMessages = 10;&lt;br/&gt;
+        assignPartition(totalMessages, topicPartition);&lt;br/&gt;
+        // records 0..4&lt;br/&gt;
+        addRecords(5, topicPartition, 0);&lt;br/&gt;
+        //EOS enabled commit marker at offset 5 so rest of records 6..10&lt;br/&gt;
+        addRecords(5, topicPartition, 6);&lt;br/&gt;
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;br/&gt;
+&lt;br/&gt;
+        // end offsets should start after commit marker of 5 from above&lt;br/&gt;
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 6L));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+        try &lt;/p&gt;
{
+            changelogReader.restore(active);
+            fail(&quot;Should have thrown task migrated exception&quot;);
+        }
&lt;p&gt; catch (final TaskMigratedException expected) &lt;/p&gt;
{
+            /* ignore */
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionDuringRestoreForChangelogTopicWhenEndOffsetNotExceededEOSEnabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        setupConsumer(totalMessages, topicPartition);
+        // records have offsets of 0..9 10 is commit marker so 11 is end offset
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 11L));
+
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionDuringRestoreForChangelogTopicWhenEndOffsetNotExceededEOSDisabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        setupConsumer(totalMessages, topicPartition);
+
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForSourceTopic() {&lt;br/&gt;
+        final int messages = 10;&lt;br/&gt;
+        setupConsumer(messages, topicPartition);&lt;br/&gt;
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 5L));&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test &amp;#8211; we pass in the `offsetLimit` as 5 in `StateRestorer` below.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16300941" author="githubbot" created="Fri, 22 Dec 2017 04:09:04 +0000"  >&lt;p&gt;mjsax commented on a change in pull request #4300: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6269&quot; title=&quot;KTable state restore fails after rebalance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6269&quot;&gt;&lt;del&gt;KAFKA-6269&lt;/del&gt;&lt;/a&gt;: KTable restore fails after rebalance&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4300#discussion_r158426440&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4300#discussion_r158426440&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -385,6 +379,130 @@ public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestore&lt;br/&gt;
         } catch (final TaskMigratedException expected) &lt;/p&gt;
{ /* ignore */ }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForChangelogTopicEOSEnabled() {&lt;br/&gt;
+        final int totalMessages = 10;&lt;br/&gt;
+        assignPartition(totalMessages, topicPartition);&lt;br/&gt;
+        // records 0..4&lt;br/&gt;
+        addRecords(5, topicPartition, 0);&lt;br/&gt;
+        //EOS enabled commit marker at offset 5 so rest of records 6..10&lt;br/&gt;
+        addRecords(5, topicPartition, 6);&lt;br/&gt;
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   Final question: some test call `consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());`, other don&apos;t. Why?&lt;/p&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16300942" author="githubbot" created="Fri, 22 Dec 2017 04:09:05 +0000"  >&lt;p&gt;mjsax commented on a change in pull request #4300: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6269&quot; title=&quot;KTable state restore fails after rebalance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6269&quot;&gt;&lt;del&gt;KAFKA-6269&lt;/del&gt;&lt;/a&gt;: KTable restore fails after rebalance&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4300#discussion_r158426722&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4300#discussion_r158426722&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -385,6 +379,130 @@ public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestore&lt;br/&gt;
         } catch (final TaskMigratedException expected) &lt;/p&gt;
{ /* ignore */ }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForChangelogTopicEOSEnabled() {&lt;br/&gt;
+        final int totalMessages = 10;&lt;br/&gt;
+        assignPartition(totalMessages, topicPartition);&lt;br/&gt;
+        // records 0..4&lt;br/&gt;
+        addRecords(5, topicPartition, 0);&lt;br/&gt;
+        //EOS enabled commit marker at offset 5 so rest of records 6..10&lt;br/&gt;
+        addRecords(5, topicPartition, 6);&lt;br/&gt;
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;br/&gt;
+&lt;br/&gt;
+        // end offsets should start after commit marker of 5 from above&lt;br/&gt;
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 6L));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+        try &lt;/p&gt;
{
+            changelogReader.restore(active);
+            fail(&quot;Should have thrown task migrated exception&quot;);
+        }
&lt;p&gt; catch (final TaskMigratedException expected) &lt;/p&gt;
{
+            /* ignore */
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionDuringRestoreForChangelogTopicWhenEndOffsetNotExceededEOSEnabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        setupConsumer(totalMessages, topicPartition);
+        // records have offsets of 0..9 10 is commit marker so 11 is end offset
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 11L));
+
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionDuringRestoreForChangelogTopicWhenEndOffsetNotExceededEOSDisabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        setupConsumer(totalMessages, topicPartition);
+
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForSourceTopic() &lt;/p&gt;
{
+        final int messages = 10;
+        setupConsumer(messages, topicPartition);
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 5L));
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(5));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetNotExceededDuringRestoreForSourceTopic() &lt;/p&gt;
{
+        final int messages = 10;
+        setupConsumer(messages, topicPartition);
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 10, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForSourceTopicEOSEnabled() {&lt;br/&gt;
+        final int totalMessages = 10;&lt;br/&gt;
+        assignPartition(totalMessages, topicPartition);&lt;br/&gt;
+        // records 0..4 last offset before commit is 4&lt;br/&gt;
+        addRecords(5, topicPartition, 0);&lt;br/&gt;
+        //EOS enabled so commit marker at offset 5 so records start at 6&lt;br/&gt;
+        addRecords(5, topicPartition, 6);&lt;br/&gt;
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;br/&gt;
+        // commit marker is 5 so ending offset is 6&lt;br/&gt;
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 6L));&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   as above. endOffset should be `12` and passed offset limit in next line should be 6.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16308416" author="githubbot" created="Tue, 2 Jan 2018 17:42:43 +0000"  >&lt;p&gt;guozhangwang closed pull request #4300: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6269&quot; title=&quot;KTable state restore fails after rebalance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6269&quot;&gt;&lt;del&gt;KAFKA-6269&lt;/del&gt;&lt;/a&gt;: KTable restore fails after rebalance&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4300&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4300&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java b/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java&lt;br/&gt;
index 10aedbb93c4..d9085b0697f 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java&lt;br/&gt;
@@ -197,7 +197,7 @@ public synchronized void addRecord(ConsumerRecord&amp;lt;K, V&amp;gt; record) {&lt;br/&gt;
             throw new IllegalStateException(&quot;Cannot add records for a partition that is not assigned to the consumer&quot;);&lt;br/&gt;
         List&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; recs = this.records.get(tp);&lt;br/&gt;
         if (recs == null) &lt;/p&gt;
{
-            recs = new ArrayList&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt;();
+            recs = new ArrayList&amp;lt;&amp;gt;();
             this.records.put(tp, recs);
         }
&lt;p&gt;         recs.add(record);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
index 178d2bb96d0..ba17ce95ede 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java&lt;br/&gt;
@@ -96,7 +96,6 @@ public void register(final StateRestorer restorer) &lt;/p&gt;
{
             restoreConsumer.seekToBeginning(partitions);
         }

&lt;p&gt;-&lt;br/&gt;
         if (needsRestoring.isEmpty()) &lt;/p&gt;
{
             restoreConsumer.unsubscribe();
         }
&lt;p&gt;@@ -174,8 +173,8 @@ private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializ&lt;br/&gt;
             if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) &lt;/p&gt;
{
                 restoreConsumer.seek(restorer.partition(), restorer.checkpoint());
                 logRestoreOffsets(restorer.partition(),
-                        restorer.checkpoint(),
-                        endOffsets.get(restorer.partition()));
+                                  restorer.checkpoint(),
+                                  endOffsets.get(restorer.partition()));
                 restorer.setStartingOffset(restoreConsumer.position(restorer.partition()));
                 restorer.restoreStarted();
             }
&lt;p&gt; else {&lt;br/&gt;
@@ -187,8 +186,8 @@ private void startRestoration(final Map&amp;lt;TopicPartition, StateRestorer&amp;gt; initializ&lt;br/&gt;
         for (final StateRestorer restorer : needsPositionUpdate) &lt;/p&gt;
{
             final long position = restoreConsumer.position(restorer.partition());
             logRestoreOffsets(restorer.partition(),
-                    position,
-                    endOffsets.get(restorer.partition()));
+                              position,
+                              endOffsets.get(restorer.partition()));
             restorer.setStartingOffset(position);
             restorer.restoreStarted();
         }
&lt;p&gt;@@ -252,7 +251,7 @@ private void restorePartition(final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; allRecords,&lt;br/&gt;
         final long pos = processNext(allRecords.records(topicPartition), restorer, endOffset);&lt;br/&gt;
         restorer.setRestoredOffset(pos);&lt;br/&gt;
         if (restorer.hasCompleted(pos, endOffset)) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (pos &amp;gt; endOffset + 1) {&lt;br/&gt;
+            if (pos &amp;gt; endOffset) 
{
                 throw new TaskMigratedException(task, topicPartition, endOffset, pos);
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -271,30 +270,40 @@ private long processNext(final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records,&lt;br/&gt;
                              final StateRestorer restorer,&lt;br/&gt;
                              final Long endOffset) {&lt;br/&gt;
         final List&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; restoreRecords = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long offset = -1;&lt;br/&gt;
-&lt;br/&gt;
+        long nextPosition = -1;&lt;br/&gt;
+        int numberRecords = records.size();&lt;br/&gt;
+        int numberRestored = 0;&lt;br/&gt;
         for (final ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : records) {&lt;/li&gt;
	&lt;li&gt;offset = record.offset();&lt;br/&gt;
+            final long offset = record.offset();&lt;br/&gt;
             if (restorer.hasCompleted(offset, endOffset)) 
{
+                nextPosition = record.offset();
                 break;
             }
&lt;p&gt;+            numberRestored++;&lt;br/&gt;
             if (record.key() != null) &lt;/p&gt;
{
                 restoreRecords.add(KeyValue.pair(record.key(), record.value()));
             }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (offset == -1) {&lt;/li&gt;
	&lt;li&gt;offset = restoreConsumer.position(restorer.partition());&lt;br/&gt;
+&lt;br/&gt;
+        // if we have changelog topic then we should have restored all records in the list&lt;br/&gt;
+        // otherwise if we did not fully restore to that point we need to set nextPosition&lt;br/&gt;
+        // to the position of the restoreConsumer and we&apos;ll cause a TaskMigratedException exception&lt;br/&gt;
+        if (nextPosition == -1 || (restorer.offsetLimit() == Long.MAX_VALUE &amp;amp;&amp;amp; numberRecords != numberRestored)) 
{
+            nextPosition = restoreConsumer.position(restorer.partition());
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (!restoreRecords.isEmpty()) &lt;/p&gt;
{
             restorer.restore(restoreRecords);
-            restorer.restoreBatchCompleted(offset + 1, records.size());
+            restorer.restoreBatchCompleted(nextPosition, records.size());
+
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return restoreConsumer.position(restorer.partition());&lt;br/&gt;
+        return nextPosition;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
     private boolean hasPartition(final TopicPartition topicPartition) {&lt;br/&gt;
         final List&amp;lt;PartitionInfo&amp;gt; partitions = partitionInfo.get(topicPartition.topic());&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KTableSourceTopicRestartIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KTableSourceTopicRestartIntegrationTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..4942b21266d&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KTableSourceTopicRestartIntegrationTest.java&lt;br/&gt;
@@ -0,0 +1,260 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.kafka.streams.integration;&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
+import org.apache.kafka.common.serialization.StringSerializer;&lt;br/&gt;
+import org.apache.kafka.common.utils.Time;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;&lt;br/&gt;
+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.ForeachAction;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KTable;&lt;br/&gt;
+import org.apache.kafka.streams.processor.StateRestoreListener;&lt;br/&gt;
+import org.apache.kafka.streams.processor.WallclockTimestampExtractor;&lt;br/&gt;
+import org.apache.kafka.test.IntegrationTest;&lt;br/&gt;
+import org.apache.kafka.test.TestCondition;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
+import org.junit.After;&lt;br/&gt;
+import org.junit.Before;&lt;br/&gt;
+import org.junit.BeforeClass;&lt;br/&gt;
+import org.junit.ClassRule;&lt;br/&gt;
+import org.junit.Test;&lt;br/&gt;
+import org.junit.experimental.categories.Category;&lt;br/&gt;
+&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
+import java.util.HashMap;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+import java.util.concurrent.ConcurrentHashMap;&lt;br/&gt;
+import java.util.concurrent.ExecutionException;&lt;br/&gt;
+import java.util.concurrent.TimeUnit;&lt;br/&gt;
+&lt;br/&gt;
+@Category(&lt;/p&gt;
{IntegrationTest.class}
&lt;p&gt;)&lt;br/&gt;
+public class KTableSourceTopicRestartIntegrationTest {&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+    private static final int NUM_BROKERS = 3;&lt;br/&gt;
+    private static final String SOURCE_TOPIC = &quot;source-topic&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    @ClassRule&lt;br/&gt;
+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(NUM_BROKERS);&lt;br/&gt;
+    private final Time time = CLUSTER.time;&lt;br/&gt;
+    private KafkaStreams streamsOne;&lt;br/&gt;
+    private final StreamsBuilder streamsBuilder = new StreamsBuilder();&lt;br/&gt;
+    private final Map&amp;lt;String, String&amp;gt; readKeyValues = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+    private static final Properties PRODUCER_CONFIG = new Properties();&lt;br/&gt;
+    private static final Properties STREAMS_CONFIG = new Properties();&lt;br/&gt;
+    private Map&amp;lt;String, String&amp;gt; expectedInitialResultsMap;&lt;br/&gt;
+    private Map&amp;lt;String, String&amp;gt; expectedResultsWithDataWrittenDuringRestoreMap;&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+    @BeforeClass&lt;br/&gt;
+    public static void setUpBeforeAllTests() throws Exception &lt;/p&gt;
{
+
+        CLUSTER.createTopic(SOURCE_TOPIC);
+
+        STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;ktable-restore-from-source&quot;);
+        STREAMS_CONFIG.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
+        STREAMS_CONFIG.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
+        STREAMS_CONFIG.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
+        STREAMS_CONFIG.put(IntegrationTestUtils.INTERNAL_LEAVE_GROUP_ON_CLOSE, true);
+        STREAMS_CONFIG.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 5);
+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, WallclockTimestampExtractor.class);
+
+        PRODUCER_CONFIG.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        PRODUCER_CONFIG.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);
+        PRODUCER_CONFIG.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
+        PRODUCER_CONFIG.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
+
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Before&lt;br/&gt;
+    public void before() {&lt;br/&gt;
+&lt;br/&gt;
+        final KTable&amp;lt;String, String&amp;gt; kTable = streamsBuilder.table(SOURCE_TOPIC);&lt;br/&gt;
+        kTable.toStream().foreach(new ForeachAction&amp;lt;String, String&amp;gt;() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void apply(final String key, final String value) &lt;/p&gt;
{
+                readKeyValues.put(key, value);
+            }
&lt;p&gt;+        });&lt;br/&gt;
+&lt;br/&gt;
+        expectedInitialResultsMap = createExpectedResultsMap(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);&lt;br/&gt;
+        expectedResultsWithDataWrittenDuringRestoreMap = createExpectedResultsMap(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @After&lt;br/&gt;
+    public void after() throws IOException &lt;/p&gt;
{
+        IntegrationTestUtils.purgeLocalStreamsState(STREAMS_CONFIG);
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosDisabled() throws Exception {&lt;br/&gt;
+&lt;br/&gt;
+        try &lt;/p&gt;
{
+            streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);
+            streamsOne.start();
+
+            produceKeyValues(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);
+
+            assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, &quot;Table did not read all values&quot;);
+
+            streamsOne.close();
+            streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);
+            // the state restore listener will append one record to the log
+            streamsOne.setGlobalStateRestoreListener(new UpdatingSourceTopicOnRestoreStartStateRestoreListener());
+            streamsOne.start();
+
+            produceKeyValues(&quot;f&quot;, &quot;g&quot;, &quot;h&quot;);
+
+            assertNumberValuesRead(readKeyValues, expectedResultsWithDataWrittenDuringRestoreMap, &quot;Table did not get all values after restart&quot;);
+
+        }
&lt;p&gt; finally &lt;/p&gt;
{
+            streamsOne.close(5, TimeUnit.SECONDS);
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosEnabled() throws Exception {&lt;br/&gt;
+&lt;br/&gt;
+        try {
+            STREAMS_CONFIG.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
+            streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);
+            streamsOne.start();
+
+            produceKeyValues(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);
+
+            assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, &quot;Table did not read all values&quot;);
+
+            streamsOne.close();
+            streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);
+            // the state restore listener will append one record to the log
+            streamsOne.setGlobalStateRestoreListener(new UpdatingSourceTopicOnRestoreStartStateRestoreListener());
+            streamsOne.start();
+
+            produceKeyValues(&quot;f&quot;, &quot;g&quot;, &quot;h&quot;);
+
+            assertNumberValuesRead(readKeyValues, expectedResultsWithDataWrittenDuringRestoreMap, &quot;Table did not get all values after restart&quot;);
+
+        } finally {+            streamsOne.close(5, TimeUnit.SECONDS);+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRestoreAndProgressWhenTopicNotWrittenToDuringRestoration() throws Exception {&lt;br/&gt;
+&lt;br/&gt;
+        try &lt;/p&gt;
{
+            streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);
+            streamsOne.start();
+
+            produceKeyValues(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);
+
+            assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, &quot;Table did not read all values&quot;);
+
+            streamsOne.close();
+            streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);
+            streamsOne.start();
+
+            produceKeyValues(&quot;f&quot;, &quot;g&quot;, &quot;h&quot;);
+
+            final Map&amp;lt;String, String&amp;gt; expectedValues = createExpectedResultsMap(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;);
+
+            assertNumberValuesRead(readKeyValues, expectedValues, &quot;Table did not get all values after restart&quot;);
+
+        }
&lt;p&gt; finally &lt;/p&gt;
{
+            streamsOne.close(5, TimeUnit.SECONDS);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void assertNumberValuesRead(final Map&amp;lt;String, String&amp;gt; valueMap,&lt;br/&gt;
+                                        final Map&amp;lt;String, String&amp;gt; expectedMap,&lt;br/&gt;
+                                        final String errorMessage) throws InterruptedException {&lt;br/&gt;
+&lt;br/&gt;
+        TestUtils.waitForCondition(new TestCondition() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public boolean conditionMet() &lt;/p&gt;
{
+                return valueMap.equals(expectedMap);
+            }
&lt;p&gt;+        }, errorMessage);&lt;br/&gt;
+&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void produceKeyValues(final String... keys) throws ExecutionException, InterruptedException {&lt;br/&gt;
+        final List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; keyValueList = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+        for (final String key : keys) &lt;/p&gt;
{
+            keyValueList.add(new KeyValue&amp;lt;&amp;gt;(key, key + &quot;1&quot;));
+        }
&lt;p&gt;+&lt;br/&gt;
+        IntegrationTestUtils.produceKeyValuesSynchronously(SOURCE_TOPIC,&lt;br/&gt;
+                                                           keyValueList,&lt;br/&gt;
+                                                           PRODUCER_CONFIG,&lt;br/&gt;
+                                                           time);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private Map&amp;lt;String, String&amp;gt; createExpectedResultsMap(final String... keys) {&lt;br/&gt;
+        final Map&amp;lt;String, String&amp;gt; expectedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final String key : keys) &lt;/p&gt;
{
+            expectedMap.put(key, key + &quot;1&quot;);
+        }
&lt;p&gt;+        return expectedMap;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private class UpdatingSourceTopicOnRestoreStartStateRestoreListener implements StateRestoreListener {&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onRestoreStart(final TopicPartition topicPartition,&lt;br/&gt;
+                                   final String storeName,&lt;br/&gt;
+                                   final long startingOffset,&lt;br/&gt;
+                                   final long endingOffset) {&lt;br/&gt;
+            try &lt;/p&gt;
{
+                produceKeyValues(&quot;d&quot;);
+            }
&lt;p&gt; catch (ExecutionException | InterruptedException e) &lt;/p&gt;
{
+                throw new RuntimeException(e);
+            }
&lt;p&gt;+        }&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onBatchRestored(final TopicPartition topicPartition,&lt;br/&gt;
+                                    final String storeName,&lt;br/&gt;
+                                    final long batchEndOffset,&lt;br/&gt;
+                                    final long numRestored) &lt;/p&gt;
{
+        }&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onRestoreEnd(final TopicPartition topicPartition,&lt;br/&gt;
+                                 final String storeName,&lt;br/&gt;
+                                 final long totalRestored) {+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
index 9f6f7121f5a..ee964513415 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java&lt;br/&gt;
@@ -91,8 +91,7 @@ public void shouldRequestTopicsAndHandleTimeoutException() {&lt;br/&gt;
         };&lt;/p&gt;

&lt;p&gt;         final StoreChangelogReader changelogReader = new StoreChangelogReader(consumer, stateRestoreListener, logContext);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         assertTrue(functionCalled.get());&lt;br/&gt;
     }&lt;br/&gt;
@@ -105,7 +104,6 @@ public void shouldThrowExceptionIfConsumerHasCurrentSubscription() {&lt;br/&gt;
         EasyMock.replay(mockRestorer);&lt;br/&gt;
         changelogReader.register(mockRestorer);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
         consumer.subscribe(Collections.singleton(&quot;sometopic&quot;));&lt;/p&gt;

&lt;p&gt;         try {&lt;br/&gt;
@@ -120,8 +118,7 @@ public void shouldThrowExceptionIfConsumerHasCurrentSubscription() {&lt;br/&gt;
     public void shouldRestoreAllMessagesFromBeginningWhenCheckpointNull() &lt;/p&gt;
{
         final int messages = 10;
         setupConsumer(messages, topicPartition);
-        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,
-                                                   &quot;storeName&quot;));
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
         changelogReader.restore(active);
         assertThat(callback.restored.size(), equalTo(messages));
     }
&lt;p&gt;@@ -137,7 +134,7 @@ public void shouldRecoverFromInvalidOffsetExceptionAndFinishRestore() {&lt;br/&gt;
             }&lt;br/&gt;
         });&lt;br/&gt;
         changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+                                                   &quot;storeName&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         EasyMock.expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
         EasyMock.replay(active);&lt;br/&gt;
@@ -294,8 +291,7 @@ public void shouldNotRestoreAnythingWhenCheckpointAtEndOffset() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldReturnRestoredOffsetsForPersistentStores() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();&lt;br/&gt;
         assertThat(restoredOffsets, equalTo(Collections.singletonMap(topicPartition, 10L)));&lt;br/&gt;
@@ -304,8 +300,7 @@ public void shouldReturnRestoredOffsetsForPersistentStores() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldNotReturnRestoredOffsetsForNonPersistentStore() {&lt;br/&gt;
         setupConsumer(10, topicPartition);&lt;/li&gt;
	&lt;li&gt;changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false,&lt;/li&gt;
	&lt;li&gt;&quot;storeName&quot;));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, &quot;storeName&quot;));&lt;br/&gt;
         changelogReader.restore(active);&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets = changelogReader.restoredOffsets();&lt;br/&gt;
         assertThat(restoredOffsets, equalTo(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap()));&lt;br/&gt;
@@ -369,12 +364,11 @@ public void shouldRestorePartitionsRegisteredPostInitialization() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestore() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForChangelogTopic() {
         final int messages = 10;
         setupConsumer(messages, topicPartition);
         consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 5L));
-        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true,
-            &quot;storeName&quot;));
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
 
         expect(active.restoringTaskFor(topicPartition)).andReturn(task);
         replay(active);
@@ -385,6 +379,125 @@ public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestore
         } catch (final TaskMigratedException expected) { /* ignore */ }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForChangelogTopicEOSEnabled() {&lt;br/&gt;
+        final int totalMessages = 10;&lt;br/&gt;
+        assignPartition(totalMessages, topicPartition);&lt;br/&gt;
+        // records 0..4&lt;br/&gt;
+        addRecords(5, topicPartition, 0);&lt;br/&gt;
+        //EOS enabled commit marker at offset 5 so rest of records 6..10&lt;br/&gt;
+        addRecords(5, topicPartition, 6);&lt;br/&gt;
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;br/&gt;
+&lt;br/&gt;
+        // end offsets should start after commit marker of 5 from above&lt;br/&gt;
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 6L));&lt;br/&gt;
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));&lt;br/&gt;
+&lt;br/&gt;
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);&lt;br/&gt;
+        replay(active);&lt;br/&gt;
+        try &lt;/p&gt;
{
+            changelogReader.restore(active);
+            fail(&quot;Should have thrown task migrated exception&quot;);
+        }
&lt;p&gt; catch (final TaskMigratedException expected) &lt;/p&gt;
{
+            /* ignore */
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionDuringRestoreForChangelogTopicWhenEndOffsetNotExceededEOSEnabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        setupConsumer(totalMessages, topicPartition);
+        // records have offsets of 0..9 10 is commit marker so 11 is end offset
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 11L));
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionDuringRestoreForChangelogTopicWhenEndOffsetNotExceededEOSDisabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        setupConsumer(totalMessages, topicPartition);
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForSourceTopic() &lt;/p&gt;
{
+        final int messages = 10;
+        setupConsumer(messages, topicPartition);
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(5));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetNotExceededDuringRestoreForSourceTopic() &lt;/p&gt;
{
+        final int messages = 10;
+        setupConsumer(messages, topicPartition);
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 10, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForSourceTopicEOSEnabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        assignPartition(totalMessages, topicPartition);
+        // records 0..4 last offset before commit is 4
+        addRecords(5, topicPartition, 0);
+        //EOS enabled so commit marker at offset 5 so records start at 6
+        addRecords(5, topicPartition, 6);
+        consumer.assign(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());
+        // commit marker is 5 so ending offset is 12
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 12L));
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 6, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(5));
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotThrowTaskMigratedExceptionIfEndOffsetNotExceededDuringRestoreForSourceTopicEOSEnabled() &lt;/p&gt;
{
+        final int totalMessages = 10;
+        setupConsumer(totalMessages, topicPartition);
+        // records have offsets 0..9 10 is commit marker so 11 is ending offset
+        consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 11L));
+
+        changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 11, true, &quot;storeName&quot;));
+
+        expect(active.restoringTaskFor(topicPartition)).andReturn(task);
+        replay(active);
+
+        changelogReader.restore(active);
+        assertThat(callback.restored.size(), equalTo(10));
+    }
&lt;p&gt;+&lt;br/&gt;
     private void setupConsumer(final long messages,&lt;br/&gt;
                                final TopicPartition topicPartition) {&lt;br/&gt;
         assignPartition(messages, topicPartition);&lt;br/&gt;
@@ -404,11 +517,11 @@ private void assignPartition(final long messages,&lt;br/&gt;
                                  final TopicPartition topicPartition) {&lt;br/&gt;
         consumer.updatePartitions(topicPartition.topic(),&lt;br/&gt;
                                   Collections.singletonList(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new PartitionInfo(topicPartition.topic(),&lt;/li&gt;
	&lt;li&gt;topicPartition.partition(),&lt;/li&gt;
	&lt;li&gt;null,&lt;/li&gt;
	&lt;li&gt;null,&lt;/li&gt;
	&lt;li&gt;null)));&lt;br/&gt;
+                                      new PartitionInfo(topicPartition.topic(),&lt;br/&gt;
+                                                        topicPartition.partition(),&lt;br/&gt;
+                                                        null,&lt;br/&gt;
+                                                        null,&lt;br/&gt;
+                                                        null)));&lt;br/&gt;
         consumer.updateBeginningOffsets(Collections.singletonMap(topicPartition, 0L));&lt;br/&gt;
         consumer.updateEndOffsets(Collections.singletonMap(topicPartition, Math.max(0, messages)));&lt;br/&gt;
         consumer.assign(Collections.singletonList(topicPartition));&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16327259" author="dminkovsky" created="Tue, 16 Jan 2018 15:42:28 +0000"  >&lt;p&gt;I&apos;m being hit by this and wondering what to do.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I upgraded my brokers to 1.0.0. Will Kafka Streams 0.11.0.1 work with a 1.0.0 broker? If not, will MirrorMaker work 1.0.0-&amp;gt;0.11.0.1?&lt;/li&gt;
	&lt;li&gt;I&apos;m fine building from trunk. Could you recommend a &quot;stable&quot; commit for me to use?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;With regard to a workaround for 1.0.0, I am confused about the root cause of this issue, so I don&apos;t understand why the null masking/groupByKey().aggregate() will mitigate it. Therefore I don&apos;t understand how to apply the workaround. &lt;/p&gt;

&lt;p&gt;&lt;b&gt;UPDATE&lt;/b&gt;&lt;br/&gt;
&lt;del&gt;With regard to building from trunk: I am not very familiar with Gradle. I looked in the README, but did not find the correct invocation to build and install the Streams artifact into my local Maven repo? And what version will it be (-SNAPSHOT? -trunk?). Thank you.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;UPDATE 2&lt;/b&gt;&lt;br/&gt;
I just ran installAll and it built 1.1.0-SNAPSHOT. Didn&apos;t take more than 15 minutes.&lt;/p&gt;</comment>
                            <comment id="16327439" author="bbejeck" created="Tue, 16 Jan 2018 17:37:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dminkovsky&quot; class=&quot;user-hover&quot; rel=&quot;dminkovsky&quot;&gt;dminkovsky&lt;/a&gt; ,&#160;are you good now with the 1.1.0-SNAPSHOT build?&lt;/p&gt;</comment>
                            <comment id="16327592" author="mjsax" created="Tue, 16 Jan 2018 19:01:23 +0000"  >&lt;p&gt;I would assume that branch `1.0` (&lt;a href=&quot;https://github.com/apache/kafka/tree/1.0)&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/tree/1.0&lt;/a&gt;)&#160;should be quite stable &#8211; it only contains bug fixes for `1.0.1`. As an alternative, you can also checkout `1.0.0` commit (&lt;a href=&quot;https://github.com/apache/kafka/tree/1.0.0)&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/tree/1.0.0&lt;/a&gt;)&#160;and cherry-pick the fix.&lt;/p&gt;

&lt;p&gt;Btw: Brokers are always backward compatible, to yes, KS 0.11.0.1 will work with `1.0.0` brokers.&lt;/p&gt;</comment>
                            <comment id="16327992" author="mjsax" created="Tue, 16 Jan 2018 23:29:09 +0000"  >&lt;p&gt;Release plan for v1.0.1 was proposed today:&#160;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+1.0.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+1.0.1&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13125317">KAFKA-6373</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 43 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3n5br:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>