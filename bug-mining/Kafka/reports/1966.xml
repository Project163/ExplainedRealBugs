<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:12:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7072] Kafka Streams may drop rocksb window segments before they expire</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7072</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The current implementation of Segments used by Rocks Session and Time window stores is in conflict with our current timestamp management model.&lt;/p&gt;

&lt;p&gt;The current segmentation approach allows configuration of a fixed number of segments (let&apos;s say&#160;&lt;b&gt;4&lt;/b&gt;) and a fixed retention time. We essentially divide up the retention time into the available number of segments:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;tt&gt;&amp;lt;--------&lt;del&gt;|&lt;/del&gt;----------------------------|&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160;expiration date&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;right now&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; ------&lt;del&gt;retention time&lt;/del&gt;-------/&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; |&#160; seg 0 &#160;|&#160; seg 1&#160; |&#160; seg 2&#160; |&#160; seg 3&#160; |&lt;/tt&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Note that we keep one extra segment so that we can record new events, while some events in seg 0 are actually expired (but we only drop whole segments, so they just get to hang around.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;tt&gt;&amp;lt;------------&lt;del&gt;|&lt;/del&gt;----------------------------|&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160;expiration date&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;right now&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; ------&lt;del&gt;retention time&lt;/del&gt;-------/&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; |&#160;&#160;seg&#160;0&#160;&#160;|&#160; seg 1&#160; |&#160; seg 2&#160; |&#160; seg 3&#160; |&lt;/tt&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When it&apos;s time to provision segment 4, we know that segment 0 is completely expired, so we drop it:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;tt&gt;&amp;lt;------------------&lt;del&gt;|&lt;/del&gt;----------------------------|&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160;expiration date&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;right now&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; ------&lt;del&gt;retention time&lt;/del&gt;-------/&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; |&#160; seg 1&#160; |&#160; seg 2&#160; |&#160; seg 3&#160; |&#160; seg 4&#160; |&lt;/tt&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;However, the current timestamp management model allows for records from the future. Namely, because we define stream time as the minimum buffered timestamp (nondecreasing), we can have a buffer like this:&#160;[ 5, 2, 6 ], and our stream time will be 2, but we&apos;ll handle a record with timestamp 5 next. referring to the example, this means we could wind up having to provision segment 4 before segment 0 expires!&lt;/p&gt;

&lt;p&gt;Let&apos;s say &quot;f&quot; is our future event:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;tt&gt;&amp;lt;------------------&lt;del&gt;|&lt;/del&gt;---------------------------&lt;del&gt;|&lt;/del&gt;---f&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160;expiration date&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;right now&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; ------&lt;del&gt;retention time&lt;/del&gt;-------/&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; |&#160; seg 1&#160; |&#160; seg 2&#160; |&#160; seg 3&#160; |&#160; seg 4&#160; |&lt;/tt&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Should we drop segment 0 prematurely? Or should we crash and refuse to process &quot;f&quot;?&lt;/p&gt;

&lt;p&gt;Today, we do the former, and this is probably the better choice. If we refuse to process &quot;f&quot;, then we cannot make progress ever again.&lt;/p&gt;

&lt;p&gt;Dropping segment 0 prematurely is a bummer, but users could also set the retention time high enough that they don&apos;t think they&apos;ll actually get any events late enough to need segment 0. Worst case, since we can have many future events without advancing stream time, sparse enough to each require their own segment, which would eat deeply into the retention time, dropping many segments that should be live.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13166807">KAFKA-7072</key>
            <summary>Kafka Streams may drop rocksb window segments before they expire</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vvcephei">John Roesler</assignee>
                                    <reporter username="vvcephei">John Roesler</reporter>
                        <labels>
                    </labels>
                <created>Mon, 18 Jun 2018 21:45:43 +0000</created>
                <updated>Thu, 21 Jun 2018 01:42:46 +0000</updated>
                            <resolved>Thu, 21 Jun 2018 01:42:46 +0000</resolved>
                                    <version>0.11.0.0</version>
                    <version>0.11.0.1</version>
                    <version>0.11.0.2</version>
                    <version>1.0.0</version>
                    <version>1.0.1</version>
                    <version>1.1.0</version>
                    <version>2.0.0</version>
                                    <fixVersion>2.1.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16517577" author="githubbot" created="Tue, 19 Jun 2018 22:01:14 +0000"  >&lt;p&gt;vvcephei opened a new pull request #5253: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7072&quot; title=&quot;Kafka Streams may drop rocksb window segments before they expire&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7072&quot;&gt;&lt;del&gt;KAFKA-7072&lt;/del&gt;&lt;/a&gt;: clean up segments only after they expire&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5253&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5253&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Significant refactor of Segments to use stream-time as the basis of segment expiration.&lt;br/&gt;
   Previously Segments assumed that the current record time was representative of stream time.&lt;/p&gt;

&lt;p&gt;   In the event of a &quot;future&quot; event (one whose record time is greater than the stream time), this&lt;br/&gt;
   would inappropriately drop live segments. Now, Segments will provision the new segment&lt;br/&gt;
   to house the future event and drop old segments only after they expire.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16518791" author="githubbot" created="Thu, 21 Jun 2018 01:40:51 +0000"  >&lt;p&gt;guozhangwang closed pull request #5253: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7072&quot; title=&quot;Kafka Streams may drop rocksb window segments before they expire&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7072&quot;&gt;&lt;del&gt;KAFKA-7072&lt;/del&gt;&lt;/a&gt;: clean up segments only after they expire&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5253&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5253&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImpl.java&lt;br/&gt;
index 717e6a7a87e..c469be966e8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImpl.java&lt;br/&gt;
@@ -96,4 +96,8 @@ public Cancellable schedule(long interval, PunctuationType type, Punctuator call&lt;br/&gt;
         throw new UnsupportedOperationException(&quot;this should not happen: schedule() not supported in global processor context.&quot;);&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @Override&lt;br/&gt;
+    public Long streamTime() &lt;/p&gt;
{
+        throw new RuntimeException(&quot;Stream time is not implemented for the global processor context.&quot;);
+    }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
index 9439abaab2d..cfc1970a7ae 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java&lt;br/&gt;
@@ -63,4 +63,6 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Mark this context as being uninitialized&lt;br/&gt;
      */&lt;br/&gt;
     void uninitialize();&lt;br/&gt;
+&lt;br/&gt;
+    Long streamTime();&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java&lt;br/&gt;
index f1ee81ff367..36a309cab6b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java&lt;br/&gt;
@@ -28,11 +28,13 @@&lt;br/&gt;
 import org.apache.kafka.streams.state.internals.ThreadCache;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.util.List;&lt;br/&gt;
+import java.util.function.Supplier;&lt;/p&gt;

&lt;p&gt; public class ProcessorContextImpl extends AbstractProcessorContext implements RecordCollector.Supplier &lt;/p&gt;
{
 
     private final StreamTask task;
     private final RecordCollector collector;
+    private Supplier&amp;lt;Long&amp;gt; streamTimeSupplier;
     private final ToInternal toInternal = new ToInternal();
     private final static To SEND_TO_ALL = To.all();
 
@@ -153,4 +155,13 @@ public Cancellable schedule(final long interval, final PunctuationType type, fin
         return task.schedule(interval, type, callback);
     }

&lt;p&gt;+    void setStreamTimeSupplier(final Supplier&amp;lt;Long&amp;gt; streamTimeSupplier) &lt;/p&gt;
{
+        this.streamTimeSupplier = streamTimeSupplier;
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public Long streamTime() &lt;/p&gt;
{
+        return this.streamTimeSupplier.get();
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
index 5c278c9c754..58c2e2c8678 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
@@ -216,4 +216,9 @@ public ProcessorNode currentNode() &lt;/p&gt;
{
         throw new UnsupportedOperationException(&quot;this should not happen: currentNode not supported in standby tasks.&quot;);
     }

&lt;p&gt;+    @Override&lt;br/&gt;
+    public Long streamTime() &lt;/p&gt;
{
+        throw new RuntimeException(&quot;Stream time is not implemented for the standby context.&quot;);
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index f99695875ea..6993ef88950 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -189,7 +189,8 @@ public StreamTask(final TaskId id,&lt;br/&gt;
         final Map&amp;lt;TopicPartition, RecordQueue&amp;gt; partitionQueues = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;         // initialize the topology with its own context&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;processorContext = new ProcessorContextImpl(id, this, config, this.recordCollector, stateMgr, metrics, cache);&lt;br/&gt;
+        final ProcessorContextImpl processorContextImpl = new ProcessorContextImpl(id, this, config, this.recordCollector, stateMgr, metrics, cache);&lt;br/&gt;
+        processorContext = processorContextImpl;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final TimestampExtractor defaultTimestampExtractor = config.defaultTimestampExtractor();&lt;br/&gt;
         final DeserializationExceptionHandler defaultDeserializationExceptionHandler = config.defaultDeserializationExceptionHandler();&lt;br/&gt;
@@ -209,6 +210,7 @@ public StreamTask(final TaskId id,&lt;/p&gt;

&lt;p&gt;         recordInfo = new PartitionGroup.RecordInfo();&lt;br/&gt;
         partitionGroup = new PartitionGroup(partitionQueues);&lt;br/&gt;
+        processorContextImpl.setStreamTimeSupplier(partitionGroup::timestamp);&lt;/p&gt;

&lt;p&gt;         stateMgr.registerGlobalStateStores(topology.globalStateStores());&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/state/Stores.java b/streams/src/main/java/org/apache/kafka/streams/state/Stores.java&lt;br/&gt;
index 27b985b7160..eebd59fb012 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/Stores.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/Stores.java&lt;br/&gt;
@@ -161,13 +161,15 @@ public static WindowBytesStoreSupplier persistentWindowStore(final String name,&lt;br/&gt;
         if (retentionPeriod &amp;lt; 0) &lt;/p&gt;
{
             throw new IllegalArgumentException(&quot;retentionPeriod cannot be negative&quot;);
         }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (numSegments &amp;lt; 1) {&lt;/li&gt;
	&lt;li&gt;throw new IllegalArgumentException(&quot;numSegments cannot must smaller than 1&quot;);&lt;br/&gt;
+        if (numSegments &amp;lt; 2) 
{
+            throw new IllegalArgumentException(&quot;numSegments cannot must smaller than 2&quot;);
         }
&lt;p&gt;         if (windowSize &amp;lt; 0) &lt;/p&gt;
{
             throw new IllegalArgumentException(&quot;windowSize cannot be negative&quot;);
         }&lt;/li&gt;
	&lt;li&gt;return new RocksDbWindowBytesStoreSupplier(name, retentionPeriod, numSegments, windowSize, retainDuplicates);&lt;br/&gt;
+        final long segmentIntervalMs = Math.max(retentionPeriod / (numSegments - 1), 60_000L);&lt;br/&gt;
+&lt;br/&gt;
+        return new RocksDbWindowBytesStoreSupplier(name, retentionPeriod, segmentIntervalMs, windowSize, retainDuplicates);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java&lt;br/&gt;
index ec9e6f7b30a..c5d15d6e5dc 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java&lt;br/&gt;
@@ -20,26 +20,30 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StateRestoreCallback;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StateStore;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;&lt;br/&gt;
 import org.apache.kafka.streams.processor.internals.ProcessorStateManager;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueIterator;&lt;br/&gt;
+import org.slf4j.Logger;&lt;br/&gt;
+import org.slf4j.LoggerFactory;&lt;/p&gt;

&lt;p&gt; import java.util.List;&lt;/p&gt;

&lt;p&gt; class RocksDBSegmentedBytesStore implements SegmentedBytesStore {&lt;br/&gt;
+    private final static Logger LOG = LoggerFactory.getLogger(RocksDBSegmentedBytesStore.class);&lt;/p&gt;

&lt;p&gt;     private final String name;&lt;br/&gt;
     private final Segments segments;&lt;br/&gt;
     private final KeySchema keySchema;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private ProcessorContext context;&lt;br/&gt;
+    private InternalProcessorContext context;&lt;br/&gt;
     private volatile boolean open;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     RocksDBSegmentedBytesStore(final String name,&lt;br/&gt;
                                final long retention,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int numSegments,&lt;br/&gt;
+                               final long segmentInterval,&lt;br/&gt;
                                final KeySchema keySchema) 
{
         this.name = name;
         this.keySchema = keySchema;
-        this.segments = new Segments(name, retention, numSegments);
+        this.segments = new Segments(name, retention, segmentInterval);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
@@ -97,8 +101,10 @@ public void remove(final Bytes key) {&lt;br/&gt;
     @Override&lt;br/&gt;
     public void put(final Bytes key, final byte[] value) {&lt;br/&gt;
         final long segmentId = segments.segmentId(keySchema.segmentTimestamp(key));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segment segment = segments.getOrCreateSegment(segmentId, context);&lt;/li&gt;
	&lt;li&gt;if (segment != null) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        final Segment segment = segments.getOrCreateSegmentIfLive(segmentId, context);+        if (segment == null) {
+            LOG.debug(&quot;Skipping record for expired segment.&quot;);
+        } else {
             segment.put(key, value);
         }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -119,11 +125,11 @@ public String name() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
     public void init(ProcessorContext context, StateStore root) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.context = context;&lt;br/&gt;
+        this.context = (InternalProcessorContext) context;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         keySchema.init(ProcessorStateManager.storeChangelogTopic(context.applicationId(), root.name()));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;segments.openExisting(context);&lt;br/&gt;
+        segments.openExisting(this.context);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // register and possibly restore the state from the logs&lt;br/&gt;
         context.register(root, new StateRestoreCallback() {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbSessionBytesStoreSupplier.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbSessionBytesStoreSupplier.java&lt;br/&gt;
index 5a87bc57d56..c83ab599f7c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbSessionBytesStoreSupplier.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbSessionBytesStoreSupplier.java&lt;br/&gt;
@@ -25,8 +25,6 @@&lt;br/&gt;
     private final String name;&lt;br/&gt;
     private final long retentionPeriod;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int NUM_SEGMENTS = 3;&lt;br/&gt;
-&lt;br/&gt;
     public RocksDbSessionBytesStoreSupplier(final String name,&lt;br/&gt;
                                             final long retentionPeriod) {&lt;br/&gt;
         this.name = name;&lt;br/&gt;
@@ -43,7 +41,7 @@ public String name() 
{
         final RocksDBSegmentedBytesStore segmented = new RocksDBSegmentedBytesStore(
             name,
             retentionPeriod,
-            NUM_SEGMENTS,
+            segmentIntervalMs(),
             new SessionKeySchema());
         return new RocksDBSessionStore&amp;lt;&amp;gt;(segmented, Serdes.Bytes(), Serdes.ByteArray());
     }
&lt;p&gt;@@ -55,8 +53,6 @@ public String metricsScope() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
     public long segmentIntervalMs() &lt;/p&gt;
{
-        return Segments.segmentInterval(
-            retentionPeriod,
-            NUM_SEGMENTS);
+        return Math.max(retentionPeriod / 2, 60_000L);
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java&lt;br/&gt;
index 5fbf491dfc4..4a7bffc850e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java&lt;br/&gt;
@@ -24,23 +24,18 @@&lt;br/&gt;
 public class RocksDbWindowBytesStoreSupplier implements WindowBytesStoreSupplier {&lt;br/&gt;
     private final String name;&lt;br/&gt;
     private final long retentionPeriod;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final int segments;&lt;br/&gt;
+    private final long segmentInterval;&lt;br/&gt;
     private final long windowSize;&lt;br/&gt;
     private final boolean retainDuplicates;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int MIN_SEGMENTS = 2;&lt;br/&gt;
-&lt;br/&gt;
     public RocksDbWindowBytesStoreSupplier(final String name,&lt;br/&gt;
                                            final long retentionPeriod,&lt;/li&gt;
	&lt;li&gt;final int segments,&lt;br/&gt;
+                                           final long segmentInterval,&lt;br/&gt;
                                            final long windowSize,&lt;br/&gt;
                                            final boolean retainDuplicates) {&lt;/li&gt;
	&lt;li&gt;if (segments &amp;lt; MIN_SEGMENTS) 
{
-            throw new IllegalArgumentException(&quot;numSegments must be &amp;gt;= &quot; + MIN_SEGMENTS);
-        }
&lt;p&gt;         this.name = name;&lt;br/&gt;
         this.retentionPeriod = retentionPeriod;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;this.segments = segments;&lt;br/&gt;
+        this.segmentInterval = segmentInterval;&lt;br/&gt;
         this.windowSize = windowSize;&lt;br/&gt;
         this.retainDuplicates = retainDuplicates;&lt;br/&gt;
     }&lt;br/&gt;
@@ -55,7 +50,7 @@ public String name() {&lt;br/&gt;
         final RocksDBSegmentedBytesStore segmentedBytesStore = new RocksDBSegmentedBytesStore(&lt;br/&gt;
                 name,&lt;br/&gt;
                 retentionPeriod,&lt;/li&gt;
	&lt;li&gt;segments,&lt;br/&gt;
+                segmentInterval,&lt;br/&gt;
                 new WindowKeySchema()&lt;br/&gt;
         );&lt;br/&gt;
         return new RocksDBWindowStore&amp;lt;&amp;gt;(segmentedBytesStore,&lt;br/&gt;
@@ -73,7 +68,7 @@ public String metricsScope() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
     public int segments() &lt;/p&gt;
{
-        return segments;
+        return (int) (retentionPeriod / segmentInterval) + 1;
     }

&lt;p&gt;     @Override&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java&lt;br/&gt;
index 7b3336a3149..8e6c524c555 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java&lt;br/&gt;
@@ -16,9 +16,8 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.state.internals;&lt;/p&gt;

&lt;p&gt;-import org.apache.kafka.streams.errors.InvalidStateStoreException;&lt;br/&gt;
 import org.apache.kafka.streams.errors.ProcessorStateException;&lt;br/&gt;
-import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;/p&gt;

&lt;p&gt;@@ -28,35 +27,29 @@&lt;br/&gt;
 import java.text.SimpleDateFormat;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
-import java.util.Collections;&lt;br/&gt;
+import java.util.Iterator;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.NavigableMap;&lt;br/&gt;
 import java.util.SimpleTimeZone;&lt;br/&gt;
-import java.util.concurrent.ConcurrentHashMap;&lt;br/&gt;
+import java.util.TreeMap;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Manages the 
{@link Segment}
&lt;p&gt;s that are used by the &lt;/p&gt;
{@link RocksDBSegmentedBytesStore}
&lt;p&gt;  */&lt;br/&gt;
 class Segments {&lt;br/&gt;
     private static final Logger log = LoggerFactory.getLogger(Segments.class);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static final long MIN_SEGMENT_INTERVAL = 60 * 1000L;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static long segmentInterval(long retentionPeriod, int numSegments) 
{
-        return Math.max(retentionPeriod / (numSegments - 1), MIN_SEGMENT_INTERVAL);
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private final ConcurrentHashMap&amp;lt;Long, Segment&amp;gt; segments = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private final TreeMap&amp;lt;Long, Segment&amp;gt; segments = new TreeMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final String name;&lt;/li&gt;
	&lt;li&gt;private final int numSegments;&lt;br/&gt;
+    private final long retentionPeriod;&lt;br/&gt;
     private final long segmentInterval;&lt;br/&gt;
     private final SimpleDateFormat formatter;&lt;/li&gt;
	&lt;li&gt;private long minSegmentId = Long.MAX_VALUE;&lt;/li&gt;
	&lt;li&gt;private long maxSegmentId = -1L;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Segments(final String name, final long retentionPeriod, final int numSegments) {&lt;br/&gt;
+    Segments(final String name, final long retentionPeriod, final long segmentInterval) {&lt;br/&gt;
         this.name = name;&lt;/li&gt;
	&lt;li&gt;this.numSegments = numSegments;&lt;/li&gt;
	&lt;li&gt;this.segmentInterval = segmentInterval(retentionPeriod, numSegments);&lt;br/&gt;
+        this.segmentInterval = segmentInterval;&lt;br/&gt;
+        this.retentionPeriod = retentionPeriod;&lt;br/&gt;
         // Create a date formatter. Formatted timestamps are used as segment name suffixes&lt;br/&gt;
         this.formatter = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;);&lt;br/&gt;
         this.formatter.setTimeZone(new SimpleTimeZone(0, &quot;UTC&quot;));&lt;br/&gt;
@@ -75,42 +68,53 @@ String segmentName(final long segmentId) {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     Segment getSegmentForTimestamp(final long timestamp) &lt;/p&gt;
{
-        return getSegment(segmentId(timestamp));
+        return segments.get(segmentId(timestamp));
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Segment getOrCreateSegment(final long segmentId, final ProcessorContext context) {&lt;/li&gt;
	&lt;li&gt;if (segmentId &amp;gt; maxSegmentId - numSegments) {&lt;/li&gt;
	&lt;li&gt;final long key = segmentId % numSegments;&lt;/li&gt;
	&lt;li&gt;final Segment segment = segments.get(key);&lt;/li&gt;
	&lt;li&gt;if (!isSegment(segment, segmentId)) 
{
-                cleanup(segmentId);
-            }&lt;/li&gt;
	&lt;li&gt;Segment newSegment = new Segment(segmentName(segmentId), name, segmentId);&lt;/li&gt;
	&lt;li&gt;Segment previousSegment = segments.putIfAbsent(key, newSegment);&lt;/li&gt;
	&lt;li&gt;if (previousSegment == null) 
{
-                newSegment.openDB(context);
-                maxSegmentId = segmentId &amp;gt; maxSegmentId ? segmentId : maxSegmentId;
-                minSegmentId = segmentId &amp;lt; minSegmentId ? segmentId : minSegmentId;
-            }&lt;/li&gt;
	&lt;li&gt;return previousSegment == null ? newSegment : previousSegment;&lt;br/&gt;
+    Segment getOrCreateSegmentIfLive(final long segmentId, final InternalProcessorContext context) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        final long minLiveSegment = segmentId(context.streamTime() - retentionPeriod);++        final Segment toReturn;+        if (segmentId &amp;gt;= minLiveSegment) {
+            // The segment is live. get it, ensure it&apos;s open, and return it.
+            toReturn = getOrCreateSegment(segmentId, context);
+        } else {
+            toReturn = null;
+        }++        cleanupEarlierThan(minLiveSegment);+        return toReturn;+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+    private Segment getOrCreateSegment(final long segmentId, final InternalProcessorContext context) {&lt;br/&gt;
+        if (segments.containsKey(segmentId)) &lt;/p&gt;
{
+            return segments.get(segmentId);
         }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;return null;&lt;br/&gt;
+            final Segment newSegment = new Segment(segmentName(segmentId), name, segmentId);&lt;br/&gt;
+            final Segment shouldBeNull = segments.put(segmentId, newSegment);&lt;br/&gt;
+&lt;br/&gt;
+            if (shouldBeNull != null) 
{
+                throw new IllegalStateException(&quot;Segment already exists. Possible concurrent access.&quot;);
+            }
&lt;p&gt;+&lt;br/&gt;
+            newSegment.openDB(context);&lt;br/&gt;
+            return newSegment;&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void openExisting(final ProcessorContext context) {&lt;br/&gt;
+    void openExisting(final InternalProcessorContext context) {&lt;br/&gt;
         try {&lt;/li&gt;
	&lt;li&gt;File dir = new File(context.stateDir(), name);&lt;br/&gt;
+            final File dir = new File(context.stateDir(), name);&lt;br/&gt;
             if (dir.exists()) {&lt;/li&gt;
	&lt;li&gt;String[] list = dir.list();&lt;br/&gt;
+                final String[] list = dir.list();&lt;br/&gt;
                 if (list != null) {&lt;/li&gt;
	&lt;li&gt;long[] segmentIds = new long&lt;span class=&quot;error&quot;&gt;&amp;#91;list.length&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+                    final long[] segmentIds = new long&lt;span class=&quot;error&quot;&gt;&amp;#91;list.length&amp;#93;&lt;/span&gt;;&lt;br/&gt;
                     for (int i = 0; i &amp;lt; list.length; i++)&lt;br/&gt;
                         segmentIds&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; = segmentIdFromSegmentName(list&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;, dir);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     // open segments in the id order&lt;br/&gt;
                     Arrays.sort(segmentIds);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (long segmentId : segmentIds) {&lt;br/&gt;
+                    for (final long segmentId : segmentIds) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                         if (segmentId &amp;gt;= 0) {
                             getOrCreateSegment(segmentId, context);
                         }@@ -121,89 +125,66 @@ void openExisting(final ProcessorContext context) {
                     throw new ProcessorStateException(String.format(&quot;dir %s doesn&apos;t exist and cannot be created for segments %s&quot;, dir, name));
                 }             }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;} catch (Exception ex) 
{
+        }
&lt;p&gt; catch (final Exception ex) &lt;/p&gt;
{
             // ignore
         }
&lt;p&gt;+&lt;br/&gt;
+        final long minLiveSegment = segmentId(context.streamTime() - retentionPeriod);&lt;br/&gt;
+        cleanupEarlierThan(minLiveSegment);&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     List&amp;lt;Segment&amp;gt; segments(final long timeFrom, final long timeTo) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long segFrom = Math.max(minSegmentId, segmentId(Math.max(0L, timeFrom)));&lt;/li&gt;
	&lt;li&gt;final long segTo = Math.min(maxSegmentId, segmentId(Math.min(maxSegmentId * segmentInterval, Math.max(0, timeTo))));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final List&amp;lt;Segment&amp;gt; segments = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (long segmentId = segFrom; segmentId &amp;lt;= segTo; segmentId++) {&lt;/li&gt;
	&lt;li&gt;Segment segment = getSegment(segmentId);&lt;/li&gt;
	&lt;li&gt;if (segment != null &amp;amp;&amp;amp; segment.isOpen()) {&lt;/li&gt;
	&lt;li&gt;try 
{
-                    segments.add(segment);
-                } catch (InvalidStateStoreException ise) {
-                    // segment may have been closed by streams thread;
-                }&lt;br/&gt;
+        final List&amp;lt;Segment&amp;gt; result = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        final NavigableMap&amp;lt;Long, Segment&amp;gt; segmentsInRange = segments.subMap(&lt;br/&gt;
+            segmentId(timeFrom), true,&lt;br/&gt;
+            segmentId(timeTo), true&lt;br/&gt;
+        );&lt;br/&gt;
+        for (final Segment segment : segmentsInRange.values()) {&lt;br/&gt;
+            if (segment.isOpen()) {
+                result.add(segment);
             }&lt;br/&gt;
         }&lt;br/&gt;
-        return segments;&lt;br/&gt;
+        return result;&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     List&amp;lt;Segment&amp;gt; allSegments() {&lt;br/&gt;
-        final List&amp;lt;Segment&amp;gt; segments = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
-        for (Segment segment : this.segments.values()) {&lt;br/&gt;
+        final List&amp;lt;Segment&amp;gt; result = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (final Segment segment : segments.values()) {&lt;br/&gt;
             if (segment.isOpen()) {&lt;br/&gt;
-                try {-                    segments.add(segment);-                }
&lt;p&gt; catch (InvalidStateStoreException ise) &lt;/p&gt;
{
-                    // segment may have been closed by streams thread;
-                }
&lt;p&gt;+                result.add(segment);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;Collections.sort(segments);&lt;/li&gt;
	&lt;li&gt;return segments;&lt;br/&gt;
+        return result;&lt;br/&gt;
     }&lt;/li&gt;
	&lt;li&gt;&lt;p&gt;+&lt;br/&gt;
     void flush() {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;for (Segment segment : segments.values()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        for (final Segment segment }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public void close() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Segment segment : segments.values()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        for (final Segment segment }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Segment getSegment(long segmentId) {&lt;/li&gt;
	&lt;li&gt;final Segment segment = segments.get(segmentId % numSegments);&lt;/li&gt;
	&lt;li&gt;if (!isSegment(segment, segmentId)) 
{
-            return null;
-        }&lt;/li&gt;
	&lt;li&gt;return segment;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private boolean isSegment(final Segment store, long segmentId) 
{
-        return store != null &amp;amp;&amp;amp; store.id == segmentId;
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private void cleanup(final long segmentId) {&lt;/li&gt;
	&lt;li&gt;final long oldestSegmentId = maxSegmentId &amp;lt; segmentId&lt;/li&gt;
	&lt;li&gt;? segmentId - numSegments&lt;/li&gt;
	&lt;li&gt;: maxSegmentId - numSegments;&lt;br/&gt;
+    private void cleanupEarlierThan(final long minLiveSegment) {&lt;br/&gt;
+        final Iterator&amp;lt;Map.Entry&amp;lt;Long, Segment&amp;gt;&amp;gt; toRemove =&lt;br/&gt;
+            segments.headMap(minLiveSegment, false).entrySet().iterator();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Long, Segment&amp;gt; segmentEntry : segments.entrySet()) {&lt;/li&gt;
	&lt;li&gt;final Segment segment = segmentEntry.getValue();&lt;/li&gt;
	&lt;li&gt;if (segment != null &amp;amp;&amp;amp; segment.id &amp;lt;= oldestSegmentId) {&lt;/li&gt;
	&lt;li&gt;segments.remove(segmentEntry.getKey());&lt;/li&gt;
	&lt;li&gt;segment.close();&lt;/li&gt;
	&lt;li&gt;try 
{
-                    segment.destroy();
-                }
&lt;p&gt; catch (IOException e) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;log.error(&quot;Error destroying {}&quot;, segment, e);&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
+        while (toRemove.hasNext()) {&lt;br/&gt;
+            final Map.Entry&amp;lt;Long, Segment&amp;gt; next = toRemove.next();&lt;br/&gt;
+            toRemove.remove();&lt;br/&gt;
+            final Segment segment = next.getValue();&lt;br/&gt;
+            segment.close();&lt;br/&gt;
+            try 
{
+                segment.destroy();
+            }
&lt;p&gt; catch (final IOException e) {&lt;br/&gt;
+                log.error(&quot;Error destroying {}&quot;, segment, e);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (oldestSegmentId &amp;gt; minSegmentId) 
{
-            minSegmentId = oldestSegmentId + 1;
-        }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private long segmentIdFromSegmentName(final String segmentName,&lt;br/&gt;
@@ -217,7 +198,7 @@ private long segmentIdFromSegmentName(final String segmentName,&lt;br/&gt;
         if (segmentSeparator == &apos;-&apos;) {&lt;br/&gt;
             try &lt;/p&gt;
{
                 segmentId = formatter.parse(segmentIdString).getTime() / segmentInterval;
-            }
&lt;p&gt; catch (ParseException e) &lt;/p&gt;
{
+            } catch (final ParseException e) {&lt;br/&gt;
                 log.warn(&quot;Unable to parse segmentName {} to a date. This segment will be skipped&quot;, segmentName);&lt;br/&gt;
                 return -1L;&lt;br/&gt;
             }&lt;br/&gt;
@@ -226,7 +207,7 @@ private long segmentIdFromSegmentName(final String segmentName,&lt;br/&gt;
             // for both new formats (with : or .) parse segment ID identically&lt;br/&gt;
             try {
                 segmentId = Long.parseLong(segmentIdString) / segmentInterval;
-            } catch (NumberFormatException e) {+            }
&lt;p&gt; catch (final NumberFormatException e) &lt;/p&gt;
{
                 throw new ProcessorStateException(&quot;Unable to parse segment id as long from segmentName: &quot; + segmentName);
             }

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContextTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContextTest.java&lt;br/&gt;
index d3f8dda8aee..1ef3f5dad76 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContextTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContextTest.java&lt;br/&gt;
@@ -216,5 +216,10 @@ public Cancellable schedule(long interval, PunctuationType type, Punctuator call&lt;/p&gt;

&lt;p&gt;         @Override&lt;br/&gt;
         public void commit() {}&lt;br/&gt;
+&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public Long streamTime() &lt;/p&gt;
{
+            throw new RuntimeException(&quot;not implemented&quot;);
+        }
&lt;p&gt;     }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java&lt;br/&gt;
index baa9ee49b44..194edb15f0a 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java&lt;br/&gt;
@@ -66,13 +66,13 @@ public void setUp() {&lt;br/&gt;
         final SessionKeySchema schema = new SessionKeySchema();&lt;br/&gt;
         schema.init(&quot;topic&quot;);&lt;br/&gt;
         final int retention = 60000;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int numSegments = 3;&lt;/li&gt;
	&lt;li&gt;underlying = new RocksDBSegmentedBytesStore(&quot;test&quot;, retention, numSegments, schema);&lt;br/&gt;
+        final int segmentInterval = 60_000;&lt;br/&gt;
+        underlying = new RocksDBSegmentedBytesStore(&quot;test&quot;, retention, segmentInterval, schema);&lt;br/&gt;
         final RocksDBSessionStore&amp;lt;Bytes, byte[]&amp;gt; sessionStore = new RocksDBSessionStore&amp;lt;&amp;gt;(underlying, Serdes.Bytes(), Serdes.ByteArray());&lt;br/&gt;
         cachingStore = new CachingSessionStore&amp;lt;&amp;gt;(sessionStore,&lt;br/&gt;
                                                  Serdes.String(),&lt;br/&gt;
                                                  Serdes.String(),&lt;/li&gt;
	&lt;li&gt;Segments.segmentInterval(retention, numSegments)&lt;br/&gt;
+                                                 segmentInterval&lt;br/&gt;
                                                  );&lt;br/&gt;
         cache = new ThreadCache(new LogContext(&quot;testCache &quot;), MAX_CACHE_SIZE_BYTES, new MockStreamsMetrics(new Metrics()));&lt;br/&gt;
         context = new InternalMockProcessorContext(TestUtils.tempDirectory(), null, null, null, cache);&lt;br/&gt;
@@ -185,13 +185,13 @@ public void shouldRemove() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldFetchCorrectlyAcrossSegments() {&lt;br/&gt;
         final Windowed&amp;lt;Bytes&amp;gt; a1 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(0, 0));&lt;/li&gt;
	&lt;li&gt;final Windowed&amp;lt;Bytes&amp;gt; a2 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(Segments.MIN_SEGMENT_INTERVAL, Segments.MIN_SEGMENT_INTERVAL));&lt;/li&gt;
	&lt;li&gt;final Windowed&amp;lt;Bytes&amp;gt; a3 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(Segments.MIN_SEGMENT_INTERVAL * 2, Segments.MIN_SEGMENT_INTERVAL * 2));&lt;br/&gt;
+        final Windowed&amp;lt;Bytes&amp;gt; a2 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(60_000, 60_000));&lt;br/&gt;
+        final Windowed&amp;lt;Bytes&amp;gt; a3 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(120_000, 120_000));&lt;br/&gt;
         cachingStore.put(a1, &quot;1&quot;.getBytes());&lt;br/&gt;
         cachingStore.put(a2, &quot;2&quot;.getBytes());&lt;br/&gt;
         cachingStore.put(a3, &quot;3&quot;.getBytes());&lt;br/&gt;
         cachingStore.flush();&lt;/li&gt;
	&lt;li&gt;final KeyValueIterator&amp;lt;Windowed&amp;lt;Bytes&amp;gt;, byte[]&amp;gt; results = cachingStore.findSessions(keyA, 0, Segments.MIN_SEGMENT_INTERVAL * 2);&lt;br/&gt;
+        final KeyValueIterator&amp;lt;Windowed&amp;lt;Bytes&amp;gt;, byte[]&amp;gt; results = cachingStore.findSessions(keyA, 0, 60_000 * 2);&lt;br/&gt;
         assertEquals(a1, results.next().key);&lt;br/&gt;
         assertEquals(a2, results.next().key);&lt;br/&gt;
         assertEquals(a3, results.next().key);&lt;br/&gt;
@@ -202,9 +202,9 @@ public void shouldFetchCorrectlyAcrossSegments() {&lt;br/&gt;
     public void shouldFetchRangeCorrectlyAcrossSegments() {&lt;br/&gt;
         final Windowed&amp;lt;Bytes&amp;gt; a1 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(0, 0));&lt;br/&gt;
         final Windowed&amp;lt;Bytes&amp;gt; aa1 = new Windowed&amp;lt;&amp;gt;(keyAA, new SessionWindow(0, 0));&lt;/li&gt;
	&lt;li&gt;final Windowed&amp;lt;Bytes&amp;gt; a2 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(Segments.MIN_SEGMENT_INTERVAL, Segments.MIN_SEGMENT_INTERVAL));&lt;/li&gt;
	&lt;li&gt;final Windowed&amp;lt;Bytes&amp;gt; a3 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(Segments.MIN_SEGMENT_INTERVAL * 2, Segments.MIN_SEGMENT_INTERVAL * 2));&lt;/li&gt;
	&lt;li&gt;final Windowed&amp;lt;Bytes&amp;gt; aa3 = new Windowed&amp;lt;&amp;gt;(keyAA, new SessionWindow(Segments.MIN_SEGMENT_INTERVAL * 2, Segments.MIN_SEGMENT_INTERVAL * 2));&lt;br/&gt;
+        final Windowed&amp;lt;Bytes&amp;gt; a2 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(60_000, 60_000));&lt;br/&gt;
+        final Windowed&amp;lt;Bytes&amp;gt; a3 = new Windowed&amp;lt;&amp;gt;(keyA, new SessionWindow(60_000 * 2, 60_000 * 2));&lt;br/&gt;
+        final Windowed&amp;lt;Bytes&amp;gt; aa3 = new Windowed&amp;lt;&amp;gt;(keyAA, new SessionWindow(60_000 * 2, 60_000 * 2));&lt;br/&gt;
         cachingStore.put(a1, &quot;1&quot;.getBytes());&lt;br/&gt;
         cachingStore.put(aa1, &quot;1&quot;.getBytes());&lt;br/&gt;
         cachingStore.put(a2, &quot;2&quot;.getBytes());&lt;br/&gt;
@@ -212,7 +212,7 @@ public void shouldFetchRangeCorrectlyAcrossSegments() {&lt;br/&gt;
         cachingStore.put(aa3, &quot;3&quot;.getBytes());&lt;br/&gt;
         cachingStore.flush();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final KeyValueIterator&amp;lt;Windowed&amp;lt;Bytes&amp;gt;, byte[]&amp;gt; rangeResults = cachingStore.findSessions(keyA, keyAA, 0, Segments.MIN_SEGMENT_INTERVAL * 2);&lt;br/&gt;
+        final KeyValueIterator&amp;lt;Windowed&amp;lt;Bytes&amp;gt;, byte[]&amp;gt; rangeResults = cachingStore.findSessions(keyA, keyAA, 0, 60_000 * 2);&lt;br/&gt;
         assertEquals(a1, rangeResults.next().key);&lt;br/&gt;
         assertEquals(aa1, rangeResults.next().key);&lt;br/&gt;
         assertEquals(a2, rangeResults.next().key);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingWindowStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingWindowStoreTest.java&lt;br/&gt;
index b8808caf475..118acecc6c1 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingWindowStoreTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingWindowStoreTest.java&lt;br/&gt;
@@ -20,7 +20,6 @@&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
 import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
 import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
-import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
 import org.apache.kafka.streams.errors.InvalidStateStoreException;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Windowed;&lt;br/&gt;
@@ -39,6 +38,7 @@&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.util.List;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import static org.apache.kafka.common.utils.Utils.mkList;&lt;br/&gt;
 import static org.apache.kafka.streams.state.internals.ThreadCacheTest.memoryCacheEntrySize;&lt;br/&gt;
 import static org.apache.kafka.test.StreamsTestUtils.toList;&lt;br/&gt;
 import static org.apache.kafka.test.StreamsTestUtils.verifyKeyValueList;&lt;br/&gt;
@@ -67,16 +67,16 @@&lt;br/&gt;
     @Before&lt;br/&gt;
     public void setUp() {&lt;br/&gt;
         keySchema = new WindowKeySchema();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int retention = 30000;&lt;/li&gt;
	&lt;li&gt;final int numSegments = 3;&lt;/li&gt;
	&lt;li&gt;underlying = new RocksDBSegmentedBytesStore(&quot;test&quot;, retention, numSegments, keySchema);&lt;br/&gt;
+        final int retention = 60_000;&lt;br/&gt;
+        final int segmentInterval = 60_000;&lt;br/&gt;
+        underlying = new RocksDBSegmentedBytesStore(&quot;test&quot;, retention, segmentInterval, keySchema);&lt;br/&gt;
         final RocksDBWindowStore&amp;lt;Bytes, byte[]&amp;gt; windowStore = new RocksDBWindowStore&amp;lt;&amp;gt;(underlying, Serdes.Bytes(), Serdes.ByteArray(), false, WINDOW_SIZE);&lt;br/&gt;
         cacheListener = new CachingKeyValueStoreTest.CacheFlushListenerStub&amp;lt;&amp;gt;();&lt;br/&gt;
         cachingStore = new CachingWindowStore&amp;lt;&amp;gt;(windowStore,&lt;br/&gt;
                                                 Serdes.String(),&lt;br/&gt;
                                                 Serdes.String(),&lt;br/&gt;
                                                 WINDOW_SIZE,&lt;/li&gt;
	&lt;li&gt;Segments.segmentInterval(retention, numSegments));&lt;br/&gt;
+                                                segmentInterval);&lt;br/&gt;
         cachingStore.setFlushListener(cacheListener, false);&lt;br/&gt;
         cache = new ThreadCache(new LogContext(&quot;testCache &quot;), MAX_CACHE_SIZE_BYTES, new MockStreamsMetrics(new Metrics()));&lt;br/&gt;
         topic = &quot;topic&quot;;&lt;br/&gt;
@@ -313,7 +313,7 @@ public void shouldFetchAndIterateOverExactKeys() 
{
         cachingStore.put(bytesKey(&quot;aa&quot;), bytesValue(&quot;0004&quot;), 1);
         cachingStore.put(bytesKey(&quot;a&quot;), bytesValue(&quot;0005&quot;), 60000);
 
-        final List&amp;lt;KeyValue&amp;lt;Long, byte[]&amp;gt;&amp;gt; expected = Utils.mkList(KeyValue.pair(0L, bytesValue(&quot;0001&quot;)), KeyValue.pair(1L, bytesValue(&quot;0003&quot;)), KeyValue.pair(60000L, bytesValue(&quot;0005&quot;)));
+        final List&amp;lt;KeyValue&amp;lt;Long, byte[]&amp;gt;&amp;gt; expected = mkList(KeyValue.pair(0L, bytesValue(&quot;0001&quot;)), KeyValue.pair(1L, bytesValue(&quot;0003&quot;)), KeyValue.pair(60000L, bytesValue(&quot;0005&quot;)));
         final List&amp;lt;KeyValue&amp;lt;Long, byte[]&amp;gt;&amp;gt; actual = toList(cachingStore.fetch(bytesKey(&quot;a&quot;), 0, Long.MAX_VALUE));
         verifyKeyValueList(expected, actual);
     }
&lt;p&gt;@@ -326,13 +326,13 @@ public void shouldFetchAndIterateOverKeyRange() &lt;/p&gt;
{
         cachingStore.put(bytesKey(&quot;aa&quot;), bytesValue(&quot;0004&quot;), 1);
         cachingStore.put(bytesKey(&quot;a&quot;), bytesValue(&quot;0005&quot;), 60000);
 
-        verifyKeyValueList(Utils.mkList(windowedPair(&quot;a&quot;, &quot;0001&quot;, 0), windowedPair(&quot;a&quot;, &quot;0003&quot;, 1), windowedPair(&quot;a&quot;, &quot;0005&quot;, 60000L)),
+        verifyKeyValueList(mkList(windowedPair(&quot;a&quot;, &quot;0001&quot;, 0), windowedPair(&quot;a&quot;, &quot;0003&quot;, 1), windowedPair(&quot;a&quot;, &quot;0005&quot;, 60000L)),
                            toList(cachingStore.fetch(bytesKey(&quot;a&quot;), bytesKey(&quot;a&quot;), 0, Long.MAX_VALUE)));
 
-        verifyKeyValueList(Utils.mkList(windowedPair(&quot;aa&quot;, &quot;0002&quot;, 0), windowedPair(&quot;aa&quot;, &quot;0004&quot;, 1)),
+        verifyKeyValueList(mkList(windowedPair(&quot;aa&quot;, &quot;0002&quot;, 0), windowedPair(&quot;aa&quot;, &quot;0004&quot;, 1)),
                            toList(cachingStore.fetch(bytesKey(&quot;aa&quot;), bytesKey(&quot;aa&quot;), 0, Long.MAX_VALUE)));
 
-        verifyKeyValueList(Utils.mkList(windowedPair(&quot;a&quot;, &quot;0001&quot;, 0), windowedPair(&quot;a&quot;, &quot;0003&quot;, 1), windowedPair(&quot;aa&quot;, &quot;0002&quot;, 0), windowedPair(&quot;aa&quot;, &quot;0004&quot;, 1), windowedPair(&quot;a&quot;, &quot;0005&quot;, 60000L)),
+        verifyKeyValueList(mkList(windowedPair(&quot;a&quot;, &quot;0001&quot;, 0), windowedPair(&quot;a&quot;, &quot;0003&quot;, 1), windowedPair(&quot;aa&quot;, &quot;0002&quot;, 0), windowedPair(&quot;aa&quot;, &quot;0004&quot;, 1), windowedPair(&quot;a&quot;, &quot;0005&quot;, 60000L)),
                            toList(cachingStore.fetch(bytesKey(&quot;a&quot;), bytesKey(&quot;aa&quot;), 0, Long.MAX_VALUE)));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java&lt;br/&gt;
index db6d1d1bf22..d7a72833765 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java&lt;br/&gt;
@@ -53,7 +53,6 @@&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.SimpleTimeZone;&lt;/p&gt;

&lt;p&gt;-import static org.apache.kafka.streams.state.internals.Segments.segmentInterval;&lt;br/&gt;
 import static org.hamcrest.CoreMatchers.equalTo;&lt;br/&gt;
 import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
@@ -65,6 +64,7 @@&lt;br/&gt;
 public class RocksDBSegmentedBytesStoreTest {&lt;/p&gt;

&lt;p&gt;     private final long retention = 1000;&lt;br/&gt;
+    private final long segmentInterval = 60_000;&lt;br/&gt;
     private final int numSegments = 3;&lt;br/&gt;
     private InternalMockProcessorContext context;&lt;br/&gt;
     private final String storeName = &quot;bytes-store&quot;;&lt;br/&gt;
@@ -102,7 +102,7 @@ public void before() {&lt;/p&gt;

&lt;p&gt;         bytesStore = new RocksDBSegmentedBytesStore(storeName,&lt;br/&gt;
                 retention,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;numSegments,&lt;br/&gt;
+                segmentInterval,&lt;br/&gt;
                 schema);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         stateDir = TestUtils.tempDirectory();&lt;br/&gt;
@@ -163,7 +163,7 @@ public void shouldRemove() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldRollSegments() {&lt;br/&gt;
         // just to validate directories&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segments segments = new Segments(storeName, retention, numSegments);&lt;br/&gt;
+        final Segments segments = new Segments(storeName, retention, segmentInterval);&lt;br/&gt;
         final String key = &quot;a&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)), serializeValue(50));&lt;br/&gt;
@@ -186,7 +186,7 @@ public void shouldRollSegments() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldGetAllSegments() {&lt;br/&gt;
         // just to validate directories&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segments segments = new Segments(storeName, retention, numSegments);&lt;br/&gt;
+        final Segments segments = new Segments(storeName, retention, segmentInterval);&lt;br/&gt;
         final String key = &quot;a&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)), serializeValue(50L));&lt;br/&gt;
@@ -206,7 +206,7 @@ public void shouldGetAllSegments() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldFetchAllSegments() {&lt;br/&gt;
         // just to validate directories&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segments segments = new Segments(storeName, retention, numSegments);&lt;br/&gt;
+        final Segments segments = new Segments(storeName, retention, segmentInterval);&lt;br/&gt;
         final String key = &quot;a&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)), serializeValue(50L));&lt;br/&gt;
@@ -225,7 +225,7 @@ public void shouldFetchAllSegments() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldLoadSegementsWithOldStyleDateFormattedName() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segments segments = new Segments(storeName, retention, numSegments);&lt;br/&gt;
+        final Segments segments = new Segments(storeName, retention, segmentInterval);&lt;br/&gt;
         final String key = &quot;a&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)), serializeValue(50L));&lt;br/&gt;
@@ -237,7 +237,7 @@ public void shouldLoadSegementsWithOldStyleDateFormattedName() {&lt;br/&gt;
         final Long segmentId = Long.parseLong(nameParts&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;);&lt;br/&gt;
         final SimpleDateFormat formatter = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;);&lt;br/&gt;
         formatter.setTimeZone(new SimpleTimeZone(0, &quot;UTC&quot;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final String formatted = formatter.format(new Date(segmentId * segmentInterval(retention, numSegments)));&lt;br/&gt;
+        final String formatted = formatter.format(new Date(segmentId * segmentInterval));&lt;br/&gt;
         final File parent = new File(stateDir, storeName);&lt;br/&gt;
         final File oldStyleName = new File(parent, nameParts&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; + &quot;-&quot; + formatted);&lt;br/&gt;
         assertTrue(new File(parent, firstSegmentName).renameTo(oldStyleName));&lt;br/&gt;
@@ -256,7 +256,7 @@ public void shouldLoadSegementsWithOldStyleDateFormattedName() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldLoadSegementsWithOldStyleColonFormattedName() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segments segments = new Segments(storeName, retention, numSegments);&lt;br/&gt;
+        final Segments segments = new Segments(storeName, retention, segmentInterval);&lt;br/&gt;
         final String key = &quot;a&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         bytesStore.put(serializeKey(new Windowed&amp;lt;&amp;gt;(key, windows&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;)), serializeValue(50L));&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java&lt;br/&gt;
index bcb411b0709..c95cbbada98 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java&lt;br/&gt;
@@ -154,7 +154,7 @@ public void shouldFindSessionsToMerge() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldFetchExactKeys() &lt;/p&gt;
{
         final RocksDBSegmentedBytesStore bytesStore =
-                new RocksDBSegmentedBytesStore(&quot;session-store&quot;, 0x7a00000000000000L, 2, new SessionKeySchema());
+                new RocksDBSegmentedBytesStore(&quot;session-store&quot;, 0x7a00000000000000L, 0x7a00000000000000L, new SessionKeySchema());
 
         sessionStore = new RocksDBSessionStore&amp;lt;&amp;gt;(bytesStore,
                                                  Serdes.String(),
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java
index c436e9e588f..2a84a7b12e9 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java
@@ -69,9 +69,9 @@
     private final int numSegments = 3;
     private final long windowSize = 3L;
     private final String windowName = &quot;window&quot;;
-    private final long segmentSize = Segments.MIN_SEGMENT_INTERVAL;
+    private final long segmentSize = 60_000;
     private final long retentionPeriod = segmentSize * (numSegments - 1);
-    private final Segments segments = new Segments(windowName, retentionPeriod, numSegments);
+    private final Segments segments = new Segments(windowName, retentionPeriod, segmentSize);
     private final StateSerdes&amp;lt;Integer, String&amp;gt; serdes = new StateSerdes&amp;lt;&amp;gt;(&quot;&quot;, Serdes.Integer(), Serdes.String());
 
     private final List&amp;lt;KeyValue&amp;lt;byte[], byte[]&amp;gt;&amp;gt; changeLog = new ArrayList&amp;lt;&amp;gt;();
@@ -120,10 +120,6 @@
         return store;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private WindowStore&amp;lt;Integer, String&amp;gt; createWindowStore(final ProcessorContext context) 
{
-        return createWindowStore(context, false);
-    }
&lt;p&gt;-&lt;br/&gt;
     @After&lt;br/&gt;
     public void closeStore() {&lt;br/&gt;
         if (windowStore != null) {&lt;br/&gt;
@@ -133,24 +129,24 @@ public void closeStore() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldOnlyIterateOpenSegments() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         long currentTime = 0;&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(currentTime));&lt;br/&gt;
+        setCurrentTime(currentTime);&lt;br/&gt;
         windowStore.put(1, &quot;one&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         currentTime = currentTime + segmentSize;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(currentTime));&lt;br/&gt;
+        setCurrentTime(currentTime);&lt;br/&gt;
         windowStore.put(1, &quot;two&quot;);&lt;br/&gt;
         currentTime = currentTime + segmentSize;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(currentTime));&lt;br/&gt;
+        setCurrentTime(currentTime);&lt;br/&gt;
         windowStore.put(1, &quot;three&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final WindowStoreIterator&amp;lt;String&amp;gt; iterator = windowStore.fetch(1, 0, currentTime);&lt;/p&gt;

&lt;p&gt;         // roll to the next segment that will close the first&lt;br/&gt;
         currentTime = currentTime + segmentSize;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(currentTime));&lt;br/&gt;
+        setCurrentTime(currentTime);&lt;br/&gt;
         windowStore.put(1, &quot;four&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // should only have 2 values as the first segment is no longer open&lt;br/&gt;
@@ -159,13 +155,18 @@ public void shouldOnlyIterateOpenSegments() &lt;/p&gt;
{
         assertFalse(iterator.hasNext());
     }

&lt;p&gt;+    private void setCurrentTime(final long currentTime) &lt;/p&gt;
{
+        context.setRecordContext(createRecordContext(currentTime));
+        context.setStreamTime(currentTime);
+    }
&lt;p&gt;+&lt;br/&gt;
     private ProcessorRecordContext createRecordContext(final long time) &lt;/p&gt;
{
         return new ProcessorRecordContext(time, 0, 0, &quot;topic&quot;, null);
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void testRangeAndSinglePointFetch() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         final long startTime = segmentSize - 4L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         putFirstBatch(windowStore, startTime, context);&lt;br/&gt;
@@ -224,7 +225,7 @@ public void testRangeAndSinglePointFetch() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldGetAll() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         final long startTime = segmentSize - 4L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         putFirstBatch(windowStore, startTime, context);&lt;br/&gt;
@@ -243,7 +244,7 @@ public void shouldGetAll() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldFetchAllInTimeRange() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         final long startTime = segmentSize - 4L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         putFirstBatch(windowStore, startTime, context);&lt;br/&gt;
@@ -272,7 +273,7 @@ public void shouldFetchAllInTimeRange() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testFetchRange() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         final long startTime = segmentSize - 4L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         putFirstBatch(windowStore, startTime, context);&lt;br/&gt;
@@ -320,7 +321,7 @@ public void testFetchRange() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testPutAndFetchBefore() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         final long startTime = segmentSize - 4L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         putFirstBatch(windowStore, startTime, context);&lt;br/&gt;
@@ -366,7 +367,7 @@ public void testPutAndFetchBefore() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testPutAndFetchAfter() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         final long startTime = segmentSize - 4L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         putFirstBatch(windowStore, startTime, context);&lt;br/&gt;
@@ -415,7 +416,7 @@ public void testPutSameKeyTimestamp() {&lt;br/&gt;
         windowStore = createWindowStore(context, true);&lt;br/&gt;
         final long startTime = segmentSize - 4L;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime));&lt;br/&gt;
+        setCurrentTime(startTime);&lt;br/&gt;
         windowStore.put(0, &quot;zero&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(Utils.mkList(&quot;zero&quot;), toList(windowStore.fetch(0, startTime - windowSize, startTime + windowSize)));&lt;br/&gt;
@@ -440,21 +441,20 @@ public void testPutSameKeyTimestamp() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testRolling() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // to validate segments&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segments segments = new Segments(windowName, retentionPeriod, numSegments);&lt;br/&gt;
         final long startTime = segmentSize * 2;&lt;br/&gt;
         final long increment = segmentSize / 2;&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime));&lt;br/&gt;
+        setCurrentTime(startTime);&lt;br/&gt;
         windowStore.put(0, &quot;zero&quot;);&lt;br/&gt;
         assertEquals(Utils.mkSet(segments.segmentName(2)), segmentDirs(baseDir));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment));&lt;br/&gt;
+        setCurrentTime(startTime + increment);&lt;br/&gt;
         windowStore.put(1, &quot;one&quot;);&lt;br/&gt;
         assertEquals(Utils.mkSet(segments.segmentName(2)), segmentDirs(baseDir));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 2));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 2);&lt;br/&gt;
         windowStore.put(2, &quot;two&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(&lt;br/&gt;
@@ -464,7 +464,7 @@ public void testRolling() {&lt;br/&gt;
             segmentDirs(baseDir)&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 4));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 4);&lt;br/&gt;
         windowStore.put(4, &quot;four&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(&lt;br/&gt;
@@ -476,7 +476,7 @@ public void testRolling() {&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 5));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 5);&lt;br/&gt;
         windowStore.put(5, &quot;five&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(&lt;br/&gt;
@@ -494,7 +494,7 @@ public void testRolling() {&lt;br/&gt;
         assertEquals(Utils.mkList(&quot;four&quot;), toList(windowStore.fetch(4, startTime + increment * 4 - windowSize, startTime + increment * 4 + windowSize)));&lt;br/&gt;
         assertEquals(Utils.mkList(&quot;five&quot;), toList(windowStore.fetch(5, startTime + increment * 5 - windowSize, startTime + increment * 5 + windowSize)));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 6));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 6);&lt;br/&gt;
         windowStore.put(6, &quot;six&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(&lt;br/&gt;
@@ -515,7 +515,7 @@ public void testRolling() {&lt;br/&gt;
         assertEquals(Utils.mkList(&quot;six&quot;), toList(windowStore.fetch(6, startTime + increment * 6 - windowSize, startTime + increment * 6 + windowSize)));&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 7));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 7);&lt;br/&gt;
         windowStore.put(7, &quot;seven&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(&lt;br/&gt;
@@ -535,7 +535,7 @@ public void testRolling() {&lt;br/&gt;
         assertEquals(Utils.mkList(&quot;six&quot;), toList(windowStore.fetch(6, startTime + increment * 6 - windowSize, startTime + increment * 6 + windowSize)));&lt;br/&gt;
         assertEquals(Utils.mkList(&quot;seven&quot;), toList(windowStore.fetch(7, startTime + increment * 7 - windowSize, startTime + increment * 7 + windowSize)));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 8));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 8);&lt;br/&gt;
         windowStore.put(8, &quot;eight&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(&lt;br/&gt;
@@ -576,24 +576,24 @@ public void testRestore() throws IOException {&lt;br/&gt;
         final long startTime = segmentSize * 2;&lt;br/&gt;
         final long increment = segmentSize / 2;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime));&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
+        setCurrentTime(startTime);&lt;br/&gt;
         windowStore.put(0, &quot;zero&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment));&lt;br/&gt;
+        setCurrentTime(startTime + increment);&lt;br/&gt;
         windowStore.put(1, &quot;one&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 2));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 2);&lt;br/&gt;
         windowStore.put(2, &quot;two&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 3));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 3);&lt;br/&gt;
         windowStore.put(3, &quot;three&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 4));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 4);&lt;br/&gt;
         windowStore.put(4, &quot;four&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 5));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 5);&lt;br/&gt;
         windowStore.put(5, &quot;five&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 6));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 6);&lt;br/&gt;
         windowStore.put(6, &quot;six&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 7));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 7);&lt;br/&gt;
         windowStore.put(7, &quot;seven&quot;);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(startTime + increment * 8));&lt;br/&gt;
+        setCurrentTime(startTime + increment * 8);&lt;br/&gt;
         windowStore.put(8, &quot;eight&quot;);&lt;br/&gt;
         windowStore.flush();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -602,7 +602,7 @@ public void testRestore() throws IOException {&lt;br/&gt;
         // remove local store image&lt;br/&gt;
         Utils.delete(baseDir);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
         assertEquals(Utils.mkList(), toList(windowStore.fetch(0, startTime - windowSize, startTime + windowSize)));&lt;br/&gt;
         assertEquals(Utils.mkList(), toList(windowStore.fetch(1, startTime + increment - windowSize, startTime + increment + windowSize)));&lt;br/&gt;
         assertEquals(Utils.mkList(), toList(windowStore.fetch(2, startTime + increment * 2 - windowSize, startTime + increment * 2 + windowSize)));&lt;br/&gt;
@@ -637,14 +637,14 @@ public void testRestore() throws IOException {&lt;br/&gt;
     public void testSegmentMaintenance() {&lt;br/&gt;
         windowStore = createWindowStore(context, true);&lt;br/&gt;
         context.setTime(0L);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(0));&lt;br/&gt;
+        setCurrentTime(0);&lt;br/&gt;
         windowStore.put(0, &quot;v&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(segments.segmentName(0L)),&lt;br/&gt;
             segmentDirs(baseDir)&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(59999));&lt;br/&gt;
+        setCurrentTime(59999);&lt;br/&gt;
         windowStore.put(0, &quot;v&quot;);&lt;br/&gt;
         windowStore.put(0, &quot;v&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
@@ -652,7 +652,7 @@ public void testSegmentMaintenance() {&lt;br/&gt;
             segmentDirs(baseDir)&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(60000));&lt;br/&gt;
+        setCurrentTime(60000);&lt;br/&gt;
         windowStore.put(0, &quot;v&quot;);&lt;br/&gt;
         assertEquals(&lt;br/&gt;
             Utils.mkSet(segments.segmentName(0L), segments.segmentName(1L)),&lt;br/&gt;
@@ -675,7 +675,7 @@ public void testSegmentMaintenance() {&lt;br/&gt;
             segmentDirs(baseDir)&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(180000));&lt;br/&gt;
+        setCurrentTime(180000);&lt;br/&gt;
         windowStore.put(0, &quot;v&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         iter = windowStore.fetch(0, 0L, 240000L);&lt;br/&gt;
@@ -691,7 +691,7 @@ public void testSegmentMaintenance() {&lt;br/&gt;
             segmentDirs(baseDir)&lt;br/&gt;
         );&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(300000));&lt;br/&gt;
+        setCurrentTime(300000);&lt;br/&gt;
         windowStore.put(0, &quot;v&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         iter = windowStore.fetch(0, 240000L, 1000000L);&lt;br/&gt;
@@ -714,7 +714,7 @@ public void testSegmentMaintenance() {&lt;br/&gt;
     public void testInitialLoading() {&lt;br/&gt;
         final File storeDir = new File(baseDir, windowName);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         new File(storeDir, segments.segmentName(0L)).mkdir();&lt;br/&gt;
         new File(storeDir, segments.segmentName(1L)).mkdir();&lt;br/&gt;
@@ -725,12 +725,16 @@ public void testInitialLoading() {&lt;br/&gt;
         new File(storeDir, segments.segmentName(6L)).mkdir();&lt;br/&gt;
         windowStore.close();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;br/&gt;
+        context.setStreamTime(segmentSize * 6L);&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(&lt;/li&gt;
	&lt;li&gt;Utils.mkSet(segments.segmentName(4L), segments.segmentName(5L), segments.segmentName(6L)),&lt;/li&gt;
	&lt;li&gt;segmentDirs(baseDir)&lt;/li&gt;
	&lt;li&gt;);&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; expected = Utils.mkList(segments.segmentName(4L), segments.segmentName(5L), segments.segmentName(6L));&lt;br/&gt;
+        expected.sort(String::compareTo);&lt;br/&gt;
+&lt;br/&gt;
+        final List&amp;lt;String&amp;gt; actual = Utils.toList(segmentDirs(baseDir).iterator());&lt;br/&gt;
+        actual.sort(String::compareTo);&lt;br/&gt;
+&lt;br/&gt;
+        assertEquals(expected, actual);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         try (final WindowStoreIterator iter = windowStore.fetch(0, 0L, 1000000L)) {&lt;br/&gt;
             while (iter.hasNext()) {&lt;br/&gt;
@@ -746,8 +750,8 @@ public void testInitialLoading() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldCloseOpenIteratorsWhenStoreIsClosedAndNotThrowInvalidStateStoreExceptionOnHasNext() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;windowStore = createWindowStore(context);&lt;/li&gt;
	&lt;li&gt;context.setRecordContext(createRecordContext(0));&lt;br/&gt;
+        windowStore = createWindowStore(context, false);&lt;br/&gt;
+        setCurrentTime(0);&lt;br/&gt;
         windowStore.put(1, &quot;one&quot;, 1L);&lt;br/&gt;
         windowStore.put(1, &quot;two&quot;, 2L);&lt;br/&gt;
         windowStore.put(1, &quot;three&quot;, 3L);&lt;br/&gt;
@@ -802,31 +806,31 @@ public void shouldFetchAndIterateOverExactKeys() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test(expected = NullPointerException.class)&lt;br/&gt;
     public void shouldThrowNullPointerExceptionOnPutNullKey() &lt;/p&gt;
{
-        windowStore = createWindowStore(context);
+        windowStore = createWindowStore(context, false);
         windowStore.put(null, &quot;anyValue&quot;);
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldNotThrowNullPointerExceptionOnPutNullValue() &lt;/p&gt;
{
-        windowStore = createWindowStore(context);
+        windowStore = createWindowStore(context, false);
         windowStore.put(1, null);
     }

&lt;p&gt;     @Test(expected = NullPointerException.class)&lt;br/&gt;
     public void shouldThrowNullPointerExceptionOnGetNullKey() &lt;/p&gt;
{
-        windowStore = createWindowStore(context);
+        windowStore = createWindowStore(context, false);
         windowStore.fetch(null, 1L, 2L);
     }

&lt;p&gt;     @Test(expected = NullPointerException.class)&lt;br/&gt;
     public void shouldThrowNullPointerExceptionOnRangeNullFromKey() &lt;/p&gt;
{
-        windowStore = createWindowStore(context);
+        windowStore = createWindowStore(context, false);
         windowStore.fetch(null, 2, 1L, 2L);
     }

&lt;p&gt;     @Test(expected = NullPointerException.class)&lt;br/&gt;
     public void shouldThrowNullPointerExceptionOnRangeNullToKey() &lt;/p&gt;
{
-        windowStore = createWindowStore(context);
+        windowStore = createWindowStore(context, false);
         windowStore.fetch(1, null, 1L, 2L);
     }

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentsTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentsTest.java&lt;br/&gt;
index bfa317ddc51..1fc08534b18 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentsTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentsTest.java&lt;br/&gt;
@@ -46,7 +46,7 @@&lt;br/&gt;
     private static final int NUM_SEGMENTS = 5;&lt;br/&gt;
     private InternalMockProcessorContext context;&lt;br/&gt;
     private Segments segments;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long segmentInterval;&lt;br/&gt;
+    private final long segmentInterval = 60_000L;&lt;br/&gt;
     private File stateDirectory;&lt;br/&gt;
     private String storeName = &quot;test&quot;;&lt;br/&gt;
     private final int retentionPeriod =  4 * 60 * 1000;&lt;br/&gt;
@@ -59,8 +59,7 @@ public void createContext() 
{
                                            Serdes.Long(),
                                            new NoOpRecordCollector(),
                                            new ThreadCache(new LogContext(&quot;testCache &quot;), 0, new MockStreamsMetrics(new Metrics())));
-        segments = new Segments(storeName, retentionPeriod, NUM_SEGMENTS);
-        segmentInterval = Segments.segmentInterval(retentionPeriod, NUM_SEGMENTS);
+        segments = new Segments(storeName, retentionPeriod, segmentInterval);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @After&lt;br/&gt;
@@ -78,7 +77,7 @@ public void shouldGetSegmentIdsFromTimestamp() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldBaseSegmentIntervalOnRetentionAndNumSegments() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segments segments = new Segments(&quot;test&quot;, 8 * 60 * 1000, 5);&lt;br/&gt;
+        final Segments segments = new Segments(&quot;test&quot;, 8 * 60 * 1000, 120_000);&lt;br/&gt;
         assertEquals(0, segments.segmentId(0));&lt;br/&gt;
         assertEquals(0, segments.segmentId(60000));&lt;br/&gt;
         assertEquals(1, segments.segmentId(120000));&lt;br/&gt;
@@ -93,9 +92,9 @@ public void shouldGetSegmentNameFromId() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldCreateSegments() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segment segment1 = segments.getOrCreateSegment(0, context);&lt;/li&gt;
	&lt;li&gt;final Segment segment2 = segments.getOrCreateSegment(1, context);&lt;/li&gt;
	&lt;li&gt;final Segment segment3 = segments.getOrCreateSegment(2, context);&lt;br/&gt;
+        final Segment segment1 = segments.getOrCreateSegmentIfLive(0, context);&lt;br/&gt;
+        final Segment segment2 = segments.getOrCreateSegmentIfLive(1, context);&lt;br/&gt;
+        final Segment segment3 = segments.getOrCreateSegmentIfLive(2, context);&lt;br/&gt;
         assertTrue(new File(context.stateDir(), &quot;test/test.0&quot;).isDirectory());&lt;br/&gt;
         assertTrue(new File(context.stateDir(), &quot;test/test.&quot; + segmentInterval).isDirectory());&lt;br/&gt;
         assertTrue(new File(context.stateDir(), &quot;test/test.&quot; + 2 * segmentInterval).isDirectory());&lt;br/&gt;
@@ -106,16 +105,17 @@ public void shouldCreateSegments() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldNotCreateSegmentThatIsAlreadyExpired() &lt;/p&gt;
{
-        segments.getOrCreateSegment(7, context);
-        assertNull(segments.getOrCreateSegment(0, context));
+        updateStreamTimeAndCreateSegment(7);
+        assertNull(segments.getOrCreateSegmentIfLive(0, context));
         assertFalse(new File(context.stateDir(), &quot;test/test.0&quot;).exists());
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldCleanupSegmentsThatHaveExpired() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segment segment1 = segments.getOrCreateSegment(0, context);&lt;/li&gt;
	&lt;li&gt;final Segment segment2 = segments.getOrCreateSegment(1, context);&lt;/li&gt;
	&lt;li&gt;final Segment segment3 = segments.getOrCreateSegment(7, context);&lt;br/&gt;
+        final Segment segment1 = segments.getOrCreateSegmentIfLive(0, context);&lt;br/&gt;
+        final Segment segment2 = segments.getOrCreateSegmentIfLive(1, context);&lt;br/&gt;
+        context.setStreamTime(segmentInterval * 7);&lt;br/&gt;
+        final Segment segment3 = segments.getOrCreateSegmentIfLive(7, context);&lt;br/&gt;
         assertFalse(segment1.isOpen());&lt;br/&gt;
         assertFalse(segment2.isOpen());&lt;br/&gt;
         assertTrue(segment3.isOpen());&lt;br/&gt;
@@ -126,22 +126,22 @@ public void shouldCleanupSegmentsThatHaveExpired() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldGetSegmentForTimestamp() &lt;/p&gt;
{
-        final Segment segment = segments.getOrCreateSegment(0, context);
-        segments.getOrCreateSegment(1, context);
+        final Segment segment = segments.getOrCreateSegmentIfLive(0, context);
+        segments.getOrCreateSegmentIfLive(1, context);
         assertEquals(segment, segments.getSegmentForTimestamp(0L));
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldGetCorrectSegmentString() &lt;/p&gt;
{
-        final Segment segment = segments.getOrCreateSegment(0, context);
+        final Segment segment = segments.getOrCreateSegmentIfLive(0, context);
         assertEquals(&quot;Segment(id=0, name=test.0)&quot;, segment.toString());
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldCloseAllOpenSegments() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Segment first = segments.getOrCreateSegment(0, context);&lt;/li&gt;
	&lt;li&gt;final Segment second = segments.getOrCreateSegment(1, context);&lt;/li&gt;
	&lt;li&gt;final Segment third = segments.getOrCreateSegment(2, context);&lt;br/&gt;
+        final Segment first = segments.getOrCreateSegmentIfLive(0, context);&lt;br/&gt;
+        final Segment second = segments.getOrCreateSegmentIfLive(1, context);&lt;br/&gt;
+        final Segment third = segments.getOrCreateSegmentIfLive(2, context);&lt;br/&gt;
         segments.close();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertFalse(first.isOpen());&lt;br/&gt;
@@ -151,15 +151,15 @@ public void shouldCloseAllOpenSegments() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldOpenExistingSegments() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;segments.getOrCreateSegment(0, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(1, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(2, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(3, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(4, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(0, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(1, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(2, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(3, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(4, context);&lt;br/&gt;
         // close existing.&lt;br/&gt;
         segments.close();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;segments = new Segments(&quot;test&quot;, 4 * 60 * 1000, 5);&lt;br/&gt;
+        segments = new Segments(&quot;test&quot;, 4 * 60 * 1000, 60_000);&lt;br/&gt;
         segments.openExisting(context);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertTrue(segments.getSegmentForTimestamp(0).isOpen());&lt;br/&gt;
@@ -171,11 +171,16 @@ public void shouldOpenExistingSegments() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldGetSegmentsWithinTimeRange() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;segments.getOrCreateSegment(0, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(1, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(2, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(3, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(4, context);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(0);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(1);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(2);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(3);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(4);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(0, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(1, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(2, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(3, context);&lt;br/&gt;
+        segments.getOrCreateSegmentIfLive(4, context);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;Segment&amp;gt; segments = this.segments.segments(0, 2 * 60 * 1000);&lt;br/&gt;
         assertEquals(3, segments.size());&lt;br/&gt;
@@ -186,11 +191,11 @@ public void shouldGetSegmentsWithinTimeRange() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldGetSegmentsWithinTimeRangeOutOfOrder() throws Exception {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;segments.getOrCreateSegment(4, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(2, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(0, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(1, context);&lt;/li&gt;
	&lt;li&gt;segments.getOrCreateSegment(3, context);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(4);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(2);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(0);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(1);&lt;br/&gt;
+        updateStreamTimeAndCreateSegment(3);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final List&amp;lt;Segment&amp;gt; segments = this.segments.segments(0, 2 * 60 * 1000);&lt;br/&gt;
         assertEquals(3, segments.size());&lt;br/&gt;
@@ -201,22 +206,45 @@ public void shouldGetSegmentsWithinTimeRangeOutOfOrder() throws Exception {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldRollSegments() &lt;/p&gt;
{
-        segments.getOrCreateSegment(0, context);
+        updateStreamTimeAndCreateSegment(0);
         verifyCorrectSegments(0, 1);
-        segments.getOrCreateSegment(1, context);
+        updateStreamTimeAndCreateSegment(1);
         verifyCorrectSegments(0, 2);
-        segments.getOrCreateSegment(2, context);
+        updateStreamTimeAndCreateSegment(2);
         verifyCorrectSegments(0, 3);
-        segments.getOrCreateSegment(3, context);
+        updateStreamTimeAndCreateSegment(3);
         verifyCorrectSegments(0, 4);
-        segments.getOrCreateSegment(4, context);
+        updateStreamTimeAndCreateSegment(4);
         verifyCorrectSegments(0, 5);
-        segments.getOrCreateSegment(5, context);
+        updateStreamTimeAndCreateSegment(5);
         verifyCorrectSegments(1, 5);
-        segments.getOrCreateSegment(6, context);
+        updateStreamTimeAndCreateSegment(6);
         verifyCorrectSegments(2, 5);
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void futureEventsShouldNotCauseSegmentRoll() &lt;/p&gt;
{
+        updateStreamTimeAndCreateSegment(0);
+        verifyCorrectSegments(0, 1);
+        updateStreamTimeAndCreateSegment(1);
+        verifyCorrectSegments(0, 2);
+        updateStreamTimeAndCreateSegment(2);
+        verifyCorrectSegments(0, 3);
+        updateStreamTimeAndCreateSegment(3);
+        verifyCorrectSegments(0, 4);
+        updateStreamTimeAndCreateSegment(4);
+        verifyCorrectSegments(0, 5);
+        segments.getOrCreateSegmentIfLive(5, context);
+        verifyCorrectSegments(0, 6);
+        segments.getOrCreateSegmentIfLive(6, context);
+        verifyCorrectSegments(0, 7);
+    }
&lt;p&gt;+&lt;br/&gt;
+    private void updateStreamTimeAndCreateSegment(final int segment) &lt;/p&gt;
{
+        context.setStreamTime(segmentInterval * segment);
+        segments.getOrCreateSegmentIfLive(segment, context);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldUpdateSegmentFileNameFromOldDateFormatToNewFormat() throws Exception {&lt;br/&gt;
         final String storeDirectoryPath = stateDirectory.getAbsolutePath() + File.separator + storeName;&lt;br/&gt;
@@ -260,7 +288,7 @@ public void shouldUpdateSegmentFileNameFromOldColonFormatToNewFormat() throws Ex&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldClearSegmentsOnClose() &lt;/p&gt;
{
-        segments.getOrCreateSegment(0, context);
+        segments.getOrCreateSegmentIfLive(0, context);
         segments.close();
         assertThat(segments.getSegmentForTimestamp(0), is(nullValue()));
     }
&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java b/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java&lt;br/&gt;
index bb42d1c4a26..3251489366d 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java&lt;br/&gt;
@@ -61,6 +61,7 @@&lt;br/&gt;
     private Serde&amp;lt;?&amp;gt; keySerde;&lt;br/&gt;
     private Serde&amp;lt;?&amp;gt; valSerde;&lt;br/&gt;
     private long timestamp = -1L;&lt;br/&gt;
+    private long streamTime = -1;&lt;/p&gt;

&lt;p&gt;     public InternalMockProcessorContext() {&lt;br/&gt;
         this(null,&lt;br/&gt;
@@ -179,6 +180,15 @@ public void setValueSerde(final Serde&amp;lt;?&amp;gt; valSerde) {&lt;br/&gt;
     @Override&lt;br/&gt;
     public void initialized() {}&lt;/p&gt;

&lt;p&gt;+    public void setStreamTime(final long time) &lt;/p&gt;
{
+        streamTime = time;
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public Long streamTime() &lt;/p&gt;
{
+        return streamTime;
+    }
&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public File stateDir() {&lt;br/&gt;
         if (stateDir == null) {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/test/NoOpProcessorContext.java b/streams/src/test/java/org/apache/kafka/test/NoOpProcessorContext.java&lt;br/&gt;
index 92f84c5526b..7b695c4640d 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/test/NoOpProcessorContext.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/test/NoOpProcessorContext.java&lt;br/&gt;
@@ -85,6 +85,11 @@ public void initialized() &lt;/p&gt;
{
         initialized = true;
     }

&lt;p&gt;+    @Override&lt;br/&gt;
+    public Long streamTime() &lt;/p&gt;
{
+        throw new RuntimeException(&quot;not implemented&quot;);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public void register(final StateStore store,&lt;br/&gt;
                          final StateRestoreCallback stateRestoreCallback) {&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 21 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3uznr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>