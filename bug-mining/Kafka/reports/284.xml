<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:38:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-927] Integrate controlled shutdown into kafka shutdown hook</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-927</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The controlled shutdown mechanism should be integrated into the software for better operational benefits. Also few optimizations can be done to reduce unnecessary rpc and zk calls. This patch has been tested on a prod like environment by doing rolling bounces continuously for a day. The average time of doing a rolling bounce with controlled shutdown for a cluster with 7 nodes without this patch is 340 seconds. With this patch it reduces to 220 seconds. Also it ensures correctness in scenarios where the controller shrinks the isr and the new leader could place the broker to be shutdown back into the isr.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12650258">KAFKA-927</key>
            <summary>Integrate controlled shutdown into kafka shutdown hook</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sriramsub">Sriram</assignee>
                                    <reporter username="sriramsub">Sriram</reporter>
                        <labels>
                    </labels>
                <created>Fri, 31 May 2013 00:50:09 +0000</created>
                <updated>Tue, 4 Jun 2013 17:40:32 +0000</updated>
                            <resolved>Mon, 3 Jun 2013 23:09:31 +0000</resolved>
                                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13671595" author="nehanarkhede" created="Fri, 31 May 2013 15:56:04 +0000"  >&lt;p&gt;Sriram,&lt;/p&gt;

&lt;p&gt;This patch doesn&apos;t compile with the following errors - &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaApis.scala:133: not found: type ControlledShutdownRequest&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;     val controlledShutdownRequest = request.requestObj.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ControlledShutdownRequest&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;                                                                     ^&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaApis.scala:135: not found: type ControlledShutdownResponse&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;     val controlledShutdownResponse = new ControlledShutdownResponse(controlledShutdownRequest.correlationId,&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;                                          ^&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:28: ControlledShutdownResponse is not a member of kafka.api&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; import kafka.api.&lt;/p&gt;
{ControlledShutdownResponse, ControlledShutdownRequest}
&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;        ^&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:157: not found: type ControlledShutdownRequest&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;             val request = new ControlledShutdownRequest(correlationId.getAndIncrement, config.brokerId)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;                               ^&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:160: not found: value ControlledShutdownResponse&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;             val shutdownResponse = ControlledShutdownResponse.readFrom(response.buffer)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;                                    ^&lt;/p&gt;</comment>
                            <comment id="13671608" author="junrao" created="Fri, 31 May 2013 16:10:52 +0000"  >&lt;p&gt;Thanks for the patch. Overall, a well thought-out patch. Some comments.&lt;/p&gt;

&lt;p&gt;1. KafkaController.shutdownBroker: We should probably only do controlled shutdown if the controller is active.&lt;/p&gt;

&lt;p&gt;2. KafkaApis: If the controller is not active, we should send an errorcode back to the ControlledShutdownRequest.&lt;/p&gt;

&lt;p&gt;3. KafkaServer: I am not sure that we should call replicaManager.replicaFetcherManager.closeAllFetchers() at the beginning of the controlled shutdown. Once the fetchers are closed, the affected leaders have to wait for the timeout before committing new messages since they have to shrink the ISR. Instead, it&apos;s better if we let the fetcher to be closed through leaderAndIsr requests from the controller.&lt;/p&gt;

&lt;p&gt;4. KafkaConfig: Could we consolidate controlledShutdownMaxRetries and controlledShutdownEnable to one config controlledShutDownWaitTime? If that value is &amp;lt;=0, no controlled shutdown is done. Otherwise, we will try controlled shutdown until that time has passed.&lt;/p&gt;

&lt;p&gt;5. With the new logic added in ReplicaManager/Partition, I am not sure if the old controlled shutdown tool still works properly. Should we just remove the tool and the jmx hook?&lt;/p&gt;

&lt;p&gt;6. There are new files not included in the patch.&lt;/p&gt;</comment>
                            <comment id="13671633" author="nehanarkhede" created="Fri, 31 May 2013 16:45:03 +0000"  >&lt;p&gt;Thanks for the patch, very well thought out! Few comments -&lt;br/&gt;
1. KafkaServer&lt;br/&gt;
1.1 doControlledShutdown()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Is there a reason why we cannot just invoke shutdown() on the ReplicaManager instead of hacking into the replica fetcher manager and shutting down the fetchers ?&lt;/li&gt;
	&lt;li&gt;&quot;starting controlled shutdown&quot; -&amp;gt; &quot;Starting controlled shutdown&quot;. Though it is not introduced in this patch, can we please change the same in the shutdown() API as well?&lt;/li&gt;
	&lt;li&gt;Typo -&amp;gt; shutdownSuceeded&lt;/li&gt;
	&lt;li&gt;This method is pretty big and slightly hard to read, for someone who is new to controlled shutdown. Can we move controller discovery/connection logic to a separate API named connectToController() ? -&lt;br/&gt;
        val controllerId = ZkUtils.getController(kafkaZookeeper.getZookeeperClient)&lt;br/&gt;
        ZkUtils.getBrokerInfo(kafkaZookeeper.getZookeeperClient, controllerId) match {&lt;br/&gt;
          case Some(broker) =&amp;gt;&lt;br/&gt;
            if (channel == null || prevController == null || !prevController.equals(broker)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {              // if this is the first attempt or if the controller has changed, create a channel to the most recent              // controller              if (channel != null) {
                channel.disconnect()
              }              channel = new BlockingChannel(broker.host, broker.port,                BlockingChannel.UseDefaultBufferSize,                BlockingChannel.UseDefaultBufferSize,                config.controllerSocketTimeoutMs)              channel.connect()              prevController = broker            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;          case None=&amp;gt;&lt;br/&gt;
            //ignore and try again&lt;br/&gt;
        }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;I also think it will be cleaner for the loop to look like, but it&apos;s upto you &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
      while (!shutdownSucceeded &amp;amp;&amp;amp; remainingRetries &amp;gt; 0) 
{
         val controller = connectToController(zkClient)
         val shutdownSucceeded = sendControllerShutdownRequest(controller)
         if(!shutdownSucceeded)
            Thread.sleep(...)
         remainingRetries -= 1
      }&lt;/li&gt;
	&lt;li&gt;Can we add either a warn or an info message that the broker will retry controlled shutdown after n ms ?&lt;br/&gt;
        if (!shutdownSuceeded) 
{
          Thread.sleep(config.controlledShutdownRetryBackoffMs)
        }&lt;/li&gt;
	&lt;li&gt;Can we rename doControlledShutdown() to just controlledShutdown(). This will follow the naming conventions in the rest of the code, since we don&apos;t name methods doSomething.&lt;/li&gt;
	&lt;li&gt;Let&apos;s remove the zkClient unused variable&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;2. KafkaApis&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If the controller is not active, we should send the appropriate error code&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;3. KafkaController&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;getPartitionsAssignedToBroker() does not need to read from zookeeper. The controller should already have the latest data available as the controllerLock is acquired at this point.&lt;/li&gt;
	&lt;li&gt;The following updates zookeeper which is not required since the leader would&apos;ve done that long before the controller does it. This is because you shutdown the replica fetchers at the beginning of controlled shutdown. It will be much faster to just send a leader and isr request with the shrunk ISR to the existing leader, though I doubt that is required as well.&lt;br/&gt;
            else 
{
              // if the broker is a follower, updates the isr in ZK and notifies the current leader
              replicaStateMachine.handleStateChanges(Set(PartitionAndReplica(topicAndPartition.topic,
                topicAndPartition.partition, id)), OfflineReplica)
            }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;4. We want to know that the broker is rejecting the become-follower request in the state change log when the following happens. So it is not enough to just surround the addFetcher call with this condition &lt;br/&gt;
5. New files are not included in the patch&lt;br/&gt;
          if (!replicaManager.isShuttingDown.get()) &lt;/p&gt;
{
            // start fetcher thread to current leader if we are not shutting down
            replicaFetcherManager.addFetcher(topic, partitionId, localReplica.logEndOffset, leaderBroker)
          }</comment>
                            <comment id="13671922" author="sriramsub" created="Sat, 1 Jun 2013 00:22:49 +0000"  >&lt;p&gt;Thank you for the review guys.&lt;/p&gt;

&lt;p&gt;Jun - &lt;/p&gt;

&lt;p&gt;1. I did not add an explicit check since the state manager anyway throw. Added one explicitly.&lt;br/&gt;
3. I have thought different approaches to do it exactly right but all of them seems ugly. I will think a little bit more but it should not be a blocker since most of the active topic would realize the isr change soon due to the number of messages.&lt;br/&gt;
4. I don&apos;t like to overload config value with multiple meanings. The config name should do exactly what it is meant to in my opinion.&lt;br/&gt;
5. It works perfectly fine. Infact there is a unit test for the old tool that works fine. The functionality of the tool has not changed.&lt;br/&gt;
6. Added the missing files&lt;/p&gt;

&lt;p&gt;Neha -&lt;br/&gt;
1.&lt;br/&gt;
1.1 Done&lt;br/&gt;
1.2 Done&lt;br/&gt;
1.3 Done&lt;br/&gt;
1.4 Refactoring the method is actually pretty tricky. There are actually multiple dependencies between the methods (previousController, channels). The individual functions (if we manage to refactor), dont really do a single operation to provide a good name to it. I have added comments to make things clear. &lt;br/&gt;
1.6 Added a warn&lt;br/&gt;
1.7 renamed the method name&lt;/p&gt;

&lt;p&gt;2. Done&lt;br/&gt;
3. If the leader did realize that the follower has fallen off the isr soon then there are no issues. However that does not seem to be the case. We use the number of messages or time period to decide when to remove the follower from the isr. The default time period used is 10sec. So it could take as long as 10 seconds to realize the isr change. However the controller may do it much sooner and hence we get the better of the two. Having said that, there is an ugly fix to make the leader realize the isr change immediately after the replica thread stops but I need more time to think about that.&lt;br/&gt;
4. updated the state change log&lt;br/&gt;
5. added the missing files&lt;/p&gt;
</comment>
                            <comment id="13673182" author="junrao" created="Mon, 3 Jun 2013 14:55:53 +0000"  >&lt;p&gt;Thanks for patch v2. A few more comments:&lt;/p&gt;

&lt;p&gt;20. KafkaController: If when shutdownBroker is called, the controller is no longer active, both state machines will throw an exception on state change calls. However, the issue is that we add the shutdown broker to controllerContext.shuttingDownBrokerIds and it&apos;s never reset. This may become a problem if this broker becomes a controller again. At the minimum, we need to reset controllerContext.shuttingDownBrokerIds in  onControllerFailover(). However, I am a bit confused why we never reset controllerContext.shuttingDownBrokerIds and the shutdown logic still works.&lt;/p&gt;

&lt;p&gt;21. ControlledShutdownRequest.handleError(): We should probably set partitionsRemaining in ControlledShutdownResponse to empty instead of null, since the serialization of ControlledShutdownResponse doesn&apos;t handle partitionsRemaining being null.&lt;/p&gt;

&lt;p&gt;22. testRollingBounce:&lt;br/&gt;
22.1 The test makes sure that the leader for topic1 is changed after broker 0 is shutdown. However, the leader for topic1 could be on broker 1 initially. In this case, the leader won&apos;t be changed after broker 0 is shutdown.&lt;br/&gt;
22.2 The default controlledShutdownRetryBackoffMs is 5secs, which is probably too long for the unit test. &lt;/p&gt;

&lt;p&gt;23. KafkaServer: We need to handle the errorCode in ControlledShutdownResponse since the controller may have moved after we send the ControlledShutdown request.&lt;/p&gt;

&lt;p&gt;From the previous review:&lt;br/&gt;
3. I think a simple solution is to (1) not call replicaManager.replicaFetcherManager.closeAllFetchers() in KafkaServer during shutdown; (2) in KafkaController.shutdownBroker(), for each partition on the shutdown broker, we first send a stopReplicaRequest to it for that partition before going through the state machine logic. Since the state machine logic involves ZK reads/writes, it&apos;s very likely that the stopReplicaRequest will reach the broker before the subsequent LeaderAndIsr requests. So, in most cases, the leader should be able to shrink ISR quicker than the timeout, without churns in ISR.&lt;/p&gt;</comment>
                            <comment id="13673360" author="sriramsub" created="Mon, 3 Jun 2013 17:50:35 +0000"  >&lt;p&gt;Realized my previous patch did not have my latest changes just the new files.&lt;/p&gt;

&lt;p&gt;20. shuttingDownBrokerIds does get updated on broker failure&lt;br/&gt;
21 done&lt;br/&gt;
22.1 i had already fixed this. The new patch should have the change&lt;br/&gt;
23. This is also handled in the new patch&lt;/p&gt;

&lt;p&gt;3. That sounds reasonable among all the hacky fixes. &lt;/p&gt;</comment>
                            <comment id="13673380" author="nehanarkhede" created="Mon, 3 Jun 2013 18:07:55 +0000"  >&lt;p&gt;Thanks for the revised v2 patch. Few more comments -&lt;/p&gt;

&lt;p&gt;1. KafkaServer&lt;br/&gt;
1.1 startupComplete should either be a volatile variable to AtomicBoolean. Two different threads call startup() and controlledShutdown(), which modify startupComplete.&lt;br/&gt;
1.2 In controlledShutdown(), we need to handle error codes in ControlledShutdownResponse explicitly. It can happen that the error code is set and partitionsRemaining are 0, which will lead to errors.&lt;/p&gt;

&lt;p&gt;2. Partition&lt;/p&gt;

&lt;p&gt;From previous review #4, if the broker has to ignore the become follower request anyway, does it make sense to even process part of it and truncate log etc ?&lt;/p&gt;

&lt;p&gt;3. From previous review #3, I meant that it is pointless to do the ZK write on the controller since right after the write, since the follower hasn&apos;t received the stop replica request and the leader hasn&apos;t received shrunk isr, the broker being shut down will get added back to ISR. You can verify that this happens from the logs. It also makes controlled shutdown very slow since typically in production we move ~1000 partitions from the broker and zk writes can take ~20ms which means several seconds wasted just doing the ZK writes. Instead, it is enough to let the leader shrink the isr by sending it the leader and isr request. On the other hand, we can argue that the OfflineReplica state change itself should be changed to avoid the ZK write. But that is a bigger change, so we should avoid that right now.&lt;/p&gt;</comment>
                            <comment id="13673538" author="sriramsub" created="Mon, 3 Jun 2013 20:49:44 +0000"  >&lt;p&gt;1.1 Done&lt;br/&gt;
1.2 Done&lt;br/&gt;
2. We would need to do some of these to ensure the new leader is updated and the log itself is going to be truncated either on startup or shutdown. Hence did not feel a strong reason to make this path more optimized.&lt;/p&gt;

&lt;p&gt;3. As we spoke offline, there seems to be edge case where not updating ZK could lead to bad things happening. So updating ZK before leaderisr request.&lt;/p&gt;</comment>
                            <comment id="13673651" author="junrao" created="Mon, 3 Jun 2013 21:57:46 +0000"  >&lt;p&gt;Thanks for patch v3. A few more comments:&lt;/p&gt;

&lt;p&gt;30. KafkaServer:&lt;br/&gt;
30.1 Could you combine isShuttingDown and startupComplete?&lt;br/&gt;
30.2 In controlledShutdown(), it&apos;s not clear if it&apos;s worth caching the socket channel. Technically, it&apos;s possible for a controller to come back on the broker with the same id, but with a different broker host/port. It&apos;s simpler to just always close the socket channel on each ControlledShutdownRequest and create a new channel on retry.&lt;/p&gt;

&lt;p&gt;31. KafkaController:&lt;br/&gt;
31.1 remove unused import java.util.concurrent.{Semaphore&lt;br/&gt;
31.2 I think we still need to set shuttingDownBrokerIds to empty in onControllerFailover(). A controller may failover during a controlled shutdown and later regain the controllership. OnBrokerFailure() is only called if the controller is active. So shuttingDownBrokerIds may not be empty when the controllership switches back.&lt;/p&gt;</comment>
                            <comment id="13673659" author="nehanarkhede" created="Mon, 3 Jun 2013 22:02:08 +0000"  >&lt;p&gt;+1 on v3 other than Jun&apos;s comments.&lt;/p&gt;</comment>
                            <comment id="13673666" author="sriramsub" created="Mon, 3 Jun 2013 22:07:57 +0000"  >&lt;p&gt;30.1 Don&apos;t feel strong about this. I think it makes things less readable with not much savings&lt;br/&gt;
30.2 The new broker includes the host and port and hence it works.&lt;/p&gt;

&lt;p&gt;31.1 Done&lt;br/&gt;
31.2 This is already there in the previous patch. It is in InitializeControllerContext&lt;/p&gt;</comment>
                            <comment id="13673718" author="sriramsub" created="Mon, 3 Jun 2013 22:44:09 +0000"  >&lt;p&gt;From offline feedback&lt;br/&gt;
1. reset startupcomplete flag on shutdown for unit test&lt;br/&gt;
2. cleaned channel before shutting down&lt;/p&gt;</comment>
                            <comment id="13673753" author="junrao" created="Mon, 3 Jun 2013 23:09:31 +0000"  >&lt;p&gt;Thanks for patch v4. +1 and committed to 0.8.&lt;/p&gt;</comment>
                            <comment id="13674598" author="jjkoshy" created="Tue, 4 Jun 2013 17:40:32 +0000"  >&lt;p&gt;+1 - sorry I got to this late.&lt;/p&gt;

&lt;p&gt;Small nit: the scaladoc for shutdown broker needs an edit which we will clean up later.&lt;br/&gt;
We probably don&apos;t need the adminTest&apos;s testShutdownBroker given that the rolling bounce test exercises the same logic.&lt;/p&gt;

&lt;p&gt;Also, I think we can close &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-817&quot; title=&quot;Implement a zookeeper path-based controlled shutdown tool&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-817&quot;&gt;&lt;del&gt;KAFKA-817&lt;/del&gt;&lt;/a&gt; - another approach with similar goals.&lt;/p&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12585900" name="KAFKA-927-v2-revised.patch" size="61298" author="sriramsub" created="Mon, 3 Jun 2013 17:50:35 +0000"/>
                            <attachment id="12585687" name="KAFKA-927-v2.patch" size="42674" author="sriramsub" created="Sat, 1 Jun 2013 00:23:13 +0000"/>
                            <attachment id="12585967" name="KAFKA-927-v3-removeimports.patch" size="67069" author="sriramsub" created="Mon, 3 Jun 2013 22:08:13 +0000"/>
                            <attachment id="12585936" name="KAFKA-927-v3.patch" size="66079" author="sriramsub" created="Mon, 3 Jun 2013 20:49:44 +0000"/>
                            <attachment id="12585977" name="KAFKA-927-v4.patch" size="73774" author="sriramsub" created="Mon, 3 Jun 2013 22:44:09 +0000"/>
                            <attachment id="12585519" name="KAFKA-927.patch" size="28674" author="sriramsub" created="Fri, 31 May 2013 00:52:00 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>330585</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 24 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1l17b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>330919</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>