<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:52:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-3068] NetworkClient may connect to a different Kafka cluster than originally configured</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-3068</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;In &lt;a href=&quot;https://github.com/apache/kafka/pull/290&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/290&lt;/a&gt;, we added the logic to cache all brokers (id and ip) that the client has ever seen. If we can&apos;t find an available broker from the current Metadata, we will pick a broker that we have ever seen (in NetworkClient.leastLoadedNode()).&lt;/p&gt;

&lt;p&gt;One potential problem this logic can introduce is the following. Suppose that we have a broker with id 1 in a Kafka cluster. A producer client remembers this broker in nodesEverSeen. At some point, we bring down this broker and use the host in a different Kafka cluster. Then, the producer client uses this broker from nodesEverSeen to refresh metadata. It will find the metadata in a different Kafka cluster and start producing data there.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12927639">KAFKA-3068</key>
            <summary>NetworkClient may connect to a different Kafka cluster than originally configured</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="enothereska">Eno Thereska</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                    </labels>
                <created>Wed, 6 Jan 2016 00:57:50 +0000</created>
                <updated>Wed, 18 Jan 2023 11:52:08 +0000</updated>
                            <resolved>Tue, 2 Feb 2016 17:36:56 +0000</resolved>
                                    <version>0.9.0.0</version>
                                    <fixVersion>0.9.0.1</fixVersion>
                    <fixVersion>0.10.0.0</fixVersion>
                                    <component>clients</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15084470" author="junrao" created="Wed, 6 Jan 2016 01:02:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enothereska&quot; class=&quot;user-hover&quot; rel=&quot;enothereska&quot;&gt;enothereska&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;, since you all worked on that pull request, do you think this is an issue?&lt;/p&gt;</comment>
                            <comment id="15084478" author="guozhang" created="Wed, 6 Jan 2016 01:08:41 +0000"  >&lt;p&gt;Even without this caching, we can still potentially hit this issue if Kafka nodes are migrating to a different cluster but with the same host, right?&lt;/p&gt;</comment>
                            <comment id="15084481" author="hachikuji" created="Wed, 6 Jan 2016 01:09:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; It does seem like an issue to me (can&apos;t believe we all missed it). From memory, I think the problem that the nodesEverSeen collection was trying to solve was what to do when the last node in the cluster becomes unreachable. In this case, the only thing the client can do is keep trying to reconnect to the one node indefinitely (since we have no other ways to discover other nodes in the cluster). By keeping track of the history of nodes, we have an out in this case and we can retry against one of the other nodes that we connected to before. But, as you say, this can lead to other problems. It seems like what we should do in this case is maybe revert to the bootstrap brokers in configuration.&lt;/p&gt;</comment>
                            <comment id="15084507" author="junrao" created="Wed, 6 Jan 2016 01:33:16 +0000"  >&lt;p&gt;Yes, without this caching, in theory, this is still possible. However, the window won&apos;t be long since the migrated broker will be removed on the next metadata refresh, which will either happen periodically, or when one of the existing connections is lost.&lt;/p&gt;</comment>
                            <comment id="15084516" author="guozhang" created="Wed, 6 Jan 2016 01:43:03 +0000"  >&lt;p&gt;That is right... maybe we should just use bootstrap servers as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; suggested.&lt;/p&gt;</comment>
                            <comment id="15084609" author="ijuma" created="Wed, 6 Jan 2016 03:09:41 +0000"  >&lt;p&gt;Reverting to the bootstrap brokers in the config seems like a sensible and less surprising option (it is not unreasonable to expect users to have to update the config if they want to move one of the brokers in the config to a different cluster).&lt;/p&gt;

&lt;p&gt;This kind of thing does raise the question of whether clusters should be have an identity to prevent accidental usage (this is easily achieved in a secured cluster so that may be enough).&lt;/p&gt;</comment>
                            <comment id="15085618" author="enothereska" created="Wed, 6 Jan 2016 15:00:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;: I think a proper solution would make use of a cluster identity in addition to broker id+host. The problems with using bootstrap brokers is that 1) users might not specify enough bootstrap brokers and 2) bootstrap brokers can also move to a different cluster as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; points out. While the windows of vulnerability will be small as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; points out, it will still be a problem. One option would have been to query zookeeper (which is the cluster identity in a way) and get the latest brokers from it instead of caching them in the client. With the new clients that is not an option anymore though.&lt;/p&gt;</comment>
                            <comment id="15085663" author="ijuma" created="Wed, 6 Jan 2016 15:23:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enothereska&quot; class=&quot;user-hover&quot; rel=&quot;enothereska&quot;&gt;enothereska&lt;/a&gt;, I agree that a better solution would involve cluster identity, but that seems like a bigger change. The reason is that the new clients should not access ZK directly, only via the Kafka brokers (security being a major reason). With regards to your point (1), I also thought of that, but failed brokers are also an issue when a client is started, so there is a good reason to include a reasonable number of brokers in the bootstrap brokers config.&lt;/p&gt;

&lt;p&gt;It seems like if we favour safety and there is no easy way to make use of cluster identity, then we should do the bootstrap brokers config change now and explore the better solution in a subsequent JIRA. Thoughts?&lt;/p&gt;</comment>
                            <comment id="15085682" author="enothereska" created="Wed, 6 Jan 2016 15:29:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;: a slightly different solution would be to keep just the latest N seen nodes (e.g., N = 3). That has the advantage of freshness over keeping the (potentially old) bootstrap brokers. Currently we have N = inf, which means we keep all seen brokers.  &lt;/p&gt;</comment>
                            <comment id="15085688" author="ijuma" created="Wed, 6 Jan 2016 15:36:50 +0000"  >&lt;p&gt;Yeah. Or expire brokers based on time. Let&apos;s see what others think. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15086554" author="hachikuji" created="Thu, 7 Jan 2016 00:17:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;,&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enothereska&quot; class=&quot;user-hover&quot; rel=&quot;enothereska&quot;&gt;enothereska&lt;/a&gt; Introducing a notion of cluster identity seems like a good idea to solve this problem more generally. I&apos;m not sure I understand the point about not having enough bootstrap brokers though. The cached metadata always contains the full list of brokers in the cluster (i.e. those registered in Zookeeper). The only time we might need to dip into an additional set is when all of the known brokers simultaneously become unreachable. When that happens, it seems to make more sense to revert to the list of bootstrap brokers rather than attempting to reach a broker which we discovered previously but had itself became unreachable at some point (if it was still reachable on the last metadata refresh, then it would be contained in the metadata). And yes, the bootstrap brokers can also move to another cluster, but I would consider this a user configuration error rather than an application error. You can hardly fault an application for trying to connect to the endpoints that had been explicitly configured, but you might if it connects to a broker it previously discovered which had been shutdown and moved to another cluster. Does that make sense?&lt;/p&gt;</comment>
                            <comment id="15086580" author="ijuma" created="Thu, 7 Jan 2016 00:39:43 +0000"  >&lt;p&gt;It does to me.&lt;/p&gt;</comment>
                            <comment id="15086717" author="junrao" created="Thu, 7 Jan 2016 03:01:47 +0000"  >&lt;p&gt;There are 2 possible ways of configuring bootstrap servers: (1) Using a VIP on a load balancer. In the case, we can expect the VIP to map to the current live brokers. (2) Using a list of broker hosts. In this case, it&apos;s the user&apos;s responsibility to make sure the list is up to date and contains at least one live broker. When we hit a case that none of the current brokers (from last metadata response) is connectable (e.g., the cluster is shrunk to 1 node, then that node dies and other brokers are restarted), falling back to the bootstrap servers will help if option (1) is used since the VIP will allow us to connect to a live broker. If option (2) is used, falling back to the bootstrap servers may not help if none of the bootstrap servers is reachable. However, in this case, it&apos;s really the user&apos;s responsibility to re-configure bootstrap servers and restart the producer. So, overall, it seems that falling back to bootstrap servers when all existing connections are gone will help.&lt;/p&gt;

&lt;p&gt;Now, on caching old brokers long than the metadata refresh interval. Currently, we can say if you ever want to reuse a server in a different Kafka cluster, wait for at least the metadata refresh interval after taking the broker down. If we cache the old brokers longer, this reasoning will be more complicated. Also, from the above, I am not sure if old brokers are more useful than the configured bootstrap servers.&lt;/p&gt;

&lt;p&gt;Finally, we discussed to fall back to the bootstrap servers in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1303&quot; title=&quot;metadata request in the new producer can be delayed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1303&quot;&gt;&lt;del&gt;KAFKA-1303&lt;/del&gt;&lt;/a&gt; for a different scenario early on, but didn&apos;t pursue that in the end.&lt;/p&gt;</comment>
                            <comment id="15087309" author="enothereska" created="Thu, 7 Jan 2016 12:47:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; I understand the concerns. What I don&apos;t like about using the bootstrap servers is that the problem is punted to the user (to provide enough bootstrap servers, to keep track of whether they have moved and to restart producers when they do so. For a long running cluster of 100+ machines that is hard to do.). &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;: between these two non-ideal solutions do we have a sense which one is the least worst? I can change the code to use the bootstrap brokers but I am worried an equal number of users may be dissatisfied from that. &lt;/p&gt;

&lt;p&gt;So it&apos;s between caching the latest N seen brokers or caching the N bootstrap servers. &lt;/p&gt;</comment>
                            <comment id="15087313" author="enothereska" created="Thu, 7 Jan 2016 12:51:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;: it boils down to whether the network client wants to cache the latest N brokers seen or cache the N bootstrap brokers. For a long running cluster my intuition is that caching the latest N brokers is best. Currently we cache too many (all) the seen brokers. I think caching the latest N is better than caching the N bootstrap servers.&lt;/p&gt;</comment>
                            <comment id="15087737" author="junrao" created="Thu, 7 Jan 2016 17:41:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enothereska&quot; class=&quot;user-hover&quot; rel=&quot;enothereska&quot;&gt;enothereska&lt;/a&gt;, yes, your concerns are valid. When running in the Cloud (say EC2), since broker hosts can change often, some of the users build their own discovery service. On startup, the producer will grab the current broker list from the discovery service and use those to build the bootstrap servers. The expectation is that this list will only be used once when the producer starts up. If we change that behavior by falling back to the bootstrap servers, weird things may happen since the bootstrap servers may not be valid, or worse reused in a different Kafka cluster. Like you said, falling back to old brokers may not be ideal either.&lt;/p&gt;

&lt;p&gt;Give that in a common setup, it should be rare to have all brokers returned in metadata response be unreachable, in this jira, we can probably just remove the caching of old brokers and just wait when all brokers are not connectable.&lt;/p&gt;

&lt;p&gt;We can do a KIP discussion on improving this (whether to cache old brokers or use bootstrap servers) if we think this is still a significant enough issue. &lt;/p&gt;</comment>
                            <comment id="15087931" author="hachikuji" created="Thu, 7 Jan 2016 19:29:32 +0000"  >&lt;p&gt;We retain in the metadata cache the full list of currently known brokers, so an additional cache wouldn&apos;t be helpful unless it contains nodes that were not in the last metadata refresh (in other words, nodes that were not registered at the time of that refresh). I guess that means you&apos;d have to set N at least as large as the maximum size of the cluster to get any benefit from this cache policy. It might be more useful to expire based on time as Ismael suggested since frees the user from tuning N and it&apos;s also more predictable. And it&apos;s probably never a good idea to try connecting to a node that hasn&apos;t been registered for more than an hour. Since this is a last-resort option, reverting to the bootstrap brokers seems simpler and less surprising for users, but as Jun pointed out, its usefulness depends on how the user has configured it.&lt;/p&gt;

&lt;p&gt;Anyway, this is a pretty unlikely case to hit in practice since all of the registered brokers have to become unreachable at the same time. For clusters larger than a few nodes, probably the only way this happens is if the client becomes partitioned or if you lose all brokers simultaneously (e.g. from a power outage). For those cases, perhaps the best the client can do anyway is continue retrying with whatever broker list it last knew about. I do think that the bootstrap brokers should also be tried in case the user has configured a VIP.&lt;/p&gt;</comment>
                            <comment id="15088145" author="ewencp" created="Thu, 7 Jan 2016 21:29:06 +0000"  >&lt;p&gt;You can get into bad states without losing everything simultaneously, and in situations that aren&apos;t unrealistic in practice: see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1843?focusedCommentId=14517659&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14517659&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-1843?focusedCommentId=14517659&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14517659&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15088200" author="ewencp" created="Thu, 7 Jan 2016 21:58:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; It&apos;s definitely a real issue &amp;#8211; see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1843?focusedCommentId=14517659&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14517659&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-1843?focusedCommentId=14517659&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14517659&lt;/a&gt; where you can get into unrecoverable situations if a Kafka node is decomissioned or in the case of a failure. This doesn&apos;t necessarily seem like it would be that rare to me &amp;#8211; sure, it requires there to be failures, but any app that produces to a single topic could encounter a case like this since it&apos;ll have a very limited set of brokers to work with. Multiple people reported encountering this issue, although as far as I&apos;ve seen it may only have been in testing environments: &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/kafka-users/201505.mbox/%3CCAJ7fV7qbfQZrD0EoDJmTRq725Q+ZiirnctwJHyhQLdm+5CwR0Q@mail.gmail.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/kafka-users/201505.mbox/%3CCAJ7fV7qbfQZrD0EoDJmTRq725Q+ZiirnctwJHyhQLdm+5CwR0Q@mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It sounds like &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1843&quot; title=&quot;Metadata fetch/refresh in new producer should handle all node connection states gracefully&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1843&quot;&gt;KAFKA-1843&lt;/a&gt; is messy enough that it needs a more complete discussion to resolve and maybe a KIP. Just waiting on the old list doesn&apos;t help in the situation described above if there&apos;s an actual hardware failure that takes the node out entirely &amp;#8211; eventually you need to try something else. If we&apos;re going to change anything now, I&apos;d suggest either making the entries in nodesEverSeen expire but at a longer interval than the metadata refresh or falling back to bootstrap nodes. I think the former is probably the better solution, though the latter may be easier for people to reason about until we work out a more complete solution. The former seems nicer because it decouples the expiration from other failures. Currently a broker failure that triggers a metadata refresh can force that broker out of the set of nodes to try immediately, which is why you can get into situations like the one linked above (and in much less time than the metadata refresh interval since these events force a metadata refresh).&lt;/p&gt;

&lt;p&gt;Ultimately, the problem boils down to the fact that we&apos;re not getting the information we want from these metadata updates &amp;#8211; we get the info we need about topic leaders, but we&apos;re also using this metadata for basic connectivity to the cluster. That breaks down if you&apos;re only working with one or a small number of topics and they end up with too few or crashed replicas. I think caching old entries is not the ideal solution here, but it&apos;s a way to work around the current situation. It would probably be better if metadata responses could include a random subset of the current set of brokers so that rather than relying on the original bootstrap servers we could get new, definitely valid bootstrap brokers with each metadata refresh. (It might even be possible to implement this just by having the broker return extra brokers in the metadata response even if there aren&apos;t entries in the topic metadata section of the response referencing those brokers; but I&apos;m not sure if that&apos;d break anything, either in our clients or in third party clients. It also doesn&apos;t fully fix the problem since the linked issue can still occur, but should only ever happen in the case of their tiny test case, not in practice.)&lt;/p&gt;</comment>
                            <comment id="15088209" author="hachikuji" created="Thu, 7 Jan 2016 22:06:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt; Is it not correct that the topic metadata response always includes all alive brokers?&lt;/p&gt;</comment>
                            <comment id="15088222" author="ewencp" created="Thu, 7 Jan 2016 22:11:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; Sorry, yeah, that&apos;s right. But you still have the same issues in any smaller cluster or in the case of a partition. So actually, even having the extra brokers returned doesn&apos;t work out great since as soon as you lose too many from the active set you can get into that same situation where you&apos;re not longer able to connect to any of the nodes from the last metadata refresh.&lt;/p&gt;</comment>
                            <comment id="15112574" author="enothereska" created="Fri, 22 Jan 2016 15:56:02 +0000"  >&lt;p&gt;We&apos;ll go with keeping the original bootstrap brokers.&lt;/p&gt;</comment>
                            <comment id="15112787" author="ewencp" created="Fri, 22 Jan 2016 17:55:14 +0000"  >&lt;p&gt;I&apos;m thinking more about this in combination with &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3112&quot; title=&quot;One incorrect bootstrap server will prevent Kafka producer from opening&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3112&quot;&gt;&lt;del&gt;KAFKA-3112&lt;/del&gt;&lt;/a&gt;. In 3112, you may get unresolvable nodes over time if your app is running for a long time and your bootstrap servers become stale.&lt;/p&gt;

&lt;p&gt;The more I think about this, most of these issues seem to ultimately be due to the fact that you have to specify a fixed list of bootstrap servers which should be valid indefinitely. Assuming you have the right setup, you can work around this by, e.g., a VIP/loadbalancer/round robin DNS. But I&apos;m wondering for how many people this requires extra work because they are already using some other service discovery mechanism? Today, if you want to pull bootstrap data from somewhere else, you need to do it at app startup and commit to using that fixed set of servers. I&apos;m hesitant to suggest adding more points for extension, but wouldn&apos;t this be addressed more generally if there was a hook to get a list of bootstrap servers and we invoked it at startup, any time we run out of options, or fail to connect for too long? The default can just be to use the bootstrap servers list, but if you want to grab your list of servers from ZK, DNS, Consul, or whatever your system of choice is, you could easily hook those in instead and avoid this entire problem.&lt;/p&gt;</comment>
                            <comment id="15112810" author="ijuma" created="Fri, 22 Jan 2016 18:15:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt;, that sounds like a worthwhile improvement, I agree. It&apos;s a bigger change and requires a KIP though so maybe we can file a separate JIRA for it? Jun expressed concerns that the problem in this JIRA could be a security issue and a fix for 0.9.0.1 would be desireable. As such, it would be great if we could find a simple fix for now and explore a more sophisticated one in the future.&lt;/p&gt;</comment>
                            <comment id="15112812" author="enothereska" created="Fri, 22 Jan 2016 18:15:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt;: sounds like a KIP? In the old Kafka we could have connected to ZK, but we don&apos;t have that option anymore. I wonder if ZK can still be used as a proxy for a directory service of sorts? We basically need a directory service to find other services in the cluster.&lt;/p&gt;</comment>
                            <comment id="15112863" author="ewencp" created="Fri, 22 Jan 2016 18:57:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enothereska&quot; class=&quot;user-hover&quot; rel=&quot;enothereska&quot;&gt;enothereska&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; Agreed, likely a KIP and separate JIRA. I was raising it here to get some feedback about whether it makes sense and see if anybody had any ideas for less intrusive solutions (i.e. do we really need that level of pluggability or could we get away with something less?). I agree that a larger change like this shouldn&apos;t block fixing this JIRA.&lt;/p&gt;</comment>
                            <comment id="15113009" author="githubbot" created="Fri, 22 Jan 2016 20:37:58 +0000"  >&lt;p&gt;GitHub user enothereska opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/804&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/804&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3068&quot; title=&quot;NetworkClient may connect to a different Kafka cluster than originally configured&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3068&quot;&gt;&lt;del&gt;KAFKA-3068&lt;/del&gt;&lt;/a&gt;: Keep track of bootstrap nodes instead of all nodes ever seen&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/enothereska/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/enothereska/kafka&lt;/a&gt; kafka-3068&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/804.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/804.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #804&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 32f3bffb2281a03fa6449627c144478a0ce666ad&lt;br/&gt;
Author: Eno Thereska &amp;lt;eno.thereska@gmail.com&amp;gt;&lt;br/&gt;
Date:   2016-01-22T20:36:27Z&lt;/p&gt;

&lt;p&gt;    Keep track of bootstrap nodes instead of all nodes ever seen&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15115702" author="junrao" created="Mon, 25 Jan 2016 18:23:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt;, having a pluggable discovery service may be reasonable. However, one will need some kind of entry points to connect to the external discovery service. Then one has to answer the same question: how to keep the entry points to external services valid all the time. If one has to rely upon things like VIP or DNS mapping there, we probably should just recommend people to use those on the bootstrap servers in the first place.&lt;/p&gt;

&lt;p&gt;Also, for the fix, my feeling is that falling back to bootstrap servers is a subtle change of behavior from 0.8.2 producer and probably should go through a KIP discussion too, since it can have implication on people who generate bootstrap servers from an external service.&lt;/p&gt;

&lt;p&gt;Alternatively, we can just revert the behavior in 0.9.0.1 to 0.8.2 (i.e., just wait when all brokers are not connectable). This may affect people who are using a small cluster and when there is a significant number of broker failures. However, this should be relatively rare. Then, we can start a separate KIP discussion in 0.9.1 to see what&apos;s the best way to fix this completely.&lt;/p&gt;</comment>
                            <comment id="15116465" author="ewencp" created="Tue, 26 Jan 2016 01:27:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; Right, for people using VIP or DNS, using it as the bootstrap works fine. If you&apos;re using other discovery mechanisms that don&apos;t expose info via DNS are stuck using a fixed setting by looking it up once and setting the bootstrap servers. (Note also that even DNS only works if you&apos;re using some fixed, default port. I faced this issue trying to work with Kafka on Mesos because I could discover the node the service was running on with mesos-dns, but it was using a randomly assigned port. At the time the only way I could figure to make this work without opting out of their port management was to use their REST API to discover host + port.)&lt;/p&gt;

&lt;p&gt;In any case, that is really a much larger KIP discussion, as I said I just wanted to gauge level of interest on it. On the solution proposed here, maybe go through the quick KIP and see if people agree it makes sense in 0.9.0.1 too &amp;#8211; it&apos;s true this is a behavioral change, but also for behavior that I&apos;m not sure we ever specified or made promises about. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enothereska&quot; class=&quot;user-hover&quot; rel=&quot;enothereska&quot;&gt;enothereska&lt;/a&gt; do you want to draft up a KIP? Should be a small one.&lt;/p&gt;</comment>
                            <comment id="15117071" author="enothereska" created="Tue, 26 Jan 2016 11:19:46 +0000"  >&lt;p&gt;Ok, I&apos;ll revert to using the 0.8.2 solution (by updating the PR) and draft a KIP for moving forward. Thanks.&lt;/p&gt;</comment>
                            <comment id="15121671" author="githubbot" created="Thu, 28 Jan 2016 15:19:02 +0000"  >&lt;p&gt;Github user enothereska closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/804&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/804&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15121772" author="githubbot" created="Thu, 28 Jan 2016 16:00:13 +0000"  >&lt;p&gt;GitHub user enothereska opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/823&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/823&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3068&quot; title=&quot;NetworkClient may connect to a different Kafka cluster than originally configured&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3068&quot;&gt;&lt;del&gt;KAFKA-3068&lt;/del&gt;&lt;/a&gt;: Remove retry with nodesEverSeen&lt;/p&gt;

&lt;p&gt;    @ewencp @ijuma if this looks good please merge when you can. Thanks.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/enothereska/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/enothereska/kafka&lt;/a&gt; kafka-3068-alt&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/823.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/823.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #823&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 820be2808bd7e399d1ecf1b26fb9388783cb3029&lt;br/&gt;
Author: Eno Thereska &amp;lt;eno.thereska@gmail.com&amp;gt;&lt;br/&gt;
Date:   2016-01-28T15:58:31Z&lt;/p&gt;

&lt;p&gt;    Remove retry with nodesEverSeen&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15128638" author="githubbot" created="Tue, 2 Feb 2016 17:36:54 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/823&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/823&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15128639" author="ewencp" created="Tue, 2 Feb 2016 17:36:56 +0000"  >&lt;p&gt;Issue resolved by pull request 823&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/823&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/823&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="13427110">KAFKA-13653</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13365593">KAFKA-12480</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13412822">KAFKA-13467</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13515277">KAFKA-14548</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 41 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2qv9b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>ewencp</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>