<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:32:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-9087] ReplicaAlterLogDirs stuck and restart fails with java.lang.IllegalStateException: Offset mismatch for the future replica</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-9087</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;I&apos;ve started multiple replica movements between log directories and some partitions were stuck. After the restart of the broker I&apos;ve got exception in server.log:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2019-06-11 17:58:46,304] ERROR [ReplicaAlterLogDirsThread-1]: Error due to (kafka.server.ReplicaAlterLogDirsThread)
 org.apache.kafka.common.KafkaException: Error processing data for partition metrics_timers-35 offset 4224887
 at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:342)
 at scala.Option.foreach(Option.scala:274)
 at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$6(AbstractFetcherThread.scala:300)
 at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$6$adapted(AbstractFetcherThread.scala:299)
 at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
 at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
 at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$5(AbstractFetcherThread.scala:299)
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
 at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
 at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:299)
 at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:132)
 at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:131)
 at scala.Option.foreach(Option.scala:274)
 at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
 at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
 Caused by: java.lang.IllegalStateException: Offset mismatch for the future replica metrics_timers-35: fetched offset = 4224887, log end offset = 0.
 at kafka.server.ReplicaAlterLogDirsThread.processPartitionData(ReplicaAlterLogDirsThread.scala:107)
 at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:311)
 ... 16 more
 [2019-06-11 17:58:46,305] INFO [ReplicaAlterLogDirsThread-1]: Stopped (kafka.server.ReplicaAlterLogDirsThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also, ReplicaAlterLogDirsThread has been stopped. Further restarts do not fix the problem. To fix it I&apos;ve stopped the broker and remove all the stuck future partitions.&lt;/p&gt;

&lt;p&gt;Detailed log below&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2019-06-11 12:09:52,833] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is 4224886 (kafka.log.Log)

[2019-06-11 12:21:34,979] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Loading producer state till offset 4224887 with message format version 2 (kafka.log.Log)
[2019-06-11 12:21:34,980] INFO [ProducerStateManager partition=metrics_timers-35] Loading producer state from snapshot file &apos;/storage2/kafka/data/metrics_timers-35/00000000000004224887.snapshot&apos; (kafka.log.ProducerStateManager)
[2019-06-11 12:21:34,980] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Completed load of log with 1 segments, log start offset 4120720 and log end offset 4224887 in 70 ms (kafka.log.Log)

[2019-06-11 12:21:45,307] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-11 12:21:45,307] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-11 12:21:45,307] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 4224887 (kafka.cluster.Replica)

[2019-06-11 12:21:47,090] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is 4224886 (kafka.log.Log)

[2019-06-11 12:30:04,757] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Retrying leaderEpoch request for partition metrics_timers-35 as the leader reported an error: UNKNOWN_LEADER_EPOCH (kafka.server.ReplicaFetcherThread)

[2019-06-11 12:30:06,157] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Retrying leaderEpoch request for partition metrics_timers-35 as the leader reported an error: UNKNOWN_LEADER_EPOCH (kafka.server.ReplicaFetcherThread)

[2019-06-11 12:30:07,238] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Retrying leaderEpoch request for partition metrics_timers-35 as the leader reported an error: UNKNOWN_LEADER_EPOCH (kafka.server.ReplicaFetcherThread)

[2019-06-11 12:30:08,251] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is 4224886 (kafka.log.Log)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;ve started replica movement at this moment.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2019-06-11 12:47:32,502] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-11 12:47:32,502] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2019-06-11 12:47:32,502] INFO Created log for partition metrics_timers-35 in /storage5/kafka/data with properties {compression.type -&amp;gt; producer, message.format.version -&amp;gt; 2.2-IV1, file.delete.delay.ms -&amp;gt; 60000, max.message.bytes -&amp;gt; 1000012, min.compaction.lag.ms -&amp;gt; 0, message.timestamp.type -&amp;gt; CreateTime, message.downconversion.enable -&amp;gt; true, min.insync.replicas -&amp;gt; 2, segment.jitter.ms -&amp;gt; 0, preallocate -&amp;gt; false, min.cleanable.dirty.ratio -&amp;gt; 0.5, index.interval.bytes -&amp;gt; 4096, unclean.leader.election.enable -&amp;gt; false, retention.bytes -&amp;gt; 137438953472, delete.retention.ms -&amp;gt; 86400000, cleanup.policy -&amp;gt; [delete], flush.ms -&amp;gt; 9223372036854775807, segment.ms -&amp;gt; 604800000, segment.bytes -&amp;gt; 1073741824, retention.ms -&amp;gt; 259200000, message.timestamp.difference.max.ms -&amp;gt; 9223372036854775807, segment.index.bytes -&amp;gt; 10485760, flush.messages -&amp;gt; 9223372036854775807}. (kafka.log.LogManager)
[2019-06-11 12:47:32,502] INFO [Partition metrics_timers-35 broker=1] No checkpointed highwatermark is found for partition metrics_timers-35 (kafka.cluster.Partition)
[2019-06-11 12:47:32,502] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)

[2019-06-11 12:47:33,083] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher to broker BrokerEndPoint(id=1, host=localhost:-1) for partitions Map(metrics_timers-35 -&amp;gt; (offset=0, leaderEpoch=27)) (kafka.server.ReplicaAlterLogDirsManager)

[2019-06-11 12:47:33,309] INFO [ReplicaAlterLogDirsThread-1]: Truncating partition metrics_timers-35 to local high watermark 0 (kafka.server.ReplicaAlterLogDirsThread)
[2019-06-11 12:47:33,309] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)

[2019-06-11 14:02:25,937] INFO [ReplicaAlterLogDirsThread-1]: Partition metrics_timers-35 has an older epoch (27) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaAlterLogDirsThread)

[2019-06-11 14:02:25,952] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(metrics_timer-35, &#8230;

[2019-06-11 14:02:25,980] INFO [ReplicaFetcherManager on broker 1] Added fetcher to broker BrokerEndPoint(id=2, host=vostok09:9092) for partitions Map(metrics_timers-35 -&amp;gt; (offset=4224887, leaderEpoch=28),&#8230;

[2019-06-11 14:02:25,998] INFO [ReplicaAlterLogDirsThread-1]: Shutting down (kafka.server.ReplicaAlterLogDirsThread)
[2019-06-11 14:02:25,998] INFO [ReplicaAlterLogDirsThread-1]: Stopped (kafka.server.ReplicaAlterLogDirsThread)
[2019-06-11 14:02:25,998] INFO [ReplicaAlterLogDirsThread-1]: Shutdown completed (kafka.server.ReplicaAlterLogDirsThread)

[2019-06-11 14:02:26,803] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Retrying leaderEpoch request for partition metrics_timers-35 as the leader reported an error: UNKNOWN_LEADER_EPOCH (kafka.server.ReplicaFetcherThread)

[2019-06-11 14:02:43,406] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is 4224886 (kafka.log.Log)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The broker has been restarted at 17:35&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2019-06-11 17:35:32,176] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(metrics_timers-35) (kafka.server.ReplicaFetcherManager)

[2019-06-11 17:37:48,265] WARN [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Found a corrupted index file corresponding to log file /storage2/kafka/data/metrics_timers-35/00000000000004120720.log due to Corrupt time index found, time index file (/storage2/kafka/data/metrics_timers-35/00000000000004120720.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1560154787249}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-11 17:37:48,265] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Loading producer state till offset 4120720 with message format version 2 (kafka.log.Log)
[2019-06-11 17:37:48,266] INFO [ProducerStateManager partition=metrics_timers-35] Writing producer snapshot at offset 4120720 (kafka.log.ProducerStateManager)
[2019-06-11 17:37:48,522] INFO [ProducerStateManager partition=metrics_timers-35] Writing producer snapshot at offset 4224887 (kafka.log.ProducerStateManager)
[2019-06-11 17:37:48,524] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Loading producer state till offset 4224887 with message format version 2 (kafka.log.Log)
[2019-06-11 17:37:48,525] INFO [ProducerStateManager partition=metrics_timers-35] Loading producer state from snapshot file &apos;/storage2/kafka/data/metrics_timers-35/00000000000004224887.snapshot&apos; (kafka.log.ProducerStateManager)
[2019-06-11 17:37:48,525] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Completed load of log with 1 segments, log start offset 4120720 and log end offset 4224887 in 298 ms (kafka.log.Log)

[2019-06-11 17:38:01,954] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-11 17:38:01,954] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-11 17:38:01,955] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 4224887 (kafka.cluster.Replica)

[2019-06-11 17:38:02,582] INFO [Partition metrics_timers-35 broker=1] No checkpointed highwatermark is found for partition metrics_timers-35 (kafka.cluster.Partition)
[2019-06-11 17:38:02,582] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)

[2019-06-11 17:38:02,588] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher to broker BrokerEndPoint(id=1, host=localhost:-1) for partitions Map(metrics_timers-35 -&amp;gt; (offset=4224887, leaderEpoch=29), traces_cloud-4 -&amp;gt; (offset=4381208630, leaderEpoch=27), metrics_histograms-11 -&amp;gt; (offset=0, leaderEpoch=28), metrics_histograms-25 -&amp;gt; (offset=0, leaderEpoch=34), metrics_histograms-39 -&amp;gt; (offset=0, leaderEpoch=29), metrics_counters-15 -&amp;gt; (offset=0, leaderEpoch=34), metrics_final-21 -&amp;gt; (offset=1852, leaderEpoch=28), metrics_final-7 -&amp;gt; (offset=1926, leaderEpoch=29), metrics_any-17 -&amp;gt; (offset=0, leaderEpoch=28), metrics_timers-14 -&amp;gt; (offset=0, leaderEpoch=29), metrics_counters-1 -&amp;gt; (offset=0, leaderEpoch=28)) (kafka.server.ReplicaAlterLogDirsManager)

[2019-06-11 17:38:02,596] INFO [ReplicaAlterLogDirsThread-1]: Truncating partition metrics_timers-35 to local high watermark 4224887 (kafka.server.ReplicaAlterLogDirsThread)

[2019-06-11 17:38:02,596] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is -1 (kafka.log.Log)

[2019-06-11 17:38:06,005] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is 4224886 (kafka.log.Log)

[2019-06-11 17:38:06,080] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is -1 (kafka.log.Log)

[2019-06-11 17:38:06,080] INFO [ReplicaAlterLogDirsThread-1]: Truncating partition metrics_timers-35 to local high watermark 4224887 (kafka.server.ReplicaAlterLogDirsThread)
[2019-06-11 17:38:06,080] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is -1 (kafka.log.Log)

[2019-06-11 17:58:46,304] ERROR [ReplicaAlterLogDirsThread-1]: Error due to (kafka.server.ReplicaAlterLogDirsThread)
org.apache.kafka.common.KafkaException: Error processing data for partition metrics_timers-35 offset 4224887
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:342)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$6(AbstractFetcherThread.scala:300)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$6$adapted(AbstractFetcherThread.scala:299)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$5(AbstractFetcherThread.scala:299)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:299)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:132)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:131)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
Caused by: java.lang.IllegalStateException: Offset mismatch for the future replica metrics_timers-35: fetched offset = 4224887, log end offset = 0.
        at kafka.server.ReplicaAlterLogDirsThread.processPartitionData(ReplicaAlterLogDirsThread.scala:107)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:311)
        ... 16 more
[2019-06-11 17:58:46,305] INFO [ReplicaAlterLogDirsThread-1]: Stopped (kafka.server.ReplicaAlterLogDirsThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The broker has been restarted at 18:21&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2019-06-11 18:21:26,422] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)

[2019-06-11 18:21:26,423] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)

[2019-06-11 18:23:21,300] WARN [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Found a corrupted index file corresponding to log file /storage2/kafka/data/metrics_timers-35/00000000000004120720.log due to Corrupt time index found, time index file (/storage2/kafka/data/metrics_timers-35/00000000000004120720.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1560154787249}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-11 18:23:21,300] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Loading producer state till offset 4120720 with message format version 2 (kafka.log.Log)
[2019-06-11 18:23:21,301] INFO [ProducerStateManager partition=metrics_timers-35] Writing producer snapshot at offset 4120720 (kafka.log.ProducerStateManager)
[2019-06-11 18:23:21,559] INFO [ProducerStateManager partition=metrics_timers-35] Writing producer snapshot at offset 4224887 (kafka.log.ProducerStateManager)
[2019-06-11 18:23:21,561] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Loading producer state till offset 4224887 with message format version 2 (kafka.log.Log)
[2019-06-11 18:23:21,562] INFO [ProducerStateManager partition=metrics_timers-35] Loading producer state from snapshot file &apos;/storage2/kafka/data/metrics_timers-35/00000000000004224887.snapshot&apos; (kafka.log.ProducerStateManager)
[2019-06-11 18:23:21,563] INFO [Log partition=metrics_timers-35, dir=/storage2/kafka/data] Completed load of log with 1 segments, log start offset 4120720 and log end offset 4224887 in 353 ms (kafka.log.Log)

[2019-06-11 18:23:35,928] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-11 18:23:35,928] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-11 18:23:35,929] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 4224887 (kafka.cluster.Replica)

[2019-06-11 18:23:36,516] INFO [Partition metrics_timers-35 broker=1] No checkpointed highwatermark is found for partition metrics_timers-35 (kafka.cluster.Partition)

[2019-06-11 18:23:36,516] INFO Replica loaded for partition metrics_timers-35 with initial high watermark 0 (kafka.cluster.Replica)

[2019-06-11 18:23:36,521] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher to broker BrokerEndPoint(id=1, host=localhost:-1) for partitions Map(metrics_timers-35 -&amp;gt; (offset=4224887, leaderEpoch=30), metrics_histograms-11 -&amp;gt; (offset=0, leaderEpoch=29), metrics_histograms-25 -&amp;gt; (offset=0, leaderEpoch=36), metrics_histograms-39 -&amp;gt; (offset=0, leaderEpoch=30), metrics_counters-15 -&amp;gt; (offset=0, leaderEpoch=36), metrics_final-21 -&amp;gt; (offset=1861, leaderEpoch=29), metrics_final-7 -&amp;gt; (offset=1931, leaderEpoch=30), metrics_any-17 -&amp;gt; (offset=0, leaderEpoch=29), metrics_timers-14 -&amp;gt; (offset=0, leaderEpoch=30), metrics_counters-1 -&amp;gt; (offset=0, leaderEpoch=29)) (kafka.server.ReplicaAlterLogDirsManager)

[2019-06-11 18:23:36,522] INFO [ReplicaAlterLogDirsThread-1]: Truncating partition metrics_timers-35 to local high watermark 4224887 (kafka.server.ReplicaAlterLogDirsThread)
[2019-06-11 18:23:36,523] INFO [Log partition=metrics_timers-35, dir=/storage5/kafka/data] Truncating to 4224887 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-06-11 18:23:36,523] INFO [ReplicaAlterLogDirsThread-1]: Truncating partition metrics_final-7 to local high watermark 1931 (kafka.server.ReplicaAlterLogDirsThread)

[2019-06-11 18:23:36,563] ERROR [ReplicaAlterLogDirsThread-1]: Error due to (kafka.server.ReplicaAlterLogDirsThread)
org.apache.kafka.common.KafkaException: Error processing data for partition metrics_timers-35 offset 4224887
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:342)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$6(AbstractFetcherThread.scala:300)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$6$adapted(AbstractFetcherThread.scala:299)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$5(AbstractFetcherThread.scala:299)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:299)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:132)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:131)
        at scala.Option.foreach(Option.scala:274)
        at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
Caused by: java.lang.IllegalStateException: Offset mismatch for the future replica metrics_timers-35: fetched offset = 4224887, log end offset = 0.
        at kafka.server.ReplicaAlterLogDirsThread.processPartitionData(ReplicaAlterLogDirsThread.scala:107)
        at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:311)
        ... 16 more
[2019-06-11 18:23:36,564] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-11 18:23:36,572] INFO [ReplicaAlterLogDirsThread-1]: Stopped (kafka.server.ReplicaAlterLogDirsThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13264017">KAFKA-9087</key>
            <summary>ReplicaAlterLogDirs stuck and restart fails with java.lang.IllegalStateException: Offset mismatch for the future replica</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="chia7712">Chia-Ping Tsai</assignee>
                                    <reporter username="kgn">Gregory Koshelev</reporter>
                        <labels>
                    </labels>
                <created>Wed, 23 Oct 2019 13:12:07 +0000</created>
                <updated>Sat, 7 Jan 2023 07:24:52 +0000</updated>
                            <resolved>Sat, 7 Jan 2023 07:24:52 +0000</resolved>
                                    <version>2.2.0</version>
                                    <fixVersion>3.5.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="16983768" author="tvkreddy2" created="Wed, 27 Nov 2019 17:48:26 +0000"  >&lt;p&gt;Is the above issue resolved? Am also facing same problem.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-11-27 17:07:24,201&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Partition XXXX-1 broker=3&amp;#93;&lt;/span&gt; XXX-1 starts at Leader Epoch 103 from offset 10681297828. Previous Leader Epoch was: 102 (kafka.cluster.Partition)&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2019-11-27 17:07:24,207&amp;#93;&lt;/span&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaAlterLogDirsThread-1&amp;#93;&lt;/span&gt;: Partition XXXX-1 has an older epoch (102) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaAlterLogDirsThread)&lt;/p&gt;</comment>
                            <comment id="17072960" author="matt.diff" created="Wed, 1 Apr 2020 16:44:38 +0000"  >&lt;p&gt;Has there been any movement on this? Also having the same issue.&#160; Would also welcome any workarounds; restarting the kafka daemon repeatedly to push a migration along is the only way that&apos;s working so far, but seems like a very risky and time-consuming method.&lt;/p&gt;</comment>
                            <comment id="17077566" author="matt.diff" created="Tue, 7 Apr 2020 20:53:06 +0000"  >&lt;p&gt;After many attempts over 2 weeks and lack of input here, I conclude that this method of partition migration is fundamentally broken.&#160; Here is what actually fixed my issue:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Create an additional replica across partitions in the desired destination log directory with kafka-reassign-partitions.sh.&#160; Wait until replication completes.&lt;/li&gt;
	&lt;li&gt;For each broker that has stuck migrations:
	&lt;ol&gt;
		&lt;li&gt;Shut down the broker.&lt;/li&gt;
		&lt;li&gt;Check that all partitions have an active leader (and that the shut down broker is not one of them).&lt;/li&gt;
		&lt;li&gt;Delete both source and future (destination) topic partition directories (e.g. /data0/kafka/topic-0 and /data1/kafka/topic-0-aa308443b6ee4ec58a28d61abdde90ea-future)&lt;/li&gt;
		&lt;li&gt;Start the broker.&lt;/li&gt;
		&lt;li&gt;Ensure that the broker has created the destination topic partition directory only.&lt;/li&gt;
		&lt;li&gt;Kafka will backfill this replica from the leader until both are in sync.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Use kafka-reassign-partitions.sh again to remove the replicas created in step 1.&#160; End state should be the replica reassignment state given in the original migration.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In the future, I would stay away from migrating at all and just adding new replicas where they are needed before removing the unwanted ones.&lt;/p&gt;</comment>
                            <comment id="17249973" author="junrao" created="Tue, 15 Dec 2020 22:00:18 +0000"  >&lt;p&gt;Not sure if this is exactly the reason, but we did fix an issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-9654&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-9654&lt;/a&gt;) related to ReplicaAlterLogDirs.  Could you try and see if this issue still exists in some of the newer versions?&lt;/p&gt;</comment>
                            <comment id="17575835" author="JIRAUSER294009" created="Fri, 5 Aug 2022 13:11:26 +0000"  >&lt;p&gt;I was able to reproduce the issue with Kafka 3.2 deployed via strimzi 0.30 in Kubernetes:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Create a second volume of type volume&lt;/li&gt;
	&lt;li&gt;Run `kafka-reassign-partitions.sh` moving partition from one volume to the other inside the replicas&lt;/li&gt;
	&lt;li&gt;The migration was stuck, so I deleted the broker&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This caused the following error&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2022-08-05 15:13:14 ERROR [ReplicaAlterLogDirsThread-1]: Unexpected error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing data &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 at offset 59173844 (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1] 
2022-08-05 15:13:14 java.lang.IllegalStateException: Offset mismatch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt; replica my-topic-0: fetched offset = 59173844, log end offset = 0.
2022-08-05 15:13:14 WARN [ReplicaAlterLogDirsThread-1]: Partition my-topic-0 marked as failed (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I managed to work around the issue by rolling back the assignment and trying it again. After multiple repetitions and going down do a single partition at a time, it worked (&#9583;&#176;&#9633;&#176;)&#9583;&#65077; &#9531;&#9473;&#9531;&lt;/p&gt;

&lt;p&gt;Here are the logs.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,245 TRACE [Broker id=2] Handling LeaderAndIsr request correlationId 86 from controller 1 epoch 30 starting the become-follower transition &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 with leader 0 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,246 INFO [Broker id=2] Follower my-topic-0 starts at leader epoch 103 from offset 59173844 with high watermark 59173844. Previous leader epoch was 102. (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,246 INFO [ReplicaFetcherManager on broker 2] Removed fetcher &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions HashSet(my-topic-0) (kafka.server.ReplicaFetcherManager) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,247 INFO [ReplicaFetcherManager on broker 2] Added fetcher to broker 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions Map(my-topic-0 -&amp;gt; InitialFetchState(Some(CPOi5CRMSxWoihVANzwwjQ),BrokerEndPoint(id=0, host=strimzi-kafka-0.strimzi-kafka-brokers.dev-kafka.svc:9091),103,59173844)) (kafka.server.ReplicaFetcherManager) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,247 TRACE [Broker id=2] Completed LeaderAndIsr request correlationId 86 from controller 1 epoch 30 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the become-follower transition &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 with leader 0 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,247 INFO The cleaning &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 is aborted and paused (kafka.log.LogManager) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,250 INFO [ReplicaAlterLogDirsManager on broker 2] Added log dir fetcher &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions with initial offsets HashMap(my-topic-0 -&amp;gt; InitialFetchState(Some(CPOi5CRMSxWoihVANzwwjQ),BrokerEndPoint(id=2, host=localhost:-1),103,59173844)) (kafka.server.ReplicaAlterLogDirsManager) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,250 INFO [ReplicaAlterLogDirsThread-1]: Truncating partition my-topic-0 with TruncationState(offset=59173844, completed=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) due to local high watermark 59173844 (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,250 INFO [UnifiedLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Truncating to 59173844 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,250 INFO [ReplicaAlterLogDirsThread-1]: Beginning/resuming copy of partition my-topic-0 from offset 59173844. Including &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; partition, there are 1 remaining partitions to copy by &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; thread. (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,250 ERROR [ReplicaAlterLogDirsThread-1]: Unexpected error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing data &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 at offset 59173844 (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
java.lang.IllegalStateException: Offset mismatch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt; replica my-topic-0: fetched offset = 59173844, log end offset = 0.
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,251 WARN [ReplicaAlterLogDirsThread-1]: Partition my-topic-0 marked as failed (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,251 TRACE [Broker id=2] Cached leader info UpdateMetadataPartitionState(topicName=&lt;span class=&quot;code-quote&quot;&gt;&apos;my-topic&apos;&lt;/span&gt;, partitionIndex=0, controllerEpoch=30, leader=0, leaderEpoch=103, isr=[1, 0, 2], zkVersion=190, replicas=[1, 2, 0], offlineReplicas=[]) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 in response to UpdateMetadata request sent by controller 1 epoch 30 with correlation id 87 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:14 &#160; &#160;
2022-08-05 13:13:14,840 TRACE [Broker id=2] Cached leader info UpdateMetadataPartitionState(topicName=&lt;span class=&quot;code-quote&quot;&gt;&apos;my-topic&apos;&lt;/span&gt;, partitionIndex=0, controllerEpoch=30, leader=0, leaderEpoch=103, isr=[1, 0, 2], zkVersion=190, replicas=[1, 2, 0], offlineReplicas=[]) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 in response to UpdateMetadata request sent by controller 1 epoch 30 with correlation id 88 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:15 &#160; &#160;
2022-08-05 13:13:14,906 INFO [ReplicaAlterLogDirsManager on broker 2] Removed fetcher &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions Set(my-topic-0) (kafka.server.ReplicaAlterLogDirsManager) [data-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:15 &#160; &#160;
2022-08-05 13:13:15,031 INFO Log &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 is renamed to /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.fdf28328606944d8b6c37ca113561f79-delete and is scheduled &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; deletion (kafka.log.LogManager) [data-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:13:15 &#160; &#160;
2022-08-05 13:13:15,031 INFO [Partition my-topic-0 broker=2] Current log directory /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-1/kafka-log2 is same as requested log dir /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-1/kafka-log2. Skipping &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt; replica creation. (kafka.cluster.Partition) [data-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:14:15 &#160; &#160;
2022-08-05 13:14:15,032 INFO [LocalLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Deleting segments as the log has been deleted: LogSegment(baseOffset=0, size=0, lastModifiedTime=1659704344817, largestRecordTimestamp=None) (kafka.log.LocalLog) [kafka-scheduler-6]
&#160; &#160;&#160;
2022-08-05 15:14:15 &#160; &#160;
2022-08-05 13:14:15,033 INFO [LocalLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Deleting segment files LogSegment(baseOffset=0, size=0, lastModifiedTime=1659704344817, largestRecordTimestamp=None) (kafka.log.LocalLog$) [kafka-scheduler-6]
&#160; &#160;&#160;
2022-08-05 15:14:15 &#160; &#160;
2022-08-05 13:14:15,033 INFO Deleted log /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.fdf28328606944d8b6c37ca113561f79-delete/00000000000000000000.log.deleted. (kafka.log.LogSegment) [kafka-scheduler-6]
&#160; &#160;&#160;
2022-08-05 15:14:15 &#160; &#160;
2022-08-05 13:14:15,033 INFO Deleted offset index /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.fdf28328606944d8b6c37ca113561f79-delete/00000000000000000000.index.deleted. (kafka.log.LogSegment) [kafka-scheduler-6]
&#160; &#160;&#160;
2022-08-05 15:14:15 &#160; &#160;
2022-08-05 13:14:15,033 INFO Deleted time index /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.fdf28328606944d8b6c37ca113561f79-delete/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment) [kafka-scheduler-6]
&#160; &#160;&#160;
2022-08-05 15:14:15 &#160; &#160;
2022-08-05 13:14:15,034 INFO Deleted log &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 in /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.fdf28328606944d8b6c37ca113561f79-delete. (kafka.log.LogManager) [kafka-scheduler-6]

2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,708 TRACE [Broker id=2] Handling LeaderAndIsr request correlationId 104 from controller 1 epoch 30 starting the become-follower transition &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 with leader 0 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,708 INFO [Broker id=2] Follower my-topic-0 starts at leader epoch 104 from offset 59173860 with high watermark 59173860. Previous leader epoch was 103. (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,709 INFO [ReplicaFetcherManager on broker 2] Removed fetcher &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions HashSet(my-topic-0) (kafka.server.ReplicaFetcherManager) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,709 INFO [ReplicaFetcherManager on broker 2] Added fetcher to broker 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions Map(my-topic-0 -&amp;gt; InitialFetchState(Some(CPOi5CRMSxWoihVANzwwjQ),BrokerEndPoint(id=0, host=strimzi-kafka-0.strimzi-kafka-brokers.dev-kafka.svc:9091),104,59173860)) (kafka.server.ReplicaFetcherManager) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,709 TRACE [Broker id=2] Completed LeaderAndIsr request correlationId 104 from controller 1 epoch 30 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the become-follower transition &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 with leader 0 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,710 TRACE [Broker id=2] Cached leader info UpdateMetadataPartitionState(topicName=&lt;span class=&quot;code-quote&quot;&gt;&apos;my-topic&apos;&lt;/span&gt;, partitionIndex=0, controllerEpoch=30, leader=0, leaderEpoch=104, isr=[1, 0, 2], zkVersion=191, replicas=[1, 2, 0], offlineReplicas=[]) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 in response to UpdateMetadata request sent by controller 1 epoch 30 with correlation id 105 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,710 TRACE [Broker id=2] Cached leader info UpdateMetadataPartitionState(topicName=&lt;span class=&quot;code-quote&quot;&gt;&apos;my-topic&apos;&lt;/span&gt;, partitionIndex=0, controllerEpoch=30, leader=0, leaderEpoch=104, isr=[1, 0, 2], zkVersion=191, replicas=[1, 2, 0], offlineReplicas=[]) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 in response to UpdateMetadata request sent by controller 1 epoch 30 with correlation id 106 (state.change.logger) [control-plane-kafka-request-handler-0]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,738 INFO [LogLoader partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$) [data-plane-kafka-request-handler-3]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,739 INFO Created log &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 in /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.bfada6953c214c3e80ab704dd4a6e1c7-&lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt; with properties {cleanup.policy=delete, retention.ms=7776000000} (kafka.log.LogManager) [data-plane-kafka-request-handler-3]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,739 INFO [Partition my-topic-0 broker=2] No checkpointed highwatermark is found &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 (kafka.cluster.Partition) [data-plane-kafka-request-handler-3]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,739 INFO [Partition my-topic-0 broker=2] Log loaded &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 with initial high watermark 0 (kafka.cluster.Partition) [data-plane-kafka-request-handler-3]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,739 INFO The cleaning &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 is aborted and paused (kafka.log.LogManager) [data-plane-kafka-request-handler-3]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,740 INFO [ReplicaAlterLogDirsManager on broker 2] Added log dir fetcher &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions with initial offsets Map(my-topic-0 -&amp;gt; InitialFetchState(Some(CPOi5CRMSxWoihVANzwwjQ),BrokerEndPoint(id=2, host=localhost:-1),104,0)) (kafka.server.ReplicaAlterLogDirsManager) [data-plane-kafka-request-handler-3]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,740 INFO [ReplicaAlterLogDirsThread-1]: Truncating partition my-topic-0 with TruncationState(offset=0, completed=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) due to local high watermark 0 (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,740 INFO [UnifiedLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,740 INFO [ReplicaAlterLogDirsThread-1]: Beginning/resuming copy of partition my-topic-0 from offset 0. Including &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; partition, there are 1 remaining partitions to copy by &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; thread. (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,741 WARN [ReplicaAlterLogDirsThread-1]: Reset fetch offset &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 from 0 to current leader&apos;s start offset 33537108 (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,743 INFO [LocalLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Deleting segments as part of log truncation: LogSegment(baseOffset=0, size=0, lastModifiedTime=1659706321735, largestRecordTimestamp=None) (kafka.log.LocalLog) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,992 INFO [UnifiedLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Loading producer state till offset 33537108 with message format version 2 (kafka.log.UnifiedLog$) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,992 INFO [UnifiedLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Reloading from producer snapshot and rebuilding producer state from offset 33537108 (kafka.log.UnifiedLog$) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:01 &#160; &#160;
2022-08-05 13:32:01,992 INFO [UnifiedLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Producer state recovery took 0ms &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; snapshot load and 0ms &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; segment recovery from offset 33537108 (kafka.log.UnifiedLog$) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:32:02 &#160; &#160;
2022-08-05 13:32:02,004 INFO [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Partition my-topic-0 has an &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; epoch (104) than the current leader. retry the partition later (kafka.server.ReplicaFetcherThread) [ReplicaFetcherThread-0-0]
&#160; &#160;&#160;
2022-08-05 15:32:02 &#160; &#160;
2022-08-05 13:32:02,655 INFO [ReplicaAlterLogDirsThread-1]: Current offset 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 is out of range, which typically implies a leader change. Reset fetch offset to 33537108 (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:33:01 &#160; &#160;
2022-08-05 13:33:01,743 INFO [LocalLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Deleting segment files LogSegment(baseOffset=0, size=0, lastModifiedTime=1659706321735, largestRecordTimestamp=None) (kafka.log.LocalLog$) [kafka-scheduler-0]
&#160; &#160;&#160;
2022-08-05 15:33:01 &#160; &#160;
2022-08-05 13:33:01,744 INFO Deleted log /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.bfada6953c214c3e80ab704dd4a6e1c7-&lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt;/00000000000000000000.log.deleted. (kafka.log.LogSegment) [kafka-scheduler-0]Show context
&#160; &#160;&#160;
2022-08-05 15:33:01 &#160; &#160;
2022-08-05 13:33:01,744 INFO Deleted offset index /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.bfada6953c214c3e80ab704dd4a6e1c7-&lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt;/00000000000000000000.index.deleted. (kafka.log.LogSegment) [kafka-scheduler-0]
&#160; &#160;&#160;
2022-08-05 15:33:01 &#160; &#160;
2022-08-05 13:33:01,744 INFO Deleted time index /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2/my-topic-0.bfada6953c214c3e80ab704dd4a6e1c7-&lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt;/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment) [kafka-scheduler-0]
&#160; &#160;&#160;
2022-08-05 15:33:13 &#160; &#160;
2022-08-05 13:33:13,342 INFO [LocalLog partition=my-topic-0, dir=/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/kafka/data-2/kafka-log2] Rolled &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log segment at offset 33880242 in 1 ms. (kafka.log.LocalLog) [ReplicaAlterLogDirsThread-1]
&#160; &#160;&#160;
2022-08-05 15:33:13 &#160; &#160;
2022-08-05 13:33:13,709 INFO [ProducerStateManager partition=my-topic-0] Wrote producer snapshot at offset 33880242 with 2 producer ids in 367 ms. (kafka.log.ProducerStateManager) [ReplicaAlterLogDirsThread-1]
 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;The key seems to be that it realizes there was a leader change.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2022-08-05 15:32:02 &#160; &#160;
2022-08-05 13:32:02,004 INFO [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Partition my-topic-0 has an &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; epoch (104) than the current leader. retry the partition later (kafka.server.ReplicaFetcherThread) [ReplicaFetcherThread-0-0]
&#160; &#160;&#160;
2022-08-05 15:32:02 &#160; &#160;
2022-08-05 13:32:02,655 INFO [ReplicaAlterLogDirsThread-1]: Current offset 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition my-topic-0 is out of range, which typically implies a leader change. Reset fetch offset to 33537108 (kafka.server.ReplicaAlterLogDirsThread) [ReplicaAlterLogDirsThread-1] &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In contrast, one of the failed tries gave&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160; &#160; 2022-08-05 14:49:31 &#160; &#160;
2022-08-05 12:49:31,078 INFO [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Partition my-topic-0 has an &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; epoch (100) than the current leader. retry the partition later (kafka.server.ReplicaFetcherThread) [ReplicaFetcherThread-0-0]
&#160; &#160;&#160;
2022-08-05 14:51:45 &#160; &#160;
2022-08-05 12:51:45,868 INFO [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Partition my-topic-0 has an older epoch (100) than the current leader. Will await the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread) [ReplicaFetcherThread-0-0]Show context
&#160; &#160;&#160;
2022-08-05 14:51:45 &#160; &#160;
2022-08-05 12:51:45,869 WARN [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Partition my-topic-0 marked as failed (kafka.server.ReplicaFetcherThread) [ReplicaFetcherThread-0-0]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and that is where it was stuck.&lt;/p&gt;</comment>
                            <comment id="17650979" author="chia7712" created="Wed, 21 Dec 2022 19:15:12 +0000"  >&lt;p&gt;We encountered this error also. The root cause is about race condition.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;ReplicaAlterLogDirsThread has fetched the data for topic partition&lt;/li&gt;
	&lt;li&gt;ReplicaManager#alterReplicaLogDirs changes the future log (the start offset is reset to 0)&lt;/li&gt;
	&lt;li&gt;ReplicaManager#alterReplicaLogDirs call AbstractFetcherManager#addFetcherForPartitions to add the topic partition (it just change the partition state in the ReplicaAlterLogDirsThread)&lt;/li&gt;
	&lt;li&gt;ReplicaAlterLogDirsThread starts to process the fetched data, and it throws IllegalStateException because the future log get renewed and start offset is zero.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This bug causes the future log of topic partition can&apos;t get synced forever as the topic partition is marked as failed. It seems to me that we should return None instead of throwing IllegalStateException when start offset of future log is zero. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;&#160;WDYT?&#160;&lt;/p&gt;</comment>
                            <comment id="17651012" author="junrao" created="Wed, 21 Dec 2022 22:08:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chia7712&quot; class=&quot;user-hover&quot; rel=&quot;chia7712&quot;&gt;chia7712&lt;/a&gt; : Thanks for the update. About the race condition. I am still wondering how we got into that state. Let&apos;s say ReplicaAlterLogDirsThread is in the about to append the fetched data to a future log when the log&apos;s dir is being changed. In this case, we will first remove the partition from the partitionState in ReplicaAlterLogDirsThread, recreate the future log and add the partition to ReplicaAlterLogDirsThread again. If ReplicaAlterLogDirsThread tries to append an old fetched data, it should fail the following test since the fetch offset in the fetch request and the currentFetchState will be different.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/AbstractFetcherThread.scala#L347&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/AbstractFetcherThread.scala#L347&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;So, ReplicaAlterLogDirsThread is supposed to ignore the old fetched data and fetch again using the new fetch offset. I am wondering why that didn&apos;t happen.&lt;/p&gt;</comment>
                            <comment id="17654133" author="chia7712" created="Tue, 3 Jan 2023 18:24:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;&#160;Sorry for late response.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;So, ReplicaAlterLogDirsThread is supposed to ignore the old fetched data and fetch again using the new fetch offset. I am wondering why that didn&apos;t happen.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You are right. The true root cause is shown below.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;tp-0 is located at broker-0:/tmp/data0&lt;/li&gt;
	&lt;li&gt;move tp-0 from /tmp/data0 to /tmp/data1. It will create a new future log (&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L765&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L765&lt;/a&gt;) and ReplicaAlterLogDirsThread. The new future log does not have leader epoch before it sync data&lt;/li&gt;
	&lt;li&gt;file a partition reassignment to trigger LeaderAndIsrRequest request. The request will update the partition state of ReplicaAlterLogDirsThread (&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1565&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1565&lt;/a&gt;), and the new offset of partition state is set with highWatermark of log&lt;/li&gt;
	&lt;li&gt;ReplicaAlterLogDirsThread uses the high watermark instead of OffsetsForLeaderEpoch API if there is no epoch cache.&lt;/li&gt;
	&lt;li&gt;The future log is new, so its end offset is 0. And the offset mismatch ( 0 v.s high watermark of log) causes the error.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In short, the race condition of processing LeaderAndIsrRequest and AlterReplicaLogDirsRequest causes this error (on V2 message format). Also, the error can be reproduced easily on V1 since there is no epoch cache. I&#8217;m not sure why it used log.highWatermark (&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1559&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1559&lt;/a&gt;). The ReplicaAlterLogDirsThread checks the offset of &#8220;future log&#8221; rather than &#8220;log. Hence, here is my two cents, we can replace log.highWatermark by futureLog.highWatermark to resolve this issue. I tested it on our cluster and it works well (on both V1 and V2).&lt;/p&gt;</comment>
                            <comment id="17655153" author="junrao" created="Thu, 5 Jan 2023 22:41:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chia7712&quot; class=&quot;user-hover&quot; rel=&quot;chia7712&quot;&gt;chia7712&lt;/a&gt; : Thanks for the explanation. Great find! I agree that this is a bug and the fix that you suggested makes sense. In alterReplicaLogDirs(), we initialize the initial offset for ReplicaAlterLogDirsThread with futureLog.highWatermark. We should do the same thing when handling the LeaderAndIsrRequest. It seems that the bug was introduced in this PR &lt;a href=&quot;https://github.com/apache/kafka/pull/6841.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6841&lt;/a&gt;. Do you plan to submit a PR?&lt;/p&gt;</comment>
                            <comment id="17655155" author="chia7712" created="Thu, 5 Jan 2023 22:45:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;&#160;Please take a look at &lt;a href=&quot;https://github.com/apache/kafka/pull/13075&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/13075&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 44 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z07vjc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>