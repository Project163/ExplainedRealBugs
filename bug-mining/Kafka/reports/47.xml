<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:03 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-305] SyncProducer does not correctly timeout</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-305</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;So it turns out that using the channel in SyncProducer like we are to perform blocking reads will not trigger socket timeouts (though we set it) and will block forever which is bad.  This bug identifies the issue: &lt;a href=&quot;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802&lt;/a&gt; and this article presents a potential work-around: &lt;a href=&quot;http://stackoverflow.com/questions/2866557/timeout-for-socketchannel&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://stackoverflow.com/questions/2866557/timeout-for-socketchannel&lt;/a&gt; for workaround. The work-around is a simple solution that involves creating a separate ReadableByteChannel instance for timeout-enabled reads.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12546581">KAFKA-305</key>
            <summary>SyncProducer does not correctly timeout</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="prashanth.menon">Prashanth Menon</assignee>
                                    <reporter username="prashanth.menon">Prashanth Menon</reporter>
                        <labels>
                    </labels>
                <created>Thu, 15 Mar 2012 13:11:14 +0000</created>
                <updated>Mon, 2 Apr 2012 01:48:37 +0000</updated>
                            <resolved>Mon, 26 Mar 2012 17:19:39 +0000</resolved>
                                    <version>0.7</version>
                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                                                                <comments>
                            <comment id="13231399" author="nehanarkhede" created="Fri, 16 Mar 2012 17:16:06 +0000"  >&lt;p&gt;Produce ACK should not be blocking. It probably makes sense to fix this before closing &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-49&quot; title=&quot;Add acknowledgement to the produce request.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-49&quot;&gt;&lt;del&gt;KAFKA-49&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13232415" author="prashanth.menon" created="Mon, 19 Mar 2012 02:15:06 +0000"  >&lt;p&gt;I, unfortunately, didn&apos;t get a chance to work on this over the weekend.  From my point of view, creating a new ReadableByteChannel that wraps the socket channel InputStream seems like the simplest solution.  Then the SyncProudcer will have a writeChannel (the SocketChannel) and a readChannel (the wrapped version).  All writes and reads go through the respective channels with the additional of timeout functionality.  &lt;/p&gt;

&lt;p&gt;Another step we can take it is to move all that logic into some class BlockingChannel which can be reused on the consumer side in SimpleConsumer.  Such a class would have, perhaps, four methods: connect, disconnect, send and receive.  Connect and disconnect would be synchronized, send would take a Request object and receive would return a Tuple2&lt;span class=&quot;error&quot;&gt;&amp;#91;Receive, Int&amp;#93;&lt;/span&gt; like usual.  Send and receive will need to be synchronized externally, meaning the class can be effectively treated like a regular Channel otherwise ...&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="13233119" author="prashanth.menon" created="Tue, 20 Mar 2012 02:00:49 +0000"  >&lt;p&gt;Hi all, I&apos;ve attached a patch.  Some notes:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;New class called BlockingChannel that has timeouts enabled.&lt;/li&gt;
	&lt;li&gt;SyncProducer uses BlockingChannel instead of creating its own SocketChannel&lt;/li&gt;
	&lt;li&gt;Re-introducsed testZKSendWithDeadBroker which passes now.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;d like to get feedback on this.  It&apos;s simple and may be reused on the consumer side.  When I think about it, it would be nice to combine SimpleConsumer and SyncProducer into one generic &quot;SimpleClient&quot; since the functionality is effectively the same.&lt;/p&gt;

&lt;p&gt;I&apos;d also like to benchmark this against a pure NIO implementation where we can use selectors to enabled timeout functionality.  It&apos;ll be more complex and will require minor adjustment to BoundedByteBuffer and BoundedByteBufferSend but it may be worth it.&lt;/p&gt;</comment>
                            <comment id="13233969" author="junrao" created="Wed, 21 Mar 2012 00:12:16 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Thanks for the patch. This is very useful. Some comments:&lt;/p&gt;

&lt;p&gt;1. I think it makes sense for SimpleConsumer to use BlockingChannel as well. Could you change that in this patch too?&lt;br/&gt;
2. ProducerTest.testZKSendWithDeadBroker: This test doesn&apos;t really test the timeout on getting a response. We probably need to create a mock kafkaserver (that don&apos;t send a response) to test this out.&lt;br/&gt;
3. BlockingChannel: &lt;br/&gt;
3.1 We probably should rename timeoutMs to readTimeoutMs since only reads are subject to the timeout.&lt;br/&gt;
3.2 We should pass in a socketSendBufferSize and a socketReceiveBufferSize.&lt;br/&gt;
3.3 Should host and port be part of the constructor? It seems to me it&apos;s cleaner if each instance of BlockingChannel is tied to 1 host and 1 port.&lt;/p&gt;

&lt;p&gt;I&apos;d also be interested in your findings on the comparison with NIO with selectors.&lt;/p&gt;</comment>
                            <comment id="13235295" author="prashanth.menon" created="Thu, 22 Mar 2012 02:18:53 +0000"  >&lt;p&gt;Thanks for review, Jun.&lt;/p&gt;

&lt;p&gt;1. Will do.&lt;br/&gt;
2. So that test actually exposed the issue to begin with - the initial send would fail and then hang forever when attempting to refresh the topic metadata.  Regardless, I&apos;ll create a separate more direct test for timeouts.  On my local machine, this test seems to be unpredictable around 30% of the time.  In these cases, it seems like the ephemeral broker nodes aren&apos;t removed from ZK and bringing back up a broker after shutdown throws a &quot;Broker already exists&quot; exception.  Is anyone else experiencing it or just me?  Increasing the wait time after shutdown helps but not 100%.&lt;br/&gt;
3. 1,2,3 Sounds fair.&lt;/p&gt;

&lt;p&gt;I should be able to get a patch in for this by Friday.  Then continue on &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-49&quot; title=&quot;Add acknowledgement to the produce request.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-49&quot;&gt;&lt;del&gt;KAFKA-49&lt;/del&gt;&lt;/a&gt; over the weekend and get it in on Saturday or Sunday should the review go okay.  Apologies for the delays &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13235614" author="junrao" created="Thu, 22 Mar 2012 15:01:56 +0000"  >&lt;p&gt;Prashant,&lt;/p&gt;

&lt;p&gt;2. If you want to make sure that a broker is shut down, you need to call kafkaServer.awaitShutdown after calling kafkaServer.shutdown. Overall, I don&apos;t quite understand how the new test works. It only brought down 1 broker and yet the comment says all brokers are down. If it is indeed that all brokers are down, any RPC call to the broker should get a broken pipe or socket closed exception immediately, not a sockettimeout exception. So, to really test that the timeout works, we need to keep the broker alive and somehow delay the response from the server. This can probably be done with a mock request handler.&lt;/p&gt;</comment>
                            <comment id="13235643" author="nehanarkhede" created="Thu, 22 Mar 2012 15:32:11 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Thanks for the patch. A couple of suggestions -&lt;/p&gt;

&lt;p&gt;1. Since you are adding a new abstraction, BlockingChannel, would it make sense to change SimpleConsumer to use it ? Its your call if you&apos;d rather fix it in another JIRA.&lt;br/&gt;
2. In BlockingChannel, since you are synchronizing on a lock, any reason the connected boolean be a volatile ? Also, you can avoid resetting the read and write channels to null values in disconnect.&lt;br/&gt;
3. Lets add some more tests for this, since it is unclear if the workaround of wrapping input stream in a channel actually works or not. I like Jun&apos;s suggestion of mocking out the request handler to achieve this. Tests would include SyncProducer as well as async producer (DefaultEventHandler)&lt;/p&gt;</comment>
                            <comment id="13236291" author="prashanth.menon" created="Fri, 23 Mar 2012 02:54:59 +0000"  >&lt;p&gt;I&apos;ve uploaded a new patch with the suggestions, but it&apos;s not ready for commit, just another review.  A few notes:&lt;/p&gt;

&lt;p&gt;1. BlockingChannel modified to meet suggestions.&lt;br/&gt;
2. SimpleConsumer uses BlockingChannel.&lt;br/&gt;
3. To test the BlockingChannel (in SyncProducer and async producer), I bring up a regular server but shutdown the requesthandler.  So the socket remains open, accepts requests and queues them in the request channel, but there are no handlers processing requests.&lt;br/&gt;
4. The original testZKSendWithDeadBroker wasn&apos;t commented entirely correctly.  I&apos;ve modified to actually test what the name suggests.&lt;br/&gt;
5. Though I wait for the broker to do down, testZKSendWithDeadBroker still unpredictably throws the &quot;Broker already registered&quot; exception.  Are you experiencing this locally?&lt;/p&gt;

&lt;p&gt;I think there might be an issue with the BrokerPartitionInfo and ProduerPool classes.  ProducerPool never removes producers even if one is connected to a downed broker, so calls to getAnyProducer (used by BrokerPartitioninfo.updateInfo to update cached topic metadata information) could return the same &quot;bad&quot; producer on consecutive calls when attempting to refresh the cache.  This could potentially cause an entire send to fail though there may exist a broker that is able to service the topic metadata request.  We need to somehow, remove &quot;bad&quot; producers, or refresh the ProducerPool when brokers go down, or have BrokerPartitionInfo retry its updateInfo call a certain number of times.  Thoughts?&lt;/p&gt;</comment>
                            <comment id="13236782" author="junrao" created="Fri, 23 Mar 2012 16:57:21 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;v2 patch looks good. &lt;/p&gt;

&lt;p&gt;As for 5, I do see transient failures of testZKSendWithDeadBroker. This is a bit weird. During broker shutdown, we close the ZK client, which should cause all ephemeral nodes to be deleted in ZK. Could you verify if this is indeed the behavior of ZK?&lt;/p&gt;

&lt;p&gt;As for BrokerPartitionInfo and ProducerPool, we should clean up dead brokers. Could you open a separate jira to track that?&lt;/p&gt;</comment>
                            <comment id="13236942" author="nehanarkhede" created="Fri, 23 Mar 2012 18:58:17 +0000"  >&lt;p&gt;v2 looks good. &lt;/p&gt;

&lt;p&gt;Regarding the test failure, I debugged it and see a probable bug with either Zookeeper or ZkClient. See below - &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;info&amp;#93;&lt;/span&gt; Test Starting: testZKSendWithDeadBroker(kafka.producer.ProducerTest)&lt;br/&gt;
Shutting down broker 0&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-03-23 11:50:36,870&amp;#93;&lt;/span&gt; DEBUG Deleting ephemeral node /brokers/ids/0 for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-03-23 11:50:36,873&amp;#93;&lt;/span&gt; DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/3/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-03-23 11:50:36,873&amp;#93;&lt;/span&gt; DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/1/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-03-23 11:50:36,873&amp;#93;&lt;/span&gt; DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/2/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-03-23 11:50:36,873&amp;#93;&lt;/span&gt; DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/0/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)&lt;br/&gt;
Shut down broker 0&lt;br/&gt;
Restarting broker 0&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-03-23 11:50:45,194&amp;#93;&lt;/span&gt; DEBUG Deleting ephemeral node /brokers/ids/1 for session 0x13640e55f24001b (org.apache.zookeeper.server.DataTree:831)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; Test Failed: testZKSendWithDeadBroker(kafka.producer.ProducerTest)&lt;br/&gt;
java.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.&lt;br/&gt;
	at kafka.utils.ZkUtils$.registerBrokerInZk(ZkUtils.scala:109)&lt;br/&gt;
	at kafka.server.KafkaZooKeeper.kafka$server$KafkaZooKeeper$$registerBrokerInZk(KafkaZooKeeper.scala:60)&lt;br/&gt;
	at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:52)&lt;br/&gt;
	at kafka.server.KafkaServer.startup(KafkaServer.scala:84)&lt;br/&gt;
	at kafka.producer.ProducerTest.testZKSendWithDeadBroker(ProducerTest.scala:173)&lt;/p&gt;

&lt;p&gt;Notice that after shutting down broker 0, the ephemeral node was deleted from its in memory data tree. That happens part of the close session workflow. Still, when we try to create the ephemeral node again, it complains that it already exists. &lt;/p&gt;

&lt;p&gt;I&apos;ll come back to this zookeeper bug later. I&apos;d say lets checkin this test since it helps reproduce this zk bug. &lt;/p&gt;

&lt;p&gt;I think your patch looks good. &lt;/p&gt;</comment>
                            <comment id="13236976" author="prashanth.menon" created="Fri, 23 Mar 2012 19:32:34 +0000"  >&lt;p&gt;Thanks for the input everyone.  Regarding the ZK failure, that is effectively the trace I&apos;m seeing on my end as well - the log makes it clear that the ephemeral nodes get deleted but the test still fails when creating them afterwards.  &lt;/p&gt;

&lt;p&gt;I would like to delay commiting this patch, atleast for the weekend, as I&apos;d like to perform a little benchmark against a pure NIO implementation.  The benefits there would be having timeouts for both read and write operations and a potential performance boost.&lt;/p&gt;</comment>
                            <comment id="13237021" author="junrao" created="Fri, 23 Mar 2012 20:22:10 +0000"  >&lt;p&gt;If this is indeed a ZK issue, we can probably check/wait that the ephemeral node is gone before restarting the broker.&lt;/p&gt;</comment>
                            <comment id="13238045" author="prashanth.menon" created="Mon, 26 Mar 2012 01:45:28 +0000"  >&lt;p&gt;I&apos;ve attached another non-blocking implementation that uses selectors, but I&apos;m not seeing any significant performance boost on my machine.  I tested it on the producer side using the ProducerPerformance class by varying the number of messages, the message sizes and the number of threads.  Each test scenario was run four times and the average result was used.  Find the results here: &lt;a href=&quot;https://gist.github.com/2202142&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/2202142&lt;/a&gt;.  &lt;/p&gt;

&lt;p&gt;For what it&apos;s worth, I think we should go ahead with the simple solution attached in the v2 path - if everyone is okay with it, please commit.  Regarding the test error, it could potentially be a valid ZK or ZKClient bug.  I can investigate a little by digging into ZKClient and asking around the mailing list and channels.  Keeping the test in breaks the test unpredictably.  Thought I&apos;m not entirely okay with it keeping the bug in, waiting for the node to go down doesn&apos;t seem to be the right solution either.  &lt;/p&gt;</comment>
                            <comment id="13238570" author="junrao" created="Mon, 26 Mar 2012 17:19:39 +0000"  >&lt;p&gt;Prashanth,&lt;/p&gt;

&lt;p&gt;Thanks for the patch. I agree that v2 is less risky than the selector approach. So, we can revisit the selector approach later. Thanks for the patch though and it will probably be useful in the future. Committed v2 patch to 0.8 branch with the following minor changes in DefaultEventHandler:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;log all unsent messages&lt;/li&gt;
	&lt;li&gt;maintain outstandingRequests properly on both successful and unsuccessful sends.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Could you file 2 jiras, one for taking out dead brokers in ProducerPool and another for transient failures due to ZK ephemeral node not deleted in time?&lt;/p&gt;</comment>
                            <comment id="13238873" author="nehanarkhede" created="Mon, 26 Mar 2012 21:36:40 +0000"  >&lt;p&gt;I found the zookeeper related problem, filed &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-320&quot; title=&quot;testZKSendWithDeadBroker fails intermittently due to ZKNodeExistsException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-320&quot;&gt;&lt;del&gt;KAFKA-320&lt;/del&gt;&lt;/a&gt; and also included a patch.&lt;/p&gt;</comment>
                            <comment id="13239050" author="prashanth.menon" created="Tue, 27 Mar 2012 00:22:54 +0000"  >&lt;p&gt;Awesome!&lt;/p&gt;</comment>
                            <comment id="13243935" author="prashanth.menon" created="Mon, 2 Apr 2012 01:48:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-300&quot; title=&quot;Implement leader election&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-300&quot;&gt;&lt;del&gt;KAFKA-300&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-305&quot; title=&quot;SyncProducer does not correctly timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-305&quot;&gt;&lt;del&gt;KAFKA-305&lt;/del&gt;&lt;/a&gt; ticket together resolve &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-295&quot; title=&quot;Bug in async producer DefaultEventHandler retry logic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-295&quot;&gt;&lt;del&gt;KAFKA-295&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12545382">KAFKA-295</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12514686">KAFKA-49</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12519889" name="BlockingChannel2.scala" size="4676" author="prashanth.menon" created="Mon, 26 Mar 2012 01:45:52 +0000"/>
                            <attachment id="12519005" name="KAFKA-305-v1.patch" size="14715" author="prashanth.menon" created="Tue, 20 Mar 2012 02:04:07 +0000"/>
                            <attachment id="12519623" name="KAFKA-305-v2.patch" size="31511" author="prashanth.menon" created="Fri, 23 Mar 2012 13:57:54 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>231739</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 34 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i09m9b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>54039</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>