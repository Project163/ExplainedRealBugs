<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:11:38 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6264] Log cleaner thread may die on legacy segment containing messages whose offsets are too large</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6264</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We encountered a problem that some of the legacy log segments contains messages whose offsets are larger than &lt;tt&gt;SegmentBaseOffset + Int.MaxValue&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Prior to 0.10.2.0, we do not assert the offset of the messages when appending them to the log segments. Due to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;, the log cleaner may append messages whose offset is greater than &lt;tt&gt;base_offset + Int.MaxValue&lt;/tt&gt; into the segment during the log compaction.&lt;/p&gt;

&lt;p&gt;After the brokers are upgraded, those log segments cannot be compacted anymore because the compaction will fail immediately due to the offset range assertion we added to the LogSegment.&lt;/p&gt;

&lt;p&gt;We have seen this issue in the __consumer_offsets topic so it could be a general problem. There is no easy solution for the users to recover from this case. &lt;/p&gt;

&lt;p&gt;One solution is to split such log segments in the log cleaner once it sees a message with problematic offset and append those messages to a separate log segment with a larger base_offset.&lt;/p&gt;

&lt;p&gt;Due to the impact of the issue. We may want to consider backporting the fix to previous affected versions.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13120368">KAFKA-6264</key>
            <summary>Log cleaner thread may die on legacy segment containing messages whose offsets are too large</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dhruvilshah">Dhruvil Shah</assignee>
                                    <reporter username="becket_qin">Jiangjie Qin</reporter>
                        <labels>
                    </labels>
                <created>Thu, 23 Nov 2017 00:30:53 +0000</created>
                <updated>Mon, 23 Jul 2018 23:39:02 +0000</updated>
                            <resolved>Fri, 8 Jun 2018 15:40:24 +0000</resolved>
                                    <version>0.10.2.1</version>
                    <version>0.11.0.2</version>
                    <version>1.0.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="16267904" author="hachikuji" created="Tue, 28 Nov 2017 01:04:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; Good catch. For users who hit this, one possible way to recover is to shutdown the broker and rename the problematic segment to use the offset of the first message. That would only work if there is a gap at the start of the log which is larger than the overflow, but might be worth trying for users who hit this and cannot update easily (once a patch is available). So far, this is the first case we&apos;ve seen so hopefully it is rare.&lt;/p&gt;</comment>
                            <comment id="16269853" author="becket_qin" created="Wed, 29 Nov 2017 01:07:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; Yes, that may be a workaround. If the first message was the problem. One corner case is that if the segment is the first segment, i.e. 000000000000000.log, in addition to change the segment name to use the first offset of the message, we need to add an empty log segment at 000000000000000.log, otherwise the log starting offset will change.&lt;/p&gt;</comment>
                            <comment id="16335026" author="ewencp" created="Mon, 22 Jan 2018 22:15:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; This is marked as critical with fix version of 1.0.1 How critical is this? Since we&apos;ll want to get a 1.0.1 out in the next week or so, should we bump this to 1.0.2?&lt;/p&gt;</comment>
                            <comment id="16335046" author="becket_qin" created="Mon, 22 Jan 2018 22:31:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt; The impact of this issue is nasty but the chance of hitting the problem seems small&#160;enough. Perhaps we can include this into 1.1.0 and backport it to 1.0.2 if necessary.&lt;/p&gt;</comment>
                            <comment id="16348433" author="damianguy" created="Thu, 1 Feb 2018 11:19:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; &#160;is this a blocker for 1.1?&lt;/p&gt;</comment>
                            <comment id="16362105" author="corlettb" created="Tue, 13 Feb 2018 10:24:33 +0000"  >&lt;p&gt;In our production cluster we&#160;are running 0.10.2.1 and are seeing the log cleaning fail on 3 of the boxes with:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2018-02-07 14:40:23,820] INFO Cleaner 0: Cleaning segment 0 in log __consumer_offsets-17 (largest timestamp Wed Aug 09 07:28:11 BST 2017) into 0, discarding deletes. (kafka.log.LogCleaner)
[2018-02-07 14:40:23,830] ERROR [kafka-log-cleaner-thread-0]: Error due to (kafka.log.LogCleaner)
java.lang.IllegalArgumentException: requirement failed: largest offset in message set can not be safely converted to relative offset.
 at scala.Predef$.require(Predef.scala:277)
 at kafka.log.LogSegment.append(LogSegment.scala:121)
 at kafka.log.Cleaner.cleanInto(LogCleaner.scala:551)
 at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:444)
 at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:385)
 at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:384)
 at scala.collection.immutable.List.foreach(List.scala:389)
 at kafka.log.Cleaner.doClean(LogCleaner.scala:384)
 at kafka.log.Cleaner.clean(LogCleaner.scala:361)
 at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:256)
 at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:236)
 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
[2018-02-07 14:40:23,833] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I can see high fd counts on these servers:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ssh xxxxxxx &lt;span class=&quot;code-quote&quot;&gt;&apos;sudo ls -latr /proc/`pgrep java`/fd | wc -l&apos;&lt;/span&gt;
130149
ssh xxxxxxx &lt;span class=&quot;code-quote&quot;&gt;&apos;sudo ls -latr /proc/`pgrep java`/fd | wc -l&apos;&lt;/span&gt;
147455
ssh xxxxxxx &lt;span class=&quot;code-quote&quot;&gt;&apos;sudo ls -latr /proc/`pgrep java`/fd | wc -l&apos;&lt;/span&gt;
155521
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I&apos;ve tried several restarts. The log cleaner would fall over each time. I tried to upgrade one of the affected servers from 0.10.2.1 to 0.11.0.2. The log cleaner still failed.&lt;/p&gt;

&lt;p&gt;I&apos;m guessing I&apos;m going to have to hack the files on the filesystem. Looking at the affected partition:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-rw-r--r-- 1 kafka root 122372 Aug 10 2017 00000000000000000000.log
-rw-r--r-- 1 kafka root 2424 Aug 14 2017 00000000004019345048.log
-rw-r--r-- 1 kafka root 20956142 Aug 15 07:28 00000000004020192019.log
-rw-r--r-- 1 kafka root 20986067 Aug 16 07:28 00000000004020403517.log
-rw-r--r-- 1 kafka root 20984625 Aug 17 07:28 00000000004020615318.log
...

-rw-r--r-- 1 kafka kafka 184 Feb 7 14:39 00000000000000000000.index
-rw-r--r-- 1 kafka root 0 Feb 7 14:36 00000000004019345048.index
-rw-r--r-- 1 kafka root 40208 Feb 7 14:36 00000000004020192019.index
-rw-r--r-- 1 kafka root 40328 Feb 7 14:36 00000000004020403517.index
-rw-r--r-- 1 kafka root 40336 Feb 7 14:36 00000000004020615318.index
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I guess I&apos;m looking for some advice on how to fix this.&lt;/p&gt;

&lt;p&gt;Should I just remove the &quot;00000000000000000000&quot; files. Loosing Consumer offsets not updated since 10th of Aug 2017 shouldn&apos;t be an issue. Or should I try to empty these files? Try to figure out the starting offset of these files?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16390535" author="dhruvilshah" created="Thu, 8 Mar 2018 00:55:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;&#160;if you don&apos;t have a working fix yet, would you mind if I picked this issue up?&lt;/p&gt;</comment>
                            <comment id="16399698" author="wushujames" created="Thu, 15 Mar 2018 01:01:39 +0000"  >&lt;p&gt;We recently ran into this on one of our clusters.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2018-02-21 14:24:17,638] INFO Cleaner 0: Beginning cleaning of log __consumer_offsets-45. (kafka.log.LogCleaner) 
[2018-02-21 14:24:17,638] INFO Cleaner 0: Building offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; __consumer_offsets-45... (kafka.log.LogCleaner) 
[2018-02-21 14:24:17,796] INFO Cleaner 0: Building offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; log __consumer_offsets-45 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 20 segments in offset range [0, 2469849110). (kafka.log.LogCleaner) 
[2018-02-21 14:24:19,143] INFO Cleaner 0: Offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; log __consumer_offsets-45 complete. (kafka.log.LogCleaner) 
[2018-02-21 14:24:19,143] INFO Cleaner 0: Cleaning log __consumer_offsets-45 (cleaning prior to Wed Feb 14 07:56:02 UTC 2018, discarding tombstones prior to Thu Jan 01 00:00:00 UTC 1970)... (kafka.log.LogCleaner) 
[2018-02-21 14:24:19,144] INFO Cleaner 0: Cleaning segment 0 in log __consumer_offsets-45 (largest timestamp Tue Feb 28 02:56:04 UTC 2017) into 0, retaining deletes. (kafka.log.LogCleaner) 
[2018-02-21 14:24:19,155] ERROR [kafka-log-cleaner-thread-0]: Error due to (kafka.log.LogCleaner) 
java.lang.IllegalArgumentException: requirement failed: largest offset in message set can not be safely converted to relative offset. 
at scala.Predef$.require(Predef.scala:224) 
at kafka.log.LogSegment.append(LogSegment.scala:121) 
at kafka.log.Cleaner.cleanInto(LogCleaner.scala:547) 
at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:443) 
at kafka.log.Cleaner$$anonfun$doClean$4.apply(LogCleaner.scala:385) 
at kafka.log.Cleaner$$anonfun$doClean$4.apply(LogCleaner.scala:384) 
at scala.collection.immutable.List.foreach(List.scala:392) 
at kafka.log.Cleaner.doClean(LogCleaner.scala:384) 
at kafka.log.Cleaner.clean(LogCleaner.scala:361) 
at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:256) 
at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:236) 
at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64) 
[2018-02-21 14:24:19,169] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We used DumpLogSegments to look at the difference between the first and last offset for each of the .log files. I didn&apos;t find any where the difference was greater than MAXINT (2^31-1).&lt;/p&gt;

&lt;p&gt;However, when reading &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-6264&lt;/a&gt;, it mentioned that the filename is relevant and is part of this calculation. So I compared the filenames vs the last offset.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
sh-4.2# ls -l *.log 
-rw-r--r--. 1 root root 12195 Apr 27 2017 00000000000000000000.log 
-rw-r--r--. 1 root root 236030 Oct 10 23:07 00000000002469031807.log 
-rw-r--r--. 1 root root 1925969 Oct 18 18:46 00000000002469285574.log 
-rw-r--r--. 1 root root 78693 Oct 25 22:26 00000000002469290346.log 
-rw-r--r--. 1 root root 94544 Nov 1 17:29 00000000002469290862.log 
-rw-r--r--. 1 root root 270094 Nov 8 22:51 00000000002469291421.log 
-rw-r--r--. 1 root root 497500 Nov 16 06:24 00000000002469292884.log 
-rw-r--r--. 1 root root 5476783 Nov 23 21:19 00000000002469295854.log 
-rw-r--r--. 1 root root 18245427 Dec 1 08:45 00000000002469321787.log 
-rw-r--r--. 1 root root 7527104 Dec 8 07:38 00000000002469414611.log 
-rw-r--r--. 1 root root 6751417 Dec 15 09:36 00000000002469453997.log 
-rw-r--r--. 1 root root 5904919 Dec 22 10:01 00000000002469487104.log 
-rw-r--r--. 1 root root 293325 Dec 29 10:02 00000000002469528006.log 
-rw-r--r--. 1 root root 798008 Jan 5 23:02 00000000002469529772.log 
-rw-r--r--. 1 root root 9366028 Jan 13 10:02 00000000002469535454.log 
-rw-r--r--. 1 root root 7357635 Jan 20 10:01 00000000002469577027.log 
-rw-r--r--. 1 root root 1073890 Jan 27 10:05 00000000002469615817.log 
-rw-r--r--. 1 root root 6390581 Feb 3 13:15 00000000002469622670.log 
-rw-r--r--. 1 root root 44379090 Feb 11 04:01 00000000002469652193.log 
-rw-r--r--. 1 root root 970717 Feb 17 16:27 00000000002469845650.log 
-rw-r--r--. 1 root root 468160 Feb 24 10:01 00000000002469850446.log 
-rw-r--r--. 1 root root 104857353 Feb 28 16:55 00000000002469854492.log 
-rw-r--r--. 1 root root 104857445 Feb 28 17:22 00000000002470671339.log 
-rw-r--r--. 1 root root 104857445 Feb 28 18:23 00000000002471488410.log 
-rw-r--r--. 1 root root 104857445 Feb 28 19:49 00000000002472305481.log 
-rw-r--r--. 1 root root 104857445 Feb 28 21:05 00000000002473122552.log 
-rw-r--r--. 1 root root 104857445 Feb 28 23:13 00000000002473939623.log 
-rw-r--r--. 1 root root 104857445 Mar 1 02:05 00000000002474756694.log 
-rw-r--r--. 1 root root 104857445 Mar 1 02:47 00000000002475573765.log 
-rw-r--r--. 1 root root 104857445 Mar 1 03:19 00000000002476390836.log 
-rw-r--r--. 1 root root 104857438 Mar 1 05:33 00000000002477207907.log 
-rw-r--r--. 1 root root 104857572 Mar 1 06:24 00000000002478024876.log 
-rw-r--r--. 1 root root 104857445 Mar 1 07:26 00000000002478841838.log 
-rw-r--r--. 1 root root 41995747 Mar 1 13:19 00000000002479658909.log
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As you can see, the first log file is named 00000000000000000000.log. The last offset in that file is 2469031806, which is greater than 2^31-1.&lt;/p&gt;

&lt;p&gt;In our case, the&#160;actual range of offsets in&#160;00000000000000000000.log (from the first message in the file to the last message in the file) was less than 2^31-1. So Confluent provided the following steps to help us recover from this issue:&lt;/p&gt;

&lt;p&gt;1) Identify the broker which has the above stack trace for __consumer_offsets-45&lt;br/&gt;
2) shutdown that broker, making sure that some other follower has taken over leadership for it&lt;br/&gt;
3) rm -rf all the files in that partition&apos;s data directory&lt;br/&gt;
4) restart broker, wait for re-replication of that partition.&lt;/p&gt;

&lt;p&gt;Repeat as necessary on any other brokers and any other partitions that have this exact problem.&lt;/p&gt;

&lt;p&gt;After this, the log cleaner was able to properly run and clean those partitions.&lt;/p&gt;

&lt;p&gt;I believe the reason this worked was because when we re-replicated the data, the new log segments used the &lt;b&gt;actual&lt;/b&gt; range of offsets, and &lt;b&gt;not&lt;/b&gt; the original filenames. And since our actual offsets were less than 2^31-1, we were under the MAX_INT threshold and so the log cleaner was able to run properly.&lt;/p&gt;

&lt;p&gt; If, instead, we had re-replicated and there was actually a range between the offsets that was greater than 2^31-1, then I believe that the newly re-replicated partition would have run into this problem again and that the workaround would not work.&lt;/p&gt;</comment>
                            <comment id="16466651" author="githubbot" created="Tue, 8 May 2018 00:09:31 +0000"  >&lt;p&gt;dhruvilshah3 opened a new pull request #4975: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; title=&quot;Log cleaner thread may die on legacy segment containing messages whose offsets are too large&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6264&quot;&gt;&lt;del&gt;KAFKA-6264&lt;/del&gt;&lt;/a&gt;: Log cleaner thread may die on legacy segment containing messages whose offsets are too large&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4975&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4975&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Implementation for splitting log segments with offset overflow.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16467795" author="githubbot" created="Tue, 8 May 2018 18:22:44 +0000"  >&lt;p&gt;dhruvilshah3 closed pull request #4692: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; title=&quot;Log cleaner thread may die on legacy segment containing messages whose offsets are too large&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6264&quot;&gt;&lt;del&gt;KAFKA-6264&lt;/del&gt;&lt;/a&gt;: Log cleaner thread may die on legacy segment containing messages whose offsets are too large&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4692&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4692&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/AbstractIndex.scala b/core/src/main/scala/kafka/log/AbstractIndex.scala&lt;br/&gt;
index 44083c186c8..4377c6f32a6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/AbstractIndex.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/AbstractIndex.scala&lt;br/&gt;
@@ -49,7 +49,7 @@ abstract class AbstractIndex&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(@volatile var file: File, val baseOffset: Lon&lt;br/&gt;
   protected val lock = new ReentrantLock&lt;/p&gt;

&lt;p&gt;   @volatile&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected var mmap: MappedByteBuffer = {&lt;br/&gt;
+  protected&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; var mmap: MappedByteBuffer = {&lt;br/&gt;
     val newlyCreated = file.createNewFile()&lt;br/&gt;
     val raf = if (writable) new RandomAccessFile(file, &quot;rw&quot;) else new RandomAccessFile(file, &quot;r&quot;)&lt;br/&gt;
     try {&lt;br/&gt;
@@ -88,7 +88,7 @@ abstract class AbstractIndex&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(@volatile var file: File, val baseOffset: Lon&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /** The number of entries in this index */&lt;br/&gt;
   @volatile&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected var _entries = mmap.position() / entrySize&lt;br/&gt;
+  protected&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; var _entries = mmap.position() / entrySize&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;True iff there are no more slots available in this index&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index cc693375079..39871872a63 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -351,7 +351,10 @@ class Log(@volatile var dir: File,&lt;br/&gt;
           time = time,&lt;br/&gt;
           fileAlreadyExists = true)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;try segment.sanityCheck(timeIndexFileNewlyCreated)&lt;br/&gt;
+        try 
{
+          segment.sanityCheck(timeIndexFileNewlyCreated)
+          segment.initializeTimestampMetadata()
+        }
&lt;p&gt;         catch {&lt;br/&gt;
           case _: NoSuchFileException =&amp;gt;&lt;br/&gt;
             error(s&quot;Could not find offset index file corresponding to log file ${segment.log.file.getAbsolutePath}, &quot; +&lt;br/&gt;
@@ -1120,15 +1123,15 @@ class Log(@volatile var dir: File,&lt;br/&gt;
           s&quot;for partition $topicPartition is ${config.messageFormatVersion} which is earlier than the minimum &quot; +&lt;br/&gt;
           s&quot;required version $KAFKA_0_10_0_IV0&quot;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Cache to avoid race conditions. `toBuffer` is faster than most alternatives and provides&lt;/li&gt;
	&lt;li&gt;// constant time access while being safe to use with concurrent collections unlike `toArray`.&lt;/li&gt;
	&lt;li&gt;val segmentsCopy = logSegments.toBuffer&lt;br/&gt;
       // For the earliest and latest, we do not need to return the timestamp.&lt;br/&gt;
       if (targetTimestamp == ListOffsetRequest.EARLIEST_TIMESTAMP)&lt;br/&gt;
         return Some(TimestampOffset(RecordBatch.NO_TIMESTAMP, logStartOffset))&lt;br/&gt;
       else if (targetTimestamp == ListOffsetRequest.LATEST_TIMESTAMP)&lt;br/&gt;
         return Some(TimestampOffset(RecordBatch.NO_TIMESTAMP, logEndOffset))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+      // Cache to avoid race conditions. `toBuffer` is faster than most alternatives and provides&lt;br/&gt;
+      // constant time access while being safe to use with concurrent collections unlike `toArray`.&lt;br/&gt;
+      val segmentsCopy = logSegments.toBuffer&lt;br/&gt;
       val targetSeg = {&lt;br/&gt;
         // Get all the segments whose largest timestamp is smaller than target timestamp&lt;br/&gt;
         val earlierSegs = segmentsCopy.takeWhile(_.largestTimestamp &amp;lt; targetTimestamp)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogCleaner.scala b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
index 0ee994252a1..c552d82710f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
@@ -502,7 +502,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
     try {&lt;br/&gt;
       // clean segments into the new destination segment&lt;br/&gt;
       val iter = segments.iterator&lt;br/&gt;
+      val numSegments = segments.length&lt;br/&gt;
       var currentSegmentOpt: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt; = Some(iter.next())&lt;br/&gt;
+&lt;br/&gt;
       while (currentSegmentOpt.isDefined) {&lt;br/&gt;
         val currentSegment = currentSegmentOpt.get&lt;br/&gt;
         val nextSegmentOpt = if (iter.hasNext) Some(iter.next()) else None&lt;br/&gt;
@@ -516,7 +518,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
         info(s&quot;Cleaning segment $startOffset in log ${log.name} (largest timestamp ${new Date(currentSegment.largestTimestamp)}) &quot; +&lt;br/&gt;
           s&quot;into ${cleaned.baseOffset}, ${if(retainDeletes) &quot;retaining&quot; else &quot;discarding&quot;} deletes.&quot;)&lt;br/&gt;
         cleanInto(log.topicPartition, currentSegment.log, cleaned, map, retainDeletes, log.config.maxMessageSize,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;transactionMetadata, log.activeProducersWithLastSequence, stats)&lt;br/&gt;
+          transactionMetadata, log.activeProducersWithLastSequence, stats, numSegments)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         currentSegmentOpt = nextSegmentOpt&lt;br/&gt;
       }&lt;br/&gt;
@@ -554,6 +556,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param retainDeletes Should delete tombstones be retained while cleaning this segment&lt;/li&gt;
	&lt;li&gt;@param maxLogMessageSize The maximum message size of the corresponding topic&lt;/li&gt;
	&lt;li&gt;@param stats Collector for cleaning statistics&lt;br/&gt;
+   * @param numSegmentsInGroup Number of segments in source group, being cleaned into &apos;dest&apos;&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def cleanInto(topicPartition: TopicPartition,&lt;br/&gt;
                              sourceRecords: FileRecords,&lt;br/&gt;
@@ -563,7 +566,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
                              maxLogMessageSize: Int,&lt;br/&gt;
                              transactionMetadata: CleanedTransactionMetadata,&lt;br/&gt;
                              activeProducers: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, Int&amp;#93;&lt;/span&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stats: CleanerStats) {&lt;br/&gt;
+                             stats: CleanerStats,&lt;br/&gt;
+                             numSegmentsInGroup: Int) {&lt;br/&gt;
     val logCleanerFilter = new RecordFilter 
{
       var discardBatchRecords: Boolean = _
 
@@ -592,6 +596,7 @@ private[log] class Cleaner(val id: Int,
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     var position = 0&lt;br/&gt;
+    var allowOversizeIndexOffset = false&lt;br/&gt;
     while (position &amp;lt; sourceRecords.sizeInBytes) {&lt;br/&gt;
       checkDone(topicPartition)&lt;br/&gt;
       // read a chunk of messages and copy any that are to be retained to the write buffer to be written out&lt;br/&gt;
@@ -612,12 +617,29 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
       if (outputBuffer.position() &amp;gt; 0) {&lt;br/&gt;
         outputBuffer.flip()&lt;br/&gt;
         val retained = MemoryRecords.readableRecords(outputBuffer)&lt;br/&gt;
+        val baseOffsetOfLog = dest.baseOffset&lt;br/&gt;
+        val largestOffsetToAppend = result.maxOffset&lt;br/&gt;
+&lt;br/&gt;
+        if ((largestOffsetToAppend - baseOffsetOfLog) &amp;gt; Integer.MAX_VALUE) &lt;/p&gt;
{
+          // Typically, a segment can only include messages whose offsets can be represented as an integer offset relative
+          // to the baseOffset. #groupSegmentsBySize must already make sure that we do not cross this threshold on a
+          // segment-boundary. So if the largest offset cannot be represented as its base-relative form, we must only
+          // have exactly one segment in such a group.
+          // Note that having a segment with messages that cannot be expressed as an integer in base-relative form is not
+          // a typical scenario, and is only true for segments created before the patch for KAFKA-5413.
+          require(numSegmentsInGroup == 1, s&quot;Constructed segment group causes index offset overflow for $topicPartition&quot;)
+          allowOversizeIndexOffset = true
+
+          debug(s&quot;Offset overflow during log cleaning topic-partition: $topicPartition largestOffset: $largestOffsetToAppend&quot;)
+        }
&lt;p&gt;+&lt;br/&gt;
         // it&apos;s OK not to hold the Log&apos;s lock in this case, because this segment is only accessed by other threads&lt;br/&gt;
         // after `Log.replaceSegments` (which acquires the lock) is called&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;dest.append(largestOffset = result.maxOffset,&lt;br/&gt;
+        dest.append(largestOffset = largestOffsetToAppend,&lt;br/&gt;
           largestTimestamp = result.maxTimestamp,&lt;br/&gt;
           shallowOffsetOfMaxTimestamp = result.shallowOffsetOfMaxTimestamp,&lt;/li&gt;
	&lt;li&gt;records = retained)&lt;br/&gt;
+          records = retained,&lt;br/&gt;
+          allowOversizeIndexOffset = allowOversizeIndexOffset)&lt;br/&gt;
         throttler.maybeThrottle(outputBuffer.limit())&lt;br/&gt;
       }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/LogSegment.scala b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
index 5130b28b597..5b850063a05 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
@@ -96,6 +96,19 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
   @volatile private var maxTimestampSoFar: Long = timeIndex.lastEntry.timestamp&lt;br/&gt;
   @volatile private var offsetOfMaxTimestamp: Long = timeIndex.lastEntry.offset&lt;/p&gt;

&lt;p&gt;+  /**&lt;br/&gt;
+    * Initialize timestamp metadata accurately for a log segment that already exists. This method must be called when an&lt;br/&gt;
+    * existing log segment is opened.&lt;br/&gt;
+    * For some cases (like &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; title=&quot;Log cleaner thread may die on legacy segment containing messages whose offsets are too large&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6264&quot;&gt;&lt;del&gt;KAFKA-6264&lt;/del&gt;&lt;/a&gt;), it might not be enough to lookup the last entry in time index because we could&lt;br/&gt;
+    * have messages that were appended after time index was last updated. So lookup the last entry and scan messages&lt;br/&gt;
+    * that appear after that to make sure we have the correct maximum timestamp and its corresponding offset.&lt;br/&gt;
+    * @throws CorruptIndexException&lt;br/&gt;
+    * @throws NoSuchFileException&lt;br/&gt;
+    */&lt;br/&gt;
+  def initializeTimestampMetadata(): Unit = &lt;/p&gt;
{
+    loadLargestTimestamp()
+  }
&lt;p&gt;+&lt;br/&gt;
   /* Return the size in bytes of this log segment */&lt;br/&gt;
   def size: Int = log.sizeInBytes()&lt;/p&gt;

&lt;p&gt;@@ -107,8 +120,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Append the given messages starting with the given offset. Add&lt;/li&gt;
	&lt;li&gt;* an entry to the index if needed.&lt;br/&gt;
+   * Append the given messages ending at the given last offset. Add an entry to the index if needed.&lt;br/&gt;
    *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;It is assumed this method is being called from within a lock.&lt;br/&gt;
    *&lt;br/&gt;
@@ -116,33 +128,69 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/li&gt;
	&lt;li&gt;@param largestTimestamp The largest timestamp in the message set.&lt;/li&gt;
	&lt;li&gt;@param shallowOffsetOfMaxTimestamp The offset of the message that has the largest timestamp in the messages to append.&lt;/li&gt;
	&lt;li&gt;@param records The log entries to append.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @return the physical position in the file of the appended records&lt;br/&gt;
    */&lt;br/&gt;
   @nonthreadsafe&lt;br/&gt;
   def append(largestOffset: Long,&lt;br/&gt;
              largestTimestamp: Long,&lt;br/&gt;
              shallowOffsetOfMaxTimestamp: Long,&lt;br/&gt;
              records: MemoryRecords): Unit = 
{
+    append(largestOffset, largestTimestamp, shallowOffsetOfMaxTimestamp, records, allowOversizeIndexOffset = false)
+  }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * NOTE: For most cases, you might want to call append(Long, Long, Long, MemoryRecords) instead of invoking this method&lt;br/&gt;
+   * directly. This option is directly exposed only for the LogCleaner.&lt;br/&gt;
+   *&lt;br/&gt;
+   * Append the given messages ending at the given last offset and allow for the possibility that the offset might&lt;br/&gt;
+   * overflow the index. If it does and overflow is allowed, append to the log, but skip the index append. Note that&lt;br/&gt;
+   * this only affects log segments that were created before the patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;. For such a segment, all oversize&lt;br/&gt;
+   * offsets will be at the end of the log segment, so it should be fine to skip the index entries.&lt;br/&gt;
+   */&lt;br/&gt;
+  @nonthreadsafe&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def append(largestOffset: Long,&lt;br/&gt;
+                          largestTimestamp: Long,&lt;br/&gt;
+                          shallowOffsetOfMaxTimestamp: Long,&lt;br/&gt;
+                          records: MemoryRecords,&lt;br/&gt;
+                          allowOversizeIndexOffset: Boolean): Unit = {&lt;br/&gt;
     if (records.sizeInBytes &amp;gt; 0) {&lt;br/&gt;
       trace(s&quot;Inserting ${records.sizeInBytes} bytes at end offset $largestOffset at position ${log.sizeInBytes} &quot; +&lt;br/&gt;
             s&quot;with largest timestamp $largestTimestamp at shallow offset $shallowOffsetOfMaxTimestamp&quot;)&lt;br/&gt;
       val physicalPosition = log.sizeInBytes()&lt;br/&gt;
+      var canAppendToIndex = true&lt;br/&gt;
+&lt;br/&gt;
       if (physicalPosition == 0)&lt;br/&gt;
         rollingBasedTimestamp = Some(largestTimestamp)&lt;br/&gt;
+&lt;br/&gt;
+      // Must be able to convert the largest offset to base-relative offset.&lt;br/&gt;
+      // Because of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;, a log segment could contain messages larger than firstOffset + Integer.MAX_VALUE. If&lt;br/&gt;
+      // that is the case, silently ignore the fact that we cannot convert all message offsets and skip the&lt;br/&gt;
+      // append to the index.&lt;br/&gt;
+      if (!canConvertToRelativeOffset(largestOffset)) &lt;/p&gt;
{
+        if (allowOversizeIndexOffset)
+          canAppendToIndex = false
+        else
+          throw new IllegalArgumentException(s&quot;Largest offset in message set $largestOffset cannot be safely &quot; +
+            s&quot;converted to relative offset for segment with base offset $baseOffset.&quot;)
+      }
&lt;p&gt;+&lt;br/&gt;
       // append the messages&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;require(canConvertToRelativeOffset(largestOffset), &quot;largest offset in message set can not be safely converted to relative offset.&quot;)&lt;br/&gt;
       val appendedBytes = log.append(records)&lt;/li&gt;
	&lt;li&gt;trace(s&quot;Appended $appendedBytes to ${log.file()} at end offset $largestOffset&quot;)&lt;br/&gt;
+      trace(s&quot;Appended $appendedBytes to ${log.file} at end offset $largestOffset&quot;)&lt;br/&gt;
       // Update the in memory max timestamp and corresponding offset.&lt;br/&gt;
       if (largestTimestamp &amp;gt; maxTimestampSoFar) 
{
         maxTimestampSoFar = largestTimestamp
         offsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp
       }
&lt;p&gt;+&lt;br/&gt;
       // append an entry to the index (if needed)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if(bytesSinceLastIndexEntry &amp;gt; indexIntervalBytes) {&lt;/li&gt;
	&lt;li&gt;offsetIndex.append(largestOffset, physicalPosition)&lt;/li&gt;
	&lt;li&gt;timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)&lt;/li&gt;
	&lt;li&gt;bytesSinceLastIndexEntry = 0&lt;br/&gt;
+      if (bytesSinceLastIndexEntry &amp;gt; indexIntervalBytes) {&lt;br/&gt;
+        if (canAppendToIndex) 
{
+          offsetIndex.append(largestOffset, physicalPosition)
+          timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipIndexFullCheck = false)
+          bytesSinceLastIndexEntry = 0
+        }
&lt;p&gt; else {&lt;br/&gt;
+          debug(s&quot;Skipping append to offset index to prevent offset overflow (${log.file})&quot;)&lt;br/&gt;
+        }&lt;br/&gt;
       }&lt;br/&gt;
       bytesSinceLastIndexEntry += records.sizeInBytes&lt;br/&gt;
     }&lt;br/&gt;
@@ -282,10 +330,20 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // Build offset index&lt;br/&gt;
         if (validBytes - lastIndexEntry &amp;gt; indexIntervalBytes) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val startOffset = batch.baseOffset&lt;/li&gt;
	&lt;li&gt;offsetIndex.append(startOffset, validBytes)&lt;/li&gt;
	&lt;li&gt;timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)&lt;/li&gt;
	&lt;li&gt;lastIndexEntry = validBytes&lt;br/&gt;
+          val lastOffsetInBatch = batch.lastOffset&lt;br/&gt;
+&lt;br/&gt;
+          /* Could have index overflow for log segments created before the patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;. For such a segment,&lt;br/&gt;
+           * all oversize offsets will be at the end of the log segment, so it should be fine to skip the index entries.&lt;br/&gt;
+           * Only need to check if we are able to convert the largest offset in this batch.&lt;br/&gt;
+           */&lt;br/&gt;
+          if (canConvertToRelativeOffset(lastOffsetInBatch)) 
{
+            offsetIndex.append(lastOffsetInBatch, validBytes)
+            timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipIndexFullCheck = false)
+            lastIndexEntry = validBytes
+          }
&lt;p&gt; else {&lt;br/&gt;
+            debug(s&quot;Ignored index offset overflow when recovering lastOffset: $lastOffsetInBatch baseOffset: $baseOffset &quot; +&lt;br/&gt;
+                  s&quot;(${log.file.getAbsolutePath})&quot;)&lt;br/&gt;
+          }&lt;br/&gt;
         }&lt;br/&gt;
         validBytes += batch.sizeInBytes()&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -304,12 +362,14 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
     }&lt;br/&gt;
     val truncated = log.sizeInBytes - validBytes&lt;br/&gt;
     if (truncated &amp;gt; 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;debug(s&quot;Truncated $truncated invalid bytes at the end of segment ${log.file.getAbsoluteFile} during recovery&quot;)&lt;br/&gt;
+      debug(s&quot;Truncating $truncated invalid bytes at the end of segment ${log.file.getAbsoluteFile} during recovery&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.truncateTo(validBytes)&lt;br/&gt;
     offsetIndex.trimToValidSize()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// A normally closed segment always appends the biggest timestamp ever seen into log segment, we do this as well.&lt;/li&gt;
	&lt;li&gt;timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true)&lt;br/&gt;
+    // A normally closed segment always appends the biggest timestamp ever seen into log segment, we do this as well. This&lt;br/&gt;
+    // segment, if created before the patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;, could have messages that overflow the index so skip the entry&lt;br/&gt;
+    // in such cases.&lt;br/&gt;
+    timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipIndexFullCheck = true, mayHaveIndexOverflow = true)&lt;br/&gt;
     timeIndex.trimToValidSize()&lt;br/&gt;
     truncated&lt;br/&gt;
   }&lt;br/&gt;
@@ -422,7 +482,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;The time index entry appended will be used to decide when to delete the segment.&lt;br/&gt;
    */&lt;br/&gt;
   def onBecomeInactiveSegment() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true)&lt;br/&gt;
+    timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipIndexFullCheck = true, mayHaveIndexOverflow = true)&lt;br/&gt;
     offsetIndex.trimToValidSize()&lt;br/&gt;
     timeIndex.trimToValidSize()&lt;br/&gt;
     log.trim()&lt;br/&gt;
@@ -486,7 +546,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Close this log segment&lt;br/&gt;
    */&lt;br/&gt;
   def close() 
{
-    CoreUtils.swallow(timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true), this)
+    CoreUtils.swallow(timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipIndexFullCheck = true, mayHaveIndexOverflow = true), this)
     CoreUtils.swallow(offsetIndex.close(), this)
     CoreUtils.swallow(timeIndex.close(), this)
     CoreUtils.swallow(log.close(), this)
diff --git a/core/src/main/scala/kafka/log/OffsetIndex.scala b/core/src/main/scala/kafka/log/OffsetIndex.scala
index 523c88c7723..e315707453a 100755
--- a/core/src/main/scala/kafka/log/OffsetIndex.scala
+++ b/core/src/main/scala/kafka/log/OffsetIndex.scala
@@ -132,6 +132,21 @@ class OffsetIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writabl
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+    * Convert given offset to base-relative form. This function throws InvalidOffsetException if the conversion results&lt;br/&gt;
+    * in overflow or underflow.&lt;br/&gt;
+    * @param offset Offset to convert&lt;br/&gt;
+    * @return Base-relative offset&lt;br/&gt;
+    * @throws InvalidOffsetException&lt;br/&gt;
+    */&lt;br/&gt;
+  def relativeOffset(offset: Long): Int = {&lt;br/&gt;
+    val relativeOffset = (offset - baseOffset)&lt;br/&gt;
+    if (relativeOffset &amp;gt; Integer.MAX_VALUE || relativeOffset &amp;lt; 0)&lt;br/&gt;
+      throw new InvalidOffsetException(&lt;br/&gt;
+        s&quot;Attempt to append offset $offset to time index with base offset $baseOffset will cause overflow (${file.getAbsolutePath})&quot;)&lt;br/&gt;
+    relativeOffset.toInt&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Append an entry for the given offset/location pair to the index. This entry must have a larger offset than all subsequent entries.&lt;br/&gt;
    */&lt;br/&gt;
@@ -139,8 +154,9 @@ class OffsetIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writabl&lt;br/&gt;
     inLock(lock) {&lt;br/&gt;
       require(!isFull, &quot;Attempt to append to a full index (size = &quot; + _entries + &quot;).&quot;)&lt;br/&gt;
       if (_entries == 0 || offset &amp;gt; _lastOffset) 
{
+        val relOffset = relativeOffset(offset)
         debug(&quot;Adding index entry %d =&amp;gt; %d to %s.&quot;.format(offset, position, file.getName))
-        mmap.putInt((offset - baseOffset).toInt)
+        mmap.putInt(relOffset)
         mmap.putInt(position)
         _entries += 1
         _lastOffset = offset
diff --git a/core/src/main/scala/kafka/log/TimeIndex.scala b/core/src/main/scala/kafka/log/TimeIndex.scala
index e505f36aec3..5932256f1e4 100644
--- a/core/src/main/scala/kafka/log/TimeIndex.scala
+++ b/core/src/main/scala/kafka/log/TimeIndex.scala
@@ -96,6 +96,21 @@ class TimeIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writable:
     TimestampOffset(timestamp(buffer, n), baseOffset + relativeOffset(buffer, n))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+    * Convert given offset to base-relative form. This function throws InvalidOffsetException if the conversion results&lt;br/&gt;
+    * in overflow or underflow.&lt;br/&gt;
+    * @param offset Offset to convert to base-relative form.&lt;br/&gt;
+    * @return Base-relative offset.&lt;br/&gt;
+    * @throws InvalidOffsetException&lt;br/&gt;
+    */&lt;br/&gt;
+  def relativeOffset(offset: Long): Int = {&lt;br/&gt;
+    val relativeOffset = (offset - baseOffset)&lt;br/&gt;
+    if (relativeOffset &amp;gt; Integer.MAX_VALUE || relativeOffset &amp;lt; 0)&lt;br/&gt;
+      throw new InvalidOffsetException(&lt;br/&gt;
+        s&quot;Attempt to append offset $offset to time index with base offset $baseOffset will cause overflow (${file.getAbsolutePath})&quot;)&lt;br/&gt;
+    relativeOffset.toInt&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Attempt to append a time index entry to the time index.&lt;/li&gt;
	&lt;li&gt;The new entry is appended only if both the timestamp and offsets are greater than the last appended timestamp and&lt;br/&gt;
@@ -103,13 +118,30 @@ class TimeIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writable:&lt;br/&gt;
    *&lt;/li&gt;
	&lt;li&gt;@param timestamp The timestamp of the new time index entry&lt;/li&gt;
	&lt;li&gt;@param offset The offset of the new time index entry&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @param skipFullCheck To skip checking whether the segment is full or not. We only skip the check when the segment&lt;br/&gt;
+   * @param skipIndexFullCheck To skip checking whether the segment is full or not. We only skip the check when the segment&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;gets rolled or the segment is closed.&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def maybeAppend(timestamp: Long, offset: Long, skipFullCheck: Boolean = false) {&lt;br/&gt;
+  def maybeAppend(timestamp: Long, offset: Long, skipIndexFullCheck: Boolean): Unit = 
{
+    maybeAppend(timestamp, offset, skipIndexFullCheck, mayHaveIndexOverflow = false)
+  }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+    * For most cases, you might want to call append(Long, Int) instead of calling this method directly. This option is&lt;br/&gt;
+    * only exposed for segments created before the patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
+    *&lt;br/&gt;
+    * Append an entry for the given offset/location pair to the index, allowing for the possibility that the offset might&lt;br/&gt;
+    * overflow the index. If it does and overflow is allowed, this function silently returns without appending to the&lt;br/&gt;
+    * index. Note that this `mayHaveIndexOverflow` must only be used for log segments that were created before the patch&lt;br/&gt;
+    * for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;. For such segments, it should be fine to skip index entries corresponding to messages that cause an&lt;br/&gt;
+    * index overflow.&lt;br/&gt;
+    *&lt;br/&gt;
+    * This option is directly exposed only for log recovery and log close.&lt;br/&gt;
+    */&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def maybeAppend(timestamp: Long, offset: Long, skipIndexFullCheck: Boolean, mayHaveIndexOverflow: Boolean) {&lt;br/&gt;
     inLock(lock) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (!skipFullCheck)&lt;br/&gt;
+      if (!skipIndexFullCheck)&lt;br/&gt;
         require(!isFull, &quot;Attempt to append to a full time index (size = &quot; + _entries + &quot;).&quot;)&lt;br/&gt;
+&lt;br/&gt;
       // We do not throw exception when the offset equals to the offset of last entry. That means we are trying&lt;br/&gt;
       // to insert the same time index entry as the last entry.&lt;br/&gt;
       // If the timestamp index entry to be inserted is the same as the last entry, we simply ignore the insertion&lt;br/&gt;
@@ -122,13 +154,23 @@ class TimeIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writable:&lt;br/&gt;
       if (_entries != 0 &amp;amp;&amp;amp; timestamp &amp;lt; lastEntry.timestamp)&lt;br/&gt;
         throw new IllegalStateException(&quot;Attempt to append a timestamp (%d) to slot %d no larger than the last timestamp appended (%d) to %s.&quot;&lt;br/&gt;
             .format(timestamp, _entries, lastEntry.timestamp, file.getAbsolutePath))&lt;br/&gt;
+&lt;br/&gt;
       // We only append to the time index when the timestamp is greater than the last inserted timestamp.&lt;br/&gt;
       // If all the messages are in message format v0, the timestamp will always be NoTimestamp. In that case, the time&lt;br/&gt;
       // index will be empty.&lt;br/&gt;
       if (timestamp &amp;gt; lastEntry.timestamp) {&lt;br/&gt;
+        val relOffset =&lt;br/&gt;
+          try relativeOffset(offset)&lt;br/&gt;
+          catch {&lt;br/&gt;
+            case _: InvalidOffsetException if (mayHaveIndexOverflow) =&amp;gt; {&lt;br/&gt;
+              debug(s&quot;Skipping time index append: overflow for offset: $offset baseOffset: $baseOffset (${file.getAbsolutePath})&quot;)&lt;br/&gt;
+              return&lt;br/&gt;
+            }&lt;br/&gt;
+          }&lt;br/&gt;
+&lt;br/&gt;
         debug(&quot;Adding index entry %d =&amp;gt; %d to %s.&quot;.format(timestamp, offset, file.getName))&lt;br/&gt;
         mmap.putLong(timestamp)&lt;/li&gt;
	&lt;li&gt;mmap.putInt((offset - baseOffset).toInt)&lt;br/&gt;
+        mmap.putInt(relOffset)&lt;br/&gt;
         _entries += 1&lt;br/&gt;
         _lastEntry = TimestampOffset(timestamp, offset)&lt;br/&gt;
         require(_entries * entrySize == mmap.position(), _entries + &quot; entries but file position in index is &quot; + mmap.position() + &quot;.&quot;)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
index 906c26da4cf..178bb8cee48 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
@@ -118,7 +118,9 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     val cleaner = makeCleaner(Int.MaxValue)&lt;br/&gt;
     val logProps = new Properties()&lt;br/&gt;
     logProps.put(LogConfig.SegmentBytesProp, 2048: java.lang.Integer)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;var log = makeLog(config = LogConfig.fromProps(logConfig.originals, logProps))&lt;br/&gt;
+&lt;br/&gt;
+    val config = LogConfig.fromProps(logConfig.originals, logProps)&lt;br/&gt;
+    var log = makeLog(config = config)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val producerEpoch = 0.toShort&lt;br/&gt;
     val pid1 = 1&lt;br/&gt;
@@ -137,13 +139,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     assertEquals(List(2, 3, 1, 4), keysInLog(log))&lt;br/&gt;
     assertEquals(List(1, 3, 6, 7), offsetsInLog(log))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// we have to reload the log to validate that the cleaner maintained sequence numbers correctly&lt;/li&gt;
	&lt;li&gt;def reloadLog(): Unit = 
{
-      log.close()
-      log = makeLog(config = LogConfig.fromProps(logConfig.originals, logProps), recoveryPoint = 0L)
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;reloadLog()&lt;br/&gt;
+    log = reloadLog(log)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // check duplicate append from producer 1&lt;br/&gt;
     var logAppendInfo = appendIdempotentAsLeader(log, pid1, producerEpoch)(Seq(1, 2, 3))&lt;br/&gt;
@@ -169,7 +165,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     assertEquals(List(3, 1, 4, 2), keysInLog(log))&lt;br/&gt;
     assertEquals(List(3, 6, 7, 8), offsetsInLog(log))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reloadLog()&lt;br/&gt;
+    log = reloadLog(log)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // duplicate append from producer1 should still be fine&lt;br/&gt;
     logAppendInfo = appendIdempotentAsLeader(log, pid1, producerEpoch)(Seq(1, 2, 3))&lt;br/&gt;
@@ -1214,6 +1210,68 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     assertEquals(&quot;The tombstone should be retained.&quot;, 1, log.logSegments.head.log.batches.iterator.next().lastOffset)
   }

&lt;p&gt;+  /**&lt;br/&gt;
+    * Verify that cleaner does not crash, and handles the case where a log segment has messages&lt;br/&gt;
+    * with offsets that overflow base_offset + Integer.MAX_VALUE due to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
+    */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testLogSegmentWithIndexOffsetOverflow(): Unit = {&lt;br/&gt;
+    val props = logProps&lt;br/&gt;
+    // Make sure every append is potentially able to create an index entry&lt;br/&gt;
+    props.put(LogConfig.IndexIntervalBytesProp, 1: java.lang.Integer)&lt;br/&gt;
+    val config = LogConfig(props)&lt;br/&gt;
+&lt;br/&gt;
+    val tp = new TopicPartition(&quot;test&quot;, 0)&lt;br/&gt;
+    val cleaner = makeCleaner(Int.MaxValue)&lt;br/&gt;
+    var log = makeLog(config = config)&lt;br/&gt;
+&lt;br/&gt;
+    val keys = List(1, 2, 2, 3)&lt;br/&gt;
+    val offsets = List(0L, 1L, Int.MaxValue + 0L, Int.MaxValue + 1L)&lt;br/&gt;
+&lt;br/&gt;
+    // Create a &quot;legacy&quot; log segment, consisting of messages that would result in index overflow&lt;br/&gt;
+    for (i &amp;lt;- keys.indices) &lt;/p&gt;
{
+      val records = MemoryRecords.withRecords(offsets(i), CompressionType.NONE, 0,
+        new SimpleRecord(i, keys(i).toString.getBytes, keys(i).toString.getBytes))
+
+      // write to the segment directly since offsets will overflow the index
+      log.activeSegment.append(offsets(i), i, offsets(i), records, allowOversizeIndexOffset = true)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Since we wrote messages directly to the log segment, bypassing the log layer, the log now would be in an&lt;br/&gt;
+    // inconsistent state. Reload the log so that the state is recomputed and built up accurately.&lt;br/&gt;
+    log = reloadLog(log)&lt;br/&gt;
+&lt;br/&gt;
+    // Sanity check to make sure records were written into a single log segment&lt;br/&gt;
+    assertEquals(1, log.numberOfSegments)&lt;br/&gt;
+    assertEquals(keys, keysInLog(log))&lt;br/&gt;
+    assertEquals(offsets, offsetsInLog(log))&lt;br/&gt;
+    assertEquals(offsets.last + 1L, log.logEndOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // Make sure the offset index and time index last offsets correspond to baseOffset + Int.MaxValue&lt;br/&gt;
+    // We&apos;d also have two index entries in total for the four messages we added, corresponding the the second and&lt;br/&gt;
+    // third append&lt;br/&gt;
+    assertEquals(2, log.activeSegment.timeIndex._entries)&lt;br/&gt;
+    assertEquals(Int.MaxValue + 0L, log.activeSegment.timeIndex.lastEntry.offset)&lt;br/&gt;
+    assertEquals(2, log.activeSegment.offsetIndex._entries)&lt;br/&gt;
+    assertEquals(Int.MaxValue + 0L, log.activeSegment.offsetIndex.lastOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // Roll the log so that the segment we are interested in becomes inactive&lt;br/&gt;
+    log.roll()&lt;br/&gt;
+    cleaner.doClean(LogToClean(tp, log, 0, Long.MaxValue), Long.MaxValue)&lt;br/&gt;
+&lt;br/&gt;
+    // We must have now cleaned up duplicate keys, and should have still retained those messages&lt;br/&gt;
+    // whose offset resulted in overflow&lt;br/&gt;
+    assertEquals(2, log.numberOfSegments)&lt;br/&gt;
+    assertEquals(List(1, 2, 3), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(0L, Int.MaxValue + 0L, Int.MaxValue + 1L), offsetsInLog(log))&lt;br/&gt;
+    assertEquals(0L, log.logStartOffset)&lt;br/&gt;
+    assertEquals(Int.MaxValue + 2L, log.logEndOffset)&lt;br/&gt;
+&lt;br/&gt;
+    val firstSegment = log.logSegments.head&lt;br/&gt;
+    assertEquals(0L, firstSegment.baseOffset)&lt;br/&gt;
+    assertEquals(Int.MaxValue + 1L, firstSegment.log.batches.asScala.map(_.lastOffset).max)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   private def writeToLog(log: Log, keysAndValues: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Int)&amp;#93;&lt;/span&gt;, offsetSeq: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     for(((key, value), offset) &amp;lt;- keysAndValues.zip(offsetSeq))&lt;br/&gt;
       yield log.appendAsFollower(messageWithOffset(key, value, offset)).lastOffset&lt;br/&gt;
@@ -1269,6 +1327,11 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     for ((key, value) &amp;lt;- seq) yield log.appendAsLeader(record(key, value), leaderEpoch = 0).firstOffset.get
   }

&lt;p&gt;+  private def reloadLog(log: Log): Log = &lt;/p&gt;
{
+    log.close()
+    makeLog(config = log.config, recoveryPoint = 0L)
+  }
&lt;p&gt;+&lt;br/&gt;
   private def key(id: Int) = ByteBuffer.wrap(id.toString.getBytes)&lt;/p&gt;

&lt;p&gt;   private def record(key: Int, value: Int,&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
index 8fa3cc19648..69b43958f84 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
@@ -35,10 +35,11 @@ class OffsetIndexTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;   var idx: OffsetIndex = null&lt;br/&gt;
   val maxEntries = 30&lt;br/&gt;
+  val baseOffset = 45L&lt;/p&gt;

&lt;p&gt;   @Before&lt;br/&gt;
   def setup() &lt;/p&gt;
{
-    this.idx = new OffsetIndex(nonExistentTempFile(), baseOffset = 45L, maxIndexSize = 30 * 8)
+    this.idx = new OffsetIndex(nonExistentTempFile(), baseOffset = baseOffset, maxIndexSize = 30 * 8)
   }

&lt;p&gt;   @After&lt;br/&gt;
@@ -102,10 +103,10 @@ class OffsetIndexTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testFetchUpperBoundOffset() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val first = OffsetPosition(0, 0)&lt;/li&gt;
	&lt;li&gt;val second = OffsetPosition(1, 10)&lt;/li&gt;
	&lt;li&gt;val third = OffsetPosition(2, 23)&lt;/li&gt;
	&lt;li&gt;val fourth = OffsetPosition(3, 37)&lt;br/&gt;
+    val first = OffsetPosition(baseOffset + 0, 0)&lt;br/&gt;
+    val second = OffsetPosition(baseOffset + 1, 10)&lt;br/&gt;
+    val third = OffsetPosition(baseOffset + 2, 23)&lt;br/&gt;
+    val fourth = OffsetPosition(baseOffset + 3, 37)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(None, idx.fetchUpperBoundOffset(first, 5))&lt;/p&gt;

&lt;p&gt;@@ -177,6 +178,24 @@ class OffsetIndexTest extends JUnitSuite &lt;/p&gt;
{
     // mmap should be null after unmap causing lookup to throw a NPE
     intercept[NullPointerException](idx.lookup(1))
   }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testIndexOffsetOverflow(): Unit = {&lt;br/&gt;
+    val first = OffsetPosition(baseOffset + 0, 0)&lt;br/&gt;
+    val second = OffsetPosition(baseOffset + 1, 10)&lt;br/&gt;
+    val third = OffsetPosition(baseOffset + 2, 23)&lt;br/&gt;
+    val fourth = OffsetPosition(baseOffset + 3, 37)&lt;br/&gt;
+    val fifth = OffsetPosition(baseOffset + Integer.MAX_VALUE, 40)&lt;br/&gt;
+    val sixth = OffsetPosition(baseOffset + Integer.MAX_VALUE + 1L, 41)&lt;br/&gt;
+&lt;br/&gt;
+    for (offsetPosition &amp;lt;- Seq(first, second, third, fourth, fifth))&lt;br/&gt;
+      idx.append(offsetPosition.offset, offsetPosition.position)&lt;br/&gt;
+    assert(5 == idx.entries)&lt;br/&gt;
+&lt;br/&gt;
+    // try to insert an index entry that would cause offset overflow&lt;br/&gt;
+    intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;InvalidOffsetException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ idx.append(sixth.offset, sixth.position) }
&lt;p&gt;+    assert(5 == idx.entries)&lt;br/&gt;
+  }&lt;/p&gt;

&lt;p&gt;   def assertWriteFails&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(message: String, idx: OffsetIndex, offset: Int, klass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
     try {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/TimeIndexTest.scala b/core/src/test/scala/unit/kafka/log/TimeIndexTest.scala&lt;br/&gt;
index 8520f8917ec..5d2174c77ce 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/TimeIndexTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/TimeIndexTest.scala&lt;br/&gt;
@@ -32,10 +32,16 @@ class TimeIndexTest extends JUnitSuite {&lt;br/&gt;
   var idx: TimeIndex = null&lt;br/&gt;
   val maxEntries = 30&lt;br/&gt;
   val baseOffset = 45L&lt;br/&gt;
+  val indexFile = nonExistantTempFile()&lt;br/&gt;
+  val maxIndexSize = maxEntries * 12&lt;br/&gt;
+&lt;br/&gt;
+  def openTimeIndex: TimeIndex = &lt;/p&gt;
{
+    new TimeIndex(indexFile, baseOffset = baseOffset, maxIndexSize = maxIndexSize)
+  }

&lt;p&gt;   @Before&lt;br/&gt;
   def setup() &lt;/p&gt;
{
-    this.idx = new TimeIndex(nonExistantTempFile(), baseOffset = baseOffset, maxIndexSize = maxEntries * 12)
+    this.idx = openTimeIndex
   }

&lt;p&gt;   @After&lt;br/&gt;
@@ -75,17 +81,17 @@ class TimeIndexTest extends JUnitSuite {&lt;br/&gt;
   def testAppend() {&lt;br/&gt;
     appendEntries(maxEntries - 1)&lt;br/&gt;
     intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;IllegalArgumentException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
-      idx.maybeAppend(10000L, 1000L)
+      idx.maybeAppend(10000L, 1000L, skipIndexFullCheck =  false)
     }
&lt;p&gt;     intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;InvalidOffsetException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
-      idx.maybeAppend(10000L, (maxEntries - 2) * 10, true)
+      idx.maybeAppend(10000L, (maxEntries - 2) * 10, skipIndexFullCheck = true)
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;idx.maybeAppend(10000L, 1000L, true)&lt;br/&gt;
+    idx.maybeAppend(10000L, 1000L, skipIndexFullCheck = true)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def appendEntries(numEntries: Int) &lt;/p&gt;
{
     for (i &amp;lt;- 1 to numEntries)
-      idx.maybeAppend(i * 10, i * 10 + baseOffset)
+      idx.maybeAppend(i * 10, i * 10 + baseOffset, skipIndexFullCheck = false)
   }

&lt;p&gt;   def nonExistantTempFile(): File = {&lt;br/&gt;
@@ -134,5 +140,53 @@ class TimeIndexTest extends JUnitSuite &lt;/p&gt;
{
     idx.close()
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testOffsetOverflow(): Unit = {&lt;br/&gt;
+    val first = TimestampOffset(0, baseOffset + 0)&lt;br/&gt;
+    val second = TimestampOffset(10, baseOffset + 1)&lt;br/&gt;
+    val third = TimestampOffset(23, baseOffset + 2)&lt;br/&gt;
+    val fourth = TimestampOffset(37, baseOffset + 3)&lt;br/&gt;
+    val fifth = TimestampOffset(40, baseOffset + Integer.MAX_VALUE)&lt;br/&gt;
+    val sixth = TimestampOffset(41, baseOffset + Integer.MAX_VALUE + 1L)&lt;br/&gt;
+&lt;br/&gt;
+    for (offsetPosition &amp;lt;- Seq(first, second, third, fourth, fifth))&lt;br/&gt;
+      idx.maybeAppend(offsetPosition.timestamp, offsetPosition.offset, skipIndexFullCheck = false)&lt;br/&gt;
+    assert(5 == idx.entries)&lt;br/&gt;
+&lt;br/&gt;
+    // try to insert an index entry that would cause offset overflow&lt;br/&gt;
+    intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;InvalidOffsetException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ idx.maybeAppend(sixth.timestamp, sixth.offset, skipIndexFullCheck = false) }
&lt;p&gt;+    assert(5 == idx.entries)&lt;br/&gt;
+&lt;br/&gt;
+    // call the internal API for ignoring offset overflow which should silently ignore the overflow, and skip appending to index&lt;br/&gt;
+    idx.maybeAppend(sixth.timestamp, sixth.offset, skipIndexFullCheck = false, mayHaveIndexOverflow = true)&lt;br/&gt;
+    assert(5 == idx.entries)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testOffsetOverflowCorruptionSanity(): Unit = {&lt;br/&gt;
+    val first = TimestampOffset(0, baseOffset + 0)&lt;br/&gt;
+    val second = TimestampOffset(10, baseOffset + 1)&lt;br/&gt;
+    val third = TimestampOffset(23, baseOffset + 2)&lt;br/&gt;
+    val fourth = TimestampOffset(37, baseOffset + 3)&lt;br/&gt;
+    val fifth = TimestampOffset(40, baseOffset + Integer.MAX_VALUE)&lt;br/&gt;
+    val sixth = TimestampOffset(41, baseOffset + Integer.MAX_VALUE + 1L)&lt;br/&gt;
+&lt;br/&gt;
+    for (offsetPosition &amp;lt;- Seq(first, second, third, fourth, fifth))&lt;br/&gt;
+      idx.maybeAppend(offsetPosition.timestamp, offsetPosition.offset, skipIndexFullCheck = false)&lt;br/&gt;
+    assert(5 == idx.entries)&lt;br/&gt;
+&lt;br/&gt;
+    // forcefully append an index overflow to simulate pre &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt; state&lt;br/&gt;
+    idx.mmap.putLong(sixth.timestamp)&lt;br/&gt;
+    idx.mmap.putInt((sixth.offset - baseOffset).toInt)&lt;br/&gt;
+    idx._entries += 1&lt;br/&gt;
+&lt;br/&gt;
+    // close the index and reload so that state is setup correctly&lt;br/&gt;
+    idx.close()&lt;br/&gt;
+    idx = openTimeIndex&lt;br/&gt;
+    assert(6 == idx.entries)&lt;br/&gt;
+&lt;br/&gt;
+    // sanity check should now throw an exception because of the index overflow&lt;br/&gt;
+    intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;CorruptIndexException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ idx.sanityCheck() }
&lt;p&gt;+  }&lt;br/&gt;
 }&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16506154" author="githubbot" created="Fri, 8 Jun 2018 15:40:05 +0000"  >&lt;p&gt;hachikuji closed pull request #4975: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; title=&quot;Log cleaner thread may die on legacy segment containing messages whose offsets are too large&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6264&quot;&gt;&lt;del&gt;KAFKA-6264&lt;/del&gt;&lt;/a&gt;: Log cleaner thread may die on legacy segment containing messages whose offsets are too large&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4975&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4975&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java b/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java&lt;br/&gt;
index 6b6e0ab03ef..7443545c4ef 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java&lt;br/&gt;
@@ -130,7 +130,7 @@ public ByteBuffer readInto(ByteBuffer buffer, int position) throws IOException {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param size The number of bytes after the start position to include&lt;/li&gt;
	&lt;li&gt;@return A sliced wrapper on this message set limited based on the given position and size&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public FileRecords read(int position, int size) throws IOException {&lt;br/&gt;
+    public FileRecords slice(int position, int size) throws IOException {&lt;br/&gt;
         if (position &amp;lt; 0)&lt;br/&gt;
             throw new IllegalArgumentException(&quot;Invalid position: &quot; + position + &quot; in read from &quot; + file);&lt;br/&gt;
         if (size &amp;lt; 0)&lt;br/&gt;
@@ -355,7 +355,14 @@ public String toString() 
{
                 &quot;)&quot;;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Iterable&amp;lt;FileChannelRecordBatch&amp;gt; batchesFrom(final int start) {&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Get an iterator over the record batches in the file, starting at a specific position. This is similar to&lt;br/&gt;
+     * 
{@link #batches()}
&lt;p&gt; except that callers specify a particular position to start reading the batches from. This&lt;br/&gt;
+     * method must be used with caution: the start position passed in must be a known start of a batch.&lt;br/&gt;
+     * @param start The position to start record iteration from; must be a known position for start of a batch&lt;br/&gt;
+     * @return An iterator over batches starting from &lt;/p&gt;
{@code start}
&lt;p&gt;+     */&lt;br/&gt;
+    public Iterable&amp;lt;FileChannelRecordBatch&amp;gt; batchesFrom(final int start) {&lt;br/&gt;
         return new Iterable&amp;lt;FileChannelRecordBatch&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public Iterator&amp;lt;FileChannelRecordBatch&amp;gt; iterator() {&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java b/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java&lt;br/&gt;
index fdd3ede16cc..dbfacddea43 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java&lt;br/&gt;
@@ -120,7 +120,7 @@ public void testIterationDoesntChangePosition() throws IOException {&lt;br/&gt;
      */&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testRead() throws IOException {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;FileRecords read = fileRecords.read(0, fileRecords.sizeInBytes());&lt;br/&gt;
+        FileRecords read = fileRecords.slice(0, fileRecords.sizeInBytes());&lt;br/&gt;
         assertEquals(fileRecords.sizeInBytes(), read.sizeInBytes());&lt;br/&gt;
         TestUtils.checkEquals(fileRecords.batches(), read.batches());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -128,35 +128,35 @@ public void testRead() throws IOException {&lt;br/&gt;
         RecordBatch first = items.get(0);&lt;/p&gt;

&lt;p&gt;         // read from second message until the end&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;read = fileRecords.read(first.sizeInBytes(), fileRecords.sizeInBytes() - first.sizeInBytes());&lt;br/&gt;
+        read = fileRecords.slice(first.sizeInBytes(), fileRecords.sizeInBytes() - first.sizeInBytes());&lt;br/&gt;
         assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());&lt;br/&gt;
         assertEquals(&quot;Read starting from the second message&quot;, items.subList(1, items.size()), batches(read));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // read from second message and size is past the end of the file&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;read = fileRecords.read(first.sizeInBytes(), fileRecords.sizeInBytes());&lt;br/&gt;
+        read = fileRecords.slice(first.sizeInBytes(), fileRecords.sizeInBytes());&lt;br/&gt;
         assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());&lt;br/&gt;
         assertEquals(&quot;Read starting from the second message&quot;, items.subList(1, items.size()), batches(read));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // read from second message and position + size overflows&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;read = fileRecords.read(first.sizeInBytes(), Integer.MAX_VALUE);&lt;br/&gt;
+        read = fileRecords.slice(first.sizeInBytes(), Integer.MAX_VALUE);&lt;br/&gt;
         assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());&lt;br/&gt;
         assertEquals(&quot;Read starting from the second message&quot;, items.subList(1, items.size()), batches(read));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // read from second message and size is past the end of the file on a view/slice&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;read = fileRecords.read(1, fileRecords.sizeInBytes() - 1)&lt;/li&gt;
	&lt;li&gt;.read(first.sizeInBytes() - 1, fileRecords.sizeInBytes());&lt;br/&gt;
+        read = fileRecords.slice(1, fileRecords.sizeInBytes() - 1)&lt;br/&gt;
+                .slice(first.sizeInBytes() - 1, fileRecords.sizeInBytes());&lt;br/&gt;
         assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());&lt;br/&gt;
         assertEquals(&quot;Read starting from the second message&quot;, items.subList(1, items.size()), batches(read));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // read from second message and position + size overflows on a view/slice&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;read = fileRecords.read(1, fileRecords.sizeInBytes() - 1)&lt;/li&gt;
	&lt;li&gt;.read(first.sizeInBytes() - 1, Integer.MAX_VALUE);&lt;br/&gt;
+        read = fileRecords.slice(1, fileRecords.sizeInBytes() - 1)&lt;br/&gt;
+                .slice(first.sizeInBytes() - 1, Integer.MAX_VALUE);&lt;br/&gt;
         assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());&lt;br/&gt;
         assertEquals(&quot;Read starting from the second message&quot;, items.subList(1, items.size()), batches(read));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // read a single message starting from second message&lt;br/&gt;
         RecordBatch second = items.get(1);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;read = fileRecords.read(first.sizeInBytes(), second.sizeInBytes());&lt;br/&gt;
+        read = fileRecords.slice(first.sizeInBytes(), second.sizeInBytes());&lt;br/&gt;
         assertEquals(second.sizeInBytes(), read.sizeInBytes());&lt;br/&gt;
         assertEquals(&quot;Read a single message starting from the second message&quot;,&lt;br/&gt;
                 Collections.singletonList(second), batches(read));&lt;br/&gt;
@@ -206,9 +206,9 @@ public void testIteratorWithLimits() throws IOException 
{
         RecordBatch batch = batches(fileRecords).get(1);
         int start = fileRecords.searchForOffsetWithSize(1, 0).position;
         int size = batch.sizeInBytes();
-        FileRecords slice = fileRecords.read(start, size);
+        FileRecords slice = fileRecords.slice(start, size);
         assertEquals(Collections.singletonList(batch), batches(slice));
-        FileRecords slice2 = fileRecords.read(start, size - 1);
+        FileRecords slice2 = fileRecords.slice(start, size - 1);
         assertEquals(Collections.emptyList(), batches(slice2));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -343,7 +343,7 @@ public void testFormatConversionWithPartialMessage() throws IOException {&lt;br/&gt;
         RecordBatch batch = batches(fileRecords).get(1);&lt;br/&gt;
         int start = fileRecords.searchForOffsetWithSize(1, 0).position;&lt;br/&gt;
         int size = batch.sizeInBytes();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FileRecords slice = fileRecords.read(start, size - 1);&lt;br/&gt;
+        FileRecords slice = fileRecords.slice(start, size - 1);&lt;br/&gt;
         Records messageV0 = slice.downConvert(RecordBatch.MAGIC_VALUE_V0, 0, time).records();&lt;br/&gt;
         assertTrue(&quot;No message should be there&quot;, batches(messageV0).isEmpty());&lt;br/&gt;
         assertEquals(&quot;There should be &quot; + (size - 1) + &quot; bytes&quot;, size - 1, messageV0.sizeInBytes());&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/common/IndexOffsetOverflowException.scala b/core/src/main/scala/kafka/common/IndexOffsetOverflowException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..7f3ea110356
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/IndexOffsetOverflowException.scala&lt;br/&gt;
@@ -0,0 +1,25 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates that an attempt was made to append a message whose offset could cause the index offset to overflow.&lt;br/&gt;
+ */&lt;br/&gt;
+class IndexOffsetOverflowException(message: String, cause: Throwable) extends KafkaException(message, cause) 
{
+  def this(message: String) = this(message, null)
+}
&lt;p&gt;diff --git a/core/src/main/scala/kafka/common/LogSegmentOffsetOverflowException.scala b/core/src/main/scala/kafka/common/LogSegmentOffsetOverflowException.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..62379dea9b9&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/core/src/main/scala/kafka/common/LogSegmentOffsetOverflowException.scala&lt;br/&gt;
@@ -0,0 +1,31 @@&lt;br/&gt;
+/**&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package kafka.common&lt;br/&gt;
+&lt;br/&gt;
+import kafka.log.LogSegment&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Indicates that the log segment contains one or more messages that overflow the offset (and / or time) index. This is&lt;br/&gt;
+ * not a typical scenario, and could only happen when brokers have log segments that were created before the patch for&lt;br/&gt;
+ * &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5413&quot; title=&quot;Log cleaner fails due to large offset in segment file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5413&quot;&gt;&lt;del&gt;KAFKA-5413&lt;/del&gt;&lt;/a&gt;. With &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; title=&quot;Log cleaner thread may die on legacy segment containing messages whose offsets are too large&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6264&quot;&gt;&lt;del&gt;KAFKA-6264&lt;/del&gt;&lt;/a&gt;, we have the ability to split such log segments into multiple log segments such that we&lt;br/&gt;
+ * do not have any segments with offset overflow.&lt;br/&gt;
+ */&lt;br/&gt;
+class LogSegmentOffsetOverflowException(message: String, cause: Throwable, val logSegment: LogSegment) extends KafkaException(message, cause) 
{
+  def this(cause: Throwable, logSegment: LogSegment) = this(null, cause, logSegment)
+  def this(message: String, logSegment: LogSegment) = this(message, null, logSegment)
+}
&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/AbstractIndex.scala b/core/src/main/scala/kafka/log/AbstractIndex.scala&lt;br/&gt;
index 44083c186c8..95f074949c1 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/AbstractIndex.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/AbstractIndex.scala&lt;br/&gt;
@@ -18,11 +18,12 @@&lt;br/&gt;
 package kafka.log&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.io.&lt;/p&gt;
{File, RandomAccessFile}
&lt;p&gt;-import java.nio.&lt;/p&gt;
{ByteBuffer, MappedByteBuffer}&lt;br/&gt;
 import java.nio.channels.FileChannel&lt;br/&gt;
 import java.nio.file.Files&lt;br/&gt;
+import java.nio.{ByteBuffer, MappedByteBuffer}
&lt;p&gt; import java.util.concurrent.locks.&lt;/p&gt;
{Lock, ReentrantLock}

&lt;p&gt;+import kafka.common.IndexOffsetOverflowException&lt;br/&gt;
 import kafka.log.IndexSearchType.IndexSearchEntity&lt;br/&gt;
 import kafka.utils.CoreUtils.inLock&lt;br/&gt;
 import kafka.utils.&lt;/p&gt;
{CoreUtils, Logging}
&lt;p&gt;@@ -226,6 +227,26 @@ abstract class AbstractIndex&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(@volatile var file: File, val baseOffset: Lon&lt;br/&gt;
     resize(maxIndexSize)&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  /**&lt;br/&gt;
+   * Get offset relative to base offset of this index&lt;br/&gt;
+   * @throws IndexOffsetOverflowException&lt;br/&gt;
+   */&lt;br/&gt;
+  def relativeOffset(offset: Long): Int = {&lt;br/&gt;
+    val relativeOffset = toRelative(offset)&lt;br/&gt;
+    if (relativeOffset.isEmpty)&lt;br/&gt;
+      throw new IndexOffsetOverflowException(s&quot;Integer overflow for offset: $offset (${file.getAbsoluteFile})&quot;)&lt;br/&gt;
+    relativeOffset.get&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Check if a particular offset is valid to be appended to this index.&lt;br/&gt;
+   * @param offset The offset to check&lt;br/&gt;
+   * @return true if this offset is valid to be appended to this index; false otherwise&lt;br/&gt;
+   */&lt;br/&gt;
+  def canAppendOffset(offset: Long): Boolean = &lt;/p&gt;
{
+    toRelative(offset).isDefined
+  }
&lt;p&gt;+&lt;br/&gt;
   protected def safeForceUnmap(): Unit = {&lt;br/&gt;
     try forceUnmap()&lt;br/&gt;
     catch {&lt;br/&gt;
@@ -325,6 +346,14 @@ abstract class AbstractIndex&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(@volatile var file: File, val baseOffset: Lon&lt;br/&gt;
    */&lt;br/&gt;
   private def roundDownToExactMultiple(number: Int, factor: Int) = factor * (number / factor)&lt;/p&gt;

&lt;p&gt;+  private def toRelative(offset: Long): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
+    val relativeOffset = offset - baseOffset
+    if (relativeOffset &amp;lt; 0 || relativeOffset &amp;gt; Int.MaxValue)
+      None
+    else
+      Some(relativeOffset.toInt)
+  }
&lt;p&gt;+&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt; object IndexSearchType extends Enumeration {&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index af83775595c..391bee3875c 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -18,33 +18,34 @@&lt;br/&gt;
 package kafka.log&lt;/p&gt;

&lt;p&gt; import java.io.&lt;/p&gt;
{File, IOException}&lt;br/&gt;
+import java.lang.{Long =&amp;gt; JLong}&lt;br/&gt;
+import java.nio.ByteBuffer&lt;br/&gt;
 import java.nio.file.{Files, NoSuchFileException}&lt;br/&gt;
 import java.text.NumberFormat&lt;br/&gt;
+import java.util.Map.{Entry =&amp;gt; JEntry}&lt;br/&gt;
 import java.util.concurrent.atomic._&lt;br/&gt;
 import java.util.concurrent.{ConcurrentNavigableMap, ConcurrentSkipListMap, TimeUnit}&lt;br/&gt;
+import java.util.regex.Pattern&lt;br/&gt;
 &lt;br/&gt;
+import com.yammer.metrics.core.Gauge&lt;br/&gt;
 import kafka.api.KAFKA_0_10_0_IV0&lt;br/&gt;
-import kafka.common.{InvalidOffsetException, KafkaException, LongRef}&lt;br/&gt;
+import kafka.common.{InvalidOffsetException, KafkaException, LogSegmentOffsetOverflowException, LongRef}&lt;br/&gt;
+import kafka.message.{BrokerCompressionCodec, CompressionCodec, NoCompressionCodec}&lt;br/&gt;
 import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
+import kafka.server.checkpoints.{LeaderEpochCheckpointFile, LeaderEpochFile}&lt;br/&gt;
+import kafka.server.epoch.{LeaderEpochCache, LeaderEpochFileCache}&lt;br/&gt;
 import kafka.server.{BrokerTopicStats, FetchDataInfo, LogDirFailureChannel, LogOffsetMetadata}&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.errors.{CorruptRecordException, KafkaStorageException, OffsetOutOfRangeException, RecordBatchTooLargeException, RecordTooLargeException, UnsupportedForMessageFormatException}&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
+import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
 import org.apache.kafka.common.requests.{IsolationLevel, ListOffsetRequest}&lt;br/&gt;
+import org.apache.kafka.common.utils.{Time, Utils}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.{ArrayBuffer, ListBuffer}&lt;br/&gt;
 import scala.collection.{Seq, Set, mutable}&lt;br/&gt;
-import com.yammer.metrics.core.Gauge&lt;br/&gt;
-import org.apache.kafka.common.utils.{Time, Utils}&lt;br/&gt;
-import kafka.message.{BrokerCompressionCodec, CompressionCodec, NoCompressionCodec}&lt;br/&gt;
-import kafka.server.checkpoints.{LeaderEpochCheckpointFile, LeaderEpochFile}&lt;br/&gt;
-import kafka.server.epoch.{LeaderEpochCache, LeaderEpochFileCache}&lt;br/&gt;
-import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
-import java.util.Map.{Entry =&amp;gt; JEntry}&lt;br/&gt;
-import java.lang.{Long =&amp;gt; JLong}&lt;br/&gt;
-import java.util.regex.Pattern&lt;br/&gt;
 &lt;br/&gt;
 object LogAppendInfo {&lt;br/&gt;
   val UnknownLogAppendInfo = LogAppendInfo(None, -1, RecordBatch.NO_TIMESTAMP, -1L, RecordBatch.NO_TIMESTAMP, -1L,&lt;br/&gt;
@@ -85,15 +86,15 @@ case class LogAppendInfo(var firstOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
                          validBytes: Int,&lt;br/&gt;
                          offsetsMonotonic: Boolean) {&lt;br/&gt;
   /**&lt;br/&gt;
-    * Get the first offset if it exists, else get the last offset.&lt;br/&gt;
-    * @return The offset of first message if it exists; else offset of the last message.&lt;br/&gt;
-    */&lt;br/&gt;
+   * Get the first offset if it exists, else get the last offset.&lt;br/&gt;
+   * @return The offset of first message if it exists; else offset of the last message.&lt;br/&gt;
+   */&lt;br/&gt;
   def firstOrLastOffset: Long = firstOffset.getOrElse(lastOffset)&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Get the (maximum) number of messages described by LogAppendInfo&lt;br/&gt;
-    * @return Maximum possible number of messages described by LogAppendInfo&lt;br/&gt;
-    */&lt;br/&gt;
+   * Get the (maximum) number of messages described by LogAppendInfo&lt;br/&gt;
+   * @return Maximum possible number of messages described by LogAppendInfo&lt;br/&gt;
+   */&lt;br/&gt;
   def numMessages: Long = {&lt;br/&gt;
     firstOffset match {
       case Some(firstOffsetVal) if (firstOffsetVal &amp;gt;= 0 &amp;amp;&amp;amp; lastOffset &amp;gt;= 0) =&amp;gt; (lastOffset - firstOffsetVal + 1)
@@ -157,7 +158,7 @@ class Log(@volatile var dir: File,
           @volatile var recoveryPoint: Long,
           scheduler: Scheduler,
           brokerTopicStats: BrokerTopicStats,
-          time: Time,
+          val time: Time,
           val maxProducerIdExpirationMs: Int,
           val producerIdExpirationCheckIntervalMs: Int,
           val topicPartition: TopicPartition,
@@ -295,42 +296,79 @@ class Log(@volatile var dir: File,
       new LeaderEpochCheckpointFile(LeaderEpochFile.newFile(dir), logDirFailureChannel))
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Removes any temporary files found in log directory, and creates a list of all .swap files which could be swapped&lt;br/&gt;
+   * in place of existing segment(s). For log splitting, we know that any .swap file whose base offset is higher than&lt;br/&gt;
+   * the smallest offset .clean file could be part of an incomplete split operation. Such .swap files are also deleted&lt;br/&gt;
+   * by this method.&lt;br/&gt;
+   * @return Set of .swap files that are valid to be swapped in as segment files&lt;br/&gt;
+   */&lt;br/&gt;
   private def removeTempFilesAndCollectSwapFiles(): Set&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
 &lt;br/&gt;
-    def deleteIndicesIfExist(baseFile: File, swapFile: File, fileType: String): Unit = {&lt;br/&gt;
-      info(s&quot;Found $fileType file ${swapFile.getAbsolutePath} from interrupted swap operation. Deleting index files (if they exist).&quot;)&lt;br/&gt;
+    def deleteIndicesIfExist(baseFile: File, suffix: String = &quot;&quot;): Unit = {
+      info(s&quot;Deleting index files with suffix $suffix for baseFile $baseFile&quot;)
       val offset = offsetFromFile(baseFile)
-      Files.deleteIfExists(Log.offsetIndexFile(dir, offset).toPath)
-      Files.deleteIfExists(Log.timeIndexFile(dir, offset).toPath)
-      Files.deleteIfExists(Log.transactionIndexFile(dir, offset).toPath)
+      Files.deleteIfExists(Log.offsetIndexFile(dir, offset, suffix).toPath)
+      Files.deleteIfExists(Log.timeIndexFile(dir, offset, suffix).toPath)
+      Files.deleteIfExists(Log.transactionIndexFile(dir, offset, suffix).toPath)
     }&lt;br/&gt;
 &lt;br/&gt;
     var swapFiles = Set&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    var cleanFiles = Set&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    var minCleanedFileOffset = Long.MaxValue&lt;br/&gt;
 &lt;br/&gt;
     for (file &amp;lt;- dir.listFiles if file.isFile) {&lt;br/&gt;
       if (!file.canRead)&lt;br/&gt;
         throw new IOException(s&quot;Could not read file $file&quot;)&lt;br/&gt;
       val filename = file.getName&lt;br/&gt;
-      if (filename.endsWith(DeletedFileSuffix) || filename.endsWith(CleanedFileSuffix)) {&lt;br/&gt;
+      if (filename.endsWith(DeletedFileSuffix)) {&lt;br/&gt;
         debug(s&quot;Deleting stray temporary file ${file.getAbsolutePath}&quot;)&lt;br/&gt;
         Files.deleteIfExists(file.toPath)&lt;br/&gt;
+      } else if (filename.endsWith(CleanedFileSuffix)) {
+        minCleanedFileOffset = Math.min(offsetFromFileName(filename), minCleanedFileOffset)
+        cleanFiles += file
       } else if (filename.endsWith(SwapFileSuffix)) {&lt;br/&gt;
         // we crashed in the middle of a swap operation, to recover:&lt;br/&gt;
         // if a log, delete the index files, complete the swap operation later&lt;br/&gt;
         // if an index just delete the index files, they will be rebuilt&lt;br/&gt;
         val baseFile = new File(CoreUtils.replaceSuffix(file.getPath, SwapFileSuffix, &quot;&quot;))&lt;br/&gt;
+        info(s&quot;Found file ${file.getAbsolutePath} from interrupted swap operation.&quot;)&lt;br/&gt;
         if (isIndexFile(baseFile)) {
-          deleteIndicesIfExist(baseFile, file, &quot;index&quot;)
+          deleteIndicesIfExist(baseFile)
         } else if (isLogFile(baseFile)) {
-          deleteIndicesIfExist(baseFile, file, &quot;log&quot;)
+          deleteIndicesIfExist(baseFile)
           swapFiles += file
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
-    swapFiles&lt;br/&gt;
+&lt;br/&gt;
+    // &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; title=&quot;Log cleaner thread may die on legacy segment containing messages whose offsets are too large&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6264&quot;&gt;&lt;del&gt;KAFKA-6264&lt;/del&gt;&lt;/a&gt;: Delete all .swap files whose base offset is greater than the minimum .cleaned segment offset. Such .swap&lt;br/&gt;
+    // files could be part of an incomplete split operation that could not complete. See Log#splitOverflowedSegment&lt;br/&gt;
+    // for more details about the split operation.&lt;br/&gt;
+    val (invalidSwapFiles, validSwapFiles) = swapFiles.partition(file =&amp;gt; offsetFromFile(file) &amp;gt;= minCleanedFileOffset)&lt;br/&gt;
+    invalidSwapFiles.foreach { file =&amp;gt;&lt;br/&gt;
+      debug(s&quot;Deleting invalid swap file ${file.getAbsoluteFile} minCleanedFileOffset: $minCleanedFileOffset&quot;)&lt;br/&gt;
+      val baseFile = new File(CoreUtils.replaceSuffix(file.getPath, SwapFileSuffix, &quot;&quot;))&lt;br/&gt;
+      deleteIndicesIfExist(baseFile, SwapFileSuffix)&lt;br/&gt;
+      Files.deleteIfExists(file.toPath)&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // Now that we have deleted all .swap files that constitute an incomplete split operation, let&apos;s delete all .clean files&lt;br/&gt;
+    cleanFiles.foreach { file =&amp;gt;&lt;br/&gt;
+      debug(s&quot;Deleting stray .clean file ${file.getAbsolutePath}&quot;)&lt;br/&gt;
+      Files.deleteIfExists(file.toPath)&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    validSwapFiles&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  // This method does not need to convert IOException to KafkaStorageException because it is only called before all logs are loaded&lt;br/&gt;
+  /**&lt;br/&gt;
+   * This method does not need to convert IOException to KafkaStorageException because it is only called before all logs are loaded&lt;br/&gt;
+   * It is possible that we encounter a segment with index offset overflow in which case the LogSegmentOffsetOverflowException&lt;br/&gt;
+   * will be thrown. Note that any segments that were opened before we encountered the exception will remain open and the&lt;br/&gt;
+   * caller is responsible for closing them appropriately, if needed.&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if the log directory contains a segment with messages that overflow the index offset&lt;br/&gt;
+   */&lt;br/&gt;
   private def loadSegmentFiles(): Unit = {
     // load segments in ascending order because transactional data from one segment may depend on the
     // segments that come before it
@@ -369,6 +407,13 @@ class Log(@volatile var dir: File,
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Recover the given segment.&lt;br/&gt;
+   * @param segment Segment to recover&lt;br/&gt;
+   * @param leaderEpochCache Optional cache for updating the leader epoch during recovery&lt;br/&gt;
+   * @return The number of bytes truncated from the segment&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if the segment contains messages that cause index offset overflow&lt;br/&gt;
+   */&lt;br/&gt;
   private def recoverSegment(segment: LogSegment, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = lock synchronized {
     val stateManager = new ProducerStateManager(topicPartition, dir, maxProducerIdExpirationMs)
     stateManager.truncateAndReload(logStartOffset, segment.baseOffset, time.milliseconds)
@@ -383,7 +428,6 @@ class Log(@volatile var dir: File,
     // take a snapshot for the first recovered segment to avoid reloading all the segments if we shutdown before we
     // checkpoint the recovery point
     stateManager.takeSnapshot()
-
     val bytesTruncated = segment.recover(stateManager, leaderEpochCache)
 
     // once we have recovered the segment&apos;s data, take a snapshot to ensure that we won&apos;t
@@ -392,7 +436,16 @@ class Log(@volatile var dir: File,
     bytesTruncated
   }&lt;br/&gt;
 &lt;br/&gt;
-  // This method does not need to convert IOException to KafkaStorageException because it is only called before all logs are loaded&lt;br/&gt;
+  /**&lt;br/&gt;
+   * This method does not need to convert IOException to KafkaStorageException because it is only called before all logs&lt;br/&gt;
+   * are loaded.&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if the swap file contains messages that cause the log segment offset to&lt;br/&gt;
+   *                                           overflow. Note that this is currently a fatal exception as we do not have&lt;br/&gt;
+   *                                           a way to deal with it. The exception is propagated all the way up to&lt;br/&gt;
+   *                                           KafkaServer#startup which will cause the broker to shut down if we are in&lt;br/&gt;
+   *                                           this situation. This is expected to be an extremely rare scenario in practice,&lt;br/&gt;
+   *                                           and manual intervention might be required to get out of it.&lt;br/&gt;
+   */&lt;br/&gt;
   private def completeSwapOperations(swapFiles: Set&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
     for (swapFile &amp;lt;- swapFiles) {&lt;br/&gt;
       val logFile = new File(CoreUtils.replaceSuffix(swapFile.getPath, SwapFileSuffix, &quot;&quot;))&lt;br/&gt;
@@ -404,20 +457,49 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         fileSuffix = SwapFileSuffix)&lt;br/&gt;
       info(s&quot;Found log file ${swapFile.getPath} from interrupted swap operation, repairing.&quot;)&lt;br/&gt;
       recoverSegment(swapSegment)&lt;br/&gt;
-      val oldSegments = logSegments(swapSegment.baseOffset, swapSegment.readNextOffset)&lt;br/&gt;
-      replaceSegments(swapSegment, oldSegments.toSeq, isRecoveredSwapFile = true)&lt;br/&gt;
+&lt;br/&gt;
+      var oldSegments = logSegments(swapSegment.baseOffset, swapSegment.readNextOffset)&lt;br/&gt;
+&lt;br/&gt;
+      // We create swap files for two cases: (1) Log cleaning where multiple segments are merged into one, and&lt;br/&gt;
+      // (2) Log splitting where one segment is split into multiple.&lt;br/&gt;
+      // Both of these mean that the resultant swap segments be composed of the original set, i.e. the swap segment&lt;br/&gt;
+      // must fall within the range of existing segment(s). If we cannot find such a segment, it means the deletion&lt;br/&gt;
+      // of that segment was successful. In such an event, we should simply rename the .swap to .log without having to&lt;br/&gt;
+      // do a replace with an existing segment.&lt;br/&gt;
+      if (oldSegments.nonEmpty) {
+        val start = oldSegments.head.baseOffset
+        val end = oldSegments.last.readNextOffset
+        if (!(swapSegment.baseOffset &amp;gt;= start &amp;amp;&amp;amp; swapSegment.baseOffset &amp;lt;= end))
+          oldSegments = List()
+      }&lt;br/&gt;
+&lt;br/&gt;
+      replaceSegments(Seq(swapSegment), oldSegments.toSeq, isRecoveredSwapFile = true)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  // Load the log segments from the log files on disk and return the next offset&lt;br/&gt;
-  // This method does not need to convert IOException to KafkaStorageException because it is only called before all logs are loaded&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Load the log segments from the log files on disk and return the next offset.&lt;br/&gt;
+   * This method does not need to convert IOException to KafkaStorageException because it is only called before all logs&lt;br/&gt;
+   * are loaded.&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if we encounter a .swap file with messages that overflow index offset; or when&lt;br/&gt;
+   *                                           we find an unexpected number of .log files with overflow&lt;br/&gt;
+   */&lt;br/&gt;
   private def loadSegments(): Long = {&lt;br/&gt;
     // first do a pass through the files in the log directory and remove any temporary files&lt;br/&gt;
     // and find any interrupted swap operations&lt;br/&gt;
     val swapFiles = removeTempFilesAndCollectSwapFiles()&lt;br/&gt;
 &lt;br/&gt;
-    // now do a second pass and load all the log and index files&lt;br/&gt;
-    loadSegmentFiles()&lt;br/&gt;
+    // Now do a second pass and load all the log and index files.&lt;br/&gt;
+    // We might encounter legacy log segments with offset overflow (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6264&quot; title=&quot;Log cleaner thread may die on legacy segment containing messages whose offsets are too large&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6264&quot;&gt;&lt;del&gt;KAFKA-6264&lt;/del&gt;&lt;/a&gt;). We need to split such segments. Whe&lt;br/&gt;
+    // this happens, restart loading segment files from scratch.&lt;br/&gt;
+    retryOnOffsetOverflow {
+      // In case we encounter a segment with offset overflow, the retry logic will split it after which we need to retry
+      // loading of segments. In that case, we also need to close all segments that could have been left open in previous
+      // call to loadSegmentFiles().
+      logSegments.foreach(_.close())
+      segments.clear()
+      loadSegmentFiles()
+    }&lt;br/&gt;
 &lt;br/&gt;
     // Finally, complete any interrupted swap operations. To be crash-safe,&lt;br/&gt;
     // log files that are replaced by the swap segment should be renamed to .deleted&lt;br/&gt;
@@ -435,7 +517,10 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         preallocate = config.preallocate))&lt;br/&gt;
       0&lt;br/&gt;
     } else if (!dir.getAbsolutePath.endsWith(Log.DeleteDirSuffix)) {&lt;br/&gt;
-      val nextOffset = recoverLog()&lt;br/&gt;
+      val nextOffset = retryOnOffsetOverflow {
+        recoverLog()
+      }&lt;br/&gt;
+&lt;br/&gt;
       // reset the index size of the currently active log segment to allow more entries&lt;br/&gt;
       activeSegment.resizeIndexes(config.maxIndexSize)&lt;br/&gt;
       nextOffset&lt;br/&gt;
@@ -448,9 +533,9 @@ class Log(@volatile var dir: File,&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Recover the log segments and return the next offset after recovery.&lt;br/&gt;
-   *&lt;br/&gt;
    * This method does not need to convert IOException to KafkaStorageException because it is only called before all&lt;br/&gt;
    * logs are loaded.&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if we encountered a legacy segment with offset overflow&lt;br/&gt;
    */&lt;br/&gt;
   private def recoverLog(): Long = {
     // if we have the clean shutdown marker, skip recovery
@@ -585,10 +670,10 @@ class Log(@volatile var dir: File,
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Rename the directory of the log&lt;br/&gt;
-    *&lt;br/&gt;
-    * @throws KafkaStorageException if rename fails&lt;br/&gt;
-    */&lt;br/&gt;
+   * Rename the directory of the log&lt;br/&gt;
+   *&lt;br/&gt;
+   * @throws KafkaStorageException if rename fails&lt;br/&gt;
+   */&lt;br/&gt;
   def renameDir(name: String) {&lt;br/&gt;
     lock synchronized {&lt;br/&gt;
       maybeHandleIOException(s&quot;Error while renaming dir for $topicPartition in log dir ${dir.getParent}&quot;) {&lt;br/&gt;
@@ -1315,9 +1400,9 @@ class Log(@volatile var dir: File,&lt;br/&gt;
 &lt;br/&gt;
     if (segment.shouldRoll(messagesSize, maxTimestampInMessages, maxOffsetInMessages, now)) {&lt;br/&gt;
       debug(s&quot;Rolling new log segment (log_size = ${segment.size}/${config.segmentSize}}, &quot; +&lt;br/&gt;
-          s&quot;offset_index_size = ${segment.offsetIndex.entries}/${segment.offsetIndex.maxEntries}, &quot; +&lt;br/&gt;
-          s&quot;time_index_size = ${segment.timeIndex.entries}/${segment.timeIndex.maxEntries}, &quot; +&lt;br/&gt;
-          s&quot;inactive_time_ms = ${segment.timeWaitedForRoll(now, maxTimestampInMessages)}/${config.segmentMs - segment.rollJitterMs}).&quot;)&lt;br/&gt;
+        s&quot;offset_index_size = ${segment.offsetIndex.entries}/${segment.offsetIndex.maxEntries}, &quot; +&lt;br/&gt;
+        s&quot;time_index_size = ${segment.timeIndex.entries}/${segment.timeIndex.maxEntries}, &quot; +&lt;br/&gt;
+        s&quot;inactive_time_ms = ${segment.timeWaitedForRoll(now, maxTimestampInMessages)}/${config.segmentMs - segment.rollJitterMs}).&quot;)&lt;br/&gt;
 &lt;br/&gt;
       /*&lt;br/&gt;
         maxOffsetInMessages - Integer.MAX_VALUE is a heuristic value for the first offset in the set of messages.&lt;br/&gt;
@@ -1644,51 +1729,59 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-   * Swap a new segment in place and delete one or more existing segments in a crash-safe manner. The old segments will&lt;br/&gt;
-   * be asynchronously deleted.&lt;br/&gt;
+   * Swap one or more new segment in place and delete one or more existing segments in a crash-safe manner. The old&lt;br/&gt;
+   * segments will be asynchronously deleted.&lt;br/&gt;
    *&lt;br/&gt;
    * This method does not need to convert IOException to KafkaStorageException because it is either called before all logs are loaded&lt;br/&gt;
    * or the caller will catch and handle IOException&lt;br/&gt;
    *&lt;br/&gt;
    * The sequence of operations is:&lt;br/&gt;
    * &amp;lt;ol&amp;gt;&lt;br/&gt;
-   *   &amp;lt;li&amp;gt; Cleaner creates new segment with suffix .cleaned and invokes replaceSegments().&lt;br/&gt;
+   *   &amp;lt;li&amp;gt; Cleaner creates one or more new segments with suffix .cleaned and invokes replaceSegments().&lt;br/&gt;
    *        If broker crashes at this point, the clean-and-swap operation is aborted and&lt;br/&gt;
-   *        the .cleaned file is deleted on recovery in loadSegments().&lt;br/&gt;
-   *   &amp;lt;li&amp;gt; New segment is renamed .swap. If the broker crashes after this point before the whole&lt;br/&gt;
-   *        operation is completed, the swap operation is resumed on recovery as described in the next step.&lt;br/&gt;
+   *        the .cleaned files are deleted on recovery in loadSegments().&lt;br/&gt;
+   *   &amp;lt;li&amp;gt; New segments are renamed .swap. If the broker crashes before all segments were renamed to .swap, the&lt;br/&gt;
+   *        clean-and-swap operation is aborted - .cleaned as well as .swap files are deleted on recovery in&lt;br/&gt;
+   *        loadSegments(). We detect this situation by maintaining a specific order in which files are renamed from&lt;br/&gt;
+   *        .cleaned to .swap. Basically, files are renamed in descending order of offsets. On recovery, all .swap files&lt;br/&gt;
+   *        whose offset is greater than the minimum-offset .clean file are deleted.&lt;br/&gt;
+   *   &amp;lt;li&amp;gt; If the broker crashes after all new segments were renamed to .swap, the operation is completed, the swap&lt;br/&gt;
+   *        operation is resumed on recovery as described in the next step.&lt;br/&gt;
    *   &amp;lt;li&amp;gt; Old segment files are renamed to .deleted and asynchronous delete is scheduled.&lt;br/&gt;
    *        If the broker crashes, any .deleted files left behind are deleted on recovery in loadSegments().&lt;br/&gt;
    *        replaceSegments() is then invoked to complete the swap with newSegment recreated from&lt;br/&gt;
    *        the .swap file and oldSegments containing segments which were not renamed before the crash.&lt;br/&gt;
-   *   &amp;lt;li&amp;gt; Swap segment is renamed to replace the existing segment, completing this operation.&lt;br/&gt;
+   *   &amp;lt;li&amp;gt; Swap segment(s) are renamed to replace the existing segments, completing this operation.&lt;br/&gt;
    *        If the broker crashes, any .deleted files which may be left behind are deleted&lt;br/&gt;
    *        on recovery in loadSegments().&lt;br/&gt;
    * &amp;lt;/ol&amp;gt;&lt;br/&gt;
    *&lt;br/&gt;
-   * @param newSegment The new log segment to add to the log&lt;br/&gt;
+   * @param newSegments The new log segment to add to the log&lt;br/&gt;
    * @param oldSegments The old log segments to delete from the log&lt;br/&gt;
    * @param isRecoveredSwapFile true if the new segment was created from a swap file during recovery after a crash&lt;br/&gt;
    */&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def replaceSegments(newSegment: LogSegment, oldSegments: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt;, isRecoveredSwapFile: Boolean = false) {&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def replaceSegments(newSegments: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt;, oldSegments: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt;, isRecoveredSwapFile: Boolean = false) {&lt;br/&gt;
+    val sortedNewSegments = newSegments.sortBy(_.baseOffset)&lt;br/&gt;
+    val sortedOldSegments = oldSegments.sortBy(_.baseOffset)&lt;br/&gt;
+&lt;br/&gt;
     lock synchronized {&lt;br/&gt;
       checkIfMemoryMappedBufferClosed()&lt;br/&gt;
       // need to do this in two phases to be crash safe AND do the delete asynchronously&lt;br/&gt;
       // if we crash in the middle of this we complete the swap in loadSegments()&lt;br/&gt;
       if (!isRecoveredSwapFile)&lt;br/&gt;
-        newSegment.changeFileSuffixes(Log.CleanedFileSuffix, Log.SwapFileSuffix)&lt;br/&gt;
-      addSegment(newSegment)&lt;br/&gt;
+        sortedNewSegments.reverse.foreach(_.changeFileSuffixes(Log.CleanedFileSuffix, Log.SwapFileSuffix))&lt;br/&gt;
+      sortedNewSegments.reverse.foreach(addSegment(_))&lt;br/&gt;
 &lt;br/&gt;
       // delete the old files&lt;br/&gt;
-      for (seg &amp;lt;- oldSegments) {&lt;br/&gt;
+      for (seg &amp;lt;- sortedOldSegments) {
         // remove the index entry
-        if (seg.baseOffset != newSegment.baseOffset)
+        if (seg.baseOffset != sortedNewSegments.head.baseOffset)
           segments.remove(seg.baseOffset)
         // delete segment
         asyncDeleteSegment(seg)
       }&lt;br/&gt;
       // okay we are safe now, remove the swap suffix&lt;br/&gt;
-      newSegment.changeFileSuffixes(Log.SwapFileSuffix, &quot;&quot;)&lt;br/&gt;
+      sortedNewSegments.foreach(_.changeFileSuffixes(Log.SwapFileSuffix, &quot;&quot;))&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -1701,12 +1794,13 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     removeMetric(&quot;LogEndOffset&quot;, tags)&lt;br/&gt;
     removeMetric(&quot;Size&quot;, tags)&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Add the given segment to the segments in this log. If this segment replaces an existing segment, delete it.&lt;br/&gt;
-   *&lt;br/&gt;
    * @param segment The segment to add&lt;br/&gt;
    */&lt;br/&gt;
-  def addSegment(segment: LogSegment) = this.segments.put(segment.baseOffset, segment)&lt;br/&gt;
+  @threadsafe&lt;br/&gt;
+  def addSegment(segment: LogSegment): LogSegment = this.segments.put(segment.baseOffset, segment)&lt;br/&gt;
 &lt;br/&gt;
   private def maybeHandleIOException&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(msg: =&amp;gt; String)(fun: =&amp;gt; T): T = {&lt;br/&gt;
     try {
@@ -1718,6 +1812,140 @@ class Log(@volatile var dir: File,
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if we encounter segments with index overflow for more than maxTries&lt;br/&gt;
+   */&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def retryOnOffsetOverflow&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(fn: =&amp;gt; T): T = {&lt;br/&gt;
+    var triesSoFar = 0&lt;br/&gt;
+    while (true) {&lt;br/&gt;
+      try {
+        return fn
+      } catch {&lt;br/&gt;
+        case e: LogSegmentOffsetOverflowException =&amp;gt;&lt;br/&gt;
+          triesSoFar += 1&lt;br/&gt;
+          info(s&quot;Caught LogOffsetOverflowException ${e.getMessage}. Split segment and retry. retry#: $triesSoFar.&quot;)&lt;br/&gt;
+          splitOverflowedSegment(e.logSegment)&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    throw new IllegalStateException()&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Split the given log segment into multiple such that there is no offset overflow in the resulting segments. The&lt;br/&gt;
+   * resulting segments will contain the exact same messages that are present in the input segment. On successful&lt;br/&gt;
+   * completion of this method, the input segment will be deleted and will be replaced by the resulting new segments.&lt;br/&gt;
+   * See replaceSegments for recovery logic, in case the broker dies in the middle of this operation.&lt;br/&gt;
+   * &amp;lt;p&amp;gt;Note that this method assumes we have already determined that the segment passed in contains records that cause&lt;br/&gt;
+   * offset overflow.&amp;lt;/p&amp;gt;&lt;br/&gt;
+   * &amp;lt;p&amp;gt;The split logic overloads the use of .clean files that LogCleaner typically uses to make the process of replacing&lt;br/&gt;
+   * the input segment with multiple new segments atomic and recoverable in the event of a crash. See replaceSegments&lt;br/&gt;
+   * and completeSwapOperations for the implementation to make this operation recoverable on crashes.&amp;lt;/p&amp;gt;&lt;br/&gt;
+   * @param segment Segment to split&lt;br/&gt;
+   * @return List of new segments that replace the input segment&lt;br/&gt;
+   */&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def splitOverflowedSegment(segment: LogSegment): List&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    require(isLogFile(segment.log.file), s&quot;Cannot split file ${segment.log.file.getAbsoluteFile}&quot;)&lt;br/&gt;
+    info(s&quot;Attempting to split segment ${segment.log.file.getAbsolutePath}&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    val newSegments = ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    var position = 0&lt;br/&gt;
+    val sourceRecords = segment.log&lt;br/&gt;
+    var readBuffer = ByteBuffer.allocate(1024 * 1024)&lt;br/&gt;
+&lt;br/&gt;
+    class CopyResult(val bytesRead: Int, val overflowOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+&lt;br/&gt;
+    // Helper method to copy `records` into `segment`. Makes sure records being appended do not result in offset overflow.&lt;br/&gt;
+    def copyRecordsToSegment(records: FileRecords, segment: LogSegment, readBuffer: ByteBuffer): CopyResult = {&lt;br/&gt;
+      var bytesRead = 0&lt;br/&gt;
+      var maxTimestamp = Long.MinValue&lt;br/&gt;
+      var offsetOfMaxTimestamp = Long.MinValue&lt;br/&gt;
+      var maxOffset = Long.MinValue&lt;br/&gt;
+&lt;br/&gt;
+      // find all batches that are valid to be appended to the current log segment&lt;br/&gt;
+      val (validBatches, overflowBatches) = records.batches.asScala.span(batch =&amp;gt; segment.offsetIndex.canAppendOffset(batch.lastOffset))&lt;br/&gt;
+      val overflowOffset = overflowBatches.headOption.map { firstBatch =&amp;gt;&lt;br/&gt;
+        info(s&quot;Found overflow at offset ${firstBatch.baseOffset} in segment $segment&quot;)&lt;br/&gt;
+        firstBatch.baseOffset&lt;br/&gt;
+      }&lt;br/&gt;
+&lt;br/&gt;
+      // return early if no valid batches were found&lt;br/&gt;
+      if (validBatches.isEmpty) {
+        require(overflowOffset.isDefined, &quot;No batches found during split&quot;)
+        return new CopyResult(0, overflowOffset)
+      }&lt;br/&gt;
+&lt;br/&gt;
+      // determine the maximum offset and timestamp in batches&lt;br/&gt;
+      for (batch &amp;lt;- validBatches) {&lt;br/&gt;
+        if (batch.maxTimestamp &amp;gt; maxTimestamp) {
+          maxTimestamp = batch.maxTimestamp
+          offsetOfMaxTimestamp = batch.lastOffset
+        }&lt;br/&gt;
+        maxOffset = batch.lastOffset&lt;br/&gt;
+        bytesRead += batch.sizeInBytes&lt;br/&gt;
+      }&lt;br/&gt;
+&lt;br/&gt;
+      // read all valid batches into memory&lt;br/&gt;
+      val validRecords = records.slice(0, bytesRead)&lt;br/&gt;
+      require(readBuffer.capacity &amp;gt;= validRecords.sizeInBytes)&lt;br/&gt;
+      readBuffer.clear()&lt;br/&gt;
+      readBuffer.limit(validRecords.sizeInBytes)&lt;br/&gt;
+      validRecords.readInto(readBuffer, 0)&lt;br/&gt;
+&lt;br/&gt;
+      // append valid batches into the segment&lt;br/&gt;
+      segment.append(maxOffset, maxTimestamp, offsetOfMaxTimestamp, MemoryRecords.readableRecords(readBuffer))&lt;br/&gt;
+      readBuffer.clear()&lt;br/&gt;
+      info(s&quot;Appended messages till $maxOffset to segment $segment during split&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      new CopyResult(bytesRead, overflowOffset)&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    try {&lt;br/&gt;
+      info(s&quot;Splitting segment $segment&quot;)&lt;br/&gt;
+      newSegments += LogCleaner.createNewCleanedSegment(this, segment.baseOffset)&lt;br/&gt;
+      while (position &amp;lt; sourceRecords.sizeInBytes) {
+        val currentSegment = newSegments.last
+
+        // grow buffers if needed
+        val firstBatch = sourceRecords.batchesFrom(position).asScala.head
+        if (firstBatch.sizeInBytes &amp;gt; readBuffer.capacity)
+          readBuffer = ByteBuffer.allocate(firstBatch.sizeInBytes)
+
+        // get records we want to copy and copy them into the new segment
+        val recordsToCopy = sourceRecords.slice(position, readBuffer.capacity)
+        val copyResult = copyRecordsToSegment(recordsToCopy, currentSegment, readBuffer)
+        position += copyResult.bytesRead
+
+        // create a new segment if there was an overflow
+        copyResult.overflowOffset.foreach(overflowOffset =&amp;gt; newSegments += LogCleaner.createNewCleanedSegment(this, overflowOffset))
+      }&lt;br/&gt;
+      require(newSegments.length &amp;gt; 1, s&quot;No offset overflow found for $segment&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      // prepare new segments&lt;br/&gt;
+      var totalSizeOfNewSegments = 0&lt;br/&gt;
+      info(s&quot;Split messages from $segment into ${newSegments.length} new segments&quot;)&lt;br/&gt;
+      newSegments.foreach { splitSegment =&amp;gt;
+        splitSegment.onBecomeInactiveSegment()
+        splitSegment.flush()
+        splitSegment.lastModified = segment.lastModified
+        totalSizeOfNewSegments += splitSegment.log.sizeInBytes
+        info(s&quot;New segment: $splitSegment&quot;)
+      }&lt;br/&gt;
+      // size of all the new segments combined must equal size of the original segment&lt;br/&gt;
+      require(totalSizeOfNewSegments == segment.log.sizeInBytes, &quot;Inconsistent segment sizes after split&quot; +&lt;br/&gt;
+        s&quot; before: ${segment.log.sizeInBytes} after: $totalSizeOfNewSegments&quot;)&lt;br/&gt;
+    } catch {&lt;br/&gt;
+      case e: Exception =&amp;gt;&lt;br/&gt;
+        newSegments.foreach { splitSegment =&amp;gt;
+          splitSegment.close()
+          splitSegment.deleteIfExists()
+        }&lt;br/&gt;
+        throw e&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // replace old segment with new ones&lt;br/&gt;
+    replaceSegments(newSegments.toList, List(segment), isRecoveredSwapFile = false)&lt;br/&gt;
+    newSegments.toList&lt;br/&gt;
+  }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
@@ -1809,17 +2037,17 @@ object Log {&lt;br/&gt;
     new File(dir, filenamePrefixFromOffset(offset) + LogFileSuffix + suffix)&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Return a directory name to rename the log directory to for async deletion. The name will be in the following&lt;br/&gt;
-    * format: topic-partition.uniqueId-delete where topic, partition and uniqueId are variables.&lt;br/&gt;
-    */&lt;br/&gt;
+   * Return a directory name to rename the log directory to for async deletion. The name will be in the following&lt;br/&gt;
+   * format: topic-partition.uniqueId-delete where topic, partition and uniqueId are variables.&lt;br/&gt;
+   */&lt;br/&gt;
   def logDeleteDirName(topicPartition: TopicPartition): String = {
     logDirNameWithSuffix(topicPartition, DeleteDirSuffix)
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Return a future directory name for the given topic partition. The name will be in the following&lt;br/&gt;
-    * format: topic-partition.uniqueId-future where topic, partition and uniqueId are variables.&lt;br/&gt;
-    */&lt;br/&gt;
+   * Return a future directory name for the given topic partition. The name will be in the following&lt;br/&gt;
+   * format: topic-partition.uniqueId-future where topic, partition and uniqueId are variables.&lt;br/&gt;
+   */&lt;br/&gt;
   def logFutureDirName(topicPartition: TopicPartition): String = {
     logDirNameWithSuffix(topicPartition, FutureDirSuffix)
   }&lt;br/&gt;
@@ -1830,9 +2058,9 @@ object Log {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Return a directory name for the given topic partition. The name will be in the following&lt;br/&gt;
-    * format: topic-partition where topic, partition are variables.&lt;br/&gt;
-    */&lt;br/&gt;
+   * Return a directory name for the given topic partition. The name will be in the following&lt;br/&gt;
+   * format: topic-partition where topic, partition are variables.&lt;br/&gt;
+   */&lt;br/&gt;
   def logDirName(topicPartition: TopicPartition): String = {&lt;br/&gt;
     s&quot;${topicPartition.topic}-${topicPartition.partition}&quot;&lt;br/&gt;
   }&lt;br/&gt;
@@ -1857,6 +2085,9 @@ object Log {&lt;br/&gt;
   def timeIndexFile(dir: File, offset: Long, suffix: String = &quot;&quot;): File =&lt;br/&gt;
     new File(dir, filenamePrefixFromOffset(offset) + TimeIndexFileSuffix + suffix)&lt;br/&gt;
 &lt;br/&gt;
+  def deleteFileIfExists(file: File, suffix: String = &quot;&quot;): Unit =&lt;br/&gt;
+    Files.deleteIfExists(new File(file.getPath + suffix).toPath)&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Construct a producer id snapshot file using the given offset.&lt;br/&gt;
    *&lt;br/&gt;
@@ -1876,17 +2107,20 @@ object Log {&lt;br/&gt;
   def transactionIndexFile(dir: File, offset: Long, suffix: String = &quot;&quot;): File =&lt;br/&gt;
     new File(dir, filenamePrefixFromOffset(offset) + TxnIndexFileSuffix + suffix)&lt;br/&gt;
 &lt;br/&gt;
-  def offsetFromFile(file: File): Long = {&lt;br/&gt;
-    val filename = file.getName&lt;br/&gt;
+  def offsetFromFileName(filename: String): Long = {
     filename.substring(0, filename.indexOf(&apos;.&apos;)).toLong
   }&lt;br/&gt;
 &lt;br/&gt;
+  def offsetFromFile(file: File): Long = {
+    offsetFromFileName(file.getName)
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
-    * Calculate a log&apos;s size (in bytes) based on its log segments&lt;br/&gt;
-    *&lt;br/&gt;
-    * @param segments The log segments to calculate the size of&lt;br/&gt;
-    * @return Sum of the log segments&apos; sizes (in bytes)&lt;br/&gt;
-    */&lt;br/&gt;
+   * Calculate a log&apos;s size (in bytes) based on its log segments&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param segments The log segments to calculate the size of&lt;br/&gt;
+   * @return Sum of the log segments&apos; sizes (in bytes)&lt;br/&gt;
+   */&lt;br/&gt;
   def sizeInBytes(segments: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt;): Long =&lt;br/&gt;
     segments.map(_.size.toLong).sum&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogCleaner.scala b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
index aa7cfe276c4..d79a84069d8 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
@@ -19,7 +19,6 @@ package kafka.log&lt;br/&gt;
 &lt;br/&gt;
 import java.io.{File, IOException}
&lt;p&gt; import java.nio._&lt;br/&gt;
-import java.nio.file.Files&lt;br/&gt;
 import java.util.Date&lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;/p&gt;

&lt;p&gt;@@ -28,16 +27,16 @@ import kafka.common._&lt;br/&gt;
 import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{BrokerReconfigurable, KafkaConfig, LogDirFailureChannel}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
-import org.apache.kafka.common.record._&lt;br/&gt;
-import org.apache.kafka.common.utils.Time&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.config.ConfigException&lt;br/&gt;
 import org.apache.kafka.common.errors.&lt;/p&gt;
{CorruptRecordException, KafkaStorageException}
&lt;p&gt; import org.apache.kafka.common.record.MemoryRecords.RecordFilter&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter.BatchRetention&lt;br/&gt;
+import org.apache.kafka.common.record._&lt;br/&gt;
+import org.apache.kafka.common.utils.Time&lt;/p&gt;

&lt;p&gt;-import scala.collection.&lt;/p&gt;
{Set, mutable}&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
+import scala.collection.{Set, mutable}

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The cleaner is responsible for removing obsolete records from logs which have the &quot;compact&quot; retention strategy.&lt;br/&gt;
@@ -382,6 +381,12 @@ object LogCleaner 
{
       enableCleaner = config.logCleanerEnable)
 
   }
&lt;p&gt;+&lt;br/&gt;
+  def createNewCleanedSegment(log: Log, baseOffset: Long): LogSegment = &lt;/p&gt;
{
+    LogSegment.deleteIfExists(log.dir, baseOffset, fileSuffix = Log.CleanedFileSuffix)
+    LogSegment.open(log.dir, baseOffset, log.config, Time.SYSTEM, fileAlreadyExists = false,
+      fileSuffix = Log.CleanedFileSuffix, initFileSize = log.initFileSize, preallocate = log.config.preallocate)
+  }
&lt;p&gt; }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; /**&lt;br/&gt;
@@ -454,7 +459,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
     // this is the lower of the last active segment and the compaction lag&lt;br/&gt;
     val cleanableHorizonMs = log.logSegments(0, cleanable.firstUncleanableOffset).lastOption.map(_.lastModified).getOrElse(0L)&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     // group the segments and clean the groups&lt;br/&gt;
     info(&quot;Cleaning log %s (cleaning prior to %s, discarding tombstones prior to %s)...&quot;.format(log.name, new Date(cleanableHorizonMs), new Date(deleteHorizonMs)))&lt;br/&gt;
     for (group &amp;lt;- groupSegmentsBySize(log.logSegments(0, endOffset), log.config.segmentSize, log.config.maxIndexSize, cleanable.firstUncleanableOffset))&lt;br/&gt;
@@ -482,21 +486,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
                                  map: OffsetMap,&lt;br/&gt;
                                  deleteHorizonMs: Long,&lt;br/&gt;
                                  stats: CleanerStats) {&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def deleteCleanedFileIfExists(file: File): Unit = 
{
-      Files.deleteIfExists(new File(file.getPath + Log.CleanedFileSuffix).toPath)
-    }
&lt;p&gt;-&lt;br/&gt;
     // create a new segment with a suffix appended to the name of the log and indexes&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val firstSegment = segments.head&lt;/li&gt;
	&lt;li&gt;deleteCleanedFileIfExists(firstSegment.log.file)&lt;/li&gt;
	&lt;li&gt;deleteCleanedFileIfExists(firstSegment.offsetIndex.file)&lt;/li&gt;
	&lt;li&gt;deleteCleanedFileIfExists(firstSegment.timeIndex.file)&lt;/li&gt;
	&lt;li&gt;deleteCleanedFileIfExists(firstSegment.txnIndex.file)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val baseOffset = firstSegment.baseOffset&lt;/li&gt;
	&lt;li&gt;val cleaned = LogSegment.open(log.dir, baseOffset, log.config, time, fileSuffix = Log.CleanedFileSuffix,&lt;/li&gt;
	&lt;li&gt;initFileSize = log.initFileSize, preallocate = log.config.preallocate)&lt;br/&gt;
+    val cleaned = LogCleaner.createNewCleanedSegment(log, segments.head.baseOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     try {&lt;br/&gt;
       // clean segments into the new destination segment&lt;br/&gt;
@@ -514,9 +505,18 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
         val retainDeletes = currentSegment.lastModified &amp;gt; deleteHorizonMs&lt;br/&gt;
         info(s&quot;Cleaning segment $startOffset in log ${log.name} (largest timestamp ${new Date(currentSegment.largestTimestamp)}) &quot; +&lt;br/&gt;
           s&quot;into ${cleaned.baseOffset}, ${if(retainDeletes) &quot;retaining&quot; else &quot;discarding&quot;} deletes.&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cleanInto(log.topicPartition, currentSegment.log, cleaned, map, retainDeletes, log.config.maxMessageSize,&lt;/li&gt;
	&lt;li&gt;transactionMetadata, log.activeProducersWithLastSequence, stats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        try &lt;/p&gt;
{
+          cleanInto(log.topicPartition, currentSegment.log, cleaned, map, retainDeletes, log.config.maxMessageSize,
+            transactionMetadata, log.activeProducersWithLastSequence, stats)
+        }
&lt;p&gt; catch &lt;/p&gt;
{
+          case e: LogSegmentOffsetOverflowException =&amp;gt;
+            // Split the current segment. It&apos;s also safest to abort the current cleaning process, so that we retry from
+            // scratch once the split is complete.
+            info(s&quot;Caught LogSegmentOverflowException during log cleaning $e&quot;)
+            log.splitOverflowedSegment(currentSegment)
+            throw new LogCleaningAbortedException()
+        }
&lt;p&gt;         currentSegmentOpt = nextSegmentOpt&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;@@ -531,7 +531,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
       // swap in new segment&lt;br/&gt;
       info(s&quot;Swapping in cleaned segment ${cleaned.baseOffset} for segment(s) ${segments.map(_.baseOffset).mkString(&quot;,&quot;)} &quot; +&lt;br/&gt;
         s&quot;in log ${log.name}&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.replaceSegments(cleaned, segments)&lt;br/&gt;
+      log.replaceSegments(List(cleaned), segments)&lt;br/&gt;
     } catch {&lt;br/&gt;
       case e: LogCleaningAbortedException =&amp;gt;&lt;br/&gt;
         try cleaned.deleteIfExists()&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogSegment.scala b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
index 55ab088f55e..6d61a4145c6 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogSegment.scala&lt;br/&gt;
@@ -21,11 +21,12 @@ import java.nio.file.
{Files, NoSuchFileException}
&lt;p&gt; import java.nio.file.attribute.FileTime&lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import kafka.common.&lt;/p&gt;
{IndexOffsetOverflowException, LogSegmentOffsetOverflowException}
&lt;p&gt; import kafka.metrics.&lt;/p&gt;
{KafkaMetricsGroup, KafkaTimer}
&lt;p&gt; import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{FetchDataInfo, LogOffsetMetadata}
&lt;p&gt; import kafka.utils._&lt;br/&gt;
-import org.apache.kafka.common.errors.CorruptRecordException&lt;br/&gt;
+import org.apache.kafka.common.errors.&lt;/p&gt;
{CorruptRecordException, InvalidOffsetException}
&lt;p&gt; import org.apache.kafka.common.record.FileRecords.LogOffsetPosition&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
 import org.apache.kafka.common.utils.Time&lt;br/&gt;
@@ -103,7 +104,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;checks that the argument offset can be represented as an integer offset relative to the baseOffset.&lt;br/&gt;
    */&lt;br/&gt;
   def canConvertToRelativeOffset(offset: Long): Boolean = 
{
-    (offset - baseOffset) &amp;lt;= Integer.MAX_VALUE
+    offsetIndex.canAppendOffset(offset)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -117,6 +118,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param shallowOffsetOfMaxTimestamp The offset of the message that has the largest timestamp in the messages to append.&lt;/li&gt;
	&lt;li&gt;@param records The log entries to append.&lt;/li&gt;
	&lt;li&gt;@return the physical position in the file of the appended records&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if the largest offset causes index offset overflow&lt;br/&gt;
    */&lt;br/&gt;
   @nonthreadsafe&lt;br/&gt;
   def append(largestOffset: Long,&lt;br/&gt;
@@ -129,8 +131,13 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
       val physicalPosition = log.sizeInBytes()&lt;br/&gt;
       if (physicalPosition == 0)&lt;br/&gt;
         rollingBasedTimestamp = Some(largestTimestamp)&lt;br/&gt;
+&lt;br/&gt;
+      if (!canConvertToRelativeOffset(largestOffset))&lt;br/&gt;
+        throw new LogSegmentOffsetOverflowException(&lt;br/&gt;
+          s&quot;largest offset $largestOffset cannot be safely converted to relative offset for segment with baseOffset $baseOffset&quot;,&lt;br/&gt;
+          this)&lt;br/&gt;
+&lt;br/&gt;
       // append the messages&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;require(canConvertToRelativeOffset(largestOffset), &quot;largest offset in message set can not be safely converted to relative offset.&quot;)&lt;br/&gt;
       val appendedBytes = log.append(records)&lt;br/&gt;
       trace(s&quot;Appended $appendedBytes to ${log.file()} at end offset $largestOffset&quot;)&lt;br/&gt;
       // Update the in memory max timestamp and corresponding offset.&lt;br/&gt;
@@ -139,9 +146,9 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
         offsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp&lt;br/&gt;
       }&lt;br/&gt;
       // append an entry to the index (if needed)&lt;/li&gt;
	&lt;li&gt;if(bytesSinceLastIndexEntry &amp;gt; indexIntervalBytes) {&lt;/li&gt;
	&lt;li&gt;offsetIndex.append(largestOffset, physicalPosition)&lt;/li&gt;
	&lt;li&gt;timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)&lt;br/&gt;
+      if (bytesSinceLastIndexEntry &amp;gt; indexIntervalBytes) 
{
+        appendToOffsetIndex(largestOffset, physicalPosition)
+        maybeAppendToTimeIndex(maxTimestampSoFar, offsetOfMaxTimestamp)
         bytesSinceLastIndexEntry = 0
       }
&lt;p&gt;       bytesSinceLastIndexEntry += records.sizeInBytes&lt;br/&gt;
@@ -193,8 +200,8 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;no more than maxSize bytes and will end before maxOffset if a maxOffset is specified.&lt;br/&gt;
    *&lt;/li&gt;
	&lt;li&gt;@param startOffset A lower bound on the first offset to include in the message set we read&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @param maxSize The maximum number of bytes to include in the message set we read&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param maxOffset An optional maximum offset for the message set we read&lt;br/&gt;
+   * @param maxSize The maximum number of bytes to include in the message set we read&lt;/li&gt;
	&lt;li&gt;@param maxPosition The maximum position in the log segment that should be exposed for read&lt;/li&gt;
	&lt;li&gt;@param minOneMessage If this is true, the first message will be returned even if it exceeds `maxSize` (if one exists)&lt;br/&gt;
    *&lt;br/&gt;
@@ -246,7 +253,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
         min(min(maxPosition, endPosition) - startPosition, adjustedMaxSize).toInt&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchDataInfo(offsetMetadata, log.read(startPosition, fetchSize),&lt;br/&gt;
+    FetchDataInfo(offsetMetadata, log.slice(startPosition, fetchSize),&lt;br/&gt;
       firstEntryIncomplete = adjustedMaxSize &amp;lt; startOffsetAndSize.size)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -261,6 +268,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;the transaction index.&lt;/li&gt;
	&lt;li&gt;@param leaderEpochCache Optionally a cache for updating the leader epoch during recovery.&lt;/li&gt;
	&lt;li&gt;@return The number of bytes truncated from the log&lt;br/&gt;
+   * @throws LogSegmentOffsetOverflowException if the log segment contains an offset that causes the index offset to overflow&lt;br/&gt;
    */&lt;br/&gt;
   @nonthreadsafe&lt;br/&gt;
   def recover(producerStateManager: ProducerStateManager, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = {&lt;br/&gt;
@@ -282,9 +290,8 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // Build offset index&lt;br/&gt;
         if (validBytes - lastIndexEntry &amp;gt; indexIntervalBytes) &lt;/p&gt;
{
-          val startOffset = batch.baseOffset
-          offsetIndex.append(startOffset, validBytes)
-          timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)
+          appendToOffsetIndex(batch.lastOffset, validBytes)
+          maybeAppendToTimeIndex(maxTimestampSoFar, offsetOfMaxTimestamp)
           lastIndexEntry = validBytes
         }
&lt;p&gt;         validBytes += batch.sizeInBytes()&lt;br/&gt;
@@ -309,7 +316,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
     log.truncateTo(validBytes)&lt;br/&gt;
     offsetIndex.trimToValidSize()&lt;br/&gt;
     // A normally closed segment always appends the biggest timestamp ever seen into log segment, we do this as well.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true)&lt;br/&gt;
+    maybeAppendToTimeIndex(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true)&lt;br/&gt;
     timeIndex.trimToValidSize()&lt;br/&gt;
     truncated&lt;br/&gt;
   }&lt;br/&gt;
@@ -372,11 +379,11 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;br/&gt;
    */&lt;br/&gt;
   @threadsafe&lt;br/&gt;
   def readNextOffset: Long = 
{
-    val ms = read(offsetIndex.lastOffset, None, log.sizeInBytes)
-    if (ms == null)
+    val fetchData = read(offsetIndex.lastOffset, None, log.sizeInBytes)
+    if (fetchData == null)
       baseOffset
     else
-      ms.records.batches.asScala.lastOption
+      fetchData.records.batches.asScala.lastOption
         .map(_.nextOffset)
         .getOrElse(baseOffset)
   }
&lt;p&gt;@@ -422,7 +429,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;The time index entry appended will be used to decide when to delete the segment.&lt;br/&gt;
    */&lt;br/&gt;
   def onBecomeInactiveSegment() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true)&lt;br/&gt;
+    maybeAppendToTimeIndex(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true)&lt;br/&gt;
     offsetIndex.trimToValidSize()&lt;br/&gt;
     timeIndex.trimToValidSize()&lt;br/&gt;
     log.trim()&lt;br/&gt;
@@ -486,7 +493,7 @@ class LogSegment private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; (val log: FileRecords,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Close this log segment&lt;br/&gt;
    */&lt;br/&gt;
   def close() 
{
-    CoreUtils.swallow(timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true), this)
+    CoreUtils.swallow(maybeAppendToTimeIndex(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = true), this)
     CoreUtils.swallow(offsetIndex.close(), this)
     CoreUtils.swallow(timeIndex.close(), this)
     CoreUtils.swallow(log.close(), this)
@@ -546,6 +553,25 @@ class LogSegment private[log] (val log: FileRecords,
     Files.setLastModifiedTime(offsetIndex.file.toPath, fileTime)
     Files.setLastModifiedTime(timeIndex.file.toPath, fileTime)
   }
&lt;p&gt;+&lt;br/&gt;
+  private def maybeAppendToTimeIndex(timestamp: Long, offset: Long, skipFullCheck: Boolean = false): Unit = &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    maybeHandleOffsetOverflowException {
+      timeIndex.maybeAppend(timestamp, offset, skipFullCheck)
+    }+  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+  private def appendToOffsetIndex(offset: Long, position: Int): Unit = &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    maybeHandleOffsetOverflowException {
+      offsetIndex.append(offset, position)
+    }+  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+  private def maybeHandleOffsetOverflowException&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(fun: =&amp;gt; T): T = &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    try fun+    catch {
+      case e: IndexOffsetOverflowException =&amp;gt; throw new LogSegmentOffsetOverflowException(e, this)
+    }+  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; object LogSegment {&lt;br/&gt;
@@ -566,6 +592,12 @@ object LogSegment &lt;/p&gt;
{
       time)
   }

&lt;p&gt;+  def deleteIfExists(dir: File, baseOffset: Long, fileSuffix: String = &quot;&quot;): Unit = &lt;/p&gt;
{
+    Log.deleteFileIfExists(Log.offsetIndexFile(dir, baseOffset, fileSuffix))
+    Log.deleteFileIfExists(Log.timeIndexFile(dir, baseOffset, fileSuffix))
+    Log.deleteFileIfExists(Log.transactionIndexFile(dir, baseOffset, fileSuffix))
+    Log.deleteFileIfExists(Log.logFile(dir, baseOffset, fileSuffix))
+  }
&lt;p&gt; }&lt;/p&gt;

&lt;p&gt; object LogFlushStats extends KafkaMetricsGroup {&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/OffsetIndex.scala b/core/src/main/scala/kafka/log/OffsetIndex.scala&lt;br/&gt;
index 523c88c7723..d185631d95a 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/OffsetIndex.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/OffsetIndex.scala&lt;br/&gt;
@@ -21,7 +21,7 @@ import java.io.File&lt;br/&gt;
 import java.nio.ByteBuffer&lt;/p&gt;

&lt;p&gt; import kafka.utils.CoreUtils.inLock&lt;br/&gt;
-import kafka.common.InvalidOffsetException&lt;br/&gt;
+import kafka.common.&lt;/p&gt;
{IndexOffsetOverflowException, InvalidOffsetException}

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;An index that maps offsets to physical file locations for a particular log segment. This index may be sparse:&lt;br/&gt;
@@ -134,13 +134,14 @@ class OffsetIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writabl&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Append an entry for the given offset/location pair to the index. This entry must have a larger offset than all subsequent entries.&lt;br/&gt;
+   * @throws IndexOffsetOverflowException if the offset causes index offset to overflow&lt;br/&gt;
    */&lt;br/&gt;
   def append(offset: Long, position: Int) {&lt;br/&gt;
     inLock(lock) {&lt;br/&gt;
       require(!isFull, &quot;Attempt to append to a full index (size = &quot; + _entries + &quot;).&quot;)&lt;br/&gt;
       if (_entries == 0 || offset &amp;gt; _lastOffset) {&lt;br/&gt;
         debug(&quot;Adding index entry %d =&amp;gt; %d to %s.&quot;.format(offset, position, file.getName))&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;mmap.putInt((offset - baseOffset).toInt)&lt;br/&gt;
+        mmap.putInt(relativeOffset(offset))&lt;br/&gt;
         mmap.putInt(position)&lt;br/&gt;
         _entries += 1&lt;br/&gt;
         _lastOffset = offset&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/TimeIndex.scala b/core/src/main/scala/kafka/log/TimeIndex.scala&lt;br/&gt;
index e505f36aec3..7fae1308512 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/TimeIndex.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/TimeIndex.scala&lt;br/&gt;
@@ -128,7 +128,7 @@ class TimeIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writable:&lt;br/&gt;
       if (timestamp &amp;gt; lastEntry.timestamp) {&lt;br/&gt;
         debug(&quot;Adding index entry %d =&amp;gt; %d to %s.&quot;.format(timestamp, offset, file.getName))&lt;br/&gt;
         mmap.putLong(timestamp)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;mmap.putInt((offset - baseOffset).toInt)&lt;br/&gt;
+        mmap.putInt(relativeOffset(offset))&lt;br/&gt;
         _entries += 1&lt;br/&gt;
         _lastEntry = TimestampOffset(timestamp, offset)&lt;br/&gt;
         require(_entries * entrySize == mmap.position(), _entries + &quot; entries but file position in index is &quot; + mmap.position() + &quot;.&quot;)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/tools/DumpLogSegments.scala b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
index 2aa7ad3495a..17fbd8f0f14 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
@@ -189,7 +189,7 @@ object DumpLogSegments {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for(i &amp;lt;- 0 until index.entries) {&lt;br/&gt;
       val entry = index.entry&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val slice = fileRecords.read(entry.position, maxMessageSize)&lt;br/&gt;
+      val slice = fileRecords.slice(entry.position, maxMessageSize)&lt;br/&gt;
       val firstRecord = slice.records.iterator.next()&lt;br/&gt;
       if (firstRecord.offset != entry.offset + index.baseOffset) {&lt;br/&gt;
         var misMatchesSeq = misMatchesForIndexFilesMap.getOrElse(file.getAbsolutePath, List&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Long)&amp;#93;&lt;/span&gt;())&lt;br/&gt;
@@ -227,7 +227,7 @@ object DumpLogSegments {&lt;br/&gt;
     for(i &amp;lt;- 0 until timeIndex.entries) {&lt;br/&gt;
       val entry = timeIndex.entry&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
       val position = index.lookup(entry.offset + timeIndex.baseOffset).position&lt;/li&gt;
	&lt;li&gt;val partialFileRecords = fileRecords.read(position, Int.MaxValue)&lt;br/&gt;
+      val partialFileRecords = fileRecords.slice(position, Int.MaxValue)&lt;br/&gt;
       val batches = partialFileRecords.batches.asScala&lt;br/&gt;
       var maxTimestamp = RecordBatch.NO_TIMESTAMP&lt;br/&gt;
       // We first find the message by offset then check if the timestamp is correct.&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
index 537c561b387..3207e15755e 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
@@ -71,11 +71,11 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     // append messages to the log until we have four segments&lt;br/&gt;
     while(log.numberOfSegments &amp;lt; 4)&lt;br/&gt;
       log.appendAsLeader(record(log.logEndOffset.toInt, log.logEndOffset.toInt), leaderEpoch = 0)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;val keysFound = keysInLog(log)&lt;br/&gt;
+    val keysFound = LogTest.keysInLog(log)&lt;br/&gt;
     assertEquals(0L until log.logEndOffset, keysFound)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // pretend we have the following keys&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val keys = immutable.ListSet(1, 3, 5, 7, 9)&lt;br/&gt;
+    val keys = immutable.ListSet(1L, 3L, 5L, 7L, 9L)&lt;br/&gt;
     val map = new FakeOffsetMap(Int.MaxValue)&lt;br/&gt;
     keys.foreach(k =&amp;gt; map.put(key(k), Long.MaxValue))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -84,8 +84,8 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     val stats = new CleanerStats()
     val expectedBytesRead = segments.map(_.size).sum
     cleaner.cleanSegments(log, segments, map, 0L, stats)
-    val shouldRemain = keysInLog(log).filter(!keys.contains(_))
-    assertEquals(shouldRemain, keysInLog(log))
+    val shouldRemain = LogTest.keysInLog(log).filter(!keys.contains(_))
+    assertEquals(shouldRemain, LogTest.keysInLog(log))
     assertEquals(expectedBytesRead, stats.bytesRead)
   }

&lt;p&gt;@@ -135,7 +135,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 0L, log.activeSegment.baseOffset))&lt;br/&gt;
     assertEquals(List(2, 5, 7), lastOffsetsPerBatchInLog(log))&lt;br/&gt;
     assertEquals(Map(pid1 -&amp;gt; 2, pid2 -&amp;gt; 2, pid3 -&amp;gt; 1), lastSequencesInLog(log))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(2, 3, 1, 4), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(2, 3, 1, 4), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(1, 3, 6, 7), offsetsInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // we have to reload the log to validate that the cleaner maintained sequence numbers correctly&lt;br/&gt;
@@ -167,7 +167,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 0L, log.activeSegment.baseOffset))&lt;br/&gt;
     assertEquals(Map(pid1 -&amp;gt; 2, pid2 -&amp;gt; 2, pid3 -&amp;gt; 1, pid4 -&amp;gt; 0), lastSequencesInLog(log))&lt;br/&gt;
     assertEquals(List(2, 5, 7, 8), lastOffsetsPerBatchInLog(log))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(3, 1, 4, 2), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(3, 1, 4, 2), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(3, 6, 7, 8), offsetsInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     reloadLog()&lt;br/&gt;
@@ -204,7 +204,7 @@ class LogCleanerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;     log.roll()&lt;br/&gt;
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 0L, log.activeSegment.baseOffset))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(3, 2), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(3, 2), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(3, 6, 7, 8, 9), offsetsInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // ensure the transaction index is still correct&lt;br/&gt;
@@ -244,7 +244,7 @@ class LogCleanerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;     // we have only cleaned the records in the first segment&lt;br/&gt;
     val dirtyOffset = cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 0L, log.activeSegment.baseOffset))._1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(2, 3, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(2, 3, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10), LogTest.keysInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.roll()&lt;/p&gt;

&lt;p&gt;@@ -254,7 +254,7 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
 
     // finally only the keys from pid3 should remain
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, dirtyOffset, log.activeSegment.baseOffset))
-    assertEquals(List(2, 3, 6, 7, 8, 9, 11, 12), keysInLog(log))
+    assertEquals(List(2, 3, 6, 7, 8, 9, 11, 12), LogTest.keysInLog(log))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -278,7 +278,7 @@ class LogCleanerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;     // cannot remove the marker in this pass because there are still valid records&lt;br/&gt;
     var dirtyOffset = cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(1, 3, 2), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(1, 3, 2), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(0, 2, 3, 4, 5), offsetsInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     appendProducer(Seq(1, 3))&lt;br/&gt;
@@ -287,17 +287,17 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
 
     // the first cleaning preserves the commit marker (at offset 3) since there were still records for the transaction
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1
-    assertEquals(List(2, 1, 3), keysInLog(log))
+    assertEquals(List(2, 1, 3), LogTest.keysInLog(log))
     assertEquals(List(3, 4, 5, 6, 7, 8), offsetsInLog(log))
 
     // delete horizon forced to 0 to verify marker is not removed early
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = 0L)._1
-    assertEquals(List(2, 1, 3), keysInLog(log))
+    assertEquals(List(2, 1, 3), LogTest.keysInLog(log))
     assertEquals(List(3, 4, 5, 6, 7, 8), offsetsInLog(log))
 
     // clean again with large delete horizon and verify the marker is removed
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1
-    assertEquals(List(2, 1, 3), keysInLog(log))
+    assertEquals(List(2, 1, 3), LogTest.keysInLog(log))
     assertEquals(List(4, 5, 6, 7, 8), offsetsInLog(log))
   }

&lt;p&gt;@@ -326,11 +326,11 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     log.roll()
 
     cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = Long.MaxValue)
-    assertEquals(List(2), keysInLog(log))
+    assertEquals(List(2), LogTest.keysInLog(log))
     assertEquals(List(1, 3, 4), offsetsInLog(log))
 
     cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = Long.MaxValue)
-    assertEquals(List(2), keysInLog(log))
+    assertEquals(List(2), LogTest.keysInLog(log))
     assertEquals(List(3, 4), offsetsInLog(log))
   }

&lt;p&gt;@@ -356,13 +356,13 @@ class LogCleanerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;     // first time through the records are removed&lt;br/&gt;
     var dirtyOffset = cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(2, 3), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(2, 3), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(2, 3, 4), offsetsInLog(log)) // commit marker is retained&lt;br/&gt;
     assertEquals(List(1, 2, 3, 4), lastOffsetsPerBatchInLog(log)) // empty batch is retained&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // the empty batch remains if cleaned again because it still holds the last sequence&lt;br/&gt;
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(2, 3), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(2, 3), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(2, 3, 4), offsetsInLog(log)) // commit marker is still retained&lt;br/&gt;
     assertEquals(List(1, 2, 3, 4), lastOffsetsPerBatchInLog(log)) // empty batch is retained&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -371,12 +371,12 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     log.roll()
 
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1
-    assertEquals(List(2, 3, 1), keysInLog(log))
+    assertEquals(List(2, 3, 1), LogTest.keysInLog(log))
     assertEquals(List(2, 3, 4, 5), offsetsInLog(log)) // commit marker is still retained
     assertEquals(List(2, 3, 4, 5), lastOffsetsPerBatchInLog(log)) // empty batch should be gone
 
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1
-    assertEquals(List(2, 3, 1), keysInLog(log))
+    assertEquals(List(2, 3, 1), LogTest.keysInLog(log))
     assertEquals(List(3, 4, 5), offsetsInLog(log)) // commit marker is gone
     assertEquals(List(3, 4, 5), lastOffsetsPerBatchInLog(log)) // empty batch is gone
   }
&lt;p&gt;@@ -402,12 +402,12 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
 
     // delete horizon set to 0 to verify marker is not removed early
     val dirtyOffset = cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = 0L)._1
-    assertEquals(List(3), keysInLog(log))
+    assertEquals(List(3), LogTest.keysInLog(log))
     assertEquals(List(3, 4, 5), offsetsInLog(log))
 
     // clean again with large delete horizon and verify the marker is removed
     cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)
-    assertEquals(List(3), keysInLog(log))
+    assertEquals(List(3), LogTest.keysInLog(log))
     assertEquals(List(4, 5), offsetsInLog(log))
   }

&lt;p&gt;@@ -440,14 +440,14 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     // first time through the records are removed&lt;br/&gt;
     var dirtyOffset = cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
     assertAbortedTransactionIndexed()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(2), offsetsInLog(log)) // abort marker is retained&lt;br/&gt;
     assertEquals(List(1, 2), lastOffsetsPerBatchInLog(log)) // empty batch is retained&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // the empty batch remains if cleaned again because it still holds the last sequence&lt;br/&gt;
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
     assertAbortedTransactionIndexed()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(2), offsetsInLog(log)) // abort marker is still retained&lt;br/&gt;
     assertEquals(List(1, 2), lastOffsetsPerBatchInLog(log)) // empty batch is retained&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -457,12 +457,12 @@ class LogCleanerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
     assertAbortedTransactionIndexed()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(1), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(1), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(2, 3), offsetsInLog(log)) // abort marker is not yet gone because we read the empty batch&lt;br/&gt;
     assertEquals(List(2, 3), lastOffsetsPerBatchInLog(log)) // but we do not preserve the empty batch&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(1), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(1), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(3), offsetsInLog(log)) // abort marker is gone&lt;br/&gt;
     assertEquals(List(3), lastOffsetsPerBatchInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -486,19 +486,19 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
 
     while(log.numberOfSegments &amp;lt; 2)
       log.appendAsLeader(record(log.logEndOffset.toInt, Array.fill(largeMessageSize)(0: Byte)), leaderEpoch = 0)
-    val keysFound = keysInLog(log)
+    val keysFound = LogTest.keysInLog(log)
     assertEquals(0L until log.logEndOffset, keysFound)
 
     // pretend we have the following keys
-    val keys = immutable.ListSet(1, 3, 5, 7, 9)
+    val keys = immutable.ListSet(1L, 3L, 5L, 7L, 9L)
     val map = new FakeOffsetMap(Int.MaxValue)
     keys.foreach(k =&amp;gt; map.put(key(k), Long.MaxValue))
 
     // clean the log
     val stats = new CleanerStats()
     cleaner.cleanSegments(log, Seq(log.logSegments.head), map, 0L, stats)
-    val shouldRemain = keysInLog(log).filter(!keys.contains(_))
-    assertEquals(shouldRemain, keysInLog(log))
+    val shouldRemain = LogTest.keysInLog(log).filter(!keys.contains(_))
+    assertEquals(shouldRemain, LogTest.keysInLog(log))
   }

&lt;p&gt;   /**&lt;br/&gt;
@@ -510,8 +510,8 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
 
     val cleaner = makeCleaner(Int.MaxValue, maxMessageSize=1024)
     cleaner.cleanSegments(log, Seq(log.logSegments.head), offsetMap, 0L, new CleanerStats)
-    val shouldRemain = keysInLog(log).filter(k =&amp;gt; !offsetMap.map.containsKey(k.toString))
-    assertEquals(shouldRemain, keysInLog(log))
+    val shouldRemain = LogTest.keysInLog(log).filter(k =&amp;gt; !offsetMap.map.containsKey(k.toString))
+    assertEquals(shouldRemain, LogTest.keysInLog(log))
   }

&lt;p&gt;   /**&lt;br/&gt;
@@ -558,7 +558,7 @@ class LogCleanerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;     while(log.numberOfSegments &amp;lt; 2)&lt;br/&gt;
       log.appendAsLeader(record(log.logEndOffset.toInt, Array.fill(largeMessageSize)(0: Byte)), leaderEpoch = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val keysFound = keysInLog(log)&lt;br/&gt;
+    val keysFound = LogTest.keysInLog(log)&lt;br/&gt;
     assertEquals(0L until log.logEndOffset, keysFound)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Decrease the log&apos;s max message size&lt;br/&gt;
@@ -595,7 +595,7 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
       log.appendAsLeader(record(log.logEndOffset.toInt, log.logEndOffset.toInt), leaderEpoch = 0)
 
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 0, log.activeSegment.baseOffset))
-    val keys = keysInLog(log).toSet
+    val keys = LogTest.keysInLog(log).toSet
     assertTrue(&quot;None of the keys we deleted should still exist.&quot;,
                (0 until leo.toInt by 2).forall(!keys.contains(_)))
   }
&lt;p&gt;@@ -647,7 +647,7 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 0L, log.activeSegment.baseOffset))
     assertEquals(List(1, 3, 4), lastOffsetsPerBatchInLog(log))
     assertEquals(Map(1L -&amp;gt; 0, 2L -&amp;gt; 1, 3L -&amp;gt; 0), lastSequencesInLog(log))
-    assertEquals(List(0, 1), keysInLog(log))
+    assertEquals(List(0, 1), LogTest.keysInLog(log))
     assertEquals(List(3, 4), offsetsInLog(log))
   }

&lt;p&gt;@@ -670,7 +670,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 0L, log.activeSegment.baseOffset))&lt;br/&gt;
     assertEquals(List(2, 3), lastOffsetsPerBatchInLog(log))&lt;br/&gt;
     assertEquals(Map(producerId -&amp;gt; 2), lastSequencesInLog(log))&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(List(), keysInLog(log))&lt;br/&gt;
+    assertEquals(List(), LogTest.keysInLog(log))&lt;br/&gt;
     assertEquals(List(3), offsetsInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Append a new entry from the producer and verify that the empty batch is cleaned up&lt;br/&gt;
@@ -680,7 +680,7 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
 
     assertEquals(List(3, 5), lastOffsetsPerBatchInLog(log))
     assertEquals(Map(producerId -&amp;gt; 4), lastSequencesInLog(log))
-    assertEquals(List(1, 5), keysInLog(log))
+    assertEquals(List(1, 5), LogTest.keysInLog(log))
     assertEquals(List(3, 4, 5), offsetsInLog(log))
   }

&lt;p&gt;@@ -703,16 +703,16 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
 
     // clean the log with only one message removed
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 2, log.activeSegment.baseOffset))
-    assertEquals(List(1,0,1,0), keysInLog(log))
+    assertEquals(List(1,0,1,0), LogTest.keysInLog(log))
     assertEquals(List(1,2,3,4), offsetsInLog(log))
 
     // continue to make progress, even though we can only clean one message at a time
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 3, log.activeSegment.baseOffset))
-    assertEquals(List(0,1,0), keysInLog(log))
+    assertEquals(List(0,1,0), LogTest.keysInLog(log))
     assertEquals(List(2,3,4), offsetsInLog(log))
 
     cleaner.clean(LogToClean(new TopicPartition(&quot;test&quot;, 0), log, 4, log.activeSegment.baseOffset))
-    assertEquals(List(1,0), keysInLog(log))
+    assertEquals(List(1,0), LogTest.keysInLog(log))
     assertEquals(List(3,4), offsetsInLog(log))
   }

&lt;p&gt;@@ -835,14 +835,6 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     assertEquals(&quot;Cleaner should have seen %d invalid messages.&quot;, numInvalidMessages, stats.invalidMessagesRead)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* extract all the keys from a log */&lt;/li&gt;
	&lt;li&gt;def keysInLog(log: Log): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = 
{
-    for (segment &amp;lt;- log.logSegments;
-         batch &amp;lt;- segment.log.batches.asScala if !batch.isControlBatch;
-         record &amp;lt;- batch.asScala if record.hasValue &amp;amp;&amp;amp; record.hasKey)
-      yield TestUtils.readString(record.key).toInt
-  }
&lt;p&gt;-&lt;br/&gt;
   def lastOffsetsPerBatchInLog(log: Log): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     for (segment &amp;lt;- log.logSegments; batch &amp;lt;- segment.log.batches.asScala)&lt;br/&gt;
       yield batch.lastOffset&lt;br/&gt;
@@ -880,7 +872,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     while(log.numberOfSegments &amp;lt; 4)&lt;br/&gt;
       log.appendAsLeader(record(log.logEndOffset.toInt, log.logEndOffset.toInt), leaderEpoch = 0)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val keys = keysInLog(log)&lt;br/&gt;
+    val keys = LogTest.keysInLog(log)&lt;br/&gt;
     val map = new FakeOffsetMap(Int.MaxValue)&lt;br/&gt;
     keys.foreach(k =&amp;gt; map.put(key(k), Long.MaxValue))&lt;br/&gt;
     intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;LogCleaningAbortedException&amp;#93;&lt;/span&gt; {&lt;br/&gt;
@@ -1065,6 +1057,43 @@ class LogCleanerTest extends JUnitSuite 
{
     checkRange(map, segments(3).baseOffset.toInt, log.logEndOffset.toInt)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def testSegmentWithOffsetOverflow(): Unit = {&lt;br/&gt;
+    val cleaner = makeCleaner(Int.MaxValue)&lt;br/&gt;
+    val logProps = new Properties()&lt;br/&gt;
+    logProps.put(LogConfig.IndexIntervalBytesProp, 1: java.lang.Integer)&lt;br/&gt;
+    logProps.put(LogConfig.FileDeleteDelayMsProp, 1000: java.lang.Integer)&lt;br/&gt;
+    val config = LogConfig.fromProps(logConfig.originals, logProps)&lt;br/&gt;
+&lt;br/&gt;
+    val time = new MockTime()&lt;br/&gt;
+    val (log, segmentWithOverflow, _) = LogTest.createLogWithOffsetOverflow(dir, new BrokerTopicStats(), Some(config), time.scheduler, time)&lt;br/&gt;
+    val numSegmentsInitial = log.logSegments.size&lt;br/&gt;
+    val allKeys = LogTest.keysInLog(log).toList&lt;br/&gt;
+    val expectedKeysAfterCleaning = mutable.MutableList&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+&lt;br/&gt;
+    // pretend we want to clean every alternate key&lt;br/&gt;
+    val offsetMap = new FakeOffsetMap(Int.MaxValue)&lt;br/&gt;
+    for (k &amp;lt;- 1 until allKeys.size by 2) &lt;/p&gt;
{
+      expectedKeysAfterCleaning += allKeys(k - 1)
+      offsetMap.put(key(allKeys(k)), Long.MaxValue)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Try to clean segment with offset overflow. This will trigger log split and the cleaning itself must abort.&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;LogCleaningAbortedException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
+      cleaner.cleanSegments(log, List(segmentWithOverflow), offsetMap, 0L, new CleanerStats())
+    }
&lt;p&gt;+    assertEquals(numSegmentsInitial + 1, log.logSegments.size)&lt;br/&gt;
+    assertEquals(allKeys, LogTest.keysInLog(log))&lt;br/&gt;
+    assertFalse(LogTest.hasOffsetOverflow(log))&lt;br/&gt;
+&lt;br/&gt;
+    // Clean each segment now that split is complete.&lt;br/&gt;
+    for (segmentToClean &amp;lt;- log.logSegments)&lt;br/&gt;
+      cleaner.cleanSegments(log, List(segmentToClean), offsetMap, 0L, new CleanerStats())&lt;br/&gt;
+    assertEquals(expectedKeysAfterCleaning, LogTest.keysInLog(log))&lt;br/&gt;
+    assertFalse(LogTest.hasOffsetOverflow(log))&lt;br/&gt;
+    log.close()&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Tests recovery if broker crashes at the following stages during the cleaning sequence&lt;/li&gt;
	&lt;li&gt;&amp;lt;ol&amp;gt;&lt;br/&gt;
@@ -1084,28 +1113,14 @@ class LogCleanerTest extends JUnitSuite {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val config = LogConfig.fromProps(logConfig.originals, logProps)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def recoverAndCheck(config: LogConfig, expectedKeys: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;): Log = {&lt;/li&gt;
	&lt;li&gt;// Recover log file and check that after recovery, keys are as expected&lt;/li&gt;
	&lt;li&gt;// and all temporary files have been deleted&lt;/li&gt;
	&lt;li&gt;val recoveredLog = makeLog(config = config)&lt;/li&gt;
	&lt;li&gt;time.sleep(config.fileDeleteDelayMs + 1)&lt;/li&gt;
	&lt;li&gt;for (file &amp;lt;- dir.listFiles) 
{
-        assertFalse(&quot;Unexpected .deleted file after recovery&quot;, file.getName.endsWith(Log.DeletedFileSuffix))
-        assertFalse(&quot;Unexpected .cleaned file after recovery&quot;, file.getName.endsWith(Log.CleanedFileSuffix))
-        assertFalse(&quot;Unexpected .swap file after recovery&quot;, file.getName.endsWith(Log.SwapFileSuffix))
-      }&lt;/li&gt;
	&lt;li&gt;assertEquals(expectedKeys, keysInLog(recoveredLog))&lt;/li&gt;
	&lt;li&gt;recoveredLog&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// create a log and append some messages&lt;br/&gt;
+   // create a log and append some messages&lt;br/&gt;
     var log = makeLog(config = config)&lt;br/&gt;
     var messageCount = 0&lt;br/&gt;
     while (log.numberOfSegments &amp;lt; 10) 
{
       log.appendAsLeader(record(log.logEndOffset.toInt, log.logEndOffset.toInt), leaderEpoch = 0)
       messageCount += 1
     }&lt;/li&gt;
	&lt;li&gt;val allKeys = keysInLog(log)&lt;br/&gt;
+    val allKeys = LogTest.keysInLog(log)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // pretend we have odd-numbered keys&lt;br/&gt;
     val offsetMap = new FakeOffsetMap(Int.MaxValue)&lt;br/&gt;
@@ -1116,7 +1131,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     cleaner.cleanSegments(log, log.logSegments.take(9).toSeq, offsetMap, 0L, new CleanerStats())&lt;br/&gt;
     // clear scheduler so that async deletes don&apos;t run&lt;br/&gt;
     time.scheduler.clear()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var cleanedKeys = keysInLog(log)&lt;br/&gt;
+    var cleanedKeys = LogTest.keysInLog(log)&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // 1) Simulate recovery just after .cleaned file is created, before rename to .swap&lt;br/&gt;
@@ -1131,7 +1146,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     cleaner.cleanSegments(log, log.logSegments.take(9).toSeq, offsetMap, 0L, new CleanerStats())&lt;br/&gt;
     // clear scheduler so that async deletes don&apos;t run&lt;br/&gt;
     time.scheduler.clear()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cleanedKeys = keysInLog(log)&lt;br/&gt;
+    cleanedKeys = LogTest.keysInLog(log)&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // 2) Simulate recovery just after swap file is created, before old segment files are&lt;br/&gt;
@@ -1152,7 +1167,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     cleaner.cleanSegments(log, log.logSegments.take(9).toSeq, offsetMap, 0L, new CleanerStats())&lt;br/&gt;
     // clear scheduler so that async deletes don&apos;t run&lt;br/&gt;
     time.scheduler.clear()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cleanedKeys = keysInLog(log)&lt;br/&gt;
+    cleanedKeys = LogTest.keysInLog(log)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // 3) Simulate recovery after swap file is created and old segments files are renamed&lt;br/&gt;
     //    to .deleted. Clean operation is resumed during recovery.&lt;br/&gt;
@@ -1169,7 +1184,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     cleaner.cleanSegments(log, log.logSegments.take(9).toSeq, offsetMap, 0L, new CleanerStats())&lt;br/&gt;
     // clear scheduler so that async deletes don&apos;t run&lt;br/&gt;
     time.scheduler.clear()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cleanedKeys = keysInLog(log)&lt;br/&gt;
+    cleanedKeys = LogTest.keysInLog(log)&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // 4) Simulate recovery after swap is complete, but async deletion&lt;br/&gt;
@@ -1375,7 +1390,7 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     for ((key, value) &amp;lt;- seq) yield log.appendAsLeader(record(key, value), leaderEpoch = 0).firstOffset.get
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def key(id: Int) = ByteBuffer.wrap(id.toString.getBytes)&lt;br/&gt;
+  private def key(id: Long) = ByteBuffer.wrap(id.toString.getBytes)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def record(key: Int, value: Int,&lt;br/&gt;
              producerId: Long = RecordBatch.NO_PRODUCER_ID,&lt;br/&gt;
@@ -1429,6 +1444,9 @@ class LogCleanerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;   private def tombstoneRecord(key: Int): MemoryRecords = record(key, null)&lt;/p&gt;

&lt;p&gt;+  private def recoverAndCheck(config: LogConfig, expectedKeys: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;): Log = &lt;/p&gt;
{
+    LogTest.recoverAndCheck(dir, config, expectedKeys, new BrokerTopicStats())
+  }
&lt;p&gt; }&lt;/p&gt;

&lt;p&gt; class FakeOffsetMap(val slots: Int) extends OffsetMap {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index 28fcdfcfac1..7dcb90a84ba 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -19,18 +19,16 @@ package kafka.log&lt;/p&gt;

&lt;p&gt; import java.io._&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
-import java.nio.file.Files&lt;br/&gt;
+import java.nio.file.&lt;/p&gt;
{Files, Paths}
&lt;p&gt; import java.util.Properties&lt;/p&gt;

&lt;p&gt;-import org.apache.kafka.common.errors._&lt;br/&gt;
 import kafka.common.KafkaException&lt;br/&gt;
 import kafka.log.Log.DeleteDirSuffix&lt;br/&gt;
-import org.junit.Assert._&lt;br/&gt;
-import org.junit.&lt;/p&gt;
{After, Before, Test}&lt;br/&gt;
-import kafka.utils._&lt;br/&gt;
-import kafka.server.{BrokerTopicStats, FetchDataInfo, KafkaConfig, LogDirFailureChannel}&lt;br/&gt;
 import kafka.server.epoch.{EpochEntry, LeaderEpochCache, LeaderEpochFileCache}&lt;br/&gt;
+import kafka.server.{BrokerTopicStats, FetchDataInfo, KafkaConfig, LogDirFailureChannel}&lt;br/&gt;
+import kafka.utils._&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.errors._&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter.BatchRetention&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
@@ -38,17 +36,19 @@ import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
 import org.apache.kafka.common.requests.IsolationLevel&lt;br/&gt;
 import org.apache.kafka.common.utils.{Time, Utils}&lt;br/&gt;
 import org.easymock.EasyMock&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
+import org.junit.{After, Before, Test}

&lt;p&gt;+import scala.collection.Iterable&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.&lt;/p&gt;
{ArrayBuffer, ListBuffer}

&lt;p&gt; class LogTest {&lt;br/&gt;
-&lt;br/&gt;
+  var config: KafkaConfig = null&lt;br/&gt;
+  val brokerTopicStats = new BrokerTopicStats&lt;br/&gt;
   val tmpDir = TestUtils.tempDir()&lt;br/&gt;
   val logDir = TestUtils.randomPartitionLogDir(tmpDir)&lt;br/&gt;
   val mockTime = new MockTime()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var config: KafkaConfig = null&lt;/li&gt;
	&lt;li&gt;val brokerTopicStats = new BrokerTopicStats&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Before&lt;br/&gt;
   def setUp() {&lt;br/&gt;
@@ -93,10 +93,10 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testTimeBasedLogRoll() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentMs = 1 * 60 * 60L)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentMs = 1 * 60 * 60L)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // create a log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig, maxProducerIdExpirationMs = 24 * 60)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, maxProducerIdExpirationMs = 24 * 60, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(&quot;Log begins with a single empty segment.&quot;, 1, log.numberOfSegments)&lt;br/&gt;
     // Test the segment rolling behavior when messages do not have a timestamp.&lt;br/&gt;
     mockTime.sleep(log.config.segmentMs + 1)&lt;br/&gt;
@@ -141,7 +141,7 @@ class LogTest {&lt;br/&gt;
   @Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;OutOfOrderSequenceException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   def testNonSequentialAppend(): Unit = {&lt;br/&gt;
     // create a log&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats)&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val epoch: Short = 0&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -154,7 +154,7 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testTruncateToEmptySegment(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Force a segment roll by using a large offset. The first segment will be empty&lt;br/&gt;
     val records = TestUtils.records(List(new SimpleRecord(mockTime.milliseconds, &quot;key&quot;.getBytes, &quot;value&quot;.getBytes)),&lt;br/&gt;
@@ -178,8 +178,8 @@ class LogTest {&lt;br/&gt;
   def testInitializationOfProducerSnapshotsUpgradePath(): Unit = {&lt;br/&gt;
     // simulate the upgrade path by creating a new log with several segments, deleting the&lt;br/&gt;
     // snapshot files, and then reloading the log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 64 * 10)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 64 * 10)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats)&lt;br/&gt;
     assertEquals(None, log.oldestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for (i &amp;lt;- 0 to 100) {&lt;br/&gt;
@@ -194,7 +194,7 @@ class LogTest {&lt;br/&gt;
     deleteProducerSnapshotFiles()&lt;/p&gt;

&lt;p&gt;     // Reload after clean shutdown&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig, recoveryPoint = logEndOffset)&lt;br/&gt;
+    log = createLog(logDir, logConfig, brokerTopicStats, recoveryPoint = logEndOffset)&lt;br/&gt;
     var expectedSnapshotOffsets = log.logSegments.map(_.baseOffset).takeRight(2).toVector :+ log.logEndOffset&lt;br/&gt;
     assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)&lt;br/&gt;
     log.close()&lt;br/&gt;
@@ -203,7 +203,7 @@ class LogTest {&lt;br/&gt;
     deleteProducerSnapshotFiles()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Reload after unclean shutdown with recoveryPoint set to log end offset&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig, recoveryPoint = logEndOffset)&lt;br/&gt;
+    log = createLog(logDir, logConfig, brokerTopicStats, recoveryPoint = logEndOffset)&lt;br/&gt;
     // Note that we don&apos;t maintain the guarantee of having a snapshot for the 2 most recent segments in this case&lt;br/&gt;
     expectedSnapshotOffsets = Vector(log.logSegments.last.baseOffset, log.logEndOffset)&lt;br/&gt;
     assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)&lt;br/&gt;
@@ -212,7 +212,7 @@ class LogTest {&lt;br/&gt;
     deleteProducerSnapshotFiles()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Reload after unclean shutdown with recoveryPoint set to 0&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig, recoveryPoint = 0L)&lt;br/&gt;
+    log = createLog(logDir, logConfig, brokerTopicStats, recoveryPoint = 0L)&lt;br/&gt;
     // Is this working as intended?&lt;br/&gt;
     expectedSnapshotOffsets = log.logSegments.map(_.baseOffset).tail.toVector :+ log.logEndOffset&lt;br/&gt;
     assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)&lt;br/&gt;
@@ -221,8 +221,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testProducerSnapshotsRecoveryAfterUncleanShutdown(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 64 * 10)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 64 * 10)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats)&lt;br/&gt;
     assertEquals(None, log.oldestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for (i &amp;lt;- 0 to 100) {&lt;br/&gt;
@@ -320,8 +320,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testProducerIdMapOffsetUpdatedForNonIdempotentData() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val records = TestUtils.records(List(new SimpleRecord(mockTime.milliseconds, &quot;key&quot;.getBytes, &quot;value&quot;.getBytes)))&lt;br/&gt;
     log.appendAsLeader(records, leaderEpoch = 0)&lt;br/&gt;
     log.takeProducerSnapshot()&lt;br/&gt;
@@ -511,8 +511,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testRebuildProducerIdMapWithCompactedData() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
     val seq = 0&lt;br/&gt;
@@ -554,8 +554,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testRebuildProducerStateWithEmptyCompactedBatch() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
     val seq = 0&lt;br/&gt;
@@ -595,8 +595,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testUpdateProducerIdMapWithCompactedData() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
     val seq = 0&lt;br/&gt;
@@ -628,8 +628,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testProducerIdMapTruncateTo() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.appendAsLeader(TestUtils.records(List(new SimpleRecord(&quot;a&quot;.getBytes))), leaderEpoch = 0)&lt;br/&gt;
     log.appendAsLeader(TestUtils.records(List(new SimpleRecord(&quot;b&quot;.getBytes))), leaderEpoch = 0)&lt;br/&gt;
     log.takeProducerSnapshot()&lt;br/&gt;
@@ -649,8 +649,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testProducerIdMapTruncateToWithNoSnapshots() {&lt;br/&gt;
     // This ensures that the upgrade optimization path cannot be hit after initial loading&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val epoch = 0.toShort&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -673,8 +673,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testLoadProducersAfterDeleteRecordsMidSegment(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid1 = 1L&lt;br/&gt;
     val pid2 = 2L&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
@@ -694,7 +694,7 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.close()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L)&lt;br/&gt;
+    val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(1, reloadedLog.activeProducersWithLastSequence.size)&lt;br/&gt;
     val reloadedLastSeqOpt = log.activeProducersWithLastSequence.get(pid2)&lt;br/&gt;
     assertEquals(retainedLastSeqOpt, reloadedLastSeqOpt)&lt;br/&gt;
@@ -702,8 +702,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testLoadProducersAfterDeleteRecordsOnSegment(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid1 = 1L&lt;br/&gt;
     val pid2 = 2L&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
@@ -729,7 +729,7 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.close()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L)&lt;br/&gt;
+    val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(1, reloadedLog.activeProducersWithLastSequence.size)&lt;br/&gt;
     val reloadedEntryOpt = log.activeProducersWithLastSequence.get(pid2)&lt;br/&gt;
     assertEquals(retainedLastSeqOpt, reloadedEntryOpt)&lt;br/&gt;
@@ -738,8 +738,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testProducerIdMapTruncateFullyAndStartAt() {&lt;br/&gt;
     val records = TestUtils.singletonRecords(&quot;foo&quot;.getBytes)&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = records.sizeInBytes, retentionBytes = records.sizeInBytes * 2)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = records.sizeInBytes, retentionBytes = records.sizeInBytes * 2)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.appendAsLeader(records, leaderEpoch = 0)&lt;br/&gt;
     log.takeProducerSnapshot()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -761,8 +761,8 @@ class LogTest {&lt;br/&gt;
   def testProducerIdExpirationOnSegmentDeletion() {&lt;br/&gt;
     val pid1 = 1L&lt;br/&gt;
     val records = TestUtils.records(Seq(new SimpleRecord(&quot;foo&quot;.getBytes)), producerId = pid1, producerEpoch = 0, sequence = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = records.sizeInBytes, retentionBytes = records.sizeInBytes * 2)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = records.sizeInBytes, retentionBytes = records.sizeInBytes * 2)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.appendAsLeader(records, leaderEpoch = 0)&lt;br/&gt;
     log.takeProducerSnapshot()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -785,8 +785,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testTakeSnapshotOnRollAndDeleteSnapshotOnRecoveryPointCheckpoint() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.appendAsLeader(TestUtils.singletonRecords(&quot;a&quot;.getBytes), leaderEpoch = 0)&lt;br/&gt;
     log.roll(1L)&lt;br/&gt;
     assertEquals(Some(1L), log.latestProducerSnapshotOffset)&lt;br/&gt;
@@ -818,8 +818,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testProducerSnapshotAfterSegmentRollOnAppend(): Unit = {&lt;br/&gt;
     val producerId = 1L&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.appendAsLeader(TestUtils.records(Seq(new SimpleRecord(mockTime.milliseconds(), new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;(512))),&lt;br/&gt;
       producerId = producerId, producerEpoch = 0, sequence = 0),&lt;br/&gt;
@@ -850,8 +850,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testRebuildTransactionalState(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val pid = 137L&lt;br/&gt;
     val epoch = 5.toShort&lt;br/&gt;
@@ -872,7 +872,7 @@ class LogTest &lt;/p&gt;
{
 
     log.close()
 
-    val reopenedLog = createLog(logDir, logConfig)
+    val reopenedLog = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)
     reopenedLog.onHighWatermarkIncremented(commitAppendInfo.lastOffset + 1)
     assertEquals(None, reopenedLog.firstUnstableOffset)
   }
&lt;p&gt;@@ -893,9 +893,9 @@ class LogTest {&lt;br/&gt;
     val producerIdExpirationCheckIntervalMs = 100&lt;/p&gt;

&lt;p&gt;     val pid = 23L&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
     val log = createLog(logDir, logConfig, maxProducerIdExpirationMs = maxProducerIdExpirationMs,&lt;/li&gt;
	&lt;li&gt;producerIdExpirationCheckIntervalMs = producerIdExpirationCheckIntervalMs)&lt;br/&gt;
+      producerIdExpirationCheckIntervalMs = producerIdExpirationCheckIntervalMs, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val records = Seq(new SimpleRecord(mockTime.milliseconds(), &quot;foo&quot;.getBytes))&lt;br/&gt;
     log.appendAsLeader(TestUtils.records(records, producerId = pid, producerEpoch = 0, sequence = 0), leaderEpoch = 0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -911,7 +911,7 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testDuplicateAppends(): Unit = {&lt;br/&gt;
     // create a log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val epoch: Short = 0&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -985,7 +985,7 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testMultipleProducerIdsPerMemoryRecord() : Unit = {&lt;br/&gt;
     // create a log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val epoch: Short = 0&lt;br/&gt;
     val buffer = ByteBuffer.allocate(512)&lt;br/&gt;
@@ -1030,8 +1030,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testDuplicateAppendToFollower() : Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val epoch: Short = 0&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val baseSequence = 0&lt;br/&gt;
@@ -1051,8 +1051,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testMultipleProducersWithDuplicatesInSingleAppend() : Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val pid1 = 1L&lt;br/&gt;
     val pid2 = 2L&lt;br/&gt;
@@ -1104,7 +1104,7 @@ class LogTest {&lt;br/&gt;
   @Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerFencedException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   def testOldProducerEpoch(): Unit = {&lt;br/&gt;
     // create a log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val newEpoch: Short = 1&lt;br/&gt;
     val oldEpoch: Short = 0&lt;br/&gt;
@@ -1125,8 +1125,8 @@ class LogTest {&lt;br/&gt;
     var set = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;br/&gt;
     val maxJitter = 20 * 60L&lt;br/&gt;
     // create a log&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentMs = 1 * 60 * 60L, segmentJitterMs = maxJitter)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentMs = 1 * 60 * 60L, segmentJitterMs = maxJitter)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(&quot;Log begins with a single empty segment.&quot;, 1, log.numberOfSegments)&lt;br/&gt;
     log.appendAsLeader(set, leaderEpoch = 0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1150,8 +1150,8 @@ class LogTest {&lt;br/&gt;
     val msgPerSeg = 10&lt;br/&gt;
     val segmentSize = msgPerSeg * (setSize - 1) // each segment will be 10 messages&lt;br/&gt;
     // create a log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = segmentSize)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = segmentSize)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(&quot;There should be exactly 1 segment.&quot;, 1, log.numberOfSegments)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // segments expire in size&lt;br/&gt;
@@ -1166,7 +1166,7 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testLoadEmptyLog() &lt;/p&gt;
{
     createEmptyLogs(logDir, 0)
-    val log = createLog(logDir, LogConfig())
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)
     log.appendAsLeader(TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds), leaderEpoch = 0)
   }

&lt;p&gt;@@ -1175,8 +1175,8 @@ class LogTest {&lt;br/&gt;
    */&lt;br/&gt;
   @Test&lt;br/&gt;
   def testAppendAndReadWithSequentialOffsets() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 71)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 71)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val values = (0 until 100 by 2).map(id =&amp;gt; id.toString.getBytes).toArray&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for(value &amp;lt;- values)&lt;br/&gt;
@@ -1199,8 +1199,8 @@ class LogTest {&lt;br/&gt;
    */&lt;br/&gt;
   @Test&lt;br/&gt;
   def testAppendAndReadWithNonSequentialOffsets() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 72)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 72)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val messageIds = ((0 until 50) ++ (50 until 200 by 7)).toArray&lt;br/&gt;
     val records = messageIds.map(id =&amp;gt; new SimpleRecord(id.toString.getBytes))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1223,8 +1223,8 @@ class LogTest {&lt;br/&gt;
    */&lt;br/&gt;
   @Test&lt;br/&gt;
   def testReadAtLogGap() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 300)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 300)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // keep appending until we have two segments with only a single message in the second segment&lt;br/&gt;
     while(log.numberOfSegments == 1)&lt;br/&gt;
@@ -1239,16 +1239,16 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaStorageException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   def testLogRollAfterLogHandlerClosed() &lt;/p&gt;
{
-    val logConfig = createLogConfig()
-    val log = createLog(logDir,  logConfig)
+    val logConfig = LogTest.createLogConfig()
+    val log = createLog(logDir,  logConfig, brokerTopicStats = brokerTopicStats)
     log.closeHandlers()
     log.roll(1)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def testReadWithMinMessage() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 72)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir,  logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 72)&lt;br/&gt;
+    val log = createLog(logDir,  logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val messageIds = ((0 until 50) ++ (50 until 200 by 7)).toArray&lt;br/&gt;
     val records = messageIds.map(id =&amp;gt; new SimpleRecord(id.toString.getBytes))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1274,8 +1274,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testReadWithTooSmallMaxLength() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 72)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir,  logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 72)&lt;br/&gt;
+    val log = createLog(logDir,  logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val messageIds = ((0 until 50) ++ (50 until 200 by 7)).toArray&lt;br/&gt;
     val records = messageIds.map(id =&amp;gt; new SimpleRecord(id.toString.getBytes))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1308,8 +1308,8 @@ class LogTest {&lt;br/&gt;
   def testReadOutOfRange() {&lt;br/&gt;
     createEmptyLogs(logDir, 1024)&lt;br/&gt;
     // set up replica log starting with offset 1024 and with one message (at offset 1024)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.appendAsLeader(TestUtils.singletonRecords(value = &quot;42&quot;.getBytes), leaderEpoch = 0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(&quot;Reading at the log end offset should produce 0 byte read.&quot;, 0,&lt;br/&gt;
@@ -1340,8 +1340,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testLogRolls() {&lt;br/&gt;
     /* create a multipart log with 100 messages */&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 100)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 100)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val numMessages = 100&lt;br/&gt;
     val messageSets = (0 until numMessages).map(i =&amp;gt; TestUtils.singletonRecords(value = i.toString.getBytes,&lt;br/&gt;
                                                                                 timestamp = mockTime.milliseconds))&lt;br/&gt;
@@ -1378,8 +1378,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testCompressedMessages() {&lt;br/&gt;
     /* this log should roll after every messageset */&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 110)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 110)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /* append 2 compressed message sets, each with two messages giving offsets 0, 1, 2, 3 */&lt;br/&gt;
     log.appendAsLeader(MemoryRecords.withRecords(CompressionType.GZIP, new SimpleRecord(&quot;hello&quot;.getBytes), new SimpleRecord(&quot;there&quot;.getBytes)), leaderEpoch = 0)&lt;br/&gt;
@@ -1402,8 +1402,8 @@ class LogTest {&lt;br/&gt;
     for(messagesToAppend &amp;lt;- List(0, 1, 25)) {&lt;br/&gt;
       logDir.mkdirs()&lt;br/&gt;
       // first test a log segment starting at 0&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 100, retentionMs = 0)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+      val logConfig = LogTest.createLogConfig(segmentBytes = 100, retentionMs = 0)&lt;br/&gt;
+      val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
       for(i &amp;lt;- 0 until messagesToAppend)&lt;br/&gt;
         log.appendAsLeader(TestUtils.singletonRecords(value = i.toString.getBytes, timestamp = mockTime.milliseconds - 10), leaderEpoch = 0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1436,8 +1436,8 @@ class LogTest {&lt;br/&gt;
     val messageSet = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(&quot;You&quot;.getBytes), new SimpleRecord(&quot;bethe&quot;.getBytes))&lt;br/&gt;
     // append messages to log&lt;br/&gt;
     val configSegmentSize = messageSet.sizeInBytes - 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = configSegmentSize)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = configSegmentSize)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     try {&lt;br/&gt;
       log.appendAsLeader(messageSet, leaderEpoch = 0)&lt;br/&gt;
@@ -1461,8 +1461,8 @@ class LogTest {&lt;br/&gt;
     val messageSetWithKeyedMessage = MemoryRecords.withRecords(CompressionType.NONE, keyedMessage)&lt;br/&gt;
     val messageSetWithKeyedMessages = MemoryRecords.withRecords(CompressionType.NONE, keyedMessage, anotherKeyedMessage)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(cleanupPolicy = LogConfig.Compact)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(cleanupPolicy = LogConfig.Compact)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     try {&lt;br/&gt;
       log.appendAsLeader(messageSetWithUnkeyedMessage, leaderEpoch = 0)&lt;br/&gt;
@@ -1502,8 +1502,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;     // append messages to log&lt;br/&gt;
     val maxMessageSize = second.sizeInBytes - 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(maxMessageBytes = maxMessageSize)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(maxMessageBytes = maxMessageSize)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // should be able to append the small message&lt;br/&gt;
     log.appendAsLeader(first, leaderEpoch = 0)&lt;br/&gt;
@@ -1524,8 +1524,8 @@ class LogTest {&lt;br/&gt;
     val messageSize = 100&lt;br/&gt;
     val segmentSize = 7 * messageSize&lt;br/&gt;
     val indexInterval = 3 * messageSize&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = segmentSize, indexIntervalBytes = indexInterval, segmentIndexBytes = 4096)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = segmentSize, indexIntervalBytes = indexInterval, segmentIndexBytes = 4096)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     for(i &amp;lt;- 0 until numMessages)&lt;br/&gt;
       log.appendAsLeader(TestUtils.singletonRecords(value = TestUtils.randomBytes(messageSize),&lt;br/&gt;
         timestamp = mockTime.milliseconds + i * 10), leaderEpoch = 0)&lt;br/&gt;
@@ -1552,12 +1552,12 @@ class LogTest 
{
       assertEquals(&quot;Should have same number of time index entries as before.&quot;, numTimeIndexEntries, log.activeSegment.timeIndex.entries)
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig, recoveryPoint = lastOffset)&lt;br/&gt;
+    log = createLog(logDir, logConfig, recoveryPoint = lastOffset, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     verifyRecoveredLog(log, lastOffset)&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // test recovery case&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig)&lt;br/&gt;
+    log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     verifyRecoveredLog(log, lastOffset)&lt;br/&gt;
     log.close()&lt;br/&gt;
   }&lt;br/&gt;
@@ -1568,8 +1568,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testBuildTimeIndexWhenNotAssigningOffsets() {&lt;br/&gt;
     val numMessages = 100&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 10000, indexIntervalBytes = 1)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 10000, indexIntervalBytes = 1)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val messages = (0 until numMessages).map { i =&amp;gt;&lt;br/&gt;
       MemoryRecords.withRecords(100 + i, CompressionType.NONE, 0, new SimpleRecord(mockTime.milliseconds + i, i.toString.getBytes()))&lt;br/&gt;
@@ -1588,8 +1588,8 @@ class LogTest {&lt;br/&gt;
   def testIndexRebuild() {&lt;br/&gt;
     // publish the messages and close the log&lt;br/&gt;
     val numMessages = 200&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 200, indexIntervalBytes = 1)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 200, indexIntervalBytes = 1)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     for(i &amp;lt;- 0 until numMessages)&lt;br/&gt;
       log.appendAsLeader(TestUtils.singletonRecords(value = TestUtils.randomBytes(10), timestamp = mockTime.milliseconds + i * 10), leaderEpoch = 0)&lt;br/&gt;
     val indexFiles = log.logSegments.map(_.offsetIndex.file)&lt;br/&gt;
@@ -1601,7 +1601,7 @@ class LogTest {&lt;br/&gt;
     timeIndexFiles.foreach(_.delete())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // reopen the log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig)&lt;br/&gt;
+    log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(&quot;Should have %d messages when log is reopened&quot;.format(numMessages), numMessages, log.logEndOffset)&lt;br/&gt;
     assertTrue(&quot;The index should have been rebuilt&quot;, log.logSegments.head.offsetIndex.entries &amp;gt; 0)&lt;br/&gt;
     assertTrue(&quot;The time index should have been rebuilt&quot;, log.logSegments.head.timeIndex.entries &amp;gt; 0)&lt;br/&gt;
@@ -1622,8 +1622,8 @@ class LogTest {&lt;br/&gt;
   def testRebuildTimeIndexForOldMessages() {&lt;br/&gt;
     val numMessages = 200&lt;br/&gt;
     val segmentSize = 200&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = segmentSize, indexIntervalBytes = 1, messageFormatVersion = &quot;0.9.0&quot;)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = segmentSize, indexIntervalBytes = 1, messageFormatVersion = &quot;0.9.0&quot;)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     for (i &amp;lt;- 0 until numMessages)&lt;br/&gt;
       log.appendAsLeader(TestUtils.singletonRecords(value = TestUtils.randomBytes(10),&lt;br/&gt;
         timestamp = mockTime.milliseconds + i * 10, magicValue = RecordBatch.MAGIC_VALUE_V1), leaderEpoch = 0)&lt;br/&gt;
@@ -1634,7 +1634,7 @@ class LogTest {&lt;br/&gt;
     timeIndexFiles.foreach(file =&amp;gt; Files.delete(file.toPath))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // The rebuilt time index should be empty&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig, recoveryPoint = numMessages + 1)&lt;br/&gt;
+    log = createLog(logDir, logConfig, recoveryPoint = numMessages + 1, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     for (segment &amp;lt;- log.logSegments.init) {&lt;br/&gt;
       assertEquals(&quot;The time index should be empty&quot;, 0, segment.timeIndex.entries)&lt;br/&gt;
       assertEquals(&quot;The time index file size should be 0&quot;, 0, segment.timeIndex.file.length)&lt;br/&gt;
@@ -1648,8 +1648,8 @@ class LogTest {&lt;br/&gt;
   def testCorruptIndexRebuild() {&lt;br/&gt;
     // publish the messages and close the log&lt;br/&gt;
     val numMessages = 200&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 200, indexIntervalBytes = 1)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 200, indexIntervalBytes = 1)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     for(i &amp;lt;- 0 until numMessages)&lt;br/&gt;
       log.appendAsLeader(TestUtils.singletonRecords(value = TestUtils.randomBytes(10), timestamp = mockTime.milliseconds + i * 10), leaderEpoch = 0)&lt;br/&gt;
     val indexFiles = log.logSegments.map(_.offsetIndex.file)&lt;br/&gt;
@@ -1671,7 +1671,7 @@ class LogTest {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // reopen the log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig, recoveryPoint = 200L)&lt;br/&gt;
+    log = createLog(logDir, logConfig, recoveryPoint = 200L, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(&quot;Should have %d messages when log is reopened&quot;.format(numMessages), numMessages, log.logEndOffset)&lt;br/&gt;
     for(i &amp;lt;- 0 until numMessages) {&lt;br/&gt;
       assertEquals(i, log.readUncommitted(i, 100, None).records.batches.iterator.next().lastOffset)&lt;br/&gt;
@@ -1694,8 +1694,8 @@ class LogTest {&lt;br/&gt;
     val segmentSize = msgPerSeg * setSize  // each segment will be 10 messages&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // create a log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = segmentSize)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = segmentSize)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(&quot;There should be exactly 1 segment.&quot;, 1, log.numberOfSegments)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for (_ &amp;lt;- 1 to msgPerSeg)&lt;br/&gt;
@@ -1746,8 +1746,8 @@ class LogTest {&lt;br/&gt;
     val setSize = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds).sizeInBytes&lt;br/&gt;
     val msgPerSeg = 10&lt;br/&gt;
     val segmentSize = msgPerSeg * setSize  // each segment will be 10 messages&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = segmentSize, indexIntervalBytes = setSize - 1)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = segmentSize, indexIntervalBytes = setSize - 1)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     assertEquals(&quot;There should be exactly 1 segment.&quot;, 1, log.numberOfSegments)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for (i&amp;lt;- 1 to msgPerSeg)&lt;br/&gt;
@@ -1785,8 +1785,8 @@ class LogTest {&lt;br/&gt;
     val bogusTimeIndex2 = Log.timeIndexFile(logDir, 5)&lt;/p&gt;

&lt;p&gt;     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, indexIntervalBytes = 1)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, indexIntervalBytes = 1)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertTrue(&quot;The first index file should have been replaced with a larger file&quot;, bogusIndex1.length &amp;gt; 0)&lt;br/&gt;
     assertTrue(&quot;The first time index file should have been replaced with a larger file&quot;, bogusTimeIndex1.length &amp;gt; 0)&lt;br/&gt;
@@ -1807,14 +1807,14 @@ class LogTest {&lt;br/&gt;
   def testReopenThenTruncate() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;br/&gt;
     // create a log&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, indexIntervalBytes = 10000)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, indexIntervalBytes = 10000)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // add enough messages to roll over several segments then close and re-open and attempt to truncate&lt;br/&gt;
     for (_ &amp;lt;- 0 until 100)&lt;br/&gt;
       log.appendAsLeader(createRecords, leaderEpoch = 0)&lt;br/&gt;
     log.close()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig)&lt;br/&gt;
+    log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.truncateTo(3)&lt;br/&gt;
     assertEquals(&quot;All but one segment should be deleted.&quot;, 1, log.numberOfSegments)&lt;br/&gt;
     assertEquals(&quot;Log end offset should be 3.&quot;, 3, log.logEndOffset)&lt;br/&gt;
@@ -1827,9 +1827,9 @@ class LogTest {&lt;br/&gt;
   def testAsyncDelete() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds - 1000L)&lt;br/&gt;
     val asyncDeleteMs = 1000&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, indexIntervalBytes = 10000,&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, indexIntervalBytes = 10000,&lt;br/&gt;
                                     retentionMs = 999, fileDeleteDelayMs = asyncDeleteMs)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 100)&lt;br/&gt;
@@ -1861,8 +1861,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testOpenDeletesObsoleteFiles() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds - 1000)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, retentionMs = 999)&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, retentionMs = 999)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 100)&lt;br/&gt;
@@ -1872,13 +1872,13 @@ class LogTest &lt;/p&gt;
{
     log.onHighWatermarkIncremented(log.logEndOffset)
     log.deleteOldSegments()
     log.close()
-    log = createLog(logDir, logConfig)
+    log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)
     assertEquals(&quot;The deleted segments should be gone.&quot;, 1, log.numberOfSegments)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def testAppendMessageWithNullPayload() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.appendAsLeader(TestUtils.singletonRecords(value = null), leaderEpoch = 0)&lt;br/&gt;
     val head = log.readUncommitted(0, 4096, None).records.records.iterator.next()&lt;br/&gt;
     assertEquals(0, head.offset)&lt;br/&gt;
@@ -1887,7 +1887,7 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IllegalArgumentException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   def testAppendWithOutOfOrderOffsetsThrowsException() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val records = (0 until 2).map(id =&amp;gt; new SimpleRecord(id.toString.getBytes)).toArray&lt;br/&gt;
     records.foreach(record =&amp;gt; log.appendAsLeader(MemoryRecords.withRecords(CompressionType.NONE, record), leaderEpoch = 0))&lt;br/&gt;
     val invalidRecord = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(1.toString.getBytes))&lt;br/&gt;
@@ -1896,7 +1896,7 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testAppendWithNoTimestamp(): Unit = &lt;/p&gt;
{
-    val log = createLog(logDir, LogConfig())
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)
     log.appendAsLeader(MemoryRecords.withRecords(CompressionType.NONE,
       new SimpleRecord(RecordBatch.NO_TIMESTAMP, &quot;key&quot;.getBytes, &quot;value&quot;.getBytes)), leaderEpoch = 0)
   }
&lt;p&gt;@@ -1904,13 +1904,13 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testCorruptLog() {&lt;br/&gt;
     // append some messages to create some segments&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;br/&gt;
     val recoveryPoint = 50L&lt;br/&gt;
     for (_ &amp;lt;- 0 until 10) {&lt;br/&gt;
       // create a log and write some messages to it&lt;br/&gt;
       logDir.mkdirs()&lt;/li&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+      var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
       val numMessages = 50 + TestUtils.random.nextInt(50)&lt;br/&gt;
       for (_ &amp;lt;- 0 until numMessages)&lt;br/&gt;
         log.appendAsLeader(createRecords, leaderEpoch = 0)&lt;br/&gt;
@@ -1922,7 +1922,7 @@ class LogTest {&lt;br/&gt;
       TestUtils.appendNonsenseToFile(log.activeSegment.log.file, TestUtils.random.nextInt(1024) + 1)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // attempt recovery&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log = createLog(logDir, logConfig, 0L, recoveryPoint)&lt;br/&gt;
+      log = createLog(logDir, logConfig, brokerTopicStats, 0L, recoveryPoint)&lt;br/&gt;
       assertEquals(numMessages, log.logEndOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       val recovered = log.logSegments.flatMap(_.log.records.asScala.toList).toList&lt;br/&gt;
@@ -1943,8 +1943,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testOverCompactedLogRecovery(): Unit = {&lt;br/&gt;
     // append some messages to create some segments&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val set1 = MemoryRecords.withRecords(0, CompressionType.NONE, 0, new SimpleRecord(&quot;v1&quot;.getBytes(), &quot;k1&quot;.getBytes()))&lt;br/&gt;
     val set2 = MemoryRecords.withRecords(Integer.MAX_VALUE.toLong + 2, CompressionType.NONE, 0, new SimpleRecord(&quot;v3&quot;.getBytes(), &quot;k3&quot;.getBytes()))&lt;br/&gt;
     val set3 = MemoryRecords.withRecords(Integer.MAX_VALUE.toLong + 3, CompressionType.NONE, 0, new SimpleRecord(&quot;v4&quot;.getBytes(), &quot;k4&quot;.getBytes()))&lt;br/&gt;
@@ -1976,8 +1976,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testOverCompactedLogRecoveryMultiRecord(): Unit = {&lt;br/&gt;
     // append some messages to create some segments&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val set1 = MemoryRecords.withRecords(0, CompressionType.NONE, 0, new SimpleRecord(&quot;v1&quot;.getBytes(), &quot;k1&quot;.getBytes()))&lt;br/&gt;
     val set2 = MemoryRecords.withRecords(Integer.MAX_VALUE.toLong + 2, CompressionType.GZIP, 0,&lt;br/&gt;
       new SimpleRecord(&quot;v3&quot;.getBytes(), &quot;k3&quot;.getBytes()),&lt;br/&gt;
@@ -2015,8 +2015,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testOverCompactedLogRecoveryMultiRecordV1(): Unit = {&lt;br/&gt;
     // append some messages to create some segments&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val set1 = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V1, 0, CompressionType.NONE,&lt;br/&gt;
       new SimpleRecord(&quot;v1&quot;.getBytes(), &quot;k1&quot;.getBytes()))&lt;br/&gt;
     val set2 = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V1, Integer.MAX_VALUE.toLong + 2, CompressionType.GZIP,&lt;br/&gt;
@@ -2053,17 +2053,185 @@ class LogTest 
{
     Utils.delete(logDir)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def testSplitOnOffsetOverflow(): Unit = &lt;/p&gt;
{
+    // create a log such that one log segment has offsets that overflow, and call the split API on that segment
+    val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)
+    val (log, segmentWithOverflow, inputRecords) = createLogWithOffsetOverflow(Some(logConfig))
+    assertTrue(&quot;At least one segment must have offset overflow&quot;, LogTest.hasOffsetOverflow(log))
+
+    // split the segment with overflow
+    log.splitOverflowedSegment(segmentWithOverflow)
+
+    // assert we were successfully able to split the segment
+    assertEquals(log.numberOfSegments, 4)
+    assertTrue(LogTest.verifyRecordsInLog(log, inputRecords))
+
+    // verify we do not have offset overflow anymore
+    assertFalse(LogTest.hasOffsetOverflow(log))
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRecoveryOfSegmentWithOffsetOverflow(): Unit = {&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)&lt;br/&gt;
+    var (log, segmentWithOverflow, initialRecords) = createLogWithOffsetOverflow(Some(logConfig))&lt;br/&gt;
+    val expectedKeys = LogTest.keysInLog(log)&lt;br/&gt;
+&lt;br/&gt;
+    // Run recovery on the log. This should split the segment underneath. Ignore .deleted files as we could have still&lt;br/&gt;
+    // have them lying around after the split.&lt;br/&gt;
+    log = LogTest.recoverAndCheck(logDir, logConfig, expectedKeys, brokerTopicStats, expectDeletedFiles = true)&lt;br/&gt;
+    assertEquals(expectedKeys, LogTest.keysInLog(log))&lt;br/&gt;
+&lt;br/&gt;
+    // Running split again would throw an error&lt;br/&gt;
+    for (segment &amp;lt;- log.logSegments) {&lt;br/&gt;
+      try &lt;/p&gt;
{
+        log.splitOverflowedSegment(segment)
+        fail()
+      }
&lt;p&gt; catch &lt;/p&gt;
{
+        case _: IllegalArgumentException =&amp;gt;
+      }
&lt;p&gt;+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRecoveryAfterCrashDuringSplitPhase1(): Unit = {&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)&lt;br/&gt;
+    var (log, segmentWithOverflow, initialRecords) = createLogWithOffsetOverflow(Some(logConfig))&lt;br/&gt;
+    val expectedKeys = LogTest.keysInLog(log)&lt;br/&gt;
+    val numSegmentsInitial = log.logSegments.size&lt;br/&gt;
+&lt;br/&gt;
+    // Split the segment&lt;br/&gt;
+    val newSegments = log.splitOverflowedSegment(segmentWithOverflow)&lt;br/&gt;
+&lt;br/&gt;
+    // Simulate recovery just after .cleaned file is created, before rename to .swap. On recovery, existing split&lt;br/&gt;
+    // operation is aborted but the recovery process itself kicks off split which should complete.&lt;br/&gt;
+    newSegments.reverse.foreach(segment =&amp;gt; &lt;/p&gt;
{
+      segment.changeFileSuffixes(&quot;&quot;, Log.CleanedFileSuffix)
+      segment.truncateTo(0)
+    }
&lt;p&gt;)&lt;br/&gt;
+    for (file &amp;lt;- logDir.listFiles if file.getName.endsWith(Log.DeletedFileSuffix))&lt;br/&gt;
+      Utils.atomicMoveWithFallback(file.toPath, Paths.get(CoreUtils.replaceSuffix(file.getPath, Log.DeletedFileSuffix, &quot;&quot;)))&lt;br/&gt;
+    log = LogTest.recoverAndCheck(logDir, logConfig, expectedKeys, brokerTopicStats, expectDeletedFiles = true)&lt;br/&gt;
+    assertEquals(expectedKeys, LogTest.keysInLog(log))&lt;br/&gt;
+    assertEquals(numSegmentsInitial + 1, log.logSegments.size)&lt;br/&gt;
+    log.close()&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRecoveryAfterCrashDuringSplitPhase2(): Unit = {&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)&lt;br/&gt;
+    var (log, segmentWithOverflow, initialRecords) = createLogWithOffsetOverflow(Some(logConfig))&lt;br/&gt;
+    val expectedKeys = LogTest.keysInLog(log)&lt;br/&gt;
+    val numSegmentsInitial = log.logSegments.size&lt;br/&gt;
+&lt;br/&gt;
+    // Split the segment&lt;br/&gt;
+    val newSegments = log.splitOverflowedSegment(segmentWithOverflow)&lt;br/&gt;
+&lt;br/&gt;
+    // Simulate recovery just after one of the new segments has been renamed to .swap. On recovery, existing split&lt;br/&gt;
+    // operation is aborted but the recovery process itself kicks off split which should complete.&lt;br/&gt;
+    newSegments.reverse.foreach(segment =&amp;gt; &lt;/p&gt;
{
+      if (segment != newSegments.tail)
+        segment.changeFileSuffixes(&quot;&quot;, Log.CleanedFileSuffix)
+      else
+        segment.changeFileSuffixes(&quot;&quot;, Log.SwapFileSuffix)
+      segment.truncateTo(0)
+    }
&lt;p&gt;)&lt;br/&gt;
+    for (file &amp;lt;- logDir.listFiles if file.getName.endsWith(Log.DeletedFileSuffix))&lt;br/&gt;
+      Utils.atomicMoveWithFallback(file.toPath, Paths.get(CoreUtils.replaceSuffix(file.getPath, Log.DeletedFileSuffix, &quot;&quot;)))&lt;br/&gt;
+    log = LogTest.recoverAndCheck(logDir, logConfig, expectedKeys, brokerTopicStats, expectDeletedFiles = true)&lt;br/&gt;
+    assertEquals(expectedKeys, LogTest.keysInLog(log))&lt;br/&gt;
+    assertEquals(numSegmentsInitial + 1, log.logSegments.size)&lt;br/&gt;
+    log.close()&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRecoveryAfterCrashDuringSplitPhase3(): Unit = {&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)&lt;br/&gt;
+    var (log, segmentWithOverflow, initialRecords) = createLogWithOffsetOverflow(Some(logConfig))&lt;br/&gt;
+    val expectedKeys = LogTest.keysInLog(log)&lt;br/&gt;
+    val numSegmentsInitial = log.logSegments.size&lt;br/&gt;
+&lt;br/&gt;
+    // Split the segment&lt;br/&gt;
+    val newSegments = log.splitOverflowedSegment(segmentWithOverflow)&lt;br/&gt;
+&lt;br/&gt;
+    // Simulate recovery right after all new segments have been renamed to .swap. On recovery, existing split operation&lt;br/&gt;
+    // is completed and the old segment must be deleted.&lt;br/&gt;
+    newSegments.reverse.foreach(segment =&amp;gt; &lt;/p&gt;
{
+        segment.changeFileSuffixes(&quot;&quot;, Log.SwapFileSuffix)
+    }
&lt;p&gt;)&lt;br/&gt;
+    for (file &amp;lt;- logDir.listFiles if file.getName.endsWith(Log.DeletedFileSuffix))&lt;br/&gt;
+      Utils.atomicMoveWithFallback(file.toPath, Paths.get(CoreUtils.replaceSuffix(file.getPath, Log.DeletedFileSuffix, &quot;&quot;)))&lt;br/&gt;
+&lt;br/&gt;
+    // Truncate the old segment&lt;br/&gt;
+    segmentWithOverflow.truncateTo(0)&lt;br/&gt;
+&lt;br/&gt;
+    log = LogTest.recoverAndCheck(logDir, logConfig, expectedKeys, brokerTopicStats, expectDeletedFiles = true)&lt;br/&gt;
+    assertEquals(expectedKeys, LogTest.keysInLog(log))&lt;br/&gt;
+    assertEquals(numSegmentsInitial + 1, log.logSegments.size)&lt;br/&gt;
+    log.close()&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRecoveryAfterCrashDuringSplitPhase4(): Unit = {&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)&lt;br/&gt;
+    var (log, segmentWithOverflow, initialRecords) = createLogWithOffsetOverflow(Some(logConfig))&lt;br/&gt;
+    val expectedKeys = LogTest.keysInLog(log)&lt;br/&gt;
+    val numSegmentsInitial = log.logSegments.size&lt;br/&gt;
+&lt;br/&gt;
+    // Split the segment&lt;br/&gt;
+    val newSegments = log.splitOverflowedSegment(segmentWithOverflow)&lt;br/&gt;
+&lt;br/&gt;
+    // Simulate recovery right after all new segments have been renamed to .swap and old segment has been deleted. On&lt;br/&gt;
+    // recovery, existing split operation is completed.&lt;br/&gt;
+    newSegments.reverse.foreach(segment =&amp;gt; &lt;/p&gt;
{
+      segment.changeFileSuffixes(&quot;&quot;, Log.SwapFileSuffix)
+    }
&lt;p&gt;)&lt;br/&gt;
+    for (file &amp;lt;- logDir.listFiles if file.getName.endsWith(Log.DeletedFileSuffix))&lt;br/&gt;
+      Utils.delete(file)&lt;br/&gt;
+&lt;br/&gt;
+    // Truncate the old segment&lt;br/&gt;
+    segmentWithOverflow.truncateTo(0)&lt;br/&gt;
+&lt;br/&gt;
+    log = LogTest.recoverAndCheck(logDir, logConfig, expectedKeys, brokerTopicStats, expectDeletedFiles = true)&lt;br/&gt;
+    assertEquals(expectedKeys, LogTest.keysInLog(log))&lt;br/&gt;
+    assertEquals(numSegmentsInitial + 1, log.logSegments.size)&lt;br/&gt;
+    log.close()&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testRecoveryAfterCrashDuringSplitPhase5(): Unit = &lt;/p&gt;
{
+    val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)
+    var (log, segmentWithOverflow, initialRecords) = createLogWithOffsetOverflow(Some(logConfig))
+    val expectedKeys = LogTest.keysInLog(log)
+    val numSegmentsInitial = log.logSegments.size
+
+    // Split the segment
+    val newSegments = log.splitOverflowedSegment(segmentWithOverflow)
+
+    // Simulate recovery right after one of the new segment has been renamed to .swap and the other to .log. On
+    // recovery, existing split operation is completed.
+    newSegments.last.changeFileSuffixes(&quot;&quot;, Log.SwapFileSuffix)
+
+    // Truncate the old segment
+    segmentWithOverflow.truncateTo(0)
+
+    log = LogTest.recoverAndCheck(logDir, logConfig, expectedKeys, brokerTopicStats, expectDeletedFiles = true)
+    assertEquals(expectedKeys, LogTest.keysInLog(log))
+    assertEquals(numSegmentsInitial + 1, log.logSegments.size)
+    log.close()
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testCleanShutdownFile() {&lt;br/&gt;
     // append some messages to create some segments&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1000, indexIntervalBytes = 1, maxMessageBytes = 64 * 1024)&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val cleanShutdownFile = createCleanShutdownFile()&lt;br/&gt;
     assertTrue(&quot;.kafka_cleanshutdown must exist&quot;, cleanShutdownFile.exists())&lt;br/&gt;
     var recoveryPoint = 0L&lt;br/&gt;
     // create a log and write some messages to it&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var log = createLog(logDir, logConfig)&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     for (_ &amp;lt;- 0 until 100)&lt;br/&gt;
       log.appendAsLeader(createRecords, leaderEpoch = 0)&lt;br/&gt;
     log.close()&lt;br/&gt;
@@ -2071,7 +2239,7 @@ class LogTest 
{
     // check if recovery was attempted. Even if the recovery point is 0L, recovery should not be attempted as the
     // clean shutdown file exists.
     recoveryPoint = log.logEndOffset
-    log = createLog(logDir, logConfig)
+    log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)
     assertEquals(recoveryPoint, log.logEndOffset)
     Utils.delete(cleanShutdownFile)
   }
&lt;p&gt;@@ -2231,8 +2399,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testDeleteOldSegments() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds - 1000)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, retentionMs = 999)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, retentionMs = 999)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 100)&lt;br/&gt;
@@ -2281,8 +2449,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testLogDeletionAfterClose() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds - 1000)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, retentionMs = 999)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, segmentIndexBytes = 1000, retentionMs = 999)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     log.appendAsLeader(createRecords, leaderEpoch = 0)&lt;br/&gt;
@@ -2299,8 +2467,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testLogDeletionAfterDeleteRecords() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for (_ &amp;lt;- 0 until 15)&lt;br/&gt;
       log.appendAsLeader(createRecords, leaderEpoch = 0)&lt;br/&gt;
@@ -2331,8 +2499,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldDeleteSizeBasedSegments() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 10)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 10)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 15)&lt;br/&gt;
@@ -2346,8 +2514,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldNotDeleteSizeBasedSegmentsWhenUnderRetentionSize() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 15)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 15)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 15)&lt;br/&gt;
@@ -2361,8 +2529,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldDeleteTimeBasedSegmentsReadyToBeDeleted() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes, timestamp = 10)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 15)&lt;br/&gt;
@@ -2376,8 +2544,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldNotDeleteTimeBasedSegmentsWhenNoneReadyToBeDeleted() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000000)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000000)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 15)&lt;br/&gt;
@@ -2391,8 +2559,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldNotDeleteSegmentsWhenPolicyDoesNotIncludeDelete() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes, key = &quot;test&quot;.getBytes(), timestamp = 10L)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000, cleanupPolicy = &quot;compact&quot;)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000, cleanupPolicy = &quot;compact&quot;)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 15)&lt;br/&gt;
@@ -2410,8 +2578,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldDeleteSegmentsReadyToBeDeletedWhenCleanupPolicyIsCompactAndDelete() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes, key = &quot;test&quot;.getBytes, timestamp = 10L)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000, cleanupPolicy = &quot;compact,delete&quot;)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionMs = 10000, cleanupPolicy = &quot;compact,delete&quot;)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append some messages to create some segments&lt;br/&gt;
     for (_ &amp;lt;- 0 until 15)&lt;br/&gt;
@@ -2428,7 +2596,7 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;     //Given this partition is on leader epoch 72&lt;br/&gt;
     val epoch = 72&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     log.leaderEpochCache.assign(epoch, records.size)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When appending messages as a leader (i.e. assignOffsets = true)&lt;br/&gt;
@@ -2460,7 +2628,7 @@ class LogTest &lt;/p&gt;
{
       recs
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When appending as follower (assignOffsets = false)&lt;br/&gt;
     for (i &amp;lt;- records.indices)&lt;br/&gt;
@@ -2472,8 +2640,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldTruncateLeaderEpochsWhenDeletingSegments() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 10)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 10)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val cache = epochCache(log)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Given three segments of 5 messages each&lt;br/&gt;
@@ -2497,8 +2665,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldUpdateOffsetForLeaderEpochsWhenDeletingSegments() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(&quot;test&quot;.getBytes)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 10)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = createRecords.sizeInBytes * 5, retentionBytes = createRecords.sizeInBytes * 10)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val cache = epochCache(log)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Given three segments of 5 messages each&lt;br/&gt;
@@ -2522,8 +2690,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldTruncateLeaderEpochFileWhenTruncatingLog() {&lt;br/&gt;
     def createRecords = TestUtils.singletonRecords(value = &quot;test&quot;.getBytes, timestamp = mockTime.milliseconds)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 10 * createRecords.sizeInBytes)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 10 * createRecords.sizeInBytes)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val cache = epochCache(log)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Given 2 segments, 10 messages per segment&lt;br/&gt;
@@ -2564,11 +2732,11 @@ class LogTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Append a bunch of messages to a log and then re-open it with recovery and check that the leader epochs are recovered properly.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+   * Append a bunch of messages to a log and then re-open it with recovery and check that the leader epochs are recovered properly.&lt;br/&gt;
+   */&lt;br/&gt;
   @Test&lt;br/&gt;
   def testLogRecoversForLeaderEpoch() {&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, LogConfig())&lt;br/&gt;
+    val log = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val leaderEpochCache = epochCache(log)&lt;br/&gt;
     val firstBatch = singletonRecordsWithLeaderEpoch(value = &quot;random&quot;.getBytes, leaderEpoch = 1, offset = 0)&lt;br/&gt;
     log.appendAsFollower(records = firstBatch)&lt;br/&gt;
@@ -2590,7 +2758,7 @@ class LogTest {&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // reopen the log and recover from the beginning&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val recoveredLog = createLog(logDir, LogConfig())&lt;br/&gt;
+    val recoveredLog = createLog(logDir, LogConfig(), brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val recoveredLeaderEpochCache = epochCache(recoveredLog)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // epoch entries should be recovered&lt;br/&gt;
@@ -2599,15 +2767,15 @@ class LogTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Wrap a single record log buffer with leader epoch.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+   * Wrap a single record log buffer with leader epoch.&lt;br/&gt;
+   */&lt;br/&gt;
   private def singletonRecordsWithLeaderEpoch(value: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;key: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = null,&lt;/li&gt;
	&lt;li&gt;leaderEpoch: Int,&lt;/li&gt;
	&lt;li&gt;offset: Long,&lt;/li&gt;
	&lt;li&gt;codec: CompressionType = CompressionType.NONE,&lt;/li&gt;
	&lt;li&gt;timestamp: Long = RecordBatch.NO_TIMESTAMP,&lt;/li&gt;
	&lt;li&gt;magicValue: Byte = RecordBatch.CURRENT_MAGIC_VALUE): MemoryRecords = {&lt;br/&gt;
+                                              key: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = null,&lt;br/&gt;
+                                              leaderEpoch: Int,&lt;br/&gt;
+                                              offset: Long,&lt;br/&gt;
+                                              codec: CompressionType = CompressionType.NONE,&lt;br/&gt;
+                                              timestamp: Long = RecordBatch.NO_TIMESTAMP,&lt;br/&gt;
+                                              magicValue: Byte = RecordBatch.CURRENT_MAGIC_VALUE): MemoryRecords = {&lt;br/&gt;
     val records = Seq(new SimpleRecord(timestamp, key, value))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))&lt;br/&gt;
@@ -2618,8 +2786,8 @@ class LogTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   def testFirstUnstableOffsetNoTransactionalData() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val records = MemoryRecords.withRecords(CompressionType.NONE,&lt;br/&gt;
       new SimpleRecord(&quot;foo&quot;.getBytes),&lt;br/&gt;
@@ -2632,8 +2800,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testFirstUnstableOffsetWithTransactionalData() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val pid = 137L&lt;br/&gt;
     val epoch = 5.toShort&lt;br/&gt;
@@ -2670,8 +2838,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testTransactionIndexUpdated(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val epoch = 0.toShort&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val pid1 = 1L&lt;br/&gt;
@@ -2711,8 +2879,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testFullTransactionIndexRecovery(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 128 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 128 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val epoch = 0.toShort&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val pid1 = 1L&lt;br/&gt;
@@ -2754,16 +2922,16 @@ class LogTest &lt;/p&gt;
{
 
     log.close()
 
-    val reloadedLogConfig = createLogConfig(segmentBytes = 1024 * 5)
-    val reloadedLog = createLog(logDir, reloadedLogConfig)
+    val reloadedLogConfig = LogTest.createLogConfig(segmentBytes = 1024 * 5)
+    val reloadedLog = createLog(logDir, reloadedLogConfig, brokerTopicStats = brokerTopicStats)
     val abortedTransactions = allAbortedTransactions(reloadedLog)
     assertEquals(List(new AbortedTxn(pid1, 0L, 29L, 8L), new AbortedTxn(pid2, 8L, 74L, 36L)), abortedTransactions)
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def testRecoverOnlyLastSegment(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 128 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 128 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val epoch = 0.toShort&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val pid1 = 1L&lt;br/&gt;
@@ -2805,16 +2973,16 @@ class LogTest &lt;/p&gt;
{
 
     log.close()
 
-    val reloadedLogConfig = createLogConfig(segmentBytes = 1024 * 5)
-    val reloadedLog = createLog(logDir, reloadedLogConfig, recoveryPoint = recoveryPoint)
+    val reloadedLogConfig = LogTest.createLogConfig(segmentBytes = 1024 * 5)
+    val reloadedLog = createLog(logDir, reloadedLogConfig, recoveryPoint = recoveryPoint, brokerTopicStats = brokerTopicStats)
     val abortedTransactions = allAbortedTransactions(reloadedLog)
     assertEquals(List(new AbortedTxn(pid1, 0L, 29L, 8L), new AbortedTxn(pid2, 8L, 74L, 36L)), abortedTransactions)
   }&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
   def testRecoverLastSegmentWithNoSnapshots(): Unit = {&lt;br/&gt;
-    val logConfig = createLogConfig(segmentBytes = 128 * 5)&lt;br/&gt;
-    val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 128 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
 &lt;br/&gt;
     val pid1 = 1L&lt;br/&gt;
@@ -2859,8 +3027,8 @@ class LogTest {      log.close() -    val reloadedLogConfig = createLogConfig(segmentBytes = 1024 * 5)-    val reloadedLog = createLog(logDir, reloadedLogConfig, recoveryPoint = recoveryPoint)+    val reloadedLogConfig = LogTest.createLogConfig(segmentBytes = 1024 * 5)+    val reloadedLog = createLog(logDir, reloadedLogConfig, recoveryPoint = recoveryPoint, brokerTopicStats = brokerTopicStats)     val abortedTransactions = allAbortedTransactions(reloadedLog)     assertEquals(List(new AbortedTxn(pid1, 0L, 29L, 8L), new AbortedTxn(pid2, 8L, 74L, 36L)), abortedTransactions)   }
&lt;p&gt;@@ -2868,8 +3036,8 @@ class LogTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def testTransactionIndexUpdatedThroughReplication(): Unit = {&lt;br/&gt;
     val epoch = 0.toShort&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val buffer = ByteBuffer.allocate(2048)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val pid1 = 1L&lt;br/&gt;
@@ -2914,8 +3082,8 @@ class LogTest {&lt;br/&gt;
   def testZombieCoordinatorFenced(): Unit = {&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val epoch = 0.toShort&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val append = appendTransactionalAsLeader(log, pid, epoch)&lt;/p&gt;

&lt;p&gt;@@ -2930,8 +3098,8 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testFirstUnstableOffsetDoesNotExceedLogStartOffsetMidSegment(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val appendPid = appendTransactionalAsLeader(log, pid, epoch)&lt;br/&gt;
@@ -2954,8 +3122,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testFirstUnstableOffsetDoesNotExceedLogStartOffsetAfterSegmentDeletion(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
     val pid = 1L&lt;br/&gt;
     val appendPid = appendTransactionalAsLeader(log, pid, epoch)&lt;br/&gt;
@@ -2981,8 +3149,8 @@ class LogTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testLastStableOffsetWithMixedProducerData() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // for convenience, both producers share the same epoch&lt;br/&gt;
     val epoch = 5.toShort&lt;br/&gt;
@@ -3042,8 +3210,8 @@ class LogTest {&lt;br/&gt;
       new SimpleRecord(&quot;b&quot;.getBytes),&lt;br/&gt;
       new SimpleRecord(&quot;c&quot;.getBytes))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val logConfig = createLogConfig(segmentBytes = records.sizeInBytes)&lt;/li&gt;
	&lt;li&gt;val log = createLog(logDir, logConfig)&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = records.sizeInBytes)&lt;br/&gt;
+    val log = createLog(logDir, logConfig, brokerTopicStats = brokerTopicStats)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val firstAppendInfo = log.appendAsLeader(records, leaderEpoch = 0)&lt;br/&gt;
     assertEquals(Some(firstAppendInfo.firstOffset.get), log.firstUnstableOffset.map(_.messageOffset))&lt;br/&gt;
@@ -3073,55 +3241,7 @@ class LogTest &lt;/p&gt;
{
     assertEquals(new AbortedTransaction(pid, 0), fetchDataInfo.abortedTransactions.get.head)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def createLogConfig(segmentMs: Long = Defaults.SegmentMs,&lt;/li&gt;
	&lt;li&gt;segmentBytes: Int = Defaults.SegmentSize,&lt;/li&gt;
	&lt;li&gt;retentionMs: Long = Defaults.RetentionMs,&lt;/li&gt;
	&lt;li&gt;retentionBytes: Long = Defaults.RetentionSize,&lt;/li&gt;
	&lt;li&gt;segmentJitterMs: Long = Defaults.SegmentJitterMs,&lt;/li&gt;
	&lt;li&gt;cleanupPolicy: String = Defaults.CleanupPolicy,&lt;/li&gt;
	&lt;li&gt;maxMessageBytes: Int = Defaults.MaxMessageSize,&lt;/li&gt;
	&lt;li&gt;indexIntervalBytes: Int = Defaults.IndexInterval,&lt;/li&gt;
	&lt;li&gt;segmentIndexBytes: Int = Defaults.MaxIndexSize,&lt;/li&gt;
	&lt;li&gt;messageFormatVersion: String = Defaults.MessageFormatVersion,&lt;/li&gt;
	&lt;li&gt;fileDeleteDelayMs: Long = Defaults.FileDeleteDelayMs): LogConfig = 
{
-    val logProps = new Properties()
-
-    logProps.put(LogConfig.SegmentMsProp, segmentMs: java.lang.Long)
-    logProps.put(LogConfig.SegmentBytesProp, segmentBytes: Integer)
-    logProps.put(LogConfig.RetentionMsProp, retentionMs: java.lang.Long)
-    logProps.put(LogConfig.RetentionBytesProp, retentionBytes: java.lang.Long)
-    logProps.put(LogConfig.SegmentJitterMsProp, segmentJitterMs: java.lang.Long)
-    logProps.put(LogConfig.CleanupPolicyProp, cleanupPolicy)
-    logProps.put(LogConfig.MaxMessageBytesProp, maxMessageBytes: Integer)
-    logProps.put(LogConfig.IndexIntervalBytesProp, indexIntervalBytes: Integer)
-    logProps.put(LogConfig.SegmentIndexBytesProp, segmentIndexBytes: Integer)
-    logProps.put(LogConfig.MessageFormatVersionProp, messageFormatVersion)
-    logProps.put(LogConfig.FileDeleteDelayMsProp, fileDeleteDelayMs: java.lang.Long)
-    LogConfig(logProps)
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;def createLog(dir: File,&lt;/li&gt;
	&lt;li&gt;config: LogConfig,&lt;/li&gt;
	&lt;li&gt;logStartOffset: Long = 0L,&lt;/li&gt;
	&lt;li&gt;recoveryPoint: Long = 0L,&lt;/li&gt;
	&lt;li&gt;scheduler: Scheduler = mockTime.scheduler,&lt;/li&gt;
	&lt;li&gt;brokerTopicStats: BrokerTopicStats = brokerTopicStats,&lt;/li&gt;
	&lt;li&gt;time: Time = mockTime,&lt;/li&gt;
	&lt;li&gt;maxProducerIdExpirationMs: Int = 60 * 60 * 1000,&lt;/li&gt;
	&lt;li&gt;producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs): Log = 
{
-    Log(dir = dir,
-        config = config,
-        logStartOffset = logStartOffset,
-        recoveryPoint = recoveryPoint,
-        scheduler = scheduler,
-        brokerTopicStats = brokerTopicStats,
-        time = time,
-        maxProducerIdExpirationMs = maxProducerIdExpirationMs,
-        producerIdExpirationCheckIntervalMs = producerIdExpirationCheckIntervalMs,
-        logDirFailureChannel = new LogDirFailureChannel(10))
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def allAbortedTransactions(log: Log) = log.logSegments.flatMap(_.txnIndex.allAbortedTxns)&lt;br/&gt;
+ private def allAbortedTransactions(log: Log) = log.logSegments.flatMap(_.txnIndex.allAbortedTxns)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def appendTransactionalAsLeader(log: Log, producerId: Long, producerEpoch: Short): Int =&amp;gt; Unit = {&lt;br/&gt;
     var sequence = 0&lt;br/&gt;
@@ -3200,4 +3320,230 @@ class LogTest {&lt;br/&gt;
   private def listProducerSnapshotOffsets: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; =&lt;br/&gt;
     ProducerStateManager.listSnapshotFiles(logDir).map(Log.offsetFromFile).sorted&lt;/p&gt;

&lt;p&gt;+  private def createLog(dir: File,&lt;br/&gt;
+                        config: LogConfig,&lt;br/&gt;
+                        brokerTopicStats: BrokerTopicStats = brokerTopicStats,&lt;br/&gt;
+                        logStartOffset: Long = 0L,&lt;br/&gt;
+                        recoveryPoint: Long = 0L,&lt;br/&gt;
+                        scheduler: Scheduler = mockTime.scheduler,&lt;br/&gt;
+                        time: Time = mockTime,&lt;br/&gt;
+                        maxProducerIdExpirationMs: Int = 60 * 60 * 1000,&lt;br/&gt;
+                        producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs): Log = &lt;/p&gt;
{
+    return LogTest.createLog(dir, config, brokerTopicStats, scheduler, time, logStartOffset, recoveryPoint,
+      maxProducerIdExpirationMs, producerIdExpirationCheckIntervalMs)
+  }
&lt;p&gt;+&lt;br/&gt;
+  private def createLogWithOffsetOverflow(logConfig: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LogConfig&amp;#93;&lt;/span&gt;): (Log, LogSegment, List&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt;) = &lt;/p&gt;
{
+    return LogTest.createLogWithOffsetOverflow(logDir, brokerTopicStats, logConfig, mockTime.scheduler, mockTime)
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+object LogTest {&lt;br/&gt;
+  def createLogConfig(segmentMs: Long = Defaults.SegmentMs,&lt;br/&gt;
+                      segmentBytes: Int = Defaults.SegmentSize,&lt;br/&gt;
+                      retentionMs: Long = Defaults.RetentionMs,&lt;br/&gt;
+                      retentionBytes: Long = Defaults.RetentionSize,&lt;br/&gt;
+                      segmentJitterMs: Long = Defaults.SegmentJitterMs,&lt;br/&gt;
+                      cleanupPolicy: String = Defaults.CleanupPolicy,&lt;br/&gt;
+                      maxMessageBytes: Int = Defaults.MaxMessageSize,&lt;br/&gt;
+                      indexIntervalBytes: Int = Defaults.IndexInterval,&lt;br/&gt;
+                      segmentIndexBytes: Int = Defaults.MaxIndexSize,&lt;br/&gt;
+                      messageFormatVersion: String = Defaults.MessageFormatVersion,&lt;br/&gt;
+                      fileDeleteDelayMs: Long = Defaults.FileDeleteDelayMs): LogConfig = &lt;/p&gt;
{
+    val logProps = new Properties()
+
+    logProps.put(LogConfig.SegmentMsProp, segmentMs: java.lang.Long)
+    logProps.put(LogConfig.SegmentBytesProp, segmentBytes: Integer)
+    logProps.put(LogConfig.RetentionMsProp, retentionMs: java.lang.Long)
+    logProps.put(LogConfig.RetentionBytesProp, retentionBytes: java.lang.Long)
+    logProps.put(LogConfig.SegmentJitterMsProp, segmentJitterMs: java.lang.Long)
+    logProps.put(LogConfig.CleanupPolicyProp, cleanupPolicy)
+    logProps.put(LogConfig.MaxMessageBytesProp, maxMessageBytes: Integer)
+    logProps.put(LogConfig.IndexIntervalBytesProp, indexIntervalBytes: Integer)
+    logProps.put(LogConfig.SegmentIndexBytesProp, segmentIndexBytes: Integer)
+    logProps.put(LogConfig.MessageFormatVersionProp, messageFormatVersion)
+    logProps.put(LogConfig.FileDeleteDelayMsProp, fileDeleteDelayMs: java.lang.Long)
+    LogConfig(logProps)
+  }
&lt;p&gt;+&lt;br/&gt;
+  def createLog(dir: File,&lt;br/&gt;
+                config: LogConfig,&lt;br/&gt;
+                brokerTopicStats: BrokerTopicStats,&lt;br/&gt;
+                scheduler: Scheduler,&lt;br/&gt;
+                time: Time,&lt;br/&gt;
+                logStartOffset: Long = 0L,&lt;br/&gt;
+                recoveryPoint: Long = 0L,&lt;br/&gt;
+                maxProducerIdExpirationMs: Int = 60 * 60 * 1000,&lt;br/&gt;
+                producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs): Log = &lt;/p&gt;
{
+    Log(dir = dir,
+      config = config,
+      logStartOffset = logStartOffset,
+      recoveryPoint = recoveryPoint,
+      scheduler = scheduler,
+      brokerTopicStats = brokerTopicStats,
+      time = time,
+      maxProducerIdExpirationMs = maxProducerIdExpirationMs,
+      producerIdExpirationCheckIntervalMs = producerIdExpirationCheckIntervalMs,
+      logDirFailureChannel = new LogDirFailureChannel(10))
+  }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Check if the given log contains any segment with records that cause offset overflow.&lt;br/&gt;
+   * @param log Log to check&lt;br/&gt;
+   * @return true if log contains at least one segment with offset overflow; false otherwise&lt;br/&gt;
+   */&lt;br/&gt;
+  def hasOffsetOverflow(log: Log): Boolean = {&lt;br/&gt;
+    for (logSegment &amp;lt;- log.logSegments) {&lt;br/&gt;
+      val baseOffset = logSegment.baseOffset&lt;br/&gt;
+      for (batch &amp;lt;- logSegment.log.batches.asScala) {&lt;br/&gt;
+        val it = batch.iterator()&lt;br/&gt;
+        while (it.hasNext()) &lt;/p&gt;
{
+          val record = it.next()
+          if (record.offset &amp;gt; baseOffset + Int.MaxValue || record.offset &amp;lt; baseOffset)
+            return true
+        }
&lt;p&gt;+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    false&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Create a log such that one of the log segments has messages with offsets that cause index offset overflow.&lt;br/&gt;
+   * @param logDir Directory in which log should be created&lt;br/&gt;
+   * @param brokerTopicStats Container for Broker Topic Yammer Metrics&lt;br/&gt;
+   * @param logConfigOpt Optional log configuration to use&lt;br/&gt;
+   * @param scheduler The thread pool scheduler used for background actions&lt;br/&gt;
+   * @param time The time instance to use&lt;br/&gt;
+   * @return (1) Created log containing segment with offset overflow, (2) Log segment within log containing messages with&lt;br/&gt;
+   *         offset overflow, and (3) List of messages in the log&lt;br/&gt;
+   */&lt;br/&gt;
+  def createLogWithOffsetOverflow(logDir: File, brokerTopicStats: BrokerTopicStats, logConfigOpt: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LogConfig&amp;#93;&lt;/span&gt; = None,&lt;br/&gt;
+                                  scheduler: Scheduler, time: Time): (Log, LogSegment, List&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt;) = {&lt;br/&gt;
+    val logConfig =&lt;br/&gt;
+      if (logConfigOpt.isDefined)&lt;br/&gt;
+        logConfigOpt.get&lt;br/&gt;
+      else&lt;br/&gt;
+        createLogConfig(indexIntervalBytes = 1)&lt;br/&gt;
+&lt;br/&gt;
+    var log = createLog(logDir, logConfig, brokerTopicStats, scheduler, time)&lt;br/&gt;
+    var inputRecords = ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+&lt;br/&gt;
+    // References to files we want to &quot;merge&quot; to emulate offset overflow&lt;br/&gt;
+    val toMerge = ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+&lt;br/&gt;
+    def getRecords(baseOffset: Long): List&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryRecords&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
+      def toBytes(value: Long): Array[Byte] = value.toString.getBytes
+
+      val set1 = MemoryRecords.withRecords(baseOffset, CompressionType.NONE, 0,
+        new SimpleRecord(toBytes(baseOffset), toBytes(baseOffset)))
+      val set2 = MemoryRecords.withRecords(baseOffset + 1, CompressionType.NONE, 0,
+        new SimpleRecord(toBytes(baseOffset + 1), toBytes(baseOffset + 1)),
+        new SimpleRecord(toBytes(baseOffset + 2), toBytes(baseOffset + 2)));
+      val set3 = MemoryRecords.withRecords(baseOffset + Int.MaxValue - 1, CompressionType.NONE, 0,
+        new SimpleRecord(toBytes(baseOffset + Int.MaxValue - 1), toBytes(baseOffset + Int.MaxValue - 1)));
+      List(set1, set2, set3)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Append some messages to the log. This will create four log segments.&lt;br/&gt;
+    var firstOffset = 0L&lt;br/&gt;
+    for (i &amp;lt;- 0 until 4) &lt;/p&gt;
{
+      val recordsToAppend = getRecords(firstOffset)
+      for (records &amp;lt;- recordsToAppend)
+        log.appendAsFollower(records)
+
+      if (i == 1 || i == 2)
+        toMerge += log.activeSegment.log.file
+
+      firstOffset += Int.MaxValue + 1L
+    }
&lt;p&gt;+&lt;br/&gt;
+    // assert that we have the correct number of segments&lt;br/&gt;
+    assertEquals(log.numberOfSegments, 4)&lt;br/&gt;
+&lt;br/&gt;
+    // assert number of batches&lt;br/&gt;
+    for (logSegment &amp;lt;- log.logSegments) &lt;/p&gt;
{
+      var numBatches = 0
+      for (_ &amp;lt;- logSegment.log.batches.asScala)
+        numBatches += 1
+      assertEquals(numBatches, 3)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // create a list of appended records&lt;br/&gt;
+    for (logSegment &amp;lt;- log.logSegments) {&lt;br/&gt;
+      for (batch &amp;lt;- logSegment.log.batches.asScala) &lt;/p&gt;
{
+        val it = batch.iterator()
+        while (it.hasNext())
+          inputRecords += it.next()
+      }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    log.flush()&lt;br/&gt;
+    log.close()&lt;br/&gt;
+&lt;br/&gt;
+    // We want to &quot;merge&quot; log segments 1 and 2. This is where the offset overflow will be.&lt;br/&gt;
+    // Current: segment #1 | segment #2 | segment #3 | segment# 4&lt;br/&gt;
+    // Final: segment #1 | segment #2&apos; | segment #4&lt;br/&gt;
+    // where 2&apos; corresponds to segment #2 and segment #3 combined together.&lt;br/&gt;
+    // Append segment #3 at the end of segment #2 to create 2&apos;&lt;br/&gt;
+    var dest: FileOutputStream = null&lt;br/&gt;
+    var source: FileInputStream = null&lt;br/&gt;
+    try &lt;/p&gt;
{
+      dest = new FileOutputStream(toMerge(0), true)
+      source = new FileInputStream(toMerge(1))
+      val sourceBytes = new Array[Byte](toMerge(1).length.toInt)
+      source.read(sourceBytes)
+      dest.write(sourceBytes)
+    }
&lt;p&gt; finally &lt;/p&gt;
{
+      dest.close()
+      source.close()
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Delete segment #3 including any index, etc.&lt;br/&gt;
+    toMerge(1).delete()&lt;br/&gt;
+    log = createLog(logDir, logConfig, brokerTopicStats, scheduler, time, recoveryPoint = Long.MaxValue)&lt;br/&gt;
+&lt;br/&gt;
+    // assert that there is now one less segment than before, and that the records in the log are same as before&lt;br/&gt;
+    assertEquals(log.numberOfSegments, 3)&lt;br/&gt;
+    assertTrue(verifyRecordsInLog(log, inputRecords.toList))&lt;br/&gt;
+&lt;br/&gt;
+    (log, log.logSegments.toList(1), inputRecords.toList)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def verifyRecordsInLog(log: Log, expectedRecords: List&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt;): Boolean = {&lt;br/&gt;
+    val recordsFound = ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    for (logSegment &amp;lt;- log.logSegments) {&lt;br/&gt;
+      for (batch &amp;lt;- logSegment.log.batches.asScala) &lt;/p&gt;
{
+        val it = batch.iterator()
+        while (it.hasNext())
+          recordsFound += it.next()
+      }
&lt;p&gt;+    }&lt;br/&gt;
+    return recordsFound.equals(expectedRecords)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /* extract all the keys from a log */&lt;br/&gt;
+  def keysInLog(log: Log): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
+    for (logSegment &amp;lt;- log.logSegments;
+         batch &amp;lt;- logSegment.log.batches.asScala if !batch.isControlBatch;
+         record &amp;lt;- batch.asScala if record.hasValue &amp;amp;&amp;amp; record.hasKey)
+      yield TestUtils.readString(record.key).toLong
+  }
&lt;p&gt;+&lt;br/&gt;
+  def recoverAndCheck(logDir: File, config: LogConfig, expectedKeys: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                      brokerTopicStats: BrokerTopicStats, expectDeletedFiles: Boolean = false): Log = {&lt;br/&gt;
+    val time = new MockTime()&lt;br/&gt;
+    // Recover log file and check that after recovery, keys are as expected&lt;br/&gt;
+    // and all temporary files have been deleted&lt;br/&gt;
+    val recoveredLog = createLog(logDir, config, brokerTopicStats, time.scheduler, time)&lt;br/&gt;
+    time.sleep(config.fileDeleteDelayMs + 1)&lt;br/&gt;
+    for (file &amp;lt;- logDir.listFiles) &lt;/p&gt;
{
+      if (!expectDeletedFiles)
+        assertFalse(&quot;Unexpected .deleted file after recovery&quot;, file.getName.endsWith(Log.DeletedFileSuffix))
+      assertFalse(&quot;Unexpected .cleaned file after recovery&quot;, file.getName.endsWith(Log.CleanedFileSuffix))
+      assertFalse(&quot;Unexpected .swap file after recovery&quot;, file.getName.endsWith(Log.SwapFileSuffix))
+    }
&lt;p&gt;+    assertEquals(expectedKeys, LogTest.keysInLog(recoveredLog))&lt;br/&gt;
+    assertFalse(LogTest.hasOffsetOverflow(recoveredLog))&lt;br/&gt;
+    recoveredLog&lt;br/&gt;
+  }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
index 8fa3cc19648..1e4e8929849 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
@@ -35,10 +35,11 @@ class OffsetIndexTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;   var idx: OffsetIndex = null&lt;br/&gt;
   val maxEntries = 30&lt;br/&gt;
+  val baseOffset = 45L&lt;/p&gt;

&lt;p&gt;   @Before&lt;br/&gt;
   def setup() &lt;/p&gt;
{
-    this.idx = new OffsetIndex(nonExistentTempFile(), baseOffset = 45L, maxIndexSize = 30 * 8)
+    this.idx = new OffsetIndex(nonExistentTempFile(), baseOffset, maxIndexSize = 30 * 8)
   }

&lt;p&gt;   @After&lt;br/&gt;
@@ -102,10 +103,10 @@ class OffsetIndexTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   def testFetchUpperBoundOffset() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val first = OffsetPosition(0, 0)&lt;/li&gt;
	&lt;li&gt;val second = OffsetPosition(1, 10)&lt;/li&gt;
	&lt;li&gt;val third = OffsetPosition(2, 23)&lt;/li&gt;
	&lt;li&gt;val fourth = OffsetPosition(3, 37)&lt;br/&gt;
+    val first = OffsetPosition(baseOffset + 0, 0)&lt;br/&gt;
+    val second = OffsetPosition(baseOffset + 1, 10)&lt;br/&gt;
+    val third = OffsetPosition(baseOffset + 2, 23)&lt;br/&gt;
+    val fourth = OffsetPosition(baseOffset + 3, 37)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(None, idx.fetchUpperBoundOffset(first, 5))&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 23 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3n4fb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>