<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:06:54 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6512] Java Producer: Excessive memory usage with compression enabled</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6512</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;h2&gt;&lt;a name=&quot;UserStory&quot;&gt;&lt;/a&gt;User Story&lt;/h2&gt;

&lt;p&gt;As a user of the Java producer, I want a predictable memory usage for the Kafka client so that I can ensure that my system is sized appropriately and will be stable even under heavy usage.&lt;/p&gt;

&lt;p&gt;As a user of the Java producer, I want a smaller memory footprint so that my systems don&apos;t consume as many resources.&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;AcceptanceCriteria&quot;&gt;&lt;/a&gt;Acceptance Criteria&lt;/h2&gt;
&lt;ul&gt;
	&lt;li&gt;Enabling Compression in Kafka should not significantly increase the memory usage of Kafka&lt;/li&gt;
	&lt;li&gt;The memory usage of Kafka&apos;s Java Producer should be roughly in line with the buffer size (buffer.memory) and the number of producers declared.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;&lt;a name=&quot;AdditionalInformation&quot;&gt;&lt;/a&gt;Additional Information&lt;/h2&gt;

&lt;p&gt;I&apos;ve observed high memory usage in the producer when enabling compression (gzip or lz4).&#160; I don&apos;t observe the behavior with compression off, but with it on I&apos;ll run out of heap (2GB).&#160; Using a Java profiler, I see the data is in the KafkaLZ4BlockOutputStream (or related class for gzip).&#160;&#160; I see that MemoryRecordsBuilder:closeForRecordAppends() is trying to deal with this, but is not successful.&#160; I&apos;m most likely network bottlenecked, so I expect the producer buffers to be full while the job is running and potentially a lot of unacknowledged records.&lt;/p&gt;

&lt;p&gt;I&apos;ve tried using the default buffer.memory with 20 producers (across 20 threads) and sending data as quickly as I can.&#160; I&apos;ve also tried 1MB of buffer.memory, which seemed to reduce memory consumption but I could still run OOM in certain cases.&#160; I have max.in.flight.requests.per.connection set to 1.&#160; In short, I should only have ~20 MB (20* 1MB) of data in buffers, but I can easily exhaust 2000 MB used by Kafka.&lt;/p&gt;

&lt;p&gt;In looking at the code more, it looks like the KafkaLZ4BlockOutputStream doesn&apos;t clear the compressedBuffer or buffer when close() is called.&#160; In my heap dump, both of those are ~65k size each, meaning that each batch is taking up ~148k of space, of which 131k is buffers. (buffer.memory=1,000,000 and messages are 1k each until the batch fills).&lt;/p&gt;

&lt;p&gt;Kafka tries to manage memory usage by calling MemoryRecordsBuilder:closeForRecordAppends(), which as documented as &quot;Release resources required for record appends (e.g. compression buffers)&quot;.&#160; However, this method doesn&apos;t actually clear those buffers because KafkaLZ4BlockOutputStream.close() only writes the block and end mark and closes the output stream.&#160; It doesn&apos;t actually clear the buffer and compressedBuffer in KafkaLZ4BlockOutputStream.&#160; Those stay allocated in RAM until the block is acknowledged by the broker, processed in Sender:handleProduceResponse(), and the batch is deallocated.&#160; This memory usage therefore increases, possibly without bound.&#160; In my test program, the program died with approximately 345 unprocessed batches per producer (20 producers), despite having max.in.flight.requests.per.connection=1.&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;StepstoReproduce&quot;&gt;&lt;/a&gt;Steps to Reproduce&lt;/h2&gt;
&lt;ol&gt;
	&lt;li&gt;Create a topic test with plenty of storage&lt;/li&gt;
	&lt;li&gt;Use a connection with a very fast upload pipe and limited download.&#160; This allows the outbound data to go out, but acknowledgements to be delayed flowing in.&lt;/li&gt;
	&lt;li&gt;Download KafkaSender.java (attached to this ticket)&lt;/li&gt;
	&lt;li&gt;Set line 17 to reference your Kafka broker&lt;/li&gt;
	&lt;li&gt;Run the program with a 1GB Xmx value&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;&lt;a name=&quot;Possiblesolutions&quot;&gt;&lt;/a&gt;Possible solutions&lt;/h2&gt;

&lt;p&gt;There are a few possible optimizations I can think of:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;We could declare KafkaLZ4BlockOutputStream.buffer and compressedBuffer as non-final and null them in the close() method&lt;/li&gt;
	&lt;li&gt;We could declare the MemoryRecordsBuilder.appendStream non-final and null it in the closeForRecordAppends() method&lt;/li&gt;
	&lt;li&gt;We could have the ProducerBatch discard the recordsBuilder in closeForRecordAppends(), however, this is likely a bad idea because the recordsBuilder contains significant metadata that is likely needed after the stream is closed.&#160; It is also final.&lt;/li&gt;
	&lt;li&gt;We could try to limit the number of non-acknowledged batches in flight.&#160; This would bound the maximum memory usage but may negatively impact performance.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Fix #1 would only improve the LZ4 algorithm, and not any other algorithms.&lt;/p&gt;

&lt;p&gt;Fix #2 would improve all algorithms, compression and otherwise.&#160; Of the 3 proposed here, it seems the best.&#160; This would also involve having to check appendStreamIsClosed in every usage of appendStream within MemoryRecordsBuilder to avoid NPE&apos;s.&lt;/p&gt;

&lt;p&gt;Fix #4 is likely necessary if we want to bound the maximum memory usage of Kafka.&#160; Removing the buffers in Fix 1 or 2 will reduce the memory usage by ~90%, but theoretically there is still no limit.&lt;/p&gt;</description>
                <environment>Windows 10</environment>
        <key id="13135250">KAFKA-6512</key>
            <summary>Java Producer: Excessive memory usage with compression enabled</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsivaram">Rajini Sivaram</assignee>
                                    <reporter username="ktinker">ktinker</reporter>
                        <labels>
                    </labels>
                <created>Wed, 31 Jan 2018 22:58:00 +0000</created>
                <updated>Thu, 15 Feb 2018 18:02:47 +0000</updated>
                            <resolved>Thu, 15 Feb 2018 18:02:47 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>clients</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16363820" author="githubbot" created="Wed, 14 Feb 2018 11:18:49 +0000"  >&lt;p&gt;rajinisivaram opened a new pull request #4570: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6512&quot; title=&quot;Java Producer: Excessive memory usage with compression enabled&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6512&quot;&gt;&lt;del&gt;KAFKA-6512&lt;/del&gt;&lt;/a&gt;: Discard references to buffers used for compression&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4570&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4570&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   `ProducerBatch` retains references to `MemoryRecordsBuilder` and cannot be freed until acks are received. Removing references to buffers used for compression after records are built will enable these to be garbage collected sooner, reducing the risk of OOM.  &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16365989" author="githubbot" created="Thu, 15 Feb 2018 17:36:47 +0000"  >&lt;p&gt;rajinisivaram closed pull request #4570: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6512&quot; title=&quot;Java Producer: Excessive memory usage with compression enabled&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6512&quot;&gt;&lt;del&gt;KAFKA-6512&lt;/del&gt;&lt;/a&gt;: Discard references to buffers used for compression&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4570&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4570&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/KafkaLZ4BlockOutputStream.java b/clients/src/main/java/org/apache/kafka/common/record/KafkaLZ4BlockOutputStream.java&lt;br/&gt;
index 8cfc37be826..591ab169364 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/KafkaLZ4BlockOutputStream.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/KafkaLZ4BlockOutputStream.java&lt;br/&gt;
@@ -16,7 +16,6 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.record;&lt;/p&gt;

&lt;p&gt;-import java.io.FilterOutputStream;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.io.OutputStream;&lt;/p&gt;

&lt;p&gt;@@ -34,7 +33,7 @@&lt;br/&gt;
  *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This class is not thread-safe.&lt;br/&gt;
  */&lt;br/&gt;
-public final class KafkaLZ4BlockOutputStream extends FilterOutputStream {&lt;br/&gt;
+public final class KafkaLZ4BlockOutputStream extends OutputStream {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static final int MAGIC = 0x184D2204;&lt;br/&gt;
     public static final int LZ4_MAX_HEADER_LENGTH = 19;&lt;br/&gt;
@@ -52,9 +51,10 @@&lt;br/&gt;
     private final boolean useBrokenFlagDescriptorChecksum;&lt;br/&gt;
     private final FLG flg;&lt;br/&gt;
     private final BD bd;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final byte[] buffer;&lt;/li&gt;
	&lt;li&gt;private final byte[] compressedBuffer;&lt;br/&gt;
     private final int maxBlockSize;&lt;br/&gt;
+    private OutputStream out;&lt;br/&gt;
+    private byte[] buffer;&lt;br/&gt;
+    private byte[] compressedBuffer;&lt;br/&gt;
     private int bufferOffset;&lt;br/&gt;
     private boolean finished;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -71,7 +71,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws IOException&lt;br/&gt;
      */&lt;br/&gt;
     public KafkaLZ4BlockOutputStream(OutputStream out, int blockSize, boolean blockChecksum, boolean useBrokenFlagDescriptorChecksum) throws IOException {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(out);&lt;br/&gt;
+        this.out = out;&lt;br/&gt;
         compressor = LZ4Factory.fastestInstance().fastCompressor();&lt;br/&gt;
         checksum = XXHashFactory.fastestInstance().hash32();&lt;br/&gt;
         this.useBrokenFlagDescriptorChecksum = useBrokenFlagDescriptorChecksum;&lt;br/&gt;
@@ -204,7 +204,6 @@ private void writeBlock() throws IOException {&lt;br/&gt;
     private void writeEndMark() throws IOException 
{
         ByteUtils.writeUnsignedIntLE(out, 0);
         // TODO implement content checksum, update flg.validate()
-        finished = true;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
@@ -259,15 +258,26 @@ private void ensureNotFinished() {&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public void close() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!finished) 
{
-            // basically flush the buffer writing the last block
-            writeBlock();
-            // write the end block and finish the stream
-            writeEndMark();
-        }&lt;/li&gt;
	&lt;li&gt;if (out != null) {&lt;/li&gt;
	&lt;li&gt;out.close();&lt;/li&gt;
	&lt;li&gt;out = null;&lt;br/&gt;
+        try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            if (!finished) {
+                // basically flush the buffer writing the last block
+                writeBlock();
+                // write the end block
+                writeEndMark();
+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; finally {&lt;br/&gt;
+            try {&lt;br/&gt;
+                if (out != null) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    try (OutputStream outStream = out) {
+                        outStream.flush();
+                    }+                }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+            } finally &lt;/p&gt;
{
+                out = null;
+                buffer = null;
+                compressedBuffer = null;
+                finished = true;
+            }
&lt;p&gt;         }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecordsBuilder.java b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecordsBuilder.java&lt;br/&gt;
index a9b57ac22df..6f6404fa2d9 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecordsBuilder.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecordsBuilder.java&lt;br/&gt;
@@ -23,6 +23,7 @@&lt;/p&gt;

&lt;p&gt; import java.io.DataOutputStream;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
+import java.io.OutputStream;&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;/p&gt;

&lt;p&gt; import static org.apache.kafka.common.utils.Utils.wrapNullable;&lt;br/&gt;
@@ -38,11 +39,15 @@&lt;br/&gt;
  */&lt;br/&gt;
 public class MemoryRecordsBuilder {&lt;br/&gt;
     private static final float COMPRESSION_RATE_ESTIMATION_FACTOR = 1.05f;&lt;br/&gt;
+    private static final DataOutputStream CLOSED_STREAM = new DataOutputStream(new OutputStream() {&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void write(int b) throws IOException &lt;/p&gt;
{
+            throw new IllegalStateException(&quot;MemoryRecordsBuilder is closed for record appends&quot;);
+        }
&lt;p&gt;+    });&lt;/p&gt;

&lt;p&gt;     private final TimestampType timestampType;&lt;br/&gt;
     private final CompressionType compressionType;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Used to append records, may compress data on the fly&lt;/li&gt;
	&lt;li&gt;private final DataOutputStream appendStream;&lt;br/&gt;
     // Used to hold a reference to the underlying ByteBuffer so that we can write the record batch header and access&lt;br/&gt;
     // the written bytes. ByteBufferOutputStream allocates a new ByteBuffer if the existing one is not large enough,&lt;br/&gt;
     // so it&apos;s not safe to hold a direct reference to the underlying ByteBuffer.&lt;br/&gt;
@@ -60,7 +65,8 @@&lt;br/&gt;
     // from previous batches before appending any records.&lt;br/&gt;
     private float estimatedCompressionRatio = 1.0F;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private boolean appendStreamIsClosed = false;&lt;br/&gt;
+    // Used to append records, may compress data on the fly&lt;br/&gt;
+    private DataOutputStream appendStream;&lt;br/&gt;
     private boolean isTransactional;&lt;br/&gt;
     private long producerId;&lt;br/&gt;
     private short producerEpoch;&lt;br/&gt;
@@ -265,12 +271,13 @@ public void overrideLastOffset(long lastOffset) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;possible to update the RecordBatch header.&lt;br/&gt;
      */&lt;br/&gt;
     public void closeForRecordAppends() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!appendStreamIsClosed) {&lt;br/&gt;
+        if (appendStream != CLOSED_STREAM) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {             try {
                 appendStream.close();
-                appendStreamIsClosed = true;
             } catch (IOException e) {
                 throw new KafkaException(e);
+            } finally {
+                appendStream = CLOSED_STREAM;
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     }&lt;br/&gt;
@@ -663,7 +670,7 @@ private void recordWritten(long offset, long timestamp, int size) {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private void ensureOpenForRecordAppend() &lt;/p&gt;
{
-        if (appendStreamIsClosed)
+        if (appendStream == CLOSED_STREAM)
             throw new IllegalStateException(&quot;Tried to append a record, but MemoryRecordsBuilder is closed for record appends&quot;);
     }

&lt;p&gt;@@ -738,7 +745,7 @@ public boolean isClosed() {&lt;br/&gt;
     public boolean isFull() &lt;/p&gt;
{
         // note that the write limit is respected only after the first record is added which ensures we can always
         // create non-empty batches (this is used to disable batching when the producer&apos;s batch size is set to 0).
-        return appendStreamIsClosed || (this.numRecords &amp;gt; 0 &amp;amp;&amp;amp; this.writeLimit &amp;lt;= estimatedBytesWritten());
+        return appendStream == CLOSED_STREAM || (this.numRecords &amp;gt; 0 &amp;amp;&amp;amp; this.writeLimit &amp;lt;= estimatedBytesWritten());
     }

&lt;p&gt;     /**&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java&lt;br/&gt;
index c713d17f615..a90fb299c52 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java&lt;br/&gt;
@@ -28,6 +28,7 @@&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
 import java.util.Collection;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
+import java.util.Random;&lt;/p&gt;

&lt;p&gt; import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertFalse;&lt;br/&gt;
@@ -644,6 +645,39 @@ public void shouldThrowIllegalStateExceptionOnAppendWhenAborted() throws Excepti&lt;br/&gt;
         return values;&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testBuffersDereferencedOnClose() {&lt;br/&gt;
+        Runtime runtime = Runtime.getRuntime();&lt;br/&gt;
+        int payloadLen = 1024 * 1024;&lt;br/&gt;
+        ByteBuffer buffer = ByteBuffer.allocate(payloadLen * 2);&lt;br/&gt;
+        byte[] key = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        byte[] value = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;payloadLen&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        new Random().nextBytes(value); // Use random payload so that compressed buffer is large&lt;br/&gt;
+        List&amp;lt;MemoryRecordsBuilder&amp;gt; builders = new ArrayList&amp;lt;&amp;gt;(100);&lt;br/&gt;
+        long startMem = 0;&lt;br/&gt;
+        long memUsed = 0;&lt;br/&gt;
+        int iterations =  0;&lt;br/&gt;
+        while (iterations++ &amp;lt; 100) &lt;/p&gt;
{
+            buffer.rewind();
+            MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V2, compressionType,
+                    TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID,
+                    RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false,
+                    RecordBatch.NO_PARTITION_LEADER_EPOCH, 0);
+            builder.append(1L, new byte[0], value);
+            builder.build();
+            builders.add(builder);
+
+            System.gc();
+            memUsed = runtime.totalMemory() - runtime.freeMemory() - startMem;
+            // Ignore memory usage during initialization
+            if (iterations == 2)
+                startMem = memUsed;
+            else if (iterations &amp;gt; 2 &amp;amp;&amp;amp; memUsed &amp;lt; (iterations - 2) * 1024)
+                break;
+        }
&lt;p&gt;+        assertTrue(&quot;Memory usage too high: &quot; + memUsed, iterations &amp;lt; 100);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     private void verifyRecordsProcessingStats(RecordsProcessingStats processingStats, int numRecords,&lt;br/&gt;
             int numRecordsConverted, long finalBytes, long preConvertedBytes) {&lt;br/&gt;
         assertNotNull(&quot;Records processing info is null&quot;, processingStats);&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16366037" author="rsivaram" created="Thu, 15 Feb 2018 18:02:47 +0000"  >&lt;p&gt;Implemented options 1) and 2) from the description.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12908677" name="KafkaSender.java" size="4756" author="ktinker" created="Wed, 31 Jan 2018 22:53:24 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 39 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3pmhb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>