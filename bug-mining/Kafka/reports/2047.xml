<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:14:54 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7322] Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7322</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The deletion thread will grab the log.lock when it tries to rename log segment and schedule for actual deletion.&lt;/p&gt;

&lt;p&gt;The compaction thread only grabs the log.lock when it tries to replace the original segments with the cleaned segment. The compaction thread doesn&apos;t grab the log when it reads records from the original segments to build offsetmap and new segments. As a result, if both deletion and compaction threads work on the same log partition. We have a race condition.&#160;&lt;/p&gt;

&lt;p&gt;This race happens when the topic cleanup policy is&#160;updated on the fly.&#160;&#160;&lt;/p&gt;

&lt;p&gt;One case to hit this race condition:&lt;/p&gt;

&lt;p&gt;1: topic clean up policy is &quot;compact&quot; initially&#160;&lt;/p&gt;

&lt;p&gt;2: log cleaner (compaction) thread picks up the partition for compaction and still in progress&lt;/p&gt;

&lt;p&gt;3: the topic clean up policy has been updated to &quot;deletion&quot;&lt;/p&gt;

&lt;p&gt;4: retention thread pick up the topic partition and delete some old segments.&lt;/p&gt;

&lt;p&gt;5: log cleaner thread reads from the deleted log and raise an IO exception.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The proposed solution is to use &quot;inprogress&quot; map that cleaner manager has to protect such a race.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13180335">KAFKA-7322</key>
            <summary>Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="xiongqiwu">xiongqi wu</assignee>
                                    <reporter username="xiongqiwu">xiongqi wu</reporter>
                        <labels>
                    </labels>
                <created>Tue, 21 Aug 2018 23:56:16 +0000</created>
                <updated>Wed, 26 Sep 2018 00:48:13 +0000</updated>
                            <resolved>Tue, 18 Sep 2018 06:40:05 +0000</resolved>
                                                    <fixVersion>2.1.0</fixVersion>
                                    <component>log</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16588450" author="dhruvilshah" created="Wed, 22 Aug 2018 06:51:50 +0000"  >&lt;p&gt;Does this issue still exist after &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7278&quot; title=&quot;replaceSegments() should not call asyncDeleteSegment() for segments which have been removed from segments list&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7278&quot;&gt;&lt;del&gt;KAFKA-7278&lt;/del&gt;&lt;/a&gt; was fixed?&lt;/p&gt;</comment>
                            <comment id="16589199" author="xiongqiwu" created="Wed, 22 Aug 2018 18:01:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dhruvilshah&quot; class=&quot;user-hover&quot; rel=&quot;dhruvilshah&quot;&gt;dhruvilshah&lt;/a&gt;&#160; I believe this race condition is different from the race in&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7278&quot; title=&quot;replaceSegments() should not call asyncDeleteSegment() for segments which have been removed from segments list&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7278&quot;&gt;&lt;del&gt;KAFKA-7278&lt;/del&gt;&lt;/a&gt;&#160; .&#160; This race is caused by compaction reading from the logsement while retention thread delete the log segment.&#160;&lt;/p&gt;

&lt;p&gt;Ideally, we should not let compaction and retention threads work on the same partition.&#160;&lt;/p&gt;</comment>
                            <comment id="16589213" author="dhruvilshah" created="Wed, 22 Aug 2018 18:13:47 +0000"  >&lt;p&gt;I think that makes sense, thanks for the confirmation.&lt;/p&gt;</comment>
                            <comment id="16596927" author="githubbot" created="Wed, 29 Aug 2018 23:17:20 +0000"  >&lt;p&gt;xiowu0 opened a new pull request #5591: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7322&quot; title=&quot;Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7322&quot;&gt;&lt;del&gt;KAFKA-7322&lt;/del&gt;&lt;/a&gt;: fix race between log cleaner thread and log retention thread&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5591&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5591&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   This is to address issue described in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7322&quot; title=&quot;Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7322&quot;&gt;&lt;del&gt;KAFKA-7322&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;   To fix race condition between log retention and log cleaner when dynamically switching topic cleanup policy,  existing log cleaner inprogress map is used to prevent more than one thread working on the same topic partition. &lt;/p&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16618552" author="githubbot" created="Tue, 18 Sep 2018 06:40:53 +0000"  >&lt;p&gt;lindong28 closed pull request #5591: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7322&quot; title=&quot;Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7322&quot;&gt;&lt;del&gt;KAFKA-7322&lt;/del&gt;&lt;/a&gt;: Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5591&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5591&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/LogCleaner.scala b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
index 91ddbf09305..0b4abe80ef1 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
@@ -36,7 +36,7 @@ import org.apache.kafka.common.record._&lt;br/&gt;
 import org.apache.kafka.common.utils.Time&lt;/p&gt;

&lt;p&gt; import scala.collection.JavaConverters._&lt;br/&gt;
-import scala.collection.&lt;/p&gt;
{Set, mutable}
&lt;p&gt;+import scala.collection.&lt;/p&gt;
{Iterable, Set, mutable}

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The cleaner is responsible for removing obsolete records from logs which have the &quot;compact&quot; retention strategy.&lt;br/&gt;
@@ -219,10 +219,10 @@ class LogCleaner(initialConfig: CleanerConfig,&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  Resume the cleaning of a paused partition. This call blocks until the cleaning of a partition is resumed.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def resumeCleaning(topicPartition: TopicPartition) {&lt;/li&gt;
	&lt;li&gt;cleanerManager.resumeCleaning(topicPartition)&lt;br/&gt;
+    *  Resume the cleaning of paused partitions.&lt;br/&gt;
+    */&lt;br/&gt;
+  def resumeCleaning(topicPartitions: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;) 
{
+    cleanerManager.resumeCleaning(topicPartitions)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -246,6 +246,15 @@ class LogCleaner(initialConfig: CleanerConfig,&lt;br/&gt;
     isCleaned&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  /**&lt;br/&gt;
+    * To prevent race between retention and compaction,&lt;br/&gt;
+    * retention threads need to make this call to obtain:&lt;br/&gt;
+    * @return A list of log partitions that retention threads can safely work on&lt;br/&gt;
+    */&lt;br/&gt;
+  def pauseCleaningForNonCompactedPartitions(): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, Log)&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
+    cleanerManager.pauseCleaningForNonCompactedPartitions()
+  }
&lt;p&gt;+&lt;br/&gt;
   // Only for testing&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; def currentConfig: CleanerConfig = config&lt;/p&gt;

&lt;p&gt;@@ -315,14 +324,16 @@ class LogCleaner(initialConfig: CleanerConfig,&lt;br/&gt;
           true&lt;br/&gt;
       }&lt;br/&gt;
       val deletable: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, Log)&amp;#93;&lt;/span&gt; = cleanerManager.deletableLogs()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;deletable.foreach{&lt;/li&gt;
	&lt;li&gt;case (topicPartition, log) =&amp;gt;&lt;/li&gt;
	&lt;li&gt;try {&lt;br/&gt;
+&lt;br/&gt;
+      try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        deletable.foreach {
+          case (_, log) =&amp;gt;
             log.deleteOldSegments()
-          } finally {
-            cleanerManager.doneDeleting(topicPartition)
-          }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+      } finally &lt;/p&gt;
{
+        cleanerManager.doneDeleting(deletable.map(_._1))
       }
&lt;p&gt;+&lt;br/&gt;
       if (!cleaned)&lt;br/&gt;
         pause(config.backOffMs, TimeUnit.MILLISECONDS)&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogCleanerManager.scala b/core/src/main/scala/kafka/log/LogCleanerManager.scala&lt;br/&gt;
index ba8d7c7e9c0..83d902f952a 100755&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/LogCleanerManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogCleanerManager.scala&lt;br/&gt;
@@ -32,7 +32,7 @@ import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.utils.Time&lt;br/&gt;
 import org.apache.kafka.common.errors.KafkaStorageException&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import scala.collection.&lt;/p&gt;
{immutable, mutable}
&lt;p&gt;+import scala.collection.&lt;/p&gt;
{Iterable, immutable, mutable}

&lt;p&gt; private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; sealed trait LogCleaningState&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; case object LogCleaningInProgress extends LogCleaningState&lt;br/&gt;
@@ -148,6 +148,28 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class LogCleanerManager(val logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  /**&lt;br/&gt;
+    * Pause logs cleaning for logs that do not have compaction enabled&lt;br/&gt;
+    * and do not have other deletion or compaction in progress.&lt;br/&gt;
+    * This is to handle potential race between retention and cleaner threads when users&lt;br/&gt;
+    * switch topic configuration between compacted and non-compacted topic.&lt;br/&gt;
+    * @return retention logs that have log cleaning successfully paused&lt;br/&gt;
+    */&lt;br/&gt;
+  def pauseCleaningForNonCompactedPartitions(): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, Log)&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    inLock(lock) {&lt;br/&gt;
+      val deletableLogs = logs.filter &lt;/p&gt;
{
+        case (_, log) =&amp;gt; !log.config.compact // pick non-compacted logs
+      }
&lt;p&gt;.filterNot &lt;/p&gt;
{
+        case (topicPartition, _) =&amp;gt; inProgress.contains(topicPartition) // skip any logs already in-progress
+      }
&lt;p&gt;+&lt;br/&gt;
+      deletableLogs.foreach &lt;/p&gt;
{
+        case (topicPartition, _) =&amp;gt; inProgress.put(topicPartition, LogCleaningPaused)
+      }
&lt;p&gt;+      deletableLogs&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Find any logs that have compact and delete enabled&lt;br/&gt;
     */&lt;br/&gt;
@@ -170,7 +192,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class LogCleanerManager(val logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
   def abortCleaning(topicPartition: TopicPartition) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     inLock(lock) {
       abortAndPauseCleaning(topicPartition)
-      resumeCleaning(topicPartition)
+      resumeCleaning(Seq(topicPartition))
     }     info(s&amp;quot;The cleaning for partition $topicPartition is aborted&amp;quot;)   }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -206,23 +228,25 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class LogCleanerManager(val logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*  Resume the cleaning of a paused partition. This call blocks until the cleaning of a partition is resumed.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def resumeCleaning(topicPartition: TopicPartition) {&lt;br/&gt;
+    *  Resume the cleaning of paused partitions.&lt;br/&gt;
+    */&lt;br/&gt;
+  def resumeCleaning(topicPartitions: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;){&lt;br/&gt;
     inLock(lock) {&lt;/li&gt;
	&lt;li&gt;inProgress.get(topicPartition) match {&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt;&lt;/li&gt;
	&lt;li&gt;throw new IllegalStateException(s&quot;Compaction for partition $topicPartition cannot be resumed since it is not paused.&quot;)&lt;/li&gt;
	&lt;li&gt;case Some(state) =&amp;gt;&lt;/li&gt;
	&lt;li&gt;state match {&lt;/li&gt;
	&lt;li&gt;case LogCleaningPaused =&amp;gt;&lt;/li&gt;
	&lt;li&gt;inProgress.remove(topicPartition)&lt;/li&gt;
	&lt;li&gt;case s =&amp;gt;&lt;/li&gt;
	&lt;li&gt;throw new IllegalStateException(s&quot;Compaction for partition $topicPartition cannot be resumed since it is in $s state.&quot;)&lt;br/&gt;
+      topicPartitions.foreach {&lt;br/&gt;
+        topicPartition =&amp;gt;&lt;br/&gt;
+          inProgress.get(topicPartition) match 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            case None =&amp;gt;+              throw new IllegalStateException(s&amp;quot;Compaction for partition $topicPartition cannot be resumed since it is not paused.&amp;quot;)+            case Some(state) =&amp;gt;+              state match {
+                case LogCleaningPaused =&amp;gt;
+                  inProgress.remove(topicPartition)
+                case s =&amp;gt;
+                  throw new IllegalStateException(s&quot;Compaction for partition $topicPartition cannot be resumed since it is in $s state.&quot;)
+              }           }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;       }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;info(s&quot;Compaction for partition $topicPartition is resumed&quot;)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -322,18 +346,21 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class LogCleanerManager(val logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def doneDeleting(topicPartition: TopicPartition): Unit = {&lt;br/&gt;
+  def doneDeleting(topicPartitions: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
     inLock(lock) {&lt;/li&gt;
	&lt;li&gt;inProgress.get(topicPartition) match {&lt;/li&gt;
	&lt;li&gt;case Some(LogCleaningInProgress) =&amp;gt;&lt;/li&gt;
	&lt;li&gt;inProgress.remove(topicPartition)&lt;/li&gt;
	&lt;li&gt;case Some(LogCleaningAborted) =&amp;gt;&lt;/li&gt;
	&lt;li&gt;inProgress.put(topicPartition, LogCleaningPaused)&lt;/li&gt;
	&lt;li&gt;pausedCleaningCond.signalAll()&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt;&lt;/li&gt;
	&lt;li&gt;throw new IllegalStateException(s&quot;State for partition $topicPartition should exist.&quot;)&lt;/li&gt;
	&lt;li&gt;case s =&amp;gt;&lt;/li&gt;
	&lt;li&gt;throw new IllegalStateException(s&quot;In-progress partition $topicPartition cannot be in $s state.&quot;)&lt;br/&gt;
+      topicPartitions.foreach 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        topicPartition =&amp;gt;+          inProgress.get(topicPartition) match {
+            case Some(LogCleaningInProgress) =&amp;gt;
+              inProgress.remove(topicPartition)
+            case Some(LogCleaningAborted) =&amp;gt;
+              inProgress.put(topicPartition, LogCleaningPaused)
+              pausedCleaningCond.signalAll()
+            case None =&amp;gt;
+              throw new IllegalStateException(s&quot;State for partition $topicPartition should exist.&quot;)
+            case s =&amp;gt;
+              throw new IllegalStateException(s&quot;In-progress partition $topicPartition cannot be in $s state.&quot;)
+          }       }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     }&lt;br/&gt;
   }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogManager.scala b/core/src/main/scala/kafka/log/LogManager.scala&lt;br/&gt;
index 32203acde9a..eab85098474 100755&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/LogManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogManager.scala&lt;br/&gt;
@@ -515,8 +515,10 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
           if (needToStopCleaner &amp;amp;&amp;amp; !isFuture)&lt;br/&gt;
             cleaner.maybeTruncateCheckpoint(log.dir.getParentFile, topicPartition, log.activeSegment.baseOffset)&lt;br/&gt;
         } finally {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;if (needToStopCleaner &amp;amp;&amp;amp; !isFuture)&lt;/li&gt;
	&lt;li&gt;cleaner.resumeCleaning(topicPartition)&lt;br/&gt;
+          if (needToStopCleaner &amp;amp;&amp;amp; !isFuture) 
{
+            cleaner.resumeCleaning(Seq(topicPartition))
+            info(s&quot;Compaction for partition $topicPartition is resumed&quot;)
+          }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
@@ -547,7 +549,8 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       log.truncateFullyAndStartAt(newOffset)&lt;br/&gt;
       if (cleaner != null &amp;amp;&amp;amp; !isFuture) &lt;/p&gt;
{
         cleaner.maybeTruncateCheckpoint(log.dir.getParentFile, topicPartition, log.activeSegment.baseOffset)
-        cleaner.resumeCleaning(topicPartition)
+        cleaner.resumeCleaning(Seq(topicPartition))
+        info(s&quot;Compaction for partition $topicPartition is resumed&quot;)
       }
&lt;p&gt;       checkpointLogRecoveryOffsetsInDir(log.dir.getParentFile)&lt;br/&gt;
     }&lt;br/&gt;
@@ -785,7 +788,8 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       currentLogs.put(topicPartition, destLog)&lt;br/&gt;
       if (cleaner != null) &lt;/p&gt;
{
         cleaner.alterCheckpointDir(topicPartition, sourceLog.dir.getParentFile, destLog.dir.getParentFile)
-        cleaner.resumeCleaning(topicPartition)
+        cleaner.resumeCleaning(Seq(topicPartition))
+        info(s&quot;Compaction for partition $topicPartition is resumed&quot;)
       }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       try {&lt;br/&gt;
@@ -869,10 +873,38 @@ class LogManager(logDirs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     debug(&quot;Beginning log cleanup...&quot;)&lt;br/&gt;
     var total = 0&lt;br/&gt;
     val startMs = time.milliseconds&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for(log &amp;lt;- allLogs; if !log.config.compact) {&lt;/li&gt;
	&lt;li&gt;debug(&quot;Garbage collecting &apos;&quot; + log.name + &quot;&apos;&quot;)&lt;/li&gt;
	&lt;li&gt;total += log.deleteOldSegments()&lt;br/&gt;
+&lt;br/&gt;
+    // clean current logs.&lt;br/&gt;
+    val deletableLogs = {&lt;br/&gt;
+      if (cleaner != null) 
{
+        // prevent cleaner from working on same partitions when changing cleanup policy
+        cleaner.pauseCleaningForNonCompactedPartitions()
+      }
&lt;p&gt; else &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        currentLogs.filter {
+          case (_, log) =&amp;gt; !log.config.compact
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     }&lt;br/&gt;
+&lt;br/&gt;
+    try {&lt;br/&gt;
+      deletableLogs.foreach &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        case (topicPartition, log) =&amp;gt;+          debug(&amp;quot;Garbage collecting &amp;#39;&amp;quot; + log.name + &amp;quot;&amp;#39;&amp;quot;)+          total += log.deleteOldSegments()++          val futureLog = futureLogs.get(topicPartition)+          if (futureLog != null) {
+            // clean future logs
+            debug(&quot;Garbage collecting future log &apos;&quot; + futureLog.name + &quot;&apos;&quot;)
+            total += futureLog.deleteOldSegments()
+          }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    } finally &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      if (cleaner != null) {
+        cleaner.resumeCleaning(deletableLogs.map(_._1))
+      }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
     debug(&quot;Log cleanup completed. &quot; + total + &quot; files deleted in &quot; +&lt;br/&gt;
                   (time.milliseconds - startMs) / 1000 + &quot; seconds&quot;)&lt;br/&gt;
   }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala&lt;br/&gt;
index 7455763f5b7..8cb2f9ec874 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala&lt;br/&gt;
@@ -90,6 +90,58 @@ class LogCleanerManagerTest extends JUnitSuite with Logging 
{
     assertEquals(&quot;should have 1 logs ready to be deleted&quot;, 0, readyToDelete)
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+    * log with retention in progress should not be picked up for compaction and vice versa when log cleanup policy&lt;br/&gt;
+    * is changed between &quot;compact&quot; and &quot;delete&quot;&lt;br/&gt;
+    */&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testLogsWithRetentionInprogressShouldNotPickedUpForCompactionAndViceVersa(): Unit = &lt;/p&gt;
{
+    val records = TestUtils.singletonRecords(&quot;test&quot;.getBytes, key=&quot;test&quot;.getBytes)
+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Delete)
+    val cleanerManager: LogCleanerManager = createCleanerManager(log)
+
+    log.appendAsLeader(records, leaderEpoch = 0)
+    log.roll()
+    log.appendAsLeader(records, leaderEpoch = 0)
+    log.onHighWatermarkIncremented(2L)
+
+    // simulate retention thread working on the log partition
+    val deletableLog = cleanerManager.pauseCleaningForNonCompactedPartitions()
+    assertEquals(&quot;should have 1 logs ready to be deleted&quot;, 1, deletableLog.size)
+
+    // change cleanup policy from delete to compact
+    val logProps = new Properties()
+    logProps.put(LogConfig.SegmentBytesProp, log.config.segmentSize)
+    logProps.put(LogConfig.RetentionMsProp, log.config.retentionMs)
+    logProps.put(LogConfig.CleanupPolicyProp, LogConfig.Compact)
+    logProps.put(LogConfig.MinCleanableDirtyRatioProp, 0: Integer)
+    val config = LogConfig(logProps)
+    log.config = config
+
+    // log retention inprogress, the log is not available for compaction
+    val cleanable = cleanerManager.grabFilthiestCompactedLog(time)
+    assertEquals(&quot;should have 0 logs ready to be compacted&quot;, 0, cleanable.size)
+
+    // log retention finished, and log can be picked up for compaction
+    cleanerManager.resumeCleaning(deletableLog.map(_._1))
+    val cleanable2 = cleanerManager.grabFilthiestCompactedLog(time)
+    assertEquals(&quot;should have 1 logs ready to be compacted&quot;, 1, cleanable2.size)
+
+    // update cleanup policy to delete
+    logProps.put(LogConfig.CleanupPolicyProp, LogConfig.Delete)
+    val config2 = LogConfig(logProps)
+    log.config = config2
+
+    // compaction in progress, should have 0 log eligible for log retention
+    val deletableLog2 = cleanerManager.pauseCleaningForNonCompactedPartitions()
+    assertEquals(&quot;should have 0 logs ready to be deleted&quot;, 0, deletableLog2.size)
+
+    // compaction done, should have 1 log eligible for log retention
+    cleanerManager.doneDeleting(Seq(cleanable2.get.topicPartition))
+    val deletableLog3 = cleanerManager.pauseCleaningForNonCompactedPartitions()
+    assertEquals(&quot;should have 1 logs ready to be deleted&quot;, 1, deletableLog3.size)
+  }
&lt;p&gt;+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Test computation of cleanable range with no minimum compaction lag settings active&lt;br/&gt;
     */&lt;br/&gt;
@@ -250,17 +302,17 @@ class LogCleanerManagerTest extends JUnitSuite with Logging 
{
 
     val tp = new TopicPartition(&quot;log&quot;, 0)
 
-    intercept[IllegalStateException](cleanerManager.doneDeleting(tp))
+    intercept[IllegalStateException](cleanerManager.doneDeleting(Seq(tp)))
 
     cleanerManager.setCleaningState(tp, LogCleaningPaused)
-    intercept[IllegalStateException](cleanerManager.doneDeleting(tp))
+    intercept[IllegalStateException](cleanerManager.doneDeleting(Seq(tp)))
 
     cleanerManager.setCleaningState(tp, LogCleaningInProgress)
-    cleanerManager.doneDeleting(tp)
+    cleanerManager.doneDeleting(Seq(tp))
     assertTrue(cleanerManager.cleaningState(tp).isEmpty)
 
     cleanerManager.setCleaningState(tp, LogCleaningAborted)
-    cleanerManager.doneDeleting(tp)
+    cleanerManager.doneDeleting(Seq(tp))
     assertEquals(LogCleaningPaused, cleanerManager.cleaningState(tp).get)
 
   }&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16628093" author="githubbot" created="Wed, 26 Sep 2018 00:48:13 +0000"  >&lt;p&gt;xiowu0 opened a new pull request #5694: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7322&quot; title=&quot;Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7322&quot;&gt;&lt;del&gt;KAFKA-7322&lt;/del&gt;&lt;/a&gt;; This is a followup Fix to previous patch.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5694&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5694&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7322&quot; title=&quot;Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7322&quot;&gt;&lt;del&gt;KAFKA-7322&lt;/del&gt;&lt;/a&gt;; This is a followup Fix. With previous fix, log retention thread can throw illegal state exception when race against topic deletion or truncation.&lt;/p&gt;

&lt;p&gt;   The following race can happen: 1) log retention set a topic partition to paused state 2) topic deletion come and see it is already in paused state and proceed 3) topic deletion removed the paused state 4)log retention tries to resume the same topic partition from a NONE state and throw out an exception.&lt;/p&gt;

&lt;p&gt;    In order to fix this situation, we allow a topic partition to be paused multiple times.  In addition, a concurrent unit test is added to test race condition.&lt;/p&gt;


&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   Special cooked high concurrent unit test.&lt;br/&gt;
   Ingesting latency to make the race condition happen in broker, and observed both the race happen and the current implementation handled it correctly.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 7 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3xa7z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>