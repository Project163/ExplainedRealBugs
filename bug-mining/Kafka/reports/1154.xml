<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:55:50 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-3894] LogCleaner thread crashes if not even one segment can fit in the offset map</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-3894</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The log-cleaner thread can crash if the number of keys in a topic grows to be too large to fit into the dedupe buffer. &lt;/p&gt;

&lt;p&gt;The result of this is a log line: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;broker=0 pri=ERROR t=kafka-log-cleaner-thread-0 at=LogCleaner [kafka-log-cleaner-thread-0], Error due to  java.lang.IllegalArgumentException: requirement failed: 9750860 messages in segment MY_FAVORITE_TOPIC-2/00000000000047580165.log but offset map can fit only 5033164. You can increase log.cleaner.dedupe.buffer.size or decrease log.cleaner.threads&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As a result, the broker is left in a potentially dangerous situation where cleaning of compacted topics is not running. &lt;/p&gt;

&lt;p&gt;It is unclear if the broader strategy for the &lt;tt&gt;LogCleaner&lt;/tt&gt; is the reason for this upper bound, or if this is a value which must be tuned for each specific use-case. &lt;/p&gt;

&lt;p&gt;Of more immediate concern is the fact that the thread crash is not visible via JMX or exposed as some form of service degradation. &lt;/p&gt;

&lt;p&gt;Some short-term remediations we have made are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;increasing the size of the dedupe buffer&lt;/li&gt;
	&lt;li&gt;monitoring the log-cleaner threads inside the JVM&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment>Oracle JDK 8&lt;br/&gt;
Ubuntu Precise</environment>
        <key id="12982130">KAFKA-3894</key>
            <summary>LogCleaner thread crashes if not even one segment can fit in the offset map</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tcrayford-heroku">Tom Crayford</assignee>
                                    <reporter username="halorgium">Tim Carey-Smith</reporter>
                        <labels>
                            <label>compaction</label>
                    </labels>
                <created>Wed, 22 Jun 2016 22:31:38 +0000</created>
                <updated>Wed, 30 Aug 2017 16:51:09 +0000</updated>
                            <resolved>Mon, 22 Aug 2016 17:58:58 +0000</resolved>
                                    <version>0.8.2.2</version>
                    <version>0.9.0.1</version>
                    <version>0.10.0.0</version>
                                    <fixVersion>0.10.1.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>15</watches>
                                                                                                                <comments>
                            <comment id="15346463" author="tcrayford-heroku" created="Thu, 23 Jun 2016 13:56:55 +0000"  >&lt;p&gt;(disclaimer: I work with Tim)&lt;/p&gt;

&lt;p&gt;It feels like there are a few pieces of work to do here:&lt;/p&gt;

&lt;p&gt;1. Expose if the log cleaner state as a JMX metric (like BrokerState)&lt;br/&gt;
2. Somehow mark logs we&apos;ve failed to clean as &quot;busted&quot; somewhere, and stop trying to clean them. This way instead of erroring when this occurs the broker doesn&apos;t stay completely busted, but continues on working on all other partitions&lt;br/&gt;
3. I&apos;m unsure, but is it possible to fix the underlying issue by only compacting partial segments of the log when the buffer size is smaller than the desired offset map? This seems like the hardest but most valuable fix here.&lt;/p&gt;

&lt;p&gt;We&apos;re happy picking up at least some of these, but would love feedback from the community about priorities and ease/appropriateness of these steps (and suggestions for other things to have).&lt;/p&gt;</comment>
                            <comment id="15346485" author="ijuma" created="Thu, 23 Jun 2016 14:06:50 +0000"  >&lt;p&gt;The exception looks like the one in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3587&quot; title=&quot;LogCleaner fails due to incorrect offset map computation on a replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3587&quot;&gt;&lt;del&gt;KAFKA-3587&lt;/del&gt;&lt;/a&gt;, which was fixed in 0.10.0.0.&lt;/p&gt;</comment>
                            <comment id="15346580" author="bigandy" created="Thu, 23 Jun 2016 15:11:26 +0000"  >&lt;p&gt;Yep, we&apos;ve ran into the same issue.&lt;/p&gt;

&lt;p&gt;Would be nice if the cleaner, at the very minimum, skipped the segments with large number of records.&lt;/p&gt;</comment>
                            <comment id="15346583" author="bigandy" created="Thu, 23 Jun 2016 15:12:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; Not sure this is related. Our situation seems to be just a legitimate large number of records in the segment.&lt;/p&gt;</comment>
                            <comment id="15346696" author="wushujames" created="Thu, 23 Jun 2016 16:24:15 +0000"  >&lt;p&gt;About log compaction JMX metrics, there is &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3857&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3857&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15360062" author="davispw" created="Sat, 2 Jul 2016 08:11:52 +0000"  >&lt;p&gt;A quick improvement would be to increase the severity of the log message when log cleaner stops. Right now there is just an &quot;INFO&quot; message that&apos;s easy to miss.&lt;/p&gt;</comment>
                            <comment id="15365026" author="kiranp" created="Wed, 6 Jul 2016 20:29:32 +0000"  >&lt;p&gt;Regarding Log cleaner JMX metrics, I just submitted a PR. Please take a look:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/1593&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1593&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;JIRA: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3857&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3857&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15365233" author="halorgium" created="Wed, 6 Jul 2016 22:20:11 +0000"  >&lt;p&gt;Woohoo, more metrics is so excellent!&lt;/p&gt;

&lt;p&gt;Regarding the issue I am reporting: it is somewhat broader than the specific issues related to the log cleaner which have been resolved across the lifetime of Kafka.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Compaction thread dies when hitting compressed and unkeyed messages (&lt;a href=&quot;https://github.com/apache/kafka/commit/1cd6ed9e2c07a63474ed80a8224bd431d5d4243c#diff-d7330411812d23e8a34889bee42fedfe&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/commit/1cd6ed9e2c07a63474ed80a8224bd431d5d4243c#diff-d7330411812d23e8a34889bee42fedfe&lt;/a&gt;) noted in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1755&quot; title=&quot;Improve error handling in log cleaner&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1755&quot;&gt;&lt;del&gt;KAFKA-1755&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Logcleaner fails due to incorrect offset map computation on a replica in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3587&quot; title=&quot;LogCleaner fails due to incorrect offset map computation on a replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3587&quot;&gt;&lt;del&gt;KAFKA-3587&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Unfortunately, there is a deeper issue: if these threads die, bad things happen.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3587&quot; title=&quot;LogCleaner fails due to incorrect offset map computation on a replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3587&quot;&gt;&lt;del&gt;KAFKA-3587&lt;/del&gt;&lt;/a&gt; was a great step forward, now this exception will only occur if a single segment is unable to fit within the dedupe buffer. Unfortunately, in pathological cases the thread could still die. &lt;/p&gt;

&lt;p&gt;Compacted topics are built to rely on the log cleaner thread and because of this, any segments which are written must be compatible with the configuration for log cleaner threads. &lt;br/&gt;
As I mentioned before, we are now monitoring the log cleaner threads and as a result do not have long periods where a broker is in a dangerous and degraded state. &lt;br/&gt;
One situation which comes to mind is from a talk at Kafka Summit where the thread was offline for a large period of time. Upon restart, the &lt;tt&gt;__consumer_offsets&lt;/tt&gt; topic took 17 minutes to load. &lt;br/&gt;
&lt;a href=&quot;http://www.slideshare.net/jjkoshy/kafkaesque-days-at-linked-in-in-2015/49&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.slideshare.net/jjkoshy/kafkaesque-days-at-linked-in-in-2015/49&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After talking with Tom, we came up with a few solutions which could help in resolving this issue. &lt;/p&gt;

&lt;p&gt;1) The monitoring suggested in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3857&quot; title=&quot;Additional log cleaner metrics&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3857&quot;&gt;&lt;del&gt;KAFKA-3857&lt;/del&gt;&lt;/a&gt; is a great start and would most definitely help with determining the state of the log cleaner.&lt;br/&gt;
2) After the change in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3587&quot; title=&quot;LogCleaner fails due to incorrect offset map computation on a replica&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3587&quot;&gt;&lt;del&gt;KAFKA-3587&lt;/del&gt;&lt;/a&gt;, it could be possible to simply leave segments which are too large and leave them as zombie segments which will never be cleaned. This is less than ideal, but means that a single large segment would not take down the whole log cleaner subsystem. &lt;br/&gt;
3) Upon encountering a large segment, we considered the possibility of splitting the segment to allow the log cleaner to continue. This would potentially delay some cleanup until a later time. &lt;br/&gt;
4) Currently, it seems like the write path allows for segments to be created which are unable to be processed by the log cleaner. Would it make sense to include log cleaner heuristics when determining segment size for compacted topics? This would allow the log cleaner to always process a segment, unless the buffer size was changed. &lt;/p&gt;

&lt;p&gt;We&apos;d love to help in any way we can. &lt;/p&gt;</comment>
                            <comment id="15365689" author="wushujames" created="Thu, 7 Jul 2016 06:40:50 +0000"  >&lt;p&gt;#4 is a good point. By looking at the buffer size, the broker can calculate how large a segment it can handle, and can thus make sure to only generate segments that it can handle.&lt;/p&gt;

&lt;p&gt;The comment about having a large segment that you are unable to process made me think about the long discussion that happened in &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3810&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3810&lt;/a&gt;. In that JIRA, a large message in the __consumer_offsets topic would block (internal) consumers who had too small of a fetch size.&lt;/p&gt;

&lt;p&gt;The solution that was chosen and was implemented was to loosen the fetch size for fetches from internal topics. Internal topics would always return at least one message, even if the message was larger than the fetch size.&lt;/p&gt;

&lt;p&gt;It made me wonder if it might make sense to treat the dedupe buffer in a similar way. In a steady state, the configured dedupe buffer size would be used but if it&apos;s too small to even fit a single segment, then the dedupe buffer would be (temporarily) grown to allow cleaning of that large segment.&lt;/p&gt;

&lt;p&gt;CC &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15366339" author="junrao" created="Thu, 7 Jul 2016 16:16:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wushujames&quot; class=&quot;user-hover&quot; rel=&quot;wushujames&quot;&gt;wushujames&lt;/a&gt;, this is slightly different from &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3810&quot; title=&quot;replication of internal topics should not be limited by replica.fetch.max.bytes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3810&quot;&gt;&lt;del&gt;KAFKA-3810&lt;/del&gt;&lt;/a&gt;. In &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3810&quot; title=&quot;replication of internal topics should not be limited by replica.fetch.max.bytes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3810&quot;&gt;&lt;del&gt;KAFKA-3810&lt;/del&gt;&lt;/a&gt;, messages are bounded by MaxMessageSize, which in turn bounds the fetch response size. For cleaning, if messages are uncompressed, the dedupBufferSize needed is bounded by segmentSize/perMessageOverhead. However, if messages are compressed, dedupBufferSize needed could be arbitrarily large. So, I am not sure if we want to auto grow the buffer size arbitrarily. &lt;/p&gt;

&lt;p&gt;#4 seems to be a safer approach. There are effective ways of estimating the number of unique keys (&lt;a href=&quot;https://people.mpi-inf.mpg.de/~rgemulla/publications/beyer07distinct.pdf&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://people.mpi-inf.mpg.de/~rgemulla/publications/beyer07distinct.pdf&lt;/a&gt;) incrementally. We will need to figure out where to store it in order to avoid rescanning the log on startup. &lt;/p&gt;</comment>
                            <comment id="15376858" author="tcrayford-heroku" created="Thu, 14 Jul 2016 12:58:45 +0000"  >&lt;p&gt;Jun:&lt;/p&gt;

&lt;p&gt;#4 seems potentially very complex to me. It also doesn&apos;t work in the case that the broker is shut down and the dedupe buffer size adjusted. I much prefer #3 - it maps fine into the existing model as far as I can tell - we&apos;d &quot;just&quot; split the log file we&apos;re cleaning once the offsetmap is full. That of course requires a little more IO, but it doesn&apos;t involve implementing (or using a library for) sketches that could potentially be incorrect. It also seems like the right long term solution, and more robust than automatically rolling log files some of the time. Am I missing something here? &lt;/p&gt;

&lt;p&gt;Upsides of #3 vs #4:&lt;br/&gt;
We can now clean the largest log segment, no matter the buffer size.&lt;br/&gt;
We don&apos;t increase complexity of the produce path, or change memory usage.&lt;br/&gt;
We don&apos;t have to implement or reuse a library for estimating unique keys&lt;br/&gt;
We don&apos;t have to figure out storing the key estimate (e.g. in the index or in a new file alongside each segment).&lt;/p&gt;

&lt;p&gt;Downsides:&lt;br/&gt;
It would increase the complexity of the cleaner.&lt;br/&gt;
The code that swaps in and out segments will also get more complex, and the crash-safety of that code is already tricky.&lt;/p&gt;

&lt;p&gt;Exists in both:&lt;br/&gt;
Larger log segments could potentially be split a lot, and not always deduplicated that well together. For example, if I write the max number of unique keys for the offset map into a topic, then the segment rolls, then I write a tombstone for every message in the previously sent messages, then neither #3 nor #4 would ever clear up any data. This is no worse than today though.&lt;/p&gt;

&lt;p&gt;Cassandra and other LSM based systems that do log structured storage and over-time compaction use similar &quot;splitting and combining&quot; mechanisms to ensure everything gets cleared up over time without using too much memory. They have a very different storage architecture and goals to Kafka&apos;s compaction, for sure, but it&apos;s interesting to note that they care about similar things.&lt;/p&gt;</comment>
                            <comment id="15382370" author="me@vrischmann.me" created="Mon, 18 Jul 2016 14:38:50 +0000"  >&lt;p&gt;Not adding much to the conversation, but I&apos;ve just been hit by this bug.&lt;/p&gt;

&lt;p&gt;I&apos;m in the process of upgrading my cluster to 0.9.0.1, and in one case the log cleaner dies because of this.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;requirement failed: 1214976153 messages in segment __consumer_offsets-15/00000000000012560043.log but offset map can fit only 40265317&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;If I&apos;m not wrong, there&apos;s no way that much messages can fit in the buffer since it&apos;s limited to 2G anyway per thread. Right now I&apos;m leaving it as is since the broker seems to be working, but it&apos;s not ideal.&lt;/p&gt;

&lt;p&gt;I&apos;m wondering if I simply delete the log file with the broker shut down, will it be fetched at startup from an other replica without problems ?&lt;br/&gt;
In my case, I believe this is only temporary: we never enabled the log cleaner when running 0.8.2.1 (mistake on my part) and now when migrating to 0.9.0.1 it does a giant cleanup at first startup.&lt;/p&gt;</comment>
                            <comment id="15382511" author="davispw" created="Mon, 18 Jul 2016 16:05:42 +0000"  >&lt;p&gt;Re: &quot;the broker seems to be working&quot;&lt;/p&gt;

&lt;p&gt;You may regret not taking action now.  As Tim mentioned from the talk at the Kafka Summit (&lt;a href=&quot;http://www.slideshare.net/jjkoshy/kafkaesque-days-at-linked-in-in-2015/49&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.slideshare.net/jjkoshy/kafkaesque-days-at-linked-in-in-2015/49&lt;/a&gt;), if __consumer_offsets is not compacted and has accumulated millions (or billions!) of messages, it can take many minutes for the broker to elect a new coordinator after any kind of hiccup.  &lt;b&gt;Your new consumers may be hung during this time!&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;However, even shutting down brokers to change the configuration will cause coordinator elections which will cause an outage.  It seems like not having a &quot;hot spare&quot; for Offset Managers is a liability here&#8230;&lt;/p&gt;

&lt;p&gt;We were bit by this bug and it caused all kinds of headaches until we managed to get __consumer_offsets cleaned up again.&lt;/p&gt;</comment>
                            <comment id="15382516" author="me@vrischmann.me" created="Mon, 18 Jul 2016 16:11:35 +0000"  >&lt;p&gt;Yeah, that&apos;s why I was hoping for a workaround &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Right now it takes a ridiculous amount of time for the broker to load some partitions, it just took like 1h+ to load a 300Gb partition. In that case it didn&apos;t impact production though.&lt;/p&gt;

&lt;p&gt;I believe I have found a workaround in my case, since as said it&apos;s a temporary thing: I note all big partitions (more than a Gb let&apos;s say) and reassign them on brokers that are already cleaned up. The reassignment takes a long time but in the end I think it&apos;ll remove the partition from the problematic broker.&lt;/p&gt;</comment>
                            <comment id="15382917" author="me@vrischmann.me" created="Mon, 18 Jul 2016 19:34:41 +0000"  >&lt;p&gt;Well that doesn&apos;t work, in fact I just realized that the log cleaner threads all died on the migrated brokers. So yep, still need to find a workaround or wait for a fix. How did you manage to cleanup the logs &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davispw&quot; class=&quot;user-hover&quot; rel=&quot;davispw&quot;&gt;davispw&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="15406970" author="junrao" created="Thu, 4 Aug 2016 01:43:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tcrayford-heroku&quot; class=&quot;user-hover&quot; rel=&quot;tcrayford-heroku&quot;&gt;tcrayford-heroku&lt;/a&gt;, I chatted with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkreps&quot; class=&quot;user-hover&quot; rel=&quot;jkreps&quot;&gt;jkreps&lt;/a&gt; on this a bit. There are a couple things that we can do to address this issue.&lt;/p&gt;

&lt;p&gt;a. We can potentially make the allocation of the dedup buffer more dynamic. We can start with something small like 100MB. If needed, we can grow the dedup buffer up to the configured size. This will allow us to set a larger default dedup buffer size (say 1GB). If there are not lots of keys, the broker won&apos;t be using that much memory. This will allow the default configuration to accommodate more keys.&lt;/p&gt;

&lt;p&gt;b. To handle the edge case where a segment still has more keys than the increased dedup buffer can handle. We can do the #3 approach as you suggested. Basically, if the dedup buffer is full when only a partial segment is loaded, we remember the next offset (say L). We scan all old log segments including this one as before. The only difference is that when scanning the last segment, we force creating a new segment starting at offset L and simply copy the existing messages after L to the new segment. Then, after we swapped in the new segments, we will move the cleaner marker to offset L. This adds a bit of inefficiency since we have to scan the last swapped-in segment again. However, this will allow the cleaner to always make progress regardless of the # of keys. I am not sure that I understand the case you mentioned that won&apos;t work in both approach #3 and #4.&lt;/p&gt;</comment>
                            <comment id="15408036" author="ijuma" created="Thu, 4 Aug 2016 16:17:55 +0000"  >&lt;p&gt;I updated the title to match the issue that is still present in 0.10.0.x. Note that the log message in 0.10.0.x would be different from the one posted in the JIRA description:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;require(offset &amp;gt; start, &lt;span class=&quot;code-quote&quot;&gt;&quot;Unable to build the offset map &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; segment %s/%s. You can increase log.cleaner.dedupe.buffer.size or decrease log.cleaner.threads&quot;&lt;/span&gt;.format(log.name, segment.log.file.getName))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It would be good to have a fix for 0.10.1.0 so I set the fix version. Tim and Tom, any of you interested in picking this up?&lt;/p&gt;
</comment>
                            <comment id="15412430" author="eliasdorneles" created="Mon, 8 Aug 2016 20:44:44 +0000"  >&lt;p&gt;I&apos;ve bumped the bumped into this same issue (log cleaner threads dying because messages wouldn&apos;t fit the offset map).&lt;/p&gt;

&lt;p&gt;For some of the topics the messages would almost fit, so I was able to get away just increasing the dedupe buffer load factor (&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/KafkaConfig.scala#L252&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/KafkaConfig.scala#L252&lt;/a&gt;) which defaults to 90% of the 2Gb max buffer size.&lt;/p&gt;

&lt;p&gt;For other topics that had more messages and wouldn&apos;t fit in the 2Gb in any way, the workaround was to:&lt;/p&gt;

&lt;p&gt;1) decrease the segment size config for that topic &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;br/&gt;
2) reassign topic partitions, in order to end up with new segments with sizes obeying the config change&lt;br/&gt;
3) rolling restart the nodes, to restart log cleaner threads&lt;/p&gt;

&lt;p&gt;I&apos;d love to know if there is another way of doing this, step 3 is particularly frustrating.&lt;/p&gt;

&lt;p&gt;Good luck!&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;: This can be done for a particular topic with: `kafka-topics.sh --zookeeper $ZK --topic $TOPIC --alter --config segment.bytes`, but if needed you can also set `log.segment.bytes` for topics across all cluster.&lt;/p&gt;</comment>
                            <comment id="15417675" author="tcrayford-heroku" created="Thu, 11 Aug 2016 17:55:46 +0000"  >&lt;p&gt;Hi Jun,&lt;/p&gt;

&lt;p&gt;We&apos;re probably going to start on b. for now. I think a. is incredibly valuable, but it doesn&apos;t impact this manner of the log cleaner crashing. I think there are some cases where we will fail to clean up data, but having those exist seems far more preferable than crashing the thread entirely.&lt;/p&gt;

&lt;p&gt;We&apos;ll get started with b., hopefully will have a patch up within a few business days.&lt;/p&gt;</comment>
                            <comment id="15419166" author="githubbot" created="Fri, 12 Aug 2016 17:31:17 +0000"  >&lt;p&gt;GitHub user tcrayford opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1725&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1725&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    WIP &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3894&quot; title=&quot;LogCleaner thread crashes if not even one segment can fit in the offset map&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3894&quot;&gt;&lt;del&gt;KAFKA-3894&lt;/del&gt;&lt;/a&gt;: split log segment to avoid crashing cleaner thread&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3894&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-3894&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This is a temporary PR, to see what Jenkins has to say about this work in progress change. It will be updated and should not be reviewed at this time.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/heroku/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/heroku/kafka&lt;/a&gt; dont_crash_log_cleaner_thread_if_segment_overflows_buffer&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1725.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1725.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1725&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 995e32398c40c5d4deddeb9e5f359dc5df770b27&lt;br/&gt;
Author: Tom Crayford &amp;lt;tcrayford@googlemail.com&amp;gt;&lt;br/&gt;
Date:   2016-08-12T17:20:26Z&lt;/p&gt;

&lt;p&gt;    WIP &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3894&quot; title=&quot;LogCleaner thread crashes if not even one segment can fit in the offset map&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3894&quot;&gt;&lt;del&gt;KAFKA-3894&lt;/del&gt;&lt;/a&gt;: split log segment to avoid crashing cleaner thread&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15431302" author="junrao" created="Mon, 22 Aug 2016 17:58:58 +0000"  >&lt;p&gt;Issue resolved by pull request 1725&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/1725&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1725&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15431303" author="githubbot" created="Mon, 22 Aug 2016 17:59:19 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/1725&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1725&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15431313" author="junrao" created="Mon, 22 Aug 2016 18:05:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tcrayford-heroku&quot; class=&quot;user-hover&quot; rel=&quot;tcrayford-heroku&quot;&gt;tcrayford-heroku&lt;/a&gt;, thanks for the patch. Filed a followup jira &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-4072&quot; title=&quot;improving memory usage in LogCleaner&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-4072&quot;&gt;KAFKA-4072&lt;/a&gt; to improve the memory usage in log cleaner.&lt;/p&gt;</comment>
                            <comment id="15836473" author="williamyu" created="Tue, 24 Jan 2017 19:14:06 +0000"  >&lt;p&gt;I know this bug is resolved, but we just encountered this bug in our 0.9.0.1 Cluster. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2017-01-24 17:17:30,035] ERROR [kafka-log-cleaner-thread-0], Error due to  (kafka.log.LogCleaner)
java.lang.IllegalArgumentException: requirement failed: 13042136566 messages in segment __consumer_offsets-32/00000000000000000000.log but offset map can fit only 5033164. You can increase log.cleaner.dedupe.buffer.size or decrease log.cleaner.threads
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We&apos;ve tried the following attempts to fix:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Increase the log.cleaner.dedupe.buffer.size but the # of messages is greater then MAX_INT and will be beyond the amount of memory we can allocated.&lt;/li&gt;
	&lt;li&gt;Wipe away the partition in question from a single broker and let the data replicate back. Did not work as all the replicas for this partition also have an issue where they cannot compact the topic.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Does anyone else know of another solution to recover this partition? Do I need to just wipe away the whole partition completely?&lt;/p&gt;</comment>
                            <comment id="15836495" author="me@vrischmann.me" created="Tue, 24 Jan 2017 19:32:04 +0000"  >&lt;p&gt;I had the exact same bug, didn&apos;t realize it at first. What I did was simply deleting all 000000... files manually with the broker stopped, and restarting the broker. &lt;/p&gt;

&lt;p&gt;I was betting that it would be fine because it&apos;s the cosumer offsets topic, and chances are the data in that file is useless anyway since my consumers commit constantly, and only fetch offsets while starting up essentially. It&apos;s a little risky, but worked. (and at that time the patch wasn&apos;t available)&lt;/p&gt;

&lt;p&gt;Still have no idea what generated those files though.&lt;/p&gt;</comment>
                            <comment id="15836524" author="williamyu" created="Tue, 24 Jan 2017 19:48:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=me%40vrischmann.me&quot; class=&quot;user-hover&quot; rel=&quot;me@vrischmann.me&quot;&gt;me@vrischmann.me&lt;/a&gt; Thanks for the quick reply. Do you mean you deleted all logs from the partition? Or where you just targeting specific files which were throwing the error?&lt;/p&gt;

&lt;p&gt;I was thinking of shutting down the brokers &amp;amp; consumers and removing the unstable partition and restarting with my `auto.offset.reset=latest` set on my consumers.&lt;/p&gt;</comment>
                            <comment id="15836543" author="me@vrischmann.me" created="Tue, 24 Jan 2017 20:00:48 +0000"  >&lt;p&gt;No not all logs, just the 00000000000000000000.log one. If you notice, Kafka computes the number of messages based on the number in the filename, hence why it reports there is 13042136566 messages in your log, which is almost surely not true. At least it wasn&apos;t for me.&lt;/p&gt;

&lt;p&gt;The file name is just wrong basically. Come to think of it, you could maybe just rename the file to some arbitrary number where you know the difference between the &lt;em&gt;next&lt;/em&gt;  segment number and &lt;em&gt;this&lt;/em&gt; segment buffer is something that would fit in your dedupe buffer ? For example, here your second segment has the number &lt;em&gt;13042136566&lt;/em&gt;, you could rename the 00000000000000000000.log to &lt;em&gt;13042136566 - 1000000&lt;/em&gt; then your offset map only needs to fit 1M offsets which it can do based on your log.&lt;/p&gt;

&lt;p&gt;I&apos;m just thinking out loud here, I didn&apos;t do this but I think it could work, and would be less risky than just deleting all data, maybe.&lt;/p&gt;</comment>
                            <comment id="16147598" author="njain2017" created="Wed, 30 Aug 2017 16:51:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=me%40vrischmann.me&quot; class=&quot;user-hover&quot; rel=&quot;me@vrischmann.me&quot;&gt;me@vrischmann.me&lt;/a&gt; I saw the exact same issue and realized the same thing that the number of messages in the file were much lower than what&apos;s reported by the log-cleaner. It is calculated based on the number in the filename as you suggested. &lt;br/&gt;
I followed the approach that you suggested and renamed the 00000000000000000000.log file with something that fits inside the dedupe buffer( which is larger than the number of messages in the 00000000000000000000.log file) and it the log cleaner starts working again and cleans up the files)&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 11 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2zxsn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>