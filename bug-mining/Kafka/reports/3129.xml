<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:29:14 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-12257]  Consumer mishandles topics deleted and recreated with the same name</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-12257</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7738&quot; title=&quot;Track partition leader epochs in client metadata&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7738&quot;&gt;&lt;del&gt;KAFKA-7738&lt;/del&gt;&lt;/a&gt;, caching of leader epochs (KIP-320) was added to o.a.k.c.Metadata to ignore metadata responses with epochs smaller than the last seen epoch.&lt;/p&gt;

&lt;p&gt;The current implementation can cause problems in cases where a consumer is subscribed to a topic that has been deleted and then recreated with the same name. This is something seen more often in consumers that subscribe to a multitude of topics using a wildcard.&lt;/p&gt;

&lt;p&gt;Currently, when a topic is deleted and the Fetcher receives UNKNOWN_TOPIC_OR_PARTITION, the leader epoch is not cleared. If at a later time while the consumer is still running a topic is created with the same name, the leader epochs are set to 0 for the new topics partitions, and are likely smaller than those for the previous topic. For example, if a broker had restarted during the lifespan of the previous topic, the leader epoch would be at least 1 or 2. In this case the metadata will be ignored since it is incorrectly considered stale. Of course, the user will sometimes get lucky, and if a topic was only recently created so that the epoch is still 0, no problem will occur on recreation. The issue is also not seen when consumers happen to have been restarted in between deletion and recreation.&lt;/p&gt;

&lt;p&gt;The most common side effect of the new metadata being disregarded is that the new partitions end up assigned but the Fetcher is unable to fetch data because it does not know the leaders. When recreating a topic with the same name it is likely that the partition leaders are not the same as for the previous topic, and the number of partitions may even be different. Besides not being able to retrieve data for the new topic, there is a more sinister side effect of the Fetcher triggering a metadata update after the fetch fails. The subsequent update will again ignore the topic&apos;s metadata if the leader epoch is still smaller than the cached value. This metadata refresh loop can continue indefinitely and with a sufficient number of consumers may even put a strain on a cluster since the requests are occurring in a tight loop. This can also be hard for clients to identify since there is nothing logged by default that would indicate what&apos;s happening. Both the Metadata class&apos;s logging of &quot;&lt;em&gt;Not replacing existing epoch&lt;/em&gt;&quot;, and the Fetcher&apos;s logging of &quot;&lt;em&gt;Leader for partition &amp;lt;T-P&amp;gt; is unknown&lt;/em&gt;&quot; are at DEBUG level.&lt;/p&gt;

&lt;p&gt;A second possible side effect was observed where if the consumer is acting as leader of the group and happens to not have any current data for the previous topic, e.g. it was cleared due to a metadata error from a broker failure, then the new topic&apos;s partitions may simply end up unassigned within the group. This is because while the subscription list contains the recreated topic the metadata for it was previously ignored due to the leader epochs. In this case the user would see logs such as:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;WARN o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=myClientId, groupId=myGroup] The following subscribed topics are not assigned to any members: [myTopic]&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Interestingly, I believe the Producer is less affected by this problem since o.a.k.c.p.i.ProducerMetadata explicitly clears knowledge of its topics in retainTopics() after each metadata expiration. ConsumerMetadata does no such thing.&lt;/p&gt;

&lt;p&gt;To reproduce this issue:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Turn on DEBUG logging, e.g. for org.apache.kafka.clients.consumer and org.apache.kafka.clients.Metadata&lt;/li&gt;
	&lt;li&gt;Begin a consumer for a topic (or multiple topics)&lt;/li&gt;
	&lt;li&gt;Restart a broker that happens to be a leader for one of the topic&apos;s partitions&lt;/li&gt;
	&lt;li&gt;Delete the topic&lt;/li&gt;
	&lt;li&gt;Create another topic with the same name&lt;/li&gt;
	&lt;li&gt;Publish data for the new topic&lt;/li&gt;
	&lt;li&gt;The consumer will not receive data for the new topic, and there will be a high rate of metadata requests.&lt;/li&gt;
	&lt;li&gt;The issue can be corrected by restarting the consumer or restarting brokers until leader epochs are large enough&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I believe KIP-516 (unique topic ids) will likely fix this problem, since after those changes the leader epoch map should be keyed off of the topic id, rather than the name.&lt;/p&gt;

&lt;p&gt;One possible workaround with the current version of Kafka is to add code to onPartitionsRevoked() to manually clear leader epochs before each rebalance, e.g.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Map&amp;lt;TopicPartition, &lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;&amp;gt; emptyLeaderEpochs = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HashMap&amp;lt;&amp;gt;();
ConsumerMetadata metadata = (ConsumerMetadata)FieldUtils.readField(consumer, &lt;span class=&quot;code-quote&quot;&gt;&quot;metadata&quot;&lt;/span&gt;, 
&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
FieldUtils.writeField(metadata, &lt;span class=&quot;code-quote&quot;&gt;&quot;lastSeenLeaderEpochs&quot;&lt;/span&gt;, emptyLeaderEpochs, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is not really recommended of course, since besides modifying private consumer state, it defeats the purpose of epochs! It does in a sense revert the consumer to pre-2.2 behavior before leader epochs existed.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13355561">KAFKA-12257</key>
            <summary> Consumer mishandles topics deleted and recreated with the same name</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="Jack-Lee">lqjacklee</assignee>
                                    <reporter username="rleslie">Ryan Leslie</reporter>
                        <labels>
                    </labels>
                <created>Sat, 30 Jan 2021 00:38:01 +0000</created>
                <updated>Wed, 17 Nov 2021 17:57:40 +0000</updated>
                            <resolved>Wed, 17 Nov 2021 17:57:40 +0000</resolved>
                                    <version>2.2.2</version>
                    <version>2.3.1</version>
                    <version>2.4.1</version>
                    <version>2.5.1</version>
                    <version>2.6.1</version>
                    <version>2.7.1</version>
                    <version>2.8.1</version>
                                    <fixVersion>2.8.2</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                    <fixVersion>3.1.0</fixVersion>
                                    <component>consumer</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="17351898" author="jolshan" created="Wed, 26 May 2021 16:03:54 +0000"  >&lt;p&gt;With the changes from &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KIP-516&lt;/a&gt;&#160;there some options to fix this bug. This ticket mentions using the topic ID as a key for the leader epoch map, but another option that might be a bit simpler to implement is checking for a new topic ID in the request.&lt;/p&gt;

&lt;p&gt;There is already work ongoing to update the Fetch path to use topic IDs (&lt;a href=&quot;https://github.com/apache/kafka/pull/9944&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/9944&lt;/a&gt;) and this will include storing the topic ID in the consumer&apos;s metadata cache.&#160; Upon receiving a new metadata response, we can check if the topic ID matches the ID already stored in the cache and set a flag if it has changed. Then, in `updateLatestMetadata`, we have code that checks the epoch:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (currentEpoch == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; || newEpoch &amp;gt;= currentEpoch) {
    log.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Updating last seen epoch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition {} from {} to epoch {} from &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; metadata&quot;&lt;/span&gt;, tp, currentEpoch, newEpoch);
    lastSeenLeaderEpochs.put(tp, newEpoch);
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; Optional.of(partitionMetadata);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;If we include an or ( || ) for the changed topic ID, I believe this will achieve the behavior this ticket is looking for. The lastSeenEpoch will be reset to the epoch of the new topic and we will use the new partitionMetadata.&lt;/p&gt;</comment>
                            <comment id="17437645" author="agencer" created="Tue, 2 Nov 2021 22:44:17 +0000"  >&lt;p&gt;We were able to reproduce this issue in Kafka 2.4.&lt;/p&gt;</comment>
                            <comment id="17441271" author="jolshan" created="Tue, 9 Nov 2021 16:40:51 +0000"  >&lt;p&gt;We should merge the change for 3.1. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; I can take another look at the PR and update it.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="13019984" name="KAFKA-12257-1.patch" size="7014" author="Jack-Lee" created="Thu, 4 Feb 2021 09:01:38 +0000"/>
                            <attachment id="13019821" name="KAFKA-12257.patch" size="6156" author="Jack-Lee" created="Tue, 2 Feb 2021 08:44:40 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 1 week ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0n5jc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>