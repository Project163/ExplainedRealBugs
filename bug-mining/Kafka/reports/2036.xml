<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:14:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7385] Log cleaner crashes when empty batches are retained with idempotent or transactional producers</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7385</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;During log compaction, we retain an empty header if the batch contains the last sequence number for a particular producer. When such headers are the only messages retained, we do not update state such as `maxOffset` in `MemoryRecords#filterTo` causing us to append these into the cleaned segment with `largestOffset` = -1. This throws a `LogSegmentOffsetOverflowException` for a segment that does not actually have an overflow. When we attempt to split the segment, the log cleaner dies.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13183660">KAFKA-7385</key>
            <summary>Log cleaner crashes when empty batches are retained with idempotent or transactional producers</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dhruvilshah">Dhruvil Shah</assignee>
                                    <reporter username="dhruvilshah">Dhruvil Shah</reporter>
                        <labels>
                    </labels>
                <created>Fri, 7 Sep 2018 15:31:17 +0000</created>
                <updated>Fri, 26 Oct 2018 15:43:31 +0000</updated>
                            <resolved>Sun, 9 Sep 2018 01:07:31 +0000</resolved>
                                                    <fixVersion>2.0.1</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16607475" author="apurva" created="Fri, 7 Sep 2018 18:22:38 +0000"  >&lt;p&gt;This is interesting. I would have expected this test &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/tests/kafkatest/tests/core/reassign_partitions_test.py#L105&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/tests/kafkatest/tests/core/reassign_partitions_test.py#L105&lt;/a&gt; to have reproduced this error. I guess the problem is that we don&apos;t check whether the log cleaner dies or not as part of that tests success.&lt;/p&gt;

&lt;p&gt;Do you agree &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dhruvilshah&quot; class=&quot;user-hover&quot; rel=&quot;dhruvilshah&quot;&gt;dhruvilshah&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="16607766" author="githubbot" created="Fri, 7 Sep 2018 23:14:14 +0000"  >&lt;p&gt;dhruvilshah3 opened a new pull request #5623: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7385&quot; title=&quot;Log cleaner crashes when empty batches are retained with idempotent or transactional producers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7385&quot;&gt;&lt;del&gt;KAFKA-7385&lt;/del&gt;&lt;/a&gt;: Fix log cleaner behavior when empty batches are retained&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5623&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5623&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16608264" author="githubbot" created="Sun, 9 Sep 2018 01:01:04 +0000"  >&lt;p&gt;hachikuji closed pull request #5623: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7385&quot; title=&quot;Log cleaner crashes when empty batches are retained with idempotent or transactional producers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7385&quot;&gt;&lt;del&gt;KAFKA-7385&lt;/del&gt;&lt;/a&gt;: Fix log cleaner behavior when empty batches are retained&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5623&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5623&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
index 55a471149c6..af62e09c57e 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
@@ -160,20 +160,14 @@ public FilterResult filterTo(TopicPartition partition, RecordFilter filter, Byte&lt;br/&gt;
     private static FilterResult filterTo(TopicPartition partition, Iterable&amp;lt;MutableRecordBatch&amp;gt; batches,&lt;br/&gt;
                                          RecordFilter filter, ByteBuffer destinationBuffer, int maxRecordBatchSize,&lt;br/&gt;
                                          BufferSupplier decompressionBufferSupplier) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long maxTimestamp = RecordBatch.NO_TIMESTAMP;&lt;/li&gt;
	&lt;li&gt;long maxOffset = -1L;&lt;/li&gt;
	&lt;li&gt;long shallowOffsetOfMaxTimestamp = -1L;&lt;/li&gt;
	&lt;li&gt;int messagesRead = 0;&lt;/li&gt;
	&lt;li&gt;int bytesRead = 0; // bytes processed from `batches`&lt;/li&gt;
	&lt;li&gt;int messagesRetained = 0;&lt;/li&gt;
	&lt;li&gt;int bytesRetained = 0;&lt;br/&gt;
-&lt;br/&gt;
+        FilterResult filterResult = new FilterResult(destinationBuffer);&lt;br/&gt;
         ByteBufferOutputStream bufferOutputStream = new ByteBufferOutputStream(destinationBuffer);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         for (MutableRecordBatch batch : batches) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;bytesRead += batch.sizeInBytes();&lt;br/&gt;
-&lt;br/&gt;
+            long maxOffset = -1L;&lt;br/&gt;
             BatchRetention batchRetention = filter.checkBatchRetention(batch);&lt;br/&gt;
+            filterResult.bytesRead += batch.sizeInBytes();&lt;br/&gt;
+&lt;br/&gt;
             if (batchRetention == BatchRetention.DELETE)&lt;br/&gt;
                 continue;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -189,7 +183,7 @@ private static FilterResult filterTo(TopicPartition partition, Iterable&amp;lt;MutableR&lt;br/&gt;
             try (final CloseableIterator&amp;lt;Record&amp;gt; iterator = batch.streamingIterator(decompressionBufferSupplier)) {&lt;br/&gt;
                 while (iterator.hasNext()) {&lt;br/&gt;
                     Record record = iterator.next();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;messagesRead += 1;&lt;br/&gt;
+                    filterResult.messagesRead += 1;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     if (filter.shouldRetainRecord(batch, record)) {&lt;br/&gt;
                         // Check for log corruption due to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-4298&quot; title=&quot;LogCleaner writes inconsistent compressed message set if topic message format != message format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-4298&quot;&gt;&lt;del&gt;KAFKA-4298&lt;/del&gt;&lt;/a&gt;. If we find it, make sure that we overwrite&lt;br/&gt;
@@ -210,20 +204,11 @@ private static FilterResult filterTo(TopicPartition partition, Iterable&amp;lt;MutableR&lt;br/&gt;
             if (!retainedRecords.isEmpty()) {&lt;br/&gt;
                 if (writeOriginalBatch) {&lt;br/&gt;
                     batch.writeTo(bufferOutputStream);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;messagesRetained += retainedRecords.size();&lt;/li&gt;
	&lt;li&gt;bytesRetained += batch.sizeInBytes();&lt;/li&gt;
	&lt;li&gt;if (batch.maxTimestamp() &amp;gt; maxTimestamp) 
{
-                        maxTimestamp = batch.maxTimestamp();
-                        shallowOffsetOfMaxTimestamp = batch.lastOffset();
-                    }
&lt;p&gt;+                    filterResult.updateRetainedBatchMetadata(batch, retainedRecords.size(), false);&lt;br/&gt;
                 } else {&lt;br/&gt;
                     MemoryRecordsBuilder builder = buildRetainedRecordsInto(batch, retainedRecords, bufferOutputStream);&lt;br/&gt;
                     MemoryRecords records = builder.build();&lt;br/&gt;
                     int filteredBatchSize = records.sizeInBytes();&lt;br/&gt;
-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;messagesRetained += retainedRecords.size();&lt;/li&gt;
	&lt;li&gt;bytesRetained += filteredBatchSize;&lt;br/&gt;
-&lt;br/&gt;
                     if (filteredBatchSize &amp;gt; batch.sizeInBytes() &amp;amp;&amp;amp; filteredBatchSize &amp;gt; maxRecordBatchSize)&lt;br/&gt;
                         log.warn(&quot;Record batch from {} with last offset {} exceeded max record batch size {} after cleaning &quot; +&lt;br/&gt;
                                         &quot;(new size is {}). Consumers with version earlier than 0.10.1.0 may need to &quot; +&lt;br/&gt;
@@ -231,10 +216,8 @@ private static FilterResult filterTo(TopicPartition partition, Iterable&amp;lt;MutableR&lt;br/&gt;
                                 partition, batch.lastOffset(), maxRecordBatchSize, filteredBatchSize);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     MemoryRecordsBuilder.RecordsInfo info = builder.info();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (info.maxTimestamp &amp;gt; maxTimestamp) 
{
-                        maxTimestamp = info.maxTimestamp;
-                        shallowOffsetOfMaxTimestamp = info.shallowOffsetOfMaxTimestamp;
-                    }
&lt;p&gt;+                    filterResult.updateRetainedBatchMetadata(info.maxTimestamp, info.shallowOffsetOfMaxTimestamp,&lt;br/&gt;
+                            maxOffset, retainedRecords.size(), filteredBatchSize);&lt;br/&gt;
                 }&lt;br/&gt;
             } else if (batchRetention == BatchRetention.RETAIN_EMPTY) &lt;/p&gt;
{
                 if (batchMagic &amp;lt; RecordBatch.MAGIC_VALUE_V2)
@@ -245,18 +228,19 @@ private static FilterResult filterTo(TopicPartition partition, Iterable&amp;lt;MutableR
                         batch.producerEpoch(), batch.baseSequence(), batch.baseOffset(), batch.lastOffset(),
                         batch.partitionLeaderEpoch(), batch.timestampType(), batch.maxTimestamp(),
                         batch.isTransactional(), batch.isControlBatch());
+                filterResult.updateRetainedBatchMetadata(batch, 0, true);
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// If we had to allocate a new buffer to fit the filtered output (see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5316&quot; title=&quot;Log cleaning can increase message size and cause cleaner to crash with buffer overflow&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5316&quot;&gt;&lt;del&gt;KAFKA-5316&lt;/del&gt;&lt;/a&gt;), return early to&lt;br/&gt;
+            // If we had to allocate a new buffer to fit the filtered buffer (see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5316&quot; title=&quot;Log cleaning can increase message size and cause cleaner to crash with buffer overflow&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5316&quot;&gt;&lt;del&gt;KAFKA-5316&lt;/del&gt;&lt;/a&gt;), return early to&lt;br/&gt;
             // avoid the need for additional allocations.&lt;br/&gt;
             ByteBuffer outputBuffer = bufferOutputStream.buffer();&lt;/li&gt;
	&lt;li&gt;if (outputBuffer != destinationBuffer)&lt;/li&gt;
	&lt;li&gt;return new FilterResult(outputBuffer, messagesRead, bytesRead, messagesRetained, bytesRetained,&lt;/li&gt;
	&lt;li&gt;maxOffset, maxTimestamp, shallowOffsetOfMaxTimestamp);&lt;br/&gt;
+            if (outputBuffer != destinationBuffer) 
{
+                filterResult.outputBuffer = outputBuffer;
+                return filterResult;
+            }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new FilterResult(destinationBuffer, messagesRead, bytesRead, messagesRetained, bytesRetained,&lt;/li&gt;
	&lt;li&gt;maxOffset, maxTimestamp, shallowOffsetOfMaxTimestamp);&lt;br/&gt;
+        return filterResult;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static MemoryRecordsBuilder buildRetainedRecordsInto(RecordBatch originalBatch,&lt;br/&gt;
@@ -369,33 +353,76 @@ public int hashCode() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static class FilterResult {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public final ByteBuffer output;&lt;/li&gt;
	&lt;li&gt;public final int messagesRead;&lt;/li&gt;
	&lt;li&gt;public final int bytesRead;&lt;/li&gt;
	&lt;li&gt;public final int messagesRetained;&lt;/li&gt;
	&lt;li&gt;public final int bytesRetained;&lt;/li&gt;
	&lt;li&gt;public final long maxOffset;&lt;/li&gt;
	&lt;li&gt;public final long maxTimestamp;&lt;/li&gt;
	&lt;li&gt;public final long shallowOffsetOfMaxTimestamp;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// Note that `bytesRead` should contain only bytes from batches that have been processed,&lt;/li&gt;
	&lt;li&gt;// i.e. bytes from `messagesRead` and any discarded batches.&lt;/li&gt;
	&lt;li&gt;public FilterResult(ByteBuffer output,&lt;/li&gt;
	&lt;li&gt;int messagesRead,&lt;/li&gt;
	&lt;li&gt;int bytesRead,&lt;/li&gt;
	&lt;li&gt;int messagesRetained,&lt;/li&gt;
	&lt;li&gt;int bytesRetained,&lt;/li&gt;
	&lt;li&gt;long maxOffset,&lt;/li&gt;
	&lt;li&gt;long maxTimestamp,&lt;/li&gt;
	&lt;li&gt;long shallowOffsetOfMaxTimestamp) {&lt;/li&gt;
	&lt;li&gt;this.output = output;&lt;/li&gt;
	&lt;li&gt;this.messagesRead = messagesRead;&lt;/li&gt;
	&lt;li&gt;this.bytesRead = bytesRead;&lt;/li&gt;
	&lt;li&gt;this.messagesRetained = messagesRetained;&lt;/li&gt;
	&lt;li&gt;this.bytesRetained = bytesRetained;&lt;/li&gt;
	&lt;li&gt;this.maxOffset = maxOffset;&lt;/li&gt;
	&lt;li&gt;this.maxTimestamp = maxTimestamp;&lt;/li&gt;
	&lt;li&gt;this.shallowOffsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp;&lt;br/&gt;
+        private ByteBuffer outputBuffer;&lt;br/&gt;
+        private int messagesRead = 0;&lt;br/&gt;
+        // Note that `bytesRead` should contain only bytes from batches that have been processed, i.e. bytes from&lt;br/&gt;
+        // `messagesRead` and any discarded batches.&lt;br/&gt;
+        private int bytesRead = 0;&lt;br/&gt;
+        private int messagesRetained = 0;&lt;br/&gt;
+        private int bytesRetained = 0;&lt;br/&gt;
+        private long maxOffset = -1L;&lt;br/&gt;
+        private long maxTimestamp = RecordBatch.NO_TIMESTAMP;&lt;br/&gt;
+        private long shallowOffsetOfMaxTimestamp = -1L;&lt;br/&gt;
+&lt;br/&gt;
+        private FilterResult(ByteBuffer outputBuffer) 
{
+            this.outputBuffer = outputBuffer;
+        }
&lt;p&gt;+&lt;br/&gt;
+        private void updateRetainedBatchMetadata(MutableRecordBatch retainedBatch, int numMessagesInBatch, boolean headerOnly) &lt;/p&gt;
{
+            int bytesRetained = headerOnly ? DefaultRecordBatch.RECORD_BATCH_OVERHEAD : retainedBatch.sizeInBytes();
+            updateRetainedBatchMetadata(retainedBatch.maxTimestamp(), retainedBatch.lastOffset(),
+                    retainedBatch.lastOffset(), numMessagesInBatch, bytesRetained);
+        }
&lt;p&gt;+&lt;br/&gt;
+        private void updateRetainedBatchMetadata(long maxTimestamp, long shallowOffsetOfMaxTimestamp, long maxOffset,&lt;br/&gt;
+                                                int messagesRetained, int bytesRetained) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            validateBatchMetadata(maxTimestamp, shallowOffsetOfMaxTimestamp, maxOffset);+            if (maxTimestamp &amp;gt; this.maxTimestamp) {
+                this.maxTimestamp = maxTimestamp;
+                this.shallowOffsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp;
+            }+            this.maxOffset = Math.max(maxOffset, this.maxOffset);+            this.messagesRetained += messagesRetained;+            this.bytesRetained += bytesRetained;+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+        private void validateBatchMetadata(long maxTimestamp, long shallowOffsetOfMaxTimestamp, long maxOffset) &lt;/p&gt;
{
+            if (maxTimestamp != RecordBatch.NO_TIMESTAMP &amp;amp;&amp;amp; shallowOffsetOfMaxTimestamp &amp;lt; 0)
+                throw new IllegalArgumentException(&quot;shallowOffset undefined for maximum timestamp &quot; + maxTimestamp);
+            if (maxOffset &amp;lt; 0)
+                throw new IllegalArgumentException(&quot;maxOffset undefined&quot;);
+        }
&lt;p&gt;+&lt;br/&gt;
+        public ByteBuffer outputBuffer() &lt;/p&gt;
{
+            return outputBuffer;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public int messagesRead() &lt;/p&gt;
{
+            return messagesRead;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public int bytesRead() &lt;/p&gt;
{
+            return bytesRead;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public int messagesRetained() &lt;/p&gt;
{
+            return messagesRetained;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public int bytesRetained() &lt;/p&gt;
{
+            return bytesRetained;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public long maxOffset() &lt;/p&gt;
{
+            return maxOffset;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public long maxTimestamp() &lt;/p&gt;
{
+            return maxTimestamp;
+        }
&lt;p&gt;+&lt;br/&gt;
+        public long shallowOffsetOfMaxTimestamp() &lt;/p&gt;
{
+            return shallowOffsetOfMaxTimestamp;
         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 1a82faa0a3d..fd550d61da5 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -2113,8 +2113,8 @@ protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) &lt;/p&gt;
{
                 return record.key() != null;
             }
&lt;p&gt;         }, ByteBuffer.allocate(1024), Integer.MAX_VALUE, BufferSupplier.NO_CACHING);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;result.output.flip();&lt;/li&gt;
	&lt;li&gt;MemoryRecords compactedRecords = MemoryRecords.readableRecords(result.output);&lt;br/&gt;
+        result.outputBuffer().flip();&lt;br/&gt;
+        MemoryRecords compactedRecords = MemoryRecords.readableRecords(result.outputBuffer());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         subscriptions.assignFromUser(singleton(tp0));&lt;br/&gt;
         subscriptions.seek(tp0, 0);&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java&lt;br/&gt;
index 61d8a00865b..579fb74b44a 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java&lt;br/&gt;
@@ -252,6 +252,7 @@ public void testFilterToEmptyBatchRetention() {&lt;br/&gt;
                 long baseOffset = 3L;&lt;br/&gt;
                 int baseSequence = 10;&lt;br/&gt;
                 int partitionLeaderEpoch = 293;&lt;br/&gt;
+                int numRecords = 2;&lt;/p&gt;

&lt;p&gt;                 MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME,&lt;br/&gt;
                         baseOffset, RecordBatch.NO_TIMESTAMP, producerId, producerEpoch, baseSequence, isTransactional,&lt;br/&gt;
@@ -259,22 +260,34 @@ public void testFilterToEmptyBatchRetention() {&lt;br/&gt;
                 builder.append(11L, &quot;2&quot;.getBytes(), &quot;b&quot;.getBytes());&lt;br/&gt;
                 builder.append(12L, &quot;3&quot;.getBytes(), &quot;c&quot;.getBytes());&lt;br/&gt;
                 builder.close();&lt;br/&gt;
+                MemoryRecords records = builder.build();&lt;/p&gt;

&lt;p&gt;                 ByteBuffer filtered = ByteBuffer.allocate(2048);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;builder.build().filterTo(new TopicPartition(&quot;foo&quot;, 0), new MemoryRecords.RecordFilter() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;protected BatchRetention checkBatchRetention(RecordBatch batch) 
{
-                        // retain all batches
-                        return BatchRetention.RETAIN_EMPTY;
-                    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) 
{
-                        // delete the records
-                        return false;
-                    }&lt;/li&gt;
	&lt;li&gt;}, filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);&lt;br/&gt;
-&lt;br/&gt;
+                MemoryRecords.FilterResult filterResult = records.filterTo(new TopicPartition(&quot;foo&quot;, 0),&lt;br/&gt;
+                        new MemoryRecords.RecordFilter() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                            @Override+                            protected BatchRetention checkBatchRetention(RecordBatch batch) {
+                                // retain all batches
+                                return BatchRetention.RETAIN_EMPTY;
+                            }++                            @Override+                            protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) {
+                                // delete the records
+                                return false;
+                            }+                        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;, filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);&lt;br/&gt;
+&lt;br/&gt;
+                // Verify filter result&lt;br/&gt;
+                assertEquals(numRecords, filterResult.messagesRead());&lt;br/&gt;
+                assertEquals(records.sizeInBytes(), filterResult.bytesRead());&lt;br/&gt;
+                assertEquals(baseOffset + 1, filterResult.maxOffset());&lt;br/&gt;
+                assertEquals(0, filterResult.messagesRetained());&lt;br/&gt;
+                assertEquals(DefaultRecordBatch.RECORD_BATCH_OVERHEAD, filterResult.bytesRetained());&lt;br/&gt;
+                assertEquals(12, filterResult.maxTimestamp());&lt;br/&gt;
+                assertEquals(baseOffset + 1, filterResult.shallowOffsetOfMaxTimestamp());&lt;br/&gt;
+&lt;br/&gt;
+                // Verify filtered records&lt;br/&gt;
                 filtered.flip();&lt;br/&gt;
                 MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -294,6 +307,55 @@ protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testEmptyBatchRetention() {&lt;br/&gt;
+        if (magic &amp;gt;= RecordBatch.MAGIC_VALUE_V2) {&lt;br/&gt;
+            ByteBuffer buffer = ByteBuffer.allocate(DefaultRecordBatch.RECORD_BATCH_OVERHEAD);&lt;br/&gt;
+            long producerId = 23L;&lt;br/&gt;
+            short producerEpoch = 5;&lt;br/&gt;
+            long baseOffset = 3L;&lt;br/&gt;
+            int baseSequence = 10;&lt;br/&gt;
+            int partitionLeaderEpoch = 293;&lt;br/&gt;
+            long timestamp = System.currentTimeMillis();&lt;br/&gt;
+&lt;br/&gt;
+            DefaultRecordBatch.writeEmptyHeader(buffer, RecordBatch.MAGIC_VALUE_V2, producerId, producerEpoch,&lt;br/&gt;
+                    baseSequence, baseOffset, baseOffset, partitionLeaderEpoch, TimestampType.CREATE_TIME,&lt;br/&gt;
+                    timestamp, false, false);&lt;br/&gt;
+            buffer.flip();&lt;br/&gt;
+&lt;br/&gt;
+            ByteBuffer filtered = ByteBuffer.allocate(2048);&lt;br/&gt;
+            MemoryRecords records = MemoryRecords.readableRecords(buffer);&lt;br/&gt;
+            MemoryRecords.FilterResult filterResult = records.filterTo(new TopicPartition(&quot;foo&quot;, 0),&lt;br/&gt;
+                    new MemoryRecords.RecordFilter() {&lt;br/&gt;
+                        @Override&lt;br/&gt;
+                        protected BatchRetention checkBatchRetention(RecordBatch batch) &lt;/p&gt;
{
+                            // retain all batches
+                            return BatchRetention.RETAIN_EMPTY;
+                        }
&lt;p&gt;+&lt;br/&gt;
+                        @Override&lt;br/&gt;
+                        protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) &lt;/p&gt;
{
+                            return false;
+                        }
&lt;p&gt;+                    }, filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);&lt;br/&gt;
+&lt;br/&gt;
+            // Verify filter result&lt;br/&gt;
+            assertEquals(0, filterResult.messagesRead());&lt;br/&gt;
+            assertEquals(records.sizeInBytes(), filterResult.bytesRead());&lt;br/&gt;
+            assertEquals(baseOffset, filterResult.maxOffset());&lt;br/&gt;
+            assertEquals(0, filterResult.messagesRetained());&lt;br/&gt;
+            assertEquals(DefaultRecordBatch.RECORD_BATCH_OVERHEAD, filterResult.bytesRetained());&lt;br/&gt;
+            assertEquals(timestamp, filterResult.maxTimestamp());&lt;br/&gt;
+            assertEquals(baseOffset, filterResult.shallowOffsetOfMaxTimestamp());&lt;br/&gt;
+            assertTrue(filterResult.outputBuffer().position() &amp;gt; 0);&lt;br/&gt;
+&lt;br/&gt;
+            // Verify filtered records&lt;br/&gt;
+            filtered.flip();&lt;br/&gt;
+            MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);&lt;br/&gt;
+            assertEquals(DefaultRecordBatch.RECORD_BATCH_OVERHEAD, filteredRecords.sizeInBytes());&lt;br/&gt;
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testEmptyBatchDeletion() {&lt;br/&gt;
         if (magic &amp;gt;= RecordBatch.MAGIC_VALUE_V2) {&lt;br/&gt;
@@ -304,25 +366,32 @@ public void testEmptyBatchDeletion() {&lt;br/&gt;
                 long baseOffset = 3L;&lt;br/&gt;
                 int baseSequence = 10;&lt;br/&gt;
                 int partitionLeaderEpoch = 293;&lt;br/&gt;
+                long timestamp = System.currentTimeMillis();&lt;/p&gt;

&lt;p&gt;                 DefaultRecordBatch.writeEmptyHeader(buffer, RecordBatch.MAGIC_VALUE_V2, producerId, producerEpoch,&lt;br/&gt;
                         baseSequence, baseOffset, baseOffset, partitionLeaderEpoch, TimestampType.CREATE_TIME,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;System.currentTimeMillis(), false, false);&lt;br/&gt;
+                        timestamp, false, false);&lt;br/&gt;
                 buffer.flip();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 ByteBuffer filtered = ByteBuffer.allocate(2048);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MemoryRecords.readableRecords(buffer).filterTo(new TopicPartition(&quot;foo&quot;, 0), new MemoryRecords.RecordFilter() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;protected BatchRetention checkBatchRetention(RecordBatch batch) 
{
-                        return deleteRetention;
-                    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) 
{
-                        return false;
-                    }&lt;/li&gt;
	&lt;li&gt;}, filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);&lt;br/&gt;
-&lt;br/&gt;
+                MemoryRecords records = MemoryRecords.readableRecords(buffer);&lt;br/&gt;
+                MemoryRecords.FilterResult filterResult = records.filterTo(new TopicPartition(&quot;foo&quot;, 0),&lt;br/&gt;
+                        new MemoryRecords.RecordFilter() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                            @Override+                            protected BatchRetention checkBatchRetention(RecordBatch batch) {
+                                return deleteRetention;
+                            }++                            @Override+                            protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) {
+                                return false;
+                            }+                        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;, filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);&lt;br/&gt;
+&lt;br/&gt;
+                // Verify filter result&lt;br/&gt;
+                assertEquals(0, filterResult.outputBuffer().position());&lt;br/&gt;
+&lt;br/&gt;
+                // Verify filtered records&lt;br/&gt;
                 filtered.flip();&lt;br/&gt;
                 MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);&lt;br/&gt;
                 assertEquals(0, filteredRecords.sizeInBytes());&lt;br/&gt;
@@ -591,15 +660,15 @@ public void testFilterToWithUndersizedBuffer() &lt;/p&gt;
{
 
             MemoryRecords.FilterResult result = MemoryRecords.readableRecords(buffer)
                     .filterTo(new TopicPartition(&quot;foo&quot;, 0), new RetainNonNullKeysFilter(), output, Integer.MAX_VALUE,
-                              BufferSupplier.NO_CACHING);
+                            BufferSupplier.NO_CACHING);
 
-            buffer.position(buffer.position() + result.bytesRead);
-            result.output.flip();
+            buffer.position(buffer.position() + result.bytesRead());
+            result.outputBuffer().flip();
 
-            if (output != result.output)
+            if (output != result.outputBuffer())
                 assertEquals(0, output.position());
 
-            MemoryRecords filtered = MemoryRecords.readableRecords(result.output);
+            MemoryRecords filtered = MemoryRecords.readableRecords(result.outputBuffer());
             records.addAll(TestUtils.toList(filtered.records()));
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -623,9 +692,9 @@ public void testToString() {&lt;br/&gt;
                 break;&lt;br/&gt;
             case RecordBatch.MAGIC_VALUE_V1:&lt;br/&gt;
                 assertEquals(&quot;[(record=LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, &quot; +&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;crc=97210616, CreateTime=1000000, key=4 bytes, value=6 bytes))), (record=LegacyRecordBatch(offset=1, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;Record(magic=1, attributes=0, compression=NONE, crc=3535988507, CreateTime=1000001, key=4 bytes, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;value=6 bytes)))]&quot;,&lt;br/&gt;
+                                &quot;crc=97210616, CreateTime=1000000, key=4 bytes, value=6 bytes))), (record=LegacyRecordBatch(offset=1, &quot; +&lt;br/&gt;
+                                &quot;Record(magic=1, attributes=0, compression=NONE, crc=3535988507, CreateTime=1000001, key=4 bytes, &quot; +&lt;br/&gt;
+                                &quot;value=6 bytes)))]&quot;,&lt;br/&gt;
                         memoryRecords.toString());&lt;br/&gt;
                 break;&lt;br/&gt;
             case RecordBatch.MAGIC_VALUE_V2:&lt;br/&gt;
@@ -669,16 +738,16 @@ public void testFilterTo() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         filtered.flip();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(7, result.messagesRead);&lt;/li&gt;
	&lt;li&gt;assertEquals(4, result.messagesRetained);&lt;/li&gt;
	&lt;li&gt;assertEquals(buffer.limit(), result.bytesRead);&lt;/li&gt;
	&lt;li&gt;assertEquals(filtered.limit(), result.bytesRetained);&lt;br/&gt;
+        assertEquals(7, result.messagesRead());&lt;br/&gt;
+        assertEquals(4, result.messagesRetained());&lt;br/&gt;
+        assertEquals(buffer.limit(), result.bytesRead());&lt;br/&gt;
+        assertEquals(filtered.limit(), result.bytesRetained());&lt;br/&gt;
         if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V0) 
{
-            assertEquals(20L, result.maxTimestamp);
+            assertEquals(20L, result.maxTimestamp());
             if (compression == CompressionType.NONE &amp;amp;&amp;amp; magic &amp;lt; RecordBatch.MAGIC_VALUE_V2)
-                assertEquals(4L, result.shallowOffsetOfMaxTimestamp);
+                assertEquals(4L, result.shallowOffsetOfMaxTimestamp());
             else
-                assertEquals(5L, result.shallowOffsetOfMaxTimestamp);
+                assertEquals(5L, result.shallowOffsetOfMaxTimestamp());
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogCleaner.scala b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
index 91ddbf09305..04b284ccbd6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
@@ -606,7 +606,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class Cleaner(val id: Int,&lt;br/&gt;
       position += result.bytesRead&lt;/p&gt;

&lt;p&gt;       // if any messages are to be retained, write them out&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val outputBuffer = result.output&lt;br/&gt;
+      val outputBuffer = result.outputBuffer&lt;br/&gt;
       if (outputBuffer.position() &amp;gt; 0) {&lt;br/&gt;
         outputBuffer.flip()&lt;br/&gt;
         val retained = MemoryRecords.readableRecords(outputBuffer)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
index 0240707ca3b..73dfa7e39cd 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
@@ -412,42 +412,57 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     val log = makeLog(config = LogConfig.fromProps(logConfig.originals, logProps))&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val producerEpoch = 0.toShort&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val producerId = 1L&lt;/li&gt;
	&lt;li&gt;val appendProducer = appendTransactionalAsLeader(log, producerId, producerEpoch)&lt;br/&gt;
+    val producer1 = appendTransactionalAsLeader(log, 1L, producerEpoch)&lt;br/&gt;
+    val producer2 = appendTransactionalAsLeader(log, 2L, producerEpoch)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;appendProducer(Seq(2, 3)) // batch last offset is 1&lt;/li&gt;
	&lt;li&gt;log.appendAsLeader(commitMarker(producerId, producerEpoch), leaderEpoch = 0, isFromClient = false)&lt;br/&gt;
+    // [
{Producer1: 2, 3}
&lt;p&gt;]&lt;br/&gt;
+    producer1(Seq(2, 3)) // offsets 0, 1&lt;br/&gt;
     log.roll()&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.appendAsLeader(record(2, 2), leaderEpoch = 0)&lt;/li&gt;
	&lt;li&gt;log.appendAsLeader(record(3, 3), leaderEpoch = 0)&lt;br/&gt;
+    // [
{Producer1: 2, 3}
&lt;p&gt;], [&lt;/p&gt;
{Producer2: 2, 3}
&lt;p&gt;, &lt;/p&gt;
{Producer2: Commit}
&lt;p&gt;]&lt;br/&gt;
+    producer2(Seq(2, 3)) // offsets 2, 3&lt;br/&gt;
+    log.appendAsLeader(commitMarker(2L, producerEpoch), leaderEpoch = 0, isFromClient = false) // offset 4&lt;br/&gt;
+    log.roll()&lt;br/&gt;
+&lt;br/&gt;
+    // [&lt;/p&gt;
{Producer1: 2, 3}
&lt;p&gt;], [&lt;/p&gt;
{Producer2: 2, 3}
&lt;p&gt;, &lt;/p&gt;
{Producer2: Commit}
&lt;p&gt;], [&lt;/p&gt;
{2}, {3}, {Producer1: Commit}]&lt;br/&gt;
+    //  {0, 1},              {2, 3},            {4},                   {5}, {6}, {7} ==&amp;gt; Offsets&lt;br/&gt;
+    log.appendAsLeader(record(2, 2), leaderEpoch = 0) // offset 5&lt;br/&gt;
+    log.appendAsLeader(record(3, 3), leaderEpoch = 0) // offset 6&lt;br/&gt;
+    log.appendAsLeader(commitMarker(1L, producerEpoch), leaderEpoch = 0, isFromClient = false) // offset 7&lt;br/&gt;
     log.roll()&lt;br/&gt;
 &lt;br/&gt;
     // first time through the records are removed&lt;br/&gt;
+    // Expected State: [{Producer1: EmptyBatch}, {Producer2: EmptyBatch}, {Producer2: Commit}, {2}
&lt;p&gt;, &lt;/p&gt;
{3}]&lt;br/&gt;
     var dirtyOffset = cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
     assertEquals(List(2, 3), LogTest.keysInLog(log))&lt;br/&gt;
-    assertEquals(List(2, 3, 4), offsetsInLog(log)) // commit marker is retained&lt;br/&gt;
-    assertEquals(List(1, 2, 3, 4), lastOffsetsPerBatchInLog(log)) // empty batch is retained&lt;br/&gt;
+    assertEquals(List(4, 5, 6), offsetsInLog(log))&lt;br/&gt;
+    assertEquals(List(1, 3, 4, 5, 6), lastOffsetsPerBatchInLog(log))&lt;br/&gt;
 &lt;br/&gt;
     // the empty batch remains if cleaned again because it still holds the last sequence&lt;br/&gt;
+    // Expected State: [{Producer1: EmptyBatch}, {Producer2: EmptyBatch}, {Producer2: Commit}, {2}, {3}
&lt;p&gt;]&lt;br/&gt;
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
     assertEquals(List(2, 3), LogTest.keysInLog(log))&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertEquals(List(2, 3, 4), offsetsInLog(log)) // commit marker is still retained&lt;/li&gt;
	&lt;li&gt;assertEquals(List(1, 2, 3, 4), lastOffsetsPerBatchInLog(log)) // empty batch is retained&lt;br/&gt;
+    assertEquals(List(4, 5, 6), offsetsInLog(log))&lt;br/&gt;
+    assertEquals(List(1, 3, 4, 5, 6), lastOffsetsPerBatchInLog(log))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // append a new record from the producer to allow cleaning of the empty batch&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;appendProducer(Seq(1))&lt;br/&gt;
+    // [
{Producer1: EmptyBatch}
&lt;p&gt;, &lt;/p&gt;
{Producer2: EmptyBatch}
&lt;p&gt;, &lt;/p&gt;
{Producer2: Commit}
&lt;p&gt;, &lt;/p&gt;
{2}, {3}] &lt;span class=&quot;error&quot;&gt;&amp;#91;{Producer2: 1}, {Producer2: Commit}&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    //  {1},                     {3},                     {4},                 {5}, {6},  {8},            {9} ==&amp;gt; Offsets&lt;br/&gt;
+    producer2(Seq(1)) // offset 8&lt;br/&gt;
+    log.appendAsLeader(commitMarker(2L, producerEpoch), leaderEpoch = 0, isFromClient = false) // offset 9&lt;br/&gt;
     log.roll()&lt;br/&gt;
 &lt;br/&gt;
+    // Expected State: [{Producer1: EmptyBatch}, {Producer2: Commit}, {2}
&lt;p&gt;, &lt;/p&gt;
{3}, {Producer2: 1}, {Producer2: Commit}]&lt;br/&gt;
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
     assertEquals(List(2, 3, 1), LogTest.keysInLog(log))&lt;br/&gt;
-    assertEquals(List(2, 3, 4, 5), offsetsInLog(log)) // commit marker is still retained&lt;br/&gt;
-    assertEquals(List(2, 3, 4, 5), lastOffsetsPerBatchInLog(log)) // empty batch should be gone&lt;br/&gt;
+    assertEquals(List(4, 5, 6, 8, 9), offsetsInLog(log))&lt;br/&gt;
+    assertEquals(List(1, 4, 5, 6, 8, 9), lastOffsetsPerBatchInLog(log))&lt;br/&gt;
 &lt;br/&gt;
+    // Expected State: [{Producer1: EmptyBatch}, {2}, {3}
&lt;p&gt;, &lt;/p&gt;
{Producer2: 1}
&lt;p&gt;, &lt;/p&gt;
{Producer2: Commit}
&lt;p&gt;]&lt;br/&gt;
     dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
     assertEquals(List(2, 3, 1), LogTest.keysInLog(log))&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertEquals(List(3, 4, 5), offsetsInLog(log)) // commit marker is gone&lt;/li&gt;
	&lt;li&gt;assertEquals(List(3, 4, 5), lastOffsetsPerBatchInLog(log)) // empty batch is gone&lt;br/&gt;
+    assertEquals(List(5, 6, 8, 9), offsetsInLog(log))&lt;br/&gt;
+    assertEquals(List(1, 5, 6, 8, 9), lastOffsetsPerBatchInLog(log))&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16665315" author="henning.spjelkavik" created="Fri, 26 Oct 2018 15:43:31 +0000"  >&lt;p&gt;We encountered this problem, but did not find this issue immediately. I&apos;m including the exception and error message for easier discovery for future readers.&lt;/p&gt;

&lt;p&gt;The exact error message we saw was:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;Caught segment overflow error during cleaning: Detected offset overflow at offset -1 in segment LogSegment(baseOffset=211750969, size=278) (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;Error due to (kafka.log.LogCleaner)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;java.lang.IllegalArgumentException: requirement failed: Split operation is only permitted for segments with overflow&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;It was solved by running 2.0.1.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 3 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3xuj3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>