<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:29:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6502] Kafka streams deserialization handler not committing offsets on error records</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6502</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;See this StackOverflow issue: &lt;a href=&quot;https://stackoverflow.com/questions/48470899/kafka-streams-deserialization-handler&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://stackoverflow.com/questions/48470899/kafka-streams-deserialization-handler&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;and this comment:&#160;&lt;a href=&quot;https://stackoverflow.com/questions/48470899/kafka-streams-deserialization-handler#comment84018564_48470899&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://stackoverflow.com/questions/48470899/kafka-streams-deserialization-handler#comment84018564_48470899&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;I am trying to use the LogAndContinueExceptionHandler on deserialization. It works fine when an error occurs by successfully logging&#160;and continuing. However, on&#160;a continuous stream of errors, it seems like these messages are not committed and on a restart of the application they reappear again. &#160;It is more problematic if I try to send the messages in error to a DLQ. On a restart, they are sent again to DLQ. As soon as I have a good record coming in, it looks like the offset moves further and not seeing the already logged messages again after a restart.&#160;&lt;/p&gt;

&lt;p&gt;I reproduced this behavior by running the sample provided here: &lt;a href=&quot;https://github.com/confluentinc/kafka-streams-examples/blob/4.0.0-post/src/main/java/io/confluent/examples/streams/WordCountLambdaExample.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/confluentinc/kafka-streams-examples/blob/4.0.0-post/src/main/java/io/confluent/examples/streams/WordCountLambdaExample.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I changed the incoming value Serde to &lt;tt&gt;Serdes.Integer().getClass().getName()&lt;/tt&gt; to force a deserialization error on input and reduced the commit interval to just 1 second. Also added the following to the config.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;streamsConfiguration.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, LogAndContinueExceptionHandler.class);&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;&#160;It looks&#160;like when deserialization exceptions occur, this flag is never set to be true here: &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L228&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L228&lt;/a&gt;. It only becomes true once processing succeeds. That might be the reason why commit is not happening even after I manually call processorContext#commit().&lt;/p&gt;</description>
                <environment></environment>
        <key id="13134739">KAFKA-6502</key>
            <summary>Kafka streams deserialization handler not committing offsets on error records</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="Gerrrr">Alex Sorokoumov</assignee>
                                    <reporter username="sobychacko">Soby Chacko</reporter>
                        <labels>
                    </labels>
                <created>Tue, 30 Jan 2018 12:33:10 +0000</created>
                <updated>Tue, 29 Oct 2024 16:54:22 +0000</updated>
                            <resolved>Thu, 20 Jan 2022 17:27:24 +0000</resolved>
                                                    <fixVersion>3.2.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16351051" author="guozhang" created="Fri, 2 Feb 2018 23:20:02 +0000"  >&lt;p&gt;Hello Soby, thanks for reporting this issue. Your reasoning is basically right: a record is firstly deserialized and then put into the buffer, then being processed. If the deser failed it will not be processed at all. And then if there is a series of records having deserialization error, none would be processed, i.e. &lt;tt&gt;StreamTask.process()&lt;/tt&gt; would not be processed at all.&lt;/p&gt;

&lt;p&gt;Before we jumped onto possible fixes, could I ask your scenarios: when you see the skipped records metric increasing, why do you want to restart your applications immediately in the middle of such series of error messages? Would you expect there is no valid records ever coming next? My reasoning is that:&lt;/p&gt;

&lt;p&gt;1. If these poison pills are due to your application&apos;s own serde was buggy, then after restarting with the fixed serde these records should be correctly processed then, so we are good here.&lt;br/&gt;
2. If these poison pills are bad themselves and cannot be processed anyways, you would not bother restarting your application; on the other hand you can just let your application continue to run and skip these records.  &lt;/p&gt;</comment>
                            <comment id="16358638" author="sobychacko" created="Fri, 9 Feb 2018 16:43:38 +0000"  >&lt;p&gt;Hi Guozhang, your reasonings are correct. However, if I introduce a custom DLQ type of exception handler (in which case, if I have a bad Message, it is sent to an error-topic), then if I happen to re-start the application, I don&apos;t want the poison pills to be re-sent to the DLQ as they have already been sent. Anyways, this is a corner case I ran into and may not happen frequently. Thanks!&lt;/p&gt;</comment>
                            <comment id="16358884" author="guozhang" created="Fri, 9 Feb 2018 20:02:39 +0000"  >&lt;p&gt;Got it, thanks! I think this is a valid point, we will keep it on track for known issues and consider a fix based on priorities. &lt;/p&gt;</comment>
                            <comment id="16365057" author="guozhang" created="Thu, 15 Feb 2018 02:13:10 +0000"  >&lt;p&gt;There are some tricky cases worth pointing out for designing the fix: say if you have offset 0,1,2,3,4,5 and deser failed on record of offset 1, 3 and succeeded in others but the successfully deserialized records have not been processed but only be sitting in the partition queue, then when you commit, you have to consider which offset to commit to:&lt;/p&gt;

&lt;p&gt;after record 0 processed, you can commit 1 (since 1 deser failed and skipped already).&lt;br/&gt;
after record 2 processed, you can commit 3 (since 3 deser failed and skipped already).&lt;br/&gt;
after record 4 processed, you can commit 4.&lt;br/&gt;
after record 5 processed, you can commit 5.&lt;/p&gt;</comment>
                            <comment id="16366010" author="mjsax" created="Thu, 15 Feb 2018 17:46:12 +0000"  >&lt;p&gt;If we really change the timestamp handling, and only look at the first record per partition to determine which record to pick next, this issue resolved itself, as we can to lazy deserialization for the first record per partition only.&lt;/p&gt;</comment>
                            <comment id="17121235" author="mjsax" created="Mon, 1 Jun 2020 18:27:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&#160;I think this issue is fixed via&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6607&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-6607&lt;/a&gt;&#160;? Or is the issue that we don&apos;t set the `commit.needed` flag for this case?&lt;/p&gt;</comment>
                            <comment id="17121269" author="guozhang" created="Mon, 1 Jun 2020 19:18:34 +0000"  >&lt;p&gt;I think it is not fixed via &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6607&quot; title=&quot;Kafka Streams lag not zero when input topic transactional&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6607&quot;&gt;&lt;del&gt;KAFKA-6607&lt;/del&gt;&lt;/a&gt; since if the deserialization failed, then the record is not put into the buffer at all and hence the consumedOffsets would not be updated.&lt;/p&gt;</comment>
                            <comment id="17474389" author="gerrrr" created="Wed, 12 Jan 2022 09:18:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jadireddi&quot; class=&quot;user-hover&quot; rel=&quot;jadireddi&quot;&gt;jadireddi&lt;/a&gt; Are you actively working on this issue? If not, I would like to give it a try.&lt;/p&gt;</comment>
                            <comment id="17477315" author="gerrrr" created="Mon, 17 Jan 2022 16:36:39 +0000"  >&lt;p&gt;Patch - &lt;a href=&quot;https://github.com/apache/kafka/pull/11683.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/11683.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Without this patch,&#160;&lt;tt&gt;RecordQueue&lt;/tt&gt; just skips records that failed to deserialize when&#160;&lt;tt&gt;DeserializationExceptionHandler&lt;/tt&gt; is set to &lt;tt&gt;LogAndContinueExceptionHandler&lt;/tt&gt;. This way, if the entire stream consists&lt;br/&gt;
of corrupted records, the task never updates consumed offsets.&lt;/p&gt;

&lt;p&gt;This patch introduces a new record type -&#160;&lt;tt&gt;CorruptedRecord&lt;/tt&gt; that wraps a raw record that failed to deserialize.&#160;&lt;tt&gt;RecordQueue&lt;/tt&gt;&apos;s&#160;&lt;tt&gt;headRecord becomes CorruptedRecord&lt;/tt&gt;&#160;if there were records in the&#160;&lt;tt&gt;fifoQueue&lt;/tt&gt; and all of them failed to deserialize. In turn,&#160;&lt;tt&gt;StreamTask#process&lt;/tt&gt; checks that a found record is not corrupted before processing it. If the record is an instance of&#160;&lt;tt&gt;CorruptedRecord&lt;/tt&gt;, it updates the offset, sets &lt;tt&gt;commitNeeded to true&lt;/tt&gt;&#160;and returns.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; , &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt; Can you please review?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13082326">KAFKA-5510</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13596669">KAFKA-17872</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 43 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3pjc7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>mjsax</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>