<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:11:09 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6927] Broker uses significant amount of memory during down-conversion</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6927</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Kafka broker could consume significant amount of memory when down-conversion is required. We have seen scenarios where this causes out of memory errors. This issue and the proposed fix is described in detail in KIP-283 - &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-283%3A+Efficient+Memory+Usage+for+Down-Conversion&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-283%3A+Efficient+Memory+Usage+for+Down-Conversion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13160822">KAFKA-6927</key>
            <summary>Broker uses significant amount of memory during down-conversion</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dhruvilshah">Dhruvil Shah</assignee>
                                    <reporter username="dhruvilshah">Dhruvil Shah</reporter>
                        <labels>
                    </labels>
                <created>Mon, 21 May 2018 21:06:35 +0000</created>
                <updated>Fri, 1 Jun 2018 10:14:57 +0000</updated>
                            <resolved>Fri, 1 Jun 2018 10:14:57 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                                                            <comments>
                            <comment id="16483063" author="dhruvilshah" created="Mon, 21 May 2018 21:07:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/pull/4871&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4871&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16496150" author="githubbot" created="Thu, 31 May 2018 06:03:54 +0000"  >&lt;p&gt;hachikuji closed pull request #4871: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6927&quot; title=&quot;Broker uses significant amount of memory during down-conversion&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6927&quot;&gt;&lt;del&gt;KAFKA-6927&lt;/del&gt;&lt;/a&gt;: Message down-conversion causes Out Of Memory on broker&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4871&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4871&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index 38f324f8bcf..ca8e0d26c81 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -52,6 +52,7 @@&lt;br/&gt;
 import org.apache.kafka.common.record.InvalidRecordException;&lt;br/&gt;
 import org.apache.kafka.common.record.Record;&lt;br/&gt;
 import org.apache.kafka.common.record.RecordBatch;&lt;br/&gt;
+import org.apache.kafka.common.record.Records;&lt;br/&gt;
 import org.apache.kafka.common.record.TimestampType;&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchResponse;&lt;br/&gt;
@@ -204,7 +205,7 @@ public int sendFetches() {&lt;br/&gt;
                     .addListener(new RequestFutureListener&amp;lt;ClientResponse&amp;gt;() {&lt;br/&gt;
                         @Override&lt;br/&gt;
                         public void onSuccess(ClientResponse resp) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse response = (FetchResponse) resp.responseBody();&lt;br/&gt;
+                            FetchResponse&amp;lt;Records&amp;gt; response = (FetchResponse&amp;lt;Records&amp;gt;) resp.responseBody();&lt;br/&gt;
                             FetchSessionHandler handler = sessionHandlers.get(fetchTarget.id());&lt;br/&gt;
                             if (handler == null) {&lt;br/&gt;
                                 log.error(&quot;Unable to find FetchSessionHandler for node {}. Ignoring fetch response.&quot;,&lt;br/&gt;
@@ -218,7 +219,7 @@ public void onSuccess(ClientResponse resp) {&lt;br/&gt;
                             Set&amp;lt;TopicPartition&amp;gt; partitions = new HashSet&amp;lt;&amp;gt;(response.responseData().keySet());&lt;br/&gt;
                             FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; entry : response.responseData().entrySet()) {&lt;br/&gt;
+                            for (Map.Entry&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;Records&amp;gt;&amp;gt; entry : response.responseData().entrySet()) {&lt;br/&gt;
                                 TopicPartition partition = entry.getKey();&lt;br/&gt;
                                 long fetchOffset = data.sessionPartitions().get(partition).fetchOffset;&lt;br/&gt;
                                 FetchResponse.PartitionData fetchData = entry.getValue();&lt;br/&gt;
@@ -894,7 +895,7 @@ public ListOffsetResult() {&lt;br/&gt;
      */&lt;br/&gt;
     private PartitionRecords parseCompletedFetch(CompletedFetch completedFetch) {&lt;br/&gt;
         TopicPartition tp = completedFetch.partition;&lt;/li&gt;
	&lt;li&gt;FetchResponse.PartitionData partition = completedFetch.partitionData;&lt;br/&gt;
+        FetchResponse.PartitionData&amp;lt;Records&amp;gt; partition = completedFetch.partitionData;&lt;br/&gt;
         long fetchOffset = completedFetch.fetchedOffset;&lt;br/&gt;
         PartitionRecords partitionRecords = null;&lt;br/&gt;
         Errors error = partition.error;&lt;br/&gt;
@@ -1252,13 +1253,13 @@ private boolean containsAbortMarker(RecordBatch batch) {&lt;br/&gt;
     private static class CompletedFetch {&lt;br/&gt;
         private final TopicPartition partition;&lt;br/&gt;
         private final long fetchedOffset;&lt;/li&gt;
	&lt;li&gt;private final FetchResponse.PartitionData partitionData;&lt;br/&gt;
+        private final FetchResponse.PartitionData&amp;lt;Records&amp;gt; partitionData;&lt;br/&gt;
         private final FetchResponseMetricAggregator metricAggregator;&lt;br/&gt;
         private final short responseVersion;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         private CompletedFetch(TopicPartition partition,&lt;br/&gt;
                                long fetchedOffset,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse.PartitionData partitionData,&lt;br/&gt;
+                               FetchResponse.PartitionData&amp;lt;Records&amp;gt; partitionData,&lt;br/&gt;
                                FetchResponseMetricAggregator metricAggregator,&lt;br/&gt;
                                short responseVersion) {&lt;br/&gt;
             this.partition = partition;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java b/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
index ac24a1b69b2..7dccc1015aa 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
@@ -16,7 +16,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.protocol.types;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import org.apache.kafka.common.record.Records;&lt;br/&gt;
+import org.apache.kafka.common.record.BaseRecords;&lt;/p&gt;

&lt;p&gt; import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
@@ -172,8 +172,8 @@ public byte getByte(String name) &lt;/p&gt;
{
         return (Byte) get(name);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Records getRecords(String name) {&lt;/li&gt;
	&lt;li&gt;return (Records) get(name);&lt;br/&gt;
+    public BaseRecords getRecords(String name) 
{
+        return (BaseRecords) get(name);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public Short getShort(BoundField field) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java b/clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java&lt;br/&gt;
index 85916d57d2d..4bd508b7798 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java&lt;br/&gt;
@@ -16,7 +16,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.protocol.types;&lt;/p&gt;

&lt;p&gt;-import org.apache.kafka.common.record.FileRecords;&lt;br/&gt;
+import org.apache.kafka.common.record.BaseRecords;&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords;&lt;br/&gt;
 import org.apache.kafka.common.record.Records;&lt;br/&gt;
 import org.apache.kafka.common.utils.ByteUtils;&lt;br/&gt;
@@ -549,14 +549,14 @@ public boolean isNullable() {&lt;/p&gt;

&lt;p&gt;         @Override&lt;br/&gt;
         public void write(ByteBuffer buffer, Object o) &lt;/p&gt;
{
-            if (o instanceof FileRecords)
-                throw new IllegalArgumentException(&quot;FileRecords must be written to the channel directly&quot;);
+            if (!(o instanceof MemoryRecords))
+                throw new IllegalArgumentException(&quot;Unexpected record type: &quot; + o.getClass());
             MemoryRecords records = (MemoryRecords) o;
             NULLABLE_BYTES.write(buffer, records.buffer().duplicate());
         }

&lt;p&gt;         @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Records read(ByteBuffer buffer) {&lt;br/&gt;
+        public MemoryRecords read(ByteBuffer buffer) 
{
             ByteBuffer recordsBuffer = (ByteBuffer) NULLABLE_BYTES.read(buffer);
             return MemoryRecords.readableRecords(recordsBuffer);
         }
&lt;p&gt;@@ -566,7 +566,7 @@ public int sizeOf(Object o) &lt;/p&gt;
{
             if (o == null)
                 return 4;
 
-            Records records = (Records) o;
+            BaseRecords records = (BaseRecords) o;
             return 4 + records.sizeInBytes();
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -576,12 +576,12 @@ public String typeName() {&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Records validate(Object item) {&lt;br/&gt;
+        public BaseRecords validate(Object item) 
{
             if (item == null)
                 return null;
 
-            if (item instanceof Records)
-                return (Records) item;
+            if (item instanceof BaseRecords)
+                return (BaseRecords) item;
 
             throw new SchemaException(item + &quot; is not an instance of &quot; + Records.class.getName());
         }
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/AbstractRecords.java b/clients/src/main/java/org/apache/kafka/common/record/AbstractRecords.java&lt;br/&gt;
index 89a5413e00c..5e41901fb88 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/record/AbstractRecords.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/AbstractRecords.java&lt;br/&gt;
@@ -18,13 +18,10 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.header.Header;&lt;br/&gt;
 import org.apache.kafka.common.utils.AbstractIterator;&lt;br/&gt;
-import org.apache.kafka.common.utils.Time;&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils;&lt;/p&gt;

&lt;p&gt; import java.nio.ByteBuffer;&lt;br/&gt;
-import java.util.ArrayList;&lt;br/&gt;
 import java.util.Iterator;&lt;br/&gt;
-import java.util.List;&lt;/p&gt;

&lt;p&gt; public abstract class AbstractRecords implements Records {&lt;/p&gt;

&lt;p&gt;@@ -51,97 +48,6 @@ public boolean hasCompatibleMagic(byte magic) &lt;/p&gt;
{
         return true;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Down convert batches to the provided message format version. The first offset parameter is only relevant in the&lt;/li&gt;
	&lt;li&gt;* conversion from uncompressed v2 or higher to v1 or lower. The reason is that uncompressed records in v0 and v1&lt;/li&gt;
	&lt;li&gt;* are not batched (put another way, each batch always has 1 record).&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* If a client requests records in v1 format starting from the middle of an uncompressed batch in v2 format, we&lt;/li&gt;
	&lt;li&gt;* need to drop records from the batch during the conversion. Some versions of librdkafka rely on this for&lt;/li&gt;
	&lt;li&gt;* correctness.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* The temporaryMemoryBytes computation assumes that the batches are not loaded into the heap&lt;/li&gt;
	&lt;li&gt;* (via classes like FileChannelRecordBatch) before this method is called. This is the case in the broker (we&lt;/li&gt;
	&lt;li&gt;* only load records into the heap when down converting), but it&apos;s not for the producer. However, down converting&lt;/li&gt;
	&lt;li&gt;* in the producer is very uncommon and the extra complexity to handle that case is not worth it.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;protected ConvertedRecords&amp;lt;MemoryRecords&amp;gt; downConvert(Iterable&amp;lt;? extends RecordBatch&amp;gt; batches, byte toMagic,&lt;/li&gt;
	&lt;li&gt;long firstOffset, Time time) {&lt;/li&gt;
	&lt;li&gt;// maintain the batch along with the decompressed records to avoid the need to decompress again&lt;/li&gt;
	&lt;li&gt;List&amp;lt;RecordBatchAndRecords&amp;gt; recordBatchAndRecordsList = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;int totalSizeEstimate = 0;&lt;/li&gt;
	&lt;li&gt;long startNanos = time.nanoseconds();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;for (RecordBatch batch : batches) {&lt;/li&gt;
	&lt;li&gt;if (toMagic &amp;lt; RecordBatch.MAGIC_VALUE_V2 &amp;amp;&amp;amp; batch.isControlBatch())&lt;/li&gt;
	&lt;li&gt;continue;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;if (batch.magic() &amp;lt;= toMagic) 
{
-                totalSizeEstimate += batch.sizeInBytes();
-                recordBatchAndRecordsList.add(new RecordBatchAndRecords(batch, null, null));
-            }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;List&amp;lt;Record&amp;gt; records = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Record record : batch) 
{
-                    // See the method javadoc for an explanation
-                    if (toMagic &amp;gt; RecordBatch.MAGIC_VALUE_V1 || batch.isCompressed() || record.offset() &amp;gt;= firstOffset)
-                        records.add(record);
-                }&lt;/li&gt;
	&lt;li&gt;if (records.isEmpty())&lt;/li&gt;
	&lt;li&gt;continue;&lt;/li&gt;
	&lt;li&gt;final long baseOffset;&lt;/li&gt;
	&lt;li&gt;if (batch.magic() &amp;gt;= RecordBatch.MAGIC_VALUE_V2 &amp;amp;&amp;amp; toMagic &amp;gt;= RecordBatch.MAGIC_VALUE_V2)&lt;/li&gt;
	&lt;li&gt;baseOffset = batch.baseOffset();&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;baseOffset = records.get(0).offset();&lt;/li&gt;
	&lt;li&gt;totalSizeEstimate += estimateSizeInBytes(toMagic, baseOffset, batch.compressionType(), records);&lt;/li&gt;
	&lt;li&gt;recordBatchAndRecordsList.add(new RecordBatchAndRecords(batch, records, baseOffset));&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;ByteBuffer buffer = ByteBuffer.allocate(totalSizeEstimate);&lt;/li&gt;
	&lt;li&gt;long temporaryMemoryBytes = 0;&lt;/li&gt;
	&lt;li&gt;int numRecordsConverted = 0;&lt;/li&gt;
	&lt;li&gt;for (RecordBatchAndRecords recordBatchAndRecords : recordBatchAndRecordsList) {&lt;/li&gt;
	&lt;li&gt;temporaryMemoryBytes += recordBatchAndRecords.batch.sizeInBytes();&lt;/li&gt;
	&lt;li&gt;if (recordBatchAndRecords.batch.magic() &amp;lt;= toMagic) 
{
-                recordBatchAndRecords.batch.writeTo(buffer);
-            }
&lt;p&gt; else &lt;/p&gt;
{
-                MemoryRecordsBuilder builder = convertRecordBatch(toMagic, buffer, recordBatchAndRecords);
-                buffer = builder.buffer();
-                temporaryMemoryBytes += builder.uncompressedBytesWritten();
-                numRecordsConverted += builder.numRecords();
-            }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;buffer.flip();&lt;/li&gt;
	&lt;li&gt;RecordsProcessingStats stats = new RecordsProcessingStats(temporaryMemoryBytes, numRecordsConverted,&lt;/li&gt;
	&lt;li&gt;time.nanoseconds() - startNanos);&lt;/li&gt;
	&lt;li&gt;return new ConvertedRecords&amp;lt;&amp;gt;(MemoryRecords.readableRecords(buffer), stats);&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Return a buffer containing the converted record batches. The returned buffer may not be the same as the received&lt;/li&gt;
	&lt;li&gt;* one (e.g. it may require expansion).&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private MemoryRecordsBuilder convertRecordBatch(byte magic, ByteBuffer buffer, RecordBatchAndRecords recordBatchAndRecords) {&lt;/li&gt;
	&lt;li&gt;RecordBatch batch = recordBatchAndRecords.batch;&lt;/li&gt;
	&lt;li&gt;final TimestampType timestampType = batch.timestampType();&lt;/li&gt;
	&lt;li&gt;long logAppendTime = timestampType == TimestampType.LOG_APPEND_TIME ? batch.maxTimestamp() : RecordBatch.NO_TIMESTAMP;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, batch.compressionType(),&lt;/li&gt;
	&lt;li&gt;timestampType, recordBatchAndRecords.baseOffset, logAppendTime);&lt;/li&gt;
	&lt;li&gt;for (Record record : recordBatchAndRecords.records) 
{
-            // Down-convert this record. Ignore headers when down-converting to V0 and V1 since they are not supported
-            if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V1)
-                builder.append(record);
-            else
-                builder.appendWithOffset(record.offset(), record.timestamp(), record.key(), record.value());
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;builder.close();&lt;/li&gt;
	&lt;li&gt;return builder;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Get an iterator over the deep records.&lt;/li&gt;
	&lt;li&gt;@return An iterator over the records&lt;br/&gt;
@@ -151,6 +57,11 @@ private MemoryRecordsBuilder convertRecordBatch(byte magic, ByteBuffer buffer, R&lt;br/&gt;
         return records;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Override&lt;br/&gt;
+    public RecordsSend toSend(String destination) &lt;/p&gt;
{
+        return new DefaultRecordsSend(destination, this);
+    }
&lt;p&gt;+&lt;br/&gt;
     private Iterator&amp;lt;Record&amp;gt; recordsIterator() {&lt;br/&gt;
         return new AbstractIterator&amp;lt;Record&amp;gt;() &lt;/p&gt;
{
             private final Iterator&amp;lt;? extends RecordBatch&amp;gt; batches = batches().iterator();
@@ -241,16 +152,5 @@ public static int recordBatchHeaderSizeInBytes(byte magic, CompressionType compr
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static class RecordBatchAndRecords {&lt;/li&gt;
	&lt;li&gt;private final RecordBatch batch;&lt;/li&gt;
	&lt;li&gt;private final List&amp;lt;Record&amp;gt; records;&lt;/li&gt;
	&lt;li&gt;private final Long baseOffset;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private RecordBatchAndRecords(RecordBatch batch, List&amp;lt;Record&amp;gt; records, Long baseOffset) 
{
-            this.batch = batch;
-            this.records = records;
-            this.baseOffset = baseOffset;
-        }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/BaseRecords.java b/clients/src/main/java/org/apache/kafka/common/record/BaseRecords.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..3ebaf792a8e&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/BaseRecords.java&lt;br/&gt;
@@ -0,0 +1,34 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Base interface for accessing records which could be contained in the log, or an in-memory materialization of log records.&lt;br/&gt;
+ */&lt;br/&gt;
+public interface BaseRecords {&lt;br/&gt;
+    /**&lt;br/&gt;
+     * The size of these records in bytes.&lt;br/&gt;
+     * @return The size in bytes of the records&lt;br/&gt;
+     */&lt;br/&gt;
+    int sizeInBytes();&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Encapsulate this &lt;/p&gt;
{@link BaseRecords}
&lt;p&gt; object into &lt;/p&gt;
{@link RecordsSend}&lt;br/&gt;
+     * @return Initialized {@link RecordsSend}
&lt;p&gt; object&lt;br/&gt;
+     */&lt;br/&gt;
+    RecordsSend toSend(String destination);&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/ConvertedRecords.java b/clients/src/main/java/org/apache/kafka/common/record/ConvertedRecords.java&lt;br/&gt;
index fe37c48976f..d9150e5044d 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/ConvertedRecords.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/ConvertedRecords.java&lt;br/&gt;
@@ -19,18 +19,18 @@&lt;br/&gt;
 public class ConvertedRecords&amp;lt;T extends Records&amp;gt; {&lt;/p&gt;

&lt;p&gt;     private final T records;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final RecordsProcessingStats recordsProcessingStats;&lt;br/&gt;
+    private final RecordConversionStats recordConversionStats;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public ConvertedRecords(T records, RecordsProcessingStats recordsProcessingStats) {&lt;br/&gt;
+    public ConvertedRecords(T records, RecordConversionStats recordConversionStats) 
{
         this.records = records;
-        this.recordsProcessingStats = recordsProcessingStats;
+        this.recordConversionStats = recordConversionStats;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public T records() &lt;/p&gt;
{
         return records;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public RecordsProcessingStats recordsProcessingStats() {&lt;/li&gt;
	&lt;li&gt;return recordsProcessingStats;&lt;br/&gt;
+    public RecordConversionStats recordConversionStats() 
{
+        return recordConversionStats;
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordsSend.java b/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordsSend.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..aa715ea4dd4&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordsSend.java&lt;br/&gt;
@@ -0,0 +1,35 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;br/&gt;
+&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
+import java.nio.channels.GatheringByteChannel;&lt;br/&gt;
+&lt;br/&gt;
+public class DefaultRecordsSend extends RecordsSend&amp;lt;Records&amp;gt; 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    public DefaultRecordsSend(String destination, Records records) {
+        this(destination, records, records.sizeInBytes());
+    }++    public DefaultRecordsSend(String destination, Records records, int maxBytesToWrite) {
+        super(destination, records, maxBytesToWrite);
+    }++    @Override+    protected long writeTo(GatheringByteChannel channel, long previouslyWritten, int remaining) throws IOException {
+        return records().writeTo(channel, previouslyWritten, remaining);
+    }+}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java b/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java&lt;br/&gt;
index 6b6e0ab03ef..e44d5d93be8 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.common.KafkaException;&lt;br/&gt;
 import org.apache.kafka.common.network.TransportLayer;&lt;br/&gt;
 import org.apache.kafka.common.record.FileLogInputStream.FileChannelRecordBatch;&lt;br/&gt;
+import org.apache.kafka.common.utils.AbstractIterator;&lt;br/&gt;
 import org.apache.kafka.common.utils.Time;&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -239,8 +240,8 @@ public int truncateTo(int targetSize) throws IOException {&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public ConvertedRecords&amp;lt;? extends Records&amp;gt; downConvert(byte toMagic, long firstOffset, Time time) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ConvertedRecords&amp;lt;MemoryRecords&amp;gt; convertedRecords = downConvert(batches, toMagic, firstOffset, time);&lt;/li&gt;
	&lt;li&gt;if (convertedRecords.recordsProcessingStats().numRecordsConverted() == 0) {&lt;br/&gt;
+        ConvertedRecords&amp;lt;MemoryRecords&amp;gt; convertedRecords = RecordsUtil.downConvert(batches, toMagic, firstOffset, time);&lt;br/&gt;
+        if (convertedRecords.recordConversionStats().numRecordsConverted() == 0) {&lt;br/&gt;
             // This indicates that the message is too large, which means that the buffer is not large&lt;br/&gt;
             // enough to hold a full record batch. We just return all the bytes in this instance.&lt;br/&gt;
             // Even though the record batch does not have the right format version, we expect old clients&lt;br/&gt;
@@ -248,7 +249,7 @@ public int truncateTo(int targetSize) throws IOException 
{
             // are not enough available bytes in the response to read it fully. Note that this is
             // only possible prior to KIP-74, after which the broker was changed to always return at least
             // one full record batch, even if it requires exceeding the max fetch size requested by the client.
-            return new ConvertedRecords&amp;lt;&amp;gt;(this, RecordsProcessingStats.EMPTY);
+            return new ConvertedRecords&amp;lt;&amp;gt;(this, RecordConversionStats.EMPTY);
         }
&lt;p&gt; else &lt;/p&gt;
{
             return convertedRecords;
         }
&lt;p&gt;@@ -364,7 +365,12 @@ public String toString() {&lt;br/&gt;
         };&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Iterator&amp;lt;FileChannelRecordBatch&amp;gt; batchIterator(int start) {&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public AbstractIterator&amp;lt;FileChannelRecordBatch&amp;gt; batchIterator() 
{
+        return batchIterator(start);
+    }
&lt;p&gt;+&lt;br/&gt;
+    private AbstractIterator&amp;lt;FileChannelRecordBatch&amp;gt; batchIterator(int start) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         final int end;         if (isSlice)             end = this.end;@@ -510,5 +516,4 @@ public String toString() {
                     &apos;)&apos;;
         }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecords.java b/clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecords.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..da14b5b494f&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecords.java&lt;br/&gt;
@@ -0,0 +1,168 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.utils.AbstractIterator;&lt;br/&gt;
+import org.apache.kafka.common.utils.Time;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+import java.util.Objects;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Encapsulation for holding records that require down-conversion in a lazy, chunked manner (KIP-283). See&lt;br/&gt;
+ * 
{@link LazyDownConversionRecordsSend}
&lt;p&gt; for the actual chunked send implementation.&lt;br/&gt;
+ */&lt;br/&gt;
+public class LazyDownConversionRecords implements BaseRecords {&lt;br/&gt;
+    private final TopicPartition topicPartition;&lt;br/&gt;
+    private final Records records;&lt;br/&gt;
+    private final byte toMagic;&lt;br/&gt;
+    private final long firstOffset;&lt;br/&gt;
+    private ConvertedRecords firstConvertedBatch;&lt;br/&gt;
+    private final int sizeInBytes;&lt;br/&gt;
+    private final Time time;&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * @param topicPartition The topic-partition to which records belong&lt;br/&gt;
+     * @param records Records to lazily down-convert&lt;br/&gt;
+     * @param toMagic Magic version to down-convert to&lt;br/&gt;
+     * @param firstOffset The starting offset for down-converted records. This only impacts some cases. See&lt;br/&gt;
+     *                    &lt;/p&gt;
{@link RecordsUtil#downConvert(Iterable, byte, long, Time)} for an explanation.&lt;br/&gt;
+     * @param time The time instance to use&lt;br/&gt;
+     */&lt;br/&gt;
+    public LazyDownConversionRecords(TopicPartition topicPartition, Records records, byte toMagic, long firstOffset, Time time) {&lt;br/&gt;
+        this.topicPartition = Objects.requireNonNull(topicPartition);&lt;br/&gt;
+        this.records = Objects.requireNonNull(records);&lt;br/&gt;
+        this.toMagic = toMagic;&lt;br/&gt;
+        this.firstOffset = firstOffset;&lt;br/&gt;
+        this.time = Objects.requireNonNull(time);&lt;br/&gt;
+&lt;br/&gt;
+        // Kafka consumers expect at least one full batch of messages for every topic-partition. To guarantee this, we&lt;br/&gt;
+        // need to make sure that we are able to accommodate one full batch of down-converted messages. The way we achieve&lt;br/&gt;
+        // this is by having sizeInBytes method factor in the size of the first down-converted batch and return at least&lt;br/&gt;
+        // its size.&lt;br/&gt;
+        AbstractIterator&amp;lt;? extends RecordBatch&amp;gt; it = records.batchIterator();&lt;br/&gt;
+        if (it.hasNext()) {
+            firstConvertedBatch = RecordsUtil.downConvert(Collections.singletonList(it.peek()), toMagic, firstOffset, time);
+            sizeInBytes = Math.max(records.sizeInBytes(), firstConvertedBatch.records().sizeInBytes());
+        } else {
+            firstConvertedBatch = null;
+            sizeInBytes = 0;
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public int sizeInBytes() {
+        return sizeInBytes;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public LazyDownConversionRecordsSend toSend(String destination) {
+        return new LazyDownConversionRecordsSend(destination, this);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public TopicPartition topicPartition() {
+        return topicPartition;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public boolean equals(Object o) {&lt;br/&gt;
+        if (o instanceof LazyDownConversionRecords) {
+            LazyDownConversionRecords that = (LazyDownConversionRecords) o;
+            return toMagic == that.toMagic &amp;amp;&amp;amp;
+                    firstOffset == that.firstOffset &amp;amp;&amp;amp;
+                    topicPartition.equals(that.topicPartition) &amp;amp;&amp;amp;
+                    records.equals(that.records);
+        }&lt;br/&gt;
+        return false;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public int hashCode() {
+        int result = toMagic;
+        result = 31 * result + (int) (firstOffset ^ (firstOffset &amp;gt;&amp;gt;&amp;gt; 32));
+        result = 31 * result + topicPartition.hashCode();
+        result = 31 * result + records.hashCode();
+        return result;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public java.util.Iterator&amp;lt;ConvertedRecords&amp;gt; iterator(long maximumReadSize) {
+        // We typically expect only one iterator instance to be created, so null out the first converted batch after
+        // first use to make it available for GC.
+        ConvertedRecords firstBatch = firstConvertedBatch;
+        firstConvertedBatch = null;
+        return new Iterator(records, maximumReadSize, firstBatch);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Implementation for being able to iterate over down-converted records. Goal of this implementation is to keep&lt;br/&gt;
+     * it as memory-efficient as possible by not having to maintain all down-converted records in-memory. Maintains&lt;br/&gt;
+     * a view into batches of down-converted records.&lt;br/&gt;
+     */&lt;br/&gt;
+    private class Iterator extends AbstractIterator&amp;lt;ConvertedRecords&amp;gt; {&lt;br/&gt;
+        private final AbstractIterator&amp;lt;? extends RecordBatch&amp;gt; batchIterator;&lt;br/&gt;
+        private final long maximumReadSize;&lt;br/&gt;
+        private ConvertedRecords firstConvertedBatch;&lt;br/&gt;
+&lt;br/&gt;
+        /**&lt;br/&gt;
+         * @param recordsToDownConvert Records that require down-conversion&lt;br/&gt;
+         * @param maximumReadSize Maximum possible size of underlying records that will be down-converted in each call to&lt;br/&gt;
+         *                        {@link #makeNext()}. This is a soft limit as {@link #makeNext()} will always convert&lt;br/&gt;
+         *                        and return at least one full message batch.&lt;br/&gt;
+         */&lt;br/&gt;
+        private Iterator(Records recordsToDownConvert, long maximumReadSize, ConvertedRecords firstConvertedBatch) {
+            this.batchIterator = recordsToDownConvert.batchIterator();
+            this.maximumReadSize = maximumReadSize;
+            this.firstConvertedBatch = firstConvertedBatch;
+            // If we already have the first down-converted batch, advance the underlying records iterator to next batch
+            if (firstConvertedBatch != null)
+                this.batchIterator.next();
+        }&lt;br/&gt;
+&lt;br/&gt;
+        /**&lt;br/&gt;
+         * Make next set of down-converted records&lt;br/&gt;
+         * @return Down-converted records&lt;br/&gt;
+         */&lt;br/&gt;
+        @Override&lt;br/&gt;
+        protected ConvertedRecords makeNext() {&lt;br/&gt;
+            // If we have cached the first down-converted batch, return that now&lt;br/&gt;
+            if (firstConvertedBatch != null) {
+                ConvertedRecords convertedBatch = firstConvertedBatch;
+                firstConvertedBatch = null;
+                return convertedBatch;
+            }&lt;br/&gt;
+&lt;br/&gt;
+            if (!batchIterator.hasNext())&lt;br/&gt;
+                return allDone();&lt;br/&gt;
+&lt;br/&gt;
+            // Figure out batches we should down-convert based on the size constraints&lt;br/&gt;
+            List&amp;lt;RecordBatch&amp;gt; batches = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+            boolean isFirstBatch = true;&lt;br/&gt;
+            long sizeSoFar = 0;&lt;br/&gt;
+            while (batchIterator.hasNext() &amp;amp;&amp;amp;&lt;br/&gt;
+                    (isFirstBatch || (batchIterator.peek().sizeInBytes() + sizeSoFar) &amp;lt;= maximumReadSize)) {
+                RecordBatch currentBatch = batchIterator.next();
+                batches.add(currentBatch);
+                sizeSoFar += currentBatch.sizeInBytes();
+                isFirstBatch = false;
+            }&lt;br/&gt;
+            return RecordsUtil.downConvert(batches, toMagic, firstOffset, time);&lt;br/&gt;
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecordsSend.java b/clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecordsSend.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..b78211418ac&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecordsSend.java&lt;br/&gt;
@@ -0,0 +1,99 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.slf4j.Logger;&lt;br/&gt;
+import org.slf4j.LoggerFactory;&lt;br/&gt;
+&lt;br/&gt;
+import java.io.EOFException;&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
+import java.nio.channels.GatheringByteChannel;&lt;br/&gt;
+import java.util.Iterator;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Encapsulation for {@link RecordsSend} for {@link LazyDownConversionRecords}. Records are down-converted in batches and&lt;br/&gt;
+ * on-demand when {@link #writeTo} method is called.&lt;br/&gt;
+ */&lt;br/&gt;
+public final class LazyDownConversionRecordsSend extends RecordsSend&amp;lt;LazyDownConversionRecords&amp;gt; {&lt;br/&gt;
+    private static final Logger log = LoggerFactory.getLogger(LazyDownConversionRecordsSend.class);&lt;br/&gt;
+    private static final int MAX_READ_SIZE = 128 * 1024;&lt;br/&gt;
+&lt;br/&gt;
+    private RecordConversionStats recordConversionStats;&lt;br/&gt;
+    private RecordsSend convertedRecordsWriter;&lt;br/&gt;
+    private Iterator&amp;lt;ConvertedRecords&amp;gt; convertedRecordsIterator;&lt;br/&gt;
+&lt;br/&gt;
+    public LazyDownConversionRecordsSend(String destination, LazyDownConversionRecords records) {
+        super(destination, records, records.sizeInBytes());
+        convertedRecordsWriter = null;
+        recordConversionStats = new RecordConversionStats();
+        convertedRecordsIterator = records().iterator(MAX_READ_SIZE);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public long writeTo(GatheringByteChannel channel, long previouslyWritten, int remaining) throws IOException {&lt;br/&gt;
+        if (convertedRecordsWriter == null || convertedRecordsWriter.completed()) {&lt;br/&gt;
+            MemoryRecords convertedRecords;&lt;br/&gt;
+&lt;br/&gt;
+            // Check if we have more chunks left to down-convert&lt;br/&gt;
+            if (convertedRecordsIterator.hasNext()) {&lt;br/&gt;
+                // Get next chunk of down-converted messages&lt;br/&gt;
+                ConvertedRecords&amp;lt;MemoryRecords&amp;gt; recordsAndStats = convertedRecordsIterator.next();&lt;br/&gt;
+                convertedRecords = recordsAndStats.records();&lt;br/&gt;
+&lt;br/&gt;
+                int sizeOfFirstConvertedBatch = convertedRecords.batchIterator().next().sizeInBytes();&lt;br/&gt;
+                if (previouslyWritten == 0 &amp;amp;&amp;amp; sizeOfFirstConvertedBatch &amp;gt; size())&lt;br/&gt;
+                    throw new EOFException(&quot;Unable to send first batch completely.&quot; +&lt;br/&gt;
+                            &quot; maximum_size: &quot; + size() +&lt;br/&gt;
+                            &quot; converted_records_size: &quot; + sizeOfFirstConvertedBatch);&lt;br/&gt;
+&lt;br/&gt;
+                recordConversionStats.add(recordsAndStats.recordConversionStats());&lt;br/&gt;
+                log.debug(&quot;Got lazy converted records for {&quot; + topicPartition() + &quot;} with length=&quot; + convertedRecords.sizeInBytes());&lt;br/&gt;
+            } else {&lt;br/&gt;
+                if (previouslyWritten == 0)&lt;br/&gt;
+                    throw new EOFException(&quot;Unable to get the first batch of down-converted records&quot;);&lt;br/&gt;
+&lt;br/&gt;
+                // We do not have any records left to down-convert. Construct a &quot;fake&quot; message for the length remaining.&lt;br/&gt;
+                // This message will be ignored by the consumer because its length will be past the length of maximum&lt;br/&gt;
+                // possible response size.&lt;br/&gt;
+                // DefaultRecordBatch =&amp;gt;&lt;br/&gt;
+                //      BaseOffset =&amp;gt; Int64&lt;br/&gt;
+                //      Length =&amp;gt; Int32&lt;br/&gt;
+                //      ...&lt;br/&gt;
+                // TODO: check if there is a better way to encapsulate this logic, perhaps in DefaultRecordBatch&lt;br/&gt;
+                log.debug(&quot;Constructing fake message batch for topic-partition {&quot; + topicPartition() + &quot;} for remaining length &quot; + remaining);&lt;br/&gt;
+                int minLength = (Long.SIZE / Byte.SIZE) + (Integer.SIZE / Byte.SIZE);&lt;br/&gt;
+                ByteBuffer fakeMessageBatch = ByteBuffer.allocate(Math.max(minLength, Math.min(remaining + 1, MAX_READ_SIZE)));&lt;br/&gt;
+                fakeMessageBatch.putLong(-1L);&lt;br/&gt;
+                fakeMessageBatch.putInt(remaining + 1);&lt;br/&gt;
+                convertedRecords = MemoryRecords.readableRecords(fakeMessageBatch);&lt;br/&gt;
+            }&lt;br/&gt;
+&lt;br/&gt;
+            convertedRecordsWriter = new DefaultRecordsSend(destination(), convertedRecords, Math.min(convertedRecords.sizeInBytes(), remaining));&lt;br/&gt;
+        }&lt;br/&gt;
+        return convertedRecordsWriter.writeTo(channel);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public RecordConversionStats recordConversionStats() {
+        return recordConversionStats;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public TopicPartition topicPartition() {
+        return records().topicPartition();
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
index be7ea6214b2..55a471149c6 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java&lt;br/&gt;
@@ -20,6 +20,7 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.errors.CorruptRecordException;&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords.RecordFilter.BatchRetention;&lt;br/&gt;
+import org.apache.kafka.common.utils.AbstractIterator;&lt;br/&gt;
 import org.apache.kafka.common.utils.ByteBufferOutputStream;&lt;br/&gt;
 import org.apache.kafka.common.utils.CloseableIterator;&lt;br/&gt;
 import org.apache.kafka.common.utils.Time;&lt;br/&gt;
@@ -49,7 +50,7 @@&lt;br/&gt;
     private final Iterable&amp;lt;MutableRecordBatch&amp;gt; batches = new Iterable&amp;lt;MutableRecordBatch&amp;gt;() {&lt;br/&gt;
         @Override&lt;br/&gt;
         public Iterator&amp;lt;MutableRecordBatch&amp;gt; iterator() {
-            return new RecordBatchIterator&amp;lt;&amp;gt;(new ByteBufferLogInputStream(buffer.duplicate(), Integer.MAX_VALUE));
+            return batchIterator();
         }&lt;br/&gt;
     };&lt;br/&gt;
 &lt;br/&gt;
@@ -115,7 +116,12 @@ public int validBytes() {&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public ConvertedRecords&amp;lt;MemoryRecords&amp;gt; downConvert(byte toMagic, long firstOffset, Time time) {
-        return downConvert(batches(), toMagic, firstOffset, time);
+        return RecordsUtil.downConvert(batches(), toMagic, firstOffset, time);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public AbstractIterator&amp;lt;MutableRecordBatch&amp;gt; batchIterator() {
+        return new RecordBatchIterator&amp;lt;&amp;gt;(new ByteBufferLogInputStream(buffer.duplicate(), Integer.MAX_VALUE));
     }&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/network/MultiSend.java b/clients/src/main/java/org/apache/kafka/common/record/MultiRecordsSend.java&lt;br/&gt;
similarity index 59%&lt;br/&gt;
rename from clients/src/main/java/org/apache/kafka/common/network/MultiSend.java&lt;br/&gt;
rename to clients/src/main/java/org/apache/kafka/common/record/MultiRecordsSend.java&lt;br/&gt;
index 6b663609305..2bc8d1c5519 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/network/MultiSend.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/MultiRecordsSend.java&lt;br/&gt;
@@ -14,34 +14,39 @@&lt;br/&gt;
  * See the License for the specific language governing permissions and&lt;br/&gt;
  * limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-package org.apache.kafka.common.network;&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.kafka.common.KafkaException;&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.network.Send;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;br/&gt;
 &lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.nio.channels.GatheringByteChannel;&lt;br/&gt;
+import java.util.HashMap;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
 import java.util.Queue;&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
- * A set of composite sends, sent one after another&lt;br/&gt;
+ * A set of composite sends with nested {@link RecordsSend}, sent one after another&lt;br/&gt;
  */&lt;br/&gt;
-public class MultiSend implements Send {&lt;br/&gt;
-    private static final Logger log = LoggerFactory.getLogger(MultiSend.class);&lt;br/&gt;
+public class MultiRecordsSend implements Send {&lt;br/&gt;
+    private static final Logger log = LoggerFactory.getLogger(MultiRecordsSend.class);&lt;br/&gt;
 &lt;br/&gt;
     private final String dest;&lt;br/&gt;
     private final Queue&amp;lt;Send&amp;gt; sendQueue;&lt;br/&gt;
     private final long size;&lt;br/&gt;
+    private Map&amp;lt;TopicPartition, RecordConversionStats&amp;gt; recordConversionStats;&lt;br/&gt;
 &lt;br/&gt;
     private long totalWritten = 0;&lt;br/&gt;
     private Send current;&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
-     * Construct a MultiSend for the given destination from a queue of Send objects. The queue will be&lt;br/&gt;
-     * consumed as the MultiSend progresses (on completion, it will be empty).&lt;br/&gt;
+     * Construct a MultiRecordsSend for the given destination from a queue of Send objects. The queue will be&lt;br/&gt;
+     * consumed as the MultiRecordsSend progresses (on completion, it will be empty).&lt;br/&gt;
      */&lt;br/&gt;
-    public MultiSend(String dest, Queue&amp;lt;Send&amp;gt; sends) {&lt;br/&gt;
+    public MultiRecordsSend(String dest, Queue&amp;lt;Send&amp;gt; sends) {&lt;br/&gt;
         this.dest = dest;&lt;br/&gt;
         this.sendQueue = sends;&lt;br/&gt;
 &lt;br/&gt;
@@ -88,8 +93,10 @@ public long writeTo(GatheringByteChannel channel) throws IOException {&lt;br/&gt;
             long written = current.writeTo(channel);&lt;br/&gt;
             totalWrittenPerCall += written;&lt;br/&gt;
             sendComplete = current.completed();&lt;br/&gt;
-            if (sendComplete)&lt;br/&gt;
+            if (sendComplete) {
+                updateRecordConversionStats(current);
                 current = sendQueue.poll();
+            }&lt;br/&gt;
         } while (!completed() &amp;amp;&amp;amp; sendComplete);&lt;br/&gt;
 &lt;br/&gt;
         totalWritten += totalWrittenPerCall;&lt;br/&gt;
@@ -103,4 +110,24 @@ public long writeTo(GatheringByteChannel channel) throws IOException {
         return totalWrittenPerCall;
     }&lt;br/&gt;
 &lt;br/&gt;
+    /**&lt;br/&gt;
+     * Get any statistics that were recorded as part of executing this {@link MultiRecordsSend}.&lt;br/&gt;
+     * @return Records processing statistics (could be null if no statistics were collected)&lt;br/&gt;
+     */&lt;br/&gt;
+    public Map&amp;lt;TopicPartition, RecordConversionStats&amp;gt; recordConversionStats() {
+        return recordConversionStats;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void updateRecordConversionStats(Send completedSend) {&lt;br/&gt;
+        // The underlying send might have accumulated statistics that need to be recorded. For example,&lt;br/&gt;
+        // LazyDownConversionRecordsSend accumulates statistics related to the number of bytes down-converted, the amount&lt;br/&gt;
+        // of temporary memory used for down-conversion, etc. Pull out any such statistics from the underlying send&lt;br/&gt;
+        // and fold it up appropriately.&lt;br/&gt;
+        if (completedSend instanceof LazyDownConversionRecordsSend) {
+            if (recordConversionStats == null)
+                recordConversionStats = new HashMap&amp;lt;&amp;gt;();
+            LazyDownConversionRecordsSend lazyRecordsSend = (LazyDownConversionRecordsSend) completedSend;
+            recordConversionStats.put(lazyRecordsSend.topicPartition(), lazyRecordsSend.recordConversionStats());
+        }&lt;br/&gt;
+    }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/RecordsProcessingStats.java b/clients/src/main/java/org/apache/kafka/common/record/RecordConversionStats.java&lt;br/&gt;
similarity index 71%&lt;br/&gt;
rename from clients/src/main/java/org/apache/kafka/common/record/RecordsProcessingStats.java&lt;br/&gt;
rename to clients/src/main/java/org/apache/kafka/common/record/RecordConversionStats.java&lt;br/&gt;
index e104bc8189f..4f0bca527fb 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/RecordsProcessingStats.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/RecordConversionStats.java&lt;br/&gt;
@@ -16,20 +16,30 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.record;&lt;br/&gt;
 &lt;br/&gt;
-public class RecordsProcessingStats {&lt;br/&gt;
+public class RecordConversionStats {&lt;br/&gt;
 &lt;br/&gt;
-    public static final RecordsProcessingStats EMPTY = new RecordsProcessingStats(0L, 0, -1);&lt;br/&gt;
+    public static final RecordConversionStats EMPTY = new RecordConversionStats();&lt;br/&gt;
 &lt;br/&gt;
-    private final long temporaryMemoryBytes;&lt;br/&gt;
-    private final int numRecordsConverted;&lt;br/&gt;
-    private final long conversionTimeNanos;&lt;br/&gt;
+    private long temporaryMemoryBytes;&lt;br/&gt;
+    private int numRecordsConverted;&lt;br/&gt;
+    private long conversionTimeNanos;&lt;br/&gt;
 &lt;br/&gt;
-    public RecordsProcessingStats(long temporaryMemoryBytes, int numRecordsConverted, long conversionTimeNanos) {&lt;br/&gt;
+    public RecordConversionStats(long temporaryMemoryBytes, int numRecordsConverted, long conversionTimeNanos) {
         this.temporaryMemoryBytes = temporaryMemoryBytes;
         this.numRecordsConverted = numRecordsConverted;
         this.conversionTimeNanos = conversionTimeNanos;
     }&lt;br/&gt;
 &lt;br/&gt;
+    public RecordConversionStats() {
+        this(0, 0, 0);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public void add(RecordConversionStats stats) {
+        temporaryMemoryBytes += stats.temporaryMemoryBytes;
+        numRecordsConverted += stats.numRecordsConverted;
+        conversionTimeNanos += stats.conversionTimeNanos;
+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * Returns the number of temporary memory bytes allocated to process the records.&lt;br/&gt;
      * This size depends on whether the records need decompression and/or conversion:&lt;br/&gt;
@@ -54,7 +64,7 @@ public long conversionTimeNanos() {&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() {
-        return String.format(&quot;RecordsProcessingStats(temporaryMemoryBytes=%d, numRecordsConverted=%d, conversionTimeNanos=%d)&quot;,
+        return String.format(&quot;RecordConversionStats(temporaryMemoryBytes=%d, numRecordsConverted=%d, conversionTimeNanos=%d)&quot;,
                 temporaryMemoryBytes, numRecordsConverted, conversionTimeNanos);
     }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/Records.java b/clients/src/main/java/org/apache/kafka/common/record/Records.java&lt;br/&gt;
index 19152bae267..23607b46171 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/record/Records.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/Records.java&lt;br/&gt;
@@ -16,10 +16,13 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.record;&lt;br/&gt;
 &lt;br/&gt;
+import org.apache.kafka.common.utils.AbstractIterator;&lt;br/&gt;
+import org.apache.kafka.common.utils.Time;&lt;br/&gt;
+&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.nio.channels.GatheringByteChannel;&lt;br/&gt;
+import java.util.Iterator;&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.kafka.common.utils.Time;&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * Interface for accessing the records contained in a log. The log itself is represented as a sequence of record&lt;br/&gt;
@@ -28,20 +31,19 @@&lt;br/&gt;
  * For magic versions 1 and below, each batch consists of an 8 byte offset, a 4 byte record size, and a &quot;shallow&quot;&lt;br/&gt;
  * {@link Record record}. If the batch is not compressed, then each batch will have only the shallow record contained&lt;br/&gt;
  * inside it. If it is compressed, the batch contains &quot;deep&quot; records, which are packed into the value field of the&lt;br/&gt;
- * shallow record. To iterate over the shallow batches, use {@link #batches()}; for the deep records, use&lt;br/&gt;
- * {@link #records()}. Note that the deep iterator handles both compressed and non-compressed batches: if the batch is&lt;br/&gt;
- * not compressed, the shallow record is returned; otherwise, the shallow batch is decompressed and the deep records&lt;br/&gt;
- * are returned.&lt;br/&gt;
+ * shallow record. To iterate over the shallow batches, use {@link Records#batches()}; for the deep records, use&lt;br/&gt;
+ * {@link Records#records()}. Note that the deep iterator handles both compressed and non-compressed batches:&lt;br/&gt;
+ * if the batch is not compressed, the shallow record is returned; otherwise, the shallow batch is decompressed and the&lt;br/&gt;
+ * deep records are returned.&lt;br/&gt;
  *&lt;br/&gt;
  * For magic version 2, every batch contains 1 or more log record, regardless of compression. You can iterate&lt;br/&gt;
- * over the batches directly using {@link #batches()}. Records can be iterated either directly from an individual&lt;br/&gt;
- * batch or through {@link #records()}. Just as in previous versions, iterating over the records typically involves&lt;br/&gt;
+ * over the batches directly using {@link Records#batches()}. Records can be iterated either directly from an individual&lt;br/&gt;
+ * batch or through {@link Records#records()}. Just as in previous versions, iterating over the records typically involves&lt;br/&gt;
  * decompression and should therefore be used with caution.&lt;br/&gt;
  *&lt;br/&gt;
  * See {@link MemoryRecords} for the in-memory representation and {@link FileRecords} for the on-disk representation.&lt;br/&gt;
  */&lt;br/&gt;
-public interface Records {&lt;br/&gt;
-&lt;br/&gt;
+public interface Records extends BaseRecords {&lt;br/&gt;
     int OFFSET_OFFSET = 0;&lt;br/&gt;
     int OFFSET_LENGTH = 8;&lt;br/&gt;
     int SIZE_OFFSET = OFFSET_OFFSET + OFFSET_LENGTH;&lt;br/&gt;
@@ -54,12 +56,6 @@&lt;br/&gt;
     int MAGIC_LENGTH = 1;&lt;br/&gt;
     int HEADER_SIZE_UP_TO_MAGIC = MAGIC_OFFSET + MAGIC_LENGTH;&lt;br/&gt;
 &lt;br/&gt;
-    /**&lt;br/&gt;
-     * The size of these records in bytes.&lt;br/&gt;
-     * @return The size in bytes of the records&lt;br/&gt;
-     */&lt;br/&gt;
-    int sizeInBytes();&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;br/&gt;
      * Attempts to write the contents of this buffer to a channel.&lt;br/&gt;
      * @param channel The channel to write to&lt;br/&gt;
@@ -79,6 +75,13 @@&lt;br/&gt;
      */&lt;br/&gt;
     Iterable&amp;lt;? extends RecordBatch&amp;gt; batches();&lt;br/&gt;
 &lt;br/&gt;
+    /**&lt;br/&gt;
+     * Get an iterator over the record batches. This is similar to {@link #batches()} but returns an {@link AbstractIterator}&lt;br/&gt;
+     * instead of {@link Iterator}, so that clients can use methods like {@link AbstractIterator#peek() peek}.&lt;br/&gt;
+     * @return An iterator over the record batches of the log&lt;br/&gt;
+     */&lt;br/&gt;
+    AbstractIterator&amp;lt;? extends RecordBatch&amp;gt; batchIterator();&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * Check whether all batches in this buffer have a certain magic value.&lt;br/&gt;
      * @param magic The magic value to check&lt;br/&gt;
@@ -99,7 +102,7 @@&lt;br/&gt;
      * deep iteration since all of the deep records must also be converted to the desired format.&lt;br/&gt;
      * @param toMagic The magic value to convert to&lt;br/&gt;
      * @param firstOffset The starting offset for returned records. This only impacts some cases. See&lt;br/&gt;
-     *                    {@link AbstractRecords#downConvert(Iterable, byte, long, Time) for an explanation.&lt;br/&gt;
+     *                    {@link RecordsUtil#downConvert(Iterable, byte, long, Time)}
&lt;p&gt; for an explanation.&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param time instance used for reporting stats&lt;/li&gt;
	&lt;li&gt;@return A ConvertedRecords instance which may or may not contain the same instance in its records field.&lt;br/&gt;
      */&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/RecordsSend.java b/clients/src/main/java/org/apache/kafka/common/record/RecordsSend.java&lt;br/&gt;
similarity index 55%&lt;br/&gt;
rename from clients/src/main/java/org/apache/kafka/common/requests/RecordsSend.java&lt;br/&gt;
rename to clients/src/main/java/org/apache/kafka/common/record/RecordsSend.java&lt;br/&gt;
index 6608e9b1218..b40d6e6a6b8 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/RecordsSend.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/RecordsSend.java&lt;br/&gt;
@@ -14,29 +14,30 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-package org.apache.kafka.common.requests;&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.network.Send;&lt;br/&gt;
 import org.apache.kafka.common.network.TransportLayers;&lt;br/&gt;
-import org.apache.kafka.common.record.Records;&lt;/p&gt;

&lt;p&gt; import java.io.EOFException;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;br/&gt;
 import java.nio.channels.GatheringByteChannel;&lt;/p&gt;

&lt;p&gt;-public class RecordsSend implements Send {&lt;br/&gt;
+public abstract class RecordsSend&amp;lt;T extends BaseRecords&amp;gt; implements Send {&lt;br/&gt;
     private static final ByteBuffer EMPTY_BYTE_BUFFER = ByteBuffer.allocate(0);&lt;/p&gt;

&lt;p&gt;     private final String destination;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Records records;&lt;br/&gt;
+    private final T records;&lt;br/&gt;
+    private final int maxBytesToWrite;&lt;br/&gt;
     private int remaining;&lt;br/&gt;
     private boolean pending = false;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public RecordsSend(String destination, Records records) {&lt;br/&gt;
+    protected RecordsSend(String destination, T records, int maxBytesToWrite) 
{
         this.destination = destination;
         this.records = records;
-        this.remaining = records.sizeInBytes();
+        this.maxBytesToWrite = maxBytesToWrite;
+        this.remaining = maxBytesToWrite;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
@@ -50,11 +51,11 @@ public boolean completed() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public long writeTo(GatheringByteChannel channel) throws IOException {&lt;br/&gt;
+    public final long writeTo(GatheringByteChannel channel) throws IOException {&lt;br/&gt;
         long written = 0;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;written = records.writeTo(channel, size() - remaining, remaining);&lt;br/&gt;
+            written = writeTo(channel, size() - remaining, remaining);&lt;br/&gt;
             if (written &amp;lt; 0)&lt;br/&gt;
                 throw new EOFException(&quot;Wrote negative bytes to channel. This shouldn&apos;t happen.&quot;);&lt;br/&gt;
             remaining -= written;&lt;br/&gt;
@@ -69,6 +70,23 @@ public long writeTo(GatheringByteChannel channel) throws IOException {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
     public long size() &lt;/p&gt;
{
-        return records.sizeInBytes();
+        return maxBytesToWrite;
     }
&lt;p&gt;+&lt;br/&gt;
+    protected T records() &lt;/p&gt;
{
+        return records;
+    }
&lt;p&gt;+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Write records up to `remaining` bytes to `channel`. The implementation is allowed to be stateful. The contract&lt;br/&gt;
+     * from the caller is that the first invocation will be with `previouslyWritten` equal to 0, and `remaining` equal to&lt;br/&gt;
+     * the to maximum bytes we want to write the to `channel`. `previouslyWritten` and `remaining` will be adjusted&lt;br/&gt;
+     * appropriately for every subsequent invocation. See &lt;/p&gt;
{@link #writeTo}
&lt;p&gt; for example expected usage.&lt;br/&gt;
+     * @param channel The channel to write to&lt;br/&gt;
+     * @param previouslyWritten Bytes written in previous calls to &lt;/p&gt;
{@link #writeTo(GatheringByteChannel, long, int)}
&lt;p&gt;; 0 if being called for the first time&lt;br/&gt;
+     * @param remaining Number of bytes remaining to be written&lt;br/&gt;
+     * @return The number of bytes actually written&lt;br/&gt;
+     * @throws IOException For any IO errors&lt;br/&gt;
+     */&lt;br/&gt;
+    protected abstract long writeTo(GatheringByteChannel channel, long previouslyWritten, int remaining) throws IOException;&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/RecordsUtil.java b/clients/src/main/java/org/apache/kafka/common/record/RecordsUtil.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..c9b73941317&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/record/RecordsUtil.java&lt;br/&gt;
@@ -0,0 +1,130 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.utils.Time;&lt;br/&gt;
+&lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+&lt;br/&gt;
+public class RecordsUtil {&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Down convert batches to the provided message format version. The first offset parameter is only relevant in the&lt;br/&gt;
+     * conversion from uncompressed v2 or higher to v1 or lower. The reason is that uncompressed records in v0 and v1&lt;br/&gt;
+     * are not batched (put another way, each batch always has 1 record).&lt;br/&gt;
+     *&lt;br/&gt;
+     * If a client requests records in v1 format starting from the middle of an uncompressed batch in v2 format, we&lt;br/&gt;
+     * need to drop records from the batch during the conversion. Some versions of librdkafka rely on this for&lt;br/&gt;
+     * correctness.&lt;br/&gt;
+     *&lt;br/&gt;
+     * The temporaryMemoryBytes computation assumes that the batches are not loaded into the heap&lt;br/&gt;
+     * (via classes like FileChannelRecordBatch) before this method is called. This is the case in the broker (we&lt;br/&gt;
+     * only load records into the heap when down converting), but it&apos;s not for the producer. However, down converting&lt;br/&gt;
+     * in the producer is very uncommon and the extra complexity to handle that case is not worth it.&lt;br/&gt;
+     */&lt;br/&gt;
+    protected static ConvertedRecords&amp;lt;MemoryRecords&amp;gt; downConvert(Iterable&amp;lt;? extends RecordBatch&amp;gt; batches, byte toMagic,&lt;br/&gt;
+                                                                 long firstOffset, Time time) {&lt;br/&gt;
+        // maintain the batch along with the decompressed records to avoid the need to decompress again&lt;br/&gt;
+        List&amp;lt;RecordBatchAndRecords&amp;gt; recordBatchAndRecordsList = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        int totalSizeEstimate = 0;&lt;br/&gt;
+        long startNanos = time.nanoseconds();&lt;br/&gt;
+&lt;br/&gt;
+        for (RecordBatch batch : batches) {&lt;br/&gt;
+            if (toMagic &amp;lt; RecordBatch.MAGIC_VALUE_V2 &amp;amp;&amp;amp; batch.isControlBatch())&lt;br/&gt;
+                continue;&lt;br/&gt;
+&lt;br/&gt;
+            if (batch.magic() &amp;lt;= toMagic) &lt;/p&gt;
{
+                totalSizeEstimate += batch.sizeInBytes();
+                recordBatchAndRecordsList.add(new RecordBatchAndRecords(batch, null, null));
+            }
&lt;p&gt; else {&lt;br/&gt;
+                List&amp;lt;Record&amp;gt; records = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+                for (Record record : batch) &lt;/p&gt;
{
+                    // See the method javadoc for an explanation
+                    if (toMagic &amp;gt; RecordBatch.MAGIC_VALUE_V1 || batch.isCompressed() || record.offset() &amp;gt;= firstOffset)
+                        records.add(record);
+                }
&lt;p&gt;+                if (records.isEmpty())&lt;br/&gt;
+                    continue;&lt;br/&gt;
+                final long baseOffset;&lt;br/&gt;
+                if (batch.magic() &amp;gt;= RecordBatch.MAGIC_VALUE_V2 &amp;amp;&amp;amp; toMagic &amp;gt;= RecordBatch.MAGIC_VALUE_V2)&lt;br/&gt;
+                    baseOffset = batch.baseOffset();&lt;br/&gt;
+                else&lt;br/&gt;
+                    baseOffset = records.get(0).offset();&lt;br/&gt;
+                totalSizeEstimate += AbstractRecords.estimateSizeInBytes(toMagic, baseOffset, batch.compressionType(), records);&lt;br/&gt;
+                recordBatchAndRecordsList.add(new RecordBatchAndRecords(batch, records, baseOffset));&lt;br/&gt;
+            }&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        ByteBuffer buffer = ByteBuffer.allocate(totalSizeEstimate);&lt;br/&gt;
+        long temporaryMemoryBytes = 0;&lt;br/&gt;
+        int numRecordsConverted = 0;&lt;br/&gt;
+        for (RecordBatchAndRecords recordBatchAndRecords : recordBatchAndRecordsList) {&lt;br/&gt;
+            temporaryMemoryBytes += recordBatchAndRecords.batch.sizeInBytes();&lt;br/&gt;
+            if (recordBatchAndRecords.batch.magic() &amp;lt;= toMagic) &lt;/p&gt;
{
+                recordBatchAndRecords.batch.writeTo(buffer);
+            }
&lt;p&gt; else &lt;/p&gt;
{
+                MemoryRecordsBuilder builder = convertRecordBatch(toMagic, buffer, recordBatchAndRecords);
+                buffer = builder.buffer();
+                temporaryMemoryBytes += builder.uncompressedBytesWritten();
+                numRecordsConverted += builder.numRecords();
+            }
&lt;p&gt;+        }&lt;br/&gt;
+&lt;br/&gt;
+        buffer.flip();&lt;br/&gt;
+        RecordConversionStats stats = new RecordConversionStats(temporaryMemoryBytes, numRecordsConverted,&lt;br/&gt;
+                time.nanoseconds() - startNanos);&lt;br/&gt;
+        return new ConvertedRecords&amp;lt;&amp;gt;(MemoryRecords.readableRecords(buffer), stats);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Return a buffer containing the converted record batches. The returned buffer may not be the same as the received&lt;br/&gt;
+     * one (e.g. it may require expansion).&lt;br/&gt;
+     */&lt;br/&gt;
+    private static MemoryRecordsBuilder convertRecordBatch(byte magic, ByteBuffer buffer, RecordBatchAndRecords recordBatchAndRecords) {&lt;br/&gt;
+        RecordBatch batch = recordBatchAndRecords.batch;&lt;br/&gt;
+        final TimestampType timestampType = batch.timestampType();&lt;br/&gt;
+        long logAppendTime = timestampType == TimestampType.LOG_APPEND_TIME ? batch.maxTimestamp() : RecordBatch.NO_TIMESTAMP;&lt;br/&gt;
+&lt;br/&gt;
+        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, batch.compressionType(),&lt;br/&gt;
+                timestampType, recordBatchAndRecords.baseOffset, logAppendTime);&lt;br/&gt;
+        for (Record record : recordBatchAndRecords.records) &lt;/p&gt;
{
+            // Down-convert this record. Ignore headers when down-converting to V0 and V1 since they are not supported
+            if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V1)
+                builder.append(record);
+            else
+                builder.appendWithOffset(record.offset(), record.timestamp(), record.key(), record.value());
+        }
&lt;p&gt;+&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+        return builder;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+    private static class RecordBatchAndRecords {&lt;br/&gt;
+        private final RecordBatch batch;&lt;br/&gt;
+        private final List&amp;lt;Record&amp;gt; records;&lt;br/&gt;
+        private final Long baseOffset;&lt;br/&gt;
+&lt;br/&gt;
+        private RecordBatchAndRecords(RecordBatch batch, List&amp;lt;Record&amp;gt; records, Long baseOffset) &lt;/p&gt;
{
+            this.batch = batch;
+            this.records = records;
+            this.baseOffset = baseOffset;
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AbstractResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/AbstractResponse.java&lt;br/&gt;
index 8d285210fbe..c0ebef1d967 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/AbstractResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AbstractResponse.java&lt;br/&gt;
@@ -73,7 +73,7 @@ public static AbstractResponse parseResponse(ApiKeys apiKey, Struct struct) {&lt;br/&gt;
             case PRODUCE:&lt;br/&gt;
                 return new ProduceResponse(struct);&lt;br/&gt;
             case FETCH:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new FetchResponse(struct);&lt;br/&gt;
+                return FetchResponse.parse(struct);&lt;br/&gt;
             case LIST_OFFSETS:&lt;br/&gt;
                 return new ListOffsetResponse(struct);&lt;br/&gt;
             case METADATA:&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java&lt;br/&gt;
index 103821bea8d..16e33965e9e 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java&lt;br/&gt;
@@ -18,7 +18,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.network.ByteBufferSend;&lt;br/&gt;
-import org.apache.kafka.common.network.MultiSend;&lt;br/&gt;
+import org.apache.kafka.common.record.MultiRecordsSend;&lt;br/&gt;
 import org.apache.kafka.common.network.Send;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
@@ -26,7 +26,8 @@&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
-import org.apache.kafka.common.record.Records;&lt;br/&gt;
+import org.apache.kafka.common.record.BaseRecords;&lt;br/&gt;
+import org.apache.kafka.common.record.MemoryRecords;&lt;/p&gt;

&lt;p&gt; import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.ArrayDeque;&lt;br/&gt;
@@ -50,7 +51,7 @@&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This wrapper supports all versions of the Fetch API&lt;br/&gt;
  */&lt;br/&gt;
-public class FetchResponse extends AbstractResponse {&lt;br/&gt;
+public class FetchResponse&amp;lt;T extends BaseRecords&amp;gt; extends AbstractResponse {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final String RESPONSES_KEY_NAME = &quot;responses&quot;;&lt;/p&gt;

&lt;p&gt;@@ -156,10 +157,10 @@&lt;br/&gt;
     public static final Field.Int32 SESSION_ID = new Field.Int32(&quot;session_id&quot;, &quot;The fetch session ID&quot;);&lt;/p&gt;

&lt;p&gt;     private static final Schema FETCH_RESPONSE_V7 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;THROTTLE_TIME_MS,&lt;/li&gt;
	&lt;li&gt;ERROR_CODE,&lt;/li&gt;
	&lt;li&gt;SESSION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V5)));&lt;br/&gt;
+            THROTTLE_TIME_MS,&lt;br/&gt;
+            ERROR_CODE,&lt;br/&gt;
+            SESSION_ID,&lt;br/&gt;
+            new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V5)));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
@@ -190,7 +191,7 @@&lt;br/&gt;
     private final int throttleTimeMs;&lt;br/&gt;
     private final Errors error;&lt;br/&gt;
     private final int sessionId;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;gt; responseData;&lt;br/&gt;
+    private final LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;lt;T&amp;gt;&amp;gt; responseData;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static final class AbortedTransaction {&lt;br/&gt;
         public final long producerId;&lt;br/&gt;
@@ -226,20 +227,20 @@ public String toString() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final class PartitionData {&lt;br/&gt;
+    public static final class PartitionData&amp;lt;T extends BaseRecords&amp;gt; {&lt;br/&gt;
         public final Errors error;&lt;br/&gt;
         public final long highWatermark;&lt;br/&gt;
         public final long lastStableOffset;&lt;br/&gt;
         public final long logStartOffset;&lt;br/&gt;
         public final List&amp;lt;AbortedTransaction&amp;gt; abortedTransactions;&lt;/li&gt;
	&lt;li&gt;public final Records records;&lt;br/&gt;
+        public final T records;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         public PartitionData(Errors error,&lt;br/&gt;
                              long highWatermark,&lt;br/&gt;
                              long lastStableOffset,&lt;br/&gt;
                              long logStartOffset,&lt;br/&gt;
                              List&amp;lt;AbortedTransaction&amp;gt; abortedTransactions,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Records records) {&lt;br/&gt;
+                             T records) {&lt;br/&gt;
             this.error = error;&lt;br/&gt;
             this.highWatermark = highWatermark;&lt;br/&gt;
             this.lastStableOffset = lastStableOffset;&lt;br/&gt;
@@ -297,16 +298,18 @@ public String toString() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param throttleTimeMs    The time in milliseconds that the response was throttled&lt;/li&gt;
	&lt;li&gt;@param sessionId         The fetch session id.&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public FetchResponse(Errors error, LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;gt; responseData,&lt;/li&gt;
	&lt;li&gt;int throttleTimeMs, int sessionId) {&lt;br/&gt;
+    public FetchResponse(Errors error,&lt;br/&gt;
+                         LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;lt;T&amp;gt;&amp;gt; responseData,&lt;br/&gt;
+                         int throttleTimeMs,&lt;br/&gt;
+                         int sessionId) 
{
         this.error = error;
         this.responseData = responseData;
         this.throttleTimeMs = throttleTimeMs;
         this.sessionId = sessionId;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public FetchResponse(Struct struct) {&lt;/li&gt;
	&lt;li&gt;LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    public static FetchResponse&amp;lt;MemoryRecords&amp;gt; parse(Struct struct) {&lt;br/&gt;
+        LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {&lt;br/&gt;
             Struct topicResponse = (Struct) topicResponseObj;&lt;br/&gt;
             String topic = topicResponse.get(TOPIC_NAME);&lt;br/&gt;
@@ -323,7 +326,10 @@ public FetchResponse(Struct struct) {&lt;br/&gt;
                 if (partitionResponseHeader.hasField(LOG_START_OFFSET_KEY_NAME))&lt;br/&gt;
                     logStartOffset = partitionResponseHeader.getLong(LOG_START_OFFSET_KEY_NAME);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Records records = partitionResponse.getRecords(RECORD_SET_KEY_NAME);&lt;br/&gt;
+                BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);&lt;br/&gt;
+                if (!(baseRecords instanceof MemoryRecords))&lt;br/&gt;
+                    throw new IllegalStateException(&quot;Unknown records type found: &quot; + baseRecords.getClass());&lt;br/&gt;
+                MemoryRecords records = (MemoryRecords) baseRecords;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 List&amp;lt;AbortedTransaction&amp;gt; abortedTransactions = null;&lt;br/&gt;
                 if (partitionResponseHeader.hasField(ABORTED_TRANSACTIONS_KEY_NAME)) {&lt;br/&gt;
@@ -339,15 +345,13 @@ public FetchResponse(Struct struct) {&lt;br/&gt;
                     }&lt;br/&gt;
                 }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;PartitionData partitionData = new PartitionData(error, highWatermark, lastStableOffset, logStartOffset,&lt;/li&gt;
	&lt;li&gt;abortedTransactions, records);&lt;br/&gt;
+                PartitionData&amp;lt;MemoryRecords&amp;gt; partitionData = new PartitionData&amp;lt;&amp;gt;(error, highWatermark, lastStableOffset,&lt;br/&gt;
+                        logStartOffset, abortedTransactions, records);&lt;br/&gt;
                 responseData.put(new TopicPartition(topic, partition), partitionData);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;this.responseData = responseData;&lt;/li&gt;
	&lt;li&gt;this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);&lt;/li&gt;
	&lt;li&gt;this.error = Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0));&lt;/li&gt;
	&lt;li&gt;this.sessionId = struct.getOrElse(SESSION_ID, INVALID_SESSION_ID);&lt;br/&gt;
+        return new FetchResponse&amp;lt;&amp;gt;(Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0)), responseData,&lt;br/&gt;
+                struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME), struct.getOrElse(SESSION_ID, INVALID_SESSION_ID));&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
@@ -369,14 +373,14 @@ protected Send toSend(String dest, ResponseHeader responseHeader, short apiVersi&lt;br/&gt;
         Queue&amp;lt;Send&amp;gt; sends = new ArrayDeque&amp;lt;&amp;gt;();&lt;br/&gt;
         sends.add(new ByteBufferSend(dest, buffer));&lt;br/&gt;
         addResponseData(responseBodyStruct, throttleTimeMs, dest, sends);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new MultiSend(dest, sends);&lt;br/&gt;
+        return new MultiRecordsSend(dest, sends);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public Errors error() &lt;/p&gt;
{
         return error;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;gt; responseData() {&lt;br/&gt;
+    public LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;lt;T&amp;gt;&amp;gt; responseData() 
{
         return responseData;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -397,8 +401,8 @@ public int sessionId() &lt;/p&gt;
{
         return errorCounts;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static FetchResponse parse(ByteBuffer buffer, short version) {&lt;/li&gt;
	&lt;li&gt;return new FetchResponse(ApiKeys.FETCH.responseSchema(version).read(buffer));&lt;br/&gt;
+    public static FetchResponse&amp;lt;MemoryRecords&amp;gt; parse(ByteBuffer buffer, short version) 
{
+        return parse(ApiKeys.FETCH.responseSchema(version).read(buffer));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static void addResponseData(Struct struct, int throttleTimeMs, String dest, Queue&amp;lt;Send&amp;gt; sends) {&lt;br/&gt;
@@ -446,7 +450,7 @@ private static void addTopicData(String dest, Queue&amp;lt;Send&amp;gt; sends, Struct topicDat&lt;/p&gt;

&lt;p&gt;     private static void addPartitionData(String dest, Queue&amp;lt;Send&amp;gt; sends, Struct partitionData) &lt;/p&gt;
{
         Struct header = partitionData.getStruct(PARTITION_HEADER_KEY_NAME);
-        Records records = partitionData.getRecords(RECORD_SET_KEY_NAME);
+        BaseRecords records = partitionData.getRecords(RECORD_SET_KEY_NAME);
 
         // include the partition header and the size of the record set
         ByteBuffer buffer = ByteBuffer.allocate(header.sizeOf() + 4);
@@ -456,24 +460,25 @@ private static void addPartitionData(String dest, Queue&amp;lt;Send&amp;gt; sends, Struct part
         sends.add(new ByteBufferSend(dest, buffer));
 
         // finally the send for the record set itself
-        sends.add(new RecordsSend(dest, records));
+        sends.add(records.toSend(dest));
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static Struct toStruct(short version, int throttleTimeMs, Errors error,&lt;/li&gt;
	&lt;li&gt;Iterator&amp;lt;Map.Entry&amp;lt;TopicPartition, PartitionData&amp;gt;&amp;gt; partIterator, int sessionId) {&lt;br/&gt;
+    private static &amp;lt;T extends BaseRecords&amp;gt; Struct toStruct(short version, int throttleTimeMs, Errors error,&lt;br/&gt;
+                                                       Iterator&amp;lt;Map.Entry&amp;lt;TopicPartition, PartitionData&amp;lt;T&amp;gt;&amp;gt;&amp;gt; partIterator,&lt;br/&gt;
+                                                       int sessionId) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.FETCH.responseSchema(version));&lt;br/&gt;
         struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);&lt;br/&gt;
         struct.setIfExists(ERROR_CODE, error.code());&lt;br/&gt;
         struct.setIfExists(SESSION_ID, sessionId);&lt;/li&gt;
	&lt;li&gt;List&amp;lt;FetchRequest.TopicAndPartitionData&amp;lt;PartitionData&amp;gt;&amp;gt; topicsData =&lt;/li&gt;
	&lt;li&gt;FetchRequest.TopicAndPartitionData.batchByTopic(partIterator);&lt;br/&gt;
+        List&amp;lt;FetchRequest.TopicAndPartitionData&amp;lt;PartitionData&amp;lt;T&amp;gt;&amp;gt;&amp;gt; topicsData =&lt;br/&gt;
+                FetchRequest.TopicAndPartitionData.batchByTopic(partIterator);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (FetchRequest.TopicAndPartitionData&amp;lt;PartitionData&amp;gt; topicEntry: topicsData) {&lt;br/&gt;
+        for (FetchRequest.TopicAndPartitionData&amp;lt;PartitionData&amp;lt;T&amp;gt;&amp;gt; topicEntry: topicsData) {&lt;br/&gt;
             Struct topicData = struct.instance(RESPONSES_KEY_NAME);&lt;br/&gt;
             topicData.set(TOPIC_NAME, topicEntry.topic);&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Integer, PartitionData&amp;gt; partitionEntry : topicEntry.partitions.entrySet()) {&lt;/li&gt;
	&lt;li&gt;PartitionData fetchPartitionData = partitionEntry.getValue();&lt;br/&gt;
+            for (Map.Entry&amp;lt;Integer, PartitionData&amp;lt;T&amp;gt;&amp;gt; partitionEntry : topicEntry.partitions.entrySet()) {&lt;br/&gt;
+                PartitionData&amp;lt;T&amp;gt; fetchPartitionData = partitionEntry.getValue();&lt;br/&gt;
                 short errorCode = fetchPartitionData.error.code();&lt;br/&gt;
                 // If consumer sends FetchRequest V5 or earlier, the client library is not guaranteed to recognize the error code&lt;br/&gt;
                 // for KafkaStorageException. In this case the client library will translate KafkaStorageException to&lt;br/&gt;
@@ -524,7 +529,8 @@ private static Struct toStruct(short version, int throttleTimeMs, Errors error,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param partIterator  The partition iterator.&lt;/li&gt;
	&lt;li&gt;@return              The response size in bytes.&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static int sizeOf(short version, Iterator&amp;lt;Map.Entry&amp;lt;TopicPartition, PartitionData&amp;gt;&amp;gt; partIterator) {&lt;br/&gt;
+    public static &amp;lt;T extends BaseRecords&amp;gt; int sizeOf(short version,&lt;br/&gt;
+                                                     Iterator&amp;lt;Map.Entry&amp;lt;TopicPartition, PartitionData&amp;lt;T&amp;gt;&amp;gt;&amp;gt; partIterator) 
{
         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can
         // use arbitrary values here without affecting the result.
         return 4 + toStruct(version, 0, Errors.NONE, partIterator, INVALID_SESSION_ID).sizeOf();
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java
index ce722cf12e3..4886c6b218e 100644
--- a/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java
@@ -44,11 +44,10 @@
 import org.apache.kafka.common.record.MemoryRecords;
 import org.apache.kafka.common.record.MemoryRecordsBuilder;
 import org.apache.kafka.common.record.TimestampType;
+import org.apache.kafka.common.requests.FetchResponse;
 import org.apache.kafka.common.requests.AbstractRequest;
 import org.apache.kafka.common.requests.AbstractResponse;
 import org.apache.kafka.common.requests.FetchRequest;
-import org.apache.kafka.common.requests.FetchResponse;
-import org.apache.kafka.common.requests.FetchResponse.PartitionData;
 import org.apache.kafka.common.requests.FindCoordinatorResponse;
 import org.apache.kafka.common.requests.HeartbeatResponse;
 import org.apache.kafka.common.requests.IsolationLevel;
@@ -1709,8 +1708,8 @@ private ListOffsetResponse listOffsetsResponse(Map&amp;lt;TopicPartition, Long&amp;gt; partiti
     }&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse fetchResponse(Map&amp;lt;TopicPartition, FetchInfo&amp;gt; fetches) {&lt;/li&gt;
	&lt;li&gt;LinkedHashMap&amp;lt;TopicPartition, PartitionData&amp;gt; tpResponses = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; fetchResponse(Map&amp;lt;TopicPartition, FetchInfo&amp;gt; fetches) {&lt;br/&gt;
+        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; tpResponses = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, FetchInfo&amp;gt; fetchEntry : fetches.entrySet()) {&lt;br/&gt;
             TopicPartition partition = fetchEntry.getKey();&lt;br/&gt;
             long fetchOffset = fetchEntry.getValue().offset;&lt;br/&gt;
@@ -1725,8 +1724,10 @@ private FetchResponse fetchResponse(Map&amp;lt;TopicPartition, FetchInfo&amp;gt; fetches) 
{
                     builder.append(0L, (&quot;key-&quot; + i).getBytes(), (&quot;value-&quot; + i).getBytes());
                 records = builder.build();
             }&lt;/li&gt;
	&lt;li&gt;tpResponses.put(partition, new FetchResponse.PartitionData(Errors.NONE, 0, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L,&lt;/li&gt;
	&lt;li&gt;null, records));&lt;br/&gt;
+            tpResponses.put(partition,&lt;br/&gt;
+                            new FetchResponse.PartitionData(&lt;br/&gt;
+                                    Errors.NONE, 0, FetchResponse.INVALID_LAST_STABLE_OFFSET,&lt;br/&gt;
+                                    0L, null, records));&lt;br/&gt;
         }&lt;br/&gt;
         return new FetchResponse(Errors.NONE, tpResponses, 0, INVALID_SESSION_ID);&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 76cde8eddca..9164daa426c 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -62,11 +62,11 @@&lt;br/&gt;
 import org.apache.kafka.common.record.Records;&lt;br/&gt;
 import org.apache.kafka.common.record.SimpleRecord;&lt;br/&gt;
 import org.apache.kafka.common.record.TimestampType;&lt;br/&gt;
+import org.apache.kafka.common.requests.FetchResponse;&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.ApiVersionsResponse;&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest.PartitionData;&lt;br/&gt;
-import org.apache.kafka.common.requests.FetchResponse;&lt;br/&gt;
 import org.apache.kafka.common.requests.IsolationLevel;&lt;br/&gt;
 import org.apache.kafka.common.requests.ListOffsetRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.ListOffsetResponse;&lt;br/&gt;
@@ -885,15 +885,15 @@ public void testCompletedFetchRemoval() {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100, FetchResponse.INVALID_LAST_STABLE_OFFSET,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse.INVALID_LOG_START_OFFSET, null, records));&lt;br/&gt;
+                FetchResponse.INVALID_LOG_START_OFFSET, null, records));&lt;br/&gt;
         partitions.put(tp0, new FetchResponse.PartitionData(Errors.OFFSET_OUT_OF_RANGE, 100,&lt;/li&gt;
	&lt;li&gt;FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));&lt;br/&gt;
+                FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));&lt;br/&gt;
         partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100L, 4,&lt;/li&gt;
	&lt;li&gt;0L, null, nextRecords));&lt;br/&gt;
+                0L, null, nextRecords));&lt;br/&gt;
         partitions.put(tp3, new FetchResponse.PartitionData(Errors.NONE, 100L, 4,&lt;/li&gt;
	&lt;li&gt;0L, null, partialRecords));&lt;br/&gt;
+                0L, null, partialRecords));&lt;br/&gt;
         client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions),&lt;/li&gt;
	&lt;li&gt;0, INVALID_SESSION_ID));&lt;br/&gt;
+                0, INVALID_SESSION_ID));&lt;br/&gt;
         consumerClient.poll(0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; fetchedRecords = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -948,7 +948,7 @@ public void testSeekBeforeException() {&lt;br/&gt;
         assertEquals(1, fetcher.sendFetches());&lt;br/&gt;
         Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         partitions.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, records));&lt;br/&gt;
+                FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, records));&lt;br/&gt;
         client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));&lt;br/&gt;
         consumerClient.poll(0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -959,7 +959,7 @@ public void testSeekBeforeException() {&lt;br/&gt;
         assertEquals(1, fetcher.sendFetches());&lt;br/&gt;
         partitions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         partitions.put(tp1, new FetchResponse.PartitionData(Errors.OFFSET_OUT_OF_RANGE, 100,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));&lt;br/&gt;
+                FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY));&lt;br/&gt;
         client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions), 0, INVALID_SESSION_ID));&lt;br/&gt;
         consumerClient.poll(0);&lt;br/&gt;
         assertEquals(1, fetcher.fetchedRecords().get(tp0).size());&lt;br/&gt;
@@ -1660,7 +1660,7 @@ public void testFetchResponseMetricsWithOnePartitionError() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(1, fetcher.sendFetches());&lt;br/&gt;
         client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;0, INVALID_SESSION_ID));&lt;br/&gt;
+                0, INVALID_SESSION_ID));&lt;br/&gt;
         consumerClient.poll(0);&lt;br/&gt;
         fetcher.fetchedRecords();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1700,7 +1700,7 @@ public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {&lt;br/&gt;
                 MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(&quot;val&quot;.getBytes()))));&lt;/p&gt;

&lt;p&gt;         client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;0, INVALID_SESSION_ID));&lt;br/&gt;
+                0, INVALID_SESSION_ID));&lt;br/&gt;
         consumerClient.poll(0);&lt;br/&gt;
         fetcher.fetchedRecords();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -2267,9 +2267,9 @@ public void testConsumingViaIncrementalFetchRequests() {&lt;br/&gt;
         // Fetch some records and establish an incremental fetch session.&lt;br/&gt;
         LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions1 = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         partitions1.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 2L,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;2, 0L, null, this.records));&lt;br/&gt;
+                2, 0L, null, this.records));&lt;br/&gt;
         partitions1.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100L,&lt;/li&gt;
	&lt;li&gt;FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, emptyRecords));&lt;br/&gt;
+                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, emptyRecords));&lt;br/&gt;
         FetchResponse resp1 = new FetchResponse(Errors.NONE, partitions1, 0, 123);&lt;br/&gt;
         client.prepareResponse(resp1);&lt;br/&gt;
         assertEquals(1, fetcher.sendFetches());&lt;br/&gt;
@@ -2308,7 +2308,7 @@ public void testConsumingViaIncrementalFetchRequests() 
{
         // The third response contains some new records for tp0.
         LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions3 = new LinkedHashMap&amp;lt;&amp;gt;();
         partitions3.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100L,
-            4, 0L, null, this.nextRecords));
+                4, 0L, null, this.nextRecords));
         new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions1), 0, INVALID_SESSION_ID);
         FetchResponse resp3 = new FetchResponse(Errors.NONE, partitions3, 0, 123);
         client.prepareResponse(resp3);
@@ -2446,33 +2446,33 @@ private ListOffsetResponse listOffsetResponse(TopicPartition tp, Errors error, l
         return new ListOffsetResponse(allPartitionData);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse fullFetchResponseWithAbortedTransactions(MemoryRecords records,&lt;/li&gt;
	&lt;li&gt;List&amp;lt;FetchResponse.AbortedTransaction&amp;gt; abortedTransactions,&lt;/li&gt;
	&lt;li&gt;Errors error,&lt;/li&gt;
	&lt;li&gt;long lastStableOffset,&lt;/li&gt;
	&lt;li&gt;long hw,&lt;/li&gt;
	&lt;li&gt;int throttleTime) {&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions = Collections.singletonMap(tp0,&lt;/li&gt;
	&lt;li&gt;new FetchResponse.PartitionData(error, hw, lastStableOffset, 0L, abortedTransactions, records));&lt;/li&gt;
	&lt;li&gt;return new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions), throttleTime, INVALID_SESSION_ID);&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; fullFetchResponseWithAbortedTransactions(MemoryRecords records,&lt;br/&gt;
+                                                                                  List&amp;lt;FetchResponse.AbortedTransaction&amp;gt; abortedTransactions,&lt;br/&gt;
+                                                                                  Errors error,&lt;br/&gt;
+                                                                                  long lastStableOffset,&lt;br/&gt;
+                                                                                  long hw,&lt;br/&gt;
+                                                                                  int throttleTime) 
{
+        Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; partitions = Collections.singletonMap(tp0,
+                new FetchResponse.PartitionData&amp;lt;&amp;gt;(error, hw, lastStableOffset, 0L, abortedTransactions, records));
+        return new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions), throttleTime, INVALID_SESSION_ID);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse fullFetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw, int throttleTime) {&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; fullFetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw, int throttleTime) 
{
         return fullFetchResponse(tp, records, error, hw, FetchResponse.INVALID_LAST_STABLE_OFFSET, throttleTime);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse fullFetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw,&lt;/li&gt;
	&lt;li&gt;long lastStableOffset, int throttleTime) {&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions = Collections.singletonMap(tp,&lt;/li&gt;
	&lt;li&gt;new FetchResponse.PartitionData(error, hw, lastStableOffset, 0L, null, records));&lt;/li&gt;
	&lt;li&gt;return new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions), throttleTime, INVALID_SESSION_ID);&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; fullFetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw,&lt;br/&gt;
+                                            long lastStableOffset, int throttleTime) 
{
+        Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; partitions = Collections.singletonMap(tp,
+                new FetchResponse.PartitionData&amp;lt;&amp;gt;(error, hw, lastStableOffset, 0L, null, records));
+        return new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions), throttleTime, INVALID_SESSION_ID);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse fetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw,&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; fetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw,&lt;br/&gt;
                                         long lastStableOffset, long logStartOffset, int throttleTime) 
{
-        Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; partitions = Collections.singletonMap(tp,
-                new FetchResponse.PartitionData(error, hw, lastStableOffset, logStartOffset, null, records));
-        return new FetchResponse(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions), throttleTime, INVALID_SESSION_ID);
+        Map&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; partitions = Collections.singletonMap(tp,
+                new FetchResponse.PartitionData&amp;lt;&amp;gt;(error, hw, lastStableOffset, logStartOffset, null, records));
+        return new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, new LinkedHashMap&amp;lt;&amp;gt;(partitions), throttleTime, INVALID_SESSION_ID);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private MetadataResponse newMetadataResponse(String topic, Errors error) {&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java b/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java&lt;br/&gt;
index fdd3ede16cc..f8b6dd4140c 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java&lt;br/&gt;
@@ -17,6 +17,7 @@&lt;br/&gt;
 package org.apache.kafka.common.record;&lt;/p&gt;

&lt;p&gt; import org.apache.kafka.common.KafkaException;&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.header.Header;&lt;br/&gt;
 import org.apache.kafka.common.header.internals.RecordHeader;&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime;&lt;br/&gt;
@@ -38,11 +39,11 @@&lt;/p&gt;

&lt;p&gt; import static java.util.Arrays.asList;&lt;br/&gt;
 import static org.apache.kafka.test.TestUtils.tempFile;&lt;br/&gt;
+import static org.junit.Assert.assertArrayEquals;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertFalse;&lt;br/&gt;
 import static org.junit.Assert.assertTrue;&lt;br/&gt;
 import static org.junit.Assert.fail;&lt;br/&gt;
-import static org.junit.Assert.assertArrayEquals;&lt;/p&gt;

&lt;p&gt; public class FileRecordsTest {&lt;/p&gt;

&lt;p&gt;@@ -347,6 +348,12 @@ public void testFormatConversionWithPartialMessage() throws IOException &lt;/p&gt;
{
         Records messageV0 = slice.downConvert(RecordBatch.MAGIC_VALUE_V0, 0, time).records();
         assertTrue(&quot;No message should be there&quot;, batches(messageV0).isEmpty());
         assertEquals(&quot;There should be &quot; + (size - 1) + &quot; bytes&quot;, size - 1, messageV0.sizeInBytes());
+
+        // Lazy down-conversion will not return any messages for a partial input batch
+        TopicPartition tp = new TopicPartition(&quot;topic-1&quot;, 0);
+        LazyDownConversionRecords lazyRecords = new LazyDownConversionRecords(tp, slice, RecordBatch.MAGIC_VALUE_V0, 0, Time.SYSTEM);
+        Iterator&amp;lt;ConvertedRecords&amp;gt; it = lazyRecords.iterator(16 * 1024L);
+        assertTrue(&quot;No messages should be returned&quot;, !it.hasNext());
     }

&lt;p&gt;     @Test&lt;br/&gt;
@@ -402,8 +409,7 @@ private void doTestConversion(CompressionType compressionType, byte toMagic) thr&lt;br/&gt;
         try (FileRecords fileRecords = FileRecords.open(tempFile())) {&lt;br/&gt;
             fileRecords.append(MemoryRecords.readableRecords(buffer));&lt;br/&gt;
             fileRecords.flush();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Records convertedRecords = fileRecords.downConvert(toMagic, 0L, time).records();&lt;/li&gt;
	&lt;li&gt;verifyConvertedRecords(records, offsets, convertedRecords, compressionType, toMagic);&lt;br/&gt;
+            downConvertAndVerifyRecords(records, offsets, fileRecords, compressionType, toMagic, 0L, time);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             if (toMagic &amp;lt;= RecordBatch.MAGIC_VALUE_V1 &amp;amp;&amp;amp; compressionType == CompressionType.NONE) &lt;/p&gt;
{
                 long firstOffset;
@@ -411,17 +417,15 @@ private void doTestConversion(CompressionType compressionType, byte toMagic) thr
                     firstOffset = 11L; // v1 record
                 else
                     firstOffset = 17; // v2 record
-                Records convertedRecords2 = fileRecords.downConvert(toMagic, firstOffset, time).records();
                 List&amp;lt;Long&amp;gt; filteredOffsets = new ArrayList&amp;lt;&amp;gt;(offsets);
                 List&amp;lt;SimpleRecord&amp;gt; filteredRecords = new ArrayList&amp;lt;&amp;gt;(records);
                 int index = filteredOffsets.indexOf(firstOffset) - 1;
                 filteredRecords.remove(index);
                 filteredOffsets.remove(index);
-                verifyConvertedRecords(filteredRecords, filteredOffsets, convertedRecords2, compressionType, toMagic);
+                downConvertAndVerifyRecords(filteredRecords, filteredOffsets, fileRecords, compressionType, toMagic, firstOffset, time);
             }
&lt;p&gt; else &lt;/p&gt;
{
                 // firstOffset doesn&apos;t have any effect in this case
-                Records convertedRecords2 = fileRecords.downConvert(toMagic, 10L, time).records();
-                verifyConvertedRecords(records, offsets, convertedRecords2, compressionType, toMagic);
+                downConvertAndVerifyRecords(records, offsets, fileRecords, compressionType, toMagic, 10L, time);
             }
&lt;p&gt;         }&lt;br/&gt;
     }&lt;br/&gt;
@@ -430,40 +434,98 @@ private String utf8(ByteBuffer buffer) &lt;/p&gt;
{
         return Utils.utf8(buffer, buffer.remaining());
     }

&lt;p&gt;+    private void downConvertAndVerifyRecords(List&amp;lt;SimpleRecord&amp;gt; initialRecords,&lt;br/&gt;
+                                             List&amp;lt;Long&amp;gt; initialOffsets,&lt;br/&gt;
+                                             FileRecords fileRecords,&lt;br/&gt;
+                                             CompressionType compressionType,&lt;br/&gt;
+                                             byte toMagic,&lt;br/&gt;
+                                             long firstOffset,&lt;br/&gt;
+                                             Time time) {&lt;br/&gt;
+        long numBatches = 0;&lt;br/&gt;
+        long minBatchSize = Long.MAX_VALUE;&lt;br/&gt;
+        long maxBatchSize = Long.MIN_VALUE;&lt;br/&gt;
+        for (RecordBatch batch : fileRecords.batches()) &lt;/p&gt;
{
+            minBatchSize = Math.min(minBatchSize, batch.sizeInBytes());
+            maxBatchSize = Math.max(maxBatchSize, batch.sizeInBytes());
+            numBatches++;
+        }
&lt;p&gt;+&lt;br/&gt;
+        // Test the normal down-conversion path&lt;br/&gt;
+        List&amp;lt;Records&amp;gt; convertedRecords = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        convertedRecords.add(fileRecords.downConvert(toMagic, firstOffset, time).records());&lt;br/&gt;
+        verifyConvertedRecords(initialRecords, initialOffsets, convertedRecords, compressionType, toMagic);&lt;br/&gt;
+        convertedRecords.clear();&lt;br/&gt;
+&lt;br/&gt;
+        // Test the lazy down-conversion path&lt;br/&gt;
+        List&amp;lt;Long&amp;gt; maximumReadSize = asList(16L * 1024L,&lt;br/&gt;
+                (long) fileRecords.sizeInBytes(),&lt;br/&gt;
+                (long) fileRecords.sizeInBytes() - 1,&lt;br/&gt;
+                (long) fileRecords.sizeInBytes() / 4,&lt;br/&gt;
+                maxBatchSize + 1,&lt;br/&gt;
+                1L);&lt;br/&gt;
+        for (long readSize : maximumReadSize) &lt;/p&gt;
{
+            TopicPartition tp = new TopicPartition(&quot;topic-1&quot;, 0);
+            LazyDownConversionRecords lazyRecords = new LazyDownConversionRecords(tp, fileRecords, toMagic, firstOffset, Time.SYSTEM);
+            Iterator&amp;lt;ConvertedRecords&amp;gt; it = lazyRecords.iterator(readSize);
+            while (it.hasNext())
+                convertedRecords.add(it.next().records());
+
+            // Check if chunking works as expected. The only way to predictably test for this is by testing the edge cases.
+            // 1. If maximum read size is greater than the size of all batches combined, we must get all down-conversion
+            //    records in exactly two batches; the first chunk is pre down-converted and returned, and the second chunk
+            //    contains the remaining batches.
+            // 2. If maximum read size is just smaller than the size of all batches combined, we must get results in two
+            //    chunks.
+            // 3. If maximum read size is less than the size of a single record, we get one batch in each chunk.
+            if (readSize &amp;gt;= fileRecords.sizeInBytes())
+                assertEquals(2, convertedRecords.size());
+            else if (readSize == fileRecords.sizeInBytes() - 1)
+                assertEquals(2, convertedRecords.size());
+            else if (readSize &amp;lt;= minBatchSize)
+                assertEquals(numBatches, convertedRecords.size());
+
+            verifyConvertedRecords(initialRecords, initialOffsets, convertedRecords, compressionType, toMagic);
+            convertedRecords.clear();
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     private void verifyConvertedRecords(List&amp;lt;SimpleRecord&amp;gt; initialRecords,&lt;br/&gt;
                                         List&amp;lt;Long&amp;gt; initialOffsets,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Records convertedRecords,&lt;br/&gt;
+                                        List&amp;lt;Records&amp;gt; convertedRecordsList,&lt;br/&gt;
                                         CompressionType compressionType,&lt;br/&gt;
                                         byte magicByte) {&lt;br/&gt;
         int i = 0;&lt;/li&gt;
	&lt;li&gt;for (RecordBatch batch : convertedRecords.batches()) {&lt;/li&gt;
	&lt;li&gt;assertTrue(&quot;Magic byte should be lower than or equal to &quot; + magicByte, batch.magic() &amp;lt;= magicByte);&lt;/li&gt;
	&lt;li&gt;if (batch.magic() == RecordBatch.MAGIC_VALUE_V0)&lt;/li&gt;
	&lt;li&gt;assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;assertEquals(TimestampType.CREATE_TIME, batch.timestampType());&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Compression type should not be affected by conversion&quot;, compressionType, batch.compressionType());&lt;/li&gt;
	&lt;li&gt;for (Record record : batch) {&lt;/li&gt;
	&lt;li&gt;assertTrue(&quot;Inner record should have magic &quot; + magicByte, record.hasMagic(batch.magic()));&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Offset should not change&quot;, initialOffsets.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.longValue(), record.offset());&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Key should not change&quot;, utf8(initialRecords.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.key()), utf8(record.key()));&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Value should not change&quot;, utf8(initialRecords.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.value()), utf8(record.value()));&lt;/li&gt;
	&lt;li&gt;assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));&lt;/li&gt;
	&lt;li&gt;if (batch.magic() == RecordBatch.MAGIC_VALUE_V0) 
{
-                    assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());
-                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));
-                    assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
-                }
&lt;p&gt; else if (batch.magic() == RecordBatch.MAGIC_VALUE_V1) &lt;/p&gt;
{
-                    assertEquals(&quot;Timestamp should not change&quot;, initialRecords.get(i).timestamp(), record.timestamp());
-                    assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));
-                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
-                }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Timestamp should not change&quot;, initialRecords.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.timestamp(), record.timestamp());&lt;/li&gt;
	&lt;li&gt;assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));&lt;/li&gt;
	&lt;li&gt;assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));&lt;/li&gt;
	&lt;li&gt;assertArrayEquals(&quot;Headers should not change&quot;, initialRecords.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.headers(), record.headers());&lt;br/&gt;
+&lt;br/&gt;
+        for (Records convertedRecords : convertedRecordsList) {&lt;br/&gt;
+            for (RecordBatch batch : convertedRecords.batches()) {&lt;br/&gt;
+                assertTrue(&quot;Magic byte should be lower than or equal to &quot; + magicByte, batch.magic() &amp;lt;= magicByte);&lt;br/&gt;
+                if (batch.magic() == RecordBatch.MAGIC_VALUE_V0)&lt;br/&gt;
+                    assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());&lt;br/&gt;
+                else&lt;br/&gt;
+                    assertEquals(TimestampType.CREATE_TIME, batch.timestampType());&lt;br/&gt;
+                assertEquals(&quot;Compression type should not be affected by conversion&quot;, compressionType, batch.compressionType());&lt;br/&gt;
+                for (Record record : batch) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    assertTrue(&amp;quot;Inner record should have magic &amp;quot; + magicByte, record.hasMagic(batch.magic()));+                    assertEquals(&amp;quot;Offset should not change&amp;quot;, initialOffsets.get(i).longValue(), record.offset());+                    assertEquals(&amp;quot;Key should not change&amp;quot;, utf8(initialRecords.get(i).key()), utf8(record.key()));+                    assertEquals(&amp;quot;Value should not change&amp;quot;, utf8(initialRecords.get(i).value()), utf8(record.value()));+                    assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));+                    if (batch.magic() == RecordBatch.MAGIC_VALUE_V0) {
+                        assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());
+                        assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));
+                        assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
+                    } else if (batch.magic() == RecordBatch.MAGIC_VALUE_V1) {
+                        assertEquals(&quot;Timestamp should not change&quot;, initialRecords.get(i).timestamp(), record.timestamp());
+                        assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));
+                        assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
+                    } else {
+                        assertEquals(&quot;Timestamp should not change&quot;, initialRecords.get(i).timestamp(), record.timestamp());
+                        assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));
+                        assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
+                        assertArrayEquals(&quot;Headers should not change&quot;, initialRecords.get(i).headers(), record.headers());
+                    }+                    i += 1;                 }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;i += 1;&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
         assertEquals(initialOffsets.size(), i);&lt;br/&gt;
@@ -490,5 +552,4 @@ private void append(FileRecords fileRecords, byte[][] values) throws IOException&lt;br/&gt;
         }&lt;br/&gt;
         fileRecords.flush();&lt;br/&gt;
     }&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/record/LazyDownConversionRecordsTest.java b/clients/src/test/java/org/apache/kafka/common/record/LazyDownConversionRecordsTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..87656038f9b
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/LazyDownConversionRecordsTest.java&lt;br/&gt;
@@ -0,0 +1,203 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.header.Header;&lt;br/&gt;
+import org.apache.kafka.common.header.internals.RecordHeader;&lt;br/&gt;
+import org.apache.kafka.common.utils.Time;&lt;br/&gt;
+import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.junit.Test;&lt;br/&gt;
+import org.junit.runner.RunWith;&lt;br/&gt;
+import org.junit.runners.Parameterized;&lt;br/&gt;
+&lt;br/&gt;
+import java.io.File;&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
+import java.io.RandomAccessFile;&lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
+import java.nio.channels.FileChannel;&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
+import java.util.Collection;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+&lt;br/&gt;
+import static java.util.Arrays.asList;&lt;br/&gt;
+import static org.apache.kafka.test.TestUtils.tempFile;&lt;br/&gt;
+import static org.junit.Assert.assertArrayEquals;&lt;br/&gt;
+import static org.junit.Assert.assertEquals;&lt;br/&gt;
+import static org.junit.Assert.assertFalse;&lt;br/&gt;
+import static org.junit.Assert.assertTrue;&lt;br/&gt;
+&lt;br/&gt;
+@RunWith(value = Parameterized.class)&lt;br/&gt;
+public class LazyDownConversionRecordsTest {&lt;br/&gt;
+    private final CompressionType compressionType;&lt;br/&gt;
+    private final byte toMagic;&lt;br/&gt;
+    private final DownConversionTest test;&lt;br/&gt;
+&lt;br/&gt;
+    public LazyDownConversionRecordsTest(CompressionType compressionType, byte toMagic, DownConversionTest test) 
{
+        this.compressionType = compressionType;
+        this.toMagic = toMagic;
+        this.test = test;
+    }
&lt;p&gt;+&lt;br/&gt;
+    enum DownConversionTest &lt;/p&gt;
{
+        DEFAULT,
+        OVERFLOW,
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Parameterized.Parameters(name = &quot;compressionType=&lt;/p&gt;
{0}
&lt;p&gt;, toMagic=&lt;/p&gt;
{1}
&lt;p&gt;, test=&lt;/p&gt;
{2}
&lt;p&gt;&quot;)&lt;br/&gt;
+    public static Collection&amp;lt;Object[]&amp;gt; data() {&lt;br/&gt;
+        List&amp;lt;Object[]&amp;gt; values = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        for (byte toMagic = RecordBatch.MAGIC_VALUE_V0; toMagic &amp;lt;= RecordBatch.CURRENT_MAGIC_VALUE; toMagic++) {&lt;br/&gt;
+            for (DownConversionTest test : DownConversionTest.values()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                values.add(new Object[]{CompressionType.NONE, toMagic, test});+                values.add(new Object[]{CompressionType.GZIP, toMagic, test});+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+        }&lt;br/&gt;
+        return values;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void doTestConversion() throws IOException {&lt;br/&gt;
+        List&amp;lt;Long&amp;gt; offsets = asList(0L, 2L, 3L, 9L, 11L, 15L, 16L, 17L, 22L, 24L);&lt;br/&gt;
+&lt;br/&gt;
+        Header[] headers = &lt;/p&gt;
{new RecordHeader(&quot;headerKey1&quot;, &quot;headerValue1&quot;.getBytes()),
+                            new RecordHeader(&quot;headerKey2&quot;, &quot;headerValue2&quot;.getBytes()),
+                            new RecordHeader(&quot;headerKey3&quot;, &quot;headerValue3&quot;.getBytes())}
&lt;p&gt;;&lt;br/&gt;
+&lt;br/&gt;
+        List&amp;lt;SimpleRecord&amp;gt; records = asList(&lt;br/&gt;
+                new SimpleRecord(1L, &quot;k1&quot;.getBytes(), &quot;hello&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(2L, &quot;k2&quot;.getBytes(), &quot;goodbye&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(3L, &quot;k3&quot;.getBytes(), &quot;hello again&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(4L, &quot;k4&quot;.getBytes(), &quot;goodbye for now&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(5L, &quot;k5&quot;.getBytes(), &quot;hello again&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(6L, &quot;k6&quot;.getBytes(), &quot;I sense indecision&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(7L, &quot;k7&quot;.getBytes(), &quot;what now&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(8L, &quot;k8&quot;.getBytes(), &quot;running out&quot;.getBytes(), headers),&lt;br/&gt;
+                new SimpleRecord(9L, &quot;k9&quot;.getBytes(), &quot;ok, almost done&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(10L, &quot;k10&quot;.getBytes(), &quot;finally&quot;.getBytes(), headers));&lt;br/&gt;
+        assertEquals(&quot;incorrect test setup&quot;, offsets.size(), records.size());&lt;br/&gt;
+&lt;br/&gt;
+        ByteBuffer buffer = ByteBuffer.allocate(1024);&lt;br/&gt;
+        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType,&lt;br/&gt;
+                TimestampType.CREATE_TIME, 0L);&lt;br/&gt;
+        for (int i = 0; i &amp;lt; 3; i++)&lt;br/&gt;
+            builder.appendWithOffset(offsets.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, records.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;);&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+&lt;br/&gt;
+        builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME,&lt;br/&gt;
+                0L);&lt;br/&gt;
+        for (int i = 3; i &amp;lt; 6; i++)&lt;br/&gt;
+            builder.appendWithOffset(offsets.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, records.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;);&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+&lt;br/&gt;
+        builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME,&lt;br/&gt;
+                0L);&lt;br/&gt;
+        for (int i = 6; i &amp;lt; 10; i++)&lt;br/&gt;
+            builder.appendWithOffset(offsets.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, records.get&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;);&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+&lt;br/&gt;
+        buffer.flip();&lt;br/&gt;
+&lt;br/&gt;
+        try (FileRecords inputRecords = FileRecords.open(tempFile())) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            MemoryRecords memoryRecords = MemoryRecords.readableRecords(buffer);+            inputRecords.append(memoryRecords);+            inputRecords.flush();++            LazyDownConversionRecords lazyRecords = new LazyDownConversionRecords(new TopicPartition(&amp;quot;test&amp;quot;, 1),+                    inputRecords, toMagic, 0L, Time.SYSTEM);+            LazyDownConversionRecordsSend lazySend = lazyRecords.toSend(&amp;quot;foo&amp;quot;);+            File outputFile = tempFile();+            FileChannel channel = new RandomAccessFile(outputFile, &amp;quot;rw&amp;quot;).getChannel();++            // Size of lazy records is at least as much as the size of underlying records+            assertTrue(lazyRecords.sizeInBytes() &amp;gt;= inputRecords.sizeInBytes());++            int toWrite;+            int written = 0;+            List&amp;lt;SimpleRecord&amp;gt; recordsBeingConverted;+            List&amp;lt;Long&amp;gt; offsetsOfRecords;+            switch (test) {
+                case DEFAULT:
+                    toWrite = inputRecords.sizeInBytes();
+                    recordsBeingConverted = records;
+                    offsetsOfRecords = offsets;
+                    break;
+                case OVERFLOW:
+                    toWrite = inputRecords.sizeInBytes() * 2;
+                    recordsBeingConverted = records;
+                    offsetsOfRecords = offsets;
+                    break;
+                default:
+                    throw new IllegalArgumentException();
+            }+            while (written &amp;lt; toWrite)+                written += lazySend.writeTo(channel, written, toWrite - written);++            FileRecords convertedRecords = FileRecords.open(outputFile, true, (int) channel.size(), false);+            ByteBuffer convertedRecordsBuffer = ByteBuffer.allocate(convertedRecords.sizeInBytes());+            convertedRecords.readInto(convertedRecordsBuffer, 0);+            MemoryRecords convertedMemoryRecords = MemoryRecords.readableRecords(convertedRecordsBuffer);+            verifyDownConvertedRecords(recordsBeingConverted, offsetsOfRecords, convertedMemoryRecords, compressionType, toMagic);++            convertedRecords.close();+            channel.close();+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    private String utf8(ByteBuffer buffer) &lt;/p&gt;
{
+        return Utils.utf8(buffer, buffer.remaining());
+    }
&lt;p&gt;+&lt;br/&gt;
+    private void verifyDownConvertedRecords(List&amp;lt;SimpleRecord&amp;gt; initialRecords,&lt;br/&gt;
+                                            List&amp;lt;Long&amp;gt; initialOffsets,&lt;br/&gt;
+                                            MemoryRecords downConvertedRecords,&lt;br/&gt;
+                                            CompressionType compressionType,&lt;br/&gt;
+                                            byte toMagic) {&lt;br/&gt;
+        int i = 0;&lt;br/&gt;
+        for (RecordBatch batch : downConvertedRecords.batches()) {&lt;br/&gt;
+            assertTrue(&quot;Magic byte should be lower than or equal to &quot; + toMagic, batch.magic() &amp;lt;= toMagic);&lt;br/&gt;
+            if (batch.magic() == RecordBatch.MAGIC_VALUE_V0)&lt;br/&gt;
+                assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());&lt;br/&gt;
+            else&lt;br/&gt;
+                assertEquals(TimestampType.CREATE_TIME, batch.timestampType());&lt;br/&gt;
+            assertEquals(&quot;Compression type should not be affected by conversion&quot;, compressionType, batch.compressionType());&lt;br/&gt;
+            for (Record record : batch) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                assertTrue(&amp;quot;Inner record should have magic &amp;quot; + toMagic, record.hasMagic(batch.magic()));+                assertEquals(&amp;quot;Offset should not change&amp;quot;, initialOffsets.get(i).longValue(), record.offset());+                assertEquals(&amp;quot;Key should not change&amp;quot;, utf8(initialRecords.get(i).key()), utf8(record.key()));+                assertEquals(&amp;quot;Value should not change&amp;quot;, utf8(initialRecords.get(i).value()), utf8(record.value()));+                assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));+                if (batch.magic() == RecordBatch.MAGIC_VALUE_V0) {
+                    assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());
+                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));
+                    assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
+                } else if (batch.magic() == RecordBatch.MAGIC_VALUE_V1) {
+                    assertEquals(&quot;Timestamp should not change&quot;, initialRecords.get(i).timestamp(), record.timestamp());
+                    assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));
+                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
+                } else {
+                    assertEquals(&quot;Timestamp should not change&quot;, initialRecords.get(i).timestamp(), record.timestamp());
+                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));
+                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));
+                    assertArrayEquals(&quot;Headers should not change&quot;, initialRecords.get(i).headers(), record.headers());
+                }+                i += 1;+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+        }&lt;br/&gt;
+        assertEquals(initialOffsets.size(), i);&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java&lt;br/&gt;
index a90fb299c52..36b14a2f40d 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java&lt;br/&gt;
@@ -472,7 +472,7 @@ public void convertV2ToV1UsingMixedCreateAndLogAppendTime() {&lt;br/&gt;
         MemoryRecords records = convertedRecords.records();&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // Transactional markers are skipped when down converting to V1, so exclude them from size&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;verifyRecordsProcessingStats(convertedRecords.recordsProcessingStats(),&lt;br/&gt;
+        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(),&lt;br/&gt;
                 3, 3, records.sizeInBytes(), sizeExcludingTxnMarkers);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;? extends RecordBatch&amp;gt; batches = Utils.toList(records.batches().iterator());&lt;br/&gt;
@@ -513,7 +513,7 @@ public void convertToV1WithMixedV0AndV2Data() {&lt;br/&gt;
         ConvertedRecords&amp;lt;MemoryRecords&amp;gt; convertedRecords = MemoryRecords.readableRecords(buffer)&lt;br/&gt;
                 .downConvert(RecordBatch.MAGIC_VALUE_V1, 0, time);&lt;br/&gt;
         MemoryRecords records = convertedRecords.records();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;verifyRecordsProcessingStats(convertedRecords.recordsProcessingStats(), 3, 2,&lt;br/&gt;
+        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 2,&lt;br/&gt;
                 records.sizeInBytes(), buffer.limit());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;? extends RecordBatch&amp;gt; batches = Utils.toList(records.batches().iterator());&lt;br/&gt;
@@ -553,7 +553,7 @@ public void convertToV1WithMixedV0AndV2Data() &lt;/p&gt;
{
             assertEquals(&quot;1&quot;, utf8(logRecords.get(0).key()));
             assertEquals(&quot;2&quot;, utf8(logRecords.get(1).key()));
             assertEquals(&quot;3&quot;, utf8(logRecords.get(2).key()));
-            verifyRecordsProcessingStats(convertedRecords.recordsProcessingStats(), 3, 2,
+            verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 2,
                     records.sizeInBytes(), buffer.limit());
         }
&lt;p&gt; else {&lt;br/&gt;
             assertEquals(2, batches.size());&lt;br/&gt;
@@ -563,7 +563,7 @@ public void convertToV1WithMixedV0AndV2Data() &lt;/p&gt;
{
             assertEquals(2, batches.get(1).baseOffset());
             assertEquals(&quot;1&quot;, utf8(logRecords.get(0).key()));
             assertEquals(&quot;3&quot;, utf8(logRecords.get(1).key()));
-            verifyRecordsProcessingStats(convertedRecords.recordsProcessingStats(), 3, 1,
+            verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 1,
                     records.sizeInBytes(), buffer.limit());
         }
&lt;p&gt;     }&lt;br/&gt;
@@ -678,8 +678,8 @@ else if (iterations &amp;gt; 2 &amp;amp;&amp;amp; memUsed &amp;lt; (iterations - 2) * 1024)&lt;br/&gt;
         assertTrue(&quot;Memory usage too high: &quot; + memUsed, iterations &amp;lt; 100);&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void verifyRecordsProcessingStats(RecordsProcessingStats processingStats, int numRecords,&lt;/li&gt;
	&lt;li&gt;int numRecordsConverted, long finalBytes, long preConvertedBytes) {&lt;br/&gt;
+    private void verifyRecordsProcessingStats(RecordConversionStats processingStats, int numRecords,&lt;br/&gt;
+                                              int numRecordsConverted, long finalBytes, long preConvertedBytes) {&lt;br/&gt;
         assertNotNull(&quot;Records processing info is null&quot;, processingStats);&lt;br/&gt;
         assertEquals(numRecordsConverted, processingStats.numRecordsConverted());&lt;br/&gt;
         // Since nanoTime accuracy on build machines may not be sufficient to measure small conversion times,&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/network/MultiSendTest.java b/clients/src/test/java/org/apache/kafka/common/record/MultiRecordsSendTest.java&lt;br/&gt;
similarity index 88%&lt;br/&gt;
rename from clients/src/test/java/org/apache/kafka/common/network/MultiSendTest.java&lt;br/&gt;
rename to clients/src/test/java/org/apache/kafka/common/record/MultiRecordsSendTest.java&lt;br/&gt;
index d2b2ef6c5d2..38d381c75e5 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/common/network/MultiSendTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/record/MultiRecordsSendTest.java&lt;br/&gt;
@@ -14,8 +14,10 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-package org.apache.kafka.common.network;&lt;br/&gt;
+package org.apache.kafka.common.record;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import org.apache.kafka.common.network.ByteBufferSend;&lt;br/&gt;
+import org.apache.kafka.common.network.Send;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt;@@ -27,7 +29,7 @@&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertTrue;&lt;/p&gt;

&lt;p&gt;-public class MultiSendTest {&lt;br/&gt;
+public class MultiRecordsSendTest {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testSendsFreedAfterWriting() throws IOException {&lt;br/&gt;
@@ -45,7 +47,7 @@ public void testSendsFreedAfterWriting() throws IOException &lt;/p&gt;
{
             sends.add(new ByteBufferSend(dest, buffer));
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MultiSend send = new MultiSend(dest, sends);&lt;br/&gt;
+        MultiRecordsSend send = new MultiRecordsSend(dest, sends);&lt;br/&gt;
         assertEquals(totalSize, send.size());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         for (int i = 0; i &amp;lt; numChunks; i++) {&lt;br/&gt;
@@ -69,7 +71,7 @@ private NonOverflowingByteBufferChannel(long size) {&lt;br/&gt;
         @Override&lt;br/&gt;
         public long write(ByteBuffer[] srcs) throws IOException {&lt;br/&gt;
             // Instead of overflowing, this channel refuses additional writes once the buffer is full,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// which allows us to test the MultiSend behavior on a per-send basis.&lt;br/&gt;
+            // which allows us to test the MultiRecordsSend behavior on a per-send basis.&lt;br/&gt;
             if (!buffer().hasRemaining())&lt;br/&gt;
                 return 0;&lt;br/&gt;
             return super.write(srcs);&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index 118288b2888..777dbb56d89 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -48,11 +48,11 @@ import java.util.regex.Pattern&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; object LogAppendInfo &lt;/p&gt;
{
   val UnknownLogAppendInfo = LogAppendInfo(None, -1, RecordBatch.NO_TIMESTAMP, -1L, RecordBatch.NO_TIMESTAMP, -1L,
-    RecordsProcessingStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false)
+    RecordConversionStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false)
 
   def unknownLogAppendInfoWithLogStartOffset(logStartOffset: Long): LogAppendInfo =
     LogAppendInfo(None, -1, RecordBatch.NO_TIMESTAMP, -1L, RecordBatch.NO_TIMESTAMP, logStartOffset,
-      RecordsProcessingStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false)
+      RecordConversionStats.EMPTY, NoCompressionCodec, NoCompressionCodec, -1, -1, offsetsMonotonic = false)
 }

&lt;p&gt; /**&lt;br/&gt;
@@ -65,7 +65,7 @@ object LogAppendInfo &lt;/p&gt;
{
  * @param offsetOfMaxTimestamp The offset of the message with the maximum timestamp.
  * @param logAppendTime The log append time (if used) of the message set, otherwise Message.NoTimestamp
  * @param logStartOffset The start offset of the log at the time of this append.
- * @param recordsProcessingStats Statistics collected during record processing, `null` if `assignOffsets` is `false`
+ * @param recordConversionStats Statistics collected during record processing, `null` if `assignOffsets` is `false`
  * @param sourceCodec The source codec used in the message set (send by the producer)
  * @param targetCodec The target codec of the message set(after applying the broker compression configuration if any)
  * @param shallowCount The number of shallow messages
@@ -78,7 +78,7 @@ case class LogAppendInfo(var firstOffset: Option[Long],
                          var offsetOfMaxTimestamp: Long,
                          var logAppendTime: Long,
                          var logStartOffset: Long,
-                         var recordsProcessingStats: RecordsProcessingStats,
+                         var recordConversionStats: RecordConversionStats,
                          sourceCodec: CompressionCodec,
                          targetCodec: CompressionCodec,
                          shallowCount: Int,
@@ -693,7 +693,7 @@ class Log(@volatile var dir: File,
           appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp
           appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp
           appendInfo.lastOffset = offset.value - 1
-          appendInfo.recordsProcessingStats = validateAndOffsetAssignResult.recordsProcessingStats
+          appendInfo.recordConversionStats = validateAndOffsetAssignResult.recordConversionStats
           if (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)
             appendInfo.logAppendTime = now
 
@@ -939,7 +939,7 @@ class Log(@volatile var dir: File,
     // Apply broker-side compression if any
     val targetCodec = BrokerCompressionCodec.getTargetCompressionCodec(config.compressionType, sourceCodec)
     LogAppendInfo(firstOffset, lastOffset, maxTimestamp, offsetOfMaxTimestamp, RecordBatch.NO_TIMESTAMP, logStartOffset,
-      RecordsProcessingStats.EMPTY, sourceCodec, targetCodec, shallowMessageCount, validBytesCount, monotonic)
+      RecordConversionStats.EMPTY, sourceCodec, targetCodec, shallowMessageCount, validBytesCount, monotonic)
   }

&lt;p&gt;   private def updateProducers(batch: RecordBatch,&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogValidator.scala b/core/src/main/scala/kafka/log/LogValidator.scala&lt;br/&gt;
index 65152605ef4..2cfbf7d93cd 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/LogValidator.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogValidator.scala&lt;br/&gt;
@@ -22,7 +22,7 @@ import kafka.common.LongRef&lt;br/&gt;
 import kafka.message.&lt;/p&gt;
{CompressionCodec, NoCompressionCodec}
&lt;p&gt; import kafka.utils.Logging&lt;br/&gt;
 import org.apache.kafka.common.errors.&lt;/p&gt;
{InvalidTimestampException, UnsupportedForMessageFormatException}
&lt;p&gt;-import org.apache.kafka.common.record._&lt;br/&gt;
+import org.apache.kafka.common.record.&lt;/p&gt;
{AbstractRecords, CompressionType, InvalidRecordException, MemoryRecords, Record, RecordBatch, RecordConversionStats, TimestampType}
&lt;p&gt; import org.apache.kafka.common.utils.Time&lt;/p&gt;

&lt;p&gt; import scala.collection.mutable&lt;br/&gt;
@@ -155,14 +155,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; object LogValidator extends Logging &lt;/p&gt;
{
     val convertedRecords = builder.build()
 
     val info = builder.info
-    val recordsProcessingStats = new RecordsProcessingStats(builder.uncompressedBytesWritten,
+    val recordConversionStats = new RecordConversionStats(builder.uncompressedBytesWritten,
       builder.numRecords, time.nanoseconds - startNanos)
     ValidationAndOffsetAssignResult(
       validatedRecords = convertedRecords,
       maxTimestamp = info.maxTimestamp,
       shallowOffsetOfMaxTimestamp = info.shallowOffsetOfMaxTimestamp,
       messageSizeMaybeChanged = true,
-      recordsProcessingStats = recordsProcessingStats)
+      recordConversionStats = recordConversionStats)
   }

&lt;p&gt;   private def assignOffsetsNonCompressed(records: MemoryRecords,&lt;br/&gt;
@@ -224,7 +224,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; object LogValidator extends Logging &lt;/p&gt;
{
       maxTimestamp = maxTimestamp,
       shallowOffsetOfMaxTimestamp = offsetOfMaxTimestamp,
       messageSizeMaybeChanged = false,
-      recordsProcessingStats = RecordsProcessingStats.EMPTY)
+      recordConversionStats = RecordConversionStats.EMPTY)
   }

&lt;p&gt;   /**&lt;br/&gt;
@@ -315,12 +315,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; object LogValidator extends Logging &lt;/p&gt;
{
         if (toMagic &amp;gt;= RecordBatch.MAGIC_VALUE_V2)
           batch.setPartitionLeaderEpoch(partitionLeaderEpoch)
 
-        val recordsProcessingStats = new RecordsProcessingStats(uncompressedSizeInBytes, 0, -1)
+        val recordConversionStats = new RecordConversionStats(uncompressedSizeInBytes, 0, 0)
         ValidationAndOffsetAssignResult(validatedRecords = records,
           maxTimestamp = maxTimestamp,
           shallowOffsetOfMaxTimestamp = lastOffset,
           messageSizeMaybeChanged = false,
-          recordsProcessingStats = recordsProcessingStats)
+          recordConversionStats = recordConversionStats)
       }
&lt;p&gt;   }&lt;/p&gt;

&lt;p&gt;@@ -358,7 +358,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; object LogValidator extends Logging {&lt;br/&gt;
     // message format V0 or if the inner offsets are not consecutive. This is OK since the impact is the same: we have&lt;br/&gt;
     // to rebuild the records (including recompression if enabled).&lt;br/&gt;
     val conversionCount = builder.numRecords&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val recordsProcessingStats = new RecordsProcessingStats(uncompresssedSizeInBytes + builder.uncompressedBytesWritten,&lt;br/&gt;
+    val recordConversionStats = new RecordConversionStats(uncompresssedSizeInBytes + builder.uncompressedBytesWritten,&lt;br/&gt;
       conversionCount, time.nanoseconds - startNanos)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     ValidationAndOffsetAssignResult(&lt;br/&gt;
@@ -366,7 +366,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; object LogValidator extends Logging &lt;/p&gt;
{
       maxTimestamp = info.maxTimestamp,
       shallowOffsetOfMaxTimestamp = info.shallowOffsetOfMaxTimestamp,
       messageSizeMaybeChanged = true,
-      recordsProcessingStats = recordsProcessingStats)
+      recordConversionStats = recordConversionStats)
   }

&lt;p&gt;   private def validateKey(record: Record, compactedTopic: Boolean) {&lt;br/&gt;
@@ -397,6 +397,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; object LogValidator extends Logging &lt;/p&gt;
{
                                              maxTimestamp: Long,
                                              shallowOffsetOfMaxTimestamp: Long,
                                              messageSizeMaybeChanged: Boolean,
-                                             recordsProcessingStats: RecordsProcessingStats)
+                                             recordConversionStats: RecordConversionStats)
 
 }
&lt;p&gt;diff --git a/core/src/main/scala/kafka/network/RequestChannel.scala b/core/src/main/scala/kafka/network/RequestChannel.scala&lt;br/&gt;
index f45e0ce3adf..eecce1d1415 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/network/RequestChannel.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/network/RequestChannel.scala&lt;br/&gt;
@@ -25,9 +25,11 @@ import com.typesafe.scalalogging.Logger&lt;br/&gt;
 import com.yammer.metrics.core.&lt;/p&gt;
{Gauge, Meter}&lt;br/&gt;
 import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.utils.{Logging, NotNothing}&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.memory.MemoryPool&lt;br/&gt;
 import org.apache.kafka.common.network.Send&lt;br/&gt;
 import org.apache.kafka.common.protocol.{ApiKeys, Errors}&lt;br/&gt;
+import org.apache.kafka.common.record.RecordConversionStats&lt;br/&gt;
 import org.apache.kafka.common.requests._&lt;br/&gt;
 import org.apache.kafka.common.security.auth.KafkaPrincipal&lt;br/&gt;
 import org.apache.kafka.common.utils.{Sanitizer, Time}&lt;br/&gt;
@@ -181,12 +183,8 @@ object RequestChannel extends Logging {&lt;br/&gt;
 &lt;br/&gt;
       if (isRequestLoggingEnabled) {&lt;br/&gt;
         val detailsEnabled = requestLogger.underlying.isTraceEnabled&lt;br/&gt;
-        val responseString =&lt;br/&gt;
-          if (response.responseSend.isDefined)&lt;br/&gt;
-            response.responseAsString.getOrElse(&lt;br/&gt;
-              throw new IllegalStateException(&quot;responseAsString should always be defined if request logging is enabled&quot;))&lt;br/&gt;
-          else &quot;&quot;&lt;br/&gt;
-&lt;br/&gt;
+        val responseString = response.responseString.getOrElse(&lt;br/&gt;
+          throw new IllegalStateException(&quot;responseAsString should always be defined if request logging is enabled&quot;))&lt;br/&gt;
         val builder = new StringBuilder(256)&lt;br/&gt;
         builder.append(&quot;Completed request:&quot;).append(requestDesc(detailsEnabled))&lt;br/&gt;
           .append(&quot;,response:&quot;).append(responseString)&lt;br/&gt;
@@ -225,24 +223,55 @@ object RequestChannel extends Logging {&lt;br/&gt;
 &lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  /** responseAsString should only be defined if request logging is enabled */&lt;br/&gt;
-  class Response(val request: Request, val responseSend: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Send&amp;#93;&lt;/span&gt;, val responseAction: ResponseAction,&lt;br/&gt;
-                 val responseAsString: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
-    request.responseCompleteTimeNanos = Time.SYSTEM.nanoseconds&lt;br/&gt;
-    if (request.apiLocalCompleteTimeNanos == -1L) request.apiLocalCompleteTimeNanos = Time.SYSTEM.nanoseconds&lt;br/&gt;
+  abstract class Response(val request: Request) {&lt;br/&gt;
+    locally {
+      val nowNs = Time.SYSTEM.nanoseconds
+      request.responseCompleteTimeNanos = nowNs
+      if (request.apiLocalCompleteTimeNanos == -1L)
+        request.apiLocalCompleteTimeNanos = nowNs
+    }&lt;br/&gt;
 &lt;br/&gt;
     def processor: Int = request.processor&lt;br/&gt;
 &lt;br/&gt;
-    override def toString =&lt;br/&gt;
-      s&quot;Response(request=$request, responseSend=$responseSend, responseAction=$responseAction, responseAsString=$responseAsString)&quot;&lt;br/&gt;
+    def responseString: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = Some(&quot;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    def onComplete: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Send =&amp;gt; Unit&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
+&lt;br/&gt;
+    override def toString: String&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  sealed trait ResponseAction&lt;br/&gt;
-  case object SendAction extends ResponseAction&lt;br/&gt;
-  case object NoOpAction extends ResponseAction&lt;br/&gt;
-  case object CloseConnectionAction extends ResponseAction&lt;br/&gt;
-  case object StartThrottlingAction extends ResponseAction&lt;br/&gt;
-  case object EndThrottlingAction extends ResponseAction&lt;br/&gt;
+  /** responseAsString should only be defined if request logging is enabled */&lt;br/&gt;
+  class SendResponse(request: Request,&lt;br/&gt;
+                     val responseSend: Send,&lt;br/&gt;
+                     val responseAsString: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                     val onCompleteCallback: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Send =&amp;gt; Unit&amp;#93;&lt;/span&gt;) extends Response(request) {
+    override def responseString: Option[String] = responseAsString
+
+    override def onComplete: Option[Send =&amp;gt; Unit] = onCompleteCallback
+
+    override def toString: String =
+      s&quot;Response(type=Send, request=$request, send=$responseSend, asString=$responseAsString)&quot;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  class NoOpResponse(request: Request) extends Response(request) {
+    override def toString: String =
+      s&quot;Response(type=NoOp, request=$request)&quot;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  class CloseConnectionResponse(request: Request) extends Response(request) {
+    override def toString: String =
+      s&quot;Response(type=CloseConnection, request=$request)&quot;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  class StartThrottlingResponse(request: Request) extends Response(request) {
+    override def toString: String =
+      s&quot;Response(type=StartThrottling, request=$request)&quot;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  class EndThrottlingResponse(request: Request) extends Response(request) {
+    override def toString: String =
+      s&quot;Response(type=EndThrottling, request=$request)&quot;
+  }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 class RequestChannel(val queueSize: Int) extends KafkaMetricsGroup {&lt;br/&gt;
@@ -287,16 +316,16 @@ class RequestChannel(val queueSize: Int) extends KafkaMetricsGroup {&lt;br/&gt;
   def sendResponse(response: RequestChannel.Response) {&lt;br/&gt;
     if (isTraceEnabled) {&lt;br/&gt;
       val requestHeader = response.request.header&lt;br/&gt;
-      val message = response.responseAction match {&lt;br/&gt;
-        case SendAction =&amp;gt;&lt;br/&gt;
-          s&quot;Sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} of ${response.responseSend.get.size} bytes.&quot;&lt;br/&gt;
-        case NoOpAction =&amp;gt;&lt;br/&gt;
+      val message = response match {&lt;br/&gt;
+        case sendResponse: SendResponse =&amp;gt;&lt;br/&gt;
+          s&quot;Sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} of ${sendResponse.responseSend.size} bytes.&quot;&lt;br/&gt;
+        case _: NoOpResponse =&amp;gt;&lt;br/&gt;
           s&quot;Not sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} as it&apos;s not required.&quot;&lt;br/&gt;
-        case CloseConnectionAction =&amp;gt;&lt;br/&gt;
+        case _: CloseConnectionResponse =&amp;gt;&lt;br/&gt;
           s&quot;Closing connection for client ${requestHeader.clientId} due to error during ${requestHeader.apiKey}.&quot;&lt;br/&gt;
-        case StartThrottlingAction =&amp;gt;&lt;br/&gt;
+        case _: StartThrottlingResponse =&amp;gt;&lt;br/&gt;
           s&quot;Notifying channel throttling has started for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;&lt;br/&gt;
-        case EndThrottlingAction =&amp;gt;&lt;br/&gt;
+        case _: EndThrottlingResponse =&amp;gt;&lt;br/&gt;
           s&quot;Notifying channel throttling has ended for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;&lt;br/&gt;
       }&lt;br/&gt;
       trace(message)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/network/SocketServer.scala b/core/src/main/scala/kafka/network/SocketServer.scala&lt;br/&gt;
index 759396dba3a..db5eda60b4f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/network/SocketServer.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/network/SocketServer.scala&lt;br/&gt;
@@ -28,6 +28,7 @@ import com.yammer.metrics.core.Gauge&lt;br/&gt;
 import kafka.cluster.{BrokerEndPoint, EndPoint}&lt;br/&gt;
 import kafka.common.KafkaException&lt;br/&gt;
 import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
+import kafka.network.RequestChannel.{CloseConnectionResponse, EndThrottlingResponse, NoOpResponse, SendResponse, StartThrottlingResponse}&lt;br/&gt;
 import kafka.security.CredentialProvider&lt;br/&gt;
 import kafka.server.KafkaConfig&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
@@ -37,6 +38,7 @@ import org.apache.kafka.common.metrics._&lt;br/&gt;
 import org.apache.kafka.common.metrics.stats.Meter&lt;br/&gt;
 import org.apache.kafka.common.network.KafkaChannel.ChannelMuteEvent&lt;br/&gt;
 import org.apache.kafka.common.network.{ChannelBuilder, ChannelBuilders, KafkaChannel, ListenerName, Selectable, Send, Selector =&amp;gt; KSelector}&lt;br/&gt;
+import org.apache.kafka.common.record.MultiRecordsSend&lt;br/&gt;
 import org.apache.kafka.common.requests.{RequestContext, RequestHeader}&lt;br/&gt;
 import org.apache.kafka.common.security.auth.SecurityProtocol&lt;br/&gt;
 import org.apache.kafka.common.utils.{KafkaThread, LogContext, Time}&lt;br/&gt;
@@ -617,36 +619,37 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; class Processor(val id: Int,&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def processNewResponses() {&lt;br/&gt;
-    var curr: RequestChannel.Response = null&lt;br/&gt;
-    while ({curr = dequeueResponse(); curr != null}) {&lt;br/&gt;
-      val channelId = curr.request.context.connectionId&lt;br/&gt;
+    var currentResponse: RequestChannel.Response = null&lt;br/&gt;
+    while ({currentResponse = dequeueResponse(); currentResponse != null}) {&lt;br/&gt;
+      val channelId = currentResponse.request.context.connectionId&lt;br/&gt;
       try {&lt;br/&gt;
-        curr.responseAction match {&lt;br/&gt;
-          case RequestChannel.NoOpAction =&amp;gt;&lt;br/&gt;
+        currentResponse match {&lt;br/&gt;
+          case response: NoOpResponse =&amp;gt;&lt;br/&gt;
             // There is no response to send to the client, we need to read more pipelined requests&lt;br/&gt;
             // that are sitting in the server&apos;s socket buffer&lt;br/&gt;
-            updateRequestMetrics(curr)&lt;br/&gt;
-            trace(&quot;Socket server received empty response to send, registering for read: &quot; + curr)&lt;br/&gt;
+            updateRequestMetrics(response)&lt;br/&gt;
+            trace(&quot;Socket server received empty response to send, registering for read: &quot; + response)&lt;br/&gt;
             // Try unmuting the channel. If there was no quota violation and the channel has not been throttled,&lt;br/&gt;
             // it will be unmuted immediately. If the channel has been throttled, it will be unmuted only if the&lt;br/&gt;
             // throttling delay has already passed by now.&lt;br/&gt;
             handleChannelMuteEvent(channelId, ChannelMuteEvent.RESPONSE_SENT)&lt;br/&gt;
             tryUnmuteChannel(channelId)&lt;br/&gt;
-          case RequestChannel.SendAction =&amp;gt;&lt;br/&gt;
-            val responseSend = curr.responseSend.getOrElse(&lt;br/&gt;
-              throw new IllegalStateException(s&quot;responseSend must be defined for SendAction, response: $curr&quot;))&lt;br/&gt;
-            sendResponse(curr, responseSend)&lt;br/&gt;
-          case RequestChannel.CloseConnectionAction =&amp;gt;&lt;br/&gt;
-            updateRequestMetrics(curr)&lt;br/&gt;
+&lt;br/&gt;
+          case response: SendResponse =&amp;gt;&lt;br/&gt;
+            sendResponse(response, response.responseSend)&lt;br/&gt;
+          case response: CloseConnectionResponse =&amp;gt;&lt;br/&gt;
+            updateRequestMetrics(response)&lt;br/&gt;
             trace(&quot;Closing socket connection actively according to the response code.&quot;)&lt;br/&gt;
             close(channelId)&lt;br/&gt;
-          case RequestChannel.StartThrottlingAction =&amp;gt;&lt;br/&gt;
+          case response: StartThrottlingResponse =&amp;gt;&lt;br/&gt;
             handleChannelMuteEvent(channelId, ChannelMuteEvent.THROTTLE_STARTED)&lt;br/&gt;
-          case RequestChannel.EndThrottlingAction =&amp;gt;&lt;br/&gt;
+          case response: EndThrottlingResponse =&amp;gt;&lt;br/&gt;
             // Try unmuting the channel. The channel will be unmuted only if the response has already been sent out to&lt;br/&gt;
             // the client.&lt;br/&gt;
             handleChannelMuteEvent(channelId, ChannelMuteEvent.THROTTLE_ENDED)&lt;br/&gt;
             tryUnmuteChannel(channelId)&lt;br/&gt;
+          case _ =&amp;gt;&lt;br/&gt;
+            throw new IllegalArgumentException(s&quot;Unknown response type: ${currentResponse.getClass}&quot;)&lt;br/&gt;
         }&lt;br/&gt;
       } catch {&lt;br/&gt;
         case e: Throwable =&amp;gt;&lt;br/&gt;
@@ -713,10 +716,13 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; class Processor(val id: Int,&lt;br/&gt;
   private def processCompletedSends() {&lt;br/&gt;
     selector.completedSends.asScala.foreach { send =&amp;gt;&lt;br/&gt;
       try {&lt;br/&gt;
-        val resp = inflightResponses.remove(send.destination).getOrElse {&lt;br/&gt;
+        val response = inflightResponses.remove(send.destination).getOrElse {&lt;br/&gt;
           throw new IllegalStateException(s&quot;Send for ${send.destination} completed, but not in `inflightResponses`&quot;)&lt;br/&gt;
         }&lt;br/&gt;
-        updateRequestMetrics(resp)&lt;br/&gt;
+        updateRequestMetrics(response)&lt;br/&gt;
+&lt;br/&gt;
+        // Invoke send completion callback&lt;br/&gt;
+        response.onComplete.foreach(onComplete =&amp;gt; onComplete(send))&lt;br/&gt;
 &lt;br/&gt;
         // Try unmuting the channel. If there was no quota violation and the channel has not been throttled,&lt;br/&gt;
         // it will be unmuted immediately. If the channel has been throttled, it will unmuted only if the throttling&lt;br/&gt;
@@ -730,7 +736,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; class Processor(val id: Int,&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def updateRequestMetrics(response: RequestChannel.Response) {&lt;br/&gt;
+  private def updateRequestMetrics(response: RequestChannel.Response): Unit = {&lt;br/&gt;
     val request = response.request&lt;br/&gt;
     val networkThreadTimeNanos = openOrClosingChannel(request.context.connectionId).fold(0L)(_.getAndResetNetworkThreadTimeNanos())&lt;br/&gt;
     request.updateRequestMetrics(networkThreadTimeNanos, response)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ClientQuotaManager.scala b/core/src/main/scala/kafka/server/ClientQuotaManager.scala&lt;br/&gt;
index 73b40d12789..41ee420b667 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ClientQuotaManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ClientQuotaManager.scala&lt;br/&gt;
@@ -269,18 +269,11 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,&lt;br/&gt;
     * @param channelThrottlingCallback Callback for channel throttling&lt;br/&gt;
     * @return ThrottledChannel object&lt;br/&gt;
     */&lt;br/&gt;
-  def throttle(request: RequestChannel.Request, throttleTimeMs: Int,&lt;br/&gt;
-               channelThrottlingCallback: (ResponseAction) =&amp;gt; Unit) {
-    throttle(request.session, request.header.clientId, throttleTimeMs, channelThrottlingCallback)
-  }&lt;br/&gt;
-&lt;br/&gt;
-  def throttle(session: Session, clientId: String, throttleTimeMs: Int,&lt;br/&gt;
-               channelThrottlingCallback: (ResponseAction) =&amp;gt; Unit) {&lt;br/&gt;
+  def throttle(request: RequestChannel.Request, throttleTimeMs: Int, channelThrottlingCallback: Response =&amp;gt; Unit): Unit = {&lt;br/&gt;
     if (throttleTimeMs &amp;gt; 0) {&lt;br/&gt;
-      val clientSensors = getOrCreateQuotaSensors(session, clientId)&lt;br/&gt;
-&lt;br/&gt;
+      val clientSensors = getOrCreateQuotaSensors(request.session, request.header.clientId)&lt;br/&gt;
       clientSensors.throttleTimeSensor.record(throttleTimeMs)&lt;br/&gt;
-      val throttledChannel = new ThrottledChannel(time, throttleTimeMs, channelThrottlingCallback)&lt;br/&gt;
+      val throttledChannel = new ThrottledChannel(request, time, throttleTimeMs, channelThrottlingCallback)&lt;br/&gt;
       delayQueue.add(throttledChannel)&lt;br/&gt;
       delayQueueSensor.record()&lt;br/&gt;
       debug(&quot;Channel throttled for sensor (%s). Delay time: (%d)&quot;.format(clientSensors.quotaSensor.name(), throttleTimeMs))&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/FetchSession.scala b/core/src/main/scala/kafka/server/FetchSession.scala&lt;br/&gt;
index 3810d90d16e..7a47780a135 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/FetchSession.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/FetchSession.scala&lt;br/&gt;
@@ -25,9 +25,9 @@ import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.utils.Logging&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
+import org.apache.kafka.common.record.Records&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchMetadata.{FINAL_EPOCH, INITIAL_EPOCH, INVALID_SESSION_ID}&lt;br/&gt;
-import org.apache.kafka.common.requests.{FetchRequest, FetchResponse}&lt;br/&gt;
-import org.apache.kafka.common.requests.{FetchMetadata =&amp;gt; JFetchMetadata}&lt;br/&gt;
+import org.apache.kafka.common.requests.{FetchRequest, FetchResponse, FetchMetadata =&amp;gt; JFetchMetadata}&lt;br/&gt;
 import org.apache.kafka.common.utils.{ImplicitLinkedHashSet, Time, Utils}&lt;br/&gt;
 &lt;br/&gt;
 import scala.math.Ordered.orderingToOrdered&lt;br/&gt;
@@ -36,9 +36,9 @@ import scala.collection.JavaConverters._&lt;br/&gt;
 &lt;br/&gt;
 object FetchSession {&lt;br/&gt;
   type REQ_MAP = util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
-  type RESP_MAP = util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  type RESP_MAP = util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
   type CACHE_MAP = ImplicitLinkedHashSet&lt;span class=&quot;error&quot;&gt;&amp;#91;CachedPartition&amp;#93;&lt;/span&gt;&lt;br/&gt;
-  type RESP_MAP_ITER = util.Iterator[util.Map.Entry&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+  type RESP_MAP_ITER = util.Iterator[util.Map.Entry[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]]&lt;br/&gt;
 &lt;br/&gt;
   val NUM_INCREMENTAL_FETCH_SESSISONS = &quot;NumIncrementalFetchSessions&quot;&lt;br/&gt;
   val NUM_INCREMENTAL_FETCH_PARTITIONS_CACHED = &quot;NumIncrementalFetchPartitionsCached&quot;&lt;br/&gt;
@@ -100,7 +100,7 @@ class CachedPartition(val topic: String,&lt;br/&gt;
       reqData.logStartOffset, -1)&lt;br/&gt;
 &lt;br/&gt;
   def this(part: TopicPartition, reqData: FetchRequest.PartitionData,&lt;br/&gt;
-           respData: FetchResponse.PartitionData) =&lt;br/&gt;
+           respData: FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;) =&lt;br/&gt;
     this(part.topic(), part.partition(),&lt;br/&gt;
       reqData.maxBytes, reqData.fetchOffset, respData.highWatermark,&lt;br/&gt;
       reqData.logStartOffset, respData.logStartOffset)&lt;br/&gt;
@@ -126,7 +126,7 @@ class CachedPartition(val topic: String,&lt;br/&gt;
     * @param updateResponseData if set to true, update this CachedPartition with new request and response data.&lt;br/&gt;
     * @return True if this partition should be included in the response; false if it can be omitted.&lt;br/&gt;
     */&lt;br/&gt;
-  def maybeUpdateResponseData(respData: FetchResponse.PartitionData, updateResponseData: Boolean): Boolean = {&lt;br/&gt;
+  def maybeUpdateResponseData(respData: FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;, updateResponseData: Boolean): Boolean = {&lt;br/&gt;
     // Check the response data.&lt;br/&gt;
     var mustRespond = false&lt;br/&gt;
     if ((respData.records != null) &amp;amp;&amp;amp; (respData.records.sizeInBytes() &amp;gt; 0)) {&lt;br/&gt;
@@ -286,7 +286,7 @@ trait FetchContext extends Logging {
     * Updates the fetch context with new partition information.  Generates response data.
     * The response data may require subsequent down-conversion.
     */
-  def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse
+  def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse[Records]
 
   def partitionsToLogString(partitions: util.Collection[TopicPartition]): String =
     FetchSession.partitionsToLogString(partitions, isTraceEnabled)
@@ -306,7 +306,7 @@ class SessionErrorContext(val error: Errors,
   }&lt;br/&gt;
 &lt;br/&gt;
   // Because of the fetch session error, we don&apos;t know what partitions were supposed to be in this request.&lt;br/&gt;
-  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse = {&lt;br/&gt;
+  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt; = {
     debug(s&quot;Session error fetch context returning $error&quot;)
     new FetchResponse(error, new FetchSession.RESP_MAP, 0, INVALID_SESSION_ID)
   }&lt;br/&gt;
@@ -329,7 +329,7 @@ class SessionlessFetchContext(val fetchData: util.Map[TopicPartition, FetchReque&lt;br/&gt;
     FetchResponse.sizeOf(versionId, updates.entrySet().iterator())&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse = {&lt;br/&gt;
+  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     debug(s&quot;Sessionless fetch context returning ${partitionsToLogString(updates.keySet())}&quot;)&lt;br/&gt;
     new FetchResponse(Errors.NONE, updates, 0, INVALID_SESSION_ID)&lt;br/&gt;
   }&lt;br/&gt;
@@ -360,7 +360,7 @@ class FullFetchContext(private val time: Time,&lt;br/&gt;
     FetchResponse.sizeOf(versionId, updates.entrySet().iterator())&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse = {&lt;br/&gt;
+  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     def createNewSession(): FetchSession.CACHE_MAP = {&lt;br/&gt;
       val cachedPartitions = new FetchSession.CACHE_MAP(updates.size())&lt;br/&gt;
       updates.entrySet().asScala.foreach(entry =&amp;gt; {&lt;br/&gt;
@@ -407,7 +407,7 @@ class IncrementalFetchContext(private val time: Time,&lt;br/&gt;
   private class PartitionIterator(val iter: FetchSession.RESP_MAP_ITER,&lt;br/&gt;
                                   val updateFetchContextAndRemoveUnselected: Boolean)&lt;br/&gt;
     extends FetchSession.RESP_MAP_ITER {&lt;br/&gt;
-    var nextElement: util.Map.Entry&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
+    var nextElement: util.Map.Entry[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;] = null&lt;br/&gt;
 &lt;br/&gt;
     override def hasNext: Boolean = {&lt;br/&gt;
       while ((nextElement == null) &amp;amp;&amp;amp; iter.hasNext()) {
@@ -431,7 +431,7 @@ class IncrementalFetchContext(private val time: Time,
       nextElement != null
     }&lt;br/&gt;
 &lt;br/&gt;
-    override def next(): util.Map.Entry&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    override def next(): util.Map.Entry[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;] = {
       if (!hasNext()) throw new NoSuchElementException()
       val element = nextElement
       nextElement = null
@@ -453,7 +453,7 @@ class IncrementalFetchContext(private val time: Time,
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse = {&lt;br/&gt;
+  override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     session.synchronized {&lt;br/&gt;
       // Check to make sure that the session epoch didn&apos;t change in between&lt;br/&gt;
       // creating this fetch context and generating this response.&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
index abca2f0cc8b..dcdfae06728 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
@@ -17,51 +17,49 @@&lt;br/&gt;
 &lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
-import java.nio.ByteBuffer&lt;br/&gt;
 import java.lang.{Long =&amp;gt; JLong}&lt;br/&gt;
-import java.util.{Collections, Properties}&lt;br/&gt;
+import java.nio.ByteBuffer&lt;br/&gt;
 import java.util&lt;br/&gt;
 import java.util.concurrent.ConcurrentHashMap&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicInteger&lt;br/&gt;
+import java.util.{Collections, Properties}&lt;br/&gt;
 &lt;br/&gt;
 import kafka.admin.{AdminUtils, RackAwareMode}&lt;br/&gt;
 import kafka.api.{ApiVersion, KAFKA_0_11_0_IV0}&lt;br/&gt;
 import kafka.cluster.Partition&lt;br/&gt;
 import kafka.common.{OffsetAndMetadata, OffsetMetadata}&lt;br/&gt;
-import kafka.server.QuotaFactory.{QuotaManagers, UnboundedQuota}&lt;br/&gt;
 import kafka.controller.KafkaController&lt;br/&gt;
 import kafka.coordinator.group.{GroupCoordinator, JoinGroupResult}&lt;br/&gt;
 import kafka.coordinator.transaction.{InitProducerIdResult, TransactionCoordinator}&lt;br/&gt;
 import kafka.log.{Log, LogManager, TimestampOffset}&lt;br/&gt;
 import kafka.network.RequestChannel&lt;br/&gt;
-import kafka.network.RequestChannel._&lt;br/&gt;
 import kafka.security.SecurityUtils&lt;br/&gt;
 import kafka.security.auth.{Resource, _}&lt;br/&gt;
+import kafka.server.QuotaFactory.{QuotaManagers, UnboundedQuota}&lt;br/&gt;
 import kafka.utils.{CoreUtils, Logging}&lt;br/&gt;
 import kafka.zk.{AdminZkClient, KafkaZkClient}&lt;br/&gt;
+import org.apache.kafka.common.acl.{AccessControlEntry, AclBinding}&lt;br/&gt;
 import org.apache.kafka.common.errors._&lt;br/&gt;
 import org.apache.kafka.common.internals.FatalExitError&lt;br/&gt;
 import org.apache.kafka.common.internals.Topic.{GROUP_METADATA_TOPIC_NAME, TRANSACTION_STATE_TOPIC_NAME, isInternal}&lt;br/&gt;
+import org.apache.kafka.common.requests.FetchMetadata.INVALID_SESSION_ID&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
-import org.apache.kafka.common.network.ListenerName&lt;br/&gt;
+import org.apache.kafka.common.network.{ListenerName, Send}&lt;br/&gt;
 import org.apache.kafka.common.protocol.{ApiKeys, Errors}&lt;br/&gt;
-import org.apache.kafka.common.record._&lt;br/&gt;
+import org.apache.kafka.common.record.{BaseRecords, ControlRecordType, EndTransactionMarker, LazyDownConversionRecords, MemoryRecords, MultiRecordsSend, RecordBatch, RecordConversionStats, Records}&lt;br/&gt;
 import org.apache.kafka.common.requests.CreateAclsResponse.AclCreationResponse&lt;br/&gt;
 import org.apache.kafka.common.requests.DeleteAclsResponse.{AclDeletionResult, AclFilterResponse}&lt;br/&gt;
-import org.apache.kafka.common.requests.{Resource =&amp;gt; RResource, ResourceType =&amp;gt; RResourceType, _}&lt;br/&gt;
+import org.apache.kafka.common.requests.DescribeLogDirsResponse.LogDirInfo&lt;br/&gt;
 import org.apache.kafka.common.requests.ProduceResponse.PartitionResponse&lt;br/&gt;
-import org.apache.kafka.common.utils.{Time, Utils}&lt;br/&gt;
-import org.apache.kafka.common.{Node, TopicPartition}&lt;br/&gt;
-import org.apache.kafka.common.requests.{SaslAuthenticateResponse, SaslHandshakeResponse}&lt;br/&gt;
-import org.apache.kafka.common.requests.FetchMetadata.INVALID_SESSION_ID&lt;br/&gt;
+import org.apache.kafka.common.requests.{Resource =&amp;gt; RResource, ResourceType =&amp;gt; RResourceType, _}&lt;br/&gt;
 import org.apache.kafka.common.resource.{Resource =&amp;gt; AdminResource}&lt;br/&gt;
-import org.apache.kafka.common.acl.{AccessControlEntry, AclBinding}&lt;br/&gt;
-import DescribeLogDirsResponse.LogDirInfo&lt;br/&gt;
 import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}&lt;br/&gt;
 import org.apache.kafka.common.security.token.delegation.{DelegationToken, TokenInformation}&lt;br/&gt;
+import org.apache.kafka.common.utils.{Time, Utils}&lt;br/&gt;
+import org.apache.kafka.common.{Node, TopicPartition}&lt;br/&gt;
 &lt;br/&gt;
-import scala.collection._&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
+import scala.collection._&lt;br/&gt;
 import scala.collection.mutable.ArrayBuffer&lt;br/&gt;
 import scala.util.{Failure, Success, Try}&lt;br/&gt;
 &lt;br/&gt;
@@ -87,6 +85,7 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
                 time: Time,&lt;br/&gt;
                 val tokenManager: DelegationTokenManager) extends Logging {&lt;br/&gt;
 &lt;br/&gt;
+  type FetchResponseStats = Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, RecordConversionStats&amp;#93;&lt;/span&gt;&lt;br/&gt;
   this.logIdent = &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaApi-%d&amp;#93;&lt;/span&gt; &quot;.format(brokerId)&lt;br/&gt;
   val adminZkClient = new AdminZkClient(zkClient)&lt;br/&gt;
 &lt;br/&gt;
@@ -410,7 +409,6 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
 &lt;br/&gt;
     // the callback for sending a produce response&lt;br/&gt;
     def sendResponseCallback(responseStatus: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionResponse&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
-&lt;br/&gt;
       val mergedResponseStatus = responseStatus ++ unauthorizedTopicResponses ++ nonExistingTopicResponses&lt;br/&gt;
       var errorInResponse = false&lt;br/&gt;
 &lt;br/&gt;
@@ -436,9 +434,9 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
       val maxThrottleTimeMs = Math.max(bandwidthThrottleTimeMs, requestThrottleTimeMs)&lt;br/&gt;
       if (maxThrottleTimeMs &amp;gt; 0) {&lt;br/&gt;
         if (bandwidthThrottleTimeMs &amp;gt; requestThrottleTimeMs) {
-          quotas.produce.throttle(request, bandwidthThrottleTimeMs, sendActionOnlyResponse(request))
+          quotas.produce.throttle(request, bandwidthThrottleTimeMs, sendResponse)
         } else {
-          quotas.request.throttle(request, requestThrottleTimeMs, sendActionOnlyResponse(request))
+          quotas.request.throttle(request, requestThrottleTimeMs, sendResponse)
         }&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
@@ -463,13 +461,13 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
           sendNoOpResponseExemptThrottle(request)&lt;br/&gt;
         }&lt;br/&gt;
       } else {
-        sendResponse(request, Some(new ProduceResponse(mergedResponseStatus.asJava, maxThrottleTimeMs)))
+        sendResponse(request, Some(new ProduceResponse(mergedResponseStatus.asJava, maxThrottleTimeMs)), None)
       }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    def processingStatsCallback(processingStats: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, RecordsProcessingStats&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
+    def processingStatsCallback(processingStats: FetchResponseStats): Unit = {&lt;br/&gt;
       processingStats.foreach { case (tp, info) =&amp;gt;
-        updateRecordsProcessingStats(request, tp, info)
+        updateRecordConversionStats(request, tp, info)
       }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -486,7 +484,7 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
         isFromClient = true,&lt;br/&gt;
         entriesPerPartition = authorizedRequestInfo,&lt;br/&gt;
         responseCallback = sendResponseCallback,&lt;br/&gt;
-        processingStatsCallback = processingStatsCallback)&lt;br/&gt;
+        recordConversionStatsCallback = processingStatsCallback)&lt;br/&gt;
 &lt;br/&gt;
       // if the request is put into the purgatory, it will have a held reference and hence cannot be garbage collected;&lt;br/&gt;
       // hence we clear its data here in order to let GC reclaim its memory since it is already appended to log&lt;br/&gt;
@@ -506,7 +504,7 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
           fetchRequest.toForget(),&lt;br/&gt;
           fetchRequest.isFromFollower())&lt;br/&gt;
 &lt;br/&gt;
-    val erroneous = mutable.ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchResponse.PartitionData)&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    val erroneous = mutable.ArrayBuffer[(TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;)]()&lt;br/&gt;
     val interesting = mutable.ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchRequest.PartitionData)&amp;#93;&lt;/span&gt;()&lt;br/&gt;
     if (fetchRequest.isFromFollower()) {
       // The follower must have ClusterAction on ClusterResource in order to fetch partition data.
@@ -543,7 +541,7 @@ class KafkaApis(val requestChannel: RequestChannel,
       })&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    def convertedPartitionData(tp: TopicPartition, data: FetchResponse.PartitionData) = {&lt;br/&gt;
+    def convertRecords(tp: TopicPartition, unconvertedRecords: Records): BaseRecords = {&lt;br/&gt;
       // Down-conversion of the fetched records is needed when the stored magic version is&lt;br/&gt;
       // greater than that supported by the client (as indicated by the fetch request version). If the&lt;br/&gt;
       // configured magic version for the topic is less than or equal to that supported by the version of the&lt;br/&gt;
@@ -553,9 +551,9 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
       // which were written in the new format prior to the version downgrade.&lt;br/&gt;
       replicaManager.getMagic(tp).flatMap { magic =&amp;gt;&lt;br/&gt;
         val downConvertMagic = {&lt;br/&gt;
-          if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V0 &amp;amp;&amp;amp; versionId &amp;lt;= 1 &amp;amp;&amp;amp; !data.records.hasCompatibleMagic(RecordBatch.MAGIC_VALUE_V0))&lt;br/&gt;
+          if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V0 &amp;amp;&amp;amp; versionId &amp;lt;= 1 &amp;amp;&amp;amp; !unconvertedRecords.hasCompatibleMagic(RecordBatch.MAGIC_VALUE_V0))&lt;br/&gt;
             Some(RecordBatch.MAGIC_VALUE_V0)&lt;br/&gt;
-          else if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V1 &amp;amp;&amp;amp; versionId &amp;lt;= 3 &amp;amp;&amp;amp; !data.records.hasCompatibleMagic(RecordBatch.MAGIC_VALUE_V1))&lt;br/&gt;
+          else if (magic &amp;gt; RecordBatch.MAGIC_VALUE_V1 &amp;amp;&amp;amp; versionId &amp;lt;= 3 &amp;amp;&amp;amp; !unconvertedRecords.hasCompatibleMagic(RecordBatch.MAGIC_VALUE_V1))&lt;br/&gt;
             Some(RecordBatch.MAGIC_VALUE_V1)&lt;br/&gt;
           else&lt;br/&gt;
             None&lt;br/&gt;
@@ -563,18 +561,19 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
 &lt;br/&gt;
         downConvertMagic.map { magic =&amp;gt;
           trace(s&quot;Down converting records from partition $tp to message format version $magic for fetch request from $clientId&quot;)
-          val converted = data.records.downConvert(magic, fetchContext.getFetchOffset(tp).get, time)
-          updateRecordsProcessingStats(request, tp, converted.recordsProcessingStats)
-          new FetchResponse.PartitionData(data.error, data.highWatermark, FetchResponse.INVALID_LAST_STABLE_OFFSET,
-            data.logStartOffset, data.abortedTransactions, converted.records)
-        }&lt;br/&gt;
 &lt;br/&gt;
-      }.getOrElse(data)&lt;br/&gt;
+          // Because down-conversion is extremely memory intensive, we want to try and delay the down-conversion as much&lt;br/&gt;
+          // as possible. With KIP-283, we have the ability to lazily down-convert in a chunked manner. The lazy, chunked&lt;br/&gt;
+          // down-conversion always guarantees that at least one batch of messages is down-converted and sent out to the&lt;br/&gt;
+          // client.&lt;br/&gt;
+          new LazyDownConversionRecords(tp, unconvertedRecords, magic, fetchContext.getFetchOffset(tp).get, time)&lt;br/&gt;
+        }&lt;br/&gt;
+      }.getOrElse(unconvertedRecords)&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     // the callback for process a fetch response, invoked before throttling&lt;br/&gt;
-    def processResponseCallback(responsePartitionData: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchPartitionData)&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
-      val partitions = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    def processResponseCallback(responsePartitionData: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchPartitionData)&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
+      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
       responsePartitionData.foreach{ case (tp, data) =&amp;gt;&lt;br/&gt;
         val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull&lt;br/&gt;
         val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)&lt;br/&gt;
@@ -587,16 +586,23 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
       // Record time before any byte-rate throttling.&lt;br/&gt;
       request.apiRemoteCompleteTimeNanos = time.nanoseconds&lt;br/&gt;
 &lt;br/&gt;
-      var unconvertedFetchResponse: FetchResponse = null&lt;br/&gt;
+      var unconvertedFetchResponse: FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
 &lt;br/&gt;
-      def createResponse(throttleTimeMs: Int): FetchResponse = {&lt;br/&gt;
-        val convertedData = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
-        unconvertedFetchResponse.responseData().asScala.foreach { case (tp, partitionData) =&amp;gt;&lt;br/&gt;
-          if (partitionData.error != Errors.NONE)&lt;br/&gt;
+      def createResponse(throttleTimeMs: Int): FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;BaseRecords&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+        // Down-convert messages for each partition if required&lt;br/&gt;
+        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;BaseRecords&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+        unconvertedFetchResponse.responseData().asScala.foreach { case (tp, unconvertedPartitionData) =&amp;gt;&lt;br/&gt;
+          if (unconvertedPartitionData.error != Errors.NONE)&lt;br/&gt;
             debug(s&quot;Fetch request with correlation id ${request.header.correlationId} from client $clientId &quot; +&lt;br/&gt;
-              s&quot;on partition $tp failed due to ${partitionData.error.exceptionName}&quot;)&lt;br/&gt;
-          convertedData.put(tp, convertedPartitionData(tp, partitionData))&lt;br/&gt;
+              s&quot;on partition $tp failed due to ${unconvertedPartitionData.error.exceptionName}&quot;)&lt;br/&gt;
+          val convertedRecords = convertRecords(tp, unconvertedPartitionData.records)&lt;br/&gt;
+          val convertedPartitionData = new FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;BaseRecords&amp;#93;&lt;/span&gt;(unconvertedPartitionData.error,&lt;br/&gt;
+            unconvertedPartitionData.highWatermark, FetchResponse.INVALID_LAST_STABLE_OFFSET, unconvertedPartitionData.logStartOffset,&lt;br/&gt;
+            unconvertedPartitionData.abortedTransactions, convertedRecords)&lt;br/&gt;
+          convertedData.put(tp, convertedPartitionData)&lt;br/&gt;
         }&lt;br/&gt;
+&lt;br/&gt;
+        // Prepare fetch resopnse from converted data&lt;br/&gt;
         val response = new FetchResponse(unconvertedFetchResponse.error(), convertedData, throttleTimeMs,&lt;br/&gt;
           unconvertedFetchResponse.sessionId())&lt;br/&gt;
         response.responseData.asScala.foreach { case (topicPartition, data) =&amp;gt;
@@ -606,6 +612,16 @@ class KafkaApis(val requestChannel: RequestChannel,
         response
       }&lt;br/&gt;
 &lt;br/&gt;
+      def updateConversionStats(send: Send): Unit = {&lt;br/&gt;
+        send match {&lt;br/&gt;
+          case send: MultiRecordsSend if send.recordConversionStats != null =&amp;gt;&lt;br/&gt;
+            send.recordConversionStats.asScala.toMap.foreach {
+              case (tp, stats) =&amp;gt; updateRecordConversionStats(request, tp, stats)
+            }&lt;br/&gt;
+          case _ =&amp;gt;&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+&lt;br/&gt;
       if (fetchRequest.isFromFollower) {&lt;br/&gt;
         // We&apos;ve already evaluated against the quota and are good to go. Just need to record it now.&lt;br/&gt;
         unconvertedFetchResponse = fetchContext.updateAndGenerateResponseData(partitions)&lt;br/&gt;
@@ -613,7 +629,7 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
         quotas.leader.record(responseSize)&lt;br/&gt;
         trace(s&quot;Sending Fetch response with partitions.size=${unconvertedFetchResponse.responseData().size()}, &quot; +&lt;br/&gt;
           s&quot;metadata=${unconvertedFetchResponse.sessionId()}&quot;)&lt;br/&gt;
-        sendResponseExemptThrottle(request, createResponse(0))&lt;br/&gt;
+        sendResponseExemptThrottle(request, createResponse(0), Some(updateConversionStats))&lt;br/&gt;
       } else {&lt;br/&gt;
         // Fetch size used to determine throttle time is calculated before any down conversions.&lt;br/&gt;
         // This may be slightly different from the actual response size. But since down conversions&lt;br/&gt;
@@ -633,22 +649,21 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
           // from the fetch quota because we are going to return an empty response.&lt;br/&gt;
           quotas.fetch.unrecordQuotaSensor(request, responseSize, timeMs)&lt;br/&gt;
           if (bandwidthThrottleTimeMs &amp;gt; requestThrottleTimeMs) {
-            quotas.fetch.throttle(request, bandwidthThrottleTimeMs, sendActionOnlyResponse(request))
+            quotas.fetch.throttle(request, bandwidthThrottleTimeMs, sendResponse)
           } else {
-            quotas.request.throttle(request, requestThrottleTimeMs, sendActionOnlyResponse(request))
+            quotas.request.throttle(request, requestThrottleTimeMs, sendResponse)
           }&lt;br/&gt;
           // If throttling is required, return an empty response.&lt;br/&gt;
-          unconvertedFetchResponse = new FetchResponse(Errors.NONE, new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;(),&lt;br/&gt;
-            maxThrottleTimeMs, INVALID_SESSION_ID)&lt;br/&gt;
+          unconvertedFetchResponse = new FetchResponse(Errors.NONE, new util.LinkedHashMap[TopicPartition,&lt;br/&gt;
+            FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;](), maxThrottleTimeMs, INVALID_SESSION_ID)&lt;br/&gt;
         } else {&lt;br/&gt;
           // Get the actual response. This will update the fetch context.&lt;br/&gt;
           unconvertedFetchResponse = fetchContext.updateAndGenerateResponseData(partitions)&lt;br/&gt;
-          trace(s&quot;Sending Fetch response with partitions.size=${responseSize}, &quot; +&lt;br/&gt;
-            s&quot;metadata=${unconvertedFetchResponse.sessionId()}&quot;)&lt;br/&gt;
+          trace(s&quot;Sending Fetch response with partitions.size=${responseSize}, metadata=${unconvertedFetchResponse.sessionId()}&quot;)&lt;br/&gt;
         }&lt;br/&gt;
 &lt;br/&gt;
         // Send the response immediately.&lt;br/&gt;
-        sendResponse(request, Some(createResponse(maxThrottleTimeMs)))&lt;br/&gt;
+        sendResponse(request, Some(createResponse(maxThrottleTimeMs)), Some(updateConversionStats))&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -669,12 +684,12 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  class SelectingIterator(val partitions: util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+  class SelectingIterator(val partitions: util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;],&lt;br/&gt;
                           val quota: ReplicationQuotaManager)&lt;br/&gt;
-                          extends util.Iterator[util.Map.Entry&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;] {&lt;br/&gt;
+                          extends util.Iterator[util.Map.Entry[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]] {&lt;br/&gt;
     val iter = partitions.entrySet().iterator()&lt;br/&gt;
 &lt;br/&gt;
-    var nextElement: util.Map.Entry&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
+    var nextElement: util.Map.Entry[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;] = null&lt;br/&gt;
 &lt;br/&gt;
     override def hasNext: Boolean = {&lt;br/&gt;
       while ((nextElement == null) &amp;amp;&amp;amp; iter.hasNext()) {
@@ -686,7 +701,7 @@ class KafkaApis(val requestChannel: RequestChannel,
       nextElement != null
     }&lt;br/&gt;
 &lt;br/&gt;
-    override def next(): util.Map.Entry&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    override def next(): util.Map.Entry[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
       if (!hasNext()) throw new NoSuchElementException()&lt;br/&gt;
       val element = nextElement&lt;br/&gt;
       nextElement = null&lt;br/&gt;
@@ -699,7 +714,7 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
   // Traffic from both in-sync and out of sync replicas are accounted for in replication quota to ensure total replication&lt;br/&gt;
   // traffic doesn&apos;t exceed quota.&lt;br/&gt;
   private def sizeOfThrottledPartitions(versionId: Short,&lt;br/&gt;
-                                        unconvertedResponse: FetchResponse,&lt;br/&gt;
+                                        unconvertedResponse: FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;,&lt;br/&gt;
                                         quota: ReplicationQuotaManager): Int = {
     val iter = new SelectingIterator(unconvertedResponse.responseData(), quota)
     FetchResponse.sizeOf(versionId, iter)
@@ -2227,9 +2242,10 @@ class KafkaApis(val requestChannel: RequestChannel,
       throw new ClusterAuthorizationException(s&quot;Request $request is not authorized.&quot;)
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def updateRecordsProcessingStats(request: RequestChannel.Request, tp: TopicPartition,&lt;br/&gt;
-                                           processingStats: RecordsProcessingStats): Unit = {&lt;br/&gt;
-    val conversionCount = processingStats.numRecordsConverted&lt;br/&gt;
+  private def updateRecordConversionStats(request: RequestChannel.Request,&lt;br/&gt;
+                                          tp: TopicPartition,&lt;br/&gt;
+                                          conversionStats: RecordConversionStats): Unit = {&lt;br/&gt;
+    val conversionCount = conversionStats.numRecordsConverted&lt;br/&gt;
     if (conversionCount &amp;gt; 0) {&lt;br/&gt;
       request.header.apiKey match {
         case ApiKeys.PRODUCE =&amp;gt;
@@ -2241,9 +2257,9 @@ class KafkaApis(val requestChannel: RequestChannel,
         case _ =&amp;gt;
           throw new IllegalStateException(&quot;Message conversion info is recorded only for Produce/Fetch requests&quot;)
       }&lt;br/&gt;
-      request.messageConversionsTimeNanos = processingStats.conversionTimeNanos&lt;br/&gt;
+      request.messageConversionsTimeNanos = conversionStats.conversionTimeNanos&lt;br/&gt;
     }&lt;br/&gt;
-    request.temporaryMemoryBytes = processingStats.temporaryMemoryBytes&lt;br/&gt;
+    request.temporaryMemoryBytes = conversionStats.temporaryMemoryBytes&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def handleError(request: RequestChannel.Request, e: Throwable) {&lt;br/&gt;
@@ -2257,21 +2273,25 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
 &lt;br/&gt;
   // Throttle the channel if the request quota is enabled but has been violated. Regardless of throttling, send the&lt;br/&gt;
   // response immediately.&lt;br/&gt;
-  private def sendResponseMaybeThrottle(request: RequestChannel.Request, createResponse: Int =&amp;gt; AbstractResponse): Unit = {&lt;br/&gt;
+  private def sendResponseMaybeThrottle(request: RequestChannel.Request,&lt;br/&gt;
+                                        createResponse: Int =&amp;gt; AbstractResponse,&lt;br/&gt;
+                                        onComplete: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Send =&amp;gt; Unit&amp;#93;&lt;/span&gt; = None): Unit = {
     val throttleTimeMs = quotas.request.maybeRecordAndGetThrottleTimeMs(request)
-    quotas.request.throttle(request, throttleTimeMs, sendActionOnlyResponse(request))
-    sendResponse(request, Some(createResponse(throttleTimeMs)))
+    quotas.request.throttle(request, throttleTimeMs, sendResponse)
+    sendResponse(request, Some(createResponse(throttleTimeMs)), onComplete)
   }&lt;br/&gt;
 &lt;br/&gt;
   private def sendErrorResponseMaybeThrottle(request: RequestChannel.Request, error: Throwable) {
     val throttleTimeMs = quotas.request.maybeRecordAndGetThrottleTimeMs(request)
-    quotas.request.throttle(request, throttleTimeMs, sendActionOnlyResponse(request))
+    quotas.request.throttle(request, throttleTimeMs, sendResponse)
     sendErrorOrCloseConnection(request, error, throttleTimeMs)
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def sendResponseExemptThrottle(request: RequestChannel.Request, response: AbstractResponse): Unit = {&lt;br/&gt;
+  private def sendResponseExemptThrottle(request: RequestChannel.Request,&lt;br/&gt;
+                                         response: AbstractResponse,&lt;br/&gt;
+                                         onComplete: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Send =&amp;gt; Unit&amp;#93;&lt;/span&gt; = None): Unit = {
     quotas.request.maybeRecordExempt(request)
-    sendResponse(request, Some(response))
+    sendResponse(request, Some(response), onComplete)
   }&lt;br/&gt;
 &lt;br/&gt;
   private def sendErrorResponseExemptThrottle(request: RequestChannel.Request, error: Throwable): Unit = {
@@ -2285,38 +2305,41 @@ class KafkaApis(val requestChannel: RequestChannel,
     if (response == null)
       closeConnection(request, requestBody.errorCounts(error))
     else
-      sendResponse(request, Some(response))
+      sendResponse(request, Some(response), None)
   }&lt;br/&gt;
 &lt;br/&gt;
   private def sendNoOpResponseExemptThrottle(request: RequestChannel.Request): Unit = {
     quotas.request.maybeRecordExempt(request)
-    sendResponse(request, None)
+    sendResponse(request, None, None)
   }&lt;br/&gt;
 &lt;br/&gt;
   private def closeConnection(request: RequestChannel.Request, errorCounts: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Errors, Integer&amp;#93;&lt;/span&gt;): Unit = {
     // This case is used when the request handler has encountered an error, but the client
     // does not expect a response (e.g. when produce request has acks set to 0)
     requestChannel.updateErrorMetrics(request.header.apiKey, errorCounts.asScala)
-    sendActionOnlyResponse(request)(CloseConnectionAction)
+    requestChannel.sendResponse(new RequestChannel.CloseConnectionResponse(request))
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def sendResponse(request: RequestChannel.Request, responseOpt: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;AbstractResponse&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
+  private def sendResponse(request: RequestChannel.Request,&lt;br/&gt;
+                           responseOpt: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;AbstractResponse&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                           onComplete: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Send =&amp;gt; Unit&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
     // Update error metrics for each error code in the response including Errors.NONE&lt;br/&gt;
     responseOpt.foreach(response =&amp;gt; requestChannel.updateErrorMetrics(request.header.apiKey, response.errorCounts.asScala))&lt;br/&gt;
 &lt;br/&gt;
-    responseOpt match {&lt;br/&gt;
+    val response = responseOpt match {
       case Some(response) =&amp;gt;
         val responseSend = request.context.buildResponse(response)
         val responseString =
           if (RequestChannel.isRequestLoggingEnabled) Some(response.toString(request.context.apiVersion))
           else None
-        requestChannel.sendResponse(new RequestChannel.Response(request, Some(responseSend), SendAction, responseString))
+        new RequestChannel.SendResponse(request, responseSend, responseString, onComplete)
       case None =&amp;gt;
-        sendActionOnlyResponse(request)(NoOpAction)
+        new RequestChannel.NoOpResponse(request)
     }&lt;br/&gt;
+    sendResponse(response)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def sendActionOnlyResponse(request: RequestChannel.Request)(responseAction: ResponseAction): Unit = {&lt;br/&gt;
-    requestChannel.sendResponse(new RequestChannel.Response(request, None, responseAction, None))&lt;br/&gt;
+  private def sendResponse(response: RequestChannel.Response): Unit = {
+    requestChannel.sendResponse(response)
   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
index ba7203edb92..5a505c3d377 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
@@ -20,19 +20,18 @@ package kafka.server&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
 import java.util&lt;br/&gt;
 &lt;br/&gt;
-import AbstractFetcherThread.ResultWithPartitions&lt;br/&gt;
-import kafka.cluster.BrokerEndPoint&lt;br/&gt;
-import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
-import org.apache.kafka.common.requests.{EpochEndOffset, FetchResponse, FetchRequest =&amp;gt; JFetchRequest}&lt;br/&gt;
-import ReplicaAlterLogDirsThread.FetchRequest&lt;br/&gt;
-import ReplicaAlterLogDirsThread.PartitionData&lt;br/&gt;
 import kafka.api.Request&lt;br/&gt;
+import kafka.cluster.BrokerEndPoint&lt;br/&gt;
+import kafka.server.AbstractFetcherThread.ResultWithPartitions&lt;br/&gt;
 import kafka.server.QuotaFactory.UnboundedQuota&lt;br/&gt;
+import kafka.server.ReplicaAlterLogDirsThread.{FetchRequest, PartitionData}&lt;br/&gt;
 import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.errors.KafkaStorageException&lt;br/&gt;
 import org.apache.kafka.common.protocol.{ApiKeys, Errors}&lt;br/&gt;
-import org.apache.kafka.common.record.{FileRecords, MemoryRecords}&lt;br/&gt;
+import org.apache.kafka.common.record.{FileRecords, MemoryRecords, Records}&lt;br/&gt;
+import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
+import org.apache.kafka.common.requests.{EpochEndOffset, FetchResponse, FetchRequest =&amp;gt; JFetchRequest}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.{Map, Seq, Set, mutable}&lt;br/&gt;
@@ -58,7 +57,7 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
   private val fetchSize = brokerConfig.replicaFetchMaxBytes&lt;br/&gt;
 &lt;br/&gt;
   def fetch(fetchRequest: FetchRequest): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, PartitionData)&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    var partitionData: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchResponse.PartitionData)&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
+    var partitionData: Seq[(TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;)] = null&lt;br/&gt;
     val request = fetchRequest.underlying.build()&lt;br/&gt;
 &lt;br/&gt;
     def processResponseCallback(responsePartitionData: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, FetchPartitionData)&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
@@ -256,7 +255,7 @@ object ReplicaAlterLogDirsThread {
     override def toString = underlying.toString
   }&lt;br/&gt;
 &lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; class PartitionData(val underlying: FetchResponse.PartitionData) extends AbstractFetcherThread.PartitionData {&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; class PartitionData(val underlying: FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;) extends AbstractFetcherThread.PartitionData {&lt;br/&gt;
 &lt;br/&gt;
     def error = underlying.error&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index 72b6616a2bd..cf8d829f850 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -33,7 +33,7 @@ import org.apache.kafka.common.errors.KafkaStorageException&lt;br/&gt;
 import org.apache.kafka.common.internals.FatalExitError&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
-import org.apache.kafka.common.record.MemoryRecords&lt;br/&gt;
+import org.apache.kafka.common.record.{MemoryRecords, Records}&lt;br/&gt;
 import org.apache.kafka.common.requests.{EpochEndOffset, FetchResponse, ListOffsetRequest, ListOffsetResponse, OffsetsForLeaderEpochRequest, OffsetsForLeaderEpochResponse, FetchRequest =&amp;gt; JFetchRequest}&lt;br/&gt;
 import org.apache.kafka.common.utils.{LogContext, Time}&lt;br/&gt;
 &lt;br/&gt;
@@ -224,7 +224,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
   protected def fetch(fetchRequest: FetchRequest): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, PartitionData)&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     try {&lt;br/&gt;
       val clientResponse = leaderEndpoint.sendRequest(fetchRequest.underlying)&lt;br/&gt;
-      val fetchResponse = clientResponse.responseBody.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FetchResponse&amp;#93;&lt;/span&gt;&lt;br/&gt;
+      val fetchResponse = clientResponse.responseBody.asInstanceOf[FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
       if (!fetchSessionHandler.handleResponse(fetchResponse)) {
         Nil
       } else {&lt;br/&gt;
@@ -389,7 +389,7 @@ object ReplicaFetcherThread {     override def toString = underlying.toString   }&lt;br/&gt;
 &lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; class PartitionData(val underlying: FetchResponse.PartitionData) extends AbstractFetcherThread.PartitionData {&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; class PartitionData(val underlying: FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;) extends AbstractFetcherThread.PartitionData {&lt;br/&gt;
 &lt;br/&gt;
     def error = underlying.error&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaManager.scala b/core/src/main/scala/kafka/server/ReplicaManager.scala&lt;br/&gt;
index 5dbe25b26e4..24f3235570f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaManager.scala&lt;br/&gt;
@@ -31,21 +31,20 @@ import kafka.server.QuotaFactory.{QuotaManagers, UnboundedQuota}&lt;br/&gt;
 import kafka.server.checkpoints.OffsetCheckpointFile&lt;br/&gt;
 import kafka.utils._&lt;br/&gt;
 import kafka.zk.KafkaZkClient&lt;br/&gt;
-import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.errors._&lt;br/&gt;
 import org.apache.kafka.common.internals.Topic&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
-import org.apache.kafka.common.protocol.Errors.UNKNOWN_TOPIC_OR_PARTITION&lt;br/&gt;
-import org.apache.kafka.common.protocol.Errors.KAFKA_STORAGE_ERROR&lt;br/&gt;
+import org.apache.kafka.common.protocol.Errors.{KAFKA_STORAGE_ERROR, UNKNOWN_TOPIC_OR_PARTITION}&lt;br/&gt;
 import org.apache.kafka.common.record._&lt;br/&gt;
+import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
 import org.apache.kafka.common.requests.DescribeLogDirsResponse.{LogDirInfo, ReplicaInfo}&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest.PartitionData&lt;br/&gt;
-import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction&lt;br/&gt;
 import org.apache.kafka.common.requests.ProduceResponse.PartitionResponse&lt;br/&gt;
 import org.apache.kafka.common.requests._&lt;br/&gt;
 import org.apache.kafka.common.utils.Time&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection._&lt;br/&gt;
@@ -471,7 +470,7 @@ class ReplicaManager(val config: KafkaConfig,&lt;br/&gt;
                     entriesPerPartition: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, MemoryRecords&amp;#93;&lt;/span&gt;,&lt;br/&gt;
                     responseCallback: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionResponse&amp;#93;&lt;/span&gt; =&amp;gt; Unit,&lt;br/&gt;
                     delayedProduceLock: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Lock&amp;#93;&lt;/span&gt; = None,&lt;br/&gt;
-                    processingStatsCallback: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, RecordsProcessingStats&amp;#93;&lt;/span&gt; =&amp;gt; Unit = _ =&amp;gt; ()) {&lt;br/&gt;
+                    recordConversionStatsCallback: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, RecordConversionStats&amp;#93;&lt;/span&gt; =&amp;gt; Unit = _ =&amp;gt; ()) {&lt;br/&gt;
     if (isValidRequiredAcks(requiredAcks)) {
       val sTime = time.milliseconds
       val localProduceResults = appendToLocalLog(internalTopicsAllowed = internalTopicsAllowed,
@@ -485,7 +484,7 @@ class ReplicaManager(val config: KafkaConfig,
                   new PartitionResponse(result.error, result.info.firstOffset.getOrElse(-1), result.info.logAppendTime, result.info.logStartOffset)) // response status
       }&lt;br/&gt;
 &lt;br/&gt;
-      processingStatsCallback(localProduceResults.mapValues(_.info.recordsProcessingStats))&lt;br/&gt;
+      recordConversionStatsCallback(localProduceResults.mapValues(_.info.recordConversionStats))&lt;br/&gt;
 &lt;br/&gt;
       if (delayedProduceRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) {&lt;br/&gt;
         // create delayed produce operation&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ThrottledChannel.scala b/core/src/main/scala/kafka/server/ThrottledChannel.scala&lt;br/&gt;
index 74357d52a10..8fe8649848d 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ThrottledChannel.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ThrottledChannel.scala&lt;br/&gt;
@@ -19,28 +19,31 @@ package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.util.concurrent.{Delayed, TimeUnit}&lt;br/&gt;
 &lt;br/&gt;
-import kafka.network.RequestChannel.{EndThrottlingAction, ResponseAction, StartThrottlingAction}&lt;br/&gt;
+import kafka.network&lt;br/&gt;
+import kafka.network.RequestChannel&lt;br/&gt;
+import kafka.network.RequestChannel.Response&lt;br/&gt;
 import kafka.utils.Logging&lt;br/&gt;
 import org.apache.kafka.common.utils.Time&lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
   * Represents a request whose response has been delayed.&lt;br/&gt;
-  * @param time @Time instance to use&lt;br/&gt;
-  * @param throttleTimeMs delay associated with this request&lt;br/&gt;
+  * @param request The request that has been delayed&lt;br/&gt;
+  * @param time Time instance to use&lt;br/&gt;
+  * @param throttleTimeMs Delay associated with this request&lt;br/&gt;
   * @param channelThrottlingCallback Callback for channel throttling&lt;br/&gt;
   */&lt;br/&gt;
-class ThrottledChannel(val time: Time, val throttleTimeMs: Int, channelThrottlingCallback: (ResponseAction) =&amp;gt; Unit)&lt;br/&gt;
+class ThrottledChannel(val request: RequestChannel.Request, val time: Time, val throttleTimeMs: Int, channelThrottlingCallback: Response =&amp;gt; Unit)&lt;br/&gt;
   extends Delayed with Logging {&lt;br/&gt;
   var endTime = time.milliseconds + throttleTimeMs&lt;br/&gt;
 &lt;br/&gt;
   // Notify the socket server that throttling has started for this channel.&lt;br/&gt;
-  channelThrottlingCallback(StartThrottlingAction)&lt;br/&gt;
+  channelThrottlingCallback(new RequestChannel.StartThrottlingResponse(request))&lt;br/&gt;
 &lt;br/&gt;
   // Notify the socket server that throttling has been done for this channel.&lt;br/&gt;
   def notifyThrottlingDone(): Unit = {
     trace(&quot;Channel throttled for: &quot; + throttleTimeMs + &quot; ms&quot;)
-    channelThrottlingCallback(EndThrottlingAction)
+    channelThrottlingCallback(new network.RequestChannel.EndThrottlingResponse(request))
   }&lt;br/&gt;
 &lt;br/&gt;
   override def getDelay(unit: TimeUnit): Long = {&lt;br/&gt;
diff --git a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
index da45be2e6be..681497ea72c 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
@@ -19,8 +19,7 @@ import java.util.regex.Pattern&lt;br/&gt;
 import java.util.{ArrayList, Collections, Properties}&lt;br/&gt;
 &lt;br/&gt;
 import kafka.admin.AdminClient&lt;br/&gt;
-import kafka.admin.ConsumerGroupCommand.ConsumerGroupCommandOptions&lt;br/&gt;
-import kafka.admin.ConsumerGroupCommand.KafkaConsumerGroupService&lt;br/&gt;
+import kafka.admin.ConsumerGroupCommand.{ConsumerGroupCommandOptions, KafkaConsumerGroupService}&lt;br/&gt;
 import kafka.common.TopicAndPartition&lt;br/&gt;
 import kafka.log.LogConfig&lt;br/&gt;
 import kafka.network.SocketServer&lt;br/&gt;
@@ -28,23 +27,21 @@ import kafka.security.auth._&lt;br/&gt;
 import kafka.server.{BaseRequestTest, KafkaConfig}&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.clients.admin.NewPartitions&lt;br/&gt;
-import org.apache.kafka.clients.consumer.OffsetAndMetadata&lt;br/&gt;
-import org.apache.kafka.clients.consumer.internals.NoOpConsumerRebalanceListener&lt;br/&gt;
 import org.apache.kafka.clients.consumer._&lt;br/&gt;
+import org.apache.kafka.clients.consumer.internals.NoOpConsumerRebalanceListener&lt;br/&gt;
 import org.apache.kafka.clients.producer._&lt;br/&gt;
+import org.apache.kafka.common.acl.{AccessControlEntry, AccessControlEntryFilter, AclBinding, AclBindingFilter, AclOperation, AclPermissionType}&lt;br/&gt;
 import org.apache.kafka.common.errors._&lt;br/&gt;
 import org.apache.kafka.common.internals.Topic.GROUP_METADATA_TOPIC_NAME&lt;br/&gt;
-import org.apache.kafka.common.KafkaException&lt;br/&gt;
-import org.apache.kafka.common.protocol.{ApiKeys, Errors}&lt;br/&gt;
-import org.apache.kafka.common.requests.{Resource =&amp;gt; RResource, ResourceType =&amp;gt; RResourceType, _}&lt;br/&gt;
-import org.apache.kafka.common.acl.{AccessControlEntry, AccessControlEntryFilter, AclBinding, AclBindingFilter, AclOperation, AclPermissionType}&lt;br/&gt;
 import org.apache.kafka.common.network.ListenerName&lt;br/&gt;
-import org.apache.kafka.common.record.{CompressionType, MemoryRecords, SimpleRecord}&lt;br/&gt;
+import org.apache.kafka.common.protocol.{ApiKeys, Errors}&lt;br/&gt;
+import org.apache.kafka.common.record.{CompressionType, MemoryRecords, Records, SimpleRecord}&lt;br/&gt;
 import org.apache.kafka.common.requests.CreateAclsRequest.AclCreation&lt;br/&gt;
 import org.apache.kafka.common.requests.CreateTopicsRequest.TopicDetails&lt;br/&gt;
+import org.apache.kafka.common.requests.{Resource =&amp;gt; RResource, ResourceType =&amp;gt; RResourceType, _}&lt;br/&gt;
 import org.apache.kafka.common.resource.{ResourceFilter, Resource =&amp;gt; AdminResource, ResourceType =&amp;gt; AdminResourceType}&lt;br/&gt;
 import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}&lt;br/&gt;
-import org.apache.kafka.common.{Node, TopicPartition, requests}&lt;br/&gt;
+import org.apache.kafka.common.{KafkaException, Node, TopicPartition, requests}&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.{After, Assert, Before, Test}&lt;br/&gt;
 &lt;br/&gt;
@@ -115,7 +112,7 @@ class AuthorizerIntegrationTest extends BaseRequestTest {&lt;br/&gt;
   val requestKeyToResponseDeserializer: Map[ApiKeys, Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_ &amp;lt;: Any&amp;#93;&lt;/span&gt;] =&lt;br/&gt;
     Map(ApiKeys.METADATA -&amp;gt; classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;requests.MetadataResponse&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       ApiKeys.PRODUCE -&amp;gt; classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;requests.ProduceResponse&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-      ApiKeys.FETCH -&amp;gt; classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;requests.FetchResponse&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+      ApiKeys.FETCH -&amp;gt; classOf[requests.FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;],&lt;br/&gt;
       ApiKeys.LIST_OFFSETS -&amp;gt; classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;requests.ListOffsetResponse&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       ApiKeys.OFFSET_COMMIT -&amp;gt; classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;requests.OffsetCommitResponse&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       ApiKeys.OFFSET_FETCH -&amp;gt; classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;requests.OffsetFetchResponse&amp;#93;&lt;/span&gt;,&lt;br/&gt;
@@ -153,7 +150,7 @@ class AuthorizerIntegrationTest extends BaseRequestTest {&lt;br/&gt;
   val requestKeyToError = Map&lt;span class=&quot;error&quot;&gt;&amp;#91;ApiKeys, Nothing =&amp;gt; Errors&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     ApiKeys.METADATA -&amp;gt; ((resp: requests.MetadataResponse) =&amp;gt; resp.errors.asScala.find(_._1 == topic).getOrElse((&quot;test&quot;, Errors.NONE))._2),&lt;br/&gt;
     ApiKeys.PRODUCE -&amp;gt; ((resp: requests.ProduceResponse) =&amp;gt; resp.responses.asScala.find(_._1 == tp).get._2.error),&lt;br/&gt;
-    ApiKeys.FETCH -&amp;gt; ((resp: requests.FetchResponse) =&amp;gt; resp.responseData.asScala.find(_._1 == tp).get._2.error),&lt;br/&gt;
+    ApiKeys.FETCH -&amp;gt; ((resp: requests.FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;) =&amp;gt; resp.responseData.asScala.find(_._1 == tp).get._2.error),&lt;br/&gt;
     ApiKeys.LIST_OFFSETS -&amp;gt; ((resp: requests.ListOffsetResponse) =&amp;gt; resp.responseData.asScala.find(_._1 == tp).get._2.error),&lt;br/&gt;
     ApiKeys.OFFSET_COMMIT -&amp;gt; ((resp: requests.OffsetCommitResponse) =&amp;gt; resp.responseData.asScala.find(_._1 == tp).get._2),&lt;br/&gt;
     ApiKeys.OFFSET_FETCH -&amp;gt; ((resp: requests.OffsetFetchResponse) =&amp;gt; resp.error),&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala b/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala&lt;br/&gt;
index 0ecc3f538b1..d4dcd9f100f 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala&lt;br/&gt;
@@ -17,8 +17,8 @@&lt;br/&gt;
 &lt;br/&gt;
 package kafka.coordinator&lt;br/&gt;
 &lt;br/&gt;
-import java.util.{ Collections, Random }&lt;br/&gt;
-import java.util.concurrent.{ ConcurrentHashMap, Executors }&lt;br/&gt;
+import java.util.{Collections, Random}&lt;br/&gt;
+import java.util.concurrent.{ConcurrentHashMap, Executors}&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicInteger&lt;br/&gt;
 import java.util.concurrent.locks.Lock&lt;br/&gt;
 &lt;br/&gt;
@@ -30,10 +30,10 @@ import kafka.utils.timer.MockTimer&lt;br/&gt;
 import kafka.zk.KafkaZkClient&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
-import org.apache.kafka.common.record.{ MemoryRecords, RecordBatch, RecordsProcessingStats }&lt;br/&gt;
+import org.apache.kafka.common.record.{MemoryRecords, RecordBatch, RecordConversionStats}&lt;br/&gt;
 import org.apache.kafka.common.requests.ProduceResponse.PartitionResponse&lt;br/&gt;
 import org.easymock.EasyMock&lt;br/&gt;
-import org.junit.{ After, Before }&lt;br/&gt;
+import org.junit.{After, Before}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection._&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
@@ -177,7 +177,7 @@ object AbstractCoordinatorConcurrencyTest {&lt;br/&gt;
                                entriesPerPartition: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, MemoryRecords&amp;#93;&lt;/span&gt;,&lt;br/&gt;
                                responseCallback: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionResponse&amp;#93;&lt;/span&gt; =&amp;gt; Unit,&lt;br/&gt;
                                delayedProduceLock: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Lock&amp;#93;&lt;/span&gt; = None,&lt;br/&gt;
-                               processingStatsCallback: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, RecordsProcessingStats&amp;#93;&lt;/span&gt; =&amp;gt; Unit = _ =&amp;gt; ()) {&lt;br/&gt;
+                               processingStatsCallback: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, RecordConversionStats&amp;#93;&lt;/span&gt; =&amp;gt; Unit = _ =&amp;gt; ()) {&lt;br/&gt;
 &lt;br/&gt;
       if (entriesPerPartition.isEmpty)&lt;br/&gt;
         return&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala b/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala&lt;br/&gt;
index f68ff9eceab..2a367e0d242 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogValidatorTest.scala&lt;br/&gt;
@@ -63,7 +63,7 @@ class LogValidatorTest {
     assertEquals(s&quot;The offset of max timestamp should be 0&quot;, 0, validatedResults.shallowOffsetOfMaxTimestamp)
     assertFalse(&quot;Message size should not have been changed&quot;, validatedResults.messageSizeMaybeChanged)
 
-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 0, records,
+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 0, records,
       compressed = false)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -103,8 +103,8 @@ class LogValidatorTest {
       records.records.asScala.size - 1, validatedResults.shallowOffsetOfMaxTimestamp)
     assertTrue(&quot;Message size may have been changed&quot;, validatedResults.messageSizeMaybeChanged)
 
-    val stats = validatedResults.recordsProcessingStats
-    verifyRecordsProcessingStats(stats, numConvertedRecords = 3, records, compressed = true)
+    val stats = validatedResults.recordConversionStats
+    verifyRecordConversionStats(stats, numConvertedRecords = 3, records, compressed = true)
   }&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
@@ -145,7 +145,7 @@ class LogValidatorTest {
       records.records.asScala.size - 1, validatedResults.shallowOffsetOfMaxTimestamp)
     assertFalse(&quot;Message size should not have been changed&quot;, validatedResults.messageSizeMaybeChanged)
 
-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 0, records,
+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 0, records,
       compressed = true)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -255,7 +255,7 @@ class LogValidatorTest {
     assertEquals(s&quot;Offset of max timestamp should be 1&quot;, 1, validatingResults.shallowOffsetOfMaxTimestamp)
     assertFalse(&quot;Message size should not have been changed&quot;, validatingResults.messageSizeMaybeChanged)
 
-    verifyRecordsProcessingStats(validatingResults.recordsProcessingStats, numConvertedRecords = 0, records,
+    verifyRecordConversionStats(validatingResults.recordConversionStats, numConvertedRecords = 0, records,
       compressed = false)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -320,7 +320,7 @@ class LogValidatorTest {
     assertEquals(&quot;Offset of max timestamp should be 2&quot;, 2, validatingResults.shallowOffsetOfMaxTimestamp)
     assertTrue(&quot;Message size should have been changed&quot;, validatingResults.messageSizeMaybeChanged)
 
-    verifyRecordsProcessingStats(validatingResults.recordsProcessingStats, numConvertedRecords = 3, records,
+    verifyRecordConversionStats(validatingResults.recordConversionStats, numConvertedRecords = 3, records,
       compressed = true)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -364,7 +364,7 @@ class LogValidatorTest {
       validatedRecords.records.asScala.size - 1, validatedResults.shallowOffsetOfMaxTimestamp)
     assertTrue(&quot;Message size should have been changed&quot;, validatedResults.messageSizeMaybeChanged)
 
-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 3, records,
+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 3, records,
       compressed = true)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -405,7 +405,7 @@ class LogValidatorTest {       validatedRecords.records.asScala.size - 1, validatedResults.shallowOffsetOfMaxTimestamp)     assertTrue(&quot;Message size should have been changed&quot;, validatedResults.messageSizeMaybeChanged) -    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 3, records,+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 3, records,       compressed = true)   }&lt;br/&gt;
 &lt;br/&gt;
@@ -466,7 +466,7 @@ class LogValidatorTest {
       validatedRecords.records.asScala.size - 1, validatedResults.shallowOffsetOfMaxTimestamp)
     assertFalse(&quot;Message size should not have been changed&quot;, validatedResults.messageSizeMaybeChanged)
 
-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 0, records,
+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 0, records,
       compressed = true)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -697,7 +697,7 @@ class LogValidatorTest {
       partitionLeaderEpoch = RecordBatch.NO_PARTITION_LEADER_EPOCH,
       isFromClient = true)
     checkOffsets(validatedResults.validatedRecords, offset)
-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 3, records,
+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 3, records,
       compressed = false)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -719,7 +719,7 @@ class LogValidatorTest {       partitionLeaderEpoch = RecordBatch.NO_PARTITION_LEADER_EPOCH,       isFromClient = true)     checkOffsets(validatedResults.validatedRecords, offset)-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 3, records,+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 3, records,       compressed = false)   }&lt;br/&gt;
 &lt;br/&gt;
@@ -741,7 +741,7 @@ class LogValidatorTest {
       partitionLeaderEpoch = RecordBatch.NO_PARTITION_LEADER_EPOCH,
       isFromClient = true)
     checkOffsets(validatedResults.validatedRecords, offset)
-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 3, records,
+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 3, records,
       compressed = true)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -763,7 +763,7 @@ class LogValidatorTest {       partitionLeaderEpoch = RecordBatch.NO_PARTITION_LEADER_EPOCH,       isFromClient = true)     checkOffsets(validatedResults.validatedRecords, offset)-    verifyRecordsProcessingStats(validatedResults.recordsProcessingStats, numConvertedRecords = 3, records,+    verifyRecordConversionStats(validatedResults.recordConversionStats, numConvertedRecords = 3, records,       compressed = true)   }&lt;br/&gt;
 &lt;br/&gt;
@@ -1131,8 +1131,8 @@ class LogValidatorTest {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def verifyRecordsProcessingStats(stats: RecordsProcessingStats, numConvertedRecords: Int, records: MemoryRecords,&lt;br/&gt;
-                                   compressed: Boolean): Unit = {&lt;br/&gt;
+  def verifyRecordConversionStats(stats: RecordConversionStats, numConvertedRecords: Int, records: MemoryRecords,&lt;br/&gt;
+                                  compressed: Boolean): Unit = {&lt;br/&gt;
     assertNotNull(&quot;Records processing info is null&quot;, stats)&lt;br/&gt;
     assertEquals(numConvertedRecords, stats.numRecordsConverted)&lt;br/&gt;
     if (numConvertedRecords &amp;gt; 0) {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/network/SocketServerTest.scala b/core/src/test/scala/unit/kafka/network/SocketServerTest.scala&lt;br/&gt;
index dfa388b87e9..c0e27cf34f0 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/network/SocketServerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/network/SocketServerTest.scala&lt;br/&gt;
@@ -20,13 +20,12 @@ package kafka.network&lt;br/&gt;
 import java.io._&lt;br/&gt;
 import java.net._&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
-import java.util.{HashMap, Random}&lt;br/&gt;
 import java.nio.channels.SocketChannel&lt;br/&gt;
-import javax.net.ssl._&lt;br/&gt;
+import java.util.{HashMap, Random}&lt;br/&gt;
 &lt;br/&gt;
 import com.yammer.metrics.core.{Gauge, Meter}
&lt;p&gt; import com.yammer.metrics.&lt;/p&gt;
{Metrics =&amp;gt; YammerMetrics}
&lt;p&gt;-import kafka.network.RequestChannel.&lt;/p&gt;
{NoOpAction, ResponseAction, SendAction}
&lt;p&gt;+import javax.net.ssl._&lt;br/&gt;
 import kafka.security.CredentialProvider&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{KafkaConfig, ThrottledChannel}
&lt;p&gt; import kafka.utils.TestUtils&lt;br/&gt;
@@ -42,7 +41,7 @@ import org.apache.kafka.common.security.auth.&lt;/p&gt;
{KafkaPrincipal, SecurityProtocol}&lt;br/&gt;
 import org.apache.kafka.common.security.scram.internal.ScramMechanism&lt;br/&gt;
 import org.apache.kafka.common.utils.{LogContext, MockTime, Time}&lt;br/&gt;
 import org.apache.log4j.Level&lt;br/&gt;
-import org.junit.Assert.{assertEquals, _}&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
 import org.junit._&lt;br/&gt;
 import org.scalatest.junit.JUnitSuite&lt;br/&gt;
 &lt;br/&gt;
@@ -132,7 +131,7 @@ class SocketServerTest extends JUnitSuite {
     byteBuffer.rewind()
 
     val send = new NetworkSend(request.context.connectionId, byteBuffer)
-    channel.sendResponse(new RequestChannel.Response(request, Some(send), SendAction, Some(request.header.toString)))
+    channel.sendResponse(new RequestChannel.SendResponse(request, send, Some(request.header.toString), None))
   }&lt;br/&gt;
 &lt;br/&gt;
   def connect(s: SocketServer = server, protocol: SecurityProtocol = SecurityProtocol.PLAINTEXT, localAddr: InetAddress = null) = {&lt;br/&gt;
@@ -215,7 +214,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
     for (_ &amp;lt;- 0 until 10) {
       val request = receiveRequest(server.requestChannel)
       assertNotNull(&quot;receiveRequest timed out&quot;, request)
-      server.requestChannel.sendResponse(new RequestChannel.Response(request, None, RequestChannel.NoOpAction, None))
+      server.requestChannel.sendResponse(new RequestChannel.NoOpResponse(request))
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -229,7 +228,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
     for (_ &amp;lt;- 0 until 3) {       val request = receiveRequest(server.requestChannel)       assertNotNull(&quot;receiveRequest timed out&quot;, request)-      server.requestChannel.sendResponse(new RequestChannel.Response(request, None, RequestChannel.NoOpAction, None))+      server.requestChannel.sendResponse(new RequestChannel.NoOpResponse(request))     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -397,7 +396,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
 &lt;br/&gt;
   // Prepares test setup for throttled channel tests. throttlingDone controls whether or not throttling has completed&lt;br/&gt;
   // in quota manager.&lt;br/&gt;
-  def throttledChannelTestSetUp(socket: Socket, serializedBytes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, action: RequestChannel.ResponseAction,&lt;br/&gt;
+  def throttledChannelTestSetUp(socket: Socket, serializedBytes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, noOpResponse: Boolean,&lt;br/&gt;
                                 throttlingInProgress: Boolean): RequestChannel.Request = {&lt;br/&gt;
     sendRequest(socket, serializedBytes)&lt;br/&gt;
 &lt;br/&gt;
@@ -406,16 +405,21 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
     val request = receiveRequest(server.requestChannel)&lt;br/&gt;
     val byteBuffer = request.body&lt;span class=&quot;error&quot;&gt;&amp;#91;AbstractRequest&amp;#93;&lt;/span&gt;.serialize(request.header)&lt;br/&gt;
     val send = new NetworkSend(request.context.connectionId, byteBuffer)&lt;br/&gt;
-    def channelThrottlingCallback(responseAction: ResponseAction): Unit = {&lt;br/&gt;
-      server.requestChannel.sendResponse(new RequestChannel.Response(request, None, responseAction, None))&lt;br/&gt;
+    def channelThrottlingCallback(response: RequestChannel.Response): Unit = {
+      server.requestChannel.sendResponse(response)
     }&lt;br/&gt;
-    val throttledChannel = new ThrottledChannel(new MockTime(), 100, channelThrottlingCallback)&lt;br/&gt;
-    server.requestChannel.sendResponse(new RequestChannel.Response(request, Some(send), action,&lt;br/&gt;
-      Some(request.header.toString)))&lt;br/&gt;
+    val throttledChannel = new ThrottledChannel(request, new MockTime(), 100, channelThrottlingCallback)&lt;br/&gt;
+    val response =&lt;br/&gt;
+      if (!noOpResponse)&lt;br/&gt;
+        new RequestChannel.SendResponse(request, send, Some(request.header.toString), None)&lt;br/&gt;
+      else&lt;br/&gt;
+        new RequestChannel.NoOpResponse(request)&lt;br/&gt;
+    server.requestChannel.sendResponse(response)&lt;br/&gt;
 &lt;br/&gt;
     // Quota manager would call notifyThrottlingDone() on throttling completion. Simulate it if throttleingInProgress is&lt;br/&gt;
     // false.&lt;br/&gt;
-    if (!throttlingInProgress) throttledChannel.notifyThrottlingDone()&lt;br/&gt;
+    if (!throttlingInProgress)&lt;br/&gt;
+      throttledChannel.notifyThrottlingDone()&lt;br/&gt;
 &lt;br/&gt;
     request&lt;br/&gt;
   }&lt;br/&gt;
@@ -428,7 +432,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
     val socket = connect(protocol = SecurityProtocol.PLAINTEXT)&lt;br/&gt;
     val serializedBytes = producerRequestBytes()&lt;br/&gt;
     // SendAction with throttling in progress&lt;br/&gt;
-    val request = throttledChannelTestSetUp(socket, serializedBytes, SendAction, true)&lt;br/&gt;
+    val request = throttledChannelTestSetUp(socket, serializedBytes, false, true)&lt;br/&gt;
 &lt;br/&gt;
     // receive response&lt;br/&gt;
     assertEquals(serializedBytes.toSeq, receiveResponse(socket).toSeq)&lt;br/&gt;
@@ -442,7 +446,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
     val socket = connect(protocol = SecurityProtocol.PLAINTEXT)&lt;br/&gt;
     val serializedBytes = producerRequestBytes()&lt;br/&gt;
     // SendAction with throttling in progress&lt;br/&gt;
-    val request = throttledChannelTestSetUp(socket, serializedBytes, SendAction, false)&lt;br/&gt;
+    val request = throttledChannelTestSetUp(socket, serializedBytes, false, false)&lt;br/&gt;
 &lt;br/&gt;
     // receive response&lt;br/&gt;
     assertEquals(serializedBytes.toSeq, receiveResponse(socket).toSeq)&lt;br/&gt;
@@ -457,7 +461,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
     val socket = connect(protocol = SecurityProtocol.PLAINTEXT)&lt;br/&gt;
     val serializedBytes = producerRequestBytes()&lt;br/&gt;
     // SendAction with throttling in progress&lt;br/&gt;
-    val request = throttledChannelTestSetUp(socket, serializedBytes, NoOpAction, true)&lt;br/&gt;
+    val request = throttledChannelTestSetUp(socket, serializedBytes, true, true)&lt;br/&gt;
 &lt;br/&gt;
     TestUtils.waitUntilTrue(() =&amp;gt; openOrClosingChannel(request).exists(c =&amp;gt; c.muteState() == ChannelMuteState.MUTED_AND_THROTTLED), &quot;fail&quot;)&lt;br/&gt;
     // Channel should still be muted.&lt;br/&gt;
@@ -469,7 +473,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
     val socket = connect(protocol = SecurityProtocol.PLAINTEXT)&lt;br/&gt;
     val serializedBytes = producerRequestBytes()&lt;br/&gt;
     // SendAction with throttling in progress&lt;br/&gt;
-    val request = throttledChannelTestSetUp(socket, serializedBytes, NoOpAction, false)&lt;br/&gt;
+    val request = throttledChannelTestSetUp(socket, serializedBytes, true, false)&lt;br/&gt;
 &lt;br/&gt;
     // Since throttling is already done, the channel can be unmuted.&lt;br/&gt;
     TestUtils.waitUntilTrue(() =&amp;gt; openOrClosingChannel(request).exists(c =&amp;gt; c.muteState() == ChannelMuteState.NOT_MUTED), &quot;fail&quot;)&lt;br/&gt;
@@ -675,7 +679,7 @@ class SocketServerTest extends JUnitSuite {&lt;br/&gt;
       // detected. If the buffer is larger than 102400 bytes, a second write is attempted and it fails with an&lt;br/&gt;
       // IOException.&lt;br/&gt;
       val send = new NetworkSend(request.context.connectionId, ByteBuffer.allocate(550000))&lt;br/&gt;
-      channel.sendResponse(new RequestChannel.Response(request, Some(send), SendAction, None))&lt;br/&gt;
+      channel.sendResponse(new RequestChannel.SendResponse(request, send, None, None))&lt;br/&gt;
       TestUtils.waitUntilTrue(() =&amp;gt; totalTimeHistCount() == expectedTotalTimeCount,&lt;br/&gt;
         s&quot;request metrics not updated, expected: $expectedTotalTimeCount, actual: ${totalTimeHistCount()}&quot;)&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala&lt;br/&gt;
index b99bac8dd99..c5275c2239e 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala&lt;br/&gt;
@@ -16,13 +16,22 @@&lt;br/&gt;
  */&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
+import java.net.InetAddress&lt;br/&gt;
+import java.util&lt;br/&gt;
 import java.util.Collections&lt;br/&gt;
 &lt;br/&gt;
-import kafka.network.RequestChannel.{EndThrottlingAction, ResponseAction, Session, StartThrottlingAction}&lt;br/&gt;
+import kafka.network.RequestChannel&lt;br/&gt;
+import kafka.network.RequestChannel.{EndThrottlingResponse, Session, StartThrottlingResponse}&lt;br/&gt;
 import kafka.server.QuotaType._&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.memory.MemoryPool&lt;br/&gt;
 import org.apache.kafka.common.metrics.{MetricConfig, Metrics, Quota}&lt;br/&gt;
-import org.apache.kafka.common.security.auth.KafkaPrincipal&lt;br/&gt;
+import org.apache.kafka.common.network.ListenerName&lt;br/&gt;
+import org.apache.kafka.common.requests.FetchRequest.PartitionData&lt;br/&gt;
+import org.apache.kafka.common.requests.{AbstractRequest, FetchRequest, RequestContext, RequestHeader}&lt;br/&gt;
+import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}
&lt;p&gt; import org.apache.kafka.common.utils.&lt;/p&gt;
{MockTime, Sanitizer}
&lt;p&gt;+import org.easymock.EasyMock&lt;br/&gt;
 import org.junit.Assert.&lt;/p&gt;
{assertEquals, assertTrue}
&lt;p&gt; import org.junit.&lt;/p&gt;
{Before, Test}

&lt;p&gt;@@ -32,11 +41,11 @@ class ClientQuotaManagerTest {&lt;br/&gt;
   private val config = ClientQuotaManagerConfig(quotaBytesPerSecondDefault = 500)&lt;/p&gt;

&lt;p&gt;   var numCallbacks: Int = 0&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def callback (responseAction: ResponseAction) {&lt;br/&gt;
+  def callback (response: RequestChannel.Response) {&lt;br/&gt;
     // Count how many times this callback is called for notifyThrottlingDone().&lt;/li&gt;
	&lt;li&gt;responseAction match {&lt;/li&gt;
	&lt;li&gt;case StartThrottlingAction =&amp;gt;&lt;/li&gt;
	&lt;li&gt;case EndThrottlingAction =&amp;gt; numCallbacks += 1&lt;br/&gt;
+    response match 
{
+      case _: StartThrottlingResponse =&amp;gt;
+      case _: EndThrottlingResponse =&amp;gt; numCallbacks += 1
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -45,15 +54,30 @@ class ClientQuotaManagerTest &lt;/p&gt;
{
     numCallbacks = 0
   }

&lt;p&gt;+  private def buildRequest&lt;span class=&quot;error&quot;&gt;&amp;#91;T &amp;lt;: AbstractRequest&amp;#93;&lt;/span&gt;(builder: AbstractRequest.Builder&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                                 listenerName: ListenerName = ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT)): (T, RequestChannel.Request) = &lt;/p&gt;
{
+
+    val request = builder.build()
+    val buffer = request.serialize(new RequestHeader(builder.apiKey, request.version, &quot;&quot;, 0))
+    val requestChannelMetrics = EasyMock.createNiceMock(classOf[RequestChannel.Metrics])
+
+    // read the header from the buffer first so that the body can be read next from the Request constructor
+    val header = RequestHeader.parse(buffer)
+    val context = new RequestContext(header, &quot;1&quot;, InetAddress.getLocalHost, KafkaPrincipal.ANONYMOUS,
+      listenerName, SecurityProtocol.PLAINTEXT)
+    (request, new RequestChannel.Request(processor = 1, context = context, startTimeNanos =  0, MemoryPool.NONE, buffer,
+      requestChannelMetrics))
+  }&lt;br/&gt;
+&lt;br/&gt;
   private def maybeRecord(quotaManager: ClientQuotaManager, user: String, clientId: String, value: Double): Int = {
     val principal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user)
-    quotaManager.maybeRecordAndGetThrottleTimeMs(Session(principal, null),clientId, value, time.milliseconds())
+    quotaManager.maybeRecordAndGetThrottleTimeMs(Session(principal, null), clientId, value, time.milliseconds())
   }&lt;br/&gt;
 &lt;br/&gt;
   private def throttle(quotaManager: ClientQuotaManager, user: String, clientId: String, throttleTimeMs: Int,&lt;br/&gt;
-                       channelThrottlingCallback: (ResponseAction) =&amp;gt; Unit) {&lt;br/&gt;
-    val principal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user)&lt;br/&gt;
-    quotaManager.throttle(Session(principal, null),clientId, throttleTimeMs, channelThrottlingCallback)&lt;br/&gt;
+                       channelThrottlingCallback: (RequestChannel.Response) =&amp;gt; Unit) {
+    val (_, request) = buildRequest(FetchRequest.Builder.forConsumer(0, 1000, new util.HashMap[TopicPartition, PartitionData]))
+    quotaManager.throttle(request, throttleTimeMs, channelThrottlingCallback)
   }&lt;br/&gt;
 &lt;br/&gt;
   private def testQuotaParsing(config: ClientQuotaManagerConfig, client1: UserClient, client2: UserClient, randomClient: UserClient, defaultConfigClient: UserClient) {&lt;br/&gt;
@@ -364,6 +388,7 @@ class ClientQuotaManagerTest {
       // the sensor should get recreated
       val throttleTimeSensor = metrics.getSensor(&quot;ProduceThrottleTime-:client1&quot;)
       assertTrue(&quot;Throttle time sensor should exist&quot;, throttleTimeSensor != null)
+      assertTrue(&quot;Throttle time sensor should exist&quot;, throttleTimeSensor != null)
     } finally {
       clientMetrics.shutdown()
     }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala b/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala&lt;br/&gt;
index 03137e10dea..424b8c79fe4 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala&lt;br/&gt;
@@ -26,9 +26,8 @@ import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.{ApiKeys, Errors}&lt;br/&gt;
-import org.apache.kafka.common.record.{Record, RecordBatch}&lt;br/&gt;
-import org.apache.kafka.common.requests.{FetchRequest, FetchResponse}&lt;br/&gt;
-import org.apache.kafka.common.requests.{FetchMetadata =&amp;gt; JFetchMetadata}&lt;br/&gt;
+import org.apache.kafka.common.record.{MemoryRecords, Record, RecordBatch, Records}&lt;br/&gt;
+import org.apache.kafka.common.requests.{FetchRequest, FetchResponse, FetchMetadata =&amp;gt; JFetchMetadata}&lt;br/&gt;
 import org.apache.kafka.common.serialization.{ByteArraySerializer, StringSerializer}&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.Test&lt;br/&gt;
@@ -64,7 +63,7 @@ class FetchRequestTest extends BaseRequestTest {
     partitionMap
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def sendFetchRequest(leaderId: Int, request: FetchRequest): FetchResponse = {&lt;br/&gt;
+  private def sendFetchRequest(leaderId: Int, request: FetchRequest): FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryRecords&amp;#93;&lt;/span&gt; = {
     val response = connectAndSend(request, ApiKeys.FETCH, destination = brokerSocketServer(leaderId))
     FetchResponse.parse(response, request.version)
   }&lt;br/&gt;
@@ -219,7 +218,7 @@ class FetchRequestTest extends BaseRequestTest {&lt;br/&gt;
     // batch is not complete, but sent when the producer is closed&lt;br/&gt;
     futures.foreach(_.get)&lt;br/&gt;
 &lt;br/&gt;
-    def fetch(version: Short, maxPartitionBytes: Int, closeAfterPartialResponse: Boolean): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;FetchResponse&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    def fetch(version: Short, maxPartitionBytes: Int, closeAfterPartialResponse: Boolean): Option[FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryRecords&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
       val fetchRequest = FetchRequest.Builder.forConsumer(Int.MaxValue, 0, createPartitionMap(maxPartitionBytes,&lt;br/&gt;
         Seq(topicPartition))).build(version)&lt;br/&gt;
 &lt;br/&gt;
@@ -279,17 +278,32 @@ class FetchRequestTest extends BaseRequestTest {&lt;br/&gt;
     secondBatchFutures.foreach(_.get)&lt;br/&gt;
 &lt;br/&gt;
     def check(fetchOffset: Long, requestVersion: Short, expectedOffset: Long, expectedNumBatches: Int, expectedMagic: Byte): Unit = {&lt;br/&gt;
-      val fetchRequest = FetchRequest.Builder.forConsumer(Int.MaxValue, 0, createPartitionMap(Int.MaxValue,&lt;br/&gt;
-        Seq(topicPartition), Map(topicPartition -&amp;gt; fetchOffset))).build(requestVersion)&lt;br/&gt;
-      val fetchResponse = sendFetchRequest(leaderId, fetchRequest)&lt;br/&gt;
-      val partitionData = fetchResponse.responseData.get(topicPartition)&lt;br/&gt;
-      assertEquals(Errors.NONE, partitionData.error)&lt;br/&gt;
-      assertTrue(partitionData.highWatermark &amp;gt; 0)&lt;br/&gt;
-      val batches = partitionData.records.batches.asScala.toBuffer&lt;br/&gt;
-      assertEquals(expectedNumBatches, batches.size)&lt;br/&gt;
-      val batch = batches.head&lt;br/&gt;
-      assertEquals(expectedMagic, batch.magic)&lt;br/&gt;
-      assertEquals(expectedOffset, batch.baseOffset)&lt;br/&gt;
+      var batchesReceived = 0&lt;br/&gt;
+      var currentFetchOffset = fetchOffset&lt;br/&gt;
+      var currentExpectedOffset = expectedOffset&lt;br/&gt;
+&lt;br/&gt;
+      // With KIP-283, we might not receive all batches in a single fetch request so loop through till we have consumed&lt;br/&gt;
+      // all batches we are interested in.&lt;br/&gt;
+      while (batchesReceived &amp;lt; expectedNumBatches) {
+        val fetchRequest = FetchRequest.Builder.forConsumer(Int.MaxValue, 0, createPartitionMap(Int.MaxValue,
+          Seq(topicPartition), Map(topicPartition -&amp;gt; currentFetchOffset))).build(requestVersion)
+        val fetchResponse = sendFetchRequest(leaderId, fetchRequest)
+
+        // validate response
+        val partitionData = fetchResponse.responseData.get(topicPartition)
+        assertEquals(Errors.NONE, partitionData.error)
+        assertTrue(partitionData.highWatermark &amp;gt; 0)
+        val batches = partitionData.records.batches.asScala.toBuffer
+        val batch = batches.head
+        assertEquals(expectedMagic, batch.magic)
+        assertEquals(currentExpectedOffset, batch.baseOffset)
+
+        currentFetchOffset = batches.last.lastOffset + 1
+        currentExpectedOffset += (batches.last.lastOffset - batches.head.baseOffset + 1)
+        batchesReceived += batches.size
+      }&lt;br/&gt;
+&lt;br/&gt;
+      assertEquals(expectedNumBatches, batchesReceived)&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     // down conversion to message format 0, batches of 1 message are returned so we receive the exact offset we requested&lt;br/&gt;
@@ -317,9 +331,9 @@ class FetchRequestTest extends BaseRequestTest {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Test that when an incremental fetch session contains partitions with an error,&lt;br/&gt;
-    * those partitions are returned in all incremental fetch requests.&lt;br/&gt;
-    */&lt;br/&gt;
+   * Test that when an incremental fetch session contains partitions with an error,&lt;br/&gt;
+   * those partitions are returned in all incremental fetch requests.&lt;br/&gt;
+   */&lt;br/&gt;
   @Test&lt;br/&gt;
   def testCreateIncrementalFetchWithPartitionsInError(): Unit = {&lt;br/&gt;
     def createFetchRequest(topicPartitions: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;,&lt;br/&gt;
@@ -327,9 +341,9 @@ class FetchRequestTest extends BaseRequestTest {&lt;br/&gt;
                            toForget: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;): FetchRequest =&lt;br/&gt;
       FetchRequest.Builder.forConsumer(Int.MaxValue, 0,&lt;br/&gt;
         createPartitionMap(Integer.MAX_VALUE, topicPartitions, Map.empty))&lt;br/&gt;
-          .toForget(toForget.asJava)&lt;br/&gt;
-          .metadata(metadata)&lt;br/&gt;
-          .build()&lt;br/&gt;
+        .toForget(toForget.asJava)&lt;br/&gt;
+        .metadata(metadata)&lt;br/&gt;
+        .build()&lt;br/&gt;
     val foo0 = new TopicPartition(&quot;foo&quot;, 0)&lt;br/&gt;
     val foo1 = new TopicPartition(&quot;foo&quot;, 1)&lt;br/&gt;
     createTopic(&quot;foo&quot;, Map(0 -&amp;gt; List(0, 1), 1 -&amp;gt; List(0, 2)))&lt;br/&gt;
@@ -370,11 +384,11 @@ class FetchRequestTest extends BaseRequestTest {
     assertFalse(resp4.responseData().containsKey(bar0))
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def records(partitionData: FetchResponse.PartitionData): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+  private def records(partitionData: FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryRecords&amp;#93;&lt;/span&gt;): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt; = {
     partitionData.records.records.asScala.toIndexedSeq
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def checkFetchResponse(expectedPartitions: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;, fetchResponse: FetchResponse,&lt;br/&gt;
+  private def checkFetchResponse(expectedPartitions: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;, fetchResponse: FetchResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryRecords&amp;#93;&lt;/span&gt;,&lt;br/&gt;
                                  maxPartitionBytes: Int, maxResponseBytes: Int, numMessagesPerPartition: Int): Unit = {&lt;br/&gt;
     assertEquals(expectedPartitions, fetchResponse.responseData.keySet.asScala.toSeq)&lt;br/&gt;
     var emptyResponseSeen = false&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala b/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala&lt;br/&gt;
index 8264c1b978c..84efa6b684d 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala&lt;br/&gt;
@@ -22,11 +22,12 @@ import java.util.Collections&lt;br/&gt;
 import kafka.utils.MockTime&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
+import org.apache.kafka.common.record.{AbstractRecords, Records}&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchMetadata.{FINAL_EPOCH, INVALID_SESSION_ID}&lt;br/&gt;
 import org.apache.kafka.common.requests.{FetchRequest, FetchResponse, FetchMetadata =&amp;gt; JFetchMetadata}&lt;br/&gt;
-import org.junit.{Rule, Test}&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.rules.Timeout&lt;br/&gt;
+import org.junit.{Rule, Test}&lt;br/&gt;
 &lt;br/&gt;
 class FetchSessionTest {&lt;br/&gt;
   @Rule&lt;br/&gt;
@@ -152,7 +153,7 @@ class FetchSessionTest {&lt;br/&gt;
     })&lt;br/&gt;
     assertEquals(0, context2.getFetchOffset(new TopicPartition(&quot;foo&quot;, 0)).get)&lt;br/&gt;
     assertEquals(10, context2.getFetchOffset(new TopicPartition(&quot;foo&quot;, 1)).get)&lt;br/&gt;
-    val respData2 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    val respData2 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
     respData2.put(new TopicPartition(&quot;foo&quot;, 0), new FetchResponse.PartitionData(&lt;br/&gt;
       Errors.NONE, 100, 100, 100, null, null))&lt;br/&gt;
     respData2.put(new TopicPartition(&quot;foo&quot;, 1), new FetchResponse.PartitionData(&lt;br/&gt;
@@ -211,7 +212,7 @@ class FetchSessionTest {&lt;br/&gt;
         new JFetchMetadata(prevSessionId, FINAL_EPOCH), reqData7, EMPTY_PART_LIST, false)&lt;br/&gt;
       assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;SessionlessFetchContext&amp;#93;&lt;/span&gt;, context7.getClass)&lt;br/&gt;
       assertEquals(0, cache.size())&lt;br/&gt;
-      val respData7 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+      val respData7 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
       respData7.put(new TopicPartition(&quot;bar&quot;, 0),&lt;br/&gt;
         new FetchResponse.PartitionData(Errors.NONE, 100, 100, 100, null, null))&lt;br/&gt;
       respData7.put(new TopicPartition(&quot;bar&quot;, 1),&lt;br/&gt;
@@ -234,7 +235,7 @@ class FetchSessionTest {&lt;br/&gt;
     reqData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100))&lt;br/&gt;
     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData1, EMPTY_PART_LIST, false)&lt;br/&gt;
     assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FullFetchContext&amp;#93;&lt;/span&gt;, context1.getClass)&lt;br/&gt;
-    val respData1 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
     respData1.put(new TopicPartition(&quot;foo&quot;, 0), new FetchResponse.PartitionData(&lt;br/&gt;
       Errors.NONE, 100, 100, 100, null, null))&lt;br/&gt;
     respData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchResponse.PartitionData(&lt;br/&gt;
@@ -261,7 +262,7 @@ class FetchSessionTest {&lt;br/&gt;
     assertEquals(10, context2.getFetchOffset(new TopicPartition(&quot;foo&quot;, 1)).get)&lt;br/&gt;
     assertEquals(15, context2.getFetchOffset(new TopicPartition(&quot;bar&quot;, 0)).get)&lt;br/&gt;
     assertEquals(None, context2.getFetchOffset(new TopicPartition(&quot;bar&quot;, 2)))&lt;br/&gt;
-    val respData2 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    val respData2 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
     respData2.put(new TopicPartition(&quot;foo&quot;, 1), new FetchResponse.PartitionData(&lt;br/&gt;
       Errors.NONE, 10, 10, 10, null, null))&lt;br/&gt;
     respData2.put(new TopicPartition(&quot;bar&quot;, 0), new FetchResponse.PartitionData(&lt;br/&gt;
@@ -284,7 +285,7 @@ class FetchSessionTest {&lt;br/&gt;
     reqData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100))&lt;br/&gt;
     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData1, EMPTY_PART_LIST, false)&lt;br/&gt;
     assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FullFetchContext&amp;#93;&lt;/span&gt;, context1.getClass)&lt;br/&gt;
-    val respData1 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
     respData1.put(new TopicPartition(&quot;foo&quot;, 0), new FetchResponse.PartitionData(&lt;br/&gt;
       Errors.NONE, 100, 100, 100, null, null))&lt;br/&gt;
     respData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchResponse.PartitionData(&lt;br/&gt;
@@ -303,7 +304,7 @@ class FetchSessionTest {&lt;br/&gt;
     val context2 = fetchManager.newContext(&lt;br/&gt;
       new JFetchMetadata(resp1.sessionId(), 1), reqData2, removed2, false)&lt;br/&gt;
     assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;SessionlessFetchContext&amp;#93;&lt;/span&gt;, context2.getClass)&lt;br/&gt;
-    val respData2 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchResponse.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    val respData2 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
     val resp2 = context2.updateAndGenerateResponseData(respData2)&lt;br/&gt;
     assertEquals(INVALID_SESSION_ID, resp2.sessionId())&lt;br/&gt;
     assertTrue(resp2.responseData().isEmpty)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
index 1d6092a60a1..d88001166f2 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
@@ -29,6 +29,7 @@ import kafka.coordinator.group.GroupCoordinator&lt;br/&gt;
 import kafka.coordinator.transaction.TransactionCoordinator&lt;br/&gt;
 import kafka.log.{Log, TimestampOffset}&lt;br/&gt;
 import kafka.network.RequestChannel&lt;br/&gt;
+import kafka.network.RequestChannel.SendResponse&lt;br/&gt;
 import kafka.security.auth.Authorizer&lt;br/&gt;
 import kafka.server.QuotaFactory.QuotaManagers&lt;br/&gt;
 import kafka.utils.{MockTime, TestUtils}&lt;br/&gt;
@@ -542,7 +543,10 @@ class KafkaApisTest {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def readResponse(api: ApiKeys, request: AbstractRequest, capturedResponse: Capture&lt;span class=&quot;error&quot;&gt;&amp;#91;RequestChannel.Response&amp;#93;&lt;/span&gt;): AbstractResponse = {&lt;br/&gt;
-    val send = capturedResponse.getValue.responseSend.get&lt;br/&gt;
+    val response = capturedResponse.getValue&lt;br/&gt;
+    assertTrue(s&quot;Unexpected response type: ${response.getClass}&quot;, response.isInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;SendResponse&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val sendResponse = response.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;SendResponse&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    val send = sendResponse.responseSend&lt;br/&gt;
     val channel = new ByteBufferChannel(send.size)&lt;br/&gt;
     send.writeTo(channel)&lt;br/&gt;
     channel.close()&lt;br/&gt;
@@ -556,7 +560,7 @@ class KafkaApisTest {&lt;br/&gt;
     EasyMock.expect(clientRequestQuotaManager.maybeRecordAndGetThrottleTimeMs(EasyMock.anyObject&lt;span class=&quot;error&quot;&gt;&amp;#91;RequestChannel.Request&amp;#93;&lt;/span&gt;()))&lt;br/&gt;
       .andReturn(0)&lt;br/&gt;
     EasyMock.expect(clientRequestQuotaManager.throttle(EasyMock.anyObject&lt;span class=&quot;error&quot;&gt;&amp;#91;RequestChannel.Request&amp;#93;&lt;/span&gt;(), EasyMock.eq(0),&lt;br/&gt;
-      EasyMock.anyObject&lt;span class=&quot;error&quot;&gt;&amp;#91;RequestChannel.ResponseAction =&amp;gt; Unit&amp;#93;&lt;/span&gt;()))&lt;br/&gt;
+      EasyMock.anyObject&lt;span class=&quot;error&quot;&gt;&amp;#91;RequestChannel.Response =&amp;gt; Unit&amp;#93;&lt;/span&gt;()))&lt;br/&gt;
 &lt;br/&gt;
     val capturedResponse = EasyMock.newCapture&lt;span class=&quot;error&quot;&gt;&amp;#91;RequestChannel.Response&amp;#93;&lt;/span&gt;()&lt;br/&gt;
     EasyMock.expect(requestChannel.sendResponse(EasyMock.capture(capturedResponse)))&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
index dcbeb21ccf6..29a1c9f0697 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
@@ -18,13 +18,13 @@ package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 import kafka.api.Request&lt;br/&gt;
-import kafka.cluster.{BrokerEndPoint, Replica, Partition}&lt;br/&gt;
+import kafka.cluster.{BrokerEndPoint, Partition, Replica}&lt;br/&gt;
 import kafka.log.LogManager&lt;br/&gt;
 import kafka.server.AbstractFetcherThread.ResultWithPartitions&lt;br/&gt;
 import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
-import org.apache.kafka.common.errors.{ReplicaNotAvailableException, KafkaStorageException}&lt;br/&gt;
 import kafka.utils.{DelayedItem, TestUtils}&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.errors.{KafkaStorageException, ReplicaNotAvailableException}&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset.{UNDEFINED_EPOCH_OFFSET, UNDEFINED_EPOCH}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
index e62c9fd7d18..3f2f66c0890 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
@@ -413,7 +413,7 @@ class RequestQuotaTest extends BaseRequestTest {&lt;br/&gt;
   private def responseThrottleTime(apiKey: ApiKeys, response: Struct): Int = {&lt;br/&gt;
     apiKey match {&lt;br/&gt;
       case ApiKeys.PRODUCE =&amp;gt; new ProduceResponse(response).throttleTimeMs&lt;br/&gt;
-      case ApiKeys.FETCH =&amp;gt; new FetchResponse(response).throttleTimeMs&lt;br/&gt;
+      case ApiKeys.FETCH =&amp;gt; FetchResponse.parse(response).throttleTimeMs&lt;br/&gt;
       case ApiKeys.LIST_OFFSETS =&amp;gt; new ListOffsetResponse(response).throttleTimeMs&lt;br/&gt;
       case ApiKeys.METADATA =&amp;gt; new MetadataResponse(response).throttleTimeMs&lt;br/&gt;
       case ApiKeys.OFFSET_COMMIT =&amp;gt; new OffsetCommitResponse(response).throttleTimeMs&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ThrottledChannelExpirationTest.scala b/core/src/test/scala/unit/kafka/server/ThrottledChannelExpirationTest.scala&lt;br/&gt;
index 8ba584cebe1..ff781a2e159 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ThrottledChannelExpirationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ThrottledChannelExpirationTest.scala&lt;br/&gt;
@@ -18,12 +18,22 @@&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
+import java.net.InetAddress&lt;br/&gt;
+import java.util&lt;br/&gt;
 import java.util.Collections&lt;br/&gt;
 import java.util.concurrent.{DelayQueue, TimeUnit}&lt;br/&gt;
 &lt;br/&gt;
-import kafka.network.RequestChannel.{EndThrottlingAction, ResponseAction, StartThrottlingAction}&lt;br/&gt;
+import kafka.network.RequestChannel&lt;br/&gt;
+import kafka.network.RequestChannel.{EndThrottlingResponse, Response, StartThrottlingResponse}&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.memory.MemoryPool&lt;br/&gt;
 import org.apache.kafka.common.metrics.MetricConfig&lt;br/&gt;
+import org.apache.kafka.common.network.ListenerName&lt;br/&gt;
+import org.apache.kafka.common.requests.FetchRequest.PartitionData&lt;br/&gt;
+import org.apache.kafka.common.requests.{AbstractRequest, FetchRequest, RequestContext, RequestHeader}&lt;br/&gt;
+import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime&lt;br/&gt;
+import org.easymock.EasyMock&lt;br/&gt;
 import org.junit.{Assert, Before, Test}&lt;br/&gt;
 &lt;br/&gt;
 class ThrottledChannelExpirationTest {&lt;br/&gt;
@@ -33,11 +43,27 @@ class ThrottledChannelExpirationTest {&lt;br/&gt;
   private val metrics = new org.apache.kafka.common.metrics.Metrics(new MetricConfig(),&lt;br/&gt;
                                                                     Collections.emptyList(),&lt;br/&gt;
                                                                     time)&lt;br/&gt;
+  private val request = buildRequest(FetchRequest.Builder.forConsumer(0, 1000, new util.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionData&amp;#93;&lt;/span&gt;))._2&lt;br/&gt;
 &lt;br/&gt;
-  def callback(responseAction: ResponseAction): Unit = {&lt;br/&gt;
-    responseAction match {&lt;br/&gt;
-      case StartThrottlingAction =&amp;gt; numCallbacksForStartThrottling += 1&lt;br/&gt;
-      case EndThrottlingAction =&amp;gt; numCallbacksForEndThrottling += 1&lt;br/&gt;
+  private def buildRequest&lt;span class=&quot;error&quot;&gt;&amp;#91;T &amp;lt;: AbstractRequest&amp;#93;&lt;/span&gt;(builder: AbstractRequest.Builder&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                                 listenerName: ListenerName = ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT)): (T, RequestChannel.Request) = {++    val request = builder.build()+    val buffer = request.serialize(new RequestHeader(builder.apiKey, request.version, &quot;&quot;, 0))+    val requestChannelMetrics = EasyMock.createNiceMock(classOf[RequestChannel.Metrics])++    // read the header from the buffer first so that the body can be read next from the Request constructor+    val header = RequestHeader.parse(buffer)+    val context = new RequestContext(header, &quot;1&quot;, InetAddress.getLocalHost, KafkaPrincipal.ANONYMOUS,+      listenerName, SecurityProtocol.PLAINTEXT)+    (request, new RequestChannel.Request(processor = 1, context = context, startTimeNanos =  0, MemoryPool.NONE, buffer,+      requestChannelMetrics))+  }
&lt;p&gt;+&lt;br/&gt;
+  def callback(response: Response): Unit = {&lt;br/&gt;
+    response match &lt;/p&gt;
{
+      case _: StartThrottlingResponse =&amp;gt; numCallbacksForStartThrottling += 1
+      case _: EndThrottlingResponse =&amp;gt; numCallbacksForEndThrottling += 1
     }
&lt;p&gt;   }&lt;/p&gt;

&lt;p&gt;@@ -55,10 +81,10 @@ class ThrottledChannelExpirationTest {&lt;br/&gt;
     val reaper = new clientMetrics.ThrottledChannelReaper(delayQueue, &quot;&quot;)&lt;br/&gt;
     try {&lt;br/&gt;
       // Add 4 elements to the queue out of order. Add 2 elements with the same expire timestamp.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val channel1 = new ThrottledChannel(time, 10, callback)&lt;/li&gt;
	&lt;li&gt;val channel2 = new ThrottledChannel(time, 30, callback)&lt;/li&gt;
	&lt;li&gt;val channel3 = new ThrottledChannel(time, 30, callback)&lt;/li&gt;
	&lt;li&gt;val channel4 = new ThrottledChannel(time, 20, callback)&lt;br/&gt;
+      val channel1 = new ThrottledChannel(request, time, 10, callback)&lt;br/&gt;
+      val channel2 = new ThrottledChannel(request, time, 30, callback)&lt;br/&gt;
+      val channel3 = new ThrottledChannel(request, time, 30, callback)&lt;br/&gt;
+      val channel4 = new ThrottledChannel(request, time, 20, callback)&lt;br/&gt;
       delayQueue.add(channel1)&lt;br/&gt;
       delayQueue.add(channel2)&lt;br/&gt;
       delayQueue.add(channel3)&lt;br/&gt;
@@ -82,9 +108,9 @@ class ThrottledChannelExpirationTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
   def testThrottledChannelDelay() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val t1: ThrottledChannel = new ThrottledChannel(time, 10, callback)&lt;/li&gt;
	&lt;li&gt;val t2: ThrottledChannel = new ThrottledChannel(time, 20, callback)&lt;/li&gt;
	&lt;li&gt;val t3: ThrottledChannel = new ThrottledChannel(time, 20, callback)&lt;br/&gt;
+    val t1: ThrottledChannel = new ThrottledChannel(request, time, 10, callback)&lt;br/&gt;
+    val t2: ThrottledChannel = new ThrottledChannel(request, time, 20, callback)&lt;br/&gt;
+    val t3: ThrottledChannel = new ThrottledChannel(request, time, 20, callback)&lt;br/&gt;
     Assert.assertEquals(10, t1.throttleTimeMs)&lt;br/&gt;
     Assert.assertEquals(20, t2.throttleTimeMs)&lt;br/&gt;
     Assert.assertEquals(20, t3.throttleTimeMs)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala b/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala&lt;br/&gt;
index 50a4d74f00d..b7c037ef86b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala&lt;br/&gt;
@@ -19,12 +19,12 @@ package kafka.server.epoch.util&lt;br/&gt;
 import kafka.cluster.BrokerEndPoint&lt;br/&gt;
 import kafka.server.BlockingSend&lt;br/&gt;
 import org.apache.kafka.clients.
{ClientRequest, ClientResponse, MockClient}
&lt;p&gt;-import org.apache.kafka.common.&lt;/p&gt;
{Node, TopicPartition}&lt;br/&gt;
 import org.apache.kafka.common.protocol.{ApiKeys, Errors}&lt;br/&gt;
+import org.apache.kafka.common.record.Records&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest.Builder&lt;br/&gt;
-import org.apache.kafka.common.requests.FetchResponse.PartitionData&lt;br/&gt;
 import org.apache.kafka.common.requests.{AbstractRequest, EpochEndOffset, FetchResponse, OffsetsForLeaderEpochResponse, FetchMetadata =&amp;gt; JFetchMetadata}&lt;br/&gt;
 import org.apache.kafka.common.utils.{SystemTime, Time}&lt;br/&gt;
+import org.apache.kafka.common.{Node, TopicPartition}&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Stub network client used for testing the ReplicaFetcher, wraps the MockClient used for consumer testing&lt;br/&gt;
@@ -66,7 +66,7 @@ class ReplicaFetcherMockBlockingSend(offsets: java.util.Map[TopicPartition, Epoc&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       case ApiKeys.FETCH =&amp;gt;&lt;br/&gt;
         fetchCount += 1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new FetchResponse(Errors.NONE, new java.util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionData&amp;#93;&lt;/span&gt;, 0,&lt;br/&gt;
+        new FetchResponse(Errors.NONE, new java.util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;], 0,&lt;br/&gt;
           JFetchMetadata.INVALID_SESSION_ID)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       case _ =&amp;gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                            <subtask id="13165143">KAFKA-7030</subtask>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 24 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3tyvb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>hachikuji</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>