<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:11:33 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6981] Missing Connector Config (errors.deadletterqueue.topic.name) kills Connect Clusters</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6981</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The trunk version of AK currently tries to incorrectly read the property&#160;(errors.deadletterqueue.topic.name) when starting a sink connector. This fails no matter what the contents of the connector config are. The&#160;ConnectorConfig does not define this property, and any calls to getString will throw a ConfigException (since&#160;only&#160;known properties are retained by AbstractConfig).&#160;&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.kafka.common.config.ConfigException: Unknown configuration &apos;errors.deadletterqueue.topic.name&apos;
    at org.apache.kafka.common.config.AbstractConfig.get(AbstractConfig.java:91)
    at org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig.get(ConnectorConfig.java:117)
    at org.apache.kafka.connect.runtime.ConnectorConfig.get(ConnectorConfig.java:162)
    at org.apache.kafka.common.config.AbstractConfig.getString(AbstractConfig.java:126)
    at org.apache.kafka.connect.runtime.Worker.sinkTaskReporters(Worker.java:531)
    at org.apache.kafka.connect.runtime.Worker.buildWorkerTask(Worker.java:508)
    at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:451)
    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:873)
    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.access$1600(DistributedHerder.java:111)
    at org.apache.kafka.connect.runtime.distributed.DistributedHerder$13.call(DistributedHerder.java:888)
    at org.apache.kafka.connect.runtime.distributed.DistributedHerder$13.call(DistributedHerder.java:884)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is related to the changes introduced in the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-298%3A+Error+Handling+in+Connect&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KIP-298 Error Handling&lt;/a&gt; feature.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13163572">KAFKA-6981</key>
            <summary>Missing Connector Config (errors.deadletterqueue.topic.name) kills Connect Clusters</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="wicknicks">Arjun Satish</assignee>
                                    <reporter username="wicknicks">Arjun Satish</reporter>
                        <labels>
                    </labels>
                <created>Fri, 1 Jun 2018 23:47:27 +0000</created>
                <updated>Wed, 6 Jun 2018 13:56:46 +0000</updated>
                            <resolved>Tue, 5 Jun 2018 20:59:41 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16498841" author="githubbot" created="Sat, 2 Jun 2018 03:07:33 +0000"  >&lt;p&gt;wicknicks opened a new pull request #5125:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6981&quot; title=&quot;Missing Connector Config (errors.deadletterqueue.topic.name) kills Connect Clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6981&quot;&gt;&lt;del&gt;KAFKA-6981&lt;/del&gt;&lt;/a&gt;: Move the error handling configuration properties into the ConnectorConfig and SinkConnectorConfig classes&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5125&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5125&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   *More detailed description of your change,&lt;br/&gt;
   if necessary. The PR title and PR message become&lt;br/&gt;
   the squashed commit message, so use a separate&lt;br/&gt;
   comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   for the feature or bug fix. Unit and/or integration&lt;br/&gt;
   tests are expected for any behaviour change and&lt;br/&gt;
   system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16502496" author="ewencp" created="Tue, 5 Jun 2018 20:59:41 +0000"  >&lt;p&gt;Issue resolved by pull request 5125&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/5125&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5125&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16502498" author="githubbot" created="Tue, 5 Jun 2018 21:01:08 +0000"  >&lt;p&gt;ewencp closed pull request #5125:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6981&quot; title=&quot;Missing Connector Config (errors.deadletterqueue.topic.name) kills Connect Clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6981&quot;&gt;&lt;del&gt;KAFKA-6981&lt;/del&gt;&lt;/a&gt;: Move the error handling configuration properties into the ConnectorConfig and SinkConnectorConfig classes&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5125&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5125&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java&lt;br/&gt;
index c54c160d5ab..f98469e5b5a 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java&lt;br/&gt;
@@ -24,6 +24,7 @@&lt;br/&gt;
 import org.apache.kafka.common.config.ConfigException;&lt;br/&gt;
 import org.apache.kafka.connect.connector.ConnectRecord;&lt;br/&gt;
 import org.apache.kafka.connect.errors.ConnectException;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.errors.ToleranceType;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.isolation.PluginDesc;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.isolation.Plugins;&lt;br/&gt;
 import org.apache.kafka.connect.transforms.Transformation;&lt;br/&gt;
@@ -36,8 +37,8 @@&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;/p&gt;

&lt;p&gt;-import static org.apache.kafka.common.config.ConfigDef.Range.atLeast;&lt;br/&gt;
 import static org.apache.kafka.common.config.ConfigDef.NonEmptyStringWithoutControlChars.nonEmptyStringWithoutControlChars;&lt;br/&gt;
+import static org.apache.kafka.common.config.ConfigDef.Range.atLeast;&lt;br/&gt;
 import static org.apache.kafka.common.config.ConfigDef.ValidString.in;&lt;/p&gt;

&lt;p&gt; /**&lt;br/&gt;
@@ -54,6 +55,7 @@&lt;br/&gt;
 public class ConnectorConfig extends AbstractConfig {&lt;br/&gt;
     protected static final String COMMON_GROUP = &quot;Common&quot;;&lt;br/&gt;
     protected static final String TRANSFORMS_GROUP = &quot;Transforms&quot;;&lt;br/&gt;
+    protected static final String ERROR_GROUP = &quot;Error Handling&quot;;&lt;/p&gt;

&lt;p&gt;     public static final String NAME_CONFIG = &quot;name&quot;;&lt;br/&gt;
     private static final String NAME_DOC = &quot;Globally unique name to use for this connector.&quot;;&lt;br/&gt;
@@ -106,6 +108,37 @@&lt;br/&gt;
     public static final String CONFIG_RELOAD_ACTION_NONE = Herder.ConfigReloadAction.NONE.toString();&lt;br/&gt;
     public static final String CONFIG_RELOAD_ACTION_RESTART = Herder.ConfigReloadAction.RESTART.toString();&lt;/p&gt;

&lt;p&gt;+    public static final String ERRORS_RETRY_TIMEOUT_CONFIG = &quot;errors.retry.timeout&quot;;&lt;br/&gt;
+    public static final String ERRORS_RETRY_TIMEOUT_DISPLAY = &quot;Retry Timeout for Errors&quot;;&lt;br/&gt;
+    public static final int ERRORS_RETRY_TIMEOUT_DEFAULT = 0;&lt;br/&gt;
+    public static final String ERRORS_RETRY_TIMEOUT_DOC = &quot;The maximum duration in milliseconds that a failed operation &quot; +&lt;br/&gt;
+            &quot;will be reattempted. The default is 0, which means no retries will be attempted. Use -1 for infinite retries.&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    public static final String ERRORS_RETRY_MAX_DELAY_CONFIG = &quot;errors.retry.delay.max.ms&quot;;&lt;br/&gt;
+    public static final String ERRORS_RETRY_MAX_DELAY_DISPLAY = &quot;Maximum Delay Between Retries for Errors&quot;;&lt;br/&gt;
+    public static final int ERRORS_RETRY_MAX_DELAY_DEFAULT = 60000;&lt;br/&gt;
+    public static final String ERRORS_RETRY_MAX_DELAY_DOC = &quot;The maximum duration in milliseconds between consecutive retry attempts. &quot; +&lt;br/&gt;
+            &quot;Jitter will be added to the delay once this limit is reached to prevent thundering herd issues.&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    public static final String ERRORS_TOLERANCE_CONFIG = &quot;errors.allowed.max&quot;;&lt;br/&gt;
+    public static final String ERRORS_TOLERANCE_DISPLAY = &quot;Error Tolerance&quot;;&lt;br/&gt;
+    public static final ToleranceType ERRORS_TOLERANCE_DEFAULT = ToleranceType.NONE;&lt;br/&gt;
+    public static final String ERRORS_TOLERANCE_DOC = &quot;Behavior for tolerating errors during connector operation. &apos;none&apos; is the default value &quot; +&lt;br/&gt;
+            &quot;and signals that any error will result in an immediate connector task failure; &apos;all&apos; changes the behavior to skip over problematic records.&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    public static final String ERRORS_LOG_ENABLE_CONFIG = &quot;errors.log.enable&quot;;&lt;br/&gt;
+    public static final String ERRORS_LOG_ENABLE_DISPLAY = &quot;Log Errors&quot;;&lt;br/&gt;
+    public static final boolean ERRORS_LOG_ENABLE_DEFAULT = false;&lt;br/&gt;
+    public static final String ERRORS_LOG_ENABLE_DOC = &quot;If true, write each error and the details of the failed operation and problematic record &quot; +&lt;br/&gt;
+            &quot;to the Connect application log. This is &apos;false&apos; by default, so that only errors that are not tolerated are reported.&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    public static final String ERRORS_LOG_INCLUDE_MESSAGES_CONFIG = &quot;errors.log.include.messages&quot;;&lt;br/&gt;
+    public static final String ERRORS_LOG_INCLUDE_MESSAGES_DISPLAY = &quot;Log Error Details&quot;;&lt;br/&gt;
+    public static final boolean ERRORS_LOG_INCLUDE_MESSAGES_DEFAULT = false;&lt;br/&gt;
+    public static final String ERRORS_LOG_INCLUDE_MESSAGES_DOC = &quot;Whether to the include in the log the Connect record that resulted in &quot; +&lt;br/&gt;
+            &quot;a failure. This is &apos;false&apos; by default, which will prevent record keys, values, and headers from being written to log files, &quot; +&lt;br/&gt;
+            &quot;although some information such as topic and partition number will still be logged.&quot;;&lt;br/&gt;
+&lt;br/&gt;
     private final EnrichedConnectorConfig enrichedConfig;&lt;br/&gt;
     private static class EnrichedConnectorConfig extends AbstractConfig {&lt;br/&gt;
         EnrichedConnectorConfig(ConfigDef configDef, Map&amp;lt;String, String&amp;gt; props) {&lt;br/&gt;
@@ -120,6 +153,7 @@ public Object get(String key) {&lt;/p&gt;

&lt;p&gt;     public static ConfigDef configDef() {&lt;br/&gt;
         int orderInGroup = 0;&lt;br/&gt;
+        int orderInErrorGroup = 0;&lt;br/&gt;
         return new ConfigDef()&lt;br/&gt;
                 .define(NAME_CONFIG, Type.STRING, ConfigDef.NO_DEFAULT_VALUE, nonEmptyStringWithoutControlChars(), Importance.HIGH, NAME_DOC, COMMON_GROUP, ++orderInGroup, Width.MEDIUM, NAME_DISPLAY)&lt;br/&gt;
                 .define(CONNECTOR_CLASS_CONFIG, Type.STRING, Importance.HIGH, CONNECTOR_CLASS_DOC, COMMON_GROUP, ++orderInGroup, Width.LONG, CONNECTOR_CLASS_DISPLAY)&lt;br/&gt;
@@ -138,7 +172,18 @@ public void ensureValid(String name, Object value) {&lt;br/&gt;
                 }), Importance.LOW, TRANSFORMS_DOC, TRANSFORMS_GROUP, ++orderInGroup, Width.LONG, TRANSFORMS_DISPLAY)&lt;br/&gt;
                 .define(CONFIG_RELOAD_ACTION_CONFIG, Type.STRING, CONFIG_RELOAD_ACTION_RESTART,&lt;br/&gt;
                         in(CONFIG_RELOAD_ACTION_NONE, CONFIG_RELOAD_ACTION_RESTART), Importance.LOW,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;CONFIG_RELOAD_ACTION_DOC, COMMON_GROUP, ++orderInGroup, Width.MEDIUM, CONFIG_RELOAD_ACTION_DISPLAY);&lt;br/&gt;
+                        CONFIG_RELOAD_ACTION_DOC, COMMON_GROUP, ++orderInGroup, Width.MEDIUM, CONFIG_RELOAD_ACTION_DISPLAY)&lt;br/&gt;
+                .define(ERRORS_RETRY_TIMEOUT_CONFIG, Type.LONG, ERRORS_RETRY_TIMEOUT_DEFAULT, Importance.MEDIUM,&lt;br/&gt;
+                        ERRORS_RETRY_TIMEOUT_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.MEDIUM, ERRORS_RETRY_TIMEOUT_DISPLAY)&lt;br/&gt;
+                .define(ERRORS_RETRY_MAX_DELAY_CONFIG, Type.LONG, ERRORS_RETRY_MAX_DELAY_DEFAULT, Importance.MEDIUM,&lt;br/&gt;
+                        ERRORS_RETRY_MAX_DELAY_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.MEDIUM, ERRORS_RETRY_MAX_DELAY_DISPLAY)&lt;br/&gt;
+                .define(ERRORS_TOLERANCE_CONFIG, Type.STRING, ERRORS_TOLERANCE_DEFAULT.value(),&lt;br/&gt;
+                        in(ToleranceType.NONE.value(), ToleranceType.ALL.value()), Importance.MEDIUM,&lt;br/&gt;
+                        ERRORS_TOLERANCE_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.SHORT, ERRORS_TOLERANCE_DISPLAY)&lt;br/&gt;
+                .define(ERRORS_LOG_ENABLE_CONFIG, Type.BOOLEAN, ERRORS_LOG_ENABLE_DEFAULT, Importance.MEDIUM,&lt;br/&gt;
+                        ERRORS_LOG_ENABLE_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.SHORT, ERRORS_LOG_ENABLE_DISPLAY)&lt;br/&gt;
+                .define(ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, Type.BOOLEAN, ERRORS_LOG_INCLUDE_MESSAGES_DEFAULT, Importance.MEDIUM,&lt;br/&gt;
+                        ERRORS_LOG_INCLUDE_MESSAGES_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.SHORT, ERRORS_LOG_INCLUDE_MESSAGES_DISPLAY);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public ConnectorConfig(Plugins plugins) {&lt;br/&gt;
@@ -162,6 +207,32 @@ public Object get(String key) &lt;/p&gt;
{
         return enrichedConfig.get(key);
     }

&lt;p&gt;+    public long errorRetryTimeout() &lt;/p&gt;
{
+        return getLong(ERRORS_RETRY_TIMEOUT_CONFIG);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public long errorMaxDelayInMillis() &lt;/p&gt;
{
+        return getLong(ERRORS_RETRY_MAX_DELAY_CONFIG);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public ToleranceType errorToleranceType() {&lt;br/&gt;
+        String tolerance = getString(ERRORS_TOLERANCE_CONFIG);&lt;br/&gt;
+        for (ToleranceType type: ToleranceType.values()) {&lt;br/&gt;
+            if (type.name().equalsIgnoreCase(tolerance)) &lt;/p&gt;
{
+                return type;
+            }
&lt;p&gt;+        }&lt;br/&gt;
+        return ERRORS_TOLERANCE_DEFAULT;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public boolean enableErrorLog() &lt;/p&gt;
{
+        return getBoolean(ERRORS_LOG_ENABLE_CONFIG);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public boolean includeRecordDetailsInErrorLog() &lt;/p&gt;
{
+        return getBoolean(ERRORS_LOG_INCLUDE_MESSAGES_CONFIG);
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Returns the initialized list of 
{@link Transformation}
&lt;p&gt; which are specified in &lt;/p&gt;
{@link #TRANSFORMS_CONFIG}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java&lt;br/&gt;
index 887a4da2dea..9629f8f0e42 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java&lt;br/&gt;
@@ -17,6 +17,7 @@&lt;br/&gt;
 package org.apache.kafka.connect.runtime;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.config.ConfigDef;&lt;br/&gt;
+import org.apache.kafka.common.config.ConfigDef.Importance;&lt;br/&gt;
 import org.apache.kafka.common.config.ConfigException;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.isolation.Plugins;&lt;br/&gt;
 import org.apache.kafka.connect.sink.SinkTask;&lt;br/&gt;
@@ -42,9 +43,19 @@&lt;br/&gt;
     public static final String TOPICS_REGEX_DEFAULT = &quot;&quot;;&lt;br/&gt;
     private static final String TOPICS_REGEX_DISPLAY = &quot;Topics regex&quot;;&lt;/p&gt;

&lt;p&gt;+    public static final String DLQ_PREFIX = &quot;errors.deadletterqueue.&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    public static final String DLQ_TOPIC_NAME_CONFIG = DLQ_PREFIX + &quot;topic.name&quot;;&lt;br/&gt;
+    public static final String DLQ_TOPIC_NAME_DOC = &quot;The name of the topic to be used as the dead letter queue (DLQ) for messages that &quot; +&lt;br/&gt;
+        &quot;result in an error when processed by this sink connector, or its transformations or converters. The topic name is blank by default, &quot; +&lt;br/&gt;
+        &quot;which means that no messages are to be recorded in the DLQ.&quot;;&lt;br/&gt;
+    public static final String DLQ_TOPIC_DEFAULT = &quot;&quot;;&lt;br/&gt;
+    private static final String DLQ_TOPIC_DISPLAY = &quot;Dead Letter Queue Topic Name&quot;;&lt;br/&gt;
+&lt;br/&gt;
     static ConfigDef config = ConnectorConfig.configDef()&lt;br/&gt;
         .define(TOPICS_CONFIG, ConfigDef.Type.LIST, TOPICS_DEFAULT, ConfigDef.Importance.HIGH, TOPICS_DOC, COMMON_GROUP, 4, ConfigDef.Width.LONG, TOPICS_DISPLAY)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.define(TOPICS_REGEX_CONFIG, ConfigDef.Type.STRING, TOPICS_REGEX_DEFAULT, new RegexValidator(), ConfigDef.Importance.HIGH, TOPICS_REGEX_DOC, COMMON_GROUP, 4, ConfigDef.Width.LONG, TOPICS_REGEX_DISPLAY);&lt;br/&gt;
+        .define(TOPICS_REGEX_CONFIG, ConfigDef.Type.STRING, TOPICS_REGEX_DEFAULT, new RegexValidator(), ConfigDef.Importance.HIGH, TOPICS_REGEX_DOC, COMMON_GROUP, 4, ConfigDef.Width.LONG, TOPICS_REGEX_DISPLAY)&lt;br/&gt;
+        .define(DLQ_TOPIC_NAME_CONFIG, ConfigDef.Type.STRING, DLQ_TOPIC_DEFAULT, Importance.MEDIUM, DLQ_TOPIC_NAME_DOC, ERROR_GROUP, 6, ConfigDef.Width.MEDIUM, DLQ_TOPIC_DISPLAY);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static ConfigDef configDef() {&lt;br/&gt;
         return config;&lt;br/&gt;
@@ -83,4 +94,7 @@ public static boolean hasTopicsRegexConfig(Map&amp;lt;String, String&amp;gt; props) &lt;/p&gt;
{
         return topicsRegexStr != null &amp;amp;&amp;amp; !topicsRegexStr.trim().isEmpty();
     }

&lt;p&gt;+    public String dlqTopicName() &lt;/p&gt;
{
+        return getString(DLQ_TOPIC_NAME_CONFIG);
+    }
&lt;p&gt; }&lt;br/&gt;
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java&lt;br/&gt;
index 7a72a0e7b26..97e68faa4ca 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java&lt;br/&gt;
@@ -485,8 +485,8 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,&lt;br/&gt;
                                        ClassLoader loader) &lt;/p&gt;
{
         ErrorHandlingMetrics errorHandlingMetrics = errorHandlingMetrics(id);
 
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator();
-        retryWithToleranceOperator.configure(connConfig.originalsWithPrefix(&quot;errors.&quot;));
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(connConfig.errorRetryTimeout(),
+                connConfig.errorMaxDelayInMillis(), connConfig.errorToleranceType(), Time.SYSTEM);
         retryWithToleranceOperator.metrics(errorHandlingMetrics);
 
         // Decide which type of worker task we need based on the type of task.
@@ -505,7 +505,8 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,
                     time, retryWithToleranceOperator);
         }
&lt;p&gt; else if (task instanceof SinkTask) {&lt;br/&gt;
             TransformationChain&amp;lt;SinkRecord&amp;gt; transformationChain = new TransformationChain&amp;lt;&amp;gt;(connConfig.&amp;lt;SinkRecord&amp;gt;transformations(), retryWithToleranceOperator);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;retryWithToleranceOperator.reporters(sinkTaskReporters(id, connConfig, errorHandlingMetrics));&lt;br/&gt;
+            SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());&lt;br/&gt;
+            retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics));&lt;br/&gt;
             return new WorkerSinkTask(id, (SinkTask) task, statusListener, initialState, config, configState, metrics, keyConverter,&lt;br/&gt;
                     valueConverter, headerConverter, transformationChain, loader, time,&lt;br/&gt;
                     retryWithToleranceOperator);&lt;br/&gt;
@@ -519,19 +520,17 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) 
{
         return new ErrorHandlingMetrics(id, metrics);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private List&amp;lt;ErrorReporter&amp;gt; sinkTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,&lt;/li&gt;
	&lt;li&gt;ErrorHandlingMetrics errorHandlingMetrics) {&lt;br/&gt;
+    private List&amp;lt;ErrorReporter&amp;gt; sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,&lt;br/&gt;
+                                                  ErrorHandlingMetrics errorHandlingMetrics) {&lt;br/&gt;
         ArrayList&amp;lt;ErrorReporter&amp;gt; reporters = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;LogReporter logReporter = new LogReporter(id);&lt;/li&gt;
	&lt;li&gt;logReporter.configure(connConfig.originalsWithPrefix(LogReporter.PREFIX + &quot;.&quot;));&lt;br/&gt;
+        LogReporter logReporter = new LogReporter(id, connConfig);&lt;br/&gt;
         logReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
         reporters.add(logReporter);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // check if topic for dead letter queue exists&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String topic = connConfig.getString(DeadLetterQueueReporter.PREFIX + &quot;.&quot; + DeadLetterQueueReporter.DLQ_TOPIC_NAME);&lt;br/&gt;
+        String topic = connConfig.dlqTopicName();&lt;br/&gt;
         if (topic != null &amp;amp;&amp;amp; !topic.isEmpty()) 
{
             DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(config, connConfig, producerProps);
-            reporter.configure(connConfig.originalsWithPrefix(DeadLetterQueueReporter.PREFIX + &quot;.&quot;));
             reporters.add(reporter);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -541,8 +540,7 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {&lt;br/&gt;
     private List&amp;lt;ErrorReporter&amp;gt; sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,&lt;br/&gt;
                                                       ErrorHandlingMetrics errorHandlingMetrics) {&lt;br/&gt;
         List&amp;lt;ErrorReporter&amp;gt; reporters = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LogReporter logReporter = new LogReporter(id);&lt;/li&gt;
	&lt;li&gt;logReporter.configure(connConfig.originalsWithPrefix(LogReporter.PREFIX + &quot;.&quot;));&lt;br/&gt;
+        LogReporter logReporter = new LogReporter(id, connConfig);&lt;br/&gt;
         logReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
         reporters.add(logReporter);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java&lt;br/&gt;
index 454a619ec76..9a8a9afb29b 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java&lt;br/&gt;
@@ -21,12 +21,10 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;
 import org.apache.kafka.clients.producer.KafkaProducer;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
-import org.apache.kafka.common.config.AbstractConfig;&lt;br/&gt;
-import org.apache.kafka.common.config.ConfigDef;&lt;br/&gt;
 import org.apache.kafka.common.errors.TopicExistsException;&lt;br/&gt;
 import org.apache.kafka.common.record.RecordBatch;&lt;br/&gt;
 import org.apache.kafka.connect.errors.ConnectException;&lt;br/&gt;
-import org.apache.kafka.connect.runtime.ConnectorConfig;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.SinkConnectorConfig;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.WorkerConfig;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;br/&gt;
@@ -49,22 +47,14 @@&lt;br/&gt;
     private static final short DLQ_MAX_DESIRED_REPLICATION_FACTOR = 3;&lt;br/&gt;
     private static final int DLQ_NUM_DESIRED_PARTITIONS = 1;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final String PREFIX = &quot;errors.deadletterqueue&quot;;&lt;br/&gt;
+    private final SinkConnectorConfig connConfig;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final String DLQ_TOPIC_NAME = &quot;topic.name&quot;;&lt;/li&gt;
	&lt;li&gt;public static final String DLQ_TOPIC_NAME_DOC = &quot;The name of the topic where these messages are written to.&quot;;&lt;/li&gt;
	&lt;li&gt;public static final String DLQ_TOPIC_DEFAULT = &quot;&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private DeadLetterQueueReporterConfig config;&lt;br/&gt;
     private KafkaProducer&amp;lt;byte[], byte[]&amp;gt; kafkaProducer;&lt;br/&gt;
     private ErrorHandlingMetrics errorHandlingMetrics;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final ConfigDef CONFIG_DEF = new ConfigDef()&lt;/li&gt;
	&lt;li&gt;.define(DLQ_TOPIC_NAME, ConfigDef.Type.STRING, DLQ_TOPIC_DEFAULT, ConfigDef.Importance.HIGH, DLQ_TOPIC_NAME_DOC);&lt;br/&gt;
-&lt;br/&gt;
     public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,&lt;/li&gt;
	&lt;li&gt;ConnectorConfig connConfig, Map&amp;lt;String, Object&amp;gt; producerProps) {&lt;/li&gt;
	&lt;li&gt;String topic = connConfig.getString(PREFIX + &quot;.&quot; + DLQ_TOPIC_NAME);&lt;br/&gt;
+                                                         SinkConnectorConfig connConfig, Map&amp;lt;String, Object&amp;gt; producerProps) {&lt;br/&gt;
+        String topic = connConfig.dlqTopicName();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         try (AdminClient admin = AdminClient.create(workerConfig.originals())) {&lt;br/&gt;
             if (!admin.listTopics().names().get().contains(topic)) &lt;/p&gt;
{
@@ -81,7 +71,7 @@ public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,
         }

&lt;p&gt;         KafkaProducer&amp;lt;byte[], byte[]&amp;gt; dlqProducer = new KafkaProducer&amp;lt;&amp;gt;(producerProps);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new DeadLetterQueueReporter(dlqProducer);&lt;br/&gt;
+        return new DeadLetterQueueReporter(dlqProducer, connConfig);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -90,13 +80,9 @@ public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param kafkaProducer a Kafka Producer to produce the original consumed records.&lt;br/&gt;
      */&lt;br/&gt;
     // Visible for testing&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;DeadLetterQueueReporter(KafkaProducer&amp;lt;byte[], byte[]&amp;gt; kafkaProducer) {&lt;br/&gt;
+    DeadLetterQueueReporter(KafkaProducer&amp;lt;byte[], byte[]&amp;gt; kafkaProducer, SinkConnectorConfig connConfig) 
{
         this.kafkaProducer = kafkaProducer;
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void configure(Map&amp;lt;String, ?&amp;gt; configs) 
{
-        config = new DeadLetterQueueReporterConfig(configs);
+        this.connConfig = connConfig;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
@@ -110,7 +96,8 @@ public void metrics(ErrorHandlingMetrics errorHandlingMetrics) {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param context processing context containing the raw record at 
{@link ProcessingContext#consumerRecord()}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     public void report(ProcessingContext context) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (config.topic().isEmpty()) {&lt;br/&gt;
+        final String dlqTopicName = connConfig.dlqTopicName();&lt;br/&gt;
+        if (dlqTopicName.isEmpty()) 
{
             return;
         }&lt;br/&gt;
 &lt;br/&gt;
@@ -124,31 +111,18 @@ public void report(ProcessingContext context) {&lt;br/&gt;
 &lt;br/&gt;
         ProducerRecord&amp;lt;byte[], byte[]&amp;gt; producerRecord;&lt;br/&gt;
         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {
-            producerRecord = new ProducerRecord&amp;lt;&amp;gt;(config.topic(), null,
+            producerRecord = new ProducerRecord&amp;lt;&amp;gt;(dlqTopicName, null,
                     originalMessage.key(), originalMessage.value(), originalMessage.headers());
         } else {
-            producerRecord = new ProducerRecord&amp;lt;&amp;gt;(config.topic(), null, originalMessage.timestamp(),
+            producerRecord = new ProducerRecord&amp;lt;&amp;gt;(dlqTopicName, null, originalMessage.timestamp(),
                     originalMessage.key(), originalMessage.value(), originalMessage.headers());
         }&lt;br/&gt;
 &lt;br/&gt;
         this.kafkaProducer.send(producerRecord, (metadata, exception) -&amp;gt; {&lt;br/&gt;
             if (exception != null) {
-                log.error(&quot;Could not produce message to dead letter queue. topic=&quot; + config.topic(), exception);
+                log.error(&quot;Could not produce message to dead letter queue. topic=&quot; + dlqTopicName, exception);
                 errorHandlingMetrics.recordDeadLetterQueueProduceFailed();
             }&lt;br/&gt;
         });&lt;br/&gt;
     }&lt;br/&gt;
-&lt;br/&gt;
-    static class DeadLetterQueueReporterConfig extends AbstractConfig {&lt;br/&gt;
-        public DeadLetterQueueReporterConfig(Map&amp;lt;?, ?&amp;gt; originals) {
-            super(CONFIG_DEF, originals, true);
-        }&lt;br/&gt;
-&lt;br/&gt;
-        /**&lt;br/&gt;
-         * @return name of the dead letter queue topic.&lt;br/&gt;
-         */&lt;br/&gt;
-        public String topic() {
-            return getString(DLQ_TOPIC_NAME);
-        }&lt;br/&gt;
-    }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java&lt;br/&gt;
index e71b6bc8ba9..f7df1b2d1a3 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java&lt;br/&gt;
@@ -16,12 +16,10 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.connect.runtime.errors;&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.kafka.common.Configurable;&lt;br/&gt;
-&lt;br/&gt;
 /**&lt;br/&gt;
  * Report an error using the information contained in the {@link ProcessingContext}.&lt;br/&gt;
  */&lt;br/&gt;
-public interface ErrorReporter extends Configurable {&lt;br/&gt;
+public interface ErrorReporter {&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
      * Report an error.&lt;br/&gt;
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java&lt;br/&gt;
index 1d2c08fd18b..e81bd547568 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java&lt;br/&gt;
@@ -16,14 +16,11 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.connect.runtime.errors;&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.kafka.common.config.AbstractConfig;&lt;br/&gt;
-import org.apache.kafka.common.config.ConfigDef;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.ConnectorConfig;&lt;br/&gt;
 import org.apache.kafka.connect.util.ConnectorTaskId;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;br/&gt;
 &lt;br/&gt;
-import java.util.Map;&lt;br/&gt;
-&lt;br/&gt;
 /**&lt;br/&gt;
  * Writes errors and their context to application logs.&lt;br/&gt;
  */&lt;br/&gt;
@@ -31,29 +28,16 @@&lt;br/&gt;
 &lt;br/&gt;
     private static final Logger log = LoggerFactory.getLogger(LogReporter.class);&lt;br/&gt;
 &lt;br/&gt;
-    public static final String PREFIX = &quot;errors.log&quot;;&lt;br/&gt;
-&lt;br/&gt;
-    public static final String LOG_ENABLE = &quot;enable&quot;;&lt;br/&gt;
-    public static final String LOG_ENABLE_DOC = &quot;If true, log to application logs the errors and the information describing where they occurred.&quot;;&lt;br/&gt;
-    public static final boolean LOG_ENABLE_DEFAULT = false;&lt;br/&gt;
-&lt;br/&gt;
-    public static final String LOG_INCLUDE_MESSAGES = &quot;include.messages&quot;;&lt;br/&gt;
-    public static final String LOG_INCLUDE_MESSAGES_DOC = &quot;If true, include in the application log the Connect key, value, and other details of records that resulted in errors and failures.&quot;;&lt;br/&gt;
-    public static final boolean LOG_INCLUDE_MESSAGES_DEFAULT = false;&lt;br/&gt;
-&lt;br/&gt;
     private final ConnectorTaskId id;&lt;br/&gt;
+    private final ConnectorConfig connConfig;&lt;br/&gt;
 &lt;br/&gt;
-    private LogReporterConfig config;&lt;br/&gt;
     private ErrorHandlingMetrics errorHandlingMetrics;&lt;br/&gt;
 &lt;br/&gt;
-    public LogReporter(ConnectorTaskId id) {&lt;br/&gt;
+    public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig) {
         this.id = id;
+        this.connConfig = connConfig;
     }&lt;br/&gt;
 &lt;br/&gt;
-    private static final ConfigDef CONFIG_DEF = new ConfigDef()&lt;br/&gt;
-                .define(LOG_ENABLE, ConfigDef.Type.BOOLEAN, LOG_ENABLE_DEFAULT, ConfigDef.Importance.MEDIUM, LOG_ENABLE_DOC)&lt;br/&gt;
-                .define(LOG_INCLUDE_MESSAGES, ConfigDef.Type.BOOLEAN, LOG_INCLUDE_MESSAGES_DEFAULT, ConfigDef.Importance.MEDIUM, LOG_INCLUDE_MESSAGES_DOC);&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;br/&gt;
      * Log error context.&lt;br/&gt;
      *&lt;br/&gt;
@@ -61,7 +45,7 @@ public LogReporter(ConnectorTaskId id) {&lt;br/&gt;
      */&lt;br/&gt;
     @Override&lt;br/&gt;
     public void report(ProcessingContext context) {&lt;br/&gt;
-        if (!config.isEnabled()) {&lt;br/&gt;
+        if (!connConfig.enableErrorLog()) {             return;         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -80,32 +64,8 @@ public void metrics(ErrorHandlingMetrics errorHandlingMetrics) {&lt;/p&gt;

&lt;p&gt;     // Visible for testing&lt;br/&gt;
     String message(ProcessingContext context) &lt;/p&gt;
{
-        return String.format(&quot;Error encountered in task %s. %s&quot;, String.valueOf(id), context.toString(config.canLogMessages()));
-    }
&lt;p&gt;-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void configure(Map&amp;lt;String, ?&amp;gt; configs) 
{
-        config = new LogReporterConfig(configs);
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private static class LogReporterConfig extends AbstractConfig {&lt;/li&gt;
	&lt;li&gt;public LogReporterConfig(Map&amp;lt;?, ?&amp;gt; originals) 
{
-            super(CONFIG_DEF, originals, true);
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* @return true, if logging of error context is desired; false otherwise.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public boolean isEnabled() 
{
-            return getBoolean(LOG_ENABLE);
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* @return if false, the connect record which caused the exception is not logged.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public boolean canLogMessages() 
{
-            return getBoolean(LOG_INCLUDE_MESSAGES);
-        }
&lt;p&gt;+        return String.format(&quot;Error encountered in task %s. %s&quot;, String.valueOf(id),&lt;br/&gt;
+                context.toString(connConfig.includeRecordDetailsInErrorLog()));&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; }&lt;br/&gt;
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java&lt;br/&gt;
index 941abf3d3d2..eadf276adb3 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java&lt;br/&gt;
@@ -17,37 +17,31 @@&lt;br/&gt;
 package org.apache.kafka.connect.runtime.errors;&lt;/p&gt;

&lt;p&gt; import org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;
-import org.apache.kafka.common.config.AbstractConfig;&lt;br/&gt;
-import org.apache.kafka.common.config.ConfigDef;&lt;br/&gt;
 import org.apache.kafka.common.config.ConfigException;&lt;br/&gt;
-import org.apache.kafka.common.utils.SystemTime;&lt;br/&gt;
 import org.apache.kafka.common.utils.Time;&lt;br/&gt;
 import org.apache.kafka.connect.errors.ConnectException;&lt;br/&gt;
 import org.apache.kafka.connect.errors.RetriableException;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.ConnectorConfig;&lt;br/&gt;
 import org.apache.kafka.connect.source.SourceRecord;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;/p&gt;

&lt;p&gt;-import java.util.Collections;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.concurrent.ThreadLocalRandom;&lt;/p&gt;

&lt;p&gt;-import static org.apache.kafka.common.config.ConfigDef.Range.atLeast;&lt;br/&gt;
-import static org.apache.kafka.common.config.ConfigDef.ValidString.in;&lt;br/&gt;
-&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Attempt to recover a failed operation with retries and tolerance limits.&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;br/&gt;
  *&lt;/li&gt;
	&lt;li&gt;A retry is attempted if the operation throws a 
{@link RetriableException}. Retries are accompanied by exponential backoffs, starting with&lt;br/&gt;
- * {@link #RETRIES_DELAY_MIN_MS}, up to what is specified with {@link RetryWithToleranceOperatorConfig#retryDelayMax()}.&lt;br/&gt;
+ * {@link #RETRIES_DELAY_MIN_MS}, up to what is specified with {@link ConnectorConfig#errorMaxDelayInMillis()}.&lt;br/&gt;
  * Including the first attempt and future retries, the total time taken to evaluate the operation should be within&lt;br/&gt;
- * {@link RetryWithToleranceOperatorConfig#retryDelayMax()} millis.&lt;br/&gt;
+ * {@link ConnectorConfig#errorMaxDelayInMillis()} millis.&lt;br/&gt;
  * &amp;lt;p&amp;gt;&lt;br/&gt;
  *&lt;br/&gt;
- * This executor will tolerate failures, as specified by {@link RetryWithToleranceOperatorConfig#toleranceLimit()}.&lt;br/&gt;
+ * This executor will tolerate failures, as specified by {@link ConnectorConfig#errorToleranceType()}.&lt;br/&gt;
  * For transformations and converters, all exceptions are tolerated. For others operations, only {@link RetriableException}
&lt;p&gt; are tolerated.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;br/&gt;
  *&lt;br/&gt;
@@ -61,27 +55,8 @@&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Logger log = LoggerFactory.getLogger(RetryWithToleranceOperator.class);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final String RETRY_TIMEOUT = &quot;retry.timeout&quot;;&lt;/li&gt;
	&lt;li&gt;public static final String RETRY_TIMEOUT_DOC = &quot;The total duration in milliseconds a failed operation will be retried for.&quot;;&lt;/li&gt;
	&lt;li&gt;public static final long RETRY_TIMEOUT_DEFAULT = 0;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;public static final String RETRY_DELAY_MAX_MS = &quot;retry.delay.max.ms&quot;;&lt;/li&gt;
	&lt;li&gt;public static final String RETRY_DELAY_MAX_MS_DOC = &quot;The maximum duration between two consecutive retries (in milliseconds).&quot;;&lt;/li&gt;
	&lt;li&gt;public static final long RETRY_DELAY_MAX_MS_DEFAULT = 60000;&lt;br/&gt;
-&lt;br/&gt;
     public static final long RETRIES_DELAY_MIN_MS = 300;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final String TOLERANCE_LIMIT = &quot;allowed.max&quot;;&lt;/li&gt;
	&lt;li&gt;public static final String TOLERANCE_LIMIT_DOC = &quot;Fail the task if we exceed specified number of errors overall.&quot;;&lt;/li&gt;
	&lt;li&gt;public static final String TOLERANCE_LIMIT_DEFAULT = &quot;none&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// for testing only&lt;/li&gt;
	&lt;li&gt;public static final RetryWithToleranceOperator NOOP_OPERATOR = new RetryWithToleranceOperator();&lt;/li&gt;
	&lt;li&gt;static 
{
-        NOOP_OPERATOR.configure(Collections.emptyMap());
-        NOOP_OPERATOR.metrics(new ErrorHandlingMetrics());
-    }
&lt;p&gt;-&lt;br/&gt;
     private static final Map&amp;lt;Stage, Class&amp;lt;? extends Exception&amp;gt;&amp;gt; TOLERABLE_EXCEPTIONS = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     static &lt;/p&gt;
{
         TOLERABLE_EXCEPTIONS.put(Stage.TRANSFORMATION, Exception.class);
@@ -90,29 +65,24 @@
         TOLERABLE_EXCEPTIONS.put(Stage.VALUE_CONVERTER, Exception.class);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private final long errorRetryTimeout;&lt;br/&gt;
+    private final long errorMaxDelayInMillis;&lt;br/&gt;
+    private final ToleranceType errorToleranceType;&lt;br/&gt;
+&lt;br/&gt;
     private long totalFailures = 0;&lt;br/&gt;
     private final Time time;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private RetryWithToleranceOperatorConfig config;&lt;br/&gt;
     private ErrorHandlingMetrics errorHandlingMetrics;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     protected ProcessingContext context = new ProcessingContext();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public RetryWithToleranceOperator() 
{
-        this(new SystemTime());
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// Visible for testing&lt;/li&gt;
	&lt;li&gt;public RetryWithToleranceOperator(Time time) {&lt;br/&gt;
+    public RetryWithToleranceOperator(long errorRetryTimeout, long errorMaxDelayInMillis,&lt;br/&gt;
+                                      ToleranceType toleranceType, Time time) 
{
+        this.errorRetryTimeout = errorRetryTimeout;
+        this.errorMaxDelayInMillis = errorMaxDelayInMillis;
+        this.errorToleranceType = toleranceType;
         this.time = time;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static ConfigDef getConfigDef() 
{
-        return new ConfigDef()
-                .define(RETRY_TIMEOUT, ConfigDef.Type.LONG, RETRY_TIMEOUT_DEFAULT, ConfigDef.Importance.HIGH, RETRY_TIMEOUT_DOC)
-                .define(RETRY_DELAY_MAX_MS, ConfigDef.Type.LONG, RETRY_DELAY_MAX_MS_DEFAULT, atLeast(1), ConfigDef.Importance.MEDIUM, RETRY_DELAY_MAX_MS_DOC)
-                .define(TOLERANCE_LIMIT, ConfigDef.Type.STRING, TOLERANCE_LIMIT_DEFAULT, in(&quot;none&quot;, &quot;all&quot;), ConfigDef.Importance.HIGH, TOLERANCE_LIMIT_DOC);
-    }
&lt;p&gt;-&lt;br/&gt;
     /**&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Execute the recoverable operation. If the operation is already in a failed state, then simply return&lt;/li&gt;
	&lt;li&gt;with the existing failure.&lt;br/&gt;
@@ -151,7 +121,7 @@ static ConfigDef getConfigDef() {&lt;br/&gt;
     protected &amp;lt;V&amp;gt; V execAndRetry(Operation&amp;lt;V&amp;gt; operation) throws Exception {&lt;br/&gt;
         int attempt = 0;&lt;br/&gt;
         long startTime = time.milliseconds();&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long deadline = startTime + config.retryTimeout();&lt;br/&gt;
+        long deadline = startTime + errorRetryTimeout;&lt;br/&gt;
         do {&lt;br/&gt;
             try {&lt;br/&gt;
                 attempt++;&lt;br/&gt;
@@ -221,27 +191,27 @@ void markAsFailed() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Visible for testing&lt;br/&gt;
     boolean withinToleranceLimits() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;switch (config.toleranceLimit()) {&lt;br/&gt;
+        switch (errorToleranceType) {&lt;br/&gt;
             case NONE:&lt;br/&gt;
                 if (totalFailures &amp;gt; 0) return false;&lt;br/&gt;
             case ALL:&lt;br/&gt;
                 return true;&lt;br/&gt;
             default:&lt;/li&gt;
	&lt;li&gt;throw new ConfigException(&quot;Unknown tolerance type: {}&quot;, config.toleranceLimit());&lt;br/&gt;
+                throw new ConfigException(&quot;Unknown tolerance type: {}&quot;, errorToleranceType);&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Visible for testing&lt;br/&gt;
     boolean checkRetry(long startTime) &lt;/p&gt;
{
-        return (time.milliseconds() - startTime) &amp;lt; config.retryTimeout();
+        return (time.milliseconds() - startTime) &amp;lt; errorRetryTimeout;
     }

&lt;p&gt;     // Visible for testing&lt;br/&gt;
     void backoff(int attempt, long deadline) {&lt;br/&gt;
         int numRetry = attempt - 1;&lt;br/&gt;
         long delay = RETRIES_DELAY_MIN_MS &amp;lt;&amp;lt; numRetry;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (delay &amp;gt; config.retryDelayMax()) {&lt;/li&gt;
	&lt;li&gt;delay = ThreadLocalRandom.current().nextLong(config.retryDelayMax());&lt;br/&gt;
+        if (delay &amp;gt; errorMaxDelayInMillis) 
{
+            delay = ThreadLocalRandom.current().nextLong(errorMaxDelayInMillis);
         }
&lt;p&gt;         if (delay + time.milliseconds() &amp;gt; deadline) {&lt;br/&gt;
             delay = deadline - time.milliseconds();&lt;br/&gt;
@@ -250,45 +220,19 @@ void backoff(int attempt, long deadline) &lt;/p&gt;
{
         time.sleep(delay);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void configure(Map&amp;lt;String, ?&amp;gt; configs) 
{
-        config = new RetryWithToleranceOperatorConfig(configs);
-    }
&lt;p&gt;-&lt;br/&gt;
     public void metrics(ErrorHandlingMetrics errorHandlingMetrics) &lt;/p&gt;
{
         this.errorHandlingMetrics = errorHandlingMetrics;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static class RetryWithToleranceOperatorConfig extends AbstractConfig {&lt;/li&gt;
	&lt;li&gt;public RetryWithToleranceOperatorConfig(Map&amp;lt;?, ?&amp;gt; originals) 
{
-            super(getConfigDef(), originals, true);
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* @return the total time an operation can take to succeed (including the first attempt and retries).&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public long retryTimeout() 
{
-            return getLong(RETRY_TIMEOUT);
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* @return the maximum delay between two subsequent retries in milliseconds.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public long retryDelayMax() 
{
-            return getLong(RETRY_DELAY_MAX_MS);
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* @return determine how many errors to tolerate.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public ToleranceType toleranceLimit() 
{
-            return ToleranceType.fromString(getString(TOLERANCE_LIMIT));
-        }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         return &amp;quot;RetryWithToleranceOperator{&quot; +
-                &quot;config=&quot; + config +
+                &quot;errorRetryTimeout=&quot; + errorRetryTimeout +
+                &quot;, errorMaxDelayInMillis=&quot; + errorMaxDelayInMillis +
+                &quot;, errorToleranceType=&quot; + errorToleranceType +
+                &quot;, totalFailures=&quot; + totalFailures +
+                &quot;, time=&quot; + time +
+                &quot;, context=&quot; + context +
                 &apos;}&amp;#39;;     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ToleranceType.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ToleranceType.java&lt;br/&gt;
index 79956ac7963..dd40a60648a 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ToleranceType.java&lt;br/&gt;
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ToleranceType.java&lt;br/&gt;
@@ -33,7 +33,8 @@&lt;br/&gt;
      */&lt;br/&gt;
     ALL;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static ToleranceType fromString(String typeStr) {&lt;/li&gt;
	&lt;li&gt;return &quot;ALL&quot;.equals(typeStr.toUpperCase(Locale.ROOT)) ? ToleranceType.ALL : ToleranceType.NONE;&lt;br/&gt;
+    public String value() 
{
+        return name().toLowerCase(Locale.ROOT);
     }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java&lt;br/&gt;
index da017e851b7..5728465095a 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java&lt;br/&gt;
@@ -170,16 +170,16 @@ public void testConfigValidationMissingName() 
{
         // We expect there to be errors due to the missing name and .... Note that these assertions depend heavily on
         // the config fields for SourceConnectorConfig, but we expect these to change rarely.
         assertEquals(TestSourceConnector.class.getName(), result.name());
-        assertEquals(Arrays.asList(ConnectorConfig.COMMON_GROUP, ConnectorConfig.TRANSFORMS_GROUP), result.groups());
+        assertEquals(Arrays.asList(ConnectorConfig.COMMON_GROUP, ConnectorConfig.TRANSFORMS_GROUP, ConnectorConfig.ERROR_GROUP), result.groups());
         assertEquals(2, result.errorCount());
-        // Base connector config has 8 fields, connector&apos;s configs add 2
-        assertEquals(10, result.values().size());
+        // Base connector config has 13 fields, connector&apos;s configs add 2
+        assertEquals(15, result.values().size());
         // Missing name should generate an error
         assertEquals(ConnectorConfig.NAME_CONFIG, result.values().get(0).configValue().name());
         assertEquals(1, result.values().get(0).configValue().errors().size());
         // &quot;required&quot; config from connector should generate an error
-        assertEquals(&quot;required&quot;, result.values().get(8).configValue().name());
-        assertEquals(1, result.values().get(8).configValue().errors().size());
+        assertEquals(&quot;required&quot;, result.values().get(13).configValue().name());
+        assertEquals(1, result.values().get(13).configValue().errors().size());
 
         verifyAll();
     }
&lt;p&gt;@@ -228,20 +228,21 @@ public void testConfigValidationTransformsExtendResults() &lt;/p&gt;
{
         List&amp;lt;String&amp;gt; expectedGroups = Arrays.asList(
                 ConnectorConfig.COMMON_GROUP,
                 ConnectorConfig.TRANSFORMS_GROUP,
+                ConnectorConfig.ERROR_GROUP,
                 &quot;Transforms: xformA&quot;,
                 &quot;Transforms: xformB&quot;
         );
         assertEquals(expectedGroups, result.groups());
         assertEquals(2, result.errorCount());
-        // Base connector config has 8 fields, connector&apos;s configs add 2, 2 type fields from the transforms, and
+        // Base connector config has 13 fields, connector&apos;s configs add 2, 2 type fields from the transforms, and
         // 1 from the valid transformation&apos;s config
-        assertEquals(13, result.values().size());
+        assertEquals(18, result.values().size());
         // Should get 2 type fields from the transforms, first adds its own config since it has a valid class
-        assertEquals(&quot;transforms.xformA.type&quot;, result.values().get(8).configValue().name());
-        assertTrue(result.values().get(8).configValue().errors().isEmpty());
-        assertEquals(&quot;transforms.xformA.subconfig&quot;, result.values().get(9).configValue().name());
-        assertEquals(&quot;transforms.xformB.type&quot;, result.values().get(10).configValue().name());
-        assertFalse(result.values().get(10).configValue().errors().isEmpty());
+        assertEquals(&quot;transforms.xformA.type&quot;, result.values().get(13).configValue().name());
+        assertTrue(result.values().get(13).configValue().errors().isEmpty());
+        assertEquals(&quot;transforms.xformA.subconfig&quot;, result.values().get(14).configValue().name());
+        assertEquals(&quot;transforms.xformB.type&quot;, result.values().get(15).configValue().name());
+        assertFalse(result.values().get(15).configValue().errors().isEmpty());
 
         verifyAll();
     }
&lt;p&gt;diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java&lt;br/&gt;
index b50e7ff0956..e931642afcc 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java&lt;br/&gt;
@@ -35,7 +35,9 @@&lt;br/&gt;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.errors.LogReporter;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.errors.ToleranceType;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.isolation.Plugins;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;&lt;br/&gt;
 import org.apache.kafka.connect.sink.SinkConnector;&lt;br/&gt;
 import org.apache.kafka.connect.sink.SinkRecord;&lt;br/&gt;
@@ -69,6 +71,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static java.util.Collections.emptyMap;&lt;br/&gt;
 import static java.util.Collections.singletonList;&lt;br/&gt;
+import static org.apache.kafka.common.utils.Time.SYSTEM;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;/p&gt;

&lt;p&gt; @RunWith(PowerMockRunner.class)&lt;br/&gt;
@@ -81,6 +84,8 @@&lt;br/&gt;
     private static final int PARTITION2 = 13;&lt;br/&gt;
     private static final long FIRST_OFFSET = 45;&lt;/p&gt;

&lt;p&gt;+    @Mock Plugins plugins;&lt;br/&gt;
+&lt;br/&gt;
     private static final Map&amp;lt;String, String&amp;gt; TASK_PROPS = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;     static &lt;/p&gt;
{
@@ -88,17 +93,11 @@
         TASK_PROPS.put(TaskConfig.TASK_CLASS_CONFIG, TestSinkTask.class.getName());
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final TaskConfig TASK_CONFIG = new TaskConfig(TASK_PROPS);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Map&amp;lt;String, String&amp;gt; OPERATION_EXECUTOR_PROPS = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    public static final long OPERATOR_RETRY_TIMEOUT_MILLIS = 60000;&lt;br/&gt;
+    public static final long OPERATOR_RETRY_MAX_DELAY_MILLIS = 5000;&lt;br/&gt;
+    public static final ToleranceType OPERATOR_TOLERANCE_TYPE = ToleranceType.ALL;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static 
{
-        OPERATION_EXECUTOR_PROPS.put(RetryWithToleranceOperator.TOLERANCE_LIMIT, &quot;all&quot;);
-        // wait up to 1 minute for an operation
-        OPERATION_EXECUTOR_PROPS.put(RetryWithToleranceOperator.RETRY_TIMEOUT, &quot;60000&quot;);
-        // wait up 5 seconds between subsequent retries
-        OPERATION_EXECUTOR_PROPS.put(RetryWithToleranceOperator.RETRY_DELAY_MAX_MS, &quot;5000&quot;);
-    }
&lt;p&gt;+    private static final TaskConfig TASK_CONFIG = new TaskConfig(TASK_PROPS);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private ConnectorTaskId taskId = new ConnectorTaskId(&quot;job&quot;, 0);&lt;br/&gt;
     private TargetState initialState = TargetState.STARTED;&lt;br/&gt;
@@ -164,15 +163,13 @@ public void tearDown() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testErrorHandlingInSinkTasks() throws Exception {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LogReporter reporter = new LogReporter(taskId);&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; reportProps = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;reportProps.put(LogReporter.LOG_ENABLE, &quot;true&quot;);&lt;/li&gt;
	&lt;li&gt;reportProps.put(LogReporter.LOG_INCLUDE_MESSAGES, &quot;true&quot;);&lt;/li&gt;
	&lt;li&gt;reporter.configure(reportProps);&lt;br/&gt;
+        Map&amp;lt;String, String&amp;gt; reportProps = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, &quot;true&quot;);&lt;br/&gt;
+        reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, &quot;true&quot;);&lt;br/&gt;
+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));&lt;br/&gt;
         reporter.metrics(errorHandlingMetrics);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(time);&lt;/li&gt;
	&lt;li&gt;retryWithToleranceOperator.configure(OPERATION_EXECUTOR_PROPS);&lt;br/&gt;
+        RetryWithToleranceOperator retryWithToleranceOperator = operator();&lt;br/&gt;
         retryWithToleranceOperator.metrics(errorHandlingMetrics);&lt;br/&gt;
         retryWithToleranceOperator.reporters(singletonList(reporter));&lt;br/&gt;
         createSinkTask(initialState, retryWithToleranceOperator);&lt;br/&gt;
@@ -212,17 +209,19 @@ public void testErrorHandlingInSinkTasks() throws Exception 
{
         PowerMock.verifyAll();
     }&lt;br/&gt;
 &lt;br/&gt;
+    private RetryWithToleranceOperator operator() {
+        return new RetryWithToleranceOperator(OPERATOR_RETRY_TIMEOUT_MILLIS, OPERATOR_RETRY_MAX_DELAY_MILLIS, OPERATOR_TOLERANCE_TYPE, SYSTEM);
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testErrorHandlingInSourceTasks() throws Exception {&lt;br/&gt;
-        LogReporter reporter = new LogReporter(taskId);&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; reportProps = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-        reportProps.put(LogReporter.LOG_ENABLE, &quot;true&quot;);&lt;br/&gt;
-        reportProps.put(LogReporter.LOG_INCLUDE_MESSAGES, &quot;true&quot;);&lt;br/&gt;
-        reporter.configure(reportProps);&lt;br/&gt;
+        Map&amp;lt;String, String&amp;gt; reportProps = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, &quot;true&quot;);&lt;br/&gt;
+        reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, &quot;true&quot;);&lt;br/&gt;
+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));&lt;br/&gt;
         reporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(time);&lt;br/&gt;
-        retryWithToleranceOperator.configure(OPERATION_EXECUTOR_PROPS);&lt;br/&gt;
+        RetryWithToleranceOperator retryWithToleranceOperator = operator();&lt;br/&gt;
         retryWithToleranceOperator.metrics(errorHandlingMetrics);&lt;br/&gt;
         retryWithToleranceOperator.reporters(singletonList(reporter));&lt;br/&gt;
         createSourceTask(initialState, retryWithToleranceOperator);&lt;br/&gt;
@@ -271,17 +270,23 @@ public void testErrorHandlingInSourceTasks() throws Exception {         PowerMock.verifyAll();     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private ConnectorConfig connConfig(Map&amp;lt;String, String&amp;gt; connProps) &lt;/p&gt;
{
+        Map&amp;lt;String, String&amp;gt; props = new HashMap&amp;lt;&amp;gt;();
+        props.put(ConnectorConfig.NAME_CONFIG, &quot;test&quot;);
+        props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, SinkTask.class.getName());
+        props.putAll(connProps);
+        return new ConnectorConfig(plugins, props);
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testErrorHandlingInSourceTasksWthBadConverter() throws Exception {&lt;br/&gt;
-        LogReporter reporter = new LogReporter(taskId);&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; reportProps = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-        reportProps.put(LogReporter.LOG_ENABLE, &quot;true&quot;);&lt;br/&gt;
-        reportProps.put(LogReporter.LOG_INCLUDE_MESSAGES, &quot;true&quot;);&lt;br/&gt;
-        reporter.configure(reportProps);&lt;br/&gt;
+        Map&amp;lt;String, String&amp;gt; reportProps = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, &quot;true&quot;);&lt;br/&gt;
+        reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, &quot;true&quot;);&lt;br/&gt;
+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));&lt;br/&gt;
         reporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(time);&lt;br/&gt;
-        retryWithToleranceOperator.configure(OPERATION_EXECUTOR_PROPS);&lt;br/&gt;
+        RetryWithToleranceOperator retryWithToleranceOperator = operator();&lt;br/&gt;
         retryWithToleranceOperator.metrics(errorHandlingMetrics);&lt;br/&gt;
         retryWithToleranceOperator.reporters(singletonList(reporter));&lt;br/&gt;
         createSourceTask(initialState, retryWithToleranceOperator, badConverter());&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java&lt;br/&gt;
index d23adbf3d69..4a7c760fc74 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java&lt;br/&gt;
@@ -34,7 +34,7 @@&lt;br/&gt;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup;&lt;br/&gt;
-import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;&lt;br/&gt;
 import org.apache.kafka.connect.sink.SinkConnector;&lt;br/&gt;
@@ -166,7 +166,7 @@ private void createTask(TargetState initialState) {
                 taskId, sinkTask, statusListener, initialState, workerConfig, ClusterConfigState.EMPTY, metrics,
                 keyConverter, valueConverter, headerConverter,
                 transformationChain, pluginLoader, time,
-                RetryWithToleranceOperator.NOOP_OPERATOR);
+                RetryWithToleranceOperatorTest.NOOP_OPERATOR);
     }&lt;br/&gt;
 &lt;br/&gt;
     @After&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java&lt;br/&gt;
index 800301e05dc..73689d35710 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java&lt;br/&gt;
@@ -29,7 +29,7 @@&lt;br/&gt;
 import org.apache.kafka.connect.data.SchemaAndValue;&lt;br/&gt;
 import org.apache.kafka.connect.errors.ConnectException;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;&lt;br/&gt;
-import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;&lt;br/&gt;
 import org.apache.kafka.connect.sink.SinkConnector;&lt;br/&gt;
@@ -140,8 +140,8 @@ public void setup() {&lt;br/&gt;
                 WorkerSinkTask.class, new String[]{&quot;createConsumer&quot;},&lt;br/&gt;
                 taskId, sinkTask, statusListener, initialState, workerConfig, ClusterConfigState.EMPTY, metrics, keyConverter,&lt;br/&gt;
                 valueConverter, headerConverter,&lt;br/&gt;
-                new TransformationChain(Collections.emptyList(), RetryWithToleranceOperator.NOOP_OPERATOR),&lt;br/&gt;
-                pluginLoader, time, RetryWithToleranceOperator.NOOP_OPERATOR);&lt;br/&gt;
+                new TransformationChain(Collections.emptyList(), RetryWithToleranceOperatorTest.NOOP_OPERATOR),&lt;br/&gt;
+                pluginLoader, time, RetryWithToleranceOperatorTest.NOOP_OPERATOR);&lt;br/&gt;
 &lt;br/&gt;
         recordsReturned = 0;&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java&lt;br/&gt;
index 1482d75b513..db73a8e0914 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java&lt;br/&gt;
@@ -27,7 +27,7 @@&lt;br/&gt;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.WorkerSourceTask.SourceTaskMetricsGroup;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;&lt;br/&gt;
-import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.isolation.Plugins;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;&lt;br/&gt;
 import org.apache.kafka.connect.source.SourceRecord;&lt;br/&gt;
@@ -151,7 +151,7 @@ private void createWorkerTask() {&lt;br/&gt;
     private void createWorkerTask(TargetState initialState) {
         workerTask = new WorkerSourceTask(taskId, sourceTask, statusListener, initialState, keyConverter, valueConverter, headerConverter,
                 transformationChain, producer, offsetReader, offsetWriter, config, clusterConfigState, metrics, plugins.delegatingLoader(), Time.SYSTEM,
-                RetryWithToleranceOperator.NOOP_OPERATOR);
+                RetryWithToleranceOperatorTest.NOOP_OPERATOR);
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTaskTest.java&lt;br/&gt;
index de0ba8a9ddf..33349f4c2f5 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTaskTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTaskTest.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.connect.errors.ConnectException;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.WorkerTask.TaskMetricsGroup;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest;&lt;br/&gt;
 import org.apache.kafka.connect.sink.SinkTask;&lt;br/&gt;
 import org.apache.kafka.connect.util.ConnectorTaskId;&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime;&lt;br/&gt;
@@ -62,7 +63,7 @@&lt;br/&gt;
     @Before&lt;br/&gt;
     public void setup() {
         metrics = new MockConnectMetrics();
-        retryWithToleranceOperator = new RetryWithToleranceOperator();
+        retryWithToleranceOperator = RetryWithToleranceOperatorTest.NOOP_OPERATOR;
     }&lt;br/&gt;
 &lt;br/&gt;
     @After&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java&lt;br/&gt;
index 6fa7ed11b9e..77238e9aaad 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java&lt;br/&gt;
@@ -70,6 +70,7 @@&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 &lt;br/&gt;
+import static org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest.NOOP_OPERATOR;&lt;br/&gt;
 import static org.easymock.EasyMock.anyObject;&lt;br/&gt;
 import static org.easymock.EasyMock.eq;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
@@ -487,7 +488,7 @@ public void testAddRemoveTask() throws Exception {&lt;br/&gt;
                 anyObject(JsonConverter.class),&lt;br/&gt;
                 anyObject(JsonConverter.class),&lt;br/&gt;
                 anyObject(JsonConverter.class),&lt;br/&gt;
-                EasyMock.eq(new TransformationChain(Collections.emptyList(), RetryWithToleranceOperator.NOOP_OPERATOR)),&lt;br/&gt;
+                EasyMock.eq(new TransformationChain(Collections.emptyList(), NOOP_OPERATOR)),&lt;br/&gt;
                 anyObject(KafkaProducer.class),&lt;br/&gt;
                 anyObject(OffsetStorageReader.class),&lt;br/&gt;
                 anyObject(OffsetStorageWriter.class),&lt;br/&gt;
@@ -626,7 +627,7 @@ public void testCleanupTasksOnStop() throws Exception {&lt;br/&gt;
                 anyObject(JsonConverter.class),&lt;br/&gt;
                 anyObject(JsonConverter.class),&lt;br/&gt;
                 anyObject(JsonConverter.class),&lt;br/&gt;
-                EasyMock.eq(new TransformationChain(Collections.emptyList(), RetryWithToleranceOperator.NOOP_OPERATOR)),&lt;br/&gt;
+                EasyMock.eq(new TransformationChain(Collections.emptyList(), NOOP_OPERATOR)),&lt;br/&gt;
                 anyObject(KafkaProducer.class),&lt;br/&gt;
                 anyObject(OffsetStorageReader.class),&lt;br/&gt;
                 anyObject(OffsetStorageWriter.class),&lt;br/&gt;
@@ -719,7 +720,7 @@ public void testConverterOverrides() throws Exception {&lt;br/&gt;
                 EasyMock.capture(keyConverter),&lt;br/&gt;
                 EasyMock.capture(valueConverter),&lt;br/&gt;
                 EasyMock.capture(headerConverter),&lt;br/&gt;
-                EasyMock.eq(new TransformationChain(Collections.emptyList(), RetryWithToleranceOperator.NOOP_OPERATOR)),&lt;br/&gt;
+                EasyMock.eq(new TransformationChain(Collections.emptyList(), NOOP_OPERATOR)),&lt;br/&gt;
                 anyObject(KafkaProducer.class),&lt;br/&gt;
                 anyObject(OffsetStorageReader.class),&lt;br/&gt;
                 anyObject(OffsetStorageWriter.class),&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java&lt;br/&gt;
index f6a0507909c..b5410d071d8 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java&lt;br/&gt;
@@ -21,7 +21,11 @@&lt;br/&gt;
 import org.apache.kafka.clients.producer.RecordMetadata;&lt;br/&gt;
 import org.apache.kafka.connect.json.JsonConverter;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.ConnectMetrics;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.ConnectorConfig;&lt;br/&gt;
 import org.apache.kafka.connect.runtime.MockConnectMetrics;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.SinkConnectorConfig;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.isolation.Plugins;&lt;br/&gt;
+import org.apache.kafka.connect.sink.SinkTask;&lt;br/&gt;
 import org.apache.kafka.connect.util.ConnectorTaskId;&lt;br/&gt;
 import org.easymock.EasyMock;&lt;br/&gt;
 import org.easymock.Mock;&lt;br/&gt;
@@ -37,6 +41,8 @@&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.concurrent.Future;&lt;br/&gt;
 &lt;br/&gt;
+import static java.util.Collections.emptyMap;&lt;br/&gt;
+import static java.util.Collections.singletonMap;&lt;br/&gt;
 import static org.easymock.EasyMock.replay;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 &lt;br/&gt;
@@ -54,13 +60,14 @@&lt;br/&gt;
     @Mock&lt;br/&gt;
     Future&amp;lt;RecordMetadata&amp;gt; metadata;&lt;br/&gt;
 &lt;br/&gt;
-    private HashMap&amp;lt;String, Object&amp;gt; config;&lt;br/&gt;
+    @Mock&lt;br/&gt;
+    Plugins plugins;&lt;br/&gt;
+&lt;br/&gt;
     private ErrorHandlingMetrics errorHandlingMetrics;&lt;br/&gt;
     private MockConnectMetrics metrics;&lt;br/&gt;
 &lt;br/&gt;
     @Before&lt;br/&gt;
     public void setup() {
-        config = new HashMap&amp;lt;&amp;gt;();
         metrics = new MockConnectMetrics();
         errorHandlingMetrics = new ErrorHandlingMetrics(new ConnectorTaskId(&quot;connector-&quot;, 1), metrics);
     }&lt;br/&gt;
@@ -74,8 +81,7 @@ public void tearDown() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testDLQConfigWithEmptyTopicName() {&lt;br/&gt;
-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer);&lt;br/&gt;
-        deadLetterQueueReporter.configure(config);&lt;br/&gt;
+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(emptyMap()));&lt;br/&gt;
         deadLetterQueueReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         ProcessingContext context = processingContext();&lt;br/&gt;
@@ -90,8 +96,7 @@ public void testDLQConfigWithEmptyTopicName() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testDLQConfigWithValidTopicName() {&lt;br/&gt;
-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer);&lt;br/&gt;
-        deadLetterQueueReporter.configure(config(DeadLetterQueueReporter.DLQ_TOPIC_NAME, DLQ_TOPIC));&lt;br/&gt;
+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)));&lt;br/&gt;
         deadLetterQueueReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         ProcessingContext context = processingContext();&lt;br/&gt;
@@ -106,8 +111,7 @@ public void testDLQConfigWithValidTopicName() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testReportDLQTwice() {&lt;br/&gt;
-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer);&lt;br/&gt;
-        deadLetterQueueReporter.configure(config(DeadLetterQueueReporter.DLQ_TOPIC_NAME, DLQ_TOPIC));&lt;br/&gt;
+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)));&lt;br/&gt;
         deadLetterQueueReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         ProcessingContext context = processingContext();&lt;br/&gt;
@@ -123,8 +127,7 @@ public void testReportDLQTwice() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testLogOnDisabledLogReporter() {&lt;br/&gt;
-        LogReporter logReporter = new LogReporter(TASK_ID);&lt;br/&gt;
-        logReporter.configure(config);&lt;br/&gt;
+        LogReporter logReporter = new LogReporter(TASK_ID, config(emptyMap()));&lt;br/&gt;
         logReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         ProcessingContext context = processingContext();&lt;br/&gt;
@@ -137,8 +140,7 @@ public void testLogOnDisabledLogReporter() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testLogOnEnabledLogReporter() {&lt;br/&gt;
-        LogReporter logReporter = new LogReporter(TASK_ID);&lt;br/&gt;
-        logReporter.configure(config(LogReporter.LOG_ENABLE, &quot;true&quot;));&lt;br/&gt;
+        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, &quot;true&quot;)));&lt;br/&gt;
         logReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         ProcessingContext context = processingContext();&lt;br/&gt;
@@ -151,8 +153,7 @@ public void testLogOnEnabledLogReporter() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testLogMessageWithNoRecords() {&lt;br/&gt;
-        LogReporter logReporter = new LogReporter(TASK_ID);&lt;br/&gt;
-        logReporter.configure(config(LogReporter.LOG_ENABLE, &quot;true&quot;));&lt;br/&gt;
+        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, &quot;true&quot;)));&lt;br/&gt;
         logReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         ProcessingContext context = processingContext();&lt;br/&gt;
@@ -164,9 +165,11 @@ public void testLogMessageWithNoRecords() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testLogMessageWithSinkRecords() {&lt;br/&gt;
-        LogReporter logReporter = new LogReporter(TASK_ID);&lt;br/&gt;
-        logReporter.configure(config(LogReporter.LOG_ENABLE, &quot;true&quot;));&lt;br/&gt;
-        logReporter.configure(config(LogReporter.LOG_INCLUDE_MESSAGES, &quot;true&quot;));&lt;br/&gt;
+        Map&amp;lt;String, String&amp;gt; props = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        props.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, &quot;true&quot;);&lt;br/&gt;
+        props.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, &quot;true&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        LogReporter logReporter = new LogReporter(TASK_ID, config(props));&lt;br/&gt;
         logReporter.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         ProcessingContext context = processingContext();&lt;br/&gt;
@@ -184,9 +187,12 @@ private ProcessingContext processingContext() {
         return context;
     }&lt;br/&gt;
 &lt;br/&gt;
-    private Map&amp;lt;String, Object&amp;gt; config(String key, Object val) {&lt;br/&gt;
-        config.put(key, val);&lt;br/&gt;
-        return config;&lt;br/&gt;
+    private SinkConnectorConfig config(Map&amp;lt;String, String&amp;gt; configProps) {
+        Map&amp;lt;String, String&amp;gt; props = new HashMap&amp;lt;&amp;gt;();
+        props.put(ConnectorConfig.NAME_CONFIG, &quot;test&quot;);
+        props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, SinkTask.class.getName());
+        props.putAll(configProps);
+        return new SinkConnectorConfig(plugins, props);
     }&lt;br/&gt;
 &lt;br/&gt;
     private void assertErrorHandlingMetricValue(String name, double expected) {&lt;br/&gt;
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java&lt;br/&gt;
index 751510d5fa6..2d340ac32e5 100644&lt;br/&gt;
&amp;#8212; a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java&lt;br/&gt;
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java&lt;br/&gt;
@@ -19,6 +19,9 @@&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime;&lt;br/&gt;
 import org.apache.kafka.connect.errors.ConnectException;&lt;br/&gt;
 import org.apache.kafka.connect.errors.RetriableException;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.ConnectorConfig;&lt;br/&gt;
+import org.apache.kafka.connect.runtime.isolation.Plugins;&lt;br/&gt;
+import org.apache.kafka.connect.sink.SinkTask;&lt;br/&gt;
 import org.easymock.EasyMock;&lt;br/&gt;
 import org.easymock.Mock;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
@@ -31,7 +34,17 @@&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 &lt;br/&gt;
-import static org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.RetryWithToleranceOperatorConfig;&lt;br/&gt;
+import static java.util.Collections.emptyMap;&lt;br/&gt;
+import static java.util.Collections.singletonMap;&lt;br/&gt;
+import static org.apache.kafka.common.utils.Time.SYSTEM;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_RETRY_MAX_DELAY_CONFIG;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_RETRY_MAX_DELAY_DEFAULT;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_RETRY_TIMEOUT_CONFIG;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_RETRY_TIMEOUT_DEFAULT;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_TOLERANCE_CONFIG;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.ConnectorConfig.ERRORS_TOLERANCE_DEFAULT;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.errors.ToleranceType.ALL;&lt;br/&gt;
+import static org.apache.kafka.connect.runtime.errors.ToleranceType.NONE;&lt;br/&gt;
 import static org.easymock.EasyMock.replay;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertFalse;&lt;br/&gt;
@@ -43,6 +56,12 @@&lt;br/&gt;
 @PowerMockIgnore(&quot;javax.management.*&quot;)&lt;br/&gt;
 public class RetryWithToleranceOperatorTest {&lt;br/&gt;
 &lt;br/&gt;
+    public static final RetryWithToleranceOperator NOOP_OPERATOR = new RetryWithToleranceOperator(&lt;br/&gt;
+            ERRORS_RETRY_TIMEOUT_DEFAULT, ERRORS_RETRY_MAX_DELAY_DEFAULT, NONE, SYSTEM);&lt;br/&gt;
+    static {
+        NOOP_OPERATOR.metrics(new ErrorHandlingMetrics());
+    }&lt;br/&gt;
+&lt;br/&gt;
     @SuppressWarnings(&quot;unused&quot;)&lt;br/&gt;
     @Mock&lt;br/&gt;
     private Operation&amp;lt;String&amp;gt; mockOperation;&lt;br/&gt;
@@ -50,6 +69,9 @@&lt;br/&gt;
     @Mock&lt;br/&gt;
     ErrorHandlingMetrics errorHandlingMetrics;&lt;br/&gt;
 &lt;br/&gt;
+    @Mock&lt;br/&gt;
+    Plugins plugins;&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testHandleExceptionInTransformations() {&lt;br/&gt;
         testHandleExceptionInStage(Stage.TRANSFORMATION, new Exception());&lt;br/&gt;
@@ -108,10 +130,7 @@ private void testHandleExceptionInStage(Stage type, Exception ex) {&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     private RetryWithToleranceOperator setupExecutor() {
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator();
-        Map&amp;lt;String, Object&amp;gt; props = config(RetryWithToleranceOperator.RETRY_TIMEOUT, &quot;0&quot;);
-        props.put(RetryWithToleranceOperator.TOLERANCE_LIMIT, &quot;all&quot;);
-        retryWithToleranceOperator.configure(props);
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(0, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, SYSTEM);
         retryWithToleranceOperator.metrics(errorHandlingMetrics);
         return retryWithToleranceOperator;
     }&lt;br/&gt;
@@ -138,10 +157,7 @@ public void testExecAndHandleNonRetriableErrorThrice() throws Exception {&lt;br/&gt;
 &lt;br/&gt;
     public void execAndHandleRetriableError(int numRetriableExceptionsThrown, long expectedWait, Exception e) throws Exception {&lt;br/&gt;
         MockTime time = new MockTime(0, 0, 0);&lt;br/&gt;
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(time);&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; props = config(RetryWithToleranceOperator.RETRY_TIMEOUT, &quot;6000&quot;);&lt;br/&gt;
-        props.put(RetryWithToleranceOperator.TOLERANCE_LIMIT, &quot;all&quot;);&lt;br/&gt;
-        retryWithToleranceOperator.configure(props);&lt;br/&gt;
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(6000, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time);&lt;br/&gt;
         retryWithToleranceOperator.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         EasyMock.expect(mockOperation.call()).andThrow(e).times(numRetriableExceptionsThrown);&lt;br/&gt;
@@ -159,10 +175,7 @@ public void execAndHandleRetriableError(int numRetriableExceptionsThrown, long e&lt;br/&gt;
 &lt;br/&gt;
     public void execAndHandleNonRetriableError(int numRetriableExceptionsThrown, long expectedWait, Exception e) throws Exception {&lt;br/&gt;
         MockTime time = new MockTime(0, 0, 0);&lt;br/&gt;
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(time);&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; props = config(RetryWithToleranceOperator.RETRY_TIMEOUT, &quot;6000&quot;);&lt;br/&gt;
-        props.put(RetryWithToleranceOperator.TOLERANCE_LIMIT, &quot;all&quot;);&lt;br/&gt;
-        retryWithToleranceOperator.configure(props);&lt;br/&gt;
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(6000, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time);&lt;br/&gt;
         retryWithToleranceOperator.metrics(errorHandlingMetrics);&lt;br/&gt;
 &lt;br/&gt;
         EasyMock.expect(mockOperation.call()).andThrow(e).times(numRetriableExceptionsThrown);&lt;br/&gt;
@@ -181,10 +194,7 @@ public void execAndHandleNonRetriableError(int numRetriableExceptionsThrown, lon&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testCheckRetryLimit() {&lt;br/&gt;
         MockTime time = new MockTime(0, 0, 0);&lt;br/&gt;
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(time);&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; props = config(RetryWithToleranceOperator.RETRY_TIMEOUT, &quot;500&quot;);&lt;br/&gt;
-        props.put(RetryWithToleranceOperator.RETRY_DELAY_MAX_MS, &quot;100&quot;);&lt;br/&gt;
-        retryWithToleranceOperator.configure(props);&lt;br/&gt;
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(500, 100, NONE, time);&lt;br/&gt;
 &lt;br/&gt;
         time.setCurrentTimeMs(100);&lt;br/&gt;
         assertTrue(retryWithToleranceOperator.checkRetry(0));&lt;br/&gt;
@@ -208,11 +218,7 @@ public void testCheckRetryLimit() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testBackoffLimit() {&lt;br/&gt;
         MockTime time = new MockTime(0, 0, 0);&lt;br/&gt;
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(time);&lt;br/&gt;
-&lt;br/&gt;
-        Map&amp;lt;String, Object&amp;gt; props = config(RetryWithToleranceOperator.RETRY_TIMEOUT, &quot;5&quot;);&lt;br/&gt;
-        props.put(RetryWithToleranceOperator.RETRY_DELAY_MAX_MS, &quot;5000&quot;);&lt;br/&gt;
-        retryWithToleranceOperator.configure(props);&lt;br/&gt;
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(5, 5000, NONE, time);&lt;br/&gt;
 &lt;br/&gt;
         long prevTs = time.hiResClockMs();&lt;br/&gt;
         retryWithToleranceOperator.backoff(1, 5000);&lt;br/&gt;
@@ -243,56 +249,54 @@ public void testBackoffLimit() {&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testToleranceLimit() {
-        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator();
-        retryWithToleranceOperator.configure(config(RetryWithToleranceOperator.TOLERANCE_LIMIT, &quot;none&quot;));
+        RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(ERRORS_RETRY_TIMEOUT_DEFAULT, ERRORS_RETRY_MAX_DELAY_DEFAULT, NONE, SYSTEM);
         retryWithToleranceOperator.metrics(errorHandlingMetrics);
         retryWithToleranceOperator.markAsFailed();
         assertFalse(&quot;should not tolerate any errors&quot;, retryWithToleranceOperator.withinToleranceLimits());
 
-        retryWithToleranceOperator = new RetryWithToleranceOperator();
-        retryWithToleranceOperator.configure(config(RetryWithToleranceOperator.TOLERANCE_LIMIT, &quot;all&quot;));
+        retryWithToleranceOperator = new RetryWithToleranceOperator(ERRORS_RETRY_TIMEOUT_DEFAULT, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, SYSTEM);
         retryWithToleranceOperator.metrics(errorHandlingMetrics);
         retryWithToleranceOperator.markAsFailed();
         retryWithToleranceOperator.markAsFailed();
         assertTrue(&quot;should tolerate all errors&quot;, retryWithToleranceOperator.withinToleranceLimits());
 
-        retryWithToleranceOperator = new RetryWithToleranceOperator();
-        retryWithToleranceOperator.configure(config(RetryWithToleranceOperator.TOLERANCE_LIMIT, &quot;none&quot;));
+        retryWithToleranceOperator = new RetryWithToleranceOperator(ERRORS_RETRY_TIMEOUT_DEFAULT, ERRORS_RETRY_MAX_DELAY_DEFAULT, NONE, SYSTEM);
         assertTrue(&quot;no tolerance is within limits if no failures&quot;, retryWithToleranceOperator.withinToleranceLimits());
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
     public void testDefaultConfigs() {
-        RetryWithToleranceOperatorConfig configuration;
-        configuration = new RetryWithToleranceOperatorConfig(new HashMap&amp;lt;&amp;gt;());
-        assertEquals(configuration.retryTimeout(), 0);
-        assertEquals(configuration.retryDelayMax(), 60000);
-        assertEquals(configuration.toleranceLimit(), ToleranceType.NONE);
+        ConnectorConfig configuration = config(emptyMap());
+        assertEquals(configuration.errorRetryTimeout(), ERRORS_RETRY_TIMEOUT_DEFAULT);
+        assertEquals(configuration.errorMaxDelayInMillis(), ERRORS_RETRY_MAX_DELAY_DEFAULT);
+        assertEquals(configuration.errorToleranceType(), ERRORS_TOLERANCE_DEFAULT);
 
         PowerMock.verifyAll();
     }&lt;br/&gt;
 &lt;br/&gt;
+    ConnectorConfig config(Map&amp;lt;String, String&amp;gt; connProps) {+        Map&amp;lt;String, String&amp;gt; props = new HashMap&amp;lt;&amp;gt;();+        props.put(ConnectorConfig.NAME_CONFIG, &quot;test&quot;);+        props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, SinkTask.class.getName());+        props.putAll(connProps);+        return new ConnectorConfig(plugins, props);+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testConfigs() {&lt;/li&gt;
	&lt;li&gt;RetryWithToleranceOperatorConfig configuration;&lt;/li&gt;
	&lt;li&gt;configuration = new RetryWithToleranceOperatorConfig(config(&quot;retry.timeout&quot;, &quot;100&quot;));&lt;/li&gt;
	&lt;li&gt;assertEquals(configuration.retryTimeout(), 100);&lt;br/&gt;
+    public void testSetConfigs() 
{
+        ConnectorConfig configuration;
+        configuration = config(singletonMap(ERRORS_RETRY_TIMEOUT_CONFIG, &quot;100&quot;));
+        assertEquals(configuration.errorRetryTimeout(), 100);
 
-        configuration = new RetryWithToleranceOperatorConfig(config(&quot;retry.delay.max.ms&quot;, &quot;100&quot;));
-        assertEquals(configuration.retryDelayMax(), 100);
+        configuration = config(singletonMap(ERRORS_RETRY_MAX_DELAY_CONFIG, &quot;100&quot;));
+        assertEquals(configuration.errorMaxDelayInMillis(), 100);
 
-        configuration = new RetryWithToleranceOperatorConfig(config(&quot;allowed.max&quot;, &quot;none&quot;));
-        assertEquals(configuration.toleranceLimit(), ToleranceType.NONE);
+        configuration = config(singletonMap(ERRORS_TOLERANCE_CONFIG, &quot;none&quot;));
+        assertEquals(configuration.errorToleranceType(), ToleranceType.NONE);
 
         PowerMock.verifyAll();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Object&amp;gt; config(String key, Object val) 
{
-        Map&amp;lt;String, Object&amp;gt; configs = new HashMap&amp;lt;&amp;gt;();
-        configs.put(key, val);
-        return configs;
-    }
&lt;p&gt;-&lt;br/&gt;
     private static class ExceptionThrower implements Operation&amp;lt;Object&amp;gt; {&lt;br/&gt;
         private Exception e;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;






&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 23 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3uffz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>