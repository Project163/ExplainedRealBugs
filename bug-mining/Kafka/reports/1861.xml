<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:08:32 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6683] ReplicaFetcher crashes with &quot;Attempted to complete a transaction which was not started&quot; </title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6683</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We have been experiencing this issue lately when restarting or replacing brokers of our Kafka clusters during maintenance operations.&lt;/p&gt;

&lt;p&gt;Having restarted or replaced a broker, after some minutes performing normally it may suddenly throw the following exception and stop replicating some partitions:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-none&quot;&gt;
2018-03-15 17:23:01,482] ERROR [ReplicaFetcher replicaId=12, leaderId=10, fetcherId=0] Error due to (kafka.server.ReplicaFetcherThread)
java.lang.IllegalArgumentException: Attempted to complete a transaction which was not started
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.ProducerStateManager.completeTxn(ProducerStateManager.scala:720)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.$anonfun$loadProducersFromLog$4(Log.scala:540)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.$anonfun$loadProducersFromLog$4$adapted(Log.scala:540)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.immutable.List.foreach(List.scala:389)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.generic.TraversableForwarder.foreach(TraversableForwarder.scala:35)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.generic.TraversableForwarder.foreach$(TraversableForwarder.scala:35)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:44)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.loadProducersFromLog(Log.scala:540)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.$anonfun$loadProducerState$5(Log.scala:521)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.$anonfun$loadProducerState$5$adapted(Log.scala:514)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.Iterator.foreach(Iterator.scala:929)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.Iterator.foreach$(Iterator.scala:929)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.IterableLike.foreach(IterableLike.scala:71)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.IterableLike.foreach$(IterableLike.scala:70)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.loadProducerState(Log.scala:514)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.$anonfun$truncateTo$2(Log.scala:1487)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.maybeHandleIOException(Log.scala:1669)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.Log.truncateTo(Log.scala:1467)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.LogManager.$anonfun$truncateTo$2(LogManager.scala:454)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.LogManager.$anonfun$truncateTo$2$adapted(LogManager.scala:445)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.immutable.Map$Map1.foreach(Map.scala:120)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.log.LogManager.truncateTo(LogManager.scala:445)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.server.ReplicaFetcherThread.$anonfun$maybeTruncate$1(ReplicaFetcherThread.scala:281)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.Iterator.foreach(Iterator.scala:929)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.Iterator.foreach$(Iterator.scala:929)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.IterableLike.foreach(IterableLike.scala:71)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.IterableLike.foreach$(IterableLike.scala:70)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.server.ReplicaFetcherThread.maybeTruncate(ReplicaFetcherThread.scala:265)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.server.AbstractFetcherThread.$anonfun$maybeTruncate$2(AbstractFetcherThread.scala:135)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:217)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.server.AbstractFetcherThread.maybeTruncate(AbstractFetcherThread.scala:132)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:102)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
[2018-03-15 17:23:01,497] INFO [ReplicaFetcher replicaId=12, leaderId=10, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As during system updates all brokers in a cluster are restarted, it happened some times the issue to manifest in different brokers holding replicas for the same partition at the same time, which caused downtime due not enough ISR replica.&lt;/p&gt;

&lt;p&gt;It is necessary to restart the faulted broker in order to recover partition replication, but after hitting this issue we often face that after restarting the broker it shuts itself down with the following error among lots of warnings due corrupted indices:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-none&quot;&gt;
[2018-03-05 16:02:22,450] ERROR There was an error in one of the threads during logs loading: org.apache.kafka.common.errors.ProducerFencedException: Invalid producer epoch: 20 (zombie): 21 (current) (kafka.log.LogManager)
[2018-03-05 16:02:22,453] FATAL [KafkaServer id=10] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
org.apache.kafka.common.errors.ProducerFencedException: Invalid producer epoch: 20 (zombie): 21 (current)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When this happened the only way to keep Kafka up has been to delete all the data inside the log directory (/var/lib/kafka in our case).&lt;/p&gt;

&lt;p&gt;The problem manifest randomly but we managed to reproduce the ReplicaFetcher crashing (although not the failed startup) out of our production cluster by doing this:&lt;br/&gt;
 &#160;1 - Setup a Kafka cluster running 3 brokers (see attached configuration): 10, 11 and 12&lt;br/&gt;
 &#160;2 - Create a topic with the following settings: Topic:mytopic2, PartitionCount:12, ReplicationFactor:3, Configs:segment.bytes=52428800,retention.ms=1800000&lt;br/&gt;
 &#160;3 - Run some producers like this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
&lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;
&#160;./kafka-producer-perf-test.sh --topic mytopic2 --record-size=2048 --producer-props bootstrap.servers=ec2-XXX-XXX-XXX-XXX.eu-west-1.compute.amazonaws.com:9092 enable.idempotence=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; --throughput 50 --num-records 6000 --transactional-id pruebatrans4 --transaction-duration-ms 100
done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;4 - Run some consumer on mytopic2.&lt;br/&gt;
&#160;5 - Wait for some time for semegments to be rotated.&lt;br/&gt;
&#160;6 - Stop broker 11, remove everything inside /var/lib/kafka, start it again.&lt;br/&gt;
&#160;7 - Wait for data to be replicated and all replicas be in ISR.&lt;br/&gt;
&#160;8 - Stop broker 12, remove everything inside /var/lib/kafka, start it again.&lt;br/&gt;
&#160;9 - Wait for data to be replicated and all replicas be in ISR.&lt;br/&gt;
&#160;10 - Wait for the issue to manifest. If it manifests, after some minutes of normal behaviour, broker 11 may suddenly stop replicating and some partitions may appear underreplicated.&lt;/p&gt;

&lt;p&gt;If replication after restarting node 12 takes long enough, node 11 may crash its ReplicaFetcher before replicas in 12 are available causing partitions to go offline. Whe have manage to reproduce the issue without deleting log data in steps 6 and 8 but it seems more likely to manifest if we do it. The broker experiencing the issue is quite random, but most of the time seems to be one of the already restarted brokers but not necessary the latest one.&lt;/p&gt;</description>
                <environment>os: GNU/Linux &lt;br/&gt;
arch: x86_64&lt;br/&gt;
Kernel: 4.9.77&lt;br/&gt;
jvm: OpenJDK 1.8.0</environment>
        <key id="13146274">KAFKA-6683</key>
            <summary>ReplicaFetcher crashes with &quot;Attempted to complete a transaction which was not started&quot; </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="chema.sanchez">Chema Sanchez</reporter>
                        <labels>
                    </labels>
                <created>Mon, 19 Mar 2018 15:38:03 +0000</created>
                <updated>Tue, 3 Apr 2018 09:59:29 +0000</updated>
                            <resolved>Fri, 23 Mar 2018 05:18:34 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>replication</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16405332" author="hachikuji" created="Mon, 19 Mar 2018 19:44:21 +0000"  >&lt;p&gt;Thanks for the report. I tried reproducing this following the steps above, but have so far been unable to. Since you are able to consistently reproduce it, it would be helpful to get a copy of the broker logs and the log data from mytopic2 on each replica following the failure. Also, if you can reproduce it on a topic with a single partition, it will be easier to debug.&lt;/p&gt;</comment>
                            <comment id="16406835" author="chema.sanchez" created="Tue, 20 Mar 2018 18:30:31 +0000"  >&lt;p&gt;Thanks to you for the response. Sadly I had no luck trying to reproduce the issue with a single partition topic.&lt;/p&gt;

&lt;p&gt;This is the full data log dir, and application logs from broker 10 I got today at debug level. The issue raised at 2018-03-20 15:17:17,148&lt;br/&gt;
&lt;a href=&quot;https://drive.google.com/file/d/19mL1drSEwDqKbkyX4MfDupr3LDIJHwfG/view?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://drive.google.com/file/d/19mL1drSEwDqKbkyX4MfDupr3LDIJHwfG/view?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is the same from broker 11, casually it experienced the same issue some minutes after broker 10, at 2018-03-20 15:19:28,132&lt;br/&gt;
&lt;a href=&quot;https://drive.google.com/file/d/1KEJvk73x1DxXQe2ob6F8V09rdASg_2FI/view?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://drive.google.com/file/d/1KEJvk73x1DxXQe2ob6F8V09rdASg_2FI/view?usp=sharing&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16409185" author="githubbot" created="Thu, 22 Mar 2018 07:53:38 +0000"  >&lt;p&gt;hachikuji opened a new pull request #4755: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6683&quot; title=&quot;ReplicaFetcher crashes with &amp;quot;Attempted to complete a transaction which was not started&amp;quot; &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6683&quot;&gt;&lt;del&gt;KAFKA-6683&lt;/del&gt;&lt;/a&gt;; Ensure producer state not mutated prior to append&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4755&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4755&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   We were unintentionally mutating the cached queue of batches prior to appending to the log. This could have several bad consequences if the append ultimately failed. In the reporter&apos;s case, it caused the snapshot to be invalid after a segment roll. The snapshot contained producer state at offsets higher than the snapshot offset. If we ever had to load from that snapshot, the state was left inconsistent, which led to an error that ultimately crashed the replica fetcher.&lt;/p&gt;

&lt;p&gt;   The fix required some refactoring to avoid sharing the same underlying queue inside `ProducerAppendInfo`. I have added test cases which reproduce the invalid snapshot state. I have also made an effort to clean up logging since it was not easy to track this problem down.&lt;/p&gt;

&lt;p&gt;   One final note: I have removed the duplicate check inside `ProducerStateManager` since it was both redundant and incorrect. The redundancy was in the checking of the cached batches: we already check these in `Log.analyzeAndValidateProducerState`. The incorrectness was the handling of sequence number overflow: we were only handling one very specific case of overflow, but others would have resulted in an invalid assertion. Instead, we now throw `OutOfOrderSequenceException`.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16410800" author="githubbot" created="Fri, 23 Mar 2018 04:42:52 +0000"  >&lt;p&gt;ijuma closed pull request #4755: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6683&quot; title=&quot;ReplicaFetcher crashes with &amp;quot;Attempted to complete a transaction which was not started&amp;quot; &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6683&quot;&gt;&lt;del&gt;KAFKA-6683&lt;/del&gt;&lt;/a&gt;; Ensure producer state not mutated prior to append&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4755&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4755&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index cc693375079..de4bb2912e9 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -166,6 +166,8 @@ class Log(@volatile var dir: File,&lt;/p&gt;

&lt;p&gt;   import kafka.log.Log._&lt;/p&gt;

&lt;p&gt;+  this.logIdent = s&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;Log partition=$topicPartition, dir=${dir.getParent}&amp;#93;&lt;/span&gt; &quot;&lt;br/&gt;
+&lt;br/&gt;
   /* A lock that guards all modifications to the log */&lt;br/&gt;
   private val lock = new Object&lt;br/&gt;
   // The memory mapped buffer for index files of this log will be closed for index files of this log will be closed with either delete() or closeHandlers()&lt;br/&gt;
@@ -242,8 +244,8 @@ class Log(@volatile var dir: File,&lt;/p&gt;

&lt;p&gt;     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info(&quot;Completed load of log %s with %d log segments, log start offset %d and log end offset %d in %d ms&quot;&lt;/li&gt;
	&lt;li&gt;.format(name, segments.size(), logStartOffset, logEndOffset, time.milliseconds - startMs))&lt;br/&gt;
+    info(s&quot;Completed load of log with ${segments.size} segments, log start offset $logStartOffset and &quot; +&lt;br/&gt;
+      s&quot;log end offset $logEndOffset in ${time.milliseconds() - startMs} ms&quot;)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private val tags = {&lt;br/&gt;
@@ -457,21 +459,20 @@ class Log(@volatile var dir: File,&lt;br/&gt;
       val unflushed = logSegments(this.recoveryPoint, Long.MaxValue).iterator&lt;br/&gt;
       while (unflushed.hasNext) {&lt;br/&gt;
         val segment = unflushed.next&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info(&quot;Recovering unflushed segment %d in log %s.&quot;.format(segment.baseOffset, name))&lt;br/&gt;
+        info(s&quot;Recovering unflushed segment ${segment.baseOffset}&quot;)&lt;br/&gt;
         val truncatedBytes =&lt;br/&gt;
           try 
{
             recoverSegment(segment, Some(_leaderEpochCache))
           }
&lt;p&gt; catch &lt;/p&gt;
{
             case _: InvalidOffsetException =&amp;gt;
               val startOffset = segment.baseOffset
-              warn(&quot;Found invalid offset during recovery for log &quot; + dir.getName + &quot;. Deleting the corrupt segment and &quot; +
-                &quot;creating an empty one with starting offset &quot; + startOffset)
+              warn(&quot;Found invalid offset during recovery. Deleting the corrupt segment and &quot; +
+                s&quot;creating an empty one with starting offset $startOffset&quot;)
               segment.truncateTo(startOffset)
           }
&lt;p&gt;         if (truncatedBytes &amp;gt; 0) {&lt;br/&gt;
           // we had an invalid message, delete all remaining log&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;warn(&quot;Corruption found in segment %d of log %s, truncating to offset %d.&quot;.format(segment.baseOffset, name,&lt;/li&gt;
	&lt;li&gt;segment.readNextOffset))&lt;br/&gt;
+          warn(s&quot;Corruption found in segment ${segment.baseOffset}, truncating to offset ${segment.readNextOffset}&quot;)&lt;br/&gt;
           unflushed.foreach(deleteSegment)&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
@@ -483,8 +484,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   private def loadProducerState(lastOffset: Long, reloadFromCleanShutdown: Boolean): Unit = lock synchronized {&lt;br/&gt;
     checkIfMemoryMappedBufferClosed()&lt;br/&gt;
     val messageFormatVersion = config.messageFormatVersion.messageFormatVersion.value&lt;/li&gt;
	&lt;li&gt;info(s&quot;Loading producer state from offset $lastOffset for partition $topicPartition with message &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;format version $messageFormatVersion&quot;)&lt;br/&gt;
+    info(s&quot;Loading producer state from offset $lastOffset with message format version $messageFormatVersion&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // We want to avoid unnecessary scanning of the log to build the producer state when the broker is being&lt;br/&gt;
     // upgraded. The basic idea is to use the absence of producer snapshot files to detect the upgrade case,&lt;br/&gt;
@@ -571,7 +571,7 @@ class Log(@volatile var dir: File,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The memory mapped buffer for index files of this log will be left open until the log is deleted.&lt;br/&gt;
    */&lt;br/&gt;
   def close() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;debug(s&quot;Closing log $name&quot;)&lt;br/&gt;
+    debug(&quot;Closing log&quot;)&lt;br/&gt;
     lock synchronized {&lt;br/&gt;
       checkIfMemoryMappedBufferClosed()&lt;br/&gt;
       maybeHandleIOException(s&quot;Error while renaming dir for $topicPartition in dir ${dir.getParent}&quot;) {&lt;br/&gt;
@@ -610,7 +610,7 @@ class Log(@volatile var dir: File,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Close file handlers used by log but don&apos;t write to disk. This is called if the log directory is offline&lt;br/&gt;
    */&lt;br/&gt;
   def closeHandlers() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;debug(s&quot;Closing handlers of log $name&quot;)&lt;br/&gt;
+    debug(&quot;Closing handlers&quot;)&lt;br/&gt;
     lock synchronized {&lt;br/&gt;
       logSegments.foreach(_.closeHandlers())&lt;br/&gt;
       isMemoryMappedBufferClosed = true&lt;br/&gt;
@@ -775,8 +775,10 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         // update the first unstable offset (which is used to compute LSO)&lt;br/&gt;
         updateFirstUnstableOffset()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;trace(s&quot;Appended message set to log ${this.name} with last offset: ${appendInfo.lastOffset}, &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;first offset: ${appendInfo.firstOffset}, next offset: ${nextOffsetMetadata.messageOffset}, and messages: $validRecords&quot;)&lt;br/&gt;
+        trace(s&quot;Appended message set with last offset: ${appendInfo.lastOffset}, &quot; +&lt;br/&gt;
+          s&quot;first offset: ${appendInfo.firstOffset}, &quot; +&lt;br/&gt;
+          s&quot;next offset: ${nextOffsetMetadata.messageOffset}, &quot; +&lt;br/&gt;
+          s&quot;and messages: $validRecords&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (unflushedMessages &amp;gt;= config.flushInterval)&lt;br/&gt;
           flush()&lt;br/&gt;
@@ -806,7 +808,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     if (updatedFirstStableOffset != this.firstUnstableOffset) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;debug(s&quot;First unstable offset for ${this.name} updated to $updatedFirstStableOffset&quot;)&lt;br/&gt;
+      debug(s&quot;First unstable offset updated to $updatedFirstStableOffset&quot;)&lt;br/&gt;
       this.firstUnstableOffset = updatedFirstStableOffset&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -822,7 +824,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
       lock synchronized {&lt;br/&gt;
         checkIfMemoryMappedBufferClosed()&lt;br/&gt;
         if (newLogStartOffset &amp;gt; logStartOffset) {&lt;/li&gt;
	&lt;li&gt;info(s&quot;Incrementing log start offset of partition $topicPartition to $newLogStartOffset in dir ${dir.getParent}&quot;)&lt;br/&gt;
+          info(s&quot;Incrementing log start offset to $newLogStartOffset&quot;)&lt;br/&gt;
           logStartOffset = newLogStartOffset&lt;br/&gt;
           _leaderEpochCache.clearAndFlushEarliest(logStartOffset)&lt;br/&gt;
           producerStateManager.truncateHead(logStartOffset)&lt;br/&gt;
@@ -839,12 +841,13 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     for (batch &amp;lt;- records.batches.asScala if batch.hasProducerId) {&lt;br/&gt;
       val maybeLastEntry = producerStateManager.lastEntry(batch.producerId)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// if this is a client produce request, there will be upto 5 batches which could have been duplicated.&lt;br/&gt;
+      // if this is a client produce request, there will be up to 5 batches which could have been duplicated.&lt;br/&gt;
       // If we find a duplicate, we return the metadata of the appended batch to the client.&lt;/li&gt;
	&lt;li&gt;if (isFromClient)&lt;/li&gt;
	&lt;li&gt;maybeLastEntry.flatMap(_.duplicateOf(batch)).foreach { duplicate =&amp;gt;&lt;br/&gt;
+      if (isFromClient) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach { duplicate =&amp;gt;
           return (updatedProducers, completedTxns.toList, Some(duplicate))
         }+      }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       val maybeCompletedTxn = updateProducers(batch, updatedProducers, isFromClient = isFromClient)&lt;br/&gt;
       maybeCompletedTxn.foreach(completedTxns += _)&lt;br/&gt;
@@ -990,7 +993,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   def read(startOffset: Long, maxLength: Int, maxOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None, minOneMessage: Boolean = false,&lt;br/&gt;
            isolationLevel: IsolationLevel): FetchDataInfo = {&lt;br/&gt;
     maybeHandleIOException(s&quot;Exception while reading from $topicPartition in dir ${dir.getParent}&quot;) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;trace(&quot;Reading %d bytes from offset %d in log %s of length %d bytes&quot;.format(maxLength, startOffset, name, size))&lt;br/&gt;
+      trace(s&quot;Reading $maxLength bytes from offset $startOffset of length $size bytes&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Because we don&apos;t use lock for reading, the synchronization is a little bit tricky.&lt;br/&gt;
       // We create the local variables to avoid race conditions with updates to the log.&lt;br/&gt;
@@ -1306,7 +1309,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     val maxOffsetInMessages = appendInfo.lastOffset&lt;/p&gt;

&lt;p&gt;     if (segment.shouldRoll(messagesSize, maxTimestampInMessages, maxOffsetInMessages, now)) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;debug(s&quot;Rolling new log segment in $name (log_size = ${segment.size}/${config.segmentSize}}, &quot; +&lt;br/&gt;
+      debug(s&quot;Rolling new log segment (log_size = ${segment.size}/${config.segmentSize}}, &quot; +&lt;br/&gt;
           s&quot;offset_index_size = ${segment.offsetIndex.entries}/${segment.offsetIndex.maxEntries}, &quot; +&lt;br/&gt;
           s&quot;time_index_size = ${segment.timeIndex.entries}/${segment.timeIndex.maxEntries}, &quot; +&lt;br/&gt;
           s&quot;inactive_time_ms = ${segment.timeWaitedForRoll(now, maxTimestampInMessages)}/${config.segmentMs - segment.rollJitterMs}).&quot;)&lt;br/&gt;
@@ -1379,7 +1382,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         // schedule an asynchronous flush of the old segment&lt;br/&gt;
         scheduler.schedule(&quot;flush-log&quot;, () =&amp;gt; flush(newOffset), delay = 0L)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info(s&quot;Rolled new log segment for &apos;$name&apos; in ${time.hiResClockMs() - start} ms.&quot;)&lt;br/&gt;
+        info(s&quot;Rolled new log segment at offset $newOffset in ${time.hiResClockMs() - start} ms.&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         segment&lt;br/&gt;
       }&lt;br/&gt;
@@ -1389,7 +1392,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The number of messages appended to the log since the last flush&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def unflushedMessages() = this.logEndOffset - this.recoveryPoint&lt;br/&gt;
+  def unflushedMessages: Long = this.logEndOffset - this.recoveryPoint&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Flush all log segments&lt;br/&gt;
@@ -1405,8 +1408,8 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     maybeHandleIOException(s&quot;Error while flushing log for $topicPartition in dir ${dir.getParent} with offset $offset&quot;) {&lt;br/&gt;
       if (offset &amp;lt;= this.recoveryPoint)&lt;br/&gt;
         return&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;debug(&quot;Flushing log &apos;&quot; + name + &quot; up to offset &quot; + offset + &quot;, last flushed: &quot; + lastFlushTime + &quot; current time: &quot; +&lt;/li&gt;
	&lt;li&gt;time.milliseconds + &quot; unflushed = &quot; + unflushedMessages)&lt;br/&gt;
+      debug(s&quot;Flushing log up to offset $offset, last flushed: $lastFlushTime,  current time: ${time.milliseconds()}, &quot; +&lt;br/&gt;
+        s&quot;unflushed: $unflushedMessages&quot;)&lt;br/&gt;
       for (segment &amp;lt;- logSegments(this.recoveryPoint, offset))&lt;br/&gt;
         segment.flush()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1504,12 +1507,12 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def truncateTo(targetOffset: Long): Boolean = {&lt;br/&gt;
     maybeHandleIOException(s&quot;Error while truncating log to offset $targetOffset for $topicPartition in dir ${dir.getParent}&quot;) {&lt;br/&gt;
       if (targetOffset &amp;lt; 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;throw new IllegalArgumentException(&quot;Cannot truncate to a negative offset (%d).&quot;.format(targetOffset))&lt;br/&gt;
+        throw new IllegalArgumentException(s&quot;Cannot truncate partition $topicPartition to a negative offset (%d).&quot;.format(targetOffset))&lt;br/&gt;
       if (targetOffset &amp;gt;= logEndOffset) {&lt;/li&gt;
	&lt;li&gt;info(&quot;Truncating %s to %d has no effect as the largest offset in the log is %d.&quot;.format(name, targetOffset, logEndOffset - 1))&lt;br/&gt;
+        info(s&quot;Truncating to $targetOffset has no effect as the largest offset in the log is ${logEndOffset - 1}&quot;)&lt;br/&gt;
         false&lt;br/&gt;
       } else {&lt;/li&gt;
	&lt;li&gt;info(&quot;Truncating log %s to offset %d.&quot;.format(name, targetOffset))&lt;br/&gt;
+        info(s&quot;Truncating to offset $targetOffset&quot;)&lt;br/&gt;
         lock synchronized {&lt;br/&gt;
           checkIfMemoryMappedBufferClosed()&lt;br/&gt;
           if (segments.firstEntry.getValue.baseOffset &amp;gt; targetOffset) {&lt;br/&gt;
@@ -1537,7 +1540,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def truncateFullyAndStartAt(newOffset: Long) {&lt;br/&gt;
     maybeHandleIOException(s&quot;Error while truncating the entire log for $topicPartition in dir ${dir.getParent}&quot;) {&lt;/li&gt;
	&lt;li&gt;debug(s&quot;Truncate and start log &apos;$name&apos; at offset $newOffset&quot;)&lt;br/&gt;
+      debug(s&quot;Truncate and start at offset $newOffset&quot;)&lt;br/&gt;
       lock synchronized {&lt;br/&gt;
         checkIfMemoryMappedBufferClosed()&lt;br/&gt;
         val segmentsToDelete = logSegments.toList&lt;br/&gt;
@@ -1608,7 +1611,7 @@ class Log(@volatile var dir: File,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param segment The log segment to schedule for deletion&lt;br/&gt;
    */&lt;br/&gt;
   private def deleteSegment(segment: LogSegment) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info(s&quot;Scheduling log segment &lt;span class=&quot;error&quot;&gt;&amp;#91;baseOffset ${segment.baseOffset}, size ${segment.size}&amp;#93;&lt;/span&gt; for log $name for deletion.&quot;)&lt;br/&gt;
+    info(s&quot;Scheduling log segment &lt;span class=&quot;error&quot;&gt;&amp;#91;baseOffset ${segment.baseOffset}, size ${segment.size}&amp;#93;&lt;/span&gt; for deletion.&quot;)&lt;br/&gt;
     lock synchronized {&lt;br/&gt;
       segments.remove(segment.baseOffset)&lt;br/&gt;
       asyncDeleteSegment(segment)&lt;br/&gt;
@@ -1626,7 +1629,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
   private def asyncDeleteSegment(segment: LogSegment) {&lt;br/&gt;
     segment.changeFileSuffixes(&quot;&quot;, Log.DeletedFileSuffix)&lt;br/&gt;
     def deleteSeg() {&lt;/li&gt;
	&lt;li&gt;info(s&quot;Deleting segment ${segment.baseOffset} from log $name.&quot;)&lt;br/&gt;
+      info(s&quot;Deleting segment ${segment.baseOffset}&quot;)&lt;br/&gt;
       maybeHandleIOException(s&quot;Error while deleting segments for $topicPartition in dir ${dir.getParent}&quot;) 
{
         segment.deleteIfExists()
       }
&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/ProducerStateManager.scala b/core/src/main/scala/kafka/log/ProducerStateManager.scala&lt;br/&gt;
index da84b19abdf..d2c3b39c25c 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/ProducerStateManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/ProducerStateManager.scala&lt;br/&gt;
@@ -70,9 +70,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; case class TxnMetadata(producerId: Long, var firstOffset: LogOffset&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; object ProducerIdEntry {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; object ProducerStateEntry &lt;/p&gt;
{
   private[log] val NumBatchesToRetain = 5
-  def empty(producerId: Long) = new ProducerIdEntry(producerId, mutable.Queue[BatchMetadata](), RecordBatch.NO_PRODUCER_EPOCH, -1, None)
+  def empty(producerId: Long) = new ProducerStateEntry(producerId, mutable.Queue[BatchMetadata](), RecordBatch.NO_PRODUCER_EPOCH, -1, None)
 }

&lt;p&gt; private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; case class BatchMetadata(lastSeq: Int, lastOffset: Long, offsetDelta: Int, timestamp: Long) &lt;/p&gt;
{
@@ -90,27 +90,31 @@ private[log] case class BatchMetadata(lastSeq: Int, lastOffset: Long, offsetDelt
 }

&lt;p&gt; // the batchMetadata is ordered such that the batch with the lowest sequence is at the head of the queue while the&lt;br/&gt;
-// batch with the highest sequence is at the tail of the queue. We will retain at most ProducerIdEntry.NumBatchesToRetain&lt;br/&gt;
+// batch with the highest sequence is at the tail of the queue. We will retain at most ProducerStateEntry.NumBatchesToRetain&lt;br/&gt;
 // elements in the queue. When the queue is at capacity, we remove the first element to make space for the incoming batch.&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerIdEntry(val producerId: Long, val batchMetadata: mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;BatchMetadata&amp;#93;&lt;/span&gt;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var producerEpoch: Short, var coordinatorEpoch: Int,&lt;/li&gt;
	&lt;li&gt;var currentTxnFirstOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerStateEntry(val producerId: Long,&lt;br/&gt;
+                                      val batchMetadata: mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;BatchMetadata&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                      var producerEpoch: Short,&lt;br/&gt;
+                                      var coordinatorEpoch: Int,&lt;br/&gt;
+                                      var currentTxnFirstOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;) {&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def firstSeq: Int = if (batchMetadata.isEmpty) RecordBatch.NO_SEQUENCE else batchMetadata.front.firstSeq&lt;/li&gt;
	&lt;li&gt;def firstOffset: Long = if (batchMetadata.isEmpty) -1L else batchMetadata.front.firstOffset&lt;br/&gt;
+  def firstSeq: Int = if (isEmpty) RecordBatch.NO_SEQUENCE else batchMetadata.front.firstSeq&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def lastSeq: Int = if (batchMetadata.isEmpty) RecordBatch.NO_SEQUENCE else batchMetadata.last.lastSeq&lt;/li&gt;
	&lt;li&gt;def lastDataOffset: Long = if (batchMetadata.isEmpty) -1L else batchMetadata.last.lastOffset&lt;/li&gt;
	&lt;li&gt;def lastTimestamp = if (batchMetadata.isEmpty) RecordBatch.NO_TIMESTAMP else batchMetadata.last.timestamp&lt;/li&gt;
	&lt;li&gt;def lastOffsetDelta : Int = if (batchMetadata.isEmpty) 0 else batchMetadata.last.offsetDelta&lt;br/&gt;
+  def firstOffset: Long = if (isEmpty) -1L else batchMetadata.front.firstOffset&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def addBatchMetadata(producerEpoch: Short, lastSeq: Int, lastOffset: Long, offsetDelta: Int, timestamp: Long) = {&lt;/li&gt;
	&lt;li&gt;maybeUpdateEpoch(producerEpoch)&lt;br/&gt;
+  def lastSeq: Int = if (isEmpty) RecordBatch.NO_SEQUENCE else batchMetadata.last.lastSeq&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (batchMetadata.size == ProducerIdEntry.NumBatchesToRetain)&lt;/li&gt;
	&lt;li&gt;batchMetadata.dequeue()&lt;br/&gt;
+  def lastDataOffset: Long = if (isEmpty) -1L else batchMetadata.last.lastOffset&lt;br/&gt;
+&lt;br/&gt;
+  def lastTimestamp = if (isEmpty) RecordBatch.NO_TIMESTAMP else batchMetadata.last.timestamp&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;batchMetadata.enqueue(BatchMetadata(lastSeq, lastOffset, offsetDelta, timestamp))&lt;br/&gt;
+  def lastOffsetDelta : Int = if (isEmpty) 0 else batchMetadata.last.offsetDelta&lt;br/&gt;
+&lt;br/&gt;
+  def isEmpty: Boolean = batchMetadata.isEmpty&lt;br/&gt;
+&lt;br/&gt;
+  def addBatch(producerEpoch: Short, lastSeq: Int, lastOffset: Long, offsetDelta: Int, timestamp: Long): Unit = 
{
+    maybeUpdateEpoch(producerEpoch)
+    addBatchMetadata(BatchMetadata(lastSeq, lastOffset, offsetDelta, timestamp))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def maybeUpdateEpoch(producerEpoch: Short): Boolean = &lt;/p&gt;
{
@@ -123,25 +127,39 @@ private[log] class ProducerIdEntry(val producerId: Long, val batchMetadata: muta
     }
&lt;p&gt;   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def removeBatchesOlderThan(offset: Long) = batchMetadata.dropWhile(_.lastOffset &amp;lt; offset)&lt;br/&gt;
+  private def addBatchMetadata(batch: BatchMetadata): Unit = 
{
+    if (batchMetadata.size == ProducerStateEntry.NumBatchesToRetain)
+      batchMetadata.dequeue()
+    batchMetadata.enqueue(batch)
+  }
&lt;p&gt;+&lt;br/&gt;
+  def update(nextEntry: ProducerStateEntry): Unit = &lt;/p&gt;
{
+    maybeUpdateEpoch(nextEntry.producerEpoch)
+    while (nextEntry.batchMetadata.nonEmpty)
+      addBatchMetadata(nextEntry.batchMetadata.dequeue())
+    this.coordinatorEpoch = nextEntry.coordinatorEpoch
+    this.currentTxnFirstOffset = nextEntry.currentTxnFirstOffset
+  }
&lt;p&gt;+&lt;br/&gt;
+  def removeBatchesOlderThan(offset: Long): Unit = batchMetadata.dropWhile(_.lastOffset &amp;lt; offset)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def duplicateOf(batch: RecordBatch): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;BatchMetadata&amp;#93;&lt;/span&gt; = {&lt;/li&gt;
	&lt;li&gt;if (batch.producerEpoch() != producerEpoch)&lt;br/&gt;
+  def findDuplicateBatch(batch: RecordBatch): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;BatchMetadata&amp;#93;&lt;/span&gt; = 
{
+    if (batch.producerEpoch != producerEpoch)
        None
     else
-      batchWithSequenceRange(batch.baseSequence(), batch.lastSequence())
+      batchWithSequenceRange(batch.baseSequence, batch.lastSequence)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   // Return the batch metadata of the cached batch having the exact sequence range, if any.&lt;br/&gt;
   def batchWithSequenceRange(firstSeq: Int, lastSeq: Int): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;BatchMetadata&amp;#93;&lt;/span&gt; = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val duplicate = batchMetadata.filter 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: { case(metadata) =&amp;gt;+    val duplicate = batchMetadata.filter { metadata =&amp;gt;
       firstSeq == metadata.firstSeq &amp;amp;&amp;amp; lastSeq == metadata.lastSeq
     }     duplicate.headOption   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override def toString: String = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;ProducerIdEntry(&quot; +&lt;br/&gt;
+    &quot;ProducerStateEntry(&quot; +&lt;br/&gt;
       s&quot;producerId=$producerId, &quot; +&lt;br/&gt;
       s&quot;producerEpoch=$producerEpoch, &quot; +&lt;br/&gt;
       s&quot;currentTxnFirstOffset=$currentTxnFirstOffset, &quot; +&lt;br/&gt;
@@ -159,7 +177,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerIdEntry(val producerId: Long, val batchMetadata: muta&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param producerId The id of the producer appending to the log&lt;/li&gt;
	&lt;li&gt;@param currentEntry  The current entry associated with the producer id which contains metadata for a fixed number of&lt;/li&gt;
	&lt;li&gt;the most recent appends made by the producer. Validation of the first incoming append will&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*                      be made against the lastest append in the current entry. New appends will replace older appends&lt;br/&gt;
+ *                      be made against the latest append in the current entry. New appends will replace older appends&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;in the current entry so that the space overhead is constant.&lt;/li&gt;
	&lt;li&gt;@param validationType Indicates the extent of validation to perform on the appends on this instance. Offset commits&lt;/li&gt;
	&lt;li&gt;coming from the producer should have ValidationType.EpochOnly. Appends which aren&apos;t from a client&lt;br/&gt;
@@ -167,66 +185,67 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerIdEntry(val producerId: Long, val batchMetadata: muta&lt;/li&gt;
	&lt;li&gt;ValidationType.Full.&lt;br/&gt;
  */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerAppendInfo(val producerId: Long,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;currentEntry: ProducerIdEntry,&lt;/li&gt;
	&lt;li&gt;validationType: ValidationType) {&lt;br/&gt;
-&lt;br/&gt;
+                                      val currentEntry: ProducerStateEntry,&lt;br/&gt;
+                                      val validationType: ValidationType) {&lt;br/&gt;
   private val transactions = ListBuffer.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TxnMetadata&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  private val updatedEntry = ProducerStateEntry.empty(producerId)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def maybeValidateAppend(producerEpoch: Short, firstSeq: Int, lastSeq: Int) = {&lt;br/&gt;
+  updatedEntry.producerEpoch = currentEntry.producerEpoch&lt;br/&gt;
+  updatedEntry.coordinatorEpoch = currentEntry.coordinatorEpoch&lt;br/&gt;
+  updatedEntry.currentTxnFirstOffset = currentEntry.currentTxnFirstOffset&lt;br/&gt;
+&lt;br/&gt;
+  private def maybeValidateAppend(producerEpoch: Short, firstSeq: Int) = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     validationType match {
       case ValidationType.None =&amp;gt;
 
       case ValidationType.EpochOnly =&amp;gt;
-        checkEpoch(producerEpoch)
+        checkProducerEpoch(producerEpoch)
 
       case ValidationType.Full =&amp;gt;
-        checkEpoch(producerEpoch)
-        checkSequence(producerEpoch, firstSeq, lastSeq)
+        checkProducerEpoch(producerEpoch)
+        checkSequence(producerEpoch, firstSeq)
     }   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def checkEpoch(producerEpoch: Short): Unit = {&lt;/li&gt;
	&lt;li&gt;if (isFenced(producerEpoch)) {&lt;br/&gt;
+  private def checkProducerEpoch(producerEpoch: Short): Unit = {&lt;br/&gt;
+    if (producerEpoch &amp;lt; updatedEntry.producerEpoch) {&lt;br/&gt;
       throw new ProducerFencedException(s&quot;Producer&apos;s epoch is no longer valid. There is probably another producer &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;with a newer epoch. $producerEpoch (request epoch), ${currentEntry.producerEpoch} (server epoch)&quot;)&lt;br/&gt;
+        s&quot;with a newer epoch. $producerEpoch (request epoch), ${updatedEntry.producerEpoch} (server epoch)&quot;)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def checkSequence(producerEpoch: Short, firstSeq: Int, lastSeq: Int): Unit = {&lt;/li&gt;
	&lt;li&gt;if (producerEpoch != currentEntry.producerEpoch) {&lt;/li&gt;
	&lt;li&gt;if (firstSeq != 0) {&lt;/li&gt;
	&lt;li&gt;if (currentEntry.producerEpoch != RecordBatch.NO_PRODUCER_EPOCH) {&lt;br/&gt;
+  private def checkSequence(producerEpoch: Short, appendFirstSeq: Int): Unit = {&lt;br/&gt;
+    if (producerEpoch != updatedEntry.producerEpoch) {&lt;br/&gt;
+      if (appendFirstSeq != 0) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        if (updatedEntry.producerEpoch != RecordBatch.NO_PRODUCER_EPOCH) {
           throw new OutOfOrderSequenceException(s&quot;Invalid sequence number for new epoch: $producerEpoch &quot; +
-            s&quot;(request epoch), $firstSeq (seq. number)&quot;)
+            s&quot;(request epoch), $appendFirstSeq (seq. number)&quot;)
         } else {
           throw new UnknownProducerIdException(s&quot;Found no record of producerId=$producerId on the broker. It is possible &quot; +
             s&quot;that the last message with the producerId=$producerId has been removed due to hitting the retention limit.&quot;)
         }       }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;} else if (currentEntry.lastSeq == RecordBatch.NO_SEQUENCE &amp;amp;&amp;amp; firstSeq != 0) 
{
-      // the epoch was bumped by a control record, so we expect the sequence number to be reset
-      throw new OutOfOrderSequenceException(s&quot;Out of order sequence number for producerId $producerId: found $firstSeq &quot; +
-        s&quot;(incoming seq. number), but expected 0&quot;)
-    }
&lt;p&gt; else if (isDuplicate(firstSeq, lastSeq)) &lt;/p&gt;
{
-      throw new DuplicateSequenceException(s&quot;Duplicate sequence number for producerId $producerId: (incomingBatch.firstSeq, &quot; +
-        s&quot;incomingBatch.lastSeq): ($firstSeq, $lastSeq).&quot;)
-    }
&lt;p&gt; else if (!inSequence(firstSeq, lastSeq)) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;throw new OutOfOrderSequenceException(s&quot;Out of order sequence number for producerId $producerId: $firstSeq &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;(incoming seq. number), ${currentEntry.lastSeq} (current end sequence number)&quot;)&lt;br/&gt;
+    } else 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      val currentLastSeq = if (!updatedEntry.isEmpty)+        updatedEntry.lastSeq+      else if (producerEpoch == currentEntry.producerEpoch)+        currentEntry.lastSeq+      else+        RecordBatch.NO_SEQUENCE++      if (currentLastSeq == RecordBatch.NO_SEQUENCE &amp;amp;&amp;amp; appendFirstSeq != 0) {
+        // the epoch was bumped by a control record, so we expect the sequence number to be reset
+        throw new OutOfOrderSequenceException(s&quot;Out of order sequence number for producerId $producerId: found $appendFirstSeq &quot; +
+          s&quot;(incoming seq. number), but expected 0&quot;)
+      } else if (!inSequence(currentLastSeq, appendFirstSeq)) {
+        throw new OutOfOrderSequenceException(s&quot;Out of order sequence number for producerId $producerId: $appendFirstSeq &quot; +
+          s&quot;(incoming seq. number), $currentLastSeq (current end sequence number)&quot;)
+      }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def isDuplicate(firstSeq: Int, lastSeq: Int): Boolean = 
{
-    ((lastSeq != 0 &amp;amp;&amp;amp; currentEntry.firstSeq != Int.MaxValue &amp;amp;&amp;amp; lastSeq &amp;lt; currentEntry.firstSeq)
-      || currentEntry.batchWithSequenceRange(firstSeq, lastSeq).isDefined)
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def inSequence(firstSeq: Int, lastSeq: Int): Boolean = 
{
-    firstSeq == currentEntry.lastSeq + 1L || (firstSeq == 0 &amp;amp;&amp;amp; currentEntry.lastSeq == Int.MaxValue)
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private def isFenced(producerEpoch: Short): Boolean = {&lt;/li&gt;
	&lt;li&gt;producerEpoch &amp;lt; currentEntry.producerEpoch&lt;br/&gt;
+  private def inSequence(lastSeq: Int, nextSeq: Int): Boolean = 
{
+    nextSeq == lastSeq + 1L || (nextSeq == 0 &amp;amp;&amp;amp; lastSeq == Int.MaxValue)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def append(batch: RecordBatch): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;CompletedTxn&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
@@ -248,17 +267,21 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerAppendInfo(val producerId: Long,&lt;br/&gt;
              lastTimestamp: Long,&lt;br/&gt;
              lastOffset: Long,&lt;br/&gt;
              isTransactional: Boolean): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;maybeValidateAppend(epoch, firstSeq, lastSeq)&lt;br/&gt;
+    maybeValidateAppend(epoch, firstSeq)&lt;br/&gt;
+    updatedEntry.addBatch(epoch, lastSeq, lastOffset, lastSeq - firstSeq, lastTimestamp)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;currentEntry.addBatchMetadata(epoch, lastSeq, lastOffset, lastSeq - firstSeq, lastTimestamp)&lt;br/&gt;
+    updatedEntry.currentTxnFirstOffset match {&lt;br/&gt;
+      case Some(_) if !isTransactional =&amp;gt;&lt;br/&gt;
+        // Received a non-transactional message while a transaction is active&lt;br/&gt;
+        throw new InvalidTxnStateException(s&quot;Expected transactional write from producer $producerId&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (currentEntry.currentTxnFirstOffset.isDefined &amp;amp;&amp;amp; !isTransactional)&lt;/li&gt;
	&lt;li&gt;throw new InvalidTxnStateException(s&quot;Expected transactional write from producer $producerId&quot;)&lt;br/&gt;
+      case None if isTransactional =&amp;gt;&lt;br/&gt;
+        // Began a new transaction&lt;br/&gt;
+        val firstOffset = lastOffset - (lastSeq - firstSeq)&lt;br/&gt;
+        updatedEntry.currentTxnFirstOffset = Some(firstOffset)&lt;br/&gt;
+        transactions += new TxnMetadata(producerId, firstOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (isTransactional &amp;amp;&amp;amp; currentEntry.currentTxnFirstOffset.isEmpty) 
{
-      val firstOffset = lastOffset - (lastSeq - firstSeq)
-      currentEntry.currentTxnFirstOffset = Some(firstOffset)
-      transactions += new TxnMetadata(producerId, firstOffset)
+      case _ =&amp;gt; // nothing to do
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -266,28 +289,27 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerAppendInfo(val producerId: Long,&lt;br/&gt;
                          producerEpoch: Short,&lt;br/&gt;
                          offset: Long,&lt;br/&gt;
                          timestamp: Long): CompletedTxn = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (isFenced(producerEpoch))&lt;/li&gt;
	&lt;li&gt;throw new ProducerFencedException(s&quot;Invalid producer epoch: $producerEpoch (zombie): ${currentEntry.producerEpoch} (current)&quot;)&lt;br/&gt;
+    checkProducerEpoch(producerEpoch)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (currentEntry.coordinatorEpoch &amp;gt; endTxnMarker.coordinatorEpoch)&lt;br/&gt;
+    if (updatedEntry.coordinatorEpoch &amp;gt; endTxnMarker.coordinatorEpoch)&lt;br/&gt;
       throw new TransactionCoordinatorFencedException(s&quot;Invalid coordinator epoch: ${endTxnMarker.coordinatorEpoch} &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;(zombie), ${currentEntry.coordinatorEpoch} (current)&quot;)&lt;br/&gt;
+        s&quot;(zombie), ${updatedEntry.coordinatorEpoch} (current)&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;currentEntry.maybeUpdateEpoch(producerEpoch)&lt;br/&gt;
+    updatedEntry.maybeUpdateEpoch(producerEpoch)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val firstOffset = currentEntry.currentTxnFirstOffset match {&lt;br/&gt;
+    val firstOffset = updatedEntry.currentTxnFirstOffset match 
{
       case Some(txnFirstOffset) =&amp;gt; txnFirstOffset
       case None =&amp;gt;
         transactions += new TxnMetadata(producerId, offset)
         offset
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;currentEntry.currentTxnFirstOffset = None&lt;/li&gt;
	&lt;li&gt;currentEntry.coordinatorEpoch = endTxnMarker.coordinatorEpoch&lt;br/&gt;
+    updatedEntry.currentTxnFirstOffset = None&lt;br/&gt;
+    updatedEntry.coordinatorEpoch = endTxnMarker.coordinatorEpoch&lt;br/&gt;
     CompletedTxn(producerId, firstOffset, offset, endTxnMarker.controlType == ControlRecordType.ABORT)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def latestEntry: ProducerIdEntry = currentEntry&lt;br/&gt;
+  def toEntry: ProducerStateEntry = updatedEntry&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def startedTransactions: List&lt;span class=&quot;error&quot;&gt;&amp;#91;TxnMetadata&amp;#93;&lt;/span&gt; = transactions.toList&lt;/p&gt;

&lt;p&gt;@@ -306,11 +328,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerAppendInfo(val producerId: Long,&lt;br/&gt;
   override def toString: String = {&lt;br/&gt;
     &quot;ProducerAppendInfo(&quot; +&lt;br/&gt;
       s&quot;producerId=$producerId, &quot; +&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;s&quot;producerEpoch=${currentEntry.producerEpoch}, &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;firstSequence=${currentEntry.firstSeq}, &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;lastSequence=${currentEntry.lastSeq}, &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;currentTxnFirstOffset=${currentEntry.currentTxnFirstOffset}, &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;coordinatorEpoch=${currentEntry.coordinatorEpoch}, &quot; +&lt;br/&gt;
+      s&quot;producerEpoch=${updatedEntry.producerEpoch}, &quot; +&lt;br/&gt;
+      s&quot;firstSequence=${updatedEntry.firstSeq}, &quot; +&lt;br/&gt;
+      s&quot;lastSequence=${updatedEntry.lastSeq}, &quot; +&lt;br/&gt;
+      s&quot;currentTxnFirstOffset=${updatedEntry.currentTxnFirstOffset}, &quot; +&lt;br/&gt;
+      s&quot;coordinatorEpoch=${updatedEntry.coordinatorEpoch}, &quot; +&lt;br/&gt;
       s&quot;startedTransactions=$transactions)&quot;&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
@@ -347,7 +369,7 @@ object ProducerStateManager {&lt;br/&gt;
     new Field(CrcField, Type.UNSIGNED_INT32, &quot;CRC of the snapshot data&quot;),&lt;br/&gt;
     new Field(ProducerEntriesField, new ArrayOf(ProducerSnapshotEntrySchema), &quot;The entries in the producer table&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def readSnapshot(file: File): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerIdEntry&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+  def readSnapshot(file: File): Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerStateEntry&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     try {&lt;br/&gt;
       val buffer = Files.readAllBytes(file.toPath)&lt;br/&gt;
       val struct = PidSnapshotMapSchema.read(ByteBuffer.wrap(buffer))&lt;br/&gt;
@@ -372,7 +394,7 @@ object ProducerStateManager 
{
         val offsetDelta = producerEntryStruct.getInt(OffsetDeltaField)
         val coordinatorEpoch = producerEntryStruct.getInt(CoordinatorEpochField)
         val currentTxnFirstOffset = producerEntryStruct.getLong(CurrentTxnFirstOffsetField)
-        val newEntry = new ProducerIdEntry(producerId, mutable.Queue[BatchMetadata](BatchMetadata(seq, offset, offsetDelta, timestamp)), producerEpoch,
+        val newEntry = new ProducerStateEntry(producerId, mutable.Queue[BatchMetadata](BatchMetadata(seq, offset, offsetDelta, timestamp)), producerEpoch,
           coordinatorEpoch, if (currentTxnFirstOffset &amp;gt;= 0) Some(currentTxnFirstOffset) else None)
         newEntry
       }
&lt;p&gt;@@ -382,7 +404,7 @@ object ProducerStateManager {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def writeSnapshot(file: File, entries: mutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, ProducerIdEntry&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
+  private def writeSnapshot(file: File, entries: mutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, ProducerStateEntry&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
     val struct = new Struct(PidSnapshotMapSchema)&lt;br/&gt;
     struct.set(VersionField, ProducerSnapshotVersion)&lt;br/&gt;
     struct.set(CrcField, 0L) // we&apos;ll fill this after writing the entries&lt;br/&gt;
@@ -462,7 +484,9 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
   import ProducerStateManager._&lt;br/&gt;
   import java.util&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private val producers = mutable.Map.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, ProducerIdEntry&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  this.logIdent = s&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerStateManager partition=$topicPartition&amp;#93;&lt;/span&gt; &quot;&lt;br/&gt;
+&lt;br/&gt;
+  private val producers = mutable.Map.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, ProducerStateEntry&amp;#93;&lt;/span&gt;&lt;br/&gt;
   private var lastMapOffset = 0L&lt;br/&gt;
   private var lastSnapOffset = 0L&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -512,7 +536,7 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Get a copy of the active producers&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def activeProducers: immutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, ProducerIdEntry&amp;#93;&lt;/span&gt; = producers.toMap&lt;br/&gt;
+  def activeProducers: immutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, ProducerStateEntry&amp;#93;&lt;/span&gt; = producers.toMap&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def isEmpty: Boolean = producers.isEmpty &amp;amp;&amp;amp; unreplicatedTxns.isEmpty&lt;/p&gt;

&lt;p&gt;@@ -521,7 +545,7 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
       latestSnapshotFile match {&lt;br/&gt;
         case Some(file) =&amp;gt;&lt;br/&gt;
           try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;info(s&quot;Loading producer state from snapshot file &apos;$file&apos; for partition $topicPartition&quot;)&lt;br/&gt;
+            info(s&quot;Loading producer state from snapshot file &apos;$file&apos;&quot;)&lt;br/&gt;
             val loadedProducers = readSnapshot(file).filter 
{ producerEntry =&amp;gt;
               isProducerRetained(producerEntry, logStartOffset) &amp;amp;&amp;amp; !isProducerExpired(currentTime, producerEntry)
             }
&lt;p&gt;@@ -543,7 +567,7 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   // visible for testing&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def loadProducerEntry(entry: ProducerIdEntry): Unit = {&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; def loadProducerEntry(entry: ProducerStateEntry): Unit = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     val producerId = entry.producerId     producers.put(producerId, entry)     entry.currentTxnFirstOffset.foreach { offset =&amp;gt;
@@ -551,8 +575,8 @@ class ProducerStateManager(val topicPartition: TopicPartition,
     }   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def isProducerExpired(currentTimeMs: Long, producerIdEntry: ProducerIdEntry): Boolean =&lt;/li&gt;
	&lt;li&gt;producerIdEntry.currentTxnFirstOffset.isEmpty &amp;amp;&amp;amp; currentTimeMs - producerIdEntry.lastTimestamp &amp;gt;= maxProducerIdExpirationMs&lt;br/&gt;
+  private def isProducerExpired(currentTimeMs: Long, producerState: ProducerStateEntry): Boolean =&lt;br/&gt;
+    producerState.currentTxnFirstOffset.isEmpty &amp;amp;&amp;amp; currentTimeMs - producerState.lastTimestamp &amp;gt;= maxProducerIdExpirationMs&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Expire any producer ids which have been idle longer than the configured maximum expiration timeout.&lt;br/&gt;
@@ -596,7 +620,8 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
       else&lt;br/&gt;
         ValidationType.Full&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new ProducerAppendInfo(producerId, lastEntry(producerId).getOrElse(ProducerIdEntry.empty(producerId)), validationToPerform)&lt;br/&gt;
+    val currentEntry = lastEntry(producerId).getOrElse(ProducerStateEntry.empty(producerId))&lt;br/&gt;
+    new ProducerAppendInfo(producerId, currentEntry, validationToPerform)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -604,12 +629,19 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
    */&lt;br/&gt;
   def update(appendInfo: ProducerAppendInfo): Unit = {&lt;br/&gt;
     if (appendInfo.producerId == RecordBatch.NO_PRODUCER_ID)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;throw new IllegalArgumentException(s&quot;Invalid producer id ${appendInfo.producerId} passed to update&quot;)&lt;br/&gt;
+      throw new IllegalArgumentException(s&quot;Invalid producer id ${appendInfo.producerId} passed to update &quot; +&lt;br/&gt;
+        s&quot;for partition $topicPartition&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     trace(s&quot;Updated producer ${appendInfo.producerId} state to $appendInfo&quot;)&lt;br/&gt;
+    val updatedEntry = appendInfo.toEntry&lt;br/&gt;
+    producers.get(appendInfo.producerId) match &lt;/p&gt;
{
+      case Some(currentEntry) =&amp;gt;
+        currentEntry.update(updatedEntry)
+
+      case None =&amp;gt;
+        producers.put(appendInfo.producerId, updatedEntry)
+    }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val entry = appendInfo.latestEntry&lt;/li&gt;
	&lt;li&gt;producers.put(appendInfo.producerId, entry)&lt;br/&gt;
     appendInfo.startedTransactions.foreach 
{ txn =&amp;gt;
       ongoingTxns.put(txn.firstOffset.messageOffset, txn)
     }
&lt;p&gt;@@ -622,7 +654,7 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
   /**&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Get the last written entry for the given producer id.&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def lastEntry(producerId: Long): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerIdEntry&amp;#93;&lt;/span&gt; = producers.get(producerId)&lt;br/&gt;
+  def lastEntry(producerId: Long): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerStateEntry&amp;#93;&lt;/span&gt; = producers.get(producerId)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Take a snapshot at the current end offset if one does not already exist.&lt;br/&gt;
@@ -631,7 +663,7 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
     // If not a new offset, then it is not worth taking another snapshot&lt;br/&gt;
     if (lastMapOffset &amp;gt; lastSnapOffset) {&lt;br/&gt;
       val snapshotFile = Log.producerSnapshotFile(logDir, lastMapOffset)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;debug(s&quot;Writing producer snapshot for partition $topicPartition at offset $lastMapOffset&quot;)&lt;br/&gt;
+      info(s&quot;Writing producer snapshot at offset $lastMapOffset&quot;)&lt;br/&gt;
       writeSnapshot(snapshotFile, producers)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Update the last snap offset according to the serialized map&lt;br/&gt;
@@ -649,9 +681,9 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
    */&lt;br/&gt;
   def oldestSnapshotOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = oldestSnapshotFile.map(file =&amp;gt; offsetFromFile(file))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def isProducerRetained(producerIdEntry: ProducerIdEntry, logStartOffset: Long): Boolean = {&lt;/li&gt;
	&lt;li&gt;producerIdEntry.removeBatchesOlderThan(logStartOffset)&lt;/li&gt;
	&lt;li&gt;producerIdEntry.lastDataOffset &amp;gt;= logStartOffset&lt;br/&gt;
+  private def isProducerRetained(producerStateEntry: ProducerStateEntry, logStartOffset: Long): Boolean = 
{
+    producerStateEntry.removeBatchesOlderThan(logStartOffset)
+    producerStateEntry.lastDataOffset &amp;gt;= logStartOffset
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -664,8 +696,8 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;the snapshot.&lt;br/&gt;
    */&lt;br/&gt;
   def truncateHead(logStartOffset: Long) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val evictedProducerEntries = producers.filter { case (_, producerIdEntry) =&amp;gt;&lt;/li&gt;
	&lt;li&gt;!isProducerRetained(producerIdEntry, logStartOffset)&lt;br/&gt;
+    val evictedProducerEntries = producers.filter 
{ case (_, producerState) =&amp;gt;
+      !isProducerRetained(producerState, logStartOffset)
     }
&lt;p&gt;     val evictedProducerIds = evictedProducerEntries.keySet&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -717,7 +749,8 @@ class ProducerStateManager(val topicPartition: TopicPartition,&lt;br/&gt;
   def completeTxn(completedTxn: CompletedTxn): Long = {&lt;br/&gt;
     val txnMetadata = ongoingTxns.remove(completedTxn.firstOffset)&lt;br/&gt;
     if (txnMetadata == null)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;throw new IllegalArgumentException(&quot;Attempted to complete a transaction which was not started&quot;)&lt;br/&gt;
+      throw new IllegalArgumentException(s&quot;Attempted to complete transaction $completedTxn on partition $topicPartition &quot; +&lt;br/&gt;
+        s&quot;which was not started&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     txnMetadata.lastOffset = Some(completedTxn.lastOffset)&lt;br/&gt;
     unreplicatedTxns.put(completedTxn.firstOffset, txnMetadata)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/tools/DumpLogSegments.scala b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
index e96e1ad5615..2aa7ad3495a 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
@@ -155,9 +155,13 @@ object DumpLogSegments {&lt;br/&gt;
   private def dumpProducerIdSnapshot(file: File): Unit = {&lt;br/&gt;
     try {&lt;br/&gt;
       ProducerStateManager.readSnapshot(file).foreach { entry =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;println(s&quot;producerId: ${entry.producerId} producerEpoch: ${entry.producerEpoch} &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;coordinatorEpoch: ${entry.coordinatorEpoch} currentTxnFirstOffset: ${entry.currentTxnFirstOffset} &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;cachedMetadata: ${entry.batchMetadata}&quot;)&lt;br/&gt;
+        print(s&quot;producerId: ${entry.producerId} producerEpoch: ${entry.producerEpoch} &quot; +&lt;br/&gt;
+          s&quot;coordinatorEpoch: ${entry.coordinatorEpoch} currentTxnFirstOffset: ${entry.currentTxnFirstOffset} &quot;)&lt;br/&gt;
+        entry.batchMetadata.headOption.foreach { metadata =&amp;gt;&lt;br/&gt;
+          print(s&quot;firstSequence: ${metadata.firstSeq} lastSequence: ${metadata.lastSeq} &quot; +&lt;br/&gt;
+            s&quot;lastOffset: ${metadata.lastOffset} offsetDelta: ${metadata.offsetDelta} timestamp: ${metadata.timestamp}&quot;)&lt;br/&gt;
+        }&lt;br/&gt;
+        println()&lt;br/&gt;
       }&lt;br/&gt;
     } catch {&lt;br/&gt;
       case e: CorruptSnapshotException =&amp;gt;&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogSegmentTest.scala b/core/src/test/scala/unit/kafka/log/LogSegmentTest.scala&lt;br/&gt;
index 31bcf9c2a3b..79d6ea23267 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogSegmentTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogSegmentTest.scala&lt;br/&gt;
@@ -360,7 +360,7 @@ class LogSegmentTest {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // recover again, but this time assuming the transaction from pid2 began on a previous segment&lt;br/&gt;
     stateManager = new ProducerStateManager(topicPartition, logDir)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stateManager.loadProducerEntry(new ProducerIdEntry(pid2,&lt;br/&gt;
+    stateManager.loadProducerEntry(new ProducerStateEntry(pid2,&lt;br/&gt;
       mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;BatchMetadata&amp;#93;&lt;/span&gt;(BatchMetadata(10, 10L, 5, RecordBatch.NO_TIMESTAMP)), producerEpoch, 0, Some(75L)))&lt;br/&gt;
     segment.recover(stateManager)&lt;br/&gt;
     assertEquals(108L, stateManager.mapEndOffset)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index ec748156e23..28fcdfcfac1 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -815,6 +815,39 @@ class LogTest 
{
     assertEquals(Some(2L), log.oldestProducerSnapshotOffset)
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def testProducerSnapshotAfterSegmentRollOnAppend(): Unit = &lt;/p&gt;
{
+    val producerId = 1L
+    val logConfig = createLogConfig(segmentBytes = 1024)
+    val log = createLog(logDir, logConfig)
+
+    log.appendAsLeader(TestUtils.records(Seq(new SimpleRecord(mockTime.milliseconds(), new Array[Byte](512))),
+      producerId = producerId, producerEpoch = 0, sequence = 0),
+      leaderEpoch = 0)
+
+    // The next append should overflow the segment and cause it to roll
+    log.appendAsLeader(TestUtils.records(Seq(new SimpleRecord(mockTime.milliseconds(), new Array[Byte](512))),
+      producerId = producerId, producerEpoch = 0, sequence = 1),
+      leaderEpoch = 0)
+
+    assertEquals(2, log.logSegments.size)
+    assertEquals(1L, log.activeSegment.baseOffset)
+    assertEquals(Some(1L), log.latestProducerSnapshotOffset)
+
+    // Force a reload from the snapshot to check its consistency
+    log.truncateTo(1L)
+
+    assertEquals(2, log.logSegments.size)
+    assertEquals(1L, log.activeSegment.baseOffset)
+    assertTrue(log.activeSegment.log.batches.asScala.isEmpty)
+    assertEquals(Some(1L), log.latestProducerSnapshotOffset)
+
+    val lastEntry = log.producerStateManager.lastEntry(producerId)
+    assertTrue(lastEntry.isDefined)
+    assertEquals(0L, lastEntry.get.firstOffset)
+    assertEquals(0L, lastEntry.get.lastDataOffset)
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testRebuildTransactionalState(): Unit = {&lt;br/&gt;
     val logConfig = createLogConfig(segmentBytes = 1024 * 1024 * 5)&lt;br/&gt;
@@ -916,7 +949,7 @@ class LogTest &lt;/p&gt;
{
           new SimpleRecord(mockTime.milliseconds, s&quot;key-$seq&quot;.getBytes, s&quot;value-$seq&quot;.getBytes)),
         producerId = pid, producerEpoch = epoch, sequence = seq - 2)
       log.appendAsLeader(records, leaderEpoch = 0)
-      fail (&quot;Should have received an OutOfOrderSequenceException since we attempted to append a duplicate of a records &quot; +
+      fail(&quot;Should have received an OutOfOrderSequenceException since we attempted to append a duplicate of a records &quot; +
         &quot;in the middle of the log.&quot;)
     }
&lt;p&gt; catch {&lt;br/&gt;
       case _: OutOfOrderSequenceException =&amp;gt; // Good!&lt;br/&gt;
@@ -928,16 +961,16 @@ class LogTest {&lt;br/&gt;
       producerId = pid, producerEpoch = epoch, sequence = 2)&lt;br/&gt;
     log.appendAsLeader(duplicateOfFourth, leaderEpoch = 0)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Append a Duplicate of an entry older than the last 5 appended batches. This should result in a DuplicateSequenceNumberException.&lt;/li&gt;
	&lt;li&gt;try {&lt;br/&gt;
+    // Duplicates at older entries are reported as OutOfOrderSequence errors&lt;br/&gt;
+    try 
{
       val records = TestUtils.records(
         List(new SimpleRecord(mockTime.milliseconds, s&quot;key-1&quot;.getBytes, s&quot;value-1&quot;.getBytes)),
         producerId = pid, producerEpoch = epoch, sequence = 1)
       log.appendAsLeader(records, leaderEpoch = 0)
-      fail (&quot;Should have received an DuplicateSequenceNumberException since we attempted to append a duplicate of a batch&quot; +
+      fail(&quot;Should have received an OutOfOrderSequenceException since we attempted to append a duplicate of a batch &quot; +
         &quot;which is older than the last 5 appended batches.&quot;)
     }
&lt;p&gt; catch &lt;/p&gt;
{
-      case _: DuplicateSequenceException =&amp;gt; // Good!
+      case _: OutOfOrderSequenceException =&amp;gt; // Good!
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Append a duplicate entry with a single records at the tail of the log. This should return the appendInfo of the original entry.&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala b/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala&lt;br/&gt;
index 67b1b15f2dc..053aed7c915 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala&lt;br/&gt;
@@ -62,8 +62,8 @@ class ProducerStateManagerTest extends JUnitSuite {&lt;br/&gt;
     // Second entry for id 0 added&lt;br/&gt;
     append(stateManager, producerId, epoch, 1, 0L, 1L)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Duplicate sequence number (matches previous sequence number)&lt;/li&gt;
	&lt;li&gt;assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;DuplicateSequenceException&amp;#93;&lt;/span&gt; {&lt;br/&gt;
+    // Duplicates are checked separately and should result in OutOfOrderSequence if appended&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;OutOfOrderSequenceException&amp;#93;&lt;/span&gt; 
{
       append(stateManager, producerId, epoch, 1, 0L, 1L)
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -159,7 +159,7 @@ class ProducerStateManagerTest extends JUnitSuite {&lt;br/&gt;
     val producerEpoch = 0.toShort&lt;br/&gt;
     val offset = 992342L&lt;br/&gt;
     val seq = 0&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val producerAppendInfo = new ProducerAppendInfo(producerId, ProducerIdEntry.empty(producerId), ValidationType.Full)&lt;br/&gt;
+    val producerAppendInfo = new ProducerAppendInfo(producerId, ProducerStateEntry.empty(producerId), ValidationType.Full)&lt;br/&gt;
     producerAppendInfo.append(producerEpoch, seq, seq, time.milliseconds(), offset, isTransactional = true)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val logOffsetMetadata = new LogOffsetMetadata(messageOffset = offset, segmentBaseOffset = 990000L,&lt;br/&gt;
@@ -175,7 +175,7 @@ class ProducerStateManagerTest extends JUnitSuite {&lt;br/&gt;
     val producerEpoch = 0.toShort&lt;br/&gt;
     val offset = 992342L&lt;br/&gt;
     val seq = 0&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val producerAppendInfo = new ProducerAppendInfo(producerId, ProducerIdEntry.empty(producerId), ValidationType.Full)&lt;br/&gt;
+    val producerAppendInfo = new ProducerAppendInfo(producerId, ProducerStateEntry.empty(producerId), ValidationType.Full)&lt;br/&gt;
     producerAppendInfo.append(producerEpoch, seq, seq, time.milliseconds(), offset, isTransactional = true)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // use some other offset to simulate a follower append where the log offset metadata won&apos;t typically&lt;br/&gt;
@@ -188,6 +188,32 @@ class ProducerStateManagerTest extends JUnitSuite &lt;/p&gt;
{
     assertEquals(Some(LogOffsetMetadata(offset)), stateManager.firstUnstableOffset)
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testPrepareUpdateDoesNotMutate(): Unit = &lt;/p&gt;
{
+    val producerEpoch = 0.toShort
+
+    val appendInfo = stateManager.prepareUpdate(producerId, isFromClient = true)
+    appendInfo.append(producerEpoch, 0, 5, time.milliseconds(), 20L, isTransactional = false)
+    assertEquals(None, stateManager.lastEntry(producerId))
+    stateManager.update(appendInfo)
+    assertTrue(stateManager.lastEntry(producerId).isDefined)
+
+    val nextAppendInfo = stateManager.prepareUpdate(producerId, isFromClient = true)
+    nextAppendInfo.append(producerEpoch, 6, 10, time.milliseconds(), 30L, isTransactional = false)
+    assertTrue(stateManager.lastEntry(producerId).isDefined)
+
+    var lastEntry = stateManager.lastEntry(producerId).get
+    assertEquals(0, lastEntry.firstSeq)
+    assertEquals(5, lastEntry.lastSeq)
+    assertEquals(20L, lastEntry.lastDataOffset)
+
+    stateManager.update(nextAppendInfo)
+    lastEntry = stateManager.lastEntry(producerId).get
+    assertEquals(0, lastEntry.firstSeq)
+    assertEquals(10, lastEntry.lastSeq)
+    assertEquals(30L, lastEntry.lastDataOffset)
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def updateProducerTransactionState(): Unit = {&lt;br/&gt;
     val producerEpoch = 0.toShort&lt;br/&gt;
@@ -197,21 +223,21 @@ class ProducerStateManagerTest extends JUnitSuite {&lt;/p&gt;

&lt;p&gt;     val appendInfo = stateManager.prepareUpdate(producerId, isFromClient = true)&lt;br/&gt;
     appendInfo.append(producerEpoch, 1, 5, time.milliseconds(), 20L, isTransactional = true)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var lastEntry = appendInfo.latestEntry&lt;br/&gt;
+    var lastEntry = appendInfo.toEntry&lt;br/&gt;
     assertEquals(producerEpoch, lastEntry.producerEpoch)&lt;/li&gt;
	&lt;li&gt;assertEquals(0, lastEntry.firstSeq)&lt;br/&gt;
+    assertEquals(1, lastEntry.firstSeq)&lt;br/&gt;
     assertEquals(5, lastEntry.lastSeq)&lt;/li&gt;
	&lt;li&gt;assertEquals(9L, lastEntry.firstOffset)&lt;br/&gt;
+    assertEquals(16L, lastEntry.firstOffset)&lt;br/&gt;
     assertEquals(20L, lastEntry.lastDataOffset)&lt;br/&gt;
     assertEquals(Some(16L), lastEntry.currentTxnFirstOffset)&lt;br/&gt;
     assertEquals(List(new TxnMetadata(producerId, 16L)), appendInfo.startedTransactions)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     appendInfo.append(producerEpoch, 6, 10, time.milliseconds(), 30L, isTransactional = true)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;lastEntry = appendInfo.latestEntry&lt;br/&gt;
+    lastEntry = appendInfo.toEntry&lt;br/&gt;
     assertEquals(producerEpoch, lastEntry.producerEpoch)&lt;/li&gt;
	&lt;li&gt;assertEquals(0, lastEntry.firstSeq)&lt;br/&gt;
+    assertEquals(1, lastEntry.firstSeq)&lt;br/&gt;
     assertEquals(10, lastEntry.lastSeq)&lt;/li&gt;
	&lt;li&gt;assertEquals(9L, lastEntry.firstOffset)&lt;br/&gt;
+    assertEquals(16L, lastEntry.firstOffset)&lt;br/&gt;
     assertEquals(30L, lastEntry.lastDataOffset)&lt;br/&gt;
     assertEquals(Some(16L), lastEntry.currentTxnFirstOffset)&lt;br/&gt;
     assertEquals(List(new TxnMetadata(producerId, 16L)), appendInfo.startedTransactions)&lt;br/&gt;
@@ -223,29 +249,40 @@ class ProducerStateManagerTest extends JUnitSuite 
{
     assertEquals(40L, completedTxn.lastOffset)
     assertFalse(completedTxn.isAborted)
 
-    lastEntry = appendInfo.latestEntry
+    lastEntry = appendInfo.toEntry
     assertEquals(producerEpoch, lastEntry.producerEpoch)
     // verify that appending the transaction marker doesn&apos;t affect the metadata of the cached record batches.
-    assertEquals(0, lastEntry.firstSeq)
+    assertEquals(1, lastEntry.firstSeq)
     assertEquals(10, lastEntry.lastSeq)
-    assertEquals(9L, lastEntry.firstOffset)
+    assertEquals(16L, lastEntry.firstOffset)
     assertEquals(30L, lastEntry.lastDataOffset)
     assertEquals(coordinatorEpoch, lastEntry.coordinatorEpoch)
     assertEquals(None, lastEntry.currentTxnFirstOffset)
     assertEquals(List(new TxnMetadata(producerId, 16L)), appendInfo.startedTransactions)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;OutOfOrderSequenceException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+  @Test&lt;br/&gt;
   def testOutOfSequenceAfterControlRecordEpochBump(): Unit = {&lt;br/&gt;
     val epoch = 0.toShort&lt;/li&gt;
	&lt;li&gt;append(stateManager, producerId, epoch, 0, 0L)&lt;/li&gt;
	&lt;li&gt;append(stateManager, producerId, epoch, 1, 1L)&lt;br/&gt;
+    append(stateManager, producerId, epoch, 0, 0L, isTransactional = true)&lt;br/&gt;
+    append(stateManager, producerId, epoch, 1, 1L, isTransactional = true)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val bumpedEpoch = 1.toShort&lt;br/&gt;
     appendEndTxnMarker(stateManager, producerId, bumpedEpoch, ControlRecordType.ABORT, 1L)&lt;/p&gt;

&lt;p&gt;     // next append is invalid since we expect the sequence to be reset&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;append(stateManager, producerId, bumpedEpoch, 2, 2L)&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;OutOfOrderSequenceException&amp;#93;&lt;/span&gt; 
{
+      append(stateManager, producerId, bumpedEpoch, 2, 2L, isTransactional = true)
+    }
&lt;p&gt;+&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;OutOfOrderSequenceException&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
+      append(stateManager, producerId, (bumpedEpoch + 1).toShort, 2, 2L, isTransactional = true)
+    }
&lt;p&gt;+&lt;br/&gt;
+    // Append with the bumped epoch should be fine if starting from sequence 0&lt;br/&gt;
+    append(stateManager, producerId, bumpedEpoch, 0, 0L, isTransactional = true)&lt;br/&gt;
+    assertEquals(bumpedEpoch, stateManager.lastEntry(producerId).get.producerEpoch)&lt;br/&gt;
+    assertEquals(0, stateManager.lastEntry(producerId).get.lastSeq)&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;InvalidTxnStateException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -334,10 +371,10 @@ class ProducerStateManagerTest extends JUnitSuite &lt;/p&gt;
{
     assertFalse(recoveredMapping.activeProducers.contains(producerId))
     append(recoveredMapping, producerId, epoch, sequence, 2L, 70001, isFromClient = false)
     assertTrue(recoveredMapping.activeProducers.contains(producerId))
-    val producerIdEntry = recoveredMapping.activeProducers.get(producerId).head
-    assertEquals(epoch, producerIdEntry.producerEpoch)
-    assertEquals(sequence, producerIdEntry.firstSeq)
-    assertEquals(sequence, producerIdEntry.lastSeq)
+    val producerStateEntry = recoveredMapping.activeProducers.get(producerId).head
+    assertEquals(epoch, producerStateEntry.producerEpoch)
+    assertEquals(sequence, producerStateEntry.firstSeq)
+    assertEquals(sequence, producerStateEntry.lastSeq)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -524,7 +561,7 @@ class ProducerStateManagerTest extends JUnitSuite {&lt;br/&gt;
     append(stateManager, producerId, epoch, 2, 3L, 4L)&lt;br/&gt;
     stateManager.takeSnapshot()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;intercept&lt;span class=&quot;error&quot;&gt;&amp;#91;UnknownProducerIdException&amp;#93;&lt;/span&gt; {&lt;br/&gt;
+    assertThrows&lt;span class=&quot;error&quot;&gt;&amp;#91;UnknownProducerIdException&amp;#93;&lt;/span&gt; {&lt;br/&gt;
       val recoveredMapping = new ProducerStateManager(partition, logDir, maxPidExpirationMs)&lt;br/&gt;
       recoveredMapping.truncateAndReload(0L, 1L, time.milliseconds)&lt;br/&gt;
       append(recoveredMapping, pid2, epoch, 1, 4L, 5L)&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16410894" author="hachikuji" created="Fri, 23 Mar 2018 06:48:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chema.sanchez&quot; class=&quot;user-hover&quot; rel=&quot;chema.sanchez&quot;&gt;chema.sanchez&lt;/a&gt; Thanks a lot for the logs. It really helped. We&apos;ve merged a fix which will be included in 1.1. If you have time to try out the same scenario with the new RC when it is available, we&apos;d appreciate it.&lt;/p&gt;</comment>
                            <comment id="16423791" author="chema.sanchez" created="Tue, 3 Apr 2018 09:59:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; I have been unable to reproduce the problem with the fixed code, it seems to work properly.&lt;br/&gt;
Thanks a lot.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12915143" name="server.properties" size="907" author="chema.sanchez" created="Mon, 19 Mar 2018 15:31:06 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 33 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3rhzb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>