<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:07:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6593] Coordinator disconnect in heartbeat thread can cause commitSync to block indefinitely</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6593</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;If a coordinator disconnect is observed in the heartbeat thread, it can cause a pending offset commit to be cancelled just before the foreground thread begins waiting on its response in poll(). Since the poll timeout is Long.MAX_VALUE, this will cause the consumer to effectively hang until some other network event causes the poll() to return. We try to protect this case with a poll condition on the future, but this isn&apos;t bulletproof since the future&#160;can be&#160;completed outside of the lock.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13141047">KAFKA-6593</key>
            <summary>Coordinator disconnect in heartbeat thread can cause commitSync to block indefinitely</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="hachikuji">Jason Gustafson</reporter>
                        <labels>
                    </labels>
                <created>Mon, 26 Feb 2018 21:13:30 +0000</created>
                <updated>Thu, 1 Mar 2018 19:09:11 +0000</updated>
                            <resolved>Thu, 1 Mar 2018 19:09:11 +0000</resolved>
                                    <version>0.11.0.2</version>
                    <version>1.0.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>consumer</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16377764" author="hachikuji" created="Mon, 26 Feb 2018 23:10:29 +0000"  >&lt;p&gt;I was wrong about the explanation. The &lt;tt&gt;ConsumerNetworkClient&lt;/tt&gt; already has some protection from this kind of scenario (e.g. it limits the maximum poll time to 5 seconds). So something else is going on in the case that I&apos;m looking at.&lt;/p&gt;</comment>
                            <comment id="16378901" author="hachikuji" created="Tue, 27 Feb 2018 16:40:26 +0000"  >&lt;p&gt;I&apos;ve attached the logs in case anyone is interested. What I was struggling to explain is the following gap in the logs:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[2018-02-15 07:36:20,902] DEBUG Updated cluster metadata version 18 to Cluster(id = CTkloGopRuCUxoicTrG1WA, nodes = [worker6:9092 (id: 2 rack: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;), worker5:9092 (id: 1 rack: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;)], partitions = [Partition(topic = test_topic, partition = 0, leader = 1, replicas = [2,1], isr = [1], offlineReplicas = [2])]) (org.apache.kafka.clients.Metadata)
[2018-02-15 07:36:33,881] DEBUG [Consumer clientId=consumer-1, groupId=test_group_id] Connection with worker5/172.31.44.80 disconnected (org.apache.kafka.common.network.Selector)
java.io.EOFException
&#160; &#160; &#160; &#160; at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:124)
&#160; &#160; &#160; &#160; at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:93)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Basically we have a situation where there is a coordinator disconnect which is observed by the heartbeat thread during a pending call to commitSync().&#160;&lt;/p&gt;

&lt;p&gt;I think we might be hitting a livelock situation of sorts. The root of the problem is basically that the threads are contending for an unfair lock. Here is the sequence of events:&lt;/p&gt;

&lt;p&gt;1. We begin a call to ConsumerCoordinator.commitOffsetsSync() which sends the offset commit and then begins polling for the response.&lt;br/&gt;
2. At some point, the heartbeat thread wakes up, observes the coordinator disconnect, and attempts to complete the callback for the offset commit.&lt;br/&gt;
3. In order to complete the callback, it has to acquire the lock in ConsumerNetworkClient because it has some logic in coordinatorDead() to disconnect the coordinator. Until this lock is obtained, the future for the offset commit response will be seen as uncompleted.&#160;&lt;br/&gt;
4. The poll loop in commitOffsetsSync() also contends for the same lock and is continually able to acquire it before the heartbeat thread. It may seem far-fetched that it is able to do so for such a long time, but actually it will hold onto the lock for up to 3 seconds since that is the poll timeout, so it does not take too many consecutive acquisitions before the session timeout expires.&lt;/p&gt;

&lt;p&gt;Switching to a fair lock in ConsumerNetworkClient should mitigate the problem, but it may not be a 100% solution since it cannot actually guarantee fairness due to thread scheduling uncertainty. In this particular case, the lock acquisition from the heartbeat thread in the callback is actually unnecessary since we know the coordinator was already disconnected, so we can probably improve this as well. We&apos;ll have to look through the code though to see whether there are other similar cases.&lt;/p&gt;

&lt;p&gt;This is probably also one more strike against the shared NetworkClient approach used in the consumer.&lt;/p&gt;</comment>
                            <comment id="16379070" author="githubbot" created="Tue, 27 Feb 2018 18:13:31 +0000"  >&lt;p&gt;hachikuji opened a new pull request #4625: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6593&quot; title=&quot;Coordinator disconnect in heartbeat thread can cause commitSync to block indefinitely&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6593&quot;&gt;&lt;del&gt;KAFKA-6593&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;WIP&amp;#93;&lt;/span&gt;; Fix livelock with consumer heartbeat thread in commitSync&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4625&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4625&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Contention for the lock in ConsumerNetworkClient can lead to a livelock situation in which an active commitSync is unable to make progress because its completion is blocked in the heartbeat thread. The fix is twofold:&lt;/p&gt;

&lt;p&gt;   1) We change ConsumerNetworkClient to use a fair lock to reduce the chance of each thread getting starved.&lt;br/&gt;
   2) We eliminate the dependence on the lock in ConsumerNetworkClient for callback completion so that callbacks will not be blocked by an active poll().&lt;/p&gt;

&lt;p&gt;   I&apos;ve left this as a WIP patch since I am still considering test cases.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16382469" author="githubbot" created="Thu, 1 Mar 2018 19:04:14 +0000"  >&lt;p&gt;hachikuji closed pull request #4625: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6593&quot; title=&quot;Coordinator disconnect in heartbeat thread can cause commitSync to block indefinitely&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6593&quot;&gt;&lt;del&gt;KAFKA-6593&lt;/del&gt;&lt;/a&gt;; Fix livelock with consumer heartbeat thread in commitSync&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4625&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4625&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java&lt;br/&gt;
index 6884ff0dff7..2daadddee63 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java&lt;br/&gt;
@@ -231,7 +231,7 @@ protected synchronized boolean ensureCoordinatorReady(long startTimeMs, long tim&lt;br/&gt;
             } else if (coordinator != null &amp;amp;&amp;amp; client.connectionFailed(coordinator)) &lt;/p&gt;
{
                 // we found the coordinator, but the connection has failed, so mark
                 // it dead and backoff before retrying discovery
-                coordinatorDead();
+                markCoordinatorUnknown();
                 time.sleep(retryBackoffMs);
             }

&lt;p&gt;@@ -487,7 +487,7 @@ public void handle(JoinGroupResponse joinResponse, RequestFuture&amp;lt;ByteBuffer&amp;gt; fut&lt;br/&gt;
             } else if (error == Errors.COORDINATOR_NOT_AVAILABLE&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; error == Errors.NOT_COORDINATOR) {&lt;br/&gt;
                 // re-discover the coordinator and retry with backoff
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;coordinatorDead();&lt;br/&gt;
+                markCoordinatorUnknown();&lt;br/&gt;
                 log.debug(&quot;Attempt to join group failed due to obsolete coordinator information: {}&quot;, error.message());&lt;br/&gt;
                 future.raise(error);&lt;br/&gt;
             } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL&lt;br/&gt;
@@ -550,7 +550,7 @@ public void handle(SyncGroupResponse syncResponse,&lt;br/&gt;
                 if (error == Errors.GROUP_AUTHORIZATION_FAILED) 
{
                     future.raise(new GroupAuthorizationException(groupId));
                 }
&lt;p&gt; else if (error == Errors.REBALANCE_IN_PROGRESS) &lt;/p&gt;
{
-                    log.debug(&quot;SyncGroup failed due to group rebalance&quot;);
+                    log.debug(&quot;SyncGroup failed because the group began another rebalance&quot;);
                     future.raise(error);
                 }
&lt;p&gt; else if (error == Errors.UNKNOWN_MEMBER_ID&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; error == Errors.ILLEGAL_GENERATION) 
{
@@ -559,8 +559,8 @@ public void handle(SyncGroupResponse syncResponse,
                     future.raise(error);
                 }
&lt;p&gt; else if (error == Errors.COORDINATOR_NOT_AVAILABLE&lt;/p&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; error == Errors.NOT_COORDINATOR) {
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.debug(&quot;SyncGroup failed:&quot;, error.message());&lt;/li&gt;
	&lt;li&gt;coordinatorDead();&lt;br/&gt;
+                    log.debug(&quot;SyncGroup failed: {}&quot;, error.message());&lt;br/&gt;
+                    markCoordinatorUnknown();&lt;br/&gt;
                     future.raise(error);&lt;br/&gt;
                 } else {&lt;br/&gt;
                     future.raise(new KafkaException(&quot;Unexpected error from SyncGroup: &quot; + error.message()));&lt;br/&gt;
@@ -627,27 +627,34 @@ public void onFailure(RuntimeException e, RequestFuture&amp;lt;Void&amp;gt; future) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return true if the coordinator is unknown&lt;br/&gt;
      */&lt;br/&gt;
     public boolean coordinatorUnknown() 
{
-        return coordinator() == null;
+        return checkAndGetCoordinator() == null;
     }&lt;/li&gt;
&lt;/ul&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Get the current coordinator&lt;br/&gt;
+     * Get the coordinator if its connection is still active. Otherwise mark it unknown and&lt;br/&gt;
+     * return null.&lt;br/&gt;
+     *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return the current coordinator or null if it is unknown&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected synchronized Node coordinator() {&lt;br/&gt;
+    protected synchronized Node checkAndGetCoordinator() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         if (coordinator != null &amp;amp;&amp;amp; client.connectionFailed(coordinator)) {
-            coordinatorDead();
+            markCoordinatorUnknown(true);
             return null;
         }         return this.coordinator;     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Mark the current coordinator as dead.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;protected synchronized void coordinatorDead() {&lt;br/&gt;
+    private synchronized Node coordinator() 
{
+        return this.coordinator;
+    }
&lt;p&gt;+&lt;br/&gt;
+    protected synchronized void markCoordinatorUnknown() &lt;/p&gt;
{
+        markCoordinatorUnknown(false);
+    }
&lt;p&gt;+&lt;br/&gt;
+    protected synchronized void markCoordinatorUnknown(boolean isDisconnected) {&lt;br/&gt;
         if (this.coordinator != null) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;log.info(&quot;Marking the coordinator {} dead&quot;, this.coordinator);&lt;br/&gt;
+            log.info(&quot;Group coordinator {} is unavailable or invalid, will attempt rediscovery&quot;, this.coordinator);&lt;br/&gt;
             Node oldCoordinator = this.coordinator;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             // Mark the coordinator dead before disconnecting requests since the callbacks for any pending&lt;br/&gt;
@@ -656,8 +663,9 @@ protected synchronized void coordinatorDead() &lt;/p&gt;
{
             this.coordinator = null;
 
             // Disconnect from the coordinator to ensure that there are no in-flight requests remaining.
-            // Pending callbacks will be invoked with a DisconnectException.
-            client.disconnect(oldCoordinator);
+            // Pending callbacks will be invoked with a DisconnectException on the next call to poll.
+            if (!isDisconnected)
+                client.disconnectAsync(oldCoordinator);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;@@ -708,7 +716,7 @@ protected void close(long timeoutMs) {&lt;br/&gt;
                 // interrupted using wakeup) and the leave group request which have been queued, but not&lt;br/&gt;
                 // yet sent to the broker. Wait up to close timeout for these pending requests to be processed.&lt;br/&gt;
                 // If coordinator is not known, requests are aborted.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Node coordinator = coordinator();&lt;br/&gt;
+                Node coordinator = checkAndGetCoordinator();&lt;br/&gt;
                 if (coordinator != null &amp;amp;&amp;amp; !client.awaitPendingRequests(coordinator, timeoutMs))&lt;br/&gt;
                     log.warn(&quot;Close timed out with {} pending requests to coordinator, terminating client connections&quot;,&lt;br/&gt;
                             client.pendingRequestCount(coordinator));&lt;br/&gt;
@@ -769,7 +777,7 @@ public void handle(HeartbeatResponse heartbeatResponse, RequestFuture&amp;lt;Void&amp;gt; futu
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; error == Errors.NOT_COORDINATOR) {&lt;br/&gt;
                 log.debug(&quot;Attempt to heartbeat since coordinator {} is either not started or not valid.&quot;,&lt;br/&gt;
                         coordinator());&lt;/th&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;coordinatorDead();&lt;br/&gt;
+                markCoordinatorUnknown();&lt;br/&gt;
                 future.raise(error);&lt;br/&gt;
             } else if (error == Errors.REBALANCE_IN_PROGRESS) {&lt;br/&gt;
                 log.debug(&quot;Attempt to heartbeat failed since group is rebalancing&quot;);&lt;br/&gt;
@@ -800,7 +808,7 @@ public void handle(HeartbeatResponse heartbeatResponse, RequestFuture&amp;lt;Void&amp;gt; futu&lt;br/&gt;
         public void onFailure(RuntimeException e, RequestFuture&amp;lt;T&amp;gt; future) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {             // mark the coordinator as dead             if (e instanceof DisconnectException) {
-                coordinatorDead();
+                markCoordinatorUnknown(true);
             }             future.raise(e);         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -948,7 +956,7 @@ public void run() {&lt;br/&gt;
                         } else if (heartbeat.sessionTimeoutExpired(now)) &lt;/p&gt;
{
                             // the session timeout has expired without seeing a successful heartbeat, so we should
                             // probably make sure the coordinator is still healthy.
-                            coordinatorDead();
+                            markCoordinatorUnknown();
                         }
&lt;p&gt; else if (heartbeat.pollTimeoutExpired(now)) &lt;/p&gt;
{
                             // the poll timeout has expired, which means that the foreground thread has stalled
                             // in between calls to poll(), so we explicitly leave the group.
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java
index 0aa61bbf442..2afa1ff9236 100644
--- a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java
@@ -684,7 +684,7 @@ public void onComplete(Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets, Exception
         if (offsets.isEmpty())
             return RequestFuture.voidSuccess();
 
-        Node coordinator = coordinator();
+        Node coordinator = checkAndGetCoordinator();
         if (coordinator == null)
             return RequestFuture.coordinatorNotAvailable();
 
@@ -762,7 +762,7 @@ public void handle(OffsetCommitResponse commitResponse, RequestFuture&amp;lt;Void&amp;gt; futu
                     }
&lt;p&gt; else if (error == Errors.COORDINATOR_NOT_AVAILABLE&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; error == Errors.NOT_COORDINATOR&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; error == Errors.REQUEST_TIMED_OUT) 
{
-                        coordinatorDead();
+                        markCoordinatorUnknown();
                         future.raise(error);
                         return;
                     }
&lt;p&gt; else if (error == Errors.UNKNOWN_MEMBER_ID&lt;br/&gt;
@@ -799,7 +799,7 @@ public void handle(OffsetCommitResponse commitResponse, RequestFuture&amp;lt;Void&amp;gt; futu&lt;/p&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return A request future containing the committed offsets.&lt;br/&gt;
      */&lt;br/&gt;
     private RequestFuture&amp;lt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt;&amp;gt; sendOffsetFetchRequest(Set&amp;lt;TopicPartition&amp;gt; partitions) 
{
-        Node coordinator = coordinator();
+        Node coordinator = checkAndGetCoordinator();
         if (coordinator == null)
             return RequestFuture.coordinatorNotAvailable();
 
@@ -825,7 +825,7 @@ public void handle(OffsetFetchResponse response, RequestFuture&amp;lt;Map&amp;lt;TopicPartitio
                     future.raise(error);
                 }
&lt;p&gt; else if (error == Errors.NOT_COORDINATOR) &lt;/p&gt;
{
                     // re-discover the coordinator and retry
-                    coordinatorDead();
+                    markCoordinatorUnknown();
                     future.raise(error);
                 }
&lt;p&gt; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &lt;/p&gt;
{
                     future.raise(new GroupAuthorizationException(groupId));
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java
index 0747e8db146..fb393a54c7e 100644
--- a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java
@@ -44,6 +44,7 @@
 import java.util.concurrent.ConcurrentLinkedQueue;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.ReentrantLock;
 
 /**
  * Higher level consumer access to the network layer with basic support for request futures. This class
@@ -65,10 +66,15 @@
     private final long unsentExpiryMs;
     private final AtomicBoolean wakeupDisabled = new AtomicBoolean();
 
+    // We do not need high throughput, so use a fair lock to try to avoid starvation
+    private final ReentrantLock lock = new ReentrantLock(true);
+
     // when requests complete, they are transferred to this queue prior to invocation. The purpose
     // is to avoid invoking them while holding this object&apos;s monitor which can open the door for deadlocks.
     private final ConcurrentLinkedQueue&amp;lt;RequestFutureCompletionHandler&amp;gt; pendingCompletion = new ConcurrentLinkedQueue&amp;lt;&amp;gt;();
 
+    private final ConcurrentLinkedQueue&amp;lt;Node&amp;gt; pendingDisconnects = new ConcurrentLinkedQueue&amp;lt;&amp;gt;();
+
     // this flag allows the client to be safely woken up without waiting on the lock above. It is
     // atomic to avoid the need to acquire the lock above in order to enable it concurrently.
     private final AtomicBoolean wakeup = new AtomicBoolean(false);
@@ -113,12 +119,22 @@ public ConsumerNetworkClient(LogContext logContext,
         return completionHandler.future;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public synchronized Node leastLoadedNode() {&lt;/li&gt;
	&lt;li&gt;return client.leastLoadedNode(time.milliseconds());&lt;br/&gt;
+    public Node leastLoadedNode() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        lock.lock();+        try {
+            return client.leastLoadedNode(time.milliseconds());
+        } finally {
+            lock.unlock();
+        }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    public synchronized boolean hasReadyNodes() {&lt;br/&gt;
-        return client.hasReadyNodes();&lt;br/&gt;
+    public boolean hasReadyNodes() {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try {
+            return client.hasReadyNodes();
+        } finally {+            lock.unlock();+        }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -227,14 +243,18 @@ public void poll(long timeout, long now, PollCondition pollCondition, boolean di&lt;br/&gt;
         // there may be handlers which need to be invoked if we woke up the previous call to poll&lt;br/&gt;
         firePendingCompletedRequests();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;synchronized (this) {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try {&lt;br/&gt;
+            // Handle async disconnects prior to attempting any sends&lt;br/&gt;
+            handlePendingDisconnects();&lt;br/&gt;
+&lt;br/&gt;
             // send all the requests we can send now&lt;br/&gt;
             trySend(now);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             // check whether the poll is still needed by the caller. Note that if the expected completion&lt;br/&gt;
             // condition becomes satisfied after the call to shouldBlock() (because of a fired completion&lt;br/&gt;
             // handler), the client will be woken up.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (pollCondition == null || pollCondition.shouldBlock()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            if (pendingCompletion.isEmpty() &amp;amp;&amp;amp; (pollCondition == null || pollCondition.shouldBlock())) {
                 // if there are no requests in flight, do not block longer than the retry backoff
                 if (client.inFlightRequestCount() == 0)
                     timeout = Math.min(timeout, retryBackoffMs);
@@ -265,6 +285,8 @@ public void poll(long timeout, long now, PollCondition pollCondition, boolean di
 
             // clean unsent requests collection to keep the map from growing indefinitely
             unsent.clean();
+        } finally {
+            lock.unlock();
         }&lt;br/&gt;
 &lt;br/&gt;
         // called without the lock to avoid deadlock potential if handlers need to acquire locks&lt;br/&gt;
@@ -303,8 +325,11 @@ public boolean awaitPendingRequests(Node node, long timeoutMs) {&lt;br/&gt;
      * @return The number of pending requests&lt;br/&gt;
      */&lt;br/&gt;
     public int pendingRequestCount(Node node) {&lt;br/&gt;
-        synchronized (this) {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try {
             return unsent.requestCount(node) + client.inFlightRequestCount(node.idString());
+        } finally {+            lock.unlock();         }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -317,8 +342,11 @@ public int pendingRequestCount(Node node) {&lt;br/&gt;
     public boolean hasPendingRequests(Node node) {&lt;br/&gt;
         if (unsent.hasRequests(node))&lt;br/&gt;
             return true;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;synchronized (this) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        lock.lock();+        try {
             return client.hasInFlightRequests(node.idString());
+        } finally {
+            lock.unlock();
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -328,8 +356,11 @@ public boolean hasPendingRequests(Node node) {&lt;br/&gt;
      * @return The total count of pending requests&lt;br/&gt;
      */&lt;br/&gt;
     public int pendingRequestCount() {&lt;br/&gt;
-        synchronized (this) {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try {
             return unsent.requestCount() + client.inFlightRequestCount();
+        } finally {+            lock.unlock();         }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -341,8 +372,11 @@ public int pendingRequestCount() {&lt;br/&gt;
     public boolean hasPendingRequests() {&lt;br/&gt;
         if (unsent.hasRequests())&lt;br/&gt;
             return true;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;synchronized (this) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        lock.lock();+        try {
             return client.hasInFlightRequests();
+        } finally {
+            lock.unlock();
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -386,15 +420,25 @@ private void checkDisconnects(long now) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    public void disconnect(Node node) {&lt;br/&gt;
-        failUnsentRequests(node, DisconnectException.INSTANCE);&lt;br/&gt;
+    private void handlePendingDisconnects() {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try {&lt;br/&gt;
+            while (true) {&lt;br/&gt;
+                Node node = pendingDisconnects.poll();&lt;br/&gt;
+                if (node == null)&lt;br/&gt;
+                    break;&lt;br/&gt;
 &lt;br/&gt;
-        synchronized (this) {
-            client.disconnect(node.idString());
+                failUnsentRequests(node, DisconnectException.INSTANCE);
+                client.disconnect(node.idString());
+            }&lt;br/&gt;
+        } finally {+            lock.unlock();         }+    }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// We need to poll to ensure callbacks from in-flight requests on the disconnected socket are fired&lt;/li&gt;
	&lt;li&gt;pollNoWakeup();&lt;br/&gt;
+    public void disconnectAsync(Node node) 
{
+        pendingDisconnects.offer(node);
+        client.wakeup();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private void failExpiredRequests(long now) {&lt;br/&gt;
@@ -408,16 +452,16 @@ private void failExpiredRequests(long now) {&lt;/p&gt;

&lt;p&gt;     private void failUnsentRequests(Node node, RuntimeException e) {&lt;br/&gt;
         // clear unsent requests to node and fail their corresponding futures&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;synchronized (this) {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {             Collection&amp;lt;ClientRequest&amp;gt; unsentRequests = unsent.remove(node);             for (ClientRequest unsentRequest }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; finally &lt;/p&gt;
{
+            lock.unlock();
         }&lt;br/&gt;
-&lt;br/&gt;
-        // called without the lock to avoid deadlock potential&lt;br/&gt;
-        firePendingCompletedRequests();&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     private boolean trySend(long now) {&lt;br/&gt;
@@ -458,8 +502,11 @@ public void disableWakeups() {&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public void close() throws IOException {&lt;br/&gt;
-        synchronized (this) {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try {
             client.close();
+        } finally {+            lock.unlock();         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -469,8 +516,11 @@ public void close() throws IOException {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param node Node to connect to if possible&lt;br/&gt;
      */&lt;br/&gt;
     public boolean connectionFailed(Node node) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;synchronized (this) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        lock.lock();+        try {
             return client.connectionFailed(node);
+        } finally {
+            lock.unlock();
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -481,8 +531,11 @@ public boolean connectionFailed(Node node) {&lt;br/&gt;
      * @param node The node to connect to&lt;br/&gt;
      */&lt;br/&gt;
     public void tryConnect(Node node) {&lt;br/&gt;
-        synchronized (this) {&lt;br/&gt;
+        lock.lock();&lt;br/&gt;
+        try {
             client.ready(node, time.milliseconds());
+        } finally {+            lock.unlock();         }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/test/java/org/apache/kafka/clients/MockClient.java b/clients/src/test/java/org/apache/kafka/clients/MockClient.java&lt;br/&gt;
index ed45e3af56d..faf9240537c 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/MockClient.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/MockClient.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.common.Cluster;&lt;br/&gt;
 import org.apache.kafka.common.Node;&lt;br/&gt;
 import org.apache.kafka.common.errors.AuthenticationException;&lt;br/&gt;
+import org.apache.kafka.common.errors.InterruptException;&lt;br/&gt;
 import org.apache.kafka.common.errors.UnsupportedVersionException;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest;&lt;br/&gt;
@@ -86,6 +87,7 @@ public FutureResponse(Node node,&lt;br/&gt;
     private final Queue&amp;lt;FutureResponse&amp;gt; futureResponses = new ArrayDeque&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Queue&amp;lt;MetadataUpdate&amp;gt; metadataUpdates = new ArrayDeque&amp;lt;&amp;gt;();&lt;br/&gt;
     private volatile NodeApiVersions nodeApiVersions = NodeApiVersions.create();&lt;br/&gt;
+    private volatile int numBlockingWakeups = 0;&lt;/p&gt;

&lt;p&gt;     public MockClient(Time time) {&lt;br/&gt;
         this(time, null);&lt;br/&gt;
@@ -195,8 +197,40 @@ public void send(ClientRequest request, long now) &lt;/p&gt;
{
         this.requests.add(request);
     }

&lt;p&gt;+    /**&lt;br/&gt;
+     * Simulate a blocking poll in order to test wakeup behavior.&lt;br/&gt;
+     *&lt;br/&gt;
+     * @param numBlockingWakeups The number of polls which will block until woken up&lt;br/&gt;
+     */&lt;br/&gt;
+    public synchronized void enableBlockingUntilWakeup(int numBlockingWakeups) &lt;/p&gt;
{
+        this.numBlockingWakeups = numBlockingWakeups;
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public synchronized void wakeup() {&lt;br/&gt;
+        if (numBlockingWakeups &amp;gt; 0) &lt;/p&gt;
{
+            numBlockingWakeups--;
+            notify();
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    private synchronized void maybeAwaitWakeup() {&lt;br/&gt;
+        try &lt;/p&gt;
{
+            int remainingBlockingWakeups = numBlockingWakeups;
+            if (remainingBlockingWakeups &amp;lt;= 0)
+                return;
+
+            while (numBlockingWakeups == remainingBlockingWakeups)
+                wait();
+        }
&lt;p&gt; catch (InterruptedException e) &lt;/p&gt;
{
+            throw new InterruptException(e);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     @Override&lt;br/&gt;
     public List&amp;lt;ClientResponse&amp;gt; poll(long timeoutMs, long now) {&lt;br/&gt;
+        maybeAwaitWakeup();&lt;br/&gt;
+&lt;br/&gt;
         List&amp;lt;ClientResponse&amp;gt; copy = new ArrayList&amp;lt;&amp;gt;(this.responses);&lt;/p&gt;

&lt;p&gt;         if (metadata != null &amp;amp;&amp;amp; metadata.updateRequested()) &lt;/p&gt;
{
@@ -427,10 +461,6 @@ public ClientRequest newClientRequest(String nodeId, AbstractRequest.Builder&amp;lt;?&amp;gt;
                 expectResponse, callback);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void wakeup() 
{
-    }
&lt;p&gt;-&lt;br/&gt;
     @Override&lt;br/&gt;
     public void close() {&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java&lt;br/&gt;
index 2e5c96019b9..ac392055f9b 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java&lt;br/&gt;
@@ -221,7 +221,8 @@ public void onComplete(Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets, Exception&lt;br/&gt;
             });&lt;br/&gt;
         }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;coordinator.coordinatorDead();&lt;br/&gt;
+        coordinator.markCoordinatorUnknown();&lt;br/&gt;
+        consumerClient.pollNoWakeup();&lt;br/&gt;
         coordinator.invokeCompletedOffsetCommitCallbacks();&lt;br/&gt;
         assertEquals(numRequests, responses.get());&lt;br/&gt;
     }&lt;br/&gt;
@@ -238,7 +239,7 @@ public void testCoordinatorUnknownInUnsentCallbacksAfterCoordinatorDead() throws&lt;br/&gt;
         final AtomicBoolean asyncCallbackInvoked = new AtomicBoolean(false);&lt;br/&gt;
         Map&amp;lt;TopicPartition, OffsetCommitRequest.PartitionData&amp;gt; offsets = Collections.singletonMap(&lt;br/&gt;
                 new TopicPartition(&quot;foo&quot;, 0), new OffsetCommitRequest.PartitionData(13L, &quot;&quot;));&lt;/li&gt;
	&lt;li&gt;consumerClient.send(coordinator.coordinator(), new OffsetCommitRequest.Builder(groupId, offsets))&lt;br/&gt;
+        consumerClient.send(coordinator.checkAndGetCoordinator(), new OffsetCommitRequest.Builder(groupId, offsets))&lt;br/&gt;
                 .compose(new RequestFutureAdapter&amp;lt;ClientResponse, Object&amp;gt;() {&lt;br/&gt;
                     @Override&lt;br/&gt;
                     public void onSuccess(ClientResponse value, RequestFuture&amp;lt;Object&amp;gt; future) {}&lt;br/&gt;
@@ -251,7 +252,8 @@ public void onFailure(RuntimeException e, RequestFuture&amp;lt;Object&amp;gt; future) {&lt;br/&gt;
                     }&lt;br/&gt;
                 });&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;coordinator.coordinatorDead();&lt;br/&gt;
+        coordinator.markCoordinatorUnknown();&lt;br/&gt;
+        consumerClient.pollNoWakeup();&lt;br/&gt;
         assertTrue(asyncCallbackInvoked.get());&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1033,6 +1035,7 @@ private void testInFlightRequestsFailedAfterCoordinatorMarkedDead(Errors error)&lt;/p&gt;

&lt;p&gt;         client.respond(offsetCommitResponse(Collections.singletonMap(t1p, error)));&lt;br/&gt;
         consumerClient.pollNoWakeup();&lt;br/&gt;
+        consumerClient.pollNoWakeup(); // second poll since coordinator disconnect is async&lt;br/&gt;
         coordinator.invokeCompletedOffsetCommitCallbacks();&lt;/p&gt;

&lt;p&gt;         assertTrue(coordinator.coordinatorUnknown());&lt;br/&gt;
@@ -1678,7 +1681,7 @@ public void testAutoCommitAfterCoordinatorBackToService() {&lt;br/&gt;
         subscriptions.assignFromUser(Collections.singleton(t1p));&lt;br/&gt;
         subscriptions.seek(t1p, 100L);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;coordinator.coordinatorDead();&lt;br/&gt;
+        coordinator.markCoordinatorUnknown();&lt;br/&gt;
         assertTrue(coordinator.coordinatorUnknown());&lt;br/&gt;
         client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));&lt;br/&gt;
         client.prepareResponse(offsetCommitResponse(Collections.singletonMap(t1p, Errors.NONE)));&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java&lt;br/&gt;
index 904270ec489..d0888fa5655 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java&lt;br/&gt;
@@ -107,7 +107,8 @@ public void testDisconnectWithUnsentRequests() 
{
         RequestFuture&amp;lt;ClientResponse&amp;gt; future = consumerClient.send(node, heartbeat());
         assertTrue(consumerClient.hasPendingRequests(node));
         assertFalse(client.hasInFlightRequests(node.idString()));
-        consumerClient.disconnect(node);
+        consumerClient.disconnectAsync(node);
+        consumerClient.pollNoWakeup();
         assertTrue(future.failed());
         assertTrue(future.exception() instanceof DisconnectException);
     }
&lt;p&gt;@@ -118,7 +119,8 @@ public void testDisconnectWithInFlightRequests() &lt;/p&gt;
{
         consumerClient.pollNoWakeup();
         assertTrue(consumerClient.hasPendingRequests(node));
         assertTrue(client.hasInFlightRequests(node.idString()));
-        consumerClient.disconnect(node);
+        consumerClient.disconnectAsync(node);
+        consumerClient.pollNoWakeup();
         assertTrue(future.failed());
         assertTrue(future.exception() instanceof DisconnectException);
     }
&lt;p&gt;@@ -205,6 +207,66 @@ public void wakeup() &lt;/p&gt;
{
         assertTrue(future.isDone());
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testDisconnectWakesUpPoll() throws Exception {&lt;br/&gt;
+        final RequestFuture&amp;lt;ClientResponse&amp;gt; future = consumerClient.send(node, heartbeat());&lt;br/&gt;
+&lt;br/&gt;
+        client.enableBlockingUntilWakeup(1);&lt;br/&gt;
+        Thread t = new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() &lt;/p&gt;
{
+                consumerClient.poll(future);
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+        t.start();&lt;br/&gt;
+&lt;br/&gt;
+        consumerClient.disconnectAsync(node);&lt;br/&gt;
+        t.join();&lt;br/&gt;
+        assertTrue(future.failed());&lt;br/&gt;
+        assertTrue(future.exception() instanceof DisconnectException);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void testFutureCompletionOutsidePoll() throws Exception {&lt;br/&gt;
+        // Tests the scenario in which the request that is being awaited in one thread&lt;br/&gt;
+        // is received and completed in another thread.&lt;br/&gt;
+&lt;br/&gt;
+        final RequestFuture&amp;lt;ClientResponse&amp;gt; future = consumerClient.send(node, heartbeat());&lt;br/&gt;
+        consumerClient.pollNoWakeup(); // dequeue and send the request&lt;br/&gt;
+&lt;br/&gt;
+        client.enableBlockingUntilWakeup(2);&lt;br/&gt;
+        Thread t1 = new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {
+                consumerClient.pollNoWakeup();
+            }&lt;br/&gt;
+        };&lt;br/&gt;
+        t1.start();&lt;br/&gt;
+&lt;br/&gt;
+        // Sleep a little so that t1 is blocking in poll&lt;br/&gt;
+        Thread.sleep(50);&lt;br/&gt;
+&lt;br/&gt;
+        Thread t2 = new Thread() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void run() {+                consumerClient.poll(future);+            }
&lt;p&gt;+        };&lt;br/&gt;
+        t2.start();&lt;br/&gt;
+&lt;br/&gt;
+        // Sleep a little so that t2 is awaiting the network client lock&lt;br/&gt;
+        Thread.sleep(50);&lt;br/&gt;
+&lt;br/&gt;
+        // Simulate a network response and return from the poll in t1&lt;br/&gt;
+        client.respond(heartbeatResponse(Errors.NONE));&lt;br/&gt;
+        client.wakeup();&lt;br/&gt;
+&lt;br/&gt;
+        // Both threads should complete since t1 should wakeup t2&lt;br/&gt;
+        t1.join();&lt;br/&gt;
+        t2.join();&lt;br/&gt;
+        assertTrue(future.succeeded());&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testAwaitForMetadataUpdateWithTimeout() {&lt;br/&gt;
         assertFalse(consumerClient.awaitMetadataUpdate(10L));&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12912298" name="consumer.log" size="81556" author="hachikuji" created="Tue, 27 Feb 2018 16:36:24 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 37 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3qm6v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>