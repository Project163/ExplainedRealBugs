<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:10:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6361] Fast leader fail over can lead to log divergence between leader and follower</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6361</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;We have observed an edge case in the replication failover logic which can cause a replica to permanently fall out of sync with the leader or, in the worst case, actually have localized divergence between logs. This occurs in spite of the improved truncation logic from KIP-101. &lt;/p&gt;

&lt;p&gt;Suppose we have brokers A and B. Initially A is the leader in epoch 1. It appends two batches: one in the range (0, 10) and the other in the range (11, 20). The first one successfully replicates to B, but the second one does not. In other words, the logs on the brokers look like this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Broker A:
0: offsets [0, 10], leader epoch: 1
1: offsets [11, 20], leader epoch: 1

Broker B:
0: offsets [0, 10], leader epoch: 1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Broker A then has a zk session expiration and broker B is elected with epoch 2. It appends a new batch with offsets (11, n) to its local log. So we now have this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Broker A:
0: offsets [0, 10], leader epoch: 1
1: offsets [11, 20], leader epoch: 1

Broker B:
0: offsets [0, 10], leader epoch: 1
1: offsets: [11, n], leader epoch: 2
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Normally we expect broker A to truncate to offset 11 on becoming the follower, but before it is able to do so, broker B has its own zk session expiration and broker A again becomes leader, now with epoch 3. It then appends a new entry in the range (21, 30). The updated logs look like this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Broker A:
0: offsets [0, 10], leader epoch: 1
1: offsets [11, 20], leader epoch: 1
2: offsets: [21, 30], leader epoch: 3

Broker B:
0: offsets [0, 10], leader epoch: 1
1: offsets: [11, n], leader epoch: 2
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now what happens next depends on the last offset of the batch appended in epoch 2. On becoming follower, broker B will send an OffsetForLeaderEpoch request to broker A with epoch 2. Broker A will respond that epoch 2 ends at offset 21. There are three cases:&lt;/p&gt;

&lt;p&gt;1) n &amp;lt; 20: In this case, broker B will not do any truncation. It will begin fetching from offset n, which will ultimately cause an out of order offset error because broker A will return the full batch beginning from offset 11 which broker B will be unable to append.&lt;/p&gt;

&lt;p&gt;2) n == 20: Again broker B does not truncate. It will fetch from offset 21 and everything will appear fine though the logs have actually diverged.&lt;/p&gt;

&lt;p&gt;3) n &amp;gt; 20: Broker B will attempt to truncate to offset 21. Since this is in the middle of the batch, it will truncate all the way to offset 10. It can begin fetching from offset 11 and everything is fine.&lt;/p&gt;

&lt;p&gt;The case we have actually seen is the first one. The second one would likely go unnoticed in practice and everything is fine in the third case. To workaround the issue, we deleted the active segment on the replica which allowed it to re-replicate consistently from the leader.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure the best solution for this scenario. Maybe if the leader isn&apos;t aware of an epoch, it should always respond with &lt;tt&gt;UNDEFINED_EPOCH_OFFSET&lt;/tt&gt; instead of using the offset of the next highest epoch. That would cause the follower to truncate using its high watermark. Or perhaps instead of doing so, it could send another OffsetForLeaderEpoch request at the next previous cached epoch and then truncate using that. &lt;/p&gt;</description>
                <environment></environment>
        <key id="13124845">KAFKA-6361</key>
            <summary>Fast leader fail over can lead to log divergence between leader and follower</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="apovzner">Anna Povzner</assignee>
                                    <reporter username="hachikuji">Jason Gustafson</reporter>
                        <labels>
                            <label>reliability</label>
                    </labels>
                <created>Thu, 14 Dec 2017 00:46:12 +0000</created>
                <updated>Tue, 20 Aug 2019 07:48:53 +0000</updated>
                            <resolved>Thu, 10 May 2018 01:50:49 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="16290252" author="junrao" created="Thu, 14 Dec 2017 02:46:33 +0000"  >&lt;p&gt;Was unclean leader election enabled in this case? When broker B takes over as the new leader because broker A&apos;s ZK session is expired, the controller is supposed to also shrink ISR to just &lt;/p&gt;
{B}
&lt;p&gt;. If unclean leader election is disabled, when broker B&apos;s ZK session expires, broker A can&apos;t take over as the new leader since it&apos;s not in ISR.&lt;/p&gt;

&lt;p&gt;In KIP-101, we didn&apos;t solve the problem with log divergence when an unclean leader election occurs (&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation#KIP-101-AlterReplicationProtocoltouseLeaderEpochratherthanHighWatermarkforTruncation-Appendix(a):PossibilityforDivergentLogswithLeaderEpochs&amp;amp;UncleanLeaderElection&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation#KIP-101-AlterReplicationProtocoltouseLeaderEpochratherthanHighWatermarkforTruncation-Appendix(a):PossibilityforDivergentLogswithLeaderEpochs&amp;amp;UncleanLeaderElection&lt;/a&gt;). Solving that problem requires more thoughts especially with compacted topics when certain leader epochs in the middle could have been fully garbage collected.&lt;/p&gt;</comment>
                            <comment id="16290268" author="hachikuji" created="Thu, 14 Dec 2017 03:04:18 +0000"  >&lt;p&gt;Unclean leader election was disabled. It may not have been a session expiration that caused B to become leader (I supposed this, but it&apos;s not clear in the logs and I haven&apos;t seen controller logs yet).  In any case, when broker B took over, broker A was still in the ISR. Broker B appended the entry as described above and then attempted to shrink the ISR, but it failed to do so because of an invalid cached zk version. Broker A had already become leader at that point.&lt;/p&gt;</comment>
                            <comment id="16291812" author="junrao" created="Fri, 15 Dec 2017 00:03:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;, thanks for the info. The problem in the description can indeed happen if the first leader change is due to preferred leader election, in which case, the ISR won&apos;t change.&lt;/p&gt;

&lt;p&gt;To address this issue, we could send another OffsetForLeaderEpoch with the previous leader epoch as you suggested. This may require multiple rounds of OffsetForLeaderEpoch requests. Another way is to change OffsetForLeaderEpoch request to send a sequence of (leader epoch, start offset) for the epoch between the follower&apos;s HW and LEO. On the leader side, we find the longest consecutive sequence of leader epoch whose start offset matches the leader&apos;s. We then return the end offset of the last matching leader epoch.&lt;/p&gt;

&lt;p&gt;The above approach doesn&apos;t fully fix the issue for a compacted topic. When all messages for a leader epoch are removed, we may lose the leader epoch. Thus, the leader epochs between the follower and the leader may not perfectly match. One way to address this issue is to preserve the offset of the first message in a leader epoch during log cleaning. This probably can be done separately since it causes problems rarely.&lt;/p&gt;</comment>
                            <comment id="16439970" author="githubbot" created="Mon, 16 Apr 2018 20:07:01 +0000"  >&lt;p&gt;apovzner opened a new pull request #4882:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6361&quot; title=&quot;Fast leader fail over can lead to log divergence between leader and follower&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6361&quot;&gt;&lt;del&gt;KAFKA-6361&lt;/del&gt;&lt;/a&gt;: Fix log divergence between leader and follower after fast leader fail over&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4882&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4882&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   WIP - will add few more unit tests.&lt;/p&gt;

&lt;p&gt;   Implementation of KIP-279 as described here: &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-279%3A+Fix+log+divergence+between+leader+and+follower+after+fast+leader+fail+over&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-279%3A+Fix+log+divergence+between+leader+and+follower+after+fast+leader+fail+over&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;   In summary:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Added leader_epoch to OFFSET_FOR_LEADER_EPOCH_RESPONSE&lt;/li&gt;
	&lt;li&gt;Leader replies with the pair( largest epoch less than or equal to the requested epoch, the end offset of this epoch)&lt;/li&gt;
	&lt;li&gt;If Follower does not know about the leader epoch that leader replies with, it truncates to the end offset of largest leader epoch less than leader epoch that leader replied with, and sends another OffsetForLeaderEpoch request. That request contains the largest leader epoch less than leader epoch that leader replied with.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   Added integration test EpochDrivenReplicationProtocolAcceptanceTest.logsShouldNotDivergeOnUncleanLeaderElections that does 3 fast leader changes where unclean leader election is enabled and min isr is 1. The test failed before the fix was implemented.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16469805" author="githubbot" created="Thu, 10 May 2018 01:49:55 +0000"  >&lt;p&gt;junrao closed pull request #4882:  &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6361&quot; title=&quot;Fast leader fail over can lead to log divergence between leader and follower&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6361&quot;&gt;&lt;del&gt;KAFKA-6361&lt;/del&gt;&lt;/a&gt;: Fix log divergence between leader and follower after fast leader fail over&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4882&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4882&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java b/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java&lt;br/&gt;
index a436dff1d03..7f43caf8696 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java&lt;br/&gt;
@@ -26,6 +26,7 @@&lt;br/&gt;
     public static final Field.Int32 PARTITION_ID = new Field.Int32(&quot;partition&quot;, &quot;Topic partition id&quot;);&lt;br/&gt;
     public static final Field.Int16 ERROR_CODE = new Field.Int16(&quot;error_code&quot;, &quot;Response error code&quot;);&lt;br/&gt;
     public static final Field.NullableStr ERROR_MESSAGE = new Field.NullableStr(&quot;error_message&quot;, &quot;Response error message&quot;);&lt;br/&gt;
+    public static final Field.Int32 LEADER_EPOCH = new Field.Int32(&quot;leader_epoch&quot;, &quot;The epoch&quot;);&lt;/p&gt;

&lt;p&gt;     // Group APIs&lt;br/&gt;
     public static final Field.Str GROUP_ID = new Field.Str(&quot;group_id&quot;, &quot;The unique group identifier&quot;);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java b/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java&lt;br/&gt;
index 0965e3612d8..ce938aad4f1 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java&lt;br/&gt;
@@ -20,24 +20,29 @@&lt;/p&gt;

&lt;p&gt; import static org.apache.kafka.common.record.RecordBatch.NO_PARTITION_LEADER_EPOCH;&lt;/p&gt;

&lt;p&gt;+import java.util.Objects;&lt;br/&gt;
+&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The offset, fetched from a leader, for a particular partition.&lt;br/&gt;
  */&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; public class EpochEndOffset {&lt;br/&gt;
     public static final long UNDEFINED_EPOCH_OFFSET = NO_PARTITION_LEADER_EPOCH;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final int UNDEFINED_EPOCH = -1;&lt;br/&gt;
+    public static final int UNDEFINED_EPOCH = NO_PARTITION_LEADER_EPOCH;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private Errors error;&lt;br/&gt;
+    private int leaderEpoch;  // introduced in V1&lt;br/&gt;
     private long endOffset;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public EpochEndOffset(Errors error, long endOffset) {&lt;br/&gt;
+    public EpochEndOffset(Errors error, int leaderEpoch, long endOffset) 
{
         this.error = error;
+        this.leaderEpoch = leaderEpoch;
         this.endOffset = endOffset;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public EpochEndOffset(long endOffset) {&lt;br/&gt;
+    public EpochEndOffset(int leaderEpoch, long endOffset) 
{
         this.error = Errors.NONE;
+        this.leaderEpoch = leaderEpoch;
         this.endOffset = endOffset;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -53,10 +58,15 @@ public long endOffset() &lt;/p&gt;
{
         return endOffset;
     }

&lt;p&gt;+    public int leaderEpoch() &lt;/p&gt;
{
+        return leaderEpoch;
+    }
&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() {&lt;br/&gt;
         return &quot;EpochEndOffset&lt;/p&gt;
{&quot; +
                 &quot;error=&quot; + error +
+                &quot;, leaderEpoch=&quot; + leaderEpoch +
                 &quot;, endOffset=&quot; + endOffset +
                 &apos;}
&lt;p&gt;&apos;;&lt;br/&gt;
     }&lt;br/&gt;
@@ -68,14 +78,13 @@ public boolean equals(Object o) &lt;/p&gt;
{
 
         EpochEndOffset that = (EpochEndOffset) o;
 
-        if (error != that.error) return false;
-        return endOffset == that.endOffset;
+        return Objects.equals(error, that.error)
+               &amp;amp;&amp;amp; Objects.equals(leaderEpoch, that.leaderEpoch)
+               &amp;amp;&amp;amp; Objects.equals(endOffset, that.endOffset);
     }

&lt;p&gt;     @Override&lt;br/&gt;
     public int hashCode() &lt;/p&gt;
{
-        int result = (int) error.code();
-        result = 31 * result + (int) (endOffset ^ (endOffset &amp;gt;&amp;gt;&amp;gt; 32));
-        return result;
+        return Objects.hash(error, leaderEpoch, endOffset);
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java&lt;br/&gt;
index d0585bed6d5..651416d97a7 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java&lt;br/&gt;
@@ -50,8 +50,11 @@&lt;br/&gt;
     private static final Schema OFFSET_FOR_LEADER_EPOCH_REQUEST_V0 = new Schema(&lt;br/&gt;
             new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_REQUEST_TOPIC_V0), &quot;An array of topics to get epochs for&quot;));&lt;/p&gt;

&lt;p&gt;+    /* v1 request is the same as v0. Per-partition leader epoch has been added to response */&lt;br/&gt;
+    private static final Schema OFFSET_FOR_LEADER_EPOCH_REQUEST_V1 = OFFSET_FOR_LEADER_EPOCH_REQUEST_V0;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new Schema[]
{OFFSET_FOR_LEADER_EPOCH_REQUEST_V0}
&lt;p&gt;;&lt;br/&gt;
+        return new Schema[]&lt;/p&gt;
{OFFSET_FOR_LEADER_EPOCH_REQUEST_V0, OFFSET_FOR_LEADER_EPOCH_REQUEST_V1}
&lt;p&gt;;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition;&lt;br/&gt;
@@ -63,12 +66,12 @@&lt;br/&gt;
     public static class Builder extends AbstractRequest.Builder&amp;lt;OffsetsForLeaderEpochRequest&amp;gt; {&lt;br/&gt;
         private Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Builder() {&lt;/li&gt;
	&lt;li&gt;super(ApiKeys.OFFSET_FOR_LEADER_EPOCH);&lt;br/&gt;
+        public Builder(short version) 
{
+            super(ApiKeys.OFFSET_FOR_LEADER_EPOCH, version);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Builder(Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition) {&lt;/li&gt;
	&lt;li&gt;super(ApiKeys.OFFSET_FOR_LEADER_EPOCH);&lt;br/&gt;
+        public Builder(short version, Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition) 
{
+            super(ApiKeys.OFFSET_FOR_LEADER_EPOCH, version);
             this.epochsByPartition = epochsByPartition;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -150,7 +153,8 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {&lt;br/&gt;
         Errors error = Errors.forException(e);&lt;br/&gt;
         Map&amp;lt;TopicPartition, EpochEndOffset&amp;gt; errorResponse = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (TopicPartition tp : epochsByPartition.keySet()) &lt;/p&gt;
{
-            errorResponse.put(tp, new EpochEndOffset(error, EpochEndOffset.UNDEFINED_EPOCH_OFFSET));
+            errorResponse.put(tp, new EpochEndOffset(
+                error, EpochEndOffset.UNDEFINED_EPOCH, EpochEndOffset.UNDEFINED_EPOCH_OFFSET));
         }
&lt;p&gt;         return new OffsetsForLeaderEpochResponse(errorResponse);&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java&lt;br/&gt;
index 4a91533938d..3da49b570f6 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java&lt;br/&gt;
@@ -23,6 +23,7 @@&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
+import org.apache.kafka.common.record.RecordBatch;&lt;br/&gt;
 import org.apache.kafka.common.utils.CollectionUtils;&lt;/p&gt;

&lt;p&gt; import java.nio.ByteBuffer;&lt;br/&gt;
@@ -34,6 +35,7 @@&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.LEADER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;/p&gt;

&lt;p&gt; public class OffsetsForLeaderEpochResponse extends AbstractResponse {&lt;br/&gt;
@@ -52,8 +54,23 @@&lt;br/&gt;
             new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_RESPONSE_TOPIC_V0),&lt;br/&gt;
                     &quot;An array of topics for which we have leader offsets for some requested Partition Leader Epoch&quot;));&lt;/p&gt;

&lt;p&gt;+&lt;br/&gt;
+    // OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V1 added a per-partition leader epoch field,&lt;br/&gt;
+    // which specifies which leader epoch the end offset belongs to&lt;br/&gt;
+    private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V1 = new Schema(&lt;br/&gt;
+            ERROR_CODE,&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            LEADER_EPOCH,&lt;br/&gt;
+            new Field(END_OFFSET_KEY_NAME, INT64, &quot;The end offset&quot;));&lt;br/&gt;
+    private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_TOPIC_V1 = new Schema(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V1)));&lt;br/&gt;
+    private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_V1 = new Schema(&lt;br/&gt;
+            new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_RESPONSE_TOPIC_V1),&lt;br/&gt;
+                  &quot;An array of topics for which we have leader offsets for some requested Partition Leader Epoch&quot;));&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new Schema[]
{OFFSET_FOR_LEADER_EPOCH_RESPONSE_V0}
&lt;p&gt;;&lt;br/&gt;
+        return new Schema[]&lt;/p&gt;
{OFFSET_FOR_LEADER_EPOCH_RESPONSE_V0, OFFSET_FOR_LEADER_EPOCH_RESPONSE_V1}
&lt;p&gt;;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private Map&amp;lt;TopicPartition, EpochEndOffset&amp;gt; epochEndOffsetsByPartition;&lt;br/&gt;
@@ -68,8 +85,9 @@ public OffsetsForLeaderEpochResponse(Struct struct) &lt;/p&gt;
{
                 Errors error = Errors.forCode(partitionAndEpoch.get(ERROR_CODE));
                 int partitionId = partitionAndEpoch.get(PARTITION_ID);
                 TopicPartition tp = new TopicPartition(topic, partitionId);
+                int leaderEpoch = partitionAndEpoch.getOrElse(LEADER_EPOCH, RecordBatch.NO_PARTITION_LEADER_EPOCH);
                 long endOffset = partitionAndEpoch.getLong(END_OFFSET_KEY_NAME);
-                epochEndOffsetsByPartition.put(tp, new EpochEndOffset(error, endOffset));
+                epochEndOffsetsByPartition.put(tp, new EpochEndOffset(error, leaderEpoch, endOffset));
             }
&lt;p&gt;         }&lt;br/&gt;
     }&lt;br/&gt;
@@ -110,6 +128,7 @@ protected Struct toStruct(short version) &lt;/p&gt;
{
                 Struct partitionStruct = topicStruct.instance(PARTITIONS_KEY_NAME);
                 partitionStruct.set(ERROR_CODE, partitionEndOffset.getValue().error().code());
                 partitionStruct.set(PARTITION_ID, partitionEndOffset.getKey());
+                partitionStruct.setIfExists(LEADER_EPOCH, partitionEndOffset.getValue().leaderEpoch());
                 partitionStruct.set(END_OFFSET_KEY_NAME, partitionEndOffset.getValue().endOffset());
                 partitions.add(partitionStruct);
             }
&lt;p&gt;diff --git a/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java b/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
index c63cecdda28..242265093db 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
@@ -1020,15 +1020,15 @@ private OffsetsForLeaderEpochRequest createLeaderEpochRequest() &lt;/p&gt;
{
         epochs.put(new TopicPartition(&quot;topic1&quot;, 1), 1);
         epochs.put(new TopicPartition(&quot;topic2&quot;, 2), 3);
 
-        return new OffsetsForLeaderEpochRequest.Builder(epochs).build();
+        return new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion(), epochs).build();
     }

&lt;p&gt;     private OffsetsForLeaderEpochResponse createLeaderEpochResponse() &lt;/p&gt;
{
         Map&amp;lt;TopicPartition, EpochEndOffset&amp;gt; epochs = new HashMap&amp;lt;&amp;gt;();
 
-        epochs.put(new TopicPartition(&quot;topic1&quot;, 0), new EpochEndOffset(Errors.NONE, 0));
-        epochs.put(new TopicPartition(&quot;topic1&quot;, 1), new EpochEndOffset(Errors.NONE, 1));
-        epochs.put(new TopicPartition(&quot;topic2&quot;, 2), new EpochEndOffset(Errors.NONE, 2));
+        epochs.put(new TopicPartition(&quot;topic1&quot;, 0), new EpochEndOffset(Errors.NONE, 1, 0));
+        epochs.put(new TopicPartition(&quot;topic1&quot;, 1), new EpochEndOffset(Errors.NONE, 1, 1));
+        epochs.put(new TopicPartition(&quot;topic2&quot;, 2), new EpochEndOffset(Errors.NONE, 1, 2));
 
         return new OffsetsForLeaderEpochResponse(epochs);
     }
&lt;p&gt;diff --git a/core/src/main/scala/kafka/api/ApiVersion.scala b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
index 62b91a0eb8b..ff011b28ce3 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
@@ -77,6 +77,7 @@ object ApiVersion {&lt;br/&gt;
     // and KafkaStorageException for fetch requests.&lt;br/&gt;
     &quot;1.1-IV0&quot; -&amp;gt; KAFKA_1_1_IV0,&lt;br/&gt;
     &quot;1.1&quot; -&amp;gt; KAFKA_1_1_IV0,&lt;br/&gt;
+    // Introduced OffsetsForLeaderEpochRequest V1 via KIP-279&lt;br/&gt;
     &quot;2.0-IV0&quot; -&amp;gt; KAFKA_2_0_IV0,&lt;br/&gt;
     &quot;2.0&quot; -&amp;gt; KAFKA_2_0_IV0&lt;br/&gt;
   )&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index 93377bad00e..b9180a45378 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -647,15 +647,20 @@ class Partition(val topic: String,&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param leaderEpoch Requested leader epoch&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @return The last offset of messages published under this leader epoch.&lt;br/&gt;
+    * @return The requested leader epoch and the end offset of this leader epoch, or if the requested&lt;br/&gt;
+    *         leader epoch is unknown, the leader epoch less than the requested leader epoch and the end offset&lt;br/&gt;
+    *         of this leader epoch. The end offset of a leader epoch is defined as the start&lt;br/&gt;
+    *         offset of the first leader epoch larger than the leader epoch, or else the log end&lt;br/&gt;
+    *         offset if the leader epoch is the latest leader epoch.&lt;br/&gt;
     */&lt;br/&gt;
   def lastOffsetForLeaderEpoch(leaderEpoch: Int): EpochEndOffset = {&lt;br/&gt;
     inReadLock(leaderIsrUpdateLock) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {       leaderReplicaIfLocal match {
         case Some(leaderReplica) =&amp;gt;
-          new EpochEndOffset(NONE, leaderReplica.epochs.get.endOffsetFor(leaderEpoch))
+          val (epoch, offset) = leaderReplica.epochs.get.endOffsetFor(leaderEpoch)
+          new EpochEndOffset(NONE, epoch, offset)
         case None =&amp;gt;
-          new EpochEndOffset(NOT_LEADER_FOR_PARTITION, UNDEFINED_EPOCH_OFFSET)
+          new EpochEndOffset(NOT_LEADER_FOR_PARTITION, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
       }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/consumer/ConsumerFetcherThread.scala b/core/src/main/scala/kafka/consumer/ConsumerFetcherThread.scala&lt;br/&gt;
index ac83fa17a76..94268846296 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/consumer/ConsumerFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/consumer/ConsumerFetcherThread.scala&lt;br/&gt;
@@ -20,7 +20,7 @@ package kafka.consumer&lt;br/&gt;
 import kafka.api.
{FetchRequestBuilder, FetchResponsePartitionData, OffsetRequest, Request}
&lt;p&gt; import kafka.cluster.BrokerEndPoint&lt;br/&gt;
 import kafka.message.ByteBufferMessageSet&lt;br/&gt;
-import kafka.server.&lt;/p&gt;
{AbstractFetcherThread, PartitionFetchState}
&lt;p&gt;+import kafka.server.&lt;/p&gt;
{AbstractFetcherThread, PartitionFetchState, OffsetTruncationState}
&lt;p&gt; import AbstractFetcherThread.ResultWithPartitions&lt;br/&gt;
 import kafka.common.&lt;/p&gt;
{ErrorMapping, TopicAndPartition}&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -129,7 +129,7 @@ class ConsumerFetcherThread(consumerIdString: String,&lt;/p&gt;

&lt;p&gt;   override def fetchEpochsFromLeader(partitions: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Int&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{ Map() }&lt;br/&gt;
 &lt;br/&gt;
-  override def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
+  override def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetTruncationState&amp;#93;&lt;/span&gt;] = {
     ResultWithPartitions(Map(), Set())
   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala&lt;br/&gt;
index f919ddf017c..f27dbfe07eb 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala&lt;br/&gt;
@@ -19,9 +19,10 @@ package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.util.concurrent.locks.ReentrantLock&lt;br/&gt;
 &lt;br/&gt;
-import kafka.cluster.BrokerEndPoint&lt;br/&gt;
+import kafka.cluster.{Replica, BrokerEndPoint}&lt;br/&gt;
 import kafka.utils.{DelayedItem, Pool, ShutdownableThread}&lt;br/&gt;
 import org.apache.kafka.common.errors.{CorruptRecordException, KafkaStorageException}&lt;br/&gt;
+import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import kafka.common.{ClientIdAndBroker, KafkaException}&lt;br/&gt;
 import kafka.metrics.KafkaMetricsGroup&lt;br/&gt;
 import kafka.utils.CoreUtils.inLock&lt;br/&gt;
@@ -39,6 +40,8 @@ import org.apache.kafka.common.internals.{FatalExitError, PartitionStates}&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset&lt;br/&gt;
 &lt;br/&gt;
+import scala.math._&lt;br/&gt;
+&lt;br/&gt;
 /**&lt;br/&gt;
  *  Abstract class for fetching data from multiple partitions from the same broker.&lt;br/&gt;
  */&lt;br/&gt;
@@ -76,7 +79,7 @@ abstract class AbstractFetcherThread(name: String,&lt;br/&gt;
 &lt;br/&gt;
   protected def fetchEpochsFromLeader(partitions: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Int&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
-  protected def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+  protected def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetTruncationState&amp;#93;&lt;/span&gt;]&lt;br/&gt;
 &lt;br/&gt;
   protected def buildFetchRequest(partitionMap: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, PartitionFetchState)&amp;#93;&lt;/span&gt;): ResultWithPartitions&lt;span class=&quot;error&quot;&gt;&amp;#91;REQ&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
@@ -132,7 +135,7 @@ abstract class AbstractFetcherThread(name: String,&lt;br/&gt;
         val leaderEpochs = fetchedEpochs.filter { case (tp, _) =&amp;gt; partitionStates.contains(tp) }&lt;br/&gt;
         val ResultWithPartitions(fetchOffsets, partitionsWithError) = maybeTruncate(leaderEpochs)&lt;br/&gt;
         handlePartitionsWithErrors(partitionsWithError)&lt;br/&gt;
-        markTruncationCompleteAndUpdateFetchOffset(fetchOffsets)&lt;br/&gt;
+        updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets)&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -272,15 +275,16 @@ abstract class AbstractFetcherThread(name: String,&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Loop through all partitions, marking them as truncation complete and update the fetch offset&lt;br/&gt;
+    * Loop through all partitions, updating their fetch offset and maybe marking them as&lt;br/&gt;
+    * truncation completed if their offsetTruncationState indicates truncation completed&lt;br/&gt;
     *&lt;br/&gt;
-    * @param fetchOffsets the partitions to mark truncation complete&lt;br/&gt;
+    * @param fetchOffsets the partitions to update fetch offset and maybe mark truncation complete&lt;br/&gt;
     */&lt;br/&gt;
-  private def markTruncationCompleteAndUpdateFetchOffset(fetchOffsets: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
+  private def updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetTruncationState&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
     val newStates: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionFetchState&amp;#93;&lt;/span&gt; = partitionStates.partitionStates.asScala&lt;br/&gt;
       .map { state =&amp;gt;&lt;br/&gt;
         val maybeTruncationComplete = fetchOffsets.get(state.topicPartition()) match {
-          case Some(offset) =&amp;gt; PartitionFetchState(offset, state.value.delay, truncatingLog = false)
+          case Some(offsetTruncationState) =&amp;gt; PartitionFetchState(offsetTruncationState.offset, state.value.delay, truncatingLog = !offsetTruncationState.truncationCompleted)
           case None =&amp;gt; state.value()
         }&lt;br/&gt;
         (state.topicPartition(), maybeTruncationComplete)&lt;br/&gt;
@@ -288,6 +292,79 @@ abstract class AbstractFetcherThread(name: String,&lt;br/&gt;
     partitionStates.set(newStates.asJava)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Called from ReplicaFetcherThread and ReplicaAlterLogDirsThread maybeTruncate for each topic&lt;br/&gt;
+   * partition. Returns truncation offset and whether this is the final offset to truncate to&lt;br/&gt;
+   *&lt;br/&gt;
+   * For each topic partition, the offset to truncate to is calculated based on leader&apos;s returned&lt;br/&gt;
+   * epoch and offset:&lt;br/&gt;
+   *  &amp;#8211; If the leader replied with undefined epoch offset, we must use the high watermark. This can&lt;br/&gt;
+   *  happen if 1) the leader is still using message format older than KAFKA_0_11_0; 2) the follower&lt;br/&gt;
+   *  requested leader epoch &amp;lt; the first leader epoch known to the leader.&lt;br/&gt;
+   *  &amp;#8211; If the leader replied with the valid offset but undefined leader epoch, we truncate to&lt;br/&gt;
+   *  leader&apos;s offset if it is lower than follower&apos;s Log End Offset. This may happen if the&lt;br/&gt;
+   *  leader is on the inter-broker protocol version &amp;lt; KAFKA_2_0_IV0&lt;br/&gt;
+   *  &amp;#8211; If the leader replied with leader epoch not known to the follower, we truncate to the&lt;br/&gt;
+   *  end offset of the largest epoch that is smaller than the epoch the leader replied with, and&lt;br/&gt;
+   *  send OffsetsForLeaderEpochRequest with that leader epoch. In a more rare case, where the&lt;br/&gt;
+   *  follower was not tracking epochs smaller than the epoch the leader replied with, we&lt;br/&gt;
+   *  truncate the leader&apos;s offset (and do not send any more leader epoch requests).&lt;br/&gt;
+   *  &amp;#8211; Otherwise, truncate to min(leader&apos;s offset, end offset on the follower for epoch that&lt;br/&gt;
+   *  leader replied with, follower&apos;s Log End Offset).&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param tp                    Topic partition&lt;br/&gt;
+   * @param leaderEpochOffset     Epoch end offset received from the leader for this topic partition&lt;br/&gt;
+   * @param replica               Follower&apos;s replica, which is either local replica&lt;br/&gt;
+   *                              (ReplicaFetcherThread) or future replica (ReplicaAlterLogDirsThread)&lt;br/&gt;
+   * @param isFutureReplica       true if called from ReplicaAlterLogDirsThread&lt;br/&gt;
+   */&lt;br/&gt;
+  def getOffsetTruncationState(tp: TopicPartition, leaderEpochOffset: EpochEndOffset, replica: Replica, isFutureReplica: Boolean = false): OffsetTruncationState = {&lt;br/&gt;
+    // to make sure we can distinguish log output for fetching from remote leader or local replica&lt;br/&gt;
+    val followerName = if (isFutureReplica) &quot;future replica&quot; else &quot;follower&quot;&lt;br/&gt;
+&lt;br/&gt;
+    if (leaderEpochOffset.endOffset == UNDEFINED_EPOCH_OFFSET) {&lt;br/&gt;
+      // truncate to initial offset which is the high watermark for follower replica. For&lt;br/&gt;
+      // future replica, it is either high watermark of the future replica or current&lt;br/&gt;
+      // replica&apos;s truncation offset (when the current replica truncates, it forces future&lt;br/&gt;
+      // replica&apos;s partition state to &apos;truncating&apos; and sets initial offset to its truncation offset)&lt;br/&gt;
+      warn(s&quot;Based on $followerName&apos;s leader epoch, leader replied with an unknown offset in ${replica.topicPartition}. &quot; +&lt;br/&gt;
+           s&quot;The initial fetch offset ${partitionStates.stateValue(tp).fetchOffset} will be used for truncation.&quot;)&lt;br/&gt;
+      OffsetTruncationState(partitionStates.stateValue(tp).fetchOffset, truncationCompleted = true)&lt;br/&gt;
+    } else if (leaderEpochOffset.leaderEpoch == UNDEFINED_EPOCH) {&lt;br/&gt;
+      // either leader or follower or both use inter-broker protocol version &amp;lt; KAFKA_2_0_IV0&lt;br/&gt;
+      // (version 0 of OffsetForLeaderEpoch request/response)&lt;br/&gt;
+      warn(s&quot;Leader or $followerName is on protocol version where leader epoch is not considered in the OffsetsForLeaderEpoch response. &quot; +&lt;br/&gt;
+           s&quot;The leader&apos;s offset ${leaderEpochOffset.endOffset} will be used for truncation in ${replica.topicPartition}.&quot;)&lt;br/&gt;
+      OffsetTruncationState(min(leaderEpochOffset.endOffset, replica.logEndOffset.messageOffset), truncationCompleted = true)&lt;br/&gt;
+    } else {&lt;br/&gt;
+      // get (leader epoch, end offset) pair that corresponds to the largest leader epoch&lt;br/&gt;
+      // less than or equal to the requested epoch.&lt;br/&gt;
+      val (followerEpoch, followerEndOffset) = replica.epochs.get.endOffsetFor(leaderEpochOffset.leaderEpoch)&lt;br/&gt;
+      if (followerEndOffset == UNDEFINED_EPOCH_OFFSET) {&lt;br/&gt;
+        // This can happen if the follower was not tracking leader epochs at that point (before the&lt;br/&gt;
+        // upgrade, or if this broker is new). Since the leader replied with epoch &amp;lt;&lt;br/&gt;
+        // requested epoch from follower, so should be safe to truncate to leader&apos;s&lt;br/&gt;
+        // offset (this is the same behavior as post-KIP-101 and pre-KIP-279)&lt;br/&gt;
+        warn(s&quot;Based on $followerName&apos;s leader epoch, leader replied with epoch ${leaderEpochOffset.leaderEpoch} &quot; +&lt;br/&gt;
+             s&quot;below any $followerName&apos;s tracked epochs for ${replica.topicPartition}. &quot; +&lt;br/&gt;
+             s&quot;The leader&apos;s offset only ${leaderEpochOffset.endOffset} will be used for truncation.&quot;)&lt;br/&gt;
+        OffsetTruncationState(min(leaderEpochOffset.endOffset, replica.logEndOffset.messageOffset), truncationCompleted = true)&lt;br/&gt;
+      } else if (followerEpoch != leaderEpochOffset.leaderEpoch) {&lt;br/&gt;
+        // the follower does not know about the epoch that leader replied with&lt;br/&gt;
+        // we truncate to the end offset of the largest epoch that is smaller than the&lt;br/&gt;
+        // epoch the leader replied with, and send another offset for leader epoch request&lt;br/&gt;
+        val intermediateOffsetToTruncateTo = min(followerEndOffset, replica.logEndOffset.messageOffset)&lt;br/&gt;
+        info(s&quot;Based on $followerName&apos;s leader epoch, leader replied with epoch ${leaderEpochOffset.leaderEpoch} &quot; +&lt;br/&gt;
+             s&quot;unknown to the $followerName for ${replica.topicPartition}. &quot; +&lt;br/&gt;
+             s&quot;Will truncate to $intermediateOffsetToTruncateTo and send another leader epoch request to the leader.&quot;)&lt;br/&gt;
+        OffsetTruncationState(intermediateOffsetToTruncateTo, truncationCompleted = false)&lt;br/&gt;
+      } else {
+        val offsetToTruncateTo = min(followerEndOffset, leaderEpochOffset.endOffset)
+        OffsetTruncationState(min(offsetToTruncateTo, replica.logEndOffset.messageOffset), truncationCompleted = true)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   def delayPartitions(partitions: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;, delay: Long) {&lt;br/&gt;
     partitionMapLock.lockInterruptibly()&lt;br/&gt;
     try {
@@ -446,3 +523,10 @@ case class PartitionFetchState(fetchOffset: Long, delay: DelayedItem, truncating
 
   override def toString = &quot;offset:%d-isReadyForFetch:%b-isTruncatingLog:%b&quot;.format(fetchOffset, isReadyForFetch, truncatingLog)
 }&lt;br/&gt;
+&lt;br/&gt;
+case class OffsetTruncationState(offset: Long, truncationCompleted: Boolean) {
+
+  def this(offset: Long) = this(offset, true)
+
+  override def toString = &quot;offset:%d-truncationCompleted:%b&quot;.format(offset, truncationCompleted)
+}&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
index 0faf5dc3838..30e6c07cca4 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
@@ -37,7 +37,6 @@ import org.apache.kafka.common.record.{FileRecords, MemoryRecords}&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.{Map, Seq, Set, mutable}&lt;br/&gt;
 &lt;br/&gt;
-&lt;br/&gt;
 class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
                                 sourceBroker: BrokerEndPoint,&lt;br/&gt;
                                 brokerConfig: KafkaConfig,&lt;br/&gt;
@@ -102,7 +101,8 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
 &lt;br/&gt;
     // Append the leader&apos;s messages to the log&lt;br/&gt;
     partition.appendRecordsToFutureReplica(records)&lt;br/&gt;
-    futureReplica.highWatermark = new LogOffsetMetadata(partitionData.highWatermark)&lt;br/&gt;
+    val futureReplicaHighWatermark = futureReplica.logEndOffset.messageOffset.min(partitionData.highWatermark)&lt;br/&gt;
+    futureReplica.highWatermark = new LogOffsetMetadata(futureReplicaHighWatermark)&lt;br/&gt;
     futureReplica.maybeIncrementLogStartOffset(partitionData.logStartOffset)&lt;br/&gt;
 &lt;br/&gt;
     if (partition.maybeReplaceCurrentWithFutureReplica())&lt;br/&gt;
@@ -164,17 +164,32 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
   def fetchEpochsFromLeader(partitions: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Int&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     partitions.map { case (tp, epoch) =&amp;gt;&lt;br/&gt;
       try {
-        tp -&amp;gt; new EpochEndOffset(Errors.NONE, replicaMgr.getReplicaOrException(tp).epochs.get.endOffsetFor(epoch))
+        val (leaderEpoch, leaderOffset) = replicaMgr.getReplicaOrException(tp).epochs.get.endOffsetFor(epoch)
+        tp -&amp;gt; new EpochEndOffset(Errors.NONE, leaderEpoch, leaderOffset)
       } catch {
         case t: Throwable =&amp;gt;
           warn(s&quot;Error when getting EpochEndOffset for $tp&quot;, t)
-          tp -&amp;gt; new EpochEndOffset(Errors.forException(t), UNDEFINED_EPOCH_OFFSET)
+          tp -&amp;gt; new EpochEndOffset(Errors.forException(t), UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
-    val fetchOffsets = scala.collection.mutable.HashMap.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Truncate the log for each partition based on current replica&apos;s returned epoch and offset.&lt;br/&gt;
+   *&lt;br/&gt;
+   * The logic for finding the truncation offset is the same as in ReplicaFetcherThread&lt;br/&gt;
+   * and mainly implemented in AbstractFetcherThread.getOffsetTruncationState. One difference is&lt;br/&gt;
+   * that the initial fetch offset for topic partition could be set to the truncation offset of&lt;br/&gt;
+   * the current replica if that replica truncates. Otherwise, it is high watermark as in ReplicaFetcherThread.&lt;br/&gt;
+   *&lt;br/&gt;
+   * The reason we have to follow the leader epoch approach for truncating a future replica is to&lt;br/&gt;
+   * cover the case where a future replica is offline when the current replica truncates and&lt;br/&gt;
+   * re-replicates offsets that may have already been copied to the future replica. In that case,&lt;br/&gt;
+   * the future replica may miss &quot;mark for truncation&quot; event and must use the offset for leader epoch&lt;br/&gt;
+   * exchange with the current replica to truncate to the largest common log prefix for the topic partition&lt;br/&gt;
+   */&lt;br/&gt;
+  def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetTruncationState&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
+    val fetchOffsets = scala.collection.mutable.HashMap.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetTruncationState&amp;#93;&lt;/span&gt;&lt;br/&gt;
     val partitionsWithError = mutable.Set&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;()&lt;br/&gt;
 &lt;br/&gt;
     fetchedEpochs.foreach { case (topicPartition, epochOffset) =&amp;gt;&lt;br/&gt;
@@ -186,16 +201,10 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
           info(s&quot;Retrying leaderEpoch request for partition $topicPartition as the current replica reported an error: ${epochOffset.error}&quot;)&lt;br/&gt;
           partitionsWithError += topicPartition&lt;br/&gt;
         } else {
-          val fetchOffset =
-            if (epochOffset.endOffset == UNDEFINED_EPOCH_OFFSET)
-              partitionStates.stateValue(topicPartition).fetchOffset
-            else if (epochOffset.endOffset &amp;gt;= futureReplica.logEndOffset.messageOffset)
-              futureReplica.logEndOffset.messageOffset
-            else
-              epochOffset.endOffset
-
-          partition.truncateTo(fetchOffset, isFuture = true)
-          fetchOffsets.put(topicPartition, fetchOffset)
+          val offsetTruncationState = getOffsetTruncationState(topicPartition, epochOffset, futureReplica, isFutureReplica = true)
+
+          partition.truncateTo(offsetTruncationState.offset, isFuture = true)
+          fetchOffsets.put(topicPartition, offsetTruncationState)
         }&lt;br/&gt;
       } catch {&lt;br/&gt;
         case e: KafkaStorageException =&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index 8344d5beb34..6805d77d47f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -21,7 +21,7 @@ import java.util&lt;br/&gt;
 &lt;br/&gt;
 import AbstractFetcherThread.ResultWithPartitions&lt;br/&gt;
 import kafka.api.{FetchRequest =&amp;gt; _, _}&lt;br/&gt;
-import kafka.cluster.{BrokerEndPoint, Replica}&lt;br/&gt;
+import kafka.cluster.BrokerEndPoint&lt;br/&gt;
 import kafka.log.LogConfig&lt;br/&gt;
 import kafka.server.ReplicaFetcherThread._&lt;br/&gt;
 import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
@@ -74,6 +74,9 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_10_0_IV0) 2&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_9_0) 1&lt;br/&gt;
     else 0&lt;br/&gt;
+  private val offsetForLeaderEpochRequestVersion: Short =&lt;br/&gt;
+    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV0) 1&lt;br/&gt;
+    else 0&lt;br/&gt;
   private val fetchMetadataSupported = brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_1_1_IV0&lt;br/&gt;
   private val maxWait = brokerConfig.replicaFetchWaitMaxMs&lt;br/&gt;
   private val minBytes = brokerConfig.replicaFetchMinBytes&lt;br/&gt;
@@ -286,37 +289,31 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * - Truncate the log to the leader&apos;s offset for each partition&apos;s epoch.&lt;br/&gt;
-    * - If the leader&apos;s offset is greater, we stick with the Log End Offset&lt;br/&gt;
-    *   otherwise we truncate to the leaders offset.&lt;br/&gt;
-    * - If the leader replied with undefined epoch offset we must use the high watermark&lt;br/&gt;
-    */&lt;br/&gt;
-  override def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
-    val fetchOffsets = scala.collection.mutable.HashMap.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;&lt;br/&gt;
+   * Truncate the log for each partition&apos;s epoch based on leader&apos;s returned epoch and offset.&lt;br/&gt;
+   * The logic for finding the truncation offset is implemented in AbstractFetcherThread.getOffsetTruncationState&lt;br/&gt;
+   */&lt;br/&gt;
+  override def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetTruncationState&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
+    val fetchOffsets = scala.collection.mutable.HashMap.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetTruncationState&amp;#93;&lt;/span&gt;&lt;br/&gt;
     val partitionsWithError = mutable.Set&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;()&lt;br/&gt;
 &lt;br/&gt;
-    fetchedEpochs.foreach { case (tp, epochOffset) =&amp;gt;&lt;br/&gt;
+    fetchedEpochs.foreach { case (tp, leaderEpochOffset) =&amp;gt;&lt;br/&gt;
       try {&lt;br/&gt;
         val replica = replicaMgr.getReplicaOrException(tp)&lt;br/&gt;
         val partition = replicaMgr.getPartition(tp).get&lt;br/&gt;
 &lt;br/&gt;
-        if (epochOffset.hasError) {&lt;br/&gt;
-          info(s&quot;Retrying leaderEpoch request for partition ${replica.topicPartition} as the leader reported an error: ${epochOffset.error}&quot;)&lt;br/&gt;
+        if (leaderEpochOffset.hasError) {&lt;br/&gt;
+          info(s&quot;Retrying leaderEpoch request for partition ${replica.topicPartition} as the leader reported an error: ${leaderEpochOffset.error}&quot;)&lt;br/&gt;
           partitionsWithError += tp&lt;br/&gt;
         } else {&lt;br/&gt;
-          val fetchOffset =&lt;br/&gt;
-            if (epochOffset.endOffset == UNDEFINED_EPOCH_OFFSET) {&lt;br/&gt;
-              warn(s&quot;Based on follower&apos;s leader epoch, leader replied with an unknown offset in ${replica.topicPartition}. &quot; +&lt;br/&gt;
-                s&quot;The initial fetch offset ${partitionStates.stateValue(tp).fetchOffset} will be used for truncation.&quot;)&lt;br/&gt;
-              partitionStates.stateValue(tp).fetchOffset&lt;br/&gt;
-            } else if (epochOffset.endOffset &amp;gt;= replica.logEndOffset.messageOffset)&lt;br/&gt;
-              logEndOffset(replica, epochOffset)&lt;br/&gt;
-            else&lt;br/&gt;
-              epochOffset.endOffset&lt;br/&gt;
-&lt;br/&gt;
-          partition.truncateTo(fetchOffset, isFuture = false)&lt;br/&gt;
-          replicaMgr.replicaAlterLogDirsManager.markPartitionsForTruncation(brokerConfig.brokerId, tp, fetchOffset)&lt;br/&gt;
-          fetchOffsets.put(tp, fetchOffset)&lt;br/&gt;
+          val offsetTruncationState = getOffsetTruncationState(tp, leaderEpochOffset, replica)&lt;br/&gt;
+          if (offsetTruncationState.offset &amp;lt; replica.highWatermark.messageOffset)&lt;br/&gt;
+            warn(s&quot;Truncating $tp to offset ${offsetTruncationState.offset} below high watermark ${replica.highWatermark.messageOffset}&quot;)&lt;br/&gt;
+&lt;br/&gt;
+          partition.truncateTo(offsetTruncationState.offset, isFuture = false)&lt;br/&gt;
+          // mark the future replica for truncation only when we do last truncation&lt;br/&gt;
+          if (offsetTruncationState.truncationCompleted)&lt;br/&gt;
+            replicaMgr.replicaAlterLogDirsManager.markPartitionsForTruncation(brokerConfig.brokerId, tp, offsetTruncationState.offset)&lt;br/&gt;
+          fetchOffsets.put(tp, offsetTruncationState)&lt;br/&gt;
         }&lt;br/&gt;
       } catch {&lt;br/&gt;
         case e: KafkaStorageException =&amp;gt;&lt;br/&gt;
@@ -344,7 +341,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
     var result: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
     if (shouldSendLeaderEpochRequest) {&lt;br/&gt;
       val partitionsAsJava = partitions.map { case (tp, epoch) =&amp;gt; tp -&amp;gt; epoch.asInstanceOf[Integer] }.toMap.asJava&lt;br/&gt;
-      val epochRequest = new OffsetsForLeaderEpochRequest.Builder(partitionsAsJava)&lt;br/&gt;
+      val epochRequest = new OffsetsForLeaderEpochRequest.Builder(offsetForLeaderEpochRequestVersion, partitionsAsJava)&lt;br/&gt;
       try {&lt;br/&gt;
         val response = leaderEndpoint.sendRequest(epochRequest)&lt;br/&gt;
         result = response.responseBody.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;OffsetsForLeaderEpochResponse&amp;#93;&lt;/span&gt;.responses.asScala&lt;br/&gt;
@@ -355,26 +352,19 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
 &lt;br/&gt;
           // if we get any unexpected exception, mark all partitions with an error&lt;br/&gt;
           result = partitions.map { case (tp, _) =&amp;gt;
-            tp -&amp;gt; new EpochEndOffset(Errors.forException(t), UNDEFINED_EPOCH_OFFSET)
+            tp -&amp;gt; new EpochEndOffset(Errors.forException(t), UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
           }&lt;br/&gt;
       }&lt;br/&gt;
     } else {&lt;br/&gt;
       // just generate a response with no error but UNDEFINED_OFFSET so that we can fall back to truncating using&lt;br/&gt;
       // high watermark in maybeTruncate()&lt;br/&gt;
       result = partitions.map { case (tp, _) =&amp;gt;
-        tp -&amp;gt; new EpochEndOffset(Errors.NONE, UNDEFINED_EPOCH_OFFSET)
+        tp -&amp;gt; new EpochEndOffset(Errors.NONE, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
       }&lt;br/&gt;
     }&lt;br/&gt;
     result&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def logEndOffset(replica: Replica, epochOffset: EpochEndOffset): Long = {&lt;br/&gt;
-    val logEndOffset = replica.logEndOffset.messageOffset&lt;br/&gt;
-    info(s&quot;Based on follower&apos;s leader epoch, leader replied with an offset ${epochOffset.endOffset} &amp;gt;= the &quot; +&lt;br/&gt;
-      s&quot;follower&apos;s log end offset $logEndOffset in ${replica.topicPartition}. No truncation needed.&quot;)&lt;br/&gt;
-    logEndOffset&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    *  To avoid ISR thrashing, we only throttle a replica on the follower if it&apos;s in the throttled replica list,&lt;br/&gt;
    *  the quota is exceeded and the replica is not in sync.&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaManager.scala b/core/src/main/scala/kafka/server/ReplicaManager.scala&lt;br/&gt;
index da501174acd..0895319f2a4 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaManager.scala&lt;br/&gt;
@@ -1475,11 +1475,11 @@ class ReplicaManager(val config: KafkaConfig,&lt;br/&gt;
       val epochEndOffset = getPartition(tp) match {
         case Some(partition) =&amp;gt;
           if (partition eq ReplicaManager.OfflinePartition)
-            new EpochEndOffset(KAFKA_STORAGE_ERROR, UNDEFINED_EPOCH_OFFSET)
+            new EpochEndOffset(KAFKA_STORAGE_ERROR, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
           else
             partition.lastOffsetForLeaderEpoch(leaderEpoch)
         case None =&amp;gt;
-          new EpochEndOffset(UNKNOWN_TOPIC_OR_PARTITION, UNDEFINED_EPOCH_OFFSET)
+          new EpochEndOffset(UNKNOWN_TOPIC_OR_PARTITION, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
       }&lt;br/&gt;
       tp -&amp;gt; epochEndOffset&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala b/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
index 220432d32c0..23a53056f32 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala&lt;br/&gt;
@@ -20,7 +20,7 @@ import java.util.concurrent.locks.ReentrantReadWriteLock&lt;br/&gt;
 &lt;br/&gt;
 import kafka.server.LogOffsetMetadata&lt;br/&gt;
 import kafka.server.checkpoints.LeaderEpochCheckpoint&lt;br/&gt;
-import org.apache.kafka.common.requests.EpochEndOffset.{UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET}&lt;br/&gt;
+import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import kafka.utils.CoreUtils._&lt;br/&gt;
 import kafka.utils.Logging&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -29,7 +29,7 @@ import scala.collection.mutable.ListBuffer&lt;br/&gt;
 trait LeaderEpochCache {
   def assign(leaderEpoch: Int, offset: Long)
   def latestEpoch(): Int
-  def endOffsetFor(epoch: Int): Long
+  def endOffsetFor(epoch: Int): (Int, Long)
   def clearAndFlushLatest(offset: Long)
   def clearAndFlushEarliest(offset: Long)
   def clearAndFlush()
@@ -81,36 +81,42 @@ class LeaderEpochFileCache(topicPartition: TopicPartition, leo: () =&amp;gt; LogOffsetM
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-    * Returns the End Offset for a requested Leader Epoch.&lt;br/&gt;
+    * Returns the Leader Epoch and the End Offset for a requested Leader Epoch.&lt;br/&gt;
     *&lt;br/&gt;
-    * This is defined as the start offset of the first Leader Epoch larger than the&lt;br/&gt;
-    * Leader Epoch requested, or else the Log End Offset if the latest epoch was requested.&lt;br/&gt;
+    * The Leader Epoch returned is the largest epoch less than or equal to the requested Leader&lt;br/&gt;
+    * Epoch. The End Offset is the end offset of this epoch, which is defined as the start offset&lt;br/&gt;
+    * of the first Leader Epoch larger than the Leader Epoch requested, or else the Log End&lt;br/&gt;
+    * Offset if the latest epoch was requested.&lt;br/&gt;
     *&lt;br/&gt;
     * During the upgrade phase, where there are existing messages may not have a leader epoch,&lt;br/&gt;
     * if requestedEpoch is &amp;lt; the first epoch cached, UNSUPPORTED_EPOCH_OFFSET will be returned&lt;br/&gt;
     * so that the follower falls back to High Water Mark.&lt;br/&gt;
     *&lt;br/&gt;
-    * @param requestedEpoch&lt;br/&gt;
-    * @return offset&lt;br/&gt;
+    * @param requestedEpoch requested leader epoch&lt;br/&gt;
+    * @return leader epoch and offset&lt;br/&gt;
     */&lt;br/&gt;
-  override def endOffsetFor(requestedEpoch: Int): Long = {&lt;br/&gt;
+  override def endOffsetFor(requestedEpoch: Int): (Int, Long) = {&lt;br/&gt;
     inReadLock(lock) {&lt;br/&gt;
-      val offset =&lt;br/&gt;
+      val epochAndOffset =&lt;br/&gt;
         if (requestedEpoch == UNDEFINED_EPOCH) {
           // this may happen if a bootstrapping follower sends a request with undefined epoch or
           // a follower is on the older message format where leader epochs are not recorded
-          UNDEFINED_EPOCH_OFFSET
+          (UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
         } else if (requestedEpoch == latestEpoch) {
-          leo().messageOffset
+          (requestedEpoch, leo().messageOffset)
         } else {&lt;br/&gt;
-          val subsequentEpochs = epochs.filter(e =&amp;gt; e.epoch &amp;gt; requestedEpoch)&lt;br/&gt;
+          val (subsequentEpochs, previousEpochs) = epochs.partition { e =&amp;gt; e.epoch &amp;gt; requestedEpoch}&lt;br/&gt;
           if (subsequentEpochs.isEmpty || requestedEpoch &amp;lt; epochs.head.epoch)&lt;br/&gt;
-            UNDEFINED_EPOCH_OFFSET&lt;br/&gt;
-          else&lt;br/&gt;
-            subsequentEpochs.head.startOffset&lt;br/&gt;
+            // no epochs recorded or requested epoch &amp;lt; the first epoch cached&lt;br/&gt;
+            (UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+          else {
+            // we must get at least one element in previous epochs list, because if we are here,
+            // it means that requestedEpoch &amp;gt;= epochs.head.epoch -- so at least the first epoch is
+            (previousEpochs.last.epoch, subsequentEpochs.head.startOffset)
+          }&lt;br/&gt;
         }&lt;br/&gt;
-      debug(s&quot;Processed offset for epoch request for partition ${topicPartition} epoch:$requestedEpoch and returning offset $offset from epoch list of size ${epochs.size}&quot;)&lt;br/&gt;
-      offset&lt;br/&gt;
+      debug(s&quot;Processed offset for epoch request for partition ${topicPartition} epoch:$requestedEpoch and returning epoch ${epochAndOffset._1} and offset ${epochAndOffset._2} from epoch list of size ${epochs.size}&quot;)&lt;br/&gt;
+      epochAndOffset&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
index ab7ca64f11c..da45be2e6be 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
@@ -290,7 +290,7 @@ class AuthorizerIntegrationTest extends BaseRequestTest {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def offsetsForLeaderEpochRequest = {
-    new OffsetsForLeaderEpochRequest.Builder().add(tp, 7).build()
+    new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion()).add(tp, 7).build()
   }&lt;br/&gt;
 &lt;br/&gt;
   private def createOffsetFetchRequest = {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala b/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala&lt;br/&gt;
index b95f66cba55..bf6db2fc3cf 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala&lt;br/&gt;
@@ -21,6 +21,7 @@ import AbstractFetcherThread._&lt;br/&gt;
 import com.yammer.metrics.Metrics&lt;br/&gt;
 import kafka.cluster.BrokerEndPoint&lt;br/&gt;
 import kafka.server.AbstractFetcherThread.{FetchRequest, PartitionData}&lt;br/&gt;
+import kafka.server.OffsetTruncationState&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
@@ -131,7 +132,7 @@ class AbstractFetcherThreadTest {&lt;br/&gt;
 &lt;br/&gt;
     override def fetchEpochsFromLeader(partitions: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Int&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = { Map() }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def maybeTruncate(fetchedEpochs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): ResultWithPartitions[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt;] = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    override def maybeTruncate(fetchedEpochs}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
index a0f1dae8c76..c7a07ecb9c7 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala&lt;br/&gt;
@@ -61,7 +61,7 @@ class ReplicaAlterLogDirsThreadTest {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Stubs&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn(leo).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, leo)).anyTimes()&lt;br/&gt;
     stub(replica, replica, futureReplica, partition, replicaManager)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     replay(leaderEpochs, replicaManager, replica)&lt;br/&gt;
@@ -78,8 +78,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     val result = thread.fetchEpochsFromLeader(Map(t1p0 -&amp;gt; leaderEpoch, t1p1 -&amp;gt; leaderEpoch))&lt;/p&gt;

&lt;p&gt;     val expected = Map(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;t1p0 -&amp;gt; new EpochEndOffset(Errors.NONE, leo),&lt;/li&gt;
	&lt;li&gt;t1p1 -&amp;gt; new EpochEndOffset(Errors.NONE, leo)&lt;br/&gt;
+      t1p0 -&amp;gt; new EpochEndOffset(Errors.NONE, leaderEpoch, leo),&lt;br/&gt;
+      t1p1 -&amp;gt; new EpochEndOffset(Errors.NONE, leaderEpoch, leo)&lt;br/&gt;
     )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(&quot;results from leader epoch request should have offset from local replica&quot;,&lt;br/&gt;
@@ -101,7 +101,7 @@ class ReplicaAlterLogDirsThreadTest {&lt;/p&gt;

&lt;p&gt;     //Stubs&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn(leo).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, leo)).anyTimes()&lt;br/&gt;
     expect(replicaManager.getReplicaOrException(t1p0)).andReturn(replica).anyTimes()&lt;br/&gt;
     expect(replicaManager.getPartition(t1p0)).andReturn(Some(partition)).anyTimes()&lt;br/&gt;
     expect(replicaManager.getReplicaOrException(t1p1)).andThrow(new KafkaStorageException).once()&lt;br/&gt;
@@ -121,8 +121,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     val result = thread.fetchEpochsFromLeader(Map(t1p0 -&amp;gt; leaderEpoch, t1p1 -&amp;gt; leaderEpoch))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val expected = Map(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;t1p0 -&amp;gt; new EpochEndOffset(Errors.NONE, leo),&lt;/li&gt;
	&lt;li&gt;t1p1 -&amp;gt; new EpochEndOffset(Errors.KAFKA_STORAGE_ERROR, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+      t1p0 -&amp;gt; new EpochEndOffset(Errors.NONE, leaderEpoch, leo),&lt;br/&gt;
+      t1p1 -&amp;gt; new EpochEndOffset(Errors.KAFKA_STORAGE_ERROR, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
     )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(expected, result)&lt;br/&gt;
@@ -161,8 +161,10 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     expect(futureReplica.epochs).andReturn(Some(futureReplicaLeaderEpochs)).anyTimes()&lt;br/&gt;
     expect(futureReplica.logEndOffset).andReturn(new LogOffsetMetadata(futureReplicaLEO)).anyTimes()&lt;br/&gt;
     expect(futureReplicaLeaderEpochs.latestEpoch).andReturn(leaderEpoch).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochsT1p0.endOffsetFor(leaderEpoch)).andReturn(replicaT1p0LEO).anyTimes()&lt;/li&gt;
	&lt;li&gt;expect(leaderEpochsT1p1.endOffsetFor(leaderEpoch)).andReturn(replicaT1p1LEO).anyTimes()&lt;br/&gt;
+    expect(leaderEpochsT1p0.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, replicaT1p0LEO)).anyTimes()&lt;br/&gt;
+    expect(leaderEpochsT1p1.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, replicaT1p1LEO)).anyTimes()&lt;br/&gt;
+    expect(futureReplicaLeaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, futureReplicaLEO)).anyTimes()&lt;br/&gt;
+&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     stubWithFetchMessages(replicaT1p0, replicaT1p1, futureReplica, partition, replicaManager, responseCallback)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -188,6 +190,73 @@ class ReplicaAlterLogDirsThreadTest &lt;/p&gt;
{
     assertTrue(truncateToCapture.getValues.asScala.contains(futureReplicaLEO))
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def shouldTruncateToEndOffsetOfLargestCommonEpoch(): Unit = &lt;/p&gt;
{
+
+    //Create a capture to track what partitions/offsets are truncated
+    val truncateToCapture: Capture[Long] = newCapture(CaptureType.ALL)
+
+    // Setup all the dependencies
+    val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))
+    val quotaManager = createNiceMock(classOf[ReplicationQuotaManager])
+    val leaderEpochs = createMock(classOf[LeaderEpochCache])
+    val futureReplicaLeaderEpochs = createMock(classOf[LeaderEpochCache])
+    val logManager = createMock(classOf[LogManager])
+    val replica = createNiceMock(classOf[Replica])
+    // one future replica mock because our mocking methods return same values for both future replicas
+    val futureReplica = createNiceMock(classOf[Replica])
+    val partition = createMock(classOf[Partition])
+    val replicaManager = createMock(classOf[ReplicaManager])
+    val responseCallback: Capture[Seq[(TopicPartition, FetchPartitionData)] =&amp;gt; Unit]  = EasyMock.newCapture()
+
+    val leaderEpoch = 5
+    val futureReplicaLEO = 195
+    val replicaLEO = 200
+    val replicaEpochEndOffset = 190
+    val futureReplicaEpochEndOffset = 191
+
+    //Stubs
+    expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).anyTimes()
+    expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()
+    expect(futureReplica.epochs).andReturn(Some(futureReplicaLeaderEpochs)).anyTimes()
+    expect(futureReplica.logEndOffset).andReturn(new LogOffsetMetadata(futureReplicaLEO)).anyTimes()
+    expect(futureReplicaLeaderEpochs.latestEpoch).andReturn(leaderEpoch).once()
+    expect(futureReplicaLeaderEpochs.latestEpoch).andReturn(leaderEpoch - 2).once()
+
+    // leader replica truncated and fetched new offsets with new leader epoch
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch - 1, replicaLEO)).anyTimes()
+    // but future replica does not know about this leader epoch, so returns a smaller leader epoch
+    expect(futureReplicaLeaderEpochs.endOffsetFor(leaderEpoch - 1)).andReturn((leaderEpoch - 2, futureReplicaLEO)).anyTimes()
+    // finally, the leader replica knows about the leader epoch and returns end offset
+    expect(leaderEpochs.endOffsetFor(leaderEpoch - 2)).andReturn((leaderEpoch - 2, replicaEpochEndOffset)).anyTimes()
+    expect(futureReplicaLeaderEpochs.endOffsetFor(leaderEpoch - 2)).andReturn((leaderEpoch - 2, futureReplicaEpochEndOffset)).anyTimes()
+
+    expect(replicaManager.logManager).andReturn(logManager).anyTimes()
+    stubWithFetchMessages(replica, replica, futureReplica, partition, replicaManager, responseCallback)
+
+    replay(leaderEpochs, futureReplicaLeaderEpochs, replicaManager, logManager, quotaManager, replica, futureReplica, partition)
+
+    //Create the thread
+    val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)
+    val thread = new ReplicaAlterLogDirsThread(
+      &quot;alter-logs-dirs-thread-test1&quot;,
+      sourceBroker = endPoint,
+      brokerConfig = config,
+      replicaMgr = replicaManager,
+      quota = quotaManager,
+      brokerTopicStats = null)
+    thread.addPartitions(Map(t1p0 -&amp;gt; 0))
+
+    // First run will result in another offset for leader epoch request
+    thread.doWork()
+    // Second run should actually truncate
+    thread.doWork()
+
+    //We should have truncated to the offsets in the response
+    assertTrue(&quot;Expected offset &quot; + replicaEpochEndOffset + &quot; in captured truncation offsets &quot; + truncateToCapture.getValues,
+               truncateToCapture.getValues.asScala.contains(replicaEpochEndOffset))
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldTruncateToInitialFetchOffsetIfReplicaReturnsUndefinedOffset(): Unit = {&lt;/p&gt;

&lt;p&gt;@@ -219,9 +288,9 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     // pretend this is a completely new future replica, with no leader epochs recorded&lt;br/&gt;
     expect(futureReplicaLeaderEpochs.latestEpoch).andReturn(UNDEFINED_EPOCH).anyTimes()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// since UNDEFINED_EPOCH is -1 wich will be lower than any valid leader epoch, the method&lt;br/&gt;
+    // since UNDEFINED_EPOCH is -1 which will be lower than any valid leader epoch, the method&lt;br/&gt;
     // will return UNDEFINED_EPOCH_OFFSET if requested epoch is lower than the first epoch cached&lt;/li&gt;
	&lt;li&gt;expect(leaderEpochs.endOffsetFor(UNDEFINED_EPOCH)).andReturn(UNDEFINED_EPOCH_OFFSET).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(UNDEFINED_EPOCH)).andReturn((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)).anyTimes()&lt;br/&gt;
     stubWithFetchMessages(replica, replica, futureReplica, partition, replicaManager, responseCallback)&lt;br/&gt;
     replay(replicaManager, logManager, quotaManager, leaderEpochs, futureReplicaLeaderEpochs,&lt;br/&gt;
            replica, futureReplica, partition)&lt;br/&gt;
@@ -273,7 +342,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;br/&gt;
     expect(futureReplica.epochs).andReturn(Some(futureReplicaLeaderEpochs)).anyTimes()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     expect(futureReplicaLeaderEpochs.latestEpoch).andReturn(futureReplicaLeaderEpoch).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.endOffsetFor(futureReplicaLeaderEpoch)).andReturn(replicaLEO).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(futureReplicaLeaderEpoch)).andReturn((futureReplicaLeaderEpoch, replicaLEO)).anyTimes()&lt;br/&gt;
+    expect(futureReplicaLeaderEpochs.endOffsetFor(futureReplicaLeaderEpoch)).andReturn((futureReplicaLeaderEpoch, futureReplicaLEO)).anyTimes()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     expect(futureReplica.logEndOffset).andReturn(new LogOffsetMetadata(futureReplicaLEO)).anyTimes()&lt;br/&gt;
     expect(replicaManager.getReplica(t1p0)).andReturn(Some(replica)).anyTimes()&lt;br/&gt;
@@ -355,7 +425,8 @@ class ReplicaAlterLogDirsThreadTest {&lt;/p&gt;

&lt;p&gt;     expect(futureReplica.logEndOffset).andReturn(new LogOffsetMetadata(futureReplicaLEO)).anyTimes()&lt;br/&gt;
     expect(futureReplicaLeaderEpochs.latestEpoch).andReturn(leaderEpoch)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn(replicaLEO)&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, replicaLEO))&lt;br/&gt;
+    expect(futureReplicaLeaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, futureReplicaLEO))&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     stubWithFetchMessages(replica, replica, futureReplica, partition, replicaManager, responseCallback)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
index 2074044d0e1..f8f4948b9a9 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
@@ -20,7 +20,6 @@ import kafka.cluster.&lt;/p&gt;
{BrokerEndPoint, Replica}
&lt;p&gt; import kafka.log.LogManager&lt;br/&gt;
 import kafka.cluster.Partition&lt;br/&gt;
 import kafka.server.epoch.LeaderEpochCache&lt;br/&gt;
-import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import kafka.server.epoch.util.ReplicaFetcherMockBlockingSend&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -28,6 +27,7 @@ import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors._&lt;br/&gt;
 import org.apache.kafka.common.requests.EpochEndOffset&lt;br/&gt;
+import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import org.apache.kafka.common.utils.SystemTime&lt;br/&gt;
 import org.easymock.EasyMock._&lt;br/&gt;
 import org.easymock.&lt;/p&gt;
{Capture, CaptureType}
&lt;p&gt;@@ -43,17 +43,18 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
   private val t1p1 = new TopicPartition(&quot;topic1&quot;, 1)&lt;br/&gt;
   private val t2p1 = new TopicPartition(&quot;topic2&quot;, 1)&lt;/p&gt;

&lt;p&gt;+  private val brokerEndPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)&lt;br/&gt;
+&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldNotIssueLeaderEpochRequestIfInterbrokerVersionBelow11(): Unit = {&lt;br/&gt;
     val props = TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;)&lt;br/&gt;
     props.put(KafkaConfig.InterBrokerProtocolVersionProp, &quot;0.10.2&quot;)&lt;br/&gt;
     props.put(KafkaConfig.LogMessageFormatVersionProp, &quot;0.10.2&quot;)&lt;br/&gt;
     val config = KafkaConfig.fromProps(props)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)&lt;br/&gt;
     val thread = new ReplicaFetcherThread(&lt;br/&gt;
       name = &quot;bob&quot;,&lt;br/&gt;
       fetcherId = 0,&lt;/li&gt;
	&lt;li&gt;sourceBroker = endPoint,&lt;br/&gt;
+      sourceBroker = brokerEndPoint,&lt;br/&gt;
       brokerConfig = config,&lt;br/&gt;
       replicaMgr = null,&lt;br/&gt;
       metrics =  new Metrics(),&lt;br/&gt;
@@ -64,8 +65,8 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val result = thread.fetchEpochsFromLeader(Map(t1p0 -&amp;gt; 0, t1p1 -&amp;gt; 0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val expected = Map(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;t1p0 -&amp;gt; new EpochEndOffset(Errors.NONE, UNDEFINED_EPOCH_OFFSET),&lt;/li&gt;
	&lt;li&gt;t1p1 -&amp;gt; new EpochEndOffset(Errors.NONE, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+      t1p0 -&amp;gt; new EpochEndOffset(Errors.NONE, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET),&lt;br/&gt;
+      t1p1 -&amp;gt; new EpochEndOffset(Errors.NONE, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
     )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(&quot;results from leader epoch request should have undefined offset&quot;, expected, result)&lt;br/&gt;
@@ -75,7 +76,6 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
   def shouldHandleExceptionFromBlockingSend(): Unit = {&lt;br/&gt;
     val props = TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;)&lt;br/&gt;
     val config = KafkaConfig.fromProps(props)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)&lt;br/&gt;
     val mockBlockingSend = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BlockingSend&amp;#93;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     expect(mockBlockingSend.sendRequest(anyObject())).andThrow(new NullPointerException).once()&lt;br/&gt;
@@ -84,7 +84,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val thread = new ReplicaFetcherThread(&lt;br/&gt;
       name = &quot;bob&quot;,&lt;br/&gt;
       fetcherId = 0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sourceBroker = endPoint,&lt;br/&gt;
+      sourceBroker = brokerEndPoint,&lt;br/&gt;
       brokerConfig = config,&lt;br/&gt;
       replicaMgr = null,&lt;br/&gt;
       metrics =  new Metrics(),&lt;br/&gt;
@@ -95,8 +95,8 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val result = thread.fetchEpochsFromLeader(Map(t1p0 -&amp;gt; 0, t1p1 -&amp;gt; 0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val expected = Map(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;t1p0 -&amp;gt; new EpochEndOffset(Errors.UNKNOWN_SERVER_ERROR, UNDEFINED_EPOCH_OFFSET),&lt;/li&gt;
	&lt;li&gt;t1p1 -&amp;gt; new EpochEndOffset(Errors.UNKNOWN_SERVER_ERROR, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+      t1p0 -&amp;gt; new EpochEndOffset(Errors.UNKNOWN_SERVER_ERROR, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET),&lt;br/&gt;
+      t1p1 -&amp;gt; new EpochEndOffset(Errors.UNKNOWN_SERVER_ERROR, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
     )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     assertEquals(&quot;results from leader epoch request should have undefined offset&quot;, expected, result)&lt;br/&gt;
@@ -104,7 +104,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def shouldFetchLeaderEpochOnFirstFetchOnly(): Unit = {&lt;br/&gt;
+  def shouldFetchLeaderEpochOnFirstFetchOnlyIfLeaderEpochKnownToBoth(): Unit = {&lt;br/&gt;
     val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Setup all dependencies&lt;br/&gt;
@@ -116,27 +116,29 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val partition = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;+    val leaderEpoch = 5&lt;br/&gt;
+&lt;br/&gt;
     //Stubs&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;br/&gt;
     expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(0)).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.latestEpoch).andReturn(5)&lt;br/&gt;
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(0)).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.latestEpoch).andReturn(leaderEpoch)&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, 0)).anyTimes()&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()&lt;br/&gt;
     stub(replica, partition, replicaManager)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;br/&gt;
     //Expectations&lt;br/&gt;
     expect(partition.truncateTo(anyLong(), anyBoolean())).once&lt;/p&gt;

&lt;p&gt;     replay(leaderEpochs, replicaManager, logManager, quota, replica)&lt;/p&gt;

&lt;p&gt;     //Define the offsets for the OffsetsForLeaderEpochResponse&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val offsets = Map(t1p0 -&amp;gt; new EpochEndOffset(1), t1p1 -&amp;gt; new EpochEndOffset(1)).asJava&lt;br/&gt;
+    val offsets = Map(t1p0 -&amp;gt; new EpochEndOffset(leaderEpoch, 1), t1p1 -&amp;gt; new EpochEndOffset(leaderEpoch, 1)).asJava&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Create the fetcher thread&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)&lt;/li&gt;
	&lt;li&gt;val mockNetwork = new ReplicaFetcherMockBlockingSend(offsets, endPoint, new SystemTime())&lt;/li&gt;
	&lt;li&gt;val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, endPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;br/&gt;
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsets, brokerEndPoint, new SystemTime())&lt;br/&gt;
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;br/&gt;
     thread.addPartitions(Map(t1p0 -&amp;gt; 0, t1p1 -&amp;gt; 0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Loop 1&lt;br/&gt;
@@ -174,13 +176,16 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val partition = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;+    val leaderEpoch = 5&lt;br/&gt;
     val initialLEO = 200&lt;/p&gt;

&lt;p&gt;     //Stubs&lt;br/&gt;
     expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).anyTimes()&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;br/&gt;
     expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(initialLEO)).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.latestEpoch).andReturn(5).anyTimes()&lt;br/&gt;
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(initialLEO - 1)).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.latestEpoch).andReturn(leaderEpoch).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, initialLEO)).anyTimes()&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()&lt;br/&gt;
     stub(replica, partition, replicaManager)&lt;br/&gt;
@@ -188,20 +193,208 @@ class ReplicaFetcherThreadTest 
{
     replay(leaderEpochs, replicaManager, logManager, quota, replica, partition)
 
     //Define the offsets for the OffsetsForLeaderEpochResponse, these are used for truncation
-    val offsetsReply = Map(t1p0 -&amp;gt; new EpochEndOffset(156), t2p1 -&amp;gt; new EpochEndOffset(172)).asJava
+    val offsetsReply = Map(t1p0 -&amp;gt; new EpochEndOffset(leaderEpoch, 156), t2p1 -&amp;gt; new EpochEndOffset(leaderEpoch, 172)).asJava
 
     //Create the thread
-    val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)
-    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, endPoint, new SystemTime())
-    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, endPoint, configs(0), replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, brokerEndPoint, new SystemTime())
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, configs(0), replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))
     thread.addPartitions(Map(t1p0 -&amp;gt; 0, t2p1 -&amp;gt; 0))
 
     //Run it
     thread.doWork()
 
     //We should have truncated to the offsets in the response
-    assertTrue(truncateToCapture.getValues.asScala.contains(156))
-    assertTrue(truncateToCapture.getValues.asScala.contains(172))
+    assertTrue(&quot;Expected &quot; + t1p0 + &quot; to truncate to offset 156 (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(156))
+    assertTrue(&quot;Expected &quot; + t2p1 + &quot; to truncate to offset 172 (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(172))
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def shouldTruncateToOffsetSpecifiedInEpochOffsetResponseIfFollowerHasNoMoreEpochs(): Unit = &lt;/p&gt;
{
+    // Create a capture to track what partitions/offsets are truncated
+    val truncateToCapture: Capture[Long] = newCapture(CaptureType.ALL)
+
+    val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))
+
+    // Setup all the dependencies
+    val configs = TestUtils.createBrokerConfigs(1, &quot;localhost:1234&quot;).map(KafkaConfig.fromProps)
+    val quota = createNiceMock(classOf[ReplicationQuotaManager])
+    val leaderEpochs = createMock(classOf[LeaderEpochCache])
+    val logManager = createMock(classOf[LogManager])
+    val replicaAlterLogDirsManager = createMock(classOf[ReplicaAlterLogDirsManager])
+    val replica = createNiceMock(classOf[Replica])
+    val partition = createMock(classOf[Partition])
+    val replicaManager = createMock(classOf[ReplicaManager])
+
+    val leaderEpochAtFollower = 5
+    val leaderEpochAtLeader = 4
+    val initialLEO = 200
+
+    //Stubs
+    expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).anyTimes()
+    expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()
+    expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(initialLEO)).anyTimes()
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(initialLEO - 3)).anyTimes()
+    expect(leaderEpochs.latestEpoch).andReturn(leaderEpochAtFollower).anyTimes()
+    expect(leaderEpochs.endOffsetFor(leaderEpochAtLeader)).andReturn((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)).anyTimes()
+    expect(replicaManager.logManager).andReturn(logManager).anyTimes()
+    expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()
+    stub(replica, partition, replicaManager)
+
+    replay(leaderEpochs, replicaManager, logManager, quota, replica, partition)
+
+    //Define the offsets for the OffsetsForLeaderEpochResponse, these are used for truncation
+    val offsetsReply = Map(t1p0 -&amp;gt; new EpochEndOffset(leaderEpochAtLeader, 156),
+                           t2p1 -&amp;gt; new EpochEndOffset(leaderEpochAtLeader, 202)).asJava
+
+    //Create the thread
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, brokerEndPoint, new SystemTime())
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, configs(0), replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))
+    thread.addPartitions(Map(t1p0 -&amp;gt; 0, t2p1 -&amp;gt; 0))
+
+    //Run it
+    thread.doWork()
+
+    //We should have truncated to the offsets in the response
+    assertTrue(&quot;Expected &quot; + t1p0 + &quot; to truncate to offset 156 (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(156))
+    assertTrue(&quot;Expected &quot; + t2p1 + &quot; to truncate to offset &quot; + initialLEO +
+               &quot; (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(initialLEO))
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def shouldFetchLeaderEpochSecondTimeIfLeaderRepliesWithEpochNotKnownToFollower(): Unit = &lt;/p&gt;
{
+
+    // Create a capture to track what partitions/offsets are truncated
+    val truncateToCapture: Capture[Long] = newCapture(CaptureType.ALL)
+
+    val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;))
+
+    // Setup all dependencies
+    val quota = createNiceMock(classOf[ReplicationQuotaManager])
+    val leaderEpochs = createNiceMock(classOf[LeaderEpochCache])
+    val logManager = createMock(classOf[LogManager])
+    val replicaAlterLogDirsManager = createMock(classOf[ReplicaAlterLogDirsManager])
+    val replica = createNiceMock(classOf[Replica])
+    val partition = createMock(classOf[Partition])
+    val replicaManager = createMock(classOf[ReplicaManager])
+
+    val initialLEO = 200
+
+    // Stubs
+    expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).anyTimes()
+    expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()
+    expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(initialLEO)).anyTimes()
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(initialLEO - 2)).anyTimes()
+    expect(leaderEpochs.latestEpoch).andReturn(5)
+    expect(leaderEpochs.endOffsetFor(4)).andReturn((3, 120)).anyTimes()
+    expect(leaderEpochs.endOffsetFor(3)).andReturn((3, 120)).anyTimes()
+    expect(replicaManager.logManager).andReturn(logManager).anyTimes()
+    expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()
+    stub(replica, partition, replicaManager)
+
+    replay(leaderEpochs, replicaManager, logManager, quota, replica, partition)
+
+    // Define the offsets for the OffsetsForLeaderEpochResponse
+    val offsets = Map(t1p0 -&amp;gt; new EpochEndOffset(4, 155), t1p1 -&amp;gt; new EpochEndOffset(4, 143)).asJava
+
+    // Create the fetcher thread
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsets, brokerEndPoint, new SystemTime())
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))
+    thread.addPartitions(Map(t1p0 -&amp;gt; 0, t1p1 -&amp;gt; 0))
+
+    // Loop 1 -- both topic partitions will need to fetch another leader epoch
+    thread.doWork()
+    assertEquals(1, mockNetwork.epochFetchCount)
+    assertEquals(0, mockNetwork.fetchCount)
+
+    // Loop 2 should do the second fetch for both topic partitions because the leader replied with
+    // epoch 4 while follower knows only about epoch 3
+    val nextOffsets = Map(t1p0 -&amp;gt; new EpochEndOffset(3, 101), t1p1 -&amp;gt; new EpochEndOffset(3, 102)).asJava
+    mockNetwork.setOffsetsForNextResponse(nextOffsets)
+    thread.doWork()
+    assertEquals(2, mockNetwork.epochFetchCount)
+    assertEquals(1, mockNetwork.fetchCount)
+    assertEquals(&quot;OffsetsForLeaderEpochRequest version.&quot;,
+                 1, mockNetwork.lastUsedOffsetForLeaderEpochVersion)
+
+    //Loop 3 we should not fetch epochs
+    thread.doWork()
+    assertEquals(2, mockNetwork.epochFetchCount)
+    assertEquals(2, mockNetwork.fetchCount)
+
+
+    //We should have truncated to the offsets in the second response
+    assertTrue(&quot;Expected &quot; + t1p1 + &quot; to truncate to offset 102 (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(102))
+    assertTrue(&quot;Expected &quot; + t1p0 + &quot; to truncate to offset 101 (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(101))
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def shouldUseLeaderEndOffsetIfInterBrokerVersionBelow20(): Unit = &lt;/p&gt;
{
+
+    // Create a capture to track what partitions/offsets are truncated
+    val truncateToCapture: Capture[Long] = newCapture(CaptureType.ALL)
+
+    val props = TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;)
+    props.put(KafkaConfig.InterBrokerProtocolVersionProp, &quot;0.11.0&quot;)
+    val config = KafkaConfig.fromProps(props)
+
+    // Setup all dependencies
+    val quota = createNiceMock(classOf[ReplicationQuotaManager])
+    val leaderEpochs = createNiceMock(classOf[LeaderEpochCache])
+    val logManager = createMock(classOf[LogManager])
+    val replicaAlterLogDirsManager = createMock(classOf[ReplicaAlterLogDirsManager])
+    val replica = createNiceMock(classOf[Replica])
+    val partition = createMock(classOf[Partition])
+    val replicaManager = createMock(classOf[ReplicaManager])
+
+    val initialLEO = 200
+
+    // Stubs
+    expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).anyTimes()
+    expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()
+    expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(initialLEO)).anyTimes()
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(initialLEO - 2)).anyTimes()
+    expect(leaderEpochs.latestEpoch).andReturn(5)
+    expect(leaderEpochs.endOffsetFor(4)).andReturn((3, 120)).anyTimes()
+    expect(leaderEpochs.endOffsetFor(3)).andReturn((3, 120)).anyTimes()
+    expect(replicaManager.logManager).andReturn(logManager).anyTimes()
+    expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()
+    stub(replica, partition, replicaManager)
+
+    replay(leaderEpochs, replicaManager, logManager, quota, replica, partition)
+
+    // Define the offsets for the OffsetsForLeaderEpochResponse with undefined epoch to simulate
+    // older protocol version
+    val offsets = Map(t1p0 -&amp;gt; new EpochEndOffset(EpochEndOffset.UNDEFINED_EPOCH, 155), t1p1 -&amp;gt; new EpochEndOffset(EpochEndOffset.UNDEFINED_EPOCH, 143)).asJava
+
+    // Create the fetcher thread
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsets, brokerEndPoint, new SystemTime())
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))
+    thread.addPartitions(Map(t1p0 -&amp;gt; 0, t1p1 -&amp;gt; 0))
+
+    // Loop 1 -- both topic partitions will truncate to leader offset even though they don&apos;t know
+    // about leader epoch
+    thread.doWork()
+    assertEquals(1, mockNetwork.epochFetchCount)
+    assertEquals(1, mockNetwork.fetchCount)
+    assertEquals(&quot;OffsetsForLeaderEpochRequest version.&quot;,
+                 0, mockNetwork.lastUsedOffsetForLeaderEpochVersion)
+
+    //Loop 2 we should not fetch epochs
+    thread.doWork()
+    assertEquals(1, mockNetwork.epochFetchCount)
+    assertEquals(2, mockNetwork.fetchCount)
+
+    //We should have truncated to the offsets in the first response
+    assertTrue(&quot;Expected &quot; + t1p0 + &quot; to truncate to offset 155 (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(155))
+    assertTrue(&quot;Expected &quot; + t1p1 + &quot; to truncate to offset 143 (truncation offsets: &quot; + truncateToCapture.getValues + &quot;)&quot;,
+               truncateToCapture.getValues.asScala.contains(143))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
@@ -227,6 +420,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     expect(partition.truncateTo(capture(truncated), anyBoolean())).anyTimes()&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;br/&gt;
     expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(initialLeo)).anyTimes()&lt;br/&gt;
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(initialFetchOffset)).anyTimes()&lt;br/&gt;
     expect(leaderEpochs.latestEpoch).andReturn(5)&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()&lt;br/&gt;
@@ -234,18 +428,17 @@ class ReplicaFetcherThreadTest &lt;/p&gt;
{
     replay(leaderEpochs, replicaManager, logManager, quota, replica, partition)
 
     //Define the offsets for the OffsetsForLeaderEpochResponse, these are used for truncation
-    val offsetsReply = Map(t1p0 -&amp;gt; new EpochEndOffset(EpochEndOffset.UNDEFINED_EPOCH_OFFSET)).asJava
+    val offsetsReply = Map(t1p0 -&amp;gt; new EpochEndOffset(EpochEndOffset.UNDEFINED_EPOCH, EpochEndOffset.UNDEFINED_EPOCH_OFFSET)).asJava
 
     //Create the thread
-    val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)
-    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, endPoint, new SystemTime())
-    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, endPoint, configs(0), replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, brokerEndPoint, new SystemTime())
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, configs(0), replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))
     thread.addPartitions(Map(t1p0 -&amp;gt; initialFetchOffset))
 
     //Run it
     thread.doWork()
 
-    //We should have truncated to the highwatermark for partitino 2 only
+    //We should have truncated to initial fetch offset
     assertEquals(initialFetchOffset, truncated.getValue)
   }

&lt;p&gt;@@ -265,6 +458,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val partition = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaManager = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.ReplicaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;+    val leaderEpoch = 5&lt;br/&gt;
     val highWaterMark = 100&lt;br/&gt;
     val initialLeo = 300&lt;/p&gt;

&lt;p&gt;@@ -273,7 +467,9 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     expect(partition.truncateTo(capture(truncated), anyBoolean())).anyTimes()&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;br/&gt;
     expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(initialLeo)).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.latestEpoch).andReturn(5)&lt;br/&gt;
+    expect(leaderEpochs.latestEpoch).andReturn(leaderEpoch)&lt;br/&gt;
+    // this is for the last reply with EpochEndOffset(5, 156)&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, initialLeo)).anyTimes()&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()&lt;br/&gt;
     stub(replica, partition, replicaManager)&lt;br/&gt;
@@ -281,14 +477,13 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Define the offsets for the OffsetsForLeaderEpochResponse, these are used for truncation&lt;br/&gt;
     val offsetsReply = mutable.Map(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;t1p0 -&amp;gt; new EpochEndOffset(NOT_LEADER_FOR_PARTITION, EpochEndOffset.UNDEFINED_EPOCH_OFFSET),&lt;/li&gt;
	&lt;li&gt;t1p1 -&amp;gt; new EpochEndOffset(UNKNOWN_SERVER_ERROR, EpochEndOffset.UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
+      t1p0 -&amp;gt; new EpochEndOffset(NOT_LEADER_FOR_PARTITION, EpochEndOffset.UNDEFINED_EPOCH, EpochEndOffset.UNDEFINED_EPOCH_OFFSET),&lt;br/&gt;
+      t1p1 -&amp;gt; new EpochEndOffset(UNKNOWN_SERVER_ERROR, EpochEndOffset.UNDEFINED_EPOCH, EpochEndOffset.UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
     ).asJava&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Create the thread&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)&lt;/li&gt;
	&lt;li&gt;val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, endPoint, new SystemTime())&lt;/li&gt;
	&lt;li&gt;val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, endPoint, configs(0), replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;br/&gt;
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, brokerEndPoint, new SystemTime())&lt;br/&gt;
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, configs(0), replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;br/&gt;
     thread.addPartitions(Map(t1p0 -&amp;gt; 0, t2p1 -&amp;gt; 0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Run thread 3 times&lt;br/&gt;
@@ -300,7 +495,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     assertEquals(0, truncated.getValues.size())&lt;/p&gt;

&lt;p&gt;     //New leader elected and replies&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;offsetsReply.put(t1p0, new EpochEndOffset(156))&lt;br/&gt;
+    offsetsReply.put(t1p0, new EpochEndOffset(leaderEpoch, 156))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     thread.doWork()&lt;/p&gt;

&lt;p&gt;@@ -321,10 +516,14 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     val partition = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val replicaManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicaManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;+    val leaderEpoch = 4&lt;br/&gt;
+&lt;br/&gt;
     //Stub return values&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;br/&gt;
     expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(0)).anyTimes()&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(leaderEpochs.latestEpoch).andReturn(5)&lt;br/&gt;
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(0)).anyTimes()&lt;br/&gt;
+    expect(leaderEpochs.latestEpoch).andReturn(leaderEpoch)&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(leaderEpoch)).andReturn((leaderEpoch, 0)).anyTimes()&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()&lt;br/&gt;
     stub(replica, partition, replicaManager)&lt;br/&gt;
@@ -333,13 +532,12 @@ class ReplicaFetcherThreadTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Define the offsets for the OffsetsForLeaderEpochResponse&lt;br/&gt;
     val offsetsReply = Map(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;t1p0 -&amp;gt; new EpochEndOffset(1), t1p1 -&amp;gt; new EpochEndOffset(1)&lt;br/&gt;
+      t1p0 -&amp;gt; new EpochEndOffset(leaderEpoch, 1), t1p1 -&amp;gt; new EpochEndOffset(leaderEpoch, 1)&lt;br/&gt;
     ).asJava&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Create the fetcher thread&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)&lt;/li&gt;
	&lt;li&gt;val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, endPoint, new SystemTime())&lt;/li&gt;
	&lt;li&gt;val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, endPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;br/&gt;
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, brokerEndPoint, new SystemTime())&lt;br/&gt;
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     thread.addPartitions(Map(t1p0 -&amp;gt; 0, t1p1 -&amp;gt; 0))&lt;br/&gt;
@@ -373,7 +571,9 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).once&lt;br/&gt;
     expect(replica.epochs).andReturn(Some(leaderEpochs)).anyTimes()&lt;br/&gt;
     expect(replica.logEndOffset).andReturn(new LogOffsetMetadata(initialLEO)).anyTimes()&lt;br/&gt;
+    expect(replica.highWatermark).andReturn(new LogOffsetMetadata(initialLEO - 2)).anyTimes()&lt;br/&gt;
     expect(leaderEpochs.latestEpoch).andReturn(5)&lt;br/&gt;
+    expect(leaderEpochs.endOffsetFor(5)).andReturn((5, initialLEO)).anyTimes()&lt;br/&gt;
     expect(replicaManager.logManager).andReturn(logManager).anyTimes()&lt;br/&gt;
     expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()&lt;br/&gt;
     stub(replica, partition, replicaManager)&lt;br/&gt;
@@ -382,13 +582,12 @@ class ReplicaFetcherThreadTest {&lt;/p&gt;

&lt;p&gt;     //Define the offsets for the OffsetsForLeaderEpochResponse&lt;br/&gt;
     val offsetsReply = Map(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;t1p0 -&amp;gt; new EpochEndOffset(52), t1p1 -&amp;gt; new EpochEndOffset(49)&lt;br/&gt;
+      t1p0 -&amp;gt; new EpochEndOffset(5, 52), t1p1 -&amp;gt; new EpochEndOffset(5, 49)&lt;br/&gt;
     ).asJava&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Create the fetcher thread&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val endPoint = new BrokerEndPoint(0, &quot;localhost&quot;, 1000)&lt;/li&gt;
	&lt;li&gt;val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, endPoint, new SystemTime())&lt;/li&gt;
	&lt;li&gt;val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, endPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;br/&gt;
+    val mockNetwork = new ReplicaFetcherMockBlockingSend(offsetsReply, brokerEndPoint, new SystemTime())&lt;br/&gt;
+    val thread = new ReplicaFetcherThread(&quot;bob&quot;, 0, brokerEndPoint, config, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     thread.addPartitions(Map(t1p0 -&amp;gt; 0, t1p1 -&amp;gt; 0))&lt;br/&gt;
@@ -417,4 +616,4 @@ class ReplicaFetcherThreadTest &lt;/p&gt;
{
     expect(replicaManager.getReplicaOrException(t2p1)).andReturn(replica).anyTimes()
     expect(replicaManager.getPartition(t2p1)).andReturn(Some(partition)).anyTimes()
   }
&lt;p&gt;-}&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
index 8a50fcafd1a..877b5c38941 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
@@ -258,7 +258,7 @@ class RequestQuotaTest extends BaseRequestTest {&lt;br/&gt;
           new InitProducerIdRequest.Builder(&quot;abc&quot;)&lt;/p&gt;

&lt;p&gt;         case ApiKeys.OFFSET_FOR_LEADER_EPOCH =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new OffsetsForLeaderEpochRequest.Builder().add(tp, 0)&lt;br/&gt;
+          new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion()).add(tp, 0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case ApiKeys.ADD_PARTITIONS_TO_TXN =&amp;gt;&lt;br/&gt;
           new AddPartitionsToTxnRequest.Builder(&quot;test-transactional-id&quot;, 1, 0, List(tp).asJava)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala b/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala&lt;br/&gt;
index 6288d8faf1d..05a6bb32553 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala&lt;br/&gt;
@@ -38,6 +38,7 @@ import org.junit.&lt;/p&gt;
{After, Before, Test}

&lt;p&gt; import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.&lt;/p&gt;
{ListBuffer =&amp;gt; Buffer}
&lt;p&gt;+import scala.collection.Seq&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;These tests were written to assert the addition of leader epochs to the replication protocol fix the problems&lt;br/&gt;
@@ -298,6 +299,86 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def logsShouldNotDivergeOnUncleanLeaderElections(): Unit = {&lt;br/&gt;
+&lt;br/&gt;
+    // Given two brokers, unclean leader election is enabled&lt;br/&gt;
+    brokers = (100 to 101).map(createBroker(_, enableUncleanLeaderElection = true))&lt;br/&gt;
+&lt;br/&gt;
+    // A single partition topic with 2 replicas, min.isr = 1&lt;br/&gt;
+    adminZkClient.createOrUpdateTopicPartitionAssignmentPathInZK(&lt;br/&gt;
+      topic, Map(0 -&amp;gt; Seq(100, 101)), config = CoreUtils.propsWith((KafkaConfig.MinInSyncReplicasProp, &quot;1&quot;))&lt;br/&gt;
+    )&lt;br/&gt;
+    producer = createNewProducer(getBrokerListStrFromServers(brokers), retries = 5, acks = 1)&lt;br/&gt;
+&lt;br/&gt;
+    // Write one message while both brokers are up&lt;br/&gt;
+    (0 until 1).foreach &lt;/p&gt;
{ i =&amp;gt;
+      producer.send(new ProducerRecord(topic, 0, null, msg))
+      producer.flush()}&lt;br/&gt;
+&lt;br/&gt;
+    // Since we use producer with acks = 1, make sure that logs match for the first epoch&lt;br/&gt;
+    waitForLogsToMatch(brokers(0), brokers(1))&lt;br/&gt;
+&lt;br/&gt;
+    // shutdown broker 100&lt;br/&gt;
+    brokers(0).shutdown()&lt;br/&gt;
+&lt;br/&gt;
+    //Write 1 message&lt;br/&gt;
+    (0 until 1).foreach { i =&amp;gt;+      producer.send(new ProducerRecord(topic, 0, null, msg))+      producer.flush()}
&lt;p&gt;+&lt;br/&gt;
+    brokers(1).shutdown()&lt;br/&gt;
+    brokers(0).startup()&lt;br/&gt;
+&lt;br/&gt;
+    //Bounce the producer (this is required, probably because the broker port changes on restart?)&lt;br/&gt;
+    producer.close()&lt;br/&gt;
+    producer = createNewProducer(getBrokerListStrFromServers(brokers), retries = 5, acks = 1)&lt;br/&gt;
+&lt;br/&gt;
+    //Write 3 messages&lt;br/&gt;
+    (0 until 3).foreach &lt;/p&gt;
{ i =&amp;gt;
+      producer.send(new ProducerRecord(topic, 0, null, msgBigger))
+      producer.flush()}&lt;br/&gt;
+&lt;br/&gt;
+    brokers(0).shutdown()&lt;br/&gt;
+    brokers(1).startup()&lt;br/&gt;
+&lt;br/&gt;
+    //Bounce the producer (this is required, probably because the broker port changes on restart?)&lt;br/&gt;
+    producer.close()&lt;br/&gt;
+    producer = createNewProducer(getBrokerListStrFromServers(brokers), retries = 5, acks = 1)&lt;br/&gt;
+&lt;br/&gt;
+    //Write 1 message&lt;br/&gt;
+    (0 until 1).foreach { i =&amp;gt;
+      producer.send(new ProducerRecord(topic, 0, null, msg))
+      producer.flush()}&lt;br/&gt;
+&lt;br/&gt;
+    brokers(1).shutdown()&lt;br/&gt;
+    brokers(0).startup()&lt;br/&gt;
+&lt;br/&gt;
+    //Bounce the producer (this is required, probably because the broker port changes on restart?)&lt;br/&gt;
+    producer.close()&lt;br/&gt;
+    producer = createNewProducer(getBrokerListStrFromServers(brokers), retries = 5, acks = 1)&lt;br/&gt;
+&lt;br/&gt;
+    //Write 2 messages&lt;br/&gt;
+    (0 until 2).foreach { i =&amp;gt;+      producer.send(new ProducerRecord(topic, 0, null, msgBigger))+      producer.flush()}
&lt;p&gt;+&lt;br/&gt;
+    printSegments()&lt;br/&gt;
+&lt;br/&gt;
+    brokers(1).startup()&lt;br/&gt;
+&lt;br/&gt;
+    waitForLogsToMatch(brokers(0), brokers(1))&lt;br/&gt;
+    printSegments()&lt;br/&gt;
+&lt;br/&gt;
+    def crcSeq(broker: KafkaServer, partition: Int = 0): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
+      val batches = getLog(broker, partition).activeSegment.read(0, None, Integer.MAX_VALUE)
+        .records.batches().asScala.toSeq
+      batches.map(_.checksum)
+    }
&lt;p&gt;+    assertTrue(s&quot;Logs on Broker 100 and Broker 101 should match&quot;,&lt;br/&gt;
+               crcSeq(brokers(0)) == crcSeq(brokers(1)))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   private def log(leader: KafkaServer, follower: KafkaServer): Unit = {&lt;br/&gt;
     info(s&quot;Bounce complete for follower ${follower.config.brokerId}&quot;)&lt;br/&gt;
     info(s&quot;Leader: leo${leader.config.brokerId}: &quot; + getLog(leader, 0).logEndOffset + &quot; cache: &quot; + epochCache(leader).epochEntries())&lt;br/&gt;
@@ -389,12 +470,13 @@ class EpochDrivenReplicationProtocolAcceptanceTest extends ZooKeeperTestHarness&lt;br/&gt;
     brokers.filter(_.config.brokerId != leader)(0)&lt;br/&gt;
   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def createBroker(id: Int): KafkaServer = {&lt;br/&gt;
+  private def createBroker(id: Int, enableUncleanLeaderElection: Boolean = false): KafkaServer = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     val config = createBrokerConfig(id, zkConnect)     if(!KIP_101_ENABLED) {
       config.setProperty(KafkaConfig.InterBrokerProtocolVersionProp, KAFKA_0_11_0_IV1.version)
       config.setProperty(KafkaConfig.LogMessageFormatVersionProp, KAFKA_0_11_0_IV1.version)
     }+    config.setProperty(KafkaConfig.UncleanLeaderElectionEnableProp, enableUncleanLeaderElection.toString)     createServer(fromProps(config))   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
index 4a8df11f8a3..d1f93900ccf 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala&lt;br/&gt;
@@ -50,7 +50,7 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     //Then
     assertEquals(2, cache.latestEpoch())
     assertEquals(EpochEntry(2, 10), cache.epochEntries()(0))
-    assertEquals(11, cache.endOffsetFor(2)) //should match leo
+    assertEquals((2, leo), cache.endOffsetFor(2)) //should match leo
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -67,23 +67,27 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     leo = 14
 
     //Then
-    assertEquals(14, cache.endOffsetFor(2))
+    assertEquals((2, leo), cache.endOffsetFor(2))
   }

&lt;p&gt;   @Test&lt;br/&gt;
   def shouldReturnUndefinedOffsetIfUndefinedEpochRequested() = &lt;/p&gt;
{
     def leoFinder() = new LogOffsetMetadata(0)
+    val expectedEpochEndOffset = (UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)
 
     //Given cache with some data on leader
     val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
+
+    // assign couple of epochs
     cache.assign(epoch = 2, offset = 11)
     cache.assign(epoch = 3, offset = 12)
 
     //When (say a bootstraping follower) sends request for UNDEFINED_EPOCH
-    val offsetFor = cache.endOffsetFor(UNDEFINED_EPOCH)
+    val epochAndOffsetFor = cache.endOffsetFor(UNDEFINED_EPOCH)
 
     //Then
-    assertEquals(UNDEFINED_EPOCH_OFFSET, offsetFor)
+    assertEquals(&quot;Expected undefined epoch and offset if undefined epoch requested. Cache not empty.&quot;,
+                 expectedEpochEndOffset, epochAndOffsetFor)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -140,7 +144,7 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
 
     //Then
-    assertEquals(UNDEFINED_EPOCH_OFFSET, cache.endOffsetFor(0))
+    assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), cache.endOffsetFor(0))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -155,7 +159,8 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     val offsetFor = cache.endOffsetFor(UNDEFINED_EPOCH)
 
     //Then
-    assertEquals(UNDEFINED_EPOCH_OFFSET, offsetFor)
+    assertEquals(&quot;Expected undefined epoch and offset if undefined epoch requested. Empty cache.&quot;,
+                 (UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), offsetFor)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -170,10 +175,10 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     cache.assign(epoch = 7, offset = 13)
 
     //When
-    val offset = cache.endOffsetFor(5 - 1)
+    val epochAndOffset = cache.endOffsetFor(5 - 1)
 
     //Then
-    assertEquals(UNDEFINED_EPOCH_OFFSET, offset)
+    assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), epochAndOffset)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -194,7 +199,7 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     leo = 17
 
     //Then get the start offset of the next epoch
-    assertEquals(15, cache.endOffsetFor(2))
+    assertEquals((2, 15), cache.endOffsetFor(2))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -210,8 +215,9 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     cache.assign(epoch = 4, offset = 17)
 
     //Then
-    assertEquals(13, cache.endOffsetFor(requestedEpoch = 1))
-    assertEquals(17, cache.endOffsetFor(requestedEpoch = 2))
+    assertEquals((0, 13), cache.endOffsetFor(requestedEpoch = 1))
+    assertEquals((2, 17), cache.endOffsetFor(requestedEpoch = 2))
+    assertEquals((2, 17), cache.endOffsetFor(requestedEpoch = 3))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -242,7 +248,7 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     cache.assign(epoch = 2, offset = 100)
 
     //Then
-    assertEquals(UNDEFINED_EPOCH_OFFSET, cache.endOffsetFor(3))
+    assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), cache.endOffsetFor(3))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -258,7 +264,7 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     leo = 7
 
     //Then
-    assertEquals(leo, cache.endOffsetFor(2))
+    assertEquals((2, leo), cache.endOffsetFor(2))
     assertEquals(1, cache.epochEntries.size)
     assertEquals(EpochEntry(2, 6), cache.epochEntries()(0))
   }
&lt;p&gt;@@ -300,10 +306,10 @@ class LeaderEpochFileCacheTest {&lt;br/&gt;
     assertEquals(2, cache.latestEpoch())&lt;/p&gt;

&lt;p&gt;     //Then end offset for epoch 1 shouldn&apos;t have changed&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(6, cache.endOffsetFor(1))&lt;br/&gt;
+    assertEquals((1, 6), cache.endOffsetFor(1))&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//Then end offset for epoch 2 has to be the offset of the epoch 1 message (I can&apos;t thing of a better option)&lt;/li&gt;
	&lt;li&gt;assertEquals(8, cache.endOffsetFor(2))&lt;br/&gt;
+    //Then end offset for epoch 2 has to be the offset of the epoch 1 message (I can&apos;t think of a better option)&lt;br/&gt;
+    assertEquals((2, 8), cache.endOffsetFor(2))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Epoch history shouldn&apos;t have changed&lt;br/&gt;
     assertEquals(EpochEntry(1, 5), cache.epochEntries()(0))&lt;br/&gt;
@@ -340,17 +346,17 @@ class LeaderEpochFileCacheTest {&lt;br/&gt;
     //Then epoch should go up&lt;br/&gt;
     assertEquals(1, cache.latestEpoch())&lt;br/&gt;
     //offset for 1 should still be 0&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(0, cache.endOffsetFor(1))&lt;br/&gt;
+    assertEquals((1, 0), cache.endOffsetFor(1))&lt;br/&gt;
     //offset for epoch 0 should still be 0&lt;/li&gt;
	&lt;li&gt;assertEquals(0, cache.endOffsetFor(0))&lt;br/&gt;
+    assertEquals((0, 0), cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When we write 5 messages as epoch 1&lt;br/&gt;
     leo = 5&lt;/p&gt;

&lt;p&gt;     //Then end offset for epoch(1) should be leo =&amp;gt; 5&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(5, cache.endOffsetFor(1))&lt;br/&gt;
+    assertEquals((1, 5), cache.endOffsetFor(1))&lt;br/&gt;
     //Epoch 0 should still be at offset 0&lt;/li&gt;
	&lt;li&gt;assertEquals(0, cache.endOffsetFor(0))&lt;br/&gt;
+    assertEquals((0, 0), cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     cache.assign(epoch = 2, offset = 5) //leo=5&lt;br/&gt;
@@ -358,13 +364,13 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     leo = 10 //write another 5 messages
 
     //Then end offset for epoch(2) should be leo =&amp;gt; 10
-    assertEquals(10, cache.endOffsetFor(2))
+    assertEquals((2, 10), cache.endOffsetFor(2))
 
     //end offset for epoch(1) should be the start offset of epoch(2) =&amp;gt; 5
-    assertEquals(5, cache.endOffsetFor(1))
+    assertEquals((1, 5), cache.endOffsetFor(1))
 
     //epoch (0) should still be 0
-    assertEquals(0, cache.endOffsetFor(0))
+    assertEquals((0, 0), cache.endOffsetFor(0))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -382,7 +388,7 @@ class LeaderEpochFileCacheTest {&lt;/p&gt;

&lt;p&gt;     //Then epoch should stay, offsets should grow&lt;br/&gt;
     assertEquals(0, cache.latestEpoch())&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(leo, cache.endOffsetFor(0))&lt;br/&gt;
+    assertEquals((0, leo), cache.endOffsetFor(0))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When messages arrive with greater epoch&lt;br/&gt;
     cache.assign(epoch = 1, offset = 3); leo = 4&lt;br/&gt;
@@ -390,7 +396,7 @@ class LeaderEpochFileCacheTest {&lt;br/&gt;
     cache.assign(epoch = 1, offset = 5); leo = 6&lt;/p&gt;

&lt;p&gt;     assertEquals(1, cache.latestEpoch())&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(leo, cache.endOffsetFor(1))&lt;br/&gt;
+    assertEquals((1, leo), cache.endOffsetFor(1))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     cache.assign(epoch = 2, offset = 6); leo = 7&lt;br/&gt;
@@ -398,11 +404,11 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     cache.assign(epoch = 2, offset = 8); leo = 9
 
     assertEquals(2, cache.latestEpoch())
-    assertEquals(leo, cache.endOffsetFor(2))
+    assertEquals((2, leo), cache.endOffsetFor(2))
 
     //Older epochs should return the start offset of the first message in the subsequent epoch.
-    assertEquals(3, cache.endOffsetFor(0))
-    assertEquals(6, cache.endOffsetFor(1))
+    assertEquals((0, 3), cache.endOffsetFor(0))
+    assertEquals((1, 6), cache.endOffsetFor(1))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -648,7 +654,7 @@ class LeaderEpochFileCacheTest &lt;/p&gt;
{
     val cache = new LeaderEpochFileCache(tp, () =&amp;gt; leoFinder, checkpoint)
 
     //Then
-    assertEquals(-1, cache.endOffsetFor(7))
+    assertEquals((UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), cache.endOffsetFor(7))
   }

&lt;p&gt;   @Test&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
index dc6ff9edc1a..907db7a71fa 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
@@ -30,6 +30,7 @@ import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import org.apache.kafka.common.serialization.StringSerializer&lt;br/&gt;
 import org.apache.kafka.common.utils.&lt;/p&gt;
{LogContext, SystemTime}
&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
+import org.apache.kafka.common.protocol.ApiKeys&lt;/p&gt;

&lt;p&gt; import org.junit.Assert._&lt;br/&gt;
 import org.junit.&lt;/p&gt;
{After, Test}
&lt;p&gt;@@ -265,7 +266,7 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;epoch&amp;#93;&lt;/span&gt; class TestFetcherThread(sender: BlockingSend) extends Logging {&lt;/p&gt;

&lt;p&gt;     def leaderOffsetsFor(partitions: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Int&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
-      val request = new OffsetsForLeaderEpochRequest.Builder(toJavaFormat(partitions))
+      val request = new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion(), toJavaFormat(partitions))
       val response = sender.sendRequest(request)
       response.responseBody.asInstanceOf[OffsetsForLeaderEpochResponse].responses.asScala
     }
&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
index 1c01d622438..5c60c0017db 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
@@ -41,7 +41,7 @@ class OffsetsForLeaderEpochTest {&lt;br/&gt;
   @Test&lt;br/&gt;
   def shouldGetEpochsFromReplica(): Unit = {&lt;br/&gt;
     //Given&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val offset = 42&lt;br/&gt;
+    val epochAndOffset = (5, 42L)&lt;br/&gt;
     val epochRequested: Integer = 5&lt;br/&gt;
     val request = Map(tp -&amp;gt; epochRequested)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -49,7 +49,7 @@ class OffsetsForLeaderEpochTest {&lt;br/&gt;
     val mockLog = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.Log&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val mockCache = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.server.epoch.LeaderEpochCache&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     val logManager = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.LogManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect(mockCache.endOffsetFor(epochRequested)).andReturn(offset)&lt;br/&gt;
+    expect(mockCache.endOffsetFor(epochRequested)).andReturn(epochAndOffset)&lt;br/&gt;
     expect(mockLog.leaderEpochCache).andReturn(mockCache).anyTimes()&lt;br/&gt;
     expect(logManager.liveLogDirs).andReturn(Array.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;).anyTimes()&lt;br/&gt;
     replay(mockCache, mockLog, logManager)&lt;br/&gt;
@@ -67,7 +67,7 @@ class OffsetsForLeaderEpochTest 
{
     val response = replicaManager.lastOffsetForLeaderEpoch(request)
 
     //Then
-    assertEquals(new EpochEndOffset(Errors.NONE, offset), response(tp))
+    assertEquals(new EpochEndOffset(Errors.NONE, epochAndOffset._1, epochAndOffset._2), response(tp))
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
@@ -90,7 +90,7 @@ class OffsetsForLeaderEpochTest &lt;/p&gt;
{
     val response = replicaManager.lastOffsetForLeaderEpoch(request)
 
     //Then
-    assertEquals(new EpochEndOffset(Errors.NOT_LEADER_FOR_PARTITION, UNDEFINED_EPOCH_OFFSET), response(tp))
+    assertEquals(new EpochEndOffset(Errors.NOT_LEADER_FOR_PARTITION, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), response(tp))
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -112,6 +112,6 @@ class OffsetsForLeaderEpochTest &lt;/p&gt;
{
     val response = replicaManager.lastOffsetForLeaderEpoch(request)
 
     //Then
-    assertEquals(new EpochEndOffset(Errors.UNKNOWN_TOPIC_OR_PARTITION, UNDEFINED_EPOCH_OFFSET), response(tp))
+    assertEquals(new EpochEndOffset(Errors.UNKNOWN_TOPIC_OR_PARTITION, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET), response(tp))
   }
&lt;p&gt;-}&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala b/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala&lt;br/&gt;
index 1f5bec1cf82..50a4d74f00d 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala&lt;br/&gt;
@@ -28,17 +28,28 @@ import org.apache.kafka.common.utils.&lt;/p&gt;
{SystemTime, Time}

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Stub network client used for testing the ReplicaFetcher, wraps the MockClient used for consumer testing&lt;br/&gt;
+  *&lt;br/&gt;
+  * The common case is that there is only one OFFSET_FOR_LEADER_EPOCH request/response. So, the&lt;br/&gt;
+  * response to OFFSET_FOR_LEADER_EPOCH is &apos;offsets&apos; map. If the test needs to set another round of&lt;br/&gt;
+  * OFFSET_FOR_LEADER_EPOCH with different offsets in response, it should update offsets using&lt;br/&gt;
+  * setOffsetsForNextResponse&lt;br/&gt;
   */&lt;br/&gt;
 class ReplicaFetcherMockBlockingSend(offsets: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;, destination: BrokerEndPoint, time: Time) extends BlockingSend {&lt;br/&gt;
   private val client = new MockClient(new SystemTime)&lt;br/&gt;
   var fetchCount = 0&lt;br/&gt;
   var epochFetchCount = 0&lt;br/&gt;
+  var lastUsedOffsetForLeaderEpochVersion = -1&lt;br/&gt;
   var callback: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;() =&amp;gt; Unit&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
+  var currentOffsets: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = offsets&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def setEpochRequestCallback(postEpochFunction: () =&amp;gt; Unit)&lt;/p&gt;
{
     callback = Some(postEpochFunction)
   }

&lt;p&gt;+  def setOffsetsForNextResponse(newOffsets: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt;): Unit = &lt;/p&gt;
{
+    currentOffsets = newOffsets
+  }
&lt;p&gt;+&lt;br/&gt;
   override def sendRequest(requestBuilder: Builder&lt;span class=&quot;error&quot;&gt;&amp;#91;_ &amp;lt;: AbstractRequest&amp;#93;&lt;/span&gt;): ClientResponse = &lt;/p&gt;
{
 
     //Send the request to the mock client
@@ -50,7 +61,8 @@ class ReplicaFetcherMockBlockingSend(offsets: java.util.Map[TopicPartition, Epoc
       case ApiKeys.OFFSET_FOR_LEADER_EPOCH =&amp;gt;
         callback.foreach(_.apply())
         epochFetchCount += 1
-        new OffsetsForLeaderEpochResponse(offsets)
+        lastUsedOffsetForLeaderEpochVersion = requestBuilder.latestAllowedVersion()
+        new OffsetsForLeaderEpochResponse(currentOffsets)
 
       case ApiKeys.FETCH =&amp;gt;
         fetchCount += 1
@@ -75,4 +87,4 @@ class ReplicaFetcherMockBlockingSend(offsets: java.util.Map[TopicPartition, Epoc
   }

&lt;p&gt;   override def close(): Unit = {}&lt;br/&gt;
-}&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/docs/upgrade.html b/docs/upgrade.html&lt;br/&gt;
index 8bfc61ef480..736d135919b 100644&lt;br/&gt;
&amp;#8212; a/docs/upgrade.html&lt;br/&gt;
+++ b/docs/upgrade.html&lt;br/&gt;
@@ -19,6 +19,58 @@&lt;/p&gt;

&lt;p&gt; &amp;lt;script id=&quot;upgrade-template&quot; type=&quot;text/x-handlebars-template&quot;&amp;gt;&lt;/p&gt;

&lt;p&gt;+&amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_2_0_0&quot; href=&quot;#upgrade_2_0_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, or 1.2.x to 2.0.0&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
+&amp;lt;p&amp;gt;Kafka 2.0.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below,&lt;br/&gt;
+    you guarantee no downtime during the upgrade. However, please review the &amp;lt;a href=&quot;#upgrade_200_notable&quot;&amp;gt;notable changes in 2.0.0&amp;lt;/a&amp;gt; before upgrading.&lt;br/&gt;
+&amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;For a rolling upgrade:&amp;lt;/b&amp;gt;&amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;ol&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you&lt;br/&gt;
+        are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously&lt;br/&gt;
+        overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior&lt;br/&gt;
+        to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt;inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1, 0.10.2, 0.11.0, 1.0, 1.1, 1.2).&amp;lt;/li&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt;log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION  (See &amp;lt;a href=&quot;#upgrade_10_performance_impact&quot;&amp;gt;potential performance impact&lt;br/&gt;
+                following the upgrade&amp;lt;/a&amp;gt; for the details on what this configuration does.)&amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+        If you are upgrading from 0.11.0.x, 1.0.x, 1.1.x or 1.2.x and you have not overridden the message format, then you only need to override&lt;br/&gt;
+        the inter-broker protocol format.&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+            &amp;lt;li&amp;gt;inter.broker.protocol.version=CURRENT_KAFKA_VERSION (0.11.0, 1.0, 1.1, 1.2).&amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
+    &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; Once the entire cluster is upgraded, bump the protocol version by editing &amp;lt;code&amp;gt;inter.broker.protocol.version&amp;lt;/code&amp;gt; and setting it to 2.0.&lt;br/&gt;
+    &amp;lt;li&amp;gt; Restart the brokers one by one for the new protocol version to take effect.&amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; If you have overridden the message format version as instructed above, then you need to do one more rolling restart to&lt;br/&gt;
+        upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later,&lt;br/&gt;
+        change log.message.format.version to 2.0 on each broker and restart them one by one. Note that the older Scala consumer&lt;br/&gt;
+        does not support the new message format introduced in 0.11, so to avoid the performance cost of down-conversion (or to&lt;br/&gt;
+        take advantage of &amp;lt;a href=&quot;#upgrade_11_exactly_once_semantics&quot;&amp;gt;exactly once semantics&amp;lt;/a&amp;gt;), the newer Java consumer must be used.&amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ol&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Additional Upgrade Notes:&amp;lt;/b&amp;gt;&amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;ol&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt;If you are willing to accept downtime, you can simply take all the brokers down, update the code and start them back up. They will start&lt;br/&gt;
+        with the new protocol by default.&amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt;Bumping the protocol version and restarting can be done any time after the brokers are upgraded. It does not have to be immediately after.&lt;br/&gt;
+        Similarly for the message format version.&amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt;If you are using Java8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguties.&lt;br/&gt;
+        Hot-swaping the jar-file only might not work.&amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ol&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_200_notable&quot; href=&quot;#upgrade_200_notable&quot;&amp;gt;Notable changes in 2.0.0&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;h5&amp;gt;&amp;lt;a id=&quot;upgrade_200_new_protocols&quot; href=&quot;#upgrade_200_new_protocols&quot;&amp;gt;New Protocol Versions&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;&lt;br/&gt;
+&amp;lt;ul&amp;gt;&lt;br/&gt;
+    &amp;lt;li&amp;gt; &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-279%3A+Fix+log+divergence+between+leader+and+follower+after+fast+leader+fail+over&quot;&amp;gt;KIP-279&amp;lt;/a&amp;gt;: OffsetsForLeaderEpochResponse v1 introduces a partition-level &amp;lt;code&amp;gt;leader_epoch&amp;lt;/code&amp;gt; field. &amp;lt;/li&amp;gt;&lt;br/&gt;
+&amp;lt;/ul&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;h4&amp;gt;&amp;lt;a id=&quot;upgrade_1_2_0&quot; href=&quot;#upgrade_1_2_0&quot;&amp;gt;Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, or 1.1.x to 1.2.x&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
 &amp;lt;p&amp;gt;Kafka 1.2.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below,&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16469806" author="junrao" created="Thu, 10 May 2018 01:50:49 +0000"  >&lt;p&gt;Merged the PR to trunk.&lt;/p&gt;</comment>
                            <comment id="16911080" author="enether" created="Tue, 20 Aug 2019 07:48:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apovzner&quot; class=&quot;user-hover&quot; rel=&quot;apovzner&quot;&gt;apovzner&lt;/a&gt;&#160;does this affect every version prior to 2.0.0? Asking because I&apos;d like to fill out the `Affected Versions` tag - it&apos;s pretty useful when searching through JIRA&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 13 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3nvxj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>