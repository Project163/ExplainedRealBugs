<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:16:24 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6388] Error while trying to roll a segment that already exists</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6388</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Recreating this issue from &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-654&quot; title=&quot;Irrecoverable error while trying to roll a segment that already exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-654&quot;&gt;&lt;del&gt;KAFKA-654&lt;/del&gt;&lt;/a&gt; as we&apos;ve been hitting it repeatedly in our attempts to get a stable 1.0 cluster running (upgrading from 0.8.2.2).&lt;/p&gt;

&lt;p&gt;After spending 30 min or more spewing log messages like this:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2017-12-19 16:44:28,998] INFO Replica loaded for partition screening.save.results.screening.save.results.processor.error-43 with initial high watermark 0 (kafka.cluster.Replica)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Eventually, the replica thread throws the error below (also referenced in the original issue).  If I remove that partition from the data directory and bounce the broker, it eventually rebalances (assuming it doesn&apos;t hit a different partition with the same error).&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2017-12-19 15:16:24,227] WARN Newly rolled segment file 00000000000000000002.log already exists; deleting it first (kafka.log.Log)
[2017-12-19 15:16:24,227] WARN Newly rolled segment file 00000000000000000002.index already exists; deleting it first (kafka.log.Log)
[2017-12-19 15:16:24,227] WARN Newly rolled segment file 00000000000000000002.timeindex already exists; deleting it first (kafka.log.Log)
[2017-12-19 15:16:24,232] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions __consumer_offsets-20 (kafka.server.ReplicaFetcherManager)
[2017-12-19 15:16:24,297] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error due to (kafka.server.ReplicaFetcherThread)
kafka.common.KafkaException: Error processing data for partition sr.new.sr.new.processor.error-38 offset 2
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:204)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:172)
        at scala.Option.foreach(Option.scala:257)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:172)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:169)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply$mcV$sp(AbstractFetcherThread.scala:169)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:169)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:169)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:217)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:167)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
Caused by: kafka.common.KafkaException: Trying to roll a new log segment for topic partition sr.new.sr.new.processor.error-38 with start offset 2 while it already exists.
        at kafka.log.Log$$anonfun$roll$2.apply(Log.scala:1338)
        at kafka.log.Log$$anonfun$roll$2.apply(Log.scala:1297)
        at kafka.log.Log.maybeHandleIOException(Log.scala:1669)
        at kafka.log.Log.roll(Log.scala:1297)
        at kafka.log.Log.kafka$log$Log$$maybeRoll(Log.scala:1284)
        at kafka.log.Log$$anonfun$append$2.apply(Log.scala:710)
        at kafka.log.Log$$anonfun$append$2.apply(Log.scala:624)
        at kafka.log.Log.maybeHandleIOException(Log.scala:1669)
        at kafka.log.Log.append(Log.scala:624)
        at kafka.log.Log.appendAsFollower(Log.scala:607)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:102)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:41)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:184)
        ... 13 more
[2017-12-19 15:16:24,302] INFO [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13126066">KAFKA-6388</key>
            <summary>Error while trying to roll a segment that already exists</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="dhay">David Hay</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Dec 2017 23:43:15 +0000</created>
                <updated>Thu, 6 Dec 2018 18:55:45 +0000</updated>
                            <resolved>Wed, 5 Dec 2018 23:09:37 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.2</fixVersion>
                    <fixVersion>2.1.1</fixVersion>
                                    <component>log</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="16297655" author="hachikuji" created="Tue, 19 Dec 2017 23:52:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dhay&quot; class=&quot;user-hover&quot; rel=&quot;dhay&quot;&gt;dhay&lt;/a&gt; One thing to check is whether the index corresponding to the segment starting at offset 2 on sr.new.sr.new.processor.error-38 has a zero size. I have seen cases where this caused the broker to unexpectedly roll on an already empty log file.&lt;/p&gt;</comment>
                            <comment id="16297667" author="dhay" created="Wed, 20 Dec 2017 00:02:37 +0000"  >&lt;p&gt;I apparently have 288 partitions with a 0 length .index file.  Does that mean if I remove those partitions from the data directory, they&apos;ll be recreated correctly from the leader?  Seems like Kafka should automatically detect that and recover gracefully.&lt;/p&gt;</comment>
                            <comment id="16297669" author="dhay" created="Wed, 20 Dec 2017 00:05:49 +0000"  >&lt;p&gt;I just got the error again.  Looking at the data directory for the bad partition, I see this:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;total 0
-rw-r--r--. 1 www www 10485760 Dec 19 16:56 00000000000000000000.index
-rw-r--r--. 1 www www        0 Dec 19 16:56 00000000000000000000.log
-rw-r--r--. 1 www www 10485756 Dec 19 16:56 00000000000000000000.timeindex
-rw-r--r--. 1 www www        0 Dec 19 16:56 leader-epoch-checkpoint
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16297674" author="ijuma" created="Wed, 20 Dec 2017 00:11:07 +0000"  >&lt;p&gt;It does look like the code is inconsistent.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
val newOffset = math.max(expectedNextOffset, logEndOffset)
        val logFile = Log.logFile(dir, newOffset)
        val offsetIdxFile = offsetIndexFile(dir, newOffset)
        val timeIdxFile = timeIndexFile(dir, newOffset)
        val txnIdxFile = transactionIndexFile(dir, newOffset)
        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (file &amp;lt;- List(logFile, offsetIdxFile, timeIdxFile, txnIdxFile) &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; file.exists) {
          warn(s&lt;span class=&quot;code-quote&quot;&gt;&quot;Newly rolled segment file ${file.getAbsolutePath} already exists; deleting it first&quot;&lt;/span&gt;)
          Files.delete(file.toPath)
        }

        Option(segments.lastEntry).foreach(_.getValue.onBecomeInactiveSegment())

        &lt;span class=&quot;code-comment&quot;&gt;// take a snapshot of the producer state to facilitate recovery. It is useful to have the snapshot
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// offset align with the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; segment offset since &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; ensures we can recover the segment by beginning
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// with the corresponding snapshot file and scanning the segment data. Because the segment base offset
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// may actually be ahead of the current producer state end offset (which corresponds to the log end offset),
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// we manually override the state offset here prior to taking the snapshot.
&lt;/span&gt;        producerStateManager.updateMapEndOffset(newOffset)
        producerStateManager.takeSnapshot()

        val segment = LogSegment.open(dir,
          baseOffset = newOffset,
          config,
          time = time,
          fileAlreadyExists = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;,
          initFileSize = initFileSize,
          preallocate = config.preallocate)
        val prev = addSegment(segment)
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (prev != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;)
          &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; KafkaException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Trying to roll a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log segment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; topic partition %s with start offset %d &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; it already exists.&quot;&lt;/span&gt;.format(name, newOffset))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note how it tries to delete the files if they already exist, but then an exception is thrown if the segment exists in memory. This also means that we potentially attempt to delete a file that is currently open.&lt;/p&gt;</comment>
                            <comment id="16297723" author="hachikuji" created="Wed, 20 Dec 2017 00:50:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dhay&quot; class=&quot;user-hover&quot; rel=&quot;dhay&quot;&gt;dhay&lt;/a&gt; I&apos;m not saying there&apos;s not a bug here &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. Just trying to help you recover. The case I&apos;m aware of is when the index file of the last segment is empty. Have you seen any instances of this? There shouldn&apos;t be any need to delete the segment itself, just the index. But make sure to restart the broker after deleting the index. &lt;/p&gt;

&lt;p&gt;Can you post the exact exception for the partition whose log directory you posted above?&lt;/p&gt;</comment>
                            <comment id="16298571" author="dhay" created="Wed, 20 Dec 2017 14:23:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;, The exception in the description is the new stack trace.  That is, I copied it from our logs, it&apos;s not the stack trace from the original issue. It doesn&apos;t seem to be limited to the last partition (assuming you mean the partition with the largest id).  Most of our topics have a default 53 partitions, and the id of the partition that has problems is all over the place.&lt;/p&gt;

&lt;p&gt;If I&apos;m understanding correctly. When we see this error, we should be able to recover by shutting down the node, deleting the index files and restarting?&lt;/p&gt;</comment>
                            <comment id="16298791" author="hachikuji" created="Wed, 20 Dec 2017 17:36:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dhay&quot; class=&quot;user-hover&quot; rel=&quot;dhay&quot;&gt;dhay&lt;/a&gt; Let me explain in a little more detail. The specific case is when the latest log segment (i.e. the one with the largest offset) and its index are both empty. There is a bug in the append logic which causes the broker to treat the zero-sized index file as if it were full, which triggers the log rolling logic. But since the log segment is empty, the new rolled log segment will have the same offset and we&apos;ll get the exception you saw initially. To fix the problem, you should shutdown the broker, remove the latest index file, and restart. I am not sure that is what is happening here, but it&apos;s the first thing I would check for. &lt;/p&gt;</comment>
                            <comment id="16298798" author="dhay" created="Wed, 20 Dec 2017 17:40:57 +0000"  >&lt;p&gt;Ok...I&apos;ll look for that the next time we attempt to upgrade to 1.0.  For now we&apos;ve rolled back to 0.8.2.2 and things seem to be (mostly) stable. (We have one cluster that refuses to spin up the ReplicaFetcherThreads...and no exceptions or log messages to indicate why)&lt;/p&gt;</comment>
                            <comment id="16339667" author="pdavidson" created="Thu, 25 Jan 2018 19:15:23 +0000"  >&lt;p&gt;We saw the same issue after an upgrade from 0.9 to 0.11.&#160; Jason Gustafson&apos;s suggestion of shutting&#160;down the broker, deleting the index files, and restarting worked.&lt;/p&gt;</comment>
                            <comment id="16349613" author="hachikuji" created="Fri, 2 Feb 2018 01:05:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pdavidson&quot; class=&quot;user-hover&quot; rel=&quot;pdavidson&quot;&gt;pdavidson&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dhay&quot; class=&quot;user-hover&quot; rel=&quot;dhay&quot;&gt;dhay&lt;/a&gt; We have addressed one known case where this problem can occur in&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6492&quot; title=&quot;LogSemgent.truncateTo() should always resize the index file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6492&quot;&gt;&lt;del&gt;KAFKA-6492&lt;/del&gt;&lt;/a&gt;. When you have encountered this, do you know if the topics were using compaction?&lt;/p&gt;</comment>
                            <comment id="16357331" author="pdavidson" created="Thu, 8 Feb 2018 18:05:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt; We saw this on non-compacted topics.&lt;/p&gt;</comment>
                            <comment id="16357401" author="gcampbell" created="Thu, 8 Feb 2018 18:50:45 +0000"  >&lt;p&gt;Most of the recent times we&apos;ve run into this it&apos;s been on non-compacted topics that have been idle for a while (no data in for &amp;gt; retention.ms) and then begin receiving data again. It&apos;s not happening to every replica, with 15 partitions and 3 replicas sometimes one or two followers will encounter this for a given topic.&lt;/p&gt;</comment>
                            <comment id="16357588" author="dhay" created="Thu, 8 Feb 2018 21:17:14 +0000"  >&lt;p&gt;I&apos;m pretty sure we&apos;re not using compacted topics.&#160; It&apos;s whatever is the default out of the box.&lt;/p&gt;</comment>
                            <comment id="16705603" author="githubbot" created="Sat, 1 Dec 2018 03:32:28 +0000"  >&lt;p&gt;apovzner opened a new pull request #5986: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6388&quot; title=&quot;Error while trying to roll a segment that already exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6388&quot;&gt;&lt;del&gt;KAFKA-6388&lt;/del&gt;&lt;/a&gt;: Recover from rolling an empty segment that already exists&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5986&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   There were several reported incidents where the log is rolled to a new segment with the same base offset as an active segment, causing KafkaException: Trying to roll a new log segment for topic partition X-N with start offset M while it already exists. From what I have investigated so far, this happens to an empty log segment where there is long idle time before the next append and somehow we get to a state where offsetIndex.isFull() returns true due to _maxEntries == 0. This PR recovers from the state where the active segment is empty and we try to roll to a new segment with the same offset: we delete segment and recreate it.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16705605" author="apovzner" created="Sat, 1 Dec 2018 03:38:40 +0000"  >&lt;p&gt;From what I have investigated so far, the issue happens when we roll an empty log segment with the same base offset.&#160;This happens because offsetIndex isFull() returns true as a result of&#160;&#160;_maxEntries == 0. I haven&apos;t found the cause of getting to this state. However, every time this happened, there was a long idle time before an append. I opened PR&#160;to recover from this state &#8211; if we roll an empty segment, we delete and recreate it.&#160;&lt;a href=&quot;https://github.com/apache/kafka/pull/5986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5986&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16710713" author="githubbot" created="Wed, 5 Dec 2018 22:49:22 +0000"  >&lt;p&gt;hachikuji closed pull request #5986: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6388&quot; title=&quot;Error while trying to roll a segment that already exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6388&quot;&gt;&lt;del&gt;KAFKA-6388&lt;/del&gt;&lt;/a&gt;: Recover from rolling an empty segment that already exists&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5986&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index 688736c7d66..c448805e0e6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -1548,8 +1548,8 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         in the header.&lt;br/&gt;
       */&lt;br/&gt;
       appendInfo.firstOffset match &lt;/p&gt;
{
-        case Some(firstOffset) =&amp;gt; roll(firstOffset)
-        case None =&amp;gt; roll(maxOffsetInMessages - Integer.MAX_VALUE)
+        case Some(firstOffset) =&amp;gt; roll(Some(firstOffset))
+        case None =&amp;gt; roll(Some(maxOffsetInMessages - Integer.MAX_VALUE))
       }
&lt;p&gt;     } else {&lt;br/&gt;
       segment&lt;br/&gt;
@@ -1562,22 +1562,45 @@ class Log(@volatile var dir: File,&lt;br/&gt;
    *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return The newly rolled segment&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def roll(expectedNextOffset: Long = 0): LogSegment = {&lt;br/&gt;
+  def roll(expectedNextOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None): LogSegment = {&lt;br/&gt;
     maybeHandleIOException(s&quot;Error while rolling log segment for $topicPartition in dir ${dir.getParent}&quot;) {&lt;br/&gt;
       val start = time.hiResClockMs()&lt;br/&gt;
       lock synchronized {&lt;br/&gt;
         checkIfMemoryMappedBufferClosed()&lt;/li&gt;
	&lt;li&gt;val newOffset = math.max(expectedNextOffset, logEndOffset)&lt;br/&gt;
+        val newOffset = math.max(expectedNextOffset.getOrElse(0L), logEndOffset)&lt;br/&gt;
         val logFile = Log.logFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;val offsetIdxFile = offsetIndexFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;val timeIdxFile = timeIndexFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;val txnIdxFile = transactionIndexFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;for (file &amp;lt;- List(logFile, offsetIdxFile, timeIdxFile, txnIdxFile) if file.exists) {&lt;/li&gt;
	&lt;li&gt;warn(s&quot;Newly rolled segment file ${file.getAbsolutePath} already exists; deleting it first&quot;)&lt;/li&gt;
	&lt;li&gt;Files.delete(file.toPath)&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Option(segments.lastEntry).foreach(_.getValue.onBecomeInactiveSegment())&lt;br/&gt;
+        if (segments.containsKey(newOffset)) {&lt;br/&gt;
+          // segment with the same base offset already exists and loaded&lt;br/&gt;
+          if (activeSegment.baseOffset == newOffset &amp;amp;&amp;amp; activeSegment.size == 0) {&lt;br/&gt;
+            // We have seen this happen (see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6388&quot; title=&quot;Error while trying to roll a segment that already exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6388&quot;&gt;&lt;del&gt;KAFKA-6388&lt;/del&gt;&lt;/a&gt;) after shouldRoll() returns true for an&lt;br/&gt;
+            // active segment of size zero because of one of the indexes is &quot;full&quot; (due to _maxEntries == 0).&lt;br/&gt;
+            warn(s&quot;Trying to roll a new log segment with start offset $newOffset &quot; +&lt;br/&gt;
+                 s&quot;=max(provided offset = $expectedNextOffset, LEO = $logEndOffset) while it already &quot; +&lt;br/&gt;
+                 s&quot;exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},&quot; +&lt;br/&gt;
+                 s&quot; size of offset index: ${activeSegment.offsetIndex.entries}.&quot;)&lt;br/&gt;
+            deleteSegment(activeSegment)&lt;br/&gt;
+          } else {&lt;br/&gt;
+            throw new KafkaException(s&quot;Trying to roll a new log segment for topic partition $topicPartition with start offset $newOffset&quot; +&lt;br/&gt;
+                                     s&quot; =max(provided offset = $expectedNextOffset, LEO = $logEndOffset) while it already exists. Existing &quot; +&lt;br/&gt;
+                                     s&quot;segment is ${segments.get(newOffset)}.&quot;)&lt;br/&gt;
+          }&lt;br/&gt;
+        } else if (!segments.isEmpty &amp;amp;&amp;amp; newOffset &amp;lt; activeSegment.baseOffset) 
{
+          throw new KafkaException(
+            s&quot;Trying to roll a new log segment for topic partition $topicPartition with &quot; +
+            s&quot;start offset $newOffset =max(provided offset = $expectedNextOffset, LEO = $logEndOffset) lower than start offset of the active segment $activeSegment&quot;)
+        }
&lt;p&gt; else {&lt;br/&gt;
+          val offsetIdxFile = offsetIndexFile(dir, newOffset)&lt;br/&gt;
+          val timeIdxFile = timeIndexFile(dir, newOffset)&lt;br/&gt;
+          val txnIdxFile = transactionIndexFile(dir, newOffset)&lt;br/&gt;
+&lt;br/&gt;
+          for (file &amp;lt;- List(logFile, offsetIdxFile, timeIdxFile, txnIdxFile) if file.exists) {&lt;br/&gt;
+            warn(s&quot;Newly rolled segment file ${file.getAbsolutePath} already exists; deleting it first&quot;)&lt;br/&gt;
+            Files.delete(file.toPath)&lt;br/&gt;
+          }&lt;br/&gt;
+&lt;br/&gt;
+          Option(segments.lastEntry).foreach(_.getValue.onBecomeInactiveSegment())&lt;br/&gt;
+        }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // take a snapshot of the producer state to facilitate recovery. It is useful to have the snapshot&lt;br/&gt;
         // offset align with the new segment offset since this ensures we can recover the segment by beginning&lt;br/&gt;
@@ -1594,10 +1617,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
           fileAlreadyExists = false,&lt;br/&gt;
           initFileSize = initFileSize,&lt;br/&gt;
           preallocate = config.preallocate)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val prev = addSegment(segment)&lt;/li&gt;
	&lt;li&gt;if (prev != null)&lt;/li&gt;
	&lt;li&gt;throw new KafkaException(s&quot;Trying to roll a new log segment for topic partition $topicPartition with &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;start offset $newOffset while it already exists.&quot;)&lt;br/&gt;
+        addSegment(segment)&lt;br/&gt;
         // We need to update the segment base offset and append position data of the metadata when log rolls.&lt;br/&gt;
         // The next offset should not change.&lt;br/&gt;
         updateLogEndOffset(nextOffsetMetadata.messageOffset)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
index ff5af6123e2..51477b627cb 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
@@ -1118,7 +1118,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     log.appendAsFollower(record1)&lt;br/&gt;
     val record2 = messageWithOffset(&quot;hello&quot;.getBytes, &quot;hello&quot;.getBytes, 1)&lt;br/&gt;
     log.appendAsFollower(record2)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;log.roll(Int.MaxValue/2) // starting a new log segment at offset Int.MaxValue/2&lt;br/&gt;
+    log.roll(Some(Int.MaxValue/2)) // starting a new log segment at offset Int.MaxValue/2&lt;br/&gt;
     val record3 = messageWithOffset(&quot;hello&quot;.getBytes, &quot;hello&quot;.getBytes, Int.MaxValue/2)&lt;br/&gt;
     log.appendAsFollower(record3)&lt;br/&gt;
     val record4 = messageWithOffset(&quot;hello&quot;.getBytes, &quot;hello&quot;.getBytes, Int.MaxValue.toLong + 1)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index fe2820cba1c..b4dc807a876 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -142,6 +142,52 @@ class LogTest 
{
     assertEquals(&quot;Appending an empty message set should not roll log even if sufficient time has passed.&quot;, numSegments, log.numberOfSegments)
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def testRollSegmentThatAlreadyExists() &lt;/p&gt;
{
+    val logConfig = LogTest.createLogConfig(segmentMs = 1 * 60 * 60L)
+
+    // create a log
+    val log = createLog(logDir, logConfig)
+    assertEquals(&quot;Log begins with a single empty segment.&quot;, 1, log.numberOfSegments)
+
+    // roll active segment with the same base offset of size zero should recreate the segment
+    log.roll(Some(0L))
+    assertEquals(&quot;Expect 1 segment after roll() empty segment with base offset.&quot;, 1, log.numberOfSegments)
+
+    // should be able to append records to active segment
+    val records = TestUtils.records(
+      List(new SimpleRecord(mockTime.milliseconds, &quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)),
+      baseOffset = 0L, partitionLeaderEpoch = 0)
+    log.appendAsFollower(records)
+    assertEquals(&quot;Expect one segment.&quot;, 1, log.numberOfSegments)
+    assertEquals(0L, log.activeSegment.baseOffset)
+
+    // make sure we can append more records
+    val records2 = TestUtils.records(
+      List(new SimpleRecord(mockTime.milliseconds + 10, &quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)),
+      baseOffset = 1L, partitionLeaderEpoch = 0)
+    log.appendAsFollower(records2)
+
+    assertEquals(&quot;Expect two records in the log&quot;, 2, log.logEndOffset)
+    assertEquals(0, readLog(log, 0, 100, Some(1)).records.batches.iterator.next().lastOffset)
+    assertEquals(1, readLog(log, 1, 100, Some(2)).records.batches.iterator.next().lastOffset)
+
+    // roll so that active segment is empty
+    log.roll()
+    assertEquals(&quot;Expect base offset of active segment to be LEO&quot;, 2L, log.activeSegment.baseOffset)
+    assertEquals(&quot;Expect two segments.&quot;, 2, log.numberOfSegments)
+
+    // manually resize offset index to force roll of an empty active segment on next append
+    log.activeSegment.offsetIndex.resize(0)
+    val records3 = TestUtils.records(
+      List(new SimpleRecord(mockTime.milliseconds + 12, &quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),
+      baseOffset = 2L, partitionLeaderEpoch = 0)
+    log.appendAsFollower(records3)
+    assertTrue(log.activeSegment.offsetIndex.maxEntries &amp;gt; 1)
+    assertEquals(2, readLog(log, 2, 100, Some(3)).records.batches.iterator.next().lastOffset)
+    assertEquals(&quot;Expect two segments.&quot;, 2, log.numberOfSegments)
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;OutOfOrderSequenceException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   def testNonSequentialAppend(): Unit = {&lt;br/&gt;
     // create a log&lt;br/&gt;
@@ -828,17 +874,17 @@ class LogTest {&lt;br/&gt;
     val logConfig = LogTest.createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
     val log = createLog(logDir, logConfig)&lt;br/&gt;
     log.appendAsLeader(TestUtils.singletonRecords(&quot;a&quot;.getBytes), leaderEpoch = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.roll(1L)&lt;br/&gt;
+    log.roll(Some(1L))&lt;br/&gt;
     assertEquals(Some(1L), log.latestProducerSnapshotOffset)&lt;br/&gt;
     assertEquals(Some(1L), log.oldestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.appendAsLeader(TestUtils.singletonRecords(&quot;b&quot;.getBytes), leaderEpoch = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.roll(2L)&lt;br/&gt;
+    log.roll(Some(2L))&lt;br/&gt;
     assertEquals(Some(2L), log.latestProducerSnapshotOffset)&lt;br/&gt;
     assertEquals(Some(1L), log.oldestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.appendAsLeader(TestUtils.singletonRecords(&quot;c&quot;.getBytes), leaderEpoch = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.roll(3L)&lt;br/&gt;
+    log.roll(Some(3L))&lt;br/&gt;
     assertEquals(Some(3L), log.latestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // roll triggers a flush at the starting offset of the new segment, we should retain all snapshots&lt;br/&gt;
@@ -1282,7 +1328,7 @@ class LogTest &lt;/p&gt;
{
     val logConfig = LogTest.createLogConfig()
     val log = createLog(logDir,  logConfig)
     log.closeHandlers()
-    log.roll(1)
+    log.roll(Some(1L))
   }

&lt;p&gt;   @Test&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16710789" author="githubbot" created="Thu, 6 Dec 2018 00:23:02 +0000"  >&lt;p&gt;apovzner opened a new pull request #6006: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6388&quot; title=&quot;Error while trying to roll a segment that already exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6388&quot;&gt;&lt;del&gt;KAFKA-6388&lt;/del&gt;&lt;/a&gt;: Recover from rolling an empty segment that already exists (branch 1.1)&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/6006&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6006&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Same as &lt;a href=&quot;https://github.com/apache/kafka/pull/5986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5986&lt;/a&gt; but for AK 1.1. &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16711821" author="githubbot" created="Thu, 6 Dec 2018 18:17:52 +0000"  >&lt;p&gt;hachikuji closed pull request #6006: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6388&quot; title=&quot;Error while trying to roll a segment that already exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6388&quot;&gt;&lt;del&gt;KAFKA-6388&lt;/del&gt;&lt;/a&gt;: Recover from rolling an empty segment that already exists (branch 1.1)&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/6006&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/6006&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index eeb569a0771..31acaeac57f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -1318,7 +1318,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
         base offset was too low to contain the next message.  This edge case is possible when a replica is recovering a&lt;br/&gt;
         highly compacted topic from scratch.&lt;br/&gt;
        */&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;roll(maxOffsetInMessages - Integer.MAX_VALUE)&lt;br/&gt;
+      roll(Some(maxOffsetInMessages - Integer.MAX_VALUE))&lt;br/&gt;
     } else 
{
       segment
     }
&lt;p&gt;@@ -1330,22 +1330,44 @@ class Log(@volatile var dir: File,&lt;br/&gt;
    *&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return The newly rolled segment&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def roll(expectedNextOffset: Long = 0): LogSegment = {&lt;br/&gt;
+  def roll(expectedNextOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None): LogSegment = {&lt;br/&gt;
     maybeHandleIOException(s&quot;Error while rolling log segment for $topicPartition in dir ${dir.getParent}&quot;) {&lt;br/&gt;
       val start = time.hiResClockMs()&lt;br/&gt;
       lock synchronized {&lt;br/&gt;
         checkIfMemoryMappedBufferClosed()&lt;/li&gt;
	&lt;li&gt;val newOffset = math.max(expectedNextOffset, logEndOffset)&lt;br/&gt;
+        val newOffset = math.max(expectedNextOffset.getOrElse(0L), logEndOffset)&lt;br/&gt;
         val logFile = Log.logFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;val offsetIdxFile = offsetIndexFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;val timeIdxFile = timeIndexFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;val txnIdxFile = transactionIndexFile(dir, newOffset)&lt;/li&gt;
	&lt;li&gt;for (file &amp;lt;- List(logFile, offsetIdxFile, timeIdxFile, txnIdxFile) if file.exists) {&lt;/li&gt;
	&lt;li&gt;warn(s&quot;Newly rolled segment file ${file.getAbsolutePath} already exists; deleting it first&quot;)&lt;/li&gt;
	&lt;li&gt;Files.delete(file.toPath)&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Option(segments.lastEntry).foreach(_.getValue.onBecomeInactiveSegment())&lt;br/&gt;
+        if (segments.containsKey(newOffset)) {&lt;br/&gt;
+          // segment with the same base offset already exists and loaded&lt;br/&gt;
+          if (activeSegment.baseOffset == newOffset &amp;amp;&amp;amp; activeSegment.size == 0) {&lt;br/&gt;
+            // We have seen this happen (see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6388&quot; title=&quot;Error while trying to roll a segment that already exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6388&quot;&gt;&lt;del&gt;KAFKA-6388&lt;/del&gt;&lt;/a&gt;) after shouldRoll() returns true for an&lt;br/&gt;
+            // active segment of size zero because of one of the indexes is &quot;full&quot; (due to _maxEntries == 0).&lt;br/&gt;
+            warn(s&quot;Trying to roll a new log segment with start offset $newOffset &quot; +&lt;br/&gt;
+                 s&quot;=max(provided offset = $expectedNextOffset, LEO = $logEndOffset) while it already &quot; +&lt;br/&gt;
+                 s&quot;exists and is active with size 0. Size of time index: ${activeSegment.timeIndex.entries},&quot; +&lt;br/&gt;
+                 s&quot; size of offset index: ${activeSegment.offsetIndex.entries}.&quot;)&lt;br/&gt;
+            deleteSegment(activeSegment)&lt;br/&gt;
+          } else {&lt;br/&gt;
+            throw new KafkaException(s&quot;Trying to roll a new log segment for topic partition $topicPartition with start offset $newOffset&quot; +&lt;br/&gt;
+                                     s&quot; =max(provided offset = $expectedNextOffset, LEO = $logEndOffset) while it already exists. Existing &quot; +&lt;br/&gt;
+                                     s&quot;segment is ${segments.get(newOffset)}.&quot;)&lt;br/&gt;
+          }&lt;br/&gt;
+        } else if (!segments.isEmpty &amp;amp;&amp;amp; newOffset &amp;lt; activeSegment.baseOffset) 
{
+          throw new KafkaException(
+            s&quot;Trying to roll a new log segment for topic partition $topicPartition with &quot; +
+            s&quot;start offset $newOffset =max(provided offset = $expectedNextOffset, LEO = $logEndOffset) lower than start offset of the active segment $activeSegment&quot;)
+        }
&lt;p&gt; else {&lt;br/&gt;
+          val offsetIdxFile = offsetIndexFile(dir, newOffset)&lt;br/&gt;
+          val timeIdxFile = timeIndexFile(dir, newOffset)&lt;br/&gt;
+          val txnIdxFile = transactionIndexFile(dir, newOffset)&lt;br/&gt;
+          for (file &amp;lt;- List(logFile, offsetIdxFile, timeIdxFile, txnIdxFile) if file.exists) {&lt;br/&gt;
+            warn(s&quot;Newly rolled segment file ${file.getAbsolutePath} already exists; deleting it first&quot;)&lt;br/&gt;
+            Files.delete(file.toPath)&lt;br/&gt;
+          }&lt;br/&gt;
+&lt;br/&gt;
+          Option(segments.lastEntry).foreach(_.getValue.onBecomeInactiveSegment())&lt;br/&gt;
+        }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // take a snapshot of the producer state to facilitate recovery. It is useful to have the snapshot&lt;br/&gt;
         // offset align with the new segment offset since this ensures we can recover the segment by beginning&lt;br/&gt;
@@ -1362,9 +1384,7 @@ class Log(@volatile var dir: File,&lt;br/&gt;
           fileAlreadyExists = false,&lt;br/&gt;
           initFileSize = initFileSize,&lt;br/&gt;
           preallocate = config.preallocate)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val prev = addSegment(segment)&lt;/li&gt;
	&lt;li&gt;if (prev != null)&lt;/li&gt;
	&lt;li&gt;throw new KafkaException(&quot;Trying to roll a new log segment for topic partition %s with start offset %d while it already exists.&quot;.format(name, newOffset))&lt;br/&gt;
+        addSegment(segment)&lt;br/&gt;
         // We need to update the segment base offset and append position data of the metadata when log rolls.&lt;br/&gt;
         // The next offset should not change.&lt;br/&gt;
         updateLogEndOffset(nextOffsetMetadata.messageOffset)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
index 6e11a1e2264..26455f1ae0f 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
@@ -1038,7 +1038,7 @@ class LogCleanerTest extends JUnitSuite {&lt;br/&gt;
     log.appendAsFollower(record1)&lt;br/&gt;
     val record2 = messageWithOffset(&quot;hello&quot;.getBytes, &quot;hello&quot;.getBytes, 1)&lt;br/&gt;
     log.appendAsFollower(record2)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;log.roll(Int.MaxValue/2) // starting a new log segment at offset Int.MaxValue/2&lt;br/&gt;
+    log.roll(Some(Int.MaxValue/2)) // starting a new log segment at offset Int.MaxValue/2&lt;br/&gt;
     val record3 = messageWithOffset(&quot;hello&quot;.getBytes, &quot;hello&quot;.getBytes, Int.MaxValue/2)&lt;br/&gt;
     log.appendAsFollower(record3)&lt;br/&gt;
     val record4 = messageWithOffset(&quot;hello&quot;.getBytes, &quot;hello&quot;.getBytes, Int.MaxValue.toLong + 1)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index e0dcdfdbb55..6be9203180a 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -139,6 +139,52 @@ class LogTest 
{
     assertEquals(&quot;Appending an empty message set should not roll log even if sufficient time has passed.&quot;, numSegments, log.numberOfSegments)
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def testRollSegmentThatAlreadyExists() &lt;/p&gt;
{
+    val logConfig = createLogConfig(segmentMs = 1 * 60 * 60L)
+
+    // create a log
+    val log = createLog(logDir, logConfig)
+    assertEquals(&quot;Log begins with a single empty segment.&quot;, 1, log.numberOfSegments)
+
+    // roll active segment with the same base offset of size zero should recreate the segment
+    log.roll(Some(0L))
+    assertEquals(&quot;Expect 1 segment after roll() empty segment with base offset.&quot;, 1, log.numberOfSegments)
+
+    // should be able to append records to active segment
+    val records = TestUtils.records(
+      List(new SimpleRecord(mockTime.milliseconds, &quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes)),
+      baseOffset = 0L, partitionLeaderEpoch = 0)
+    log.appendAsFollower(records)
+    assertEquals(&quot;Expect one segment.&quot;, 1, log.numberOfSegments)
+    assertEquals(0L, log.activeSegment.baseOffset)
+
+    // make sure we can append more records
+    val records2 = TestUtils.records(
+      List(new SimpleRecord(mockTime.milliseconds + 10, &quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)),
+      baseOffset = 1L, partitionLeaderEpoch = 0)
+    log.appendAsFollower(records2)
+
+    assertEquals(&quot;Expect two records in the log.&quot;, 2, log.logEndOffset)
+    assertEquals(0, log.readUncommitted(0, 100, Some(1)).records.batches.iterator.next().lastOffset)
+    assertEquals(1, log.readUncommitted(1, 100, Some(2)).records.batches.iterator.next().lastOffset)
+
+    // roll so that active segment is empty
+    log.roll()
+    assertEquals(&quot;Expect base offset of active segment to be LEO&quot;, 2L, log.activeSegment.baseOffset)
+    assertEquals(&quot;Expect two segments.&quot;, 2, log.numberOfSegments)
+
+    // manually resize offset index to force roll of an empty active segment on next append
+    log.activeSegment.offsetIndex.resize(0)
+    val records3 = TestUtils.records(
+      List(new SimpleRecord(mockTime.milliseconds + 12, &quot;k3&quot;.getBytes, &quot;v3&quot;.getBytes)),
+      baseOffset = 2L, partitionLeaderEpoch = 0)
+    log.appendAsFollower(records3)
+    assertTrue(log.activeSegment.offsetIndex.maxEntries &amp;gt; 1)
+    assertEquals(2, log.readUncommitted(2, 100, Some(3)).records.batches.iterator.next().lastOffset)
+    assertEquals(&quot;Expect two segments.&quot;, 2, log.numberOfSegments)
+  }
&lt;p&gt;+&lt;br/&gt;
   @Test(expected = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;OutOfOrderSequenceException&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   def testNonSequentialAppend(): Unit = {&lt;br/&gt;
     // create a log&lt;br/&gt;
@@ -789,17 +835,17 @@ class LogTest {&lt;br/&gt;
     val logConfig = createLogConfig(segmentBytes = 2048 * 5)&lt;br/&gt;
     val log = createLog(logDir, logConfig)&lt;br/&gt;
     log.appendAsLeader(TestUtils.singletonRecords(&quot;a&quot;.getBytes), leaderEpoch = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.roll(1L)&lt;br/&gt;
+    log.roll(Some(1L))&lt;br/&gt;
     assertEquals(Some(1L), log.latestProducerSnapshotOffset)&lt;br/&gt;
     assertEquals(Some(1L), log.oldestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.appendAsLeader(TestUtils.singletonRecords(&quot;b&quot;.getBytes), leaderEpoch = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.roll(2L)&lt;br/&gt;
+    log.roll(Some(2L))&lt;br/&gt;
     assertEquals(Some(2L), log.latestProducerSnapshotOffset)&lt;br/&gt;
     assertEquals(Some(1L), log.oldestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     log.appendAsLeader(TestUtils.singletonRecords(&quot;c&quot;.getBytes), leaderEpoch = 0)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.roll(3L)&lt;br/&gt;
+    log.roll(Some(3L))&lt;br/&gt;
     assertEquals(Some(3L), log.latestProducerSnapshotOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // roll triggers a flush at the starting offset of the new segment, we should retain all snapshots&lt;br/&gt;
@@ -1243,7 +1289,7 @@ class LogTest &lt;/p&gt;
{
     val logConfig = createLogConfig()
     val log = createLog(logDir,  logConfig)
     log.closeHandlers()
-    log.roll(1)
+    log.roll(Some(1))
   }

&lt;p&gt;   @Test&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10020">
                    <name>Cloners</name>
                                            <outwardlinks description="is a clone of">
                                        <issuelink>
            <issuekey id="12618991">KAFKA-654</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 49 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3o3e7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>