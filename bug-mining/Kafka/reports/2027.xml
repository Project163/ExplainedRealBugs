<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:14:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7128] Lagging high watermark can lead to committed data loss after ISR expansion</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7128</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Some model checking exposed a weakness in the ISR expansion logic. We know that the high watermark can go backwards after a leader failover, but we may not have known that this can lead to the loss of committed data.&lt;/p&gt;

&lt;p&gt;Say we have three replicas: r1, r2, and r3. Initially, the ISR consists of (r1, r2) and the leader is r1. r3 is a new replica which has not begun fetching. The data up to offset 10 has been committed to the ISR. Here is the initial state:&lt;/p&gt;

&lt;p&gt;State 1&lt;br/&gt;
ISR: (r1, r2)&lt;br/&gt;
 Leader: r1&lt;br/&gt;
 r1: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=10, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r2: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=5, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r3: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=0, leo=0&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Replica 1 then initiates shutdown (or fails) and leaves the ISR, which makes r2 the new leader. The high watermark is still lagging r1.&lt;/p&gt;

&lt;p&gt;State 2&lt;br/&gt;
ISR: (r2)&lt;br/&gt;
 Leader: r2&lt;br/&gt;
 r1 (offline): &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=10, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r2: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=5, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r3: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=0, leo=0&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Replica 3 then catch up to the high watermark on r2 and joins the ISR. Perhaps it&apos;s high watermark is lagging behind r2, but this is unimportant.&lt;/p&gt;

&lt;p&gt;State 3&lt;br/&gt;
ISR: (r2, r3)&lt;br/&gt;
 Leader: r2&lt;br/&gt;
 r1 (offline): &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=10, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r2: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=5, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r3: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=0, leo=5&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Now r2 fails and r3 is elected leader and is the only member of the ISR. The committed data from offsets 5 to 10 has been lost.&lt;/p&gt;

&lt;p&gt;State 4&lt;br/&gt;
ISR: (r3)&lt;br/&gt;
 Leader: r3&lt;br/&gt;
 r1 (offline): &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=10, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r2 (offline): &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=5, leo=10&amp;#93;&lt;/span&gt;&lt;br/&gt;
 r3: &lt;span class=&quot;error&quot;&gt;&amp;#91;hw=0, leo=5&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The bug is the fact that we allowed r3 into the ISR after the local high watermark had been reached. Since the follower does not know the true high watermark for the previous leader&apos;s epoch, it should not allow a replica to join the ISR until it has caught up to an offset within its own epoch.&lt;/p&gt;

&lt;p&gt;Note this is related to &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-207%3A+Offsets+returned+by+ListOffsetsResponse+should+be+monotonically+increasing+even+during+a+partition+leader+change&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-207%3A+Offsets+returned+by+ListOffsetsResponse+should+be+monotonically+increasing+even+during+a+partition+leader+change&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13169614">KAFKA-7128</key>
            <summary>Lagging high watermark can lead to committed data loss after ISR expansion</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="apovzner">Anna Povzner</assignee>
                                    <reporter username="hachikuji">Jason Gustafson</reporter>
                        <labels>
                    </labels>
                <created>Mon, 2 Jul 2018 19:58:57 +0000</created>
                <updated>Tue, 28 Aug 2018 18:09:59 +0000</updated>
                            <resolved>Tue, 28 Aug 2018 18:09:59 +0000</resolved>
                                                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16548596" author="apovzner" created="Thu, 19 Jul 2018 00:05:01 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;, do you mind if pick up this JIRA?&lt;/p&gt;</comment>
                            <comment id="16548600" author="hachikuji" created="Thu, 19 Jul 2018 00:14:47 +0000"  >&lt;p&gt;Yes, go ahead. Sorry, I assigned myself initially because I thought I had time, but it doesn&apos;t look like I will be able to get to it for a couple weeks.&lt;/p&gt;</comment>
                            <comment id="16556274" author="lindong" created="Wed, 25 Jul 2018 21:12:22 +0000"  >&lt;p&gt;Thanks for the explanation &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hachikuji&quot; class=&quot;user-hover&quot; rel=&quot;hachikuji&quot;&gt;hachikuji&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I slightly modified the description to provide state for each step for ease of discussion. It is not obvious from the description yet whether state 3 is possible.&#160;In order to move from state 2 to state 3, LEO in replica 3&#160;needs to increase from 0 to 5, which means replica 3 needs to receive a FetchResponse containing messages up to offset 5. Since the hw in the&#160;FetchResponse should be 5, should replica 3&apos;s hw be increased to 5 as well?&lt;/p&gt;</comment>
                            <comment id="16556277" author="lindong" created="Wed, 25 Jul 2018 21:14:17 +0000"  >&lt;p&gt;BTW, if we want to&#160;make sure there is no&#160;message loss, it may be worthwhile to work &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7040&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-7040&lt;/a&gt;&#160;as well.&lt;/p&gt;</comment>
                            <comment id="16589253" author="githubbot" created="Wed, 22 Aug 2018 18:58:30 +0000"  >&lt;p&gt;apovzner opened a new pull request #5557: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7128&quot; title=&quot;Lagging high watermark can lead to committed data loss after ISR expansion&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7128&quot;&gt;&lt;del&gt;KAFKA-7128&lt;/del&gt;&lt;/a&gt;: Follower has to catch up to offset within current leader epoch to join ISR&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5557&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5557&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   If follower is not in ISR, it has to fetch up to start offset of the current leader epoch. Added unit test to verify this behavior.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16589473" author="apovzner" created="Wed, 22 Aug 2018 23:30:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lindong&quot; class=&quot;user-hover&quot; rel=&quot;lindong&quot;&gt;lindong&lt;/a&gt; I just saw your comment from Jul 25. Even if replica 3 updates its HW to 5 and state 3 has&#160; r3:&#160;&lt;span class=&quot;error&quot;&gt;&amp;#91;hw=5, leo=5&amp;#93;&lt;/span&gt;, we would still have the same issue of losing&#160;records at offsets&#160;5 to 10 once we get to state 4. Do you agree?&lt;/p&gt;</comment>
                            <comment id="16589533" author="lindong" created="Thu, 23 Aug 2018 00:58:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apovzner&quot; class=&quot;user-hover&quot; rel=&quot;apovzner&quot;&gt;apovzner&lt;/a&gt; Yes you are right. Previously I stopped at the first issue without thinking beyond that.&lt;/p&gt;</comment>
                            <comment id="16595357" author="githubbot" created="Tue, 28 Aug 2018 17:45:03 +0000"  >&lt;p&gt;hachikuji closed pull request #5557: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7128&quot; title=&quot;Lagging high watermark can lead to committed data loss after ISR expansion&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7128&quot;&gt;&lt;del&gt;KAFKA-7128&lt;/del&gt;&lt;/a&gt;: Follower has to catch up to offset within current leader epoch to join ISR&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5557&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5557&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
index a92340f2a4b..22c1508bb8c 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/cluster/Partition.scala&lt;br/&gt;
@@ -62,6 +62,9 @@ class Partition(val topic: String,&lt;br/&gt;
   private val leaderIsrUpdateLock = new ReentrantReadWriteLock&lt;br/&gt;
   private var zkVersion: Int = LeaderAndIsr.initialZKVersion&lt;br/&gt;
   @volatile private var leaderEpoch: Int = LeaderAndIsr.initialLeaderEpoch - 1&lt;br/&gt;
+  // start offset for &apos;leaderEpoch&apos; above (leader epoch of the current leader for this partition),&lt;br/&gt;
+  // defined when this broker is leader for partition&lt;br/&gt;
+  @volatile private var leaderEpochStartOffsetOpt: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
   @volatile var leaderReplicaIdOpt: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
   @volatile var inSyncReplicas: Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt; = Set.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;@@ -263,6 +266,7 @@ class Partition(val topic: String,&lt;br/&gt;
       allReplicasMap.clear()&lt;br/&gt;
       inSyncReplicas = Set.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;&lt;br/&gt;
       leaderReplicaIdOpt = None&lt;br/&gt;
+      leaderEpochStartOffsetOpt = None&lt;br/&gt;
       removePartitionMetrics()&lt;br/&gt;
       logManager.asyncDelete(topicPartition)&lt;br/&gt;
       logManager.asyncDelete(topicPartition, isFuture = true)&lt;br/&gt;
@@ -287,18 +291,19 @@ class Partition(val topic: String,&lt;br/&gt;
       // remove assigned replicas that have been removed by the controller&lt;br/&gt;
       (assignedReplicas.map(_.brokerId) &amp;#8211; newAssignedReplicas).foreach(removeReplica)&lt;br/&gt;
       inSyncReplicas = newInSyncReplicas&lt;br/&gt;
+      newAssignedReplicas.foreach(id =&amp;gt; getOrCreateReplica(id, partitionStateInfo.isNew))&lt;/p&gt;

&lt;p&gt;+      val leaderReplica = getReplica().get&lt;br/&gt;
+      val leaderEpochStartOffset = leaderReplica.logEndOffset.messageOffset&lt;br/&gt;
       info(s&quot;$topicPartition starts at Leader Epoch ${partitionStateInfo.basePartitionState.leaderEpoch} from &quot; +&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;s&quot;offset ${getReplica().get.logEndOffset.messageOffset}. Previous Leader Epoch was: $leaderEpoch&quot;)&lt;br/&gt;
+        s&quot;offset $leaderEpochStartOffset. Previous Leader Epoch was: $leaderEpoch&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       //We cache the leader epoch here, persisting it only if it&apos;s local (hence having a log dir)&lt;br/&gt;
       leaderEpoch = partitionStateInfo.basePartitionState.leaderEpoch&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;newAssignedReplicas.foreach(id =&amp;gt; getOrCreateReplica(id, partitionStateInfo.isNew))&lt;br/&gt;
-&lt;br/&gt;
+      leaderEpochStartOffsetOpt = Some(leaderEpochStartOffset)&lt;br/&gt;
       zkVersion = partitionStateInfo.basePartitionState.zkVersion&lt;br/&gt;
       val isNewLeader = leaderReplicaIdOpt.map(_ != localBrokerId).getOrElse(true)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val leaderReplica = getReplica().get&lt;br/&gt;
       val curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset&lt;br/&gt;
       val curTimeMs = time.milliseconds&lt;br/&gt;
       // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.&lt;br/&gt;
@@ -344,6 +349,7 @@ class Partition(val topic: String,&lt;br/&gt;
       (assignedReplicas.map(_.brokerId) &amp;#8211; newAssignedReplicas).foreach(removeReplica)&lt;br/&gt;
       inSyncReplicas = Set.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Replica&amp;#93;&lt;/span&gt;&lt;br/&gt;
       leaderEpoch = partitionStateInfo.basePartitionState.leaderEpoch&lt;br/&gt;
+      leaderEpochStartOffsetOpt = None&lt;br/&gt;
       zkVersion = partitionStateInfo.basePartitionState.zkVersion&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // If the leader is unchanged and the epochs are no more than one change apart, indicate that no follower changes are required&lt;br/&gt;
@@ -388,7 +394,11 @@ class Partition(val topic: String,&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Check and maybe expand the ISR of the partition.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* A replica will be added to ISR if its LEO &amp;gt;= current hw of the partition.&lt;br/&gt;
+   * A replica will be added to ISR if its LEO &amp;gt;= current hw of the partition and it is caught up to&lt;br/&gt;
+   * an offset within the current leader epoch. A replica must be caught up to the current leader&lt;br/&gt;
+   * epoch before it can join ISR, because otherwise, if there is committed data between current&lt;br/&gt;
+   * leader&apos;s HW and LEO, the replica may become the leader before it fetches the committed data&lt;br/&gt;
+   * and the data will be lost.&lt;br/&gt;
    *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Technically, a replica shouldn&apos;t be in ISR if it hasn&apos;t caught up for longer than replicaLagTimeMaxMs,&lt;/li&gt;
	&lt;li&gt;even if its log end offset is &amp;gt;= HW. However, to be consistent with how the follower determines&lt;br/&gt;
@@ -405,9 +415,11 @@ class Partition(val topic: String,&lt;br/&gt;
         case Some(leaderReplica) =&amp;gt;&lt;br/&gt;
           val replica = getReplica(replicaId).get&lt;br/&gt;
           val leaderHW = leaderReplica.highWatermark&lt;br/&gt;
+          val fetchOffset = logReadResult.info.fetchOffsetMetadata.messageOffset&lt;br/&gt;
           if (!inSyncReplicas.contains(replica) &amp;amp;&amp;amp;&lt;br/&gt;
              assignedReplicas.map(_.brokerId).contains(replicaId) &amp;amp;&amp;amp;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;replica.logEndOffset.offsetDiff(leaderHW) &amp;gt;= 0) {&lt;br/&gt;
+             replica.logEndOffset.offsetDiff(leaderHW) &amp;gt;= 0 &amp;amp;&amp;amp;&lt;br/&gt;
+             leaderEpochStartOffsetOpt.exists(fetchOffset &amp;gt;= _)) {&lt;br/&gt;
             val newInSyncReplicas = inSyncReplicas + replica&lt;br/&gt;
             info(s&quot;Expanding ISR from ${inSyncReplicas.map(_.brokerId).mkString(&quot;,&quot;)} &quot; +&lt;br/&gt;
               s&quot;to ${newInSyncReplicas.map(_.brokerId).mkString(&quot;,&quot;)}&quot;)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
index 96c1147c9bf..343693e82c7 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/cluster/PartitionTest.scala&lt;br/&gt;
@@ -27,6 +27,7 @@ import kafka.common.UnexpectedAppendOffsetException&lt;br/&gt;
 import kafka.log.
{LogConfig, LogManager, CleanerConfig}
&lt;p&gt; import kafka.server._&lt;br/&gt;
 import kafka.utils.&lt;/p&gt;
{MockTime, TestUtils, MockScheduler}
&lt;p&gt;+import kafka.zk.KafkaZkClient&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.errors.ReplicaNotAvailableException&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
@@ -36,6 +37,7 @@ import org.apache.kafka.common.requests.LeaderAndIsrRequest&lt;br/&gt;
 import org.junit.&lt;/p&gt;
{After, Before, Test}
&lt;p&gt; import org.junit.Assert._&lt;br/&gt;
 import org.scalatest.Assertions.assertThrows&lt;br/&gt;
+import org.easymock.EasyMock&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import scala.collection.JavaConverters._&lt;/p&gt;

&lt;p&gt;@@ -72,10 +74,16 @@ class PartitionTest &lt;/p&gt;
{
     val brokerProps = TestUtils.createBrokerConfig(brokerId, TestUtils.MockZkConnect)
     brokerProps.put(KafkaConfig.LogDirsProp, Seq(logDir1, logDir2).map(_.getAbsolutePath).mkString(&quot;,&quot;))
     val brokerConfig = KafkaConfig.fromProps(brokerProps)
+    val kafkaZkClient = EasyMock.createMock(classOf[KafkaZkClient])
     replicaManager = new ReplicaManager(
-      config = brokerConfig, metrics, time, zkClient = null, new MockScheduler(time),
+      config = brokerConfig, metrics, time, zkClient = kafkaZkClient, new MockScheduler(time),
       logManager, new AtomicBoolean(false), QuotaFactory.instantiate(brokerConfig, metrics, time, &quot;&quot;),
       brokerTopicStats, new MetadataCache(brokerId), new LogDirFailureChannel(brokerConfig.logDirs.size))
+
+    EasyMock.expect(kafkaZkClient.getEntityConfigs(EasyMock.anyString(), EasyMock.anyString())).andReturn(logProps).anyTimes()
+    EasyMock.expect(kafkaZkClient.conditionalUpdatePath(EasyMock.anyObject(), EasyMock.anyObject(), EasyMock.anyObject(), EasyMock.anyObject()))
+      .andReturn((true, 0)).anyTimes()
+    EasyMock.replay(kafkaZkClient)
   }

&lt;p&gt;   @After&lt;br/&gt;
@@ -230,6 +238,82 @@ class PartitionTest &lt;/p&gt;
{
     assertFalse(partition.makeFollower(0, partitionStateInfo, 2))
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testFollowerDoesNotJoinISRUntilCaughtUpToOffsetWithinCurrentLeaderEpoch(): Unit = {&lt;br/&gt;
+    val controllerEpoch = 3&lt;br/&gt;
+    val leader = brokerId&lt;br/&gt;
+    val follower1 = brokerId + 1&lt;br/&gt;
+    val follower2 = brokerId + 2&lt;br/&gt;
+    val controllerId = brokerId + 3&lt;br/&gt;
+    val replicas = List&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower1, follower2).asJava&lt;br/&gt;
+    val isr = List&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower2).asJava&lt;br/&gt;
+    val leaderEpoch = 8&lt;br/&gt;
+    val batch1 = TestUtils.records(records = List(new SimpleRecord(&quot;k1&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                                  new SimpleRecord(&quot;k2&quot;.getBytes, &quot;v2&quot;.getBytes)))&lt;br/&gt;
+    val batch2 = TestUtils.records(records = List(new SimpleRecord(&quot;k3&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                                  new SimpleRecord(&quot;k4&quot;.getBytes, &quot;v2&quot;.getBytes),&lt;br/&gt;
+                                                  new SimpleRecord(&quot;k5&quot;.getBytes, &quot;v3&quot;.getBytes)))&lt;br/&gt;
+    val batch3 = TestUtils.records(records = List(new SimpleRecord(&quot;k6&quot;.getBytes, &quot;v1&quot;.getBytes),&lt;br/&gt;
+                                                  new SimpleRecord(&quot;k7&quot;.getBytes, &quot;v2&quot;.getBytes)))&lt;br/&gt;
+&lt;br/&gt;
+    val partition = new Partition(topicPartition.topic, topicPartition.partition, time, replicaManager)&lt;br/&gt;
+    assertTrue(&quot;Expected first makeLeader() to return &apos;leader changed&apos;&quot;,&lt;br/&gt;
+               partition.makeLeader(controllerId, new LeaderAndIsrRequest.PartitionState(controllerEpoch, leader, leaderEpoch, isr, 1, replicas, true), 0))&lt;br/&gt;
+    assertEquals(&quot;Current leader epoch&quot;, leaderEpoch, partition.getLeaderEpoch)&lt;br/&gt;
+    assertEquals(&quot;ISR&quot;, Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower2), partition.inSyncReplicas.map(_.brokerId))&lt;br/&gt;
+&lt;br/&gt;
+    // after makeLeader(() call, partition should know about all the replicas&lt;br/&gt;
+    val leaderReplica = partition.getReplica(leader).get&lt;br/&gt;
+    val follower1Replica = partition.getReplica(follower1).get&lt;br/&gt;
+    val follower2Replica = partition.getReplica(follower2).get&lt;br/&gt;
+&lt;br/&gt;
+    // append records with initial leader epoch&lt;br/&gt;
+    val lastOffsetOfFirstBatch = partition.appendRecordsToLeader(batch1, isFromClient = true).lastOffset&lt;br/&gt;
+    partition.appendRecordsToLeader(batch2, isFromClient = true)&lt;br/&gt;
+    assertEquals(&quot;Expected leader&apos;s HW not move&quot;, leaderReplica.logStartOffset, leaderReplica.highWatermark.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // let the follower in ISR move leader&apos;s HW to move further but below LEO&lt;br/&gt;
+    def readResult(fetchInfo: FetchDataInfo, leaderReplica: Replica): LogReadResult = &lt;/p&gt;
{
+      LogReadResult(info = fetchInfo,
+                    highWatermark = leaderReplica.highWatermark.messageOffset,
+                    leaderLogStartOffset = leaderReplica.logStartOffset,
+                    leaderLogEndOffset = leaderReplica.logEndOffset.messageOffset,
+                    followerLogStartOffset = 0,
+                    fetchTimeMs = time.milliseconds,
+                    readSize = 10240,
+                    lastStableOffset = None)
+    }
&lt;p&gt;+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower2Replica, readResult(FetchDataInfo(LogOffsetMetadata(0), batch1), leaderReplica))&lt;br/&gt;
+    partition.updateReplicaLogReadResult(&lt;br/&gt;
+      follower2Replica, readResult(FetchDataInfo(LogOffsetMetadata(lastOffsetOfFirstBatch), batch2), leaderReplica))&lt;br/&gt;
+    assertEquals(&quot;Expected leader&apos;s HW&quot;, lastOffsetOfFirstBatch, leaderReplica.highWatermark.messageOffset)&lt;br/&gt;
+&lt;br/&gt;
+    // current leader becomes follower and then leader again (without any new records appended)&lt;br/&gt;
+    partition.makeFollower(&lt;br/&gt;
+      controllerId, new LeaderAndIsrRequest.PartitionState(controllerEpoch, follower2, leaderEpoch + 1, isr, 1, replicas, false), 1)&lt;br/&gt;
+    assertTrue(&quot;Expected makeLeader() to return &apos;leader changed&apos; after makeFollower()&quot;,&lt;br/&gt;
+               partition.makeLeader(controllerEpoch, new LeaderAndIsrRequest.PartitionState(&lt;br/&gt;
+                 controllerEpoch, leader, leaderEpoch + 2, isr, 1, replicas, false), 2))&lt;br/&gt;
+    val currentLeaderEpochStartOffset = leaderReplica.logEndOffset.messageOffset&lt;br/&gt;
+&lt;br/&gt;
+    // append records with the latest leader epoch&lt;br/&gt;
+    partition.appendRecordsToLeader(batch3, isFromClient = true)&lt;br/&gt;
+&lt;br/&gt;
+    // fetch from follower not in ISR from log start offset should not add this follower to ISR&lt;br/&gt;
+    partition.updateReplicaLogReadResult(follower1Replica,&lt;br/&gt;
+                                         readResult(FetchDataInfo(LogOffsetMetadata(0), batch1), leaderReplica))&lt;br/&gt;
+    partition.updateReplicaLogReadResult(follower1Replica,&lt;br/&gt;
+                                         readResult(FetchDataInfo(LogOffsetMetadata(lastOffsetOfFirstBatch), batch2), leaderReplica))&lt;br/&gt;
+    assertEquals(&quot;ISR&quot;, Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower2), partition.inSyncReplicas.map(_.brokerId))&lt;br/&gt;
+&lt;br/&gt;
+    // fetch from the follower not in ISR from start offset of the current leader epoch should&lt;br/&gt;
+    // add this follower to ISR&lt;br/&gt;
+    partition.updateReplicaLogReadResult(follower1Replica,&lt;br/&gt;
+                                         readResult(FetchDataInfo(LogOffsetMetadata(currentLeaderEpochStartOffset), batch3), leaderReplica))&lt;br/&gt;
+    assertEquals(&quot;ISR&quot;, Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(leader, follower1, follower2), partition.inSyncReplicas.map(_.brokerId))&lt;br/&gt;
+ }&lt;br/&gt;
+&lt;br/&gt;
   def createRecords(records: Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;SimpleRecord&amp;#93;&lt;/span&gt;, baseOffset: Long, partitionLeaderEpoch: Int = 0): MemoryRecords = {&lt;br/&gt;
     val buf = ByteBuffer.allocate(DefaultRecordBatch.sizeInBytes(records.asJava))&lt;br/&gt;
     val builder = MemoryRecords.builder(&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 11 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3vgpr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>