<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:04 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-48] Implement optional &quot;long poll&quot; support in fetch request</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-48</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Currently, the fetch request is non-blocking. If there is nothing on the broker for the consumer to retrieve, the broker simply returns an empty set to the consumer. This can be inefficient, if you want to ensure low-latency because you keep polling over and over. We should make a blocking version of the fetch request so that the fetch request is not returned until the broker has at least one message for the fetcher or some timeout passes.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12514685">KAFKA-48</key>
            <summary>Implement optional &quot;long poll&quot; support in fetch request</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jkreps">Jay Kreps</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Jul 2011 21:32:21 +0000</created>
                <updated>Mon, 30 Apr 2012 21:42:45 +0000</updated>
                            <resolved>Mon, 30 Apr 2012 21:42:45 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                                                                <comments>
                            <comment id="13084912" author="jkreps" created="Sun, 14 Aug 2011 22:01:07 +0000"  >&lt;p&gt;This latency issue is important for replication because the latency for the producer will now depend on the replication (fetching) on the followers. This means our current polling mechanism is not going to be good, because we have to either back off for a period of time to avoid busy waiting on the server. In addition with replication we need a similar ability to process requests asynchronously--we do not want to block any threads while waiting for acks from followers. This also breaks our simple request/response model.&lt;/p&gt;

&lt;p&gt;This is also important for the streaming use cases as they involve a large number of stacked topics, and hence the end-to-end latency is a multiple of the single-hop consumer latency.&lt;/p&gt;

&lt;p&gt;Fixing this is slightly tricky.&lt;/p&gt;

&lt;p&gt;The first thing I think we would need to do is move the execution of request handling out of the socket server threads. This would generally be a good thing to do anyway, as I/O currently blocks request handling for all sockets sharing a thread. This can add unnecessary latency.&lt;/p&gt;

&lt;p&gt;The design for this could be a request BlockingQueue that the SocketServer submits all requests to, and N response BlockingQueues, one for each socket thread. The request processing would happen in a separate threadpool that would feed off the request queue and send responses back to the response queue. For asynchronous requests, no response need be enqueued.&lt;/p&gt;

&lt;p&gt;The request handling would now be an ExecutorService with a fixed number of processes. Each process would poll the request queue, process any request it gets, and send back responses.&lt;/p&gt;

&lt;p&gt;Long poll requests from fetchers would either be handled immediately, or, if there is no available data, would add themselves to a list of watchers on the topic. When a request comes in on that topic, it would and responses for all watchers. The request would specify a max timeout after which the request would return empty, this could be implemented with a DelayQueue that was checked periodically for expired requests. A generalization of this would be to have the fetch request provide not only a max_wait_time but also a min_data_size, which would make the request block until the given number of bytes of data have accumulated. This would actually enable the opposite of simple long poll--instead of trying to minimize latency the fetcher would be able to ensure they got a good size chunk of data on each request to ensure good throughput and avoid lots of little requests each fetching only a few small messages.&lt;/p&gt;

&lt;p&gt;A similar mechanism would be possible for acknowledgements coming from followers. When a produce request occurs with a min_ack_count &amp;gt; 1, the request would go into a list of waiting requests for that topic/partition. When the ack request comes in from followers, we would check the waiting producers and add responses to the response queue for any newly unblocked request.&lt;/p&gt;

&lt;p&gt;I would like to do a round of refactoring on the SocketServer anyway, so let me know if anyone has comments on this before i go do anything too crazy. For example, I want someone else to validate the interaction with the replication design.&lt;/p&gt;</comment>
                            <comment id="13085155" author="junrao" created="Mon, 15 Aug 2011 16:43:34 +0000"  >&lt;p&gt;Jay, thanks for carefully thinking ahead. I agree that we will need to decouple the socket processor thread from the handler thread, to make it easy for producers to wait for acks from followers, and for the consumers to block until new data is produced. We probably need 1 request queue and 1 response queue per socket processor thread. That way, we can ensure that the response is always handled by the socket thread that has registered the needed socket key for the response.&lt;/p&gt;</comment>
                            <comment id="13139925" author="jkreps" created="Mon, 31 Oct 2011 05:17:14 +0000"  >&lt;p&gt;This is a draft patch that refactors the socket server to make requests and responses asynchronous. No need for a detailed review, it still needs a lot of cleanup, but I wanted to show people the idea in more detail.&lt;/p&gt;</comment>
                            <comment id="13140231" author="tgautier" created="Mon, 31 Oct 2011 15:38:50 +0000"  >&lt;p&gt;Hi - please keep in mind the use case where a consumer is interested in more than one topic.&lt;/p&gt;

&lt;p&gt;This feature if implemented only for one topic will not be useful for this use case - assuming it&apos;s infeasible to open multiple tcp connections.&lt;/p&gt;

&lt;p&gt;The first proposal I have is to allow the request to contain a list of topics.  However, upon consideration, this would require the response to also be adjusted such that it would contain the name of the topic, otherwise it would be next to impossible to ascertain which topic the response corresponds to - well it could be done such that the response is returned in the same way as the request was requested, and for topics with no messages, an empty response is given, but this seems pretty bad from a network bandwidth standpoint.&lt;/p&gt;

&lt;p&gt;So my final proposal would be to introduce an epoll like request/response.  The consumer would submit a request with a list of interested topics, and the response would be a topic and # of messages available on that topic when the topic(s) have messages.&lt;/p&gt;

&lt;p&gt;The advantage to this solution is that it would be entirely backward compatible, since you would simply introduce a new request/response pair and it would also allow the consumer to decide which topics to poll (or pull) from first, so that it could prioritize, if it wanted.  &lt;/p&gt;

&lt;p&gt;Finally, I like the idea of allowing the consumer to specify a min # of messages required to trigger the poll, you might want to copy the pattern you already setup for log flushing, e.g. max time and/or min # of messages.  So the request might look like:&lt;/p&gt;

&lt;p&gt;list-of :  topic-name:min msgs:max time&lt;/p&gt;

&lt;p&gt;and the response might be:&lt;/p&gt;

&lt;p&gt;list-of : topic-name:# msgs available&lt;/p&gt;

</comment>
                            <comment id="13140419" author="jkreps" created="Mon, 31 Oct 2011 18:31:52 +0000"  >&lt;p&gt;Yes, these are all good points. The work I have done so far just splits request processing into a separate thread pool and enables asynchronous handling. This is a fairly general thing we need for a few different use cases. Perhaps this should be broken into a separate JIRA.&lt;/p&gt;

&lt;p&gt;I have thought a little bit about how to do long poll, though. Logically what I want to do is make it possible to give a minimum byte size for the response and a maximum delay in ms; then have the server delay the response until we have at least min_bytes messages in the response OR we hit the maximum delay time. The goal is both to improve latency (by avoiding waiting in between poll requests), to reduce load on the server (by not polling), and to make it possible to improve throughput. If you set min_bytes = 0 or max_delay_ms = 0 you effectively get the current behavior. The throughput improvement comes if you set the min_bytes &amp;gt; 1; this would give a way to artificially increase the response size for requests to the topic (i.e. avoid fetching only a few messages at a time) while still giving hard latency guarantees. We have seen, the request size is one of the important things for network throughput.&lt;/p&gt;

&lt;p&gt;As you say, the only case to really consider is the multi-fetch case. The single topic fetch can just be seen as a special case of this. I think your first proposal is closer to what I had in mind. Having the response contain an empty message set for the topics that have no data has very little overhead since it is just positionally indexed, so it is like 4 bytes or something. I don&apos;t like doing a poll() style interface that just returns ready topics doesn&apos;t seem very useful to me because the only logical thing you can do is then initiate a fetch on those topics, right? So might as well just send back the data and have a single request type to worry about?&lt;/p&gt;

&lt;p&gt;One of the tricky questions for multifetch is what does the minimum byte size pertain to? A straight-forward implementation in the current system would be to add the min_bytes and timeout to the fetch request which would effectively bundle it up N times in the multi-fetch (currently multi-fetch is just N fetches glued together). This doesn&apos;t really make sense, though. Which of these minimum sizes would cause the single response to be sent? Would it be when all conditions were satisfied or when one was satisfied? I think the only thing that makes sense is to set these things at the request level. Ideally what I would like to do is remove the fetch request entirely because it is redundant and fix multi-fetch to have the following:&lt;br/&gt;
   &lt;span class=&quot;error&quot;&gt;&amp;#91;(topic1, partitions1), (topic2, partitions2),...&amp;#93;&lt;/span&gt;, max_total_size, max_wait_ms&lt;br/&gt;
This also fixes the weird thing in multifetch now where you have to specify the topic with each partition, so a request for 10 partitions on the same topic repeats the topic name 10 times. This is an invasive change, though, since it means request format changes.&lt;/p&gt;

&lt;p&gt;I am also not 100% sure how to implement the min_bytes parameter efficiently for multi-fetch. For the single fetch case it is pretty easy, the implementation would be to keep a sort of hybrid priority queue by timeout time (e.g. the unix timestamp at which we owe a response). When a fetch request came in we would try to service it immediately, and if we could meet its requirements we would immediately send a response. If we can&apos;t meet its min_bytes requirement then we would calculate the offset for that topic/partition at which the request would be unblocked (e.g. if the current offset is X and the min_bytes is M then the target size is X+M). We would insert new requests into this watchers list maintaining a sort by increasing target size. Each time a produce request is handled we would respond to all the watching requests whose target size is &amp;lt; then new offset, this would just require walking the list until we see a request with a target size greater than the current offset. All the newly unblocked requests would be added to the response queue. So this means the only work added to a produce request is the work of transferring newly unblocked requests to the response queue and at most we only need to examine one blocked request.&lt;/p&gt;

&lt;p&gt;The timeout could be implemented by keeping a priority queue of requests based on the unix timestamp of the latest allowable response (i.e. the ts the request came in, plus the max_wait_ms). We could add a background thread to remove items from this as their timeout occurs, and add them to the response queue with an empty response.&lt;/p&gt;

&lt;p&gt;For the multifetch case, things are harder to do efficiently. The timeouts can still work the same way. However the min_bytes is now over all the topics the request covers. The only way I can see to implement this is to keep a counter associated with each watcher, and have the watcher watch all the requested topics. But now on each produce request we need to increment ALL the watchers on the topic produced to.&lt;/p&gt;

&lt;p&gt;Dunno, maybe for practical numbers of blocked requests (a few hundred? a thousand?) this doesn&apos;t matter. Or maybe there is a more clever approach. Ideas welcome.&lt;/p&gt;</comment>
                            <comment id="13140450" author="tgautier" created="Mon, 31 Oct 2011 19:02:36 +0000"  >&lt;p&gt;I can see how it would be reasonable to do the first approach.  It does limit one use case I was considering, which is to allow the consumer to decide in which order to fetch the topics after the poll is triggered, however, this can be done at request time when the topics are requested.&lt;/p&gt;

&lt;p&gt;As you say, the response is 100% compatible, it&apos;s just the request that changes.  Therefore it would make sense I think to go ahead and make a new request type that doesn&apos;t yet exist and then the current fetch request remains the same on the wire and the behavior of it is just a degenerate case of this new use case with delay and bytes set to 0.&lt;/p&gt;

&lt;p&gt;I think you might consider how useful is it to worry about user specified time/bytes?  It will add a lot of complexity to your implementation, and frankly if I have just the ability to do a multi-fetch that will wait until something has arrived and send me whatever it has at the current moment that will be good enough.  A minimum implementation should also probably provide a simple timeout that will respond with nothing if the timeout expires.&lt;/p&gt;

&lt;p&gt;I think the simple implementation by itself a huge win and you might consider &amp;#8211; is that good enough?&lt;/p&gt;

&lt;p&gt;For me it is - I would prefer to get the simple thing in the short term and wait for the harder thing in the long-term.&lt;/p&gt;</comment>
                            <comment id="13140467" author="jkreps" created="Mon, 31 Oct 2011 19:18:15 +0000"  >&lt;p&gt;Hi Taylor,&lt;/p&gt;

&lt;p&gt;Could you give a little more detail on your use case for ordering the fetches? I think you have a use case I haven&apos;t thought of, but I don&apos;t know if I understand it. Is your motivation some kind of quality of service over the topics?&lt;/p&gt;

&lt;p&gt;As you say, this would definitely be a new request type for compatibility, and we would probably try to deprecate the old format over the next few releases as we can get clients updated.&lt;/p&gt;

&lt;p&gt;Your point about complexity is valid. I think for our usage since we use kafka very heavily the pain of grandfathering in new APIs is the hardest part, and the socket server refactoring is next, so I was thinking the difficulty of implementing a few internal data structures is not too bad. I suppose it depends on if I work out a concrete plan there or not. If the best we can do is iterate over the full set of watchers it may not be worth it.&lt;/p&gt;</comment>
                            <comment id="13140531" author="tgautier" created="Mon, 31 Oct 2011 20:36:48 +0000"  >&lt;p&gt;Actually, I don&apos;t have a valid use case for priority fetches, I was just thinking ahead.&lt;/p&gt;

&lt;p&gt;I agree that it&apos;s painful to have message format upgrades.  On the flip side of course we probably also agree it&apos;s bad to have parameters in the message header that don&apos;t correspond to real features.  &lt;/p&gt;

&lt;p&gt;Can you make a trade-off and reserve some bytes for these two int (or long) parameters and/or a few others but just call the space reserved?&lt;/p&gt;</comment>
                            <comment id="13148835" author="junrao" created="Fri, 11 Nov 2011 23:32:30 +0000"  >&lt;p&gt;Just had a chance to look at the patch. Agree in principle this would work. It&apos;s probably better to create a separate jira for moving the requesthandler out of socket server. The long poll jira will depend on that jira.&lt;/p&gt;</comment>
                            <comment id="13148971" author="jkreps" created="Sat, 12 Nov 2011 03:25:11 +0000"  >&lt;p&gt;Cool, moved it.&lt;/p&gt;</comment>
                            <comment id="13154069" author="tgautier" created="Mon, 21 Nov 2011 08:15:01 +0000"  >&lt;p&gt;I&apos;ve been staring at the code for a while - and I&apos;m not sure I understand why you need &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-202&quot; title=&quot;Make the request processing in kafka asynchonous&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-202&quot;&gt;&lt;del&gt;KAFKA-202&lt;/del&gt;&lt;/a&gt; to implement this feature.&lt;/p&gt;

&lt;p&gt;What I am thinking to do is:&lt;br/&gt;
1) Every thread has to open a local socket for read/write&lt;br/&gt;
2) Each thread puts the socket into the poll set for reading&lt;br/&gt;
3) If a read request fails to read any messages, when it comes back to the handler, the handler adds a callback method to the appropriate log and puts the read request into a special queue.  When that log gets messages for write, it calls the callback.  The callback writes a byte into the special thread socket.&lt;br/&gt;
4) The byte wakes up the thread, which sees that the special socket had a byte written to it, and so it goes and re-handles the read requests in the special queue as if they had just come in from the network. Thus if there are any messages available in the log for a given request, they are read just like normal and transferred out onto the channel.  If not, they&apos;re re-queued as per step 3.&lt;/p&gt;

&lt;p&gt;I think there is some pieces I haven&apos;t quite got right - in particular, I think there can only be one active response at a time.  Thus there will have to be some sort of response queue built up as each request generates a response, but I think that&apos;s simple - the handler just writes responses with non-zero messages into a response queue and the write logic of the socketserver is updated to drain this queue on write events (at the moment, it only deals with one response at a time, but now it may have many to send out queued up).&lt;/p&gt;

&lt;p&gt;Some other work that is probably going to be more difficult is that the binary protocol has to change to include the topic name or else there is no way to disambiguate the responses coming back.&lt;/p&gt;</comment>
                            <comment id="13178466" author="junrao" created="Mon, 2 Jan 2012 17:19:13 +0000"  >&lt;p&gt;Taylor,&lt;/p&gt;

&lt;p&gt;Sorry for the late response. I am not sure that I understand your proposal. &lt;/p&gt;

&lt;p&gt;a. Why do we need a local socket? It seems that the same thing can be achieved by just turning on the write_interesting bit in the socket key corresponding to a client request.&lt;/p&gt;

&lt;p&gt;b. It&apos;s not clear to me how you correlate a queued client request with the corresponding client socket.&lt;/p&gt;</comment>
                            <comment id="13200122" author="jkreps" created="Fri, 3 Feb 2012 22:49:51 +0000"  >&lt;p&gt;This is a very rough draft of long poll support. It appears to work. Here are some remaining issues:&lt;br/&gt;
1. I need the updated request objects to properly get the new fields (min_bytes, max_wait). Currently I am just hard-coding some made-up values.&lt;br/&gt;
2. This patch is very specific to long poll support for fetch requests, it will require more generalization to support our other async case, namely delaying produce requests until a certain number of slaves are caught up.&lt;br/&gt;
3. There are still some unit test problems.&lt;br/&gt;
4. Code is a little rough still.&lt;/p&gt;

&lt;p&gt;Take a look if interested, I will discuss with a few people and clean up a little more before asking for a real review.&lt;/p&gt;</comment>
                            <comment id="13200138" author="tgautier" created="Fri, 3 Feb 2012 23:08:28 +0000"  >&lt;p&gt;Jay - that&apos;s great to hear!! Would you mind summarizing the way that the long-poll works?  I know that several different implementations were suggested here on the thread and I wanted to know which one you ultimately decided to go with.&lt;/p&gt;</comment>
                            <comment id="13200155" author="jkreps" created="Fri, 3 Feb 2012 23:22:02 +0000"  >&lt;p&gt;Hey Taylor, here are the nitty gritty details:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;When a fetch request comes in we immediately check if we have sufficient data to satisfy it&lt;/li&gt;
	&lt;li&gt;if so we respond immediately&lt;/li&gt;
	&lt;li&gt;If not we add a &quot;watch&quot; on the topics that the fetch is for, and add it to a delay queue to expire it after the given timeout&lt;/li&gt;
	&lt;li&gt;There is a background thread that checks the delay queue for expired requests and responds to them with whatever data is available&lt;/li&gt;
	&lt;li&gt;When a produce request comes in we update the watchers for all the topics it produces to, and increment their byte count. Any requests that have been satisfied by this produce, are then executed and responses are sent.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So one of the earlier questions was how to support polling on a very large number of topics AND wants very low latency, I think as you described it would be possible to implement this by simply multiplexing the requests on the single socket and letting the server respond to these as possible.&lt;/p&gt;</comment>
                            <comment id="13200159" author="jkreps" created="Fri, 3 Feb 2012 23:29:50 +0000"  >&lt;p&gt;Two other issues with this patch, I forgot to mention:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;There is a race condition between checking the available bytes, and adding the watchers for the topics. I &lt;b&gt;think&lt;/b&gt; this is okay since the min_bytes is a minimum not a maximum, so in the rare case that a produce comes in before the watchers are added we will just wait slightly longer than we should have. I think this is probably better than properly synchronizing and locking out all produces on that partition.&lt;/li&gt;
	&lt;li&gt;The other issues is that the delay queue is only emptied right now when the delay expires. If the request is fulfilled before the delay expires, the request is marked completed, but it remains in the delay queue until it expires. This is a problem and needs to be fixed. The problem is that if the client sets a low min_bytes and a high max_wait these requests may accumulate. Currently we would have to do an O(N) walk of the waiting requests to fix this. I am going to try to come up with an improved set of data structures to fix this without requiring that.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13212329" author="junrao" created="Tue, 21 Feb 2012 03:48:33 +0000"  >&lt;p&gt;Overall, the patch looks good. Some comments:&lt;/p&gt;

&lt;p&gt;1. DelayedItem.compareTo: yourEnd should be delayed.createdMs + delayed.delayMs&lt;br/&gt;
2. Suppose that a client issues MultiFetch requests on a hot topic and a cold topic. What can happen is that the watcher list for the cold topic won&apos;t be cleaned up for a long time. One solution is to have a cleaner thread that periodically wakes up to remove satisfied items. The cleaner thread can be used to clean up the DelayQueue too.&lt;br/&gt;
3. MessageSetSend.empty is not used.&lt;/p&gt;</comment>
                            <comment id="13246554" author="jkreps" created="Wed, 4 Apr 2012 18:29:19 +0000"  >&lt;p&gt;This version of the patch updates the code to work with the new request objects and correctly respect the min_bytes and max_fetch_wait settings.&lt;/p&gt;

&lt;p&gt;Please review the new configs and make sure we are happy with the naming.&lt;/p&gt;</comment>
                            <comment id="13246570" author="jkreps" created="Wed, 4 Apr 2012 18:39:58 +0000"  >&lt;p&gt;Oops, missing about a bazillion files on that last patch.&lt;/p&gt;</comment>
                            <comment id="13247409" author="junrao" created="Thu, 5 Apr 2012 17:39:12 +0000"  >&lt;p&gt;Thanks for patch v3. Some comments:&lt;/p&gt;

&lt;p&gt;31. DelayedFetch is keyed off topic. It should be keyed off (topic, partition) since a consumer may be interested in only a subset of partitions within a topic.&lt;/p&gt;

&lt;p&gt;32. KafkaApis: The following 3 lines are duplicated in 2 places.&lt;br/&gt;
      val topicData = readMessageSets(delayed.fetch.offsetInfo)&lt;br/&gt;
      val response = new FetchResponse(FetchRequest.CurrentVersion, delayed.fetch.correlationId, topicData)&lt;br/&gt;
      requestChannel.sendResponse(new RequestChannel.Response(delayed.request, new FetchResponseSend(response, ErrorMapping.NoError), -1))&lt;br/&gt;
Should we put them in a private method and share the code?&lt;/p&gt;

&lt;p&gt;33. ExpiredRequestReaper.purgeExpired(): We need to decrement unsatisfied count here.&lt;/p&gt;

&lt;p&gt;34. FetchRequest: Can we have the default constants for correlationId, clientid, etc defined and shared btw the constructor and the request builder?&lt;/p&gt;

&lt;p&gt;35. MessageSetSend.empty is unused. Should we remove it?&lt;/p&gt;</comment>
                            <comment id="13250154" author="jkreps" created="Mon, 9 Apr 2012 20:35:37 +0000"  >&lt;p&gt;Jun, thanks for the feedback. This patch hopefully addresses your comments:&lt;br/&gt;
1. I removed the empty flag, as you suggested from MessageSetSend&lt;br/&gt;
2. I would like to leave the ugly duplicate code for now. Making a seperate method for this doesn&apos;t really make sense as it isn&apos;t really a stand alone piece of code. I think the root problem is that action you do when the request is satisfied can be done either synchronously (if possible), asynchronously when the criteria are satisfied, or asychronously when the request expires. I think the right way to do this is to refactor RequestPurgatory a bit and somehow always use the same callback for all three cases. I would like to address this as a seperate patch because this idea is not fully baked yet.&lt;br/&gt;
3. The default values are now shared between the builder and constructor.&lt;br/&gt;
4. I changed the key to be (topic, partition) for FetchRequestPurgatory. That was a major oversite.&lt;br/&gt;
5. The purgeExpired method is actually misnamed it is really purgeSatisfied, so it doesn&apos;t need to decrement the satisfied count. However there is a major bug in that count, it wasn&apos;t getting decremented by the processing thread. I added a new method to cover this.&lt;/p&gt;</comment>
                            <comment id="13250156" author="jkreps" created="Mon, 9 Apr 2012 20:37:27 +0000"  >&lt;p&gt;I attached a diff that just shows the changes between v3 and v4 for folks who already looked at v3.&lt;/p&gt;</comment>
                            <comment id="13250856" author="junrao" created="Tue, 10 Apr 2012 17:35:56 +0000"  >&lt;p&gt;Patch v4 looks good. Just one more comment.&lt;/p&gt;

&lt;p&gt;41. RequestPurgatory.update(): if(w == null), could we return a singleton empty array, instead of creating a new one every time?&lt;/p&gt;</comment>
                            <comment id="13250878" author="jkreps" created="Tue, 10 Apr 2012 17:54:29 +0000"  >&lt;p&gt;Good point Jun, now it is&lt;br/&gt;
    if(w == null)&lt;br/&gt;
      Seq.empty&lt;br/&gt;
    else&lt;br/&gt;
      w.collectSatisfiedRequests(request)&lt;/p&gt;

&lt;p&gt;I will wait for more feedback before making a new patch since this is a pretty trivial change.&lt;/p&gt;</comment>
                            <comment id="13250912" author="nehanarkhede" created="Tue, 10 Apr 2012 18:20:19 +0000"  >&lt;p&gt;This patch looks very good. Here are a few questions - &lt;/p&gt;

&lt;p&gt;1. I like the way the expired requests are handled by implementing the logic inside the FetchRequestPurgatory. However, can we not do the same for satisfied requests by providing a satisfy() abstract API in RequestPurgatory ? That gets rid of the handling of fetch requests inside handleProducerRequest() in KafkaApis, which is a little awkward to read. When we have the ProduceRequestPurgatory, the same satisfy() operation can send responses for produce requests once the fetch responses for the followers come in. &lt;/p&gt;

&lt;p&gt;2. I gave the RequestPurgatory data structure some thought. Not sure if this buys us anything over the current data structure. How about the following data structure for the RequestPurgatory - &lt;/p&gt;

&lt;p&gt;2.1. The watchers would be a priority heap (PriorityQueue), with the head being the DelayedItem with the least delay value (earliest expiration time). So for each (topic, partition), we have a PQ of watchers. &lt;/p&gt;

&lt;p&gt;2.2. The expiration data structure is another PQ of size n, where n is the number of keys in RequestPurgatory. This expiration PQ has the heads of each of the watcher lists above. &lt;/p&gt;

&lt;p&gt;2.3. The expiration thread will await on a condition variable with a timeout = delay of the head of the expiration PQ. The condition also gets signaled whenever the head of any of the n watcher list changes. &lt;/p&gt;

&lt;p&gt;2.4. When the expiration thread gets signaled, it removes its head element, expires it if its ready, ignores if its satisfied, and adds an element from the watch list it came from. It keeps doing this until its head has expiration time in the future. Then it goes back to awaiting on the condition variable. &lt;/p&gt;

&lt;p&gt;2.5. The item to be expired gets removed from its watch list as well as expiration PQ in O(1). &lt;/p&gt;

&lt;p&gt;2.6. The item that gets satisfied sets a flag and gets removed from its watcher list. If the satisfied item is the head of the watcher list, the expiration thread gets signaled to add new head to its PQ. &lt;/p&gt;

&lt;p&gt;2.7 Pros &lt;br/&gt;
2.7.1. The watcher list doesn&apos;t maintain expired items, so doesn&apos;t need state-keeping for liveCount and maybePurge() &lt;br/&gt;
2.7.2. During a watch operation, items only enter the expiration PQ if they are the head of the watcher list &lt;br/&gt;
2.7.3. The expiration thread does a more informed get operation, instead of polling the queue in a loop. &lt;/p&gt;

&lt;p&gt;2.8. Cons &lt;br/&gt;
2.8.1. watch operation is O(logn) where n is the number of DelayedItems for a key &lt;br/&gt;
2.8.2 The forcePurge() operation on the expiration data structure still needs to happen in O&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;Did I miss something here ? Thoughts ? &lt;/p&gt;

&lt;p&gt;3. On the other hand, this is a huge non-trivial patch and you must be pretty tired of rebasing and working through unit tests. We could just discuss the above changes, and maybe file another JIRA to track it, instead of delaying this patch further. But that is your call.&lt;/p&gt;</comment>
                            <comment id="13250982" author="jkreps" created="Tue, 10 Apr 2012 19:25:05 +0000"  >&lt;p&gt;Hey Neha, yes, my hope is to get the patch evaluated as is, and then take another pass at cleaning up the way we handle the satisfaction action as Jun and you requested and try out other approaches to the purgatory data structure asynchronously. That should take these cleanup/polishing items out of the critical path.&lt;/p&gt;

&lt;p&gt;I like your idea of the dual priority queues, but I need to work through it more to fully understand it.&lt;/p&gt;</comment>
                            <comment id="13256073" author="jjkoshy" created="Tue, 17 Apr 2012 23:50:59 +0000"  >&lt;p&gt;+1 on the patch. I have a few minor comments:&lt;/p&gt;

&lt;p&gt;KafkaRequestHandlers :&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;requestLogger unused.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;ConsumerConfig:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;maxFetchWait -&amp;gt; rename the prop to max.fetch.wait.ms and the val to&lt;br/&gt;
  maxFetchWaitMs&lt;/li&gt;
	&lt;li&gt;Can we get rid of fetcherBackoffMs? It says it is deprecated, but had a&lt;br/&gt;
  reference in FetcherRunnable which you removed.&lt;/li&gt;
	&lt;li&gt;May want to have an explicit constraint that consumerTimeoutMs &amp;lt;=&lt;br/&gt;
  maxFetchWait&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;RequestPurgatory:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Unused import.&lt;/li&gt;
	&lt;li&gt;The parameterized types and overall tricky nature of this component make&lt;br/&gt;
  it somewhat difficult to follow. I (think) I understood it only after&lt;br/&gt;
  looking at its usage in KafkaApis, so the comments and javadocs (including&lt;br/&gt;
  class&apos; summary on top) can only go so far.  Even so, I think the comments&lt;br/&gt;
  seem slightly out of sync with the code and can be improved a bit. E.g.,&lt;br/&gt;
  what is &quot;given size&quot; in the update method&apos;s comment? current keys in the&lt;br/&gt;
  comment for watch == the given request&apos;s keys. and so on.&lt;/li&gt;
	&lt;li&gt;Also, it may be easier to follow if we do some renaming, but it&apos;s a matter&lt;br/&gt;
  of taste and I may have misunderstood the code to begin with:&lt;/li&gt;
	&lt;li&gt;I find it confusing that there&apos;s a map called watchers which is a map&lt;br/&gt;
    from keys to Watcher objects, and the Watcher class itself has a&lt;br/&gt;
    linked-list of delayed requests called watchers. May be unwieldy, but&lt;br/&gt;
    how about renaming:&lt;/li&gt;
	&lt;li&gt;RequestPurgatory.watchers to watchedRequestsForKey&lt;/li&gt;
	&lt;li&gt;Watchers to WatchedRequests&lt;/li&gt;
	&lt;li&gt;Watchers.watchers to requests&lt;/li&gt;
	&lt;li&gt;Rename DelayedRequest.satisfied to satisfiedOrExpired (I find it weird&lt;br/&gt;
    that the reaper marks expired requests as satisfied.)&lt;/li&gt;
	&lt;li&gt;update -&amp;gt; maybeNotify?&lt;/li&gt;
	&lt;li&gt;In collectSatisfiedRequests, the comment on &quot;another thread has satisfied&lt;br/&gt;
  this request&quot;. That can only be the ExpiredRequestReaper thread right?&lt;/li&gt;
	&lt;li&gt;It is slightly odd that we have to call the reaper&apos;s satisfyRequest method&lt;br/&gt;
  from Watcher. Would it work to move the unsatisfied counter up to&lt;br/&gt;
  RequestPurgatory?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13256099" author="jkreps" created="Wed, 18 Apr 2012 00:23:33 +0000"  >&lt;p&gt;Joel, this is great feedback. I will address these issues in the commit since most are naming/documentation related.&lt;/p&gt;</comment>
                            <comment id="13265403" author="jkreps" created="Mon, 30 Apr 2012 21:42:45 +0000"  >&lt;p&gt;Included most of Joel&apos;s comments, and fixed a few lagging unit tests (in particular refactored AutoOffsetResetTest).&lt;/p&gt;

&lt;p&gt;Comments on the general structure of request purgatory I am going to put off until we have our second use case ready to implement--the producer acks. When we have that I am going to look at refactoring so that the &quot;satisfaction action&quot; is a function included with the DelayedRequest which is executed regardless of whether the request is satsified or times out. But I want to put this off until we can check it against the specifics of the second use case.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12514687">KAFKA-50</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12531209">KAFKA-202</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12521353" name="KAFKA-48-v2.patch" size="47180" author="jkreps" created="Wed, 4 Apr 2012 18:29:19 +0000"/>
                            <attachment id="12521358" name="KAFKA-48-v3.patch" size="62382" author="jkreps" created="Wed, 4 Apr 2012 18:39:58 +0000"/>
                            <attachment id="12522016" name="KAFKA-48-v4.patch" size="63525" author="jkreps" created="Mon, 9 Apr 2012 20:35:37 +0000"/>
                            <attachment id="12513197" name="KAFKA-48.patch" size="30997" author="jkreps" created="Fri, 3 Feb 2012 22:49:51 +0000"/>
                            <attachment id="12522017" name="kafka-48-v3-to-v4-changes.diff" size="7885" author="jkreps" created="Mon, 9 Apr 2012 20:37:27 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>63278</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 30 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l46f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>121327</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>