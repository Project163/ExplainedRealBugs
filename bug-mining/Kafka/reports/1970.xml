<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:12:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7076] Broker startup could be inefficient when using old message format</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7076</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;During broker startup, we call `Log#recoverSegment`&#160;when we find corrupted indexes, for segments beyond the last&#160;check-pointed recovery point, and for any &quot;.swap&quot; segments created by log cleaner. One of the things `Log#recoverSegments` does is to build up the producer state, starting from any previous snapshot file that is available. For logs using message formats older than V2, we could skip building up this producer state which would essentially speed up recovery.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13167036">KAFKA-7076</key>
            <summary>Broker startup could be inefficient when using old message format</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dhruvilshah">Dhruvil Shah</assignee>
                                    <reporter username="dhruvilshah">Dhruvil Shah</reporter>
                        <labels>
                    </labels>
                <created>Wed, 20 Jun 2018 00:24:10 +0000</created>
                <updated>Fri, 29 Jun 2018 04:58:47 +0000</updated>
                            <resolved>Wed, 27 Jun 2018 22:00:10 +0000</resolved>
                                    <version>0.11.0.0</version>
                    <version>0.11.0.1</version>
                    <version>0.11.0.2</version>
                    <version>1.0.0</version>
                    <version>1.0.1</version>
                    <version>1.1.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16517658" author="dhruvilshah" created="Wed, 20 Jun 2018 00:35:29 +0000"  >&lt;p&gt;PR is here: &lt;a href=&quot;https://github.com/apache/kafka/pull/5254&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5254&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16518381" author="rsivaram" created="Wed, 20 Jun 2018 17:29:22 +0000"  >&lt;p&gt;Removed fix version for 2.0.0 RC0, will update if merged for next RC.&lt;/p&gt;</comment>
                            <comment id="16525657" author="githubbot" created="Wed, 27 Jun 2018 21:48:11 +0000"  >&lt;p&gt;hachikuji closed pull request #5254: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7076&quot; title=&quot;Broker startup could be inefficient when using old message format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7076&quot;&gt;&lt;del&gt;KAFKA-7076&lt;/del&gt;&lt;/a&gt;: Prevent reading through log data for constructing producer state when using old message format&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5254&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5254&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
index 3036018dbda..e4be8fcc43d 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/Log.scala&lt;br/&gt;
@@ -246,6 +246,10 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     // The earliest leader epoch may not be flushed during a hard failure. Recover it here.&lt;br/&gt;
     _leaderEpochCache.clearAndFlushEarliest(logStartOffset)&lt;/p&gt;

&lt;p&gt;+    // Any segment loading or recovery code must not use producerStateManager, so that we can build the full state here&lt;br/&gt;
+    // from scratch.&lt;br/&gt;
+    if (!producerStateManager.isEmpty)&lt;br/&gt;
+      throw new IllegalStateException(&quot;Producer state must be empty during log initialization&quot;)&lt;br/&gt;
     loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)&lt;/p&gt;

&lt;p&gt;     info(s&quot;Completed load of log with ${segments.size} segments, log start offset $logStartOffset and &quot; +&lt;br/&gt;
@@ -417,25 +421,14 @@ class Log(@volatile var dir: File,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return The number of bytes truncated from the segment&lt;/li&gt;
	&lt;li&gt;@throws LogSegmentOffsetOverflowException if the segment contains messages that cause index offset overflow&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def recoverSegment(segment: LogSegment, leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = lock synchronized {&lt;/li&gt;
	&lt;li&gt;val stateManager = new ProducerStateManager(topicPartition, dir, maxProducerIdExpirationMs)&lt;/li&gt;
	&lt;li&gt;stateManager.truncateAndReload(logStartOffset, segment.baseOffset, time.milliseconds)&lt;/li&gt;
	&lt;li&gt;logSegments(stateManager.mapEndOffset, segment.baseOffset).foreach 
{ segment =&amp;gt;
-      val startOffset = math.max(segment.baseOffset, stateManager.mapEndOffset)
-      val fetchDataInfo = segment.read(startOffset, None, Int.MaxValue)
-      if (fetchDataInfo != null)
-        loadProducersFromLog(stateManager, fetchDataInfo.records)
-    }&lt;/li&gt;
	&lt;li&gt;stateManager.updateMapEndOffset(segment.baseOffset)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// take a snapshot for the first recovered segment to avoid reloading all the segments if we shutdown before we&lt;/li&gt;
	&lt;li&gt;// checkpoint the recovery point&lt;/li&gt;
	&lt;li&gt;stateManager.takeSnapshot()&lt;/li&gt;
	&lt;li&gt;val bytesTruncated = segment.recover(stateManager, leaderEpochCache)&lt;br/&gt;
-&lt;br/&gt;
+  private def recoverSegment(segment: LogSegment,&lt;br/&gt;
+                             leaderEpochCache: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;LeaderEpochCache&amp;#93;&lt;/span&gt; = None): Int = lock synchronized 
{
+    val producerStateManager = new ProducerStateManager(topicPartition, dir, maxProducerIdExpirationMs)
+    rebuildProducerState(segment.baseOffset, reloadFromCleanShutdown = false, producerStateManager)
+    val bytesTruncated = segment.recover(producerStateManager, leaderEpochCache)
     // once we have recovered the segment&apos;s data, take a snapshot to ensure that we won&apos;t
     // need to reload the same segment again while recovering another segment.
-    stateManager.takeSnapshot()
+    producerStateManager.takeSnapshot()
     bytesTruncated
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -565,10 +558,22 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     recoveryPoint&lt;br/&gt;
   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def loadProducerState(lastOffset: Long, reloadFromCleanShutdown: Boolean): Unit = lock synchronized {&lt;br/&gt;
+  // Rebuild producer state until lastOffset. This method may be called from the recovery code path, and thus must be&lt;br/&gt;
+  // free of all side-effects, i.e. it must not update any log-specific state.&lt;br/&gt;
+  private def rebuildProducerState(lastOffset: Long,&lt;br/&gt;
+                                   reloadFromCleanShutdown: Boolean,&lt;br/&gt;
+                                   producerStateManager: ProducerStateManager): Unit = lock synchronized {&lt;br/&gt;
     checkIfMemoryMappedBufferClosed()&lt;br/&gt;
     val messageFormatVersion = config.messageFormatVersion.recordVersion.value&lt;/li&gt;
	&lt;li&gt;info(s&quot;Loading producer state from offset $lastOffset with message format version $messageFormatVersion&quot;)&lt;br/&gt;
+    val segments = logSegments&lt;br/&gt;
+    val offsetsToSnapshot =&lt;br/&gt;
+      if (segments.nonEmpty) 
{
+        val nextLatestSegmentBaseOffset = lowerSegment(segments.last.baseOffset).map(_.baseOffset)
+        Seq(nextLatestSegmentBaseOffset, Some(segments.last.baseOffset), Some(lastOffset))
+      }
&lt;p&gt; else &lt;/p&gt;
{
+        Seq(Some(lastOffset))
+      }
&lt;p&gt;+    info(s&quot;Loading producer state till offset $lastOffset with message format version $messageFormatVersion&quot;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // We want to avoid unnecessary scanning of the log to build the producer state when the broker is being&lt;br/&gt;
     // upgraded. The basic idea is to use the absence of producer snapshot files to detect the upgrade case,&lt;br/&gt;
@@ -582,13 +587,11 @@ class Log(@volatile var dir: File,&lt;br/&gt;
     // offset (see below). The next time the log is reloaded, we will load producer state using this snapshot&lt;br/&gt;
     // (or later snapshots). Otherwise, if there is no snapshot file, then we have to rebuild producer state&lt;br/&gt;
     // from the first segment.&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (producerStateManager.latestSnapshotOffset.isEmpty &amp;amp;&amp;amp; (messageFormatVersion &amp;lt; RecordBatch.MAGIC_VALUE_V2 || reloadFromCleanShutdown)) {&lt;br/&gt;
+    if (messageFormatVersion &amp;lt; RecordBatch.MAGIC_VALUE_V2 ||&lt;br/&gt;
+        (producerStateManager.latestSnapshotOffset.isEmpty &amp;amp;&amp;amp; reloadFromCleanShutdown)) {&lt;br/&gt;
       // To avoid an expensive scan through all of the segments, we take empty snapshots from the start of the&lt;br/&gt;
       // last two segments and the last offset. This should avoid the full scan in the case that the log needs&lt;br/&gt;
       // truncation.&lt;/li&gt;
	&lt;li&gt;val nextLatestSegmentBaseOffset = lowerSegment(activeSegment.baseOffset).map(_.baseOffset)&lt;/li&gt;
	&lt;li&gt;val offsetsToSnapshot = Seq(nextLatestSegmentBaseOffset, Some(activeSegment.baseOffset), Some(lastOffset))&lt;br/&gt;
       offsetsToSnapshot.flatten.foreach 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: { offset =&amp;gt;         producerStateManager.updateMapEndOffset(offset)         producerStateManager.takeSnapshot()@@ -607,19 +610,25 @@ class Log(@volatile var dir}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;-&lt;br/&gt;
       producerStateManager.updateMapEndOffset(lastOffset)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;updateFirstUnstableOffset()&lt;br/&gt;
+      producerStateManager.takeSnapshot()&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  private def loadProducerState(lastOffset: Long, reloadFromCleanShutdown: Boolean): Unit = lock synchronized &lt;/p&gt;
{
+    rebuildProducerState(lastOffset, reloadFromCleanShutdown, producerStateManager)
+    updateFirstUnstableOffset()
+  }
&lt;p&gt;+&lt;br/&gt;
   private def loadProducersFromLog(producerStateManager: ProducerStateManager, records: Records): Unit = &lt;/p&gt;
{
     val loadedProducers = mutable.Map.empty[Long, ProducerAppendInfo]
     val completedTxns = ListBuffer.empty[CompletedTxn]
diff --git a/core/src/main/scala/kafka/log/OffsetIndex.scala b/core/src/main/scala/kafka/log/OffsetIndex.scala
index 2babd007a5a..6f246eedf1f 100755
--- a/core/src/main/scala/kafka/log/OffsetIndex.scala
+++ b/core/src/main/scala/kafka/log/OffsetIndex.scala
@@ -188,9 +188,9 @@ class OffsetIndex(_file: File, baseOffset: Long, maxIndexSize: Int = -1, writabl
   }

&lt;p&gt;   override def sanityCheck() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (_entries != 0 &amp;amp;&amp;amp; _lastOffset &amp;lt;= baseOffset)&lt;br/&gt;
+    if (_entries != 0 &amp;amp;&amp;amp; _lastOffset &amp;lt; baseOffset)&lt;br/&gt;
       throw new CorruptIndexException(s&quot;Corrupt index found, index file (${file.getAbsolutePath}) has non-zero size &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;but the last offset is ${_lastOffset} which is no greater than the base offset $baseOffset.&quot;)&lt;br/&gt;
+        s&quot;but the last offset is ${_lastOffset} which is less than the base offset $baseOffset.&quot;)&lt;br/&gt;
     if (length % entrySize != 0)&lt;br/&gt;
       throw new CorruptIndexException(s&quot;Index file ${file.getAbsolutePath} is corrupt, found $length bytes which is &quot; +&lt;br/&gt;
         s&quot;neither positive nor a multiple of $entrySize.&quot;)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogTest.scala b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
index 3b5b2fa7875..9a9bc613585 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogTest.scala&lt;br/&gt;
@@ -22,6 +22,7 @@ import java.nio.ByteBuffer&lt;br/&gt;
 import java.nio.file.
{Files, Paths}
&lt;p&gt; import java.util.Properties&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import kafka.api.&lt;/p&gt;
{ApiVersion, KAFKA_0_11_0_IV0}
&lt;p&gt; import kafka.common.&lt;/p&gt;
{OffsetsOutOfOrderException, UnexpectedAppendOffsetException}
&lt;p&gt; import kafka.log.Log.DeleteDirSuffix&lt;br/&gt;
 import kafka.server.epoch.&lt;/p&gt;
{EpochEntry, LeaderEpochCache, LeaderEpochFileCache}
&lt;p&gt;@@ -206,8 +207,6 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;     // Reload after unclean shutdown with recoveryPoint set to log end offset&lt;br/&gt;
     log = createLog(logDir, logConfig, recoveryPoint = logEndOffset)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Note that we don&apos;t maintain the guarantee of having a snapshot for the 2 most recent segments in this case&lt;/li&gt;
	&lt;li&gt;expectedSnapshotOffsets = Vector(log.logSegments.last.baseOffset, log.logEndOffset)&lt;br/&gt;
     assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)&lt;br/&gt;
     log.close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -215,15 +214,24 @@ class LogTest &lt;/p&gt;
{
 
     // Reload after unclean shutdown with recoveryPoint set to 0
     log = createLog(logDir, logConfig, recoveryPoint = 0L)
-    // Is this working as intended?
+    // We progressively create a snapshot for each segment after the recovery point
     expectedSnapshotOffsets = log.logSegments.map(_.baseOffset).tail.toVector :+ log.logEndOffset
     assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)
     log.close()
   }

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def testProducerSnapshotsRecoveryAfterUncleanShutdown(): Unit = {&lt;/li&gt;
	&lt;li&gt;val logConfig = LogTest.createLogConfig(segmentBytes = 64 * 10)&lt;br/&gt;
+  def testProducerSnapshotsRecoveryAfterUncleanShutdownV1(): Unit = 
{
+    testProducerSnapshotsRecoveryAfterUncleanShutdown(ApiVersion.minSupportedFor(RecordVersion.V1).version)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testProducerSnapshotsRecoveryAfterUncleanShutdownCurrentMessageFormat(): Unit = &lt;/p&gt;
{
+    testProducerSnapshotsRecoveryAfterUncleanShutdown(ApiVersion.latestVersion.version)
+  }
&lt;p&gt;+&lt;br/&gt;
+  private def testProducerSnapshotsRecoveryAfterUncleanShutdown(messageFormatVersion: String): Unit = {&lt;br/&gt;
+    val logConfig = LogTest.createLogConfig(segmentBytes = 64 * 10, messageFormatVersion = messageFormatVersion)&lt;br/&gt;
     var log = createLog(logDir, logConfig)&lt;br/&gt;
     assertEquals(None, log.oldestProducerSnapshotOffset)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -247,6 +255,16 @@ class LogTest {&lt;/p&gt;

&lt;p&gt;     val segmentsWithReads = ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt;()&lt;br/&gt;
     val recoveredSegments = ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;LogSegment&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    val expectedSegmentsWithReads = ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    val expectedSnapshotOffsets = ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+&lt;br/&gt;
+    if (logConfig.messageFormatVersion &amp;lt; KAFKA_0_11_0_IV0) &lt;/p&gt;
{
+      expectedSegmentsWithReads += activeSegmentOffset
+      expectedSnapshotOffsets ++= log.logSegments.map(_.baseOffset).toVector.takeRight(2) :+ log.logEndOffset
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      expectedSegmentsWithReads ++= segOffsetsBeforeRecovery ++ Seq(activeSegmentOffset)
+      expectedSnapshotOffsets ++= log.logSegments.map(_.baseOffset).toVector.takeRight(4) :+ log.logEndOffset
+    }

&lt;p&gt;     def createLogWithInterceptedReads(recoveryPoint: Long) = {&lt;br/&gt;
       val maxProducerIdExpirationMs = 60 * 60 * 1000&lt;br/&gt;
@@ -283,9 +301,8 @@ class LogTest {&lt;br/&gt;
     ProducerStateManager.deleteSnapshotsBefore(logDir, segmentOffsets(segmentOffsets.size - 2))&lt;br/&gt;
     log = createLogWithInterceptedReads(offsetForRecoveryPointSegment)&lt;br/&gt;
     // We will reload all segments because the recovery point is behind the producer snapshot files (pre &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5829&quot; title=&quot;Speedup broker startup after unclean shutdown by reducing unnecessary snapshot files deletion&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5829&quot;&gt;&lt;del&gt;KAFKA-5829&lt;/del&gt;&lt;/a&gt; behaviour)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(segOffsetsBeforeRecovery, segmentsWithReads.map(_.baseOffset) &amp;#8211; Seq(activeSegmentOffset))&lt;br/&gt;
+    assertEquals(expectedSegmentsWithReads, segmentsWithReads.map(_.baseOffset))&lt;br/&gt;
     assertEquals(segOffsetsAfterRecovery, recoveredSegments.map(_.baseOffset))&lt;/li&gt;
	&lt;li&gt;var expectedSnapshotOffsets = segmentOffsets.takeRight(4) :+ log.logEndOffset&lt;br/&gt;
     assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)&lt;br/&gt;
     log.close()&lt;br/&gt;
     segmentsWithReads.clear()&lt;br/&gt;
@@ -297,13 +314,12 @@ class LogTest 
{
     log = createLogWithInterceptedReads(recoveryPoint = recoveryPoint)
     assertEquals(Seq(activeSegmentOffset), segmentsWithReads.map(_.baseOffset))
     assertEquals(segOffsetsAfterRecovery, recoveredSegments.map(_.baseOffset))
-    expectedSnapshotOffsets = log.logSegments.map(_.baseOffset).toVector.takeRight(4) :+ log.logEndOffset
     assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)
 
     // Verify that we keep 2 snapshot files if we checkpoint the log end offset
     log.deleteSnapshotsAfterRecoveryPointCheckpoint()
-    expectedSnapshotOffsets = log.logSegments.map(_.baseOffset).toVector.takeRight(2) :+ log.logEndOffset
-    assertEquals(expectedSnapshotOffsets, listProducerSnapshotOffsets)
+    val expectedSnapshotsAfterDelete = log.logSegments.map(_.baseOffset).toVector.takeRight(2) :+ log.logEndOffset
+    assertEquals(expectedSnapshotsAfterDelete, listProducerSnapshotOffsets)
     log.close()
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -398,6 +414,9 @@ class LogTest {&lt;br/&gt;
     // We skip directly to updating the map end offset&lt;br/&gt;
     stateManager.updateMapEndOffset(1L)&lt;br/&gt;
     EasyMock.expectLastCall()&lt;br/&gt;
+    // Finally, we take a snapshot&lt;br/&gt;
+    stateManager.takeSnapshot()&lt;br/&gt;
+    EasyMock.expectLastCall().once()&lt;/p&gt;

&lt;p&gt;     EasyMock.replay(stateManager)&lt;/p&gt;

&lt;p&gt;@@ -410,14 +429,18 @@ class LogTest {&lt;br/&gt;
   def testSkipTruncateAndReloadIfOldMessageFormatAndNoCleanShutdown(): Unit = {&lt;br/&gt;
     val stateManager = EasyMock.mock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerStateManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(stateManager.latestSnapshotOffset).andReturn(None)&lt;br/&gt;
-&lt;br/&gt;
     stateManager.updateMapEndOffset(0L)&lt;br/&gt;
     EasyMock.expectLastCall().anyTimes()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     stateManager.takeSnapshot()&lt;br/&gt;
     EasyMock.expectLastCall().anyTimes()&lt;/p&gt;

&lt;p&gt;+    EasyMock.expect(stateManager.isEmpty).andReturn(true)&lt;br/&gt;
+    EasyMock.expectLastCall().once()&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.expect(stateManager.firstUnstableOffset).andReturn(None)&lt;br/&gt;
+    EasyMock.expectLastCall().once()&lt;br/&gt;
+&lt;br/&gt;
     EasyMock.replay(stateManager)&lt;/p&gt;

&lt;p&gt;     val logProps = new Properties()&lt;br/&gt;
@@ -443,14 +466,18 @@ class LogTest {&lt;br/&gt;
   def testSkipTruncateAndReloadIfOldMessageFormatAndCleanShutdown(): Unit = {&lt;br/&gt;
     val stateManager = EasyMock.mock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ProducerStateManager&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(stateManager.latestSnapshotOffset).andReturn(None)&lt;br/&gt;
-&lt;br/&gt;
     stateManager.updateMapEndOffset(0L)&lt;br/&gt;
     EasyMock.expectLastCall().anyTimes()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     stateManager.takeSnapshot()&lt;br/&gt;
     EasyMock.expectLastCall().anyTimes()&lt;/p&gt;

&lt;p&gt;+    EasyMock.expect(stateManager.isEmpty).andReturn(true)&lt;br/&gt;
+    EasyMock.expectLastCall().once()&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.expect(stateManager.firstUnstableOffset).andReturn(None)&lt;br/&gt;
+    EasyMock.expectLastCall().once()&lt;br/&gt;
+&lt;br/&gt;
     EasyMock.replay(stateManager)&lt;/p&gt;

&lt;p&gt;     val cleanShutdownFile = createCleanShutdownFile()&lt;br/&gt;
@@ -487,6 +514,12 @@ class LogTest {&lt;br/&gt;
     stateManager.takeSnapshot()&lt;br/&gt;
     EasyMock.expectLastCall().anyTimes()&lt;/p&gt;

&lt;p&gt;+    EasyMock.expect(stateManager.isEmpty).andReturn(true)&lt;br/&gt;
+    EasyMock.expectLastCall().once()&lt;br/&gt;
+&lt;br/&gt;
+    EasyMock.expect(stateManager.firstUnstableOffset).andReturn(None)&lt;br/&gt;
+    EasyMock.expectLastCall().once()&lt;br/&gt;
+&lt;br/&gt;
     EasyMock.replay(stateManager)&lt;/p&gt;

&lt;p&gt;     val cleanShutdownFile = createCleanShutdownFile()&lt;br/&gt;
@@ -644,8 +677,12 @@ class LogTest &lt;/p&gt;
{
     assertEquals(2, log.latestProducerStateEndOffset)
 
     log.truncateTo(1)
-    assertEquals(None, log.latestProducerSnapshotOffset)
+    assertEquals(Some(1), log.latestProducerSnapshotOffset)
     assertEquals(1, log.latestProducerStateEndOffset)
+
+    log.truncateTo(0)
+    assertEquals(None, log.latestProducerSnapshotOffset)
+    assertEquals(0, log.latestProducerStateEndOffset)
   }

&lt;p&gt;   @Test&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
index 1529597cf41..f47da995dd5 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala&lt;br/&gt;
@@ -178,6 +178,15 @@ class OffsetIndexTest extends JUnitSuite &lt;/p&gt;
{
     // mmap should be null after unmap causing lookup to throw a NPE
     intercept[NullPointerException](idx.lookup(1))
   }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testSanityLastOffsetEqualToBaseOffset(): Unit = &lt;/p&gt;
{
+    // Test index sanity for the case where the last offset appended to the index is equal to the base offset
+    val baseOffset = 20L
+    val idx = new OffsetIndex(nonExistentTempFile(), baseOffset = baseOffset, maxIndexSize = 10 * 8)
+    idx.append(baseOffset, 0)
+    idx.sanityCheck()
+  }

&lt;p&gt;   def assertWriteFails&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(message: String, idx: OffsetIndex, offset: Int, klass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
     try {&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 20 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3v12n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>