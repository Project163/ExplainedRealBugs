<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:54 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-42] Support rebalancing the partitions with replication</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-42</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;As new brokers are added, we need to support moving partition replicas from one set of brokers to another, online.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12514679">KAFKA-42</key>
            <summary>Support rebalancing the partitions with replication</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nehanarkhede">Neha Narkhede</assignee>
                                    <reporter username="junrao">Jun Rao</reporter>
                        <labels>
                            <label>features</label>
                    </labels>
                <created>Tue, 19 Jul 2011 21:32:19 +0000</created>
                <updated>Tue, 30 Dec 2014 00:36:48 +0000</updated>
                            <resolved>Wed, 10 Oct 2012 21:27:59 +0000</resolved>
                                                    <fixVersion>0.8.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                    <workratio workratioPercent="0"/>
                                    <progress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                    <timeoriginalestimate seconds="864000">240h</timeoriginalestimate>
                            <timeestimate seconds="864000">240h</timeestimate>
                                        <comments>
                            <comment id="13463015" author="nehanarkhede" created="Tue, 25 Sep 2012 17:34:54 +0000"  >&lt;p&gt;This is a pretty tricky feature. Since it involves multiple state changes before reassignment can be marked complete, there are many failure conditions to think about and handle recovery correctly&lt;/p&gt;

&lt;p&gt;1. Admin tool changes&lt;br/&gt;
1.1 Added a new admin command reassign-partition. Right now, it handles one partition, since I thought the failure/exit conditions and error messages are simpler to handle. But if people think we should add multiple partitions support in the same command invocation, that is fine too.&lt;br/&gt;
1.2 Added a new reassignPartition(topic, partition, RAR) API that registers a data change listener on /admin/reassign_partitions path and then creates the /admin/reassign_partitions={} path in zookeeper. It waits until that path is deleted from zookeeper. Once it is deleted, it checks if AR == RAR. If yes, it reports success otherwise failure.&lt;br/&gt;
1.3 Added a shutdown hook to handle command cancellation by the admin. In this case, it checks if reassignment was completed or not and logs the output accordingly.&lt;/p&gt;

&lt;p&gt;2. Controller changes&lt;/p&gt;

&lt;p&gt;Reassigning replicas for a partition goes through a few stages -&lt;br/&gt;
RAR = Reassigned replicas&lt;br/&gt;
AR = Original list of replicas for partition&lt;/p&gt;

&lt;p&gt;1. Register listener for ISR changes to detect when the RAR is a subset of the ISR&lt;br/&gt;
2. Start new replicas RAR - AR.&lt;br/&gt;
3. Wait until new replicas are in sync with the leader&lt;br/&gt;
4. If the leader is not in RAR, elect a new leader from RAR&lt;br/&gt;
5. Stop old replicas AR - RAR&lt;br/&gt;
6. Write new AR&lt;br/&gt;
7. Remove partition from the /admin/reassign_partitions path&lt;/p&gt;

&lt;p&gt;The above state changes steps are inside the onPartitionReassignment() callback in KafkaController.scala&lt;/p&gt;

&lt;p&gt;3. Partition Reassignment failure cases&lt;/p&gt;

&lt;p&gt;Broadly there are 2 types of failures we need to worry about -&lt;br/&gt;
1. Controller failover&lt;br/&gt;
2. Runtime error at the broker hosting the replica&lt;/p&gt;

&lt;p&gt;Let&apos;s go through the failure cases and recovery -&lt;br/&gt;
1. If the controller fails over between steps 1 and 2, the new controller on startup will read the non-empty admin path and just restart the partition reassignment process from scratch&lt;br/&gt;
2a. If the controller fails over between steps 2 and 3 above, the new controller will check if the new replicas are in sync with the leader or not. In either case, it will resume partition reassignment for the partitions listed in the admin path&lt;br/&gt;
2b. If, for some reason, the broker is not able to start the replicas, the isr listener for reassigned partitions will not trigger. So, the controller will not resume partition reassignment process for that partition. After some time, the admin command can be killed and it will report failure and delete the admin path so it can be retried.&lt;br/&gt;
3. If the controller fails over between steps 4 and 5, the new controller will realize that the new replicas are already in sync. If the new leader is part of the new replicas and is alive, it will not trigger leader re-election. Else it will re-elect the leader from amongst the live reassigned replicas.&lt;br/&gt;
4a. If the controller fails over between steps 5 and 6, the new controller resumes partition reassignment and repeats steps 4 onwards&lt;br/&gt;
4b. If, for some reason, the broker does not complete the leader state change, the partition after reassignment will be offline. This is a problem we have today even for leader election of newly created partitions. The controller doesn&apos;t wait for an acknowledgement from the broker for the make-leader state change. Nevertheless, the broker can fail even after sending a successful ack, so there isn&apos;t much value in waiting for an ack. However, I think the leader broker should expose an mbean to signify the availability of a partition. If people think this is a good idea, I can file a bug to fix this.&lt;br/&gt;
5. If the controller fails over between steps 6 and 7, it deletes the partition from the admin path marking the completion of this partition&apos;s reassignment. The partition reassignment zookeeper listener should record partition to be reassigned only if RAR not equal AR.&lt;/p&gt;

&lt;p&gt;4. PartitionReassignedListener&lt;/p&gt;

&lt;p&gt;Starts the partition reassignment process unless -&lt;br/&gt;
1. Partition previously existed&lt;br/&gt;
2. New replicas are the same as existing replicas&lt;br/&gt;
3. Any replica in the new set of replicas are dead&lt;/p&gt;

&lt;p&gt;If any of the above conditions are satisfies, it logs an error and removes the partition from list of reassigned partitions notifying the admin command about the failure/completion.&lt;/p&gt;

&lt;p&gt;5. PartitionLeaderSelector&lt;/p&gt;

&lt;p&gt;Added a self transition on the OnlinePartition state change. This is because, with cluster expansion and preferred replica leader election features, we need to move the leader for online partitions as well.&lt;/p&gt;

&lt;p&gt;Added partition leader selector module since we have 3 different ways of selecting the leader for a partition -&lt;br/&gt;
1. Offline leader selector - Pick an alive in sync replica as the leader. Otherwise, pick an alive assigned replica&lt;br/&gt;
2. Reassigned partition leader selector - Pick one of the alive in-sync reassigned replicas as the new leader&lt;br/&gt;
3. Preferred replica leader selector - Pick the preferred replica as the new leader&lt;br/&gt;
4. Testing&lt;/p&gt;

&lt;p&gt;6. Replica state machine changes&lt;br/&gt;
Added 2 new states to the replica state machine -&lt;br/&gt;
1. NewReplica        : The controller can create new replicas during partition reassignment. In this state, a&lt;br/&gt;
                       replica can only get become follower state change request.  Valid previous&lt;br/&gt;
                       state is NonExistentReplica&lt;br/&gt;
2. OnlineReplica     : Once a replica is started and part of the assigned replicas for its partition, it is in this&lt;br/&gt;
                       state. In this state, it can get either become leader or become follower state change requests.&lt;br/&gt;
                       Valid previous state are NewReplica, OnlineReplica or OfflineReplica&lt;br/&gt;
3. OfflineReplica    : If a replica dies, it moves to this state. This happens when the broker hosting the replica&lt;br/&gt;
                       is down. Valid previous state are NewReplica, OnlineReplica&lt;br/&gt;
4. NonExistentReplica: If a replica is deleted, it is moved to this state. Valid previous state is OfflineReplica&lt;/p&gt;

&lt;p&gt;7. Added 6 unit test cases to test -&lt;br/&gt;
1. Partition reassignment with leader of the partition in the new list of replicas&lt;br/&gt;
2. Partition reassignment with leader of the partition NOT in the new list of replicas&lt;br/&gt;
3. Partition reassignment with existing assigned replicas NOT overlapping with new list of replicas&lt;br/&gt;
4. Partition reassignment for a non existing partition. This is a negative test case&lt;br/&gt;
5. Partition reassignment for a partition that was completed upto step 6 by previous controller. This tests if after controller failover, it handles marking that partition&apos;s reassignment as completed.&lt;br/&gt;
6. Partition reassignment for a partition that was completed upto step 3 by previous controller. This tests if after controller failover, it handles leader re-election correctly and completes rest of the partition reassignment process.&lt;/p&gt;</comment>
                            <comment id="13463071" author="nehanarkhede" created="Tue, 25 Sep 2012 18:10:14 +0000"  >&lt;p&gt;Patch v1 contains a fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-525&quot; title=&quot;newly created partitions are not added to ReplicaStateMachine&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-525&quot;&gt;&lt;del&gt;KAFKA-525&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13464224" author="junrao" created="Wed, 26 Sep 2012 22:00:40 +0000"  >&lt;p&gt;Thanks for patch v1. Looks good overall. Some comments:&lt;/p&gt;

&lt;p&gt;1. ReassignPartitionsCommand:&lt;br/&gt;
1.1 If a partition doesn&apos;t exist, we should fail the operation immediately without updating ReassignPartitionsPath.&lt;br/&gt;
1.2 I think it would be useful to support migrating multiple topics and partitions. We can just take a JSON file that describes the new replicas as input.&lt;br/&gt;
1.3 If ReassignPartitionsPath already exists, we should quit immediately and not overwrite the path. This means that we will only allow 1 outstanding cluster rebalance at a given point of time, which is ok as long as the admin command allows multiple topic/partition being specified. &lt;/p&gt;

&lt;p&gt;2. Currently, we fail the partition reassignment operation if any broker in RAR is down, during initialization. However, brokers in RAR can go down after initialization. So, it would be good if we can handle RAR failures. Probably the only change needed is that when a broker is online, we need to start those replicas in RAR too.&lt;/p&gt;

&lt;p&gt;3. The logic is now get more complicated with the reassginment logic. Could we describe how it works in a comment? &lt;/p&gt;

&lt;p&gt;4. PartitionLeaderSelector.selectLeader(): describe what the return value is in the comment.&lt;/p&gt;

&lt;p&gt;5. ReassignedPartitionsIsrChangeListener.handleDataChange(): The following statement is weird. Only controller can change leaders and the controller always updates the leader cache every time a leader is changed. So, there shouldn&apos;t be a need for updating the leader cache on ZK listeners.&lt;br/&gt;
                controllerContext.allLeaders.put((topic, partition), leaderAndIsr.leader)&lt;/p&gt;

&lt;p&gt;6. KafkaController.onPartitionReassignment(): Could we put the logic that makes sure all replicas RAR are is ISR in  onPartitionReassignment()? Currently, that logic is duplicated in 2-3 places and that logic is always followed by a call to onPartitionReassignment(). If we do this, do we still need ReassignedPartitionsContext.areNewReplicasInIsr?&lt;/p&gt;

&lt;p&gt;7. ReplicaStateChangeMachine:&lt;br/&gt;
7.1 NonExistentReplica: The controller holds on to all replicas in this state. Is this necessary? Can we just remove them from the replicaState map.&lt;br/&gt;
7.2 In the following code, we don&apos;t really need to read from ZK and can use the cached data.&lt;br/&gt;
            case _ =&amp;gt;&lt;br/&gt;
              // check if the leader for this partition is alive or even exists&lt;br/&gt;
              // NOTE: technically, we could get the leader from the allLeaders cache, but we need to read zookeeper&lt;br/&gt;
              // for the ISR anyways&lt;br/&gt;
              val leaderAndIsrOpt = ZkUtils.getLeaderAndIsrForPartition(zkClient, topic, partition)&lt;/p&gt;

&lt;p&gt;8. AdminTest:&lt;br/&gt;
8.1 testPartitionReassignmentWithLeaderInNewReplicas: How do we make sure that replica 0 is always the leader?&lt;br/&gt;
8.2 testResumePartitionReassignmentThatWasCompleted: Towards the end, the comment says leader should be 2, but there is no broker 2 in the test.&lt;/p&gt;

&lt;p&gt;9. ControllerBrokerRequestBatch: Should we rename the two addRequestForBrokers to addLeaderAndIsrRequestForBrokers and addStopReplicaRequestForBrokers respectively?&lt;/p&gt;

&lt;p&gt;10. PartitionOfflineException,StateChangeFailedException: We can probably change the implementation to use RuntimeException(message, throwable) directly.&lt;/p&gt;

&lt;p&gt;11. LeaderElectionTest.testLeaderElectionAndEpoch(): Not sure if the change is correct. If there is no leadership change, leader epoch shouldn&apos;t change, right?&lt;/p&gt;</comment>
                            <comment id="13467533" author="nehanarkhede" created="Tue, 2 Oct 2012 06:43:54 +0000"  >&lt;p&gt;1. ReassignPartitionsCommand:&lt;br/&gt;
1.1  Makes sense, changed that.&lt;/p&gt;

&lt;p&gt;1.2 I think that makes sense. Thinking about this more, I guess it is not such a good idea to block the admin command until all the partitions are successfully reassigned. I changed the reassign partitions admin command to issue the partition reassignment request if that path doesn&apos;t already exist. This protects accidentally overwriting the zookeeper path. I also added a check reassignment status admin command that will report if the reassignment status of a partition is completed/failed/in progress. Also, another thing to be careful about a batch reassignment API is to avoid piling up important state change requests on the controller while it reassigns multiple partitions. Since reassignment of partitions is not an urgent state change, we should give up the controller lock after each partition is reassigned. That will ensure that other state changes can sneak in, if necessary&lt;/p&gt;

&lt;p&gt;1.3 Yes, forgot to include that in v1 patch.&lt;/p&gt;

&lt;p&gt;2. Initially, I thought the admin could just re-run the partition reassignment command, but I realize that it involes one manual step.&lt;/p&gt;

&lt;p&gt;3, 4 Sure&lt;/p&gt;

&lt;p&gt;5. Good point, removed it.&lt;/p&gt;

&lt;p&gt;6. This check is not done on every single invocation of onPartitionReassignment, it is done on controller failover and isr change listener. It is not required to be done when the partition reassigned callback triggers. But I think it is a good idea to move it to the callback, just in case we have not covered scenarios when the check should be done.&lt;/p&gt;

&lt;p&gt;7.1  While changing the state of a replica to NewReplica, we need to ensure that it was in the NonExistentReplica state. We can remove the replica from the replicaState map after it moves to the NonExistentReplica state explicitly, but there is a chance it will be added back to the map again. This can happen if we re-start the replica after stopping it. But, since this is infrequent, I made this change.&lt;/p&gt;

&lt;p&gt;7.2 We do not cache the isr which is required for the controller to be able to send a leader and isr request to the broker&lt;br/&gt;
Besides, this operation is only invoked when a new broker is started or controller fails over. Both of these operations are rare enough that we don&apos;t need to worry about optimizing this.&lt;/p&gt;


&lt;p&gt;8.1 There is a very good chance that it will be. This is because, we always pick the first alive assigned replica as the leader. Since replica 0 is the first assigned replica and is never shut down during the test, it will be the leader. Even if, due to some rare zookeeper session expiration issue, it is not the leader, the test will not fail.&lt;/p&gt;

&lt;p&gt;8.2 The comment is redundant there, so I removed it&lt;/p&gt;

&lt;p&gt;9, 10. Good point, fixed it&lt;/p&gt;

&lt;p&gt;11. It is correct since the controller increments the epoch for isr changes made by itself.&lt;/p&gt;</comment>
                            <comment id="13468947" author="junrao" created="Wed, 3 Oct 2012 22:53:20 +0000"  >&lt;p&gt;Thanks for patch v2. Some more comments:&lt;/p&gt;

&lt;p&gt;20. ReassignPartitionsCommand:&lt;br/&gt;
20.1 Could we add a description of the format of the jaon file in the command line option?&lt;br/&gt;
20.2 If partitionsToBeReassigned is an empty, should we just fail the command?&lt;br/&gt;
20.3 reassignPartitions(): Instead of check the existence of ReassignPartitionsPath and then write in ZK, it&apos;s better to use ZkUtils.createPersistentPath(), which throws an exception if node already exists. This will prevent the corner case that the path is created just after the existence check.&lt;br/&gt;
20.4 createReassignedPartitionsPathInZK: It seems that each call to this method just overwrites ReassignPartitionsPath with 1 partition&apos;s assignment. So we will lose the assignments of all partitions except the last one?&lt;/p&gt;

&lt;p&gt;21. CheckReassignmentStatus: It&apos;s better to move checkIfReassignmentSucceeded and checkIfPartitionReassignmentSucceeded from ZkUtils to CheckReassignmentStatus since they are only used here and ZkUtils is getting big.&lt;/p&gt;

&lt;p&gt;22. KafkaController.onBrokerStartup() : It seems that we can get partitionsBeingReassigned from the cache in controllerContext, instead of from ZK.&lt;/p&gt;

&lt;p&gt;23. PartitionStateMachine.initializeLeaderAndIsrForPartiiton(): When writing the initial leaderAndIsr path for a new partition, there is no need to read the path first to make sure that it doesn&apos;t exists. createPersistentPath will throw an exception if the path exists.&lt;/p&gt;</comment>
                            <comment id="13471284" author="nehanarkhede" created="Sun, 7 Oct 2012 18:11:09 +0000"  >&lt;p&gt;20. ReassignPartitionsCommand:&lt;br/&gt;
20.1, 20.2 Sure, that is a good idea&lt;br/&gt;
20.3 For this corner case to happen, another instance of the admin command would have to run at the right time. If that happens, both the admin commands might see that the path doesn&apos;t exist and try to create it. At this point, one of the admin commands will get an error and it will exit.&lt;br/&gt;
20.4 Good catch, that is a bug. Initially, I wrote the entire map of all partitions using that API. But later, for per-partition sanity checks, changed it to get invoked for every partition and that probably introduced the bug.&lt;/p&gt;

&lt;p&gt;21. They are used in AdminTest as well, but this makes sense.&lt;/p&gt;

&lt;p&gt;22, 23. Included these optimizations.&lt;/p&gt;</comment>
                            <comment id="13471733" author="junrao" created="Mon, 8 Oct 2012 18:29:11 +0000"  >&lt;p&gt;Thanks for patch v3. Looks good to me overall. Just one comment:&lt;/p&gt;

&lt;p&gt;20.3 The problem is that reassignPartitions() uses updatePartitionReassignmentData, which in turn uses updatePersistentPath. updatePersistentPath won&apos;t throw an exception if a node already exists. So, what could happen is that 2 admin commands are issued at the same time. Both pass the existence test of the ZK path. One command writes its data in the reassignment path first. The other one then overwrites it. Now, both commands appear to have completed successfully. Using ZkUtils.createPersistentPath() instead of updatePersistentPath() would prevent this since the former throws an exception if the path already exists.&lt;/p&gt;
</comment>
                            <comment id="13471922" author="nehanarkhede" created="Mon, 8 Oct 2012 22:30:02 +0000"  >&lt;p&gt;20.3 Good point, I see what you are saying. Fixed it&lt;/p&gt;</comment>
                            <comment id="13472121" author="junrao" created="Tue, 9 Oct 2012 04:33:26 +0000"  >&lt;p&gt;Thanks for patch v4. AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved seems to fail.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-10-08 21:30:06,005&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionsReassignedListener on 0&amp;#93;&lt;/span&gt;: Error completing reassignment of partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; (kafka.controller.PartitionsReassignedListener:102)&lt;br/&gt;
kafka.common.KafkaException: Only  replicas out of the new set of replicas 2,3 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; to be reassigned are alive. Failing partition reassignment&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.liftedTree1$1(KafkaController.scala:512)&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:495)&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:489)&lt;br/&gt;
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener.handleDataChange(KafkaController.scala:489)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:547)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-10-08 21:30:06,280&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionsReassignedListener on 0&amp;#93;&lt;/span&gt;: Error completing reassignment of partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; (kafka.controller.PartitionsReassignedListener:102)&lt;br/&gt;
org.I0Itec.zkclient.exception.ZkInterruptedException: java.lang.InterruptedException&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:687)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:766)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:761)&lt;br/&gt;
	at kafka.utils.ZkUtils$.readDataMaybeNull(ZkUtils.scala:363)&lt;br/&gt;
	at kafka.utils.ZkUtils$.getLeaderAndIsrForPartition(ZkUtils.scala:78)&lt;br/&gt;
	at kafka.controller.KafkaController.areReplicasInIsr(KafkaController.scala:323)&lt;br/&gt;
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:183)&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.liftedTree1$1(KafkaController.scala:509)&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:495)&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:489)&lt;br/&gt;
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)&lt;br/&gt;
	at kafka.controller.PartitionsReassignedListener.handleDataChange(KafkaController.scala:489)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:547)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)&lt;br/&gt;
Caused by: java.lang.InterruptedException&lt;br/&gt;
	at java.lang.Object.wait(Native Method)&lt;br/&gt;
	at java.lang.Object.wait(Object.java:485)&lt;br/&gt;
	at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1344)&lt;br/&gt;
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:925)&lt;br/&gt;
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:956)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkConnection.readData(ZkConnection.java:103)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:770)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:766)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)&lt;br/&gt;
	... 13 more&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-10-08 21:30:06,377&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica state machine on Controller 3&amp;#93;&lt;/span&gt;: Error while changing state of replica 2 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; to OnlineReplica (kafka.controller.ReplicaStateMachine:102)&lt;br/&gt;
java.lang.AssertionError: assertion failed: Replica 2 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; should be in the NewReplica,OnlineReplica,OfflineReplica states before moving to OnlineReplica state. Instead it is in NonExistentReplica state&lt;br/&gt;
	at scala.Predef$.assert(Predef.scala:91)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:130)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:45)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply$mcVI$sp(KafkaController.scala:187)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)&lt;br/&gt;
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:45)&lt;br/&gt;
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:186)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)&lt;br/&gt;
	at kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)&lt;br/&gt;
	at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)&lt;br/&gt;
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)&lt;br/&gt;
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)&lt;br/&gt;
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-10-08 21:30:06,379&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica state machine on Controller 3&amp;#93;&lt;/span&gt;: Error while changing state of replica 3 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; to OnlineReplica (kafka.controller.ReplicaStateMachine:102)&lt;br/&gt;
java.lang.AssertionError: assertion failed: Replica 3 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; should be in the NewReplica,OnlineReplica,OfflineReplica states before moving to OnlineReplica state. Instead it is in NonExistentReplica state&lt;br/&gt;
	at scala.Predef$.assert(Predef.scala:91)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:130)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:45)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply$mcVI$sp(KafkaController.scala:187)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)&lt;br/&gt;
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:45)&lt;br/&gt;
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:186)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)&lt;br/&gt;
	at kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)&lt;br/&gt;
	at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)&lt;br/&gt;
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)&lt;br/&gt;
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)&lt;br/&gt;
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2012-10-08 21:30:06,381&amp;#93;&lt;/span&gt; ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Replica state machine on Controller 3&amp;#93;&lt;/span&gt;: Error while changing state of replica 0 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; to OfflineReplica (kafka.controller.ReplicaStateMachine:102)&lt;br/&gt;
java.lang.AssertionError: assertion failed: Replica 0 for partition &lt;span class=&quot;error&quot;&gt;&amp;#91;test, 0&amp;#93;&lt;/span&gt; should be in the NewReplica,OnlineReplica states before moving to OfflineReplica state. Instead it is in NonExistentReplica state&lt;br/&gt;
	at scala.Predef$.assert(Predef.scala:91)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:156)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:45)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply$mcVI$sp(KafkaController.scala:363)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply(KafkaController.scala:362)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply(KafkaController.scala:362)&lt;br/&gt;
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:101)&lt;br/&gt;
	at kafka.controller.KafkaController.stopOldReplicasOfReassignedPartition(KafkaController.scala:362)&lt;br/&gt;
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:193)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)&lt;br/&gt;
	at kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)&lt;br/&gt;
	at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)&lt;br/&gt;
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)&lt;br/&gt;
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)&lt;br/&gt;
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)&lt;/p&gt;

&lt;p&gt;Partition should have been reassigned to 0, 1 expected:&amp;lt;List(2, 3)&amp;gt; but was:&amp;lt;List(0, 1)&amp;gt;&lt;br/&gt;
junit.framework.AssertionFailedError: Partition should have been reassigned to 0, 1 expected:&amp;lt;List(2, 3)&amp;gt; but was:&amp;lt;List(0, 1)&amp;gt;&lt;br/&gt;
	at junit.framework.Assert.fail(Assert.java:47)&lt;br/&gt;
	at junit.framework.Assert.failNotEquals(Assert.java:277)&lt;br/&gt;
	at junit.framework.Assert.assertEquals(Assert.java:64)&lt;br/&gt;
	at kafka.admin.AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved(AdminTest.scala:361)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
	at junit.framework.TestCase.runTest(TestCase.java:164)&lt;/p&gt;</comment>
                            <comment id="13472123" author="junrao" created="Tue, 9 Oct 2012 04:34:57 +0000"  >&lt;p&gt;Also, could you make the two new scripts in bin/ executable?&lt;/p&gt;</comment>
                            <comment id="13472831" author="nehanarkhede" created="Tue, 9 Oct 2012 22:54:50 +0000"  >&lt;p&gt;The intermittent test failure is due to the partition reassignment failing to complete due to ZkInterruptedException. This is probably due to the test trying to introduce a controller failover. But controller failover takes some time to restart the partition reassignment and the test failed due to a lower value of the timeout. I fixed the comment in the test assertion and increased the wait time. Now all test seem to pass after couple of iterations&lt;/p&gt;</comment>
                            <comment id="13472833" author="nehanarkhede" created="Tue, 9 Oct 2012 22:55:09 +0000"  >&lt;p&gt;Also, I will make the scripts executable before checking in&lt;/p&gt;</comment>
                            <comment id="13473354" author="nehanarkhede" created="Wed, 10 Oct 2012 16:45:39 +0000"  >&lt;p&gt;This is blocking work on &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-43&quot; title=&quot;Rebalance to preferred broke with intra-cluster replication support&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-43&quot;&gt;&lt;del&gt;KAFKA-43&lt;/del&gt;&lt;/a&gt;, if no one has objections on v5, I will commit this today&lt;/p&gt;</comment>
                            <comment id="13473359" author="junrao" created="Wed, 10 Oct 2012 16:52:36 +0000"  >&lt;p&gt;+1 on the patch. Still see AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved fails occasionally due to the same error. However, it seems to be less frequent now. &lt;/p&gt;

&lt;p&gt;Unfortunately, you will need to rebase. The patch can be committed after the rebase.&lt;/p&gt;</comment>
                            <comment id="13473379" author="nehanarkhede" created="Wed, 10 Oct 2012 17:23:51 +0000"  >&lt;p&gt;I fear I might end up rebasing incorrectly and volunteer for applying &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-510&quot; title=&quot;broker needs to know the replication factor per partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-510&quot;&gt;&lt;del&gt;KAFKA-510&lt;/del&gt;&lt;/a&gt; top of this rather than other way around. This is going to require me to revert &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-510&quot; title=&quot;broker needs to know the replication factor per partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-510&quot;&gt;&lt;del&gt;KAFKA-510&lt;/del&gt;&lt;/a&gt;, apply &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-42&quot; title=&quot;Support rebalancing the partitions with replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-42&quot;&gt;&lt;del&gt;KAFKA-42&lt;/del&gt;&lt;/a&gt; and then re-apply &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-510&quot; title=&quot;broker needs to know the replication factor per partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-510&quot;&gt;&lt;del&gt;KAFKA-510&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="13473575" author="nehanarkhede" created="Wed, 10 Oct 2012 21:27:59 +0000"  >&lt;p&gt;Committed this&lt;/p&gt;</comment>
                            <comment id="14064470" author="bongster" created="Thu, 17 Jul 2014 01:35:34 +0000"  >&lt;p&gt;Created reviewboard  against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14073298" author="edgefox" created="Thu, 24 Jul 2014 15:38:01 +0000"  >&lt;p&gt;Updated reviewboard  against branch apache/0.8.1&lt;/p&gt;</comment>
                            <comment id="14073383" author="junrao" created="Thu, 24 Jul 2014 17:07:57 +0000"  >&lt;p&gt;This jira is already closed. Is the patch for this jira?&lt;/p&gt;</comment>
                            <comment id="14073394" author="guozhang" created="Thu, 24 Jul 2014 17:17:33 +0000"  >&lt;p&gt;I think this is just due to the review-tool, which use the magic number &quot;42&quot; when no jira number is specified.&lt;/p&gt;</comment>
                            <comment id="14260596" author="parth.brahmbhatt" created="Tue, 30 Dec 2014 00:36:48 +0000"  >&lt;p&gt;Updated reviewboard &lt;a href=&quot;https://reviews.apache.org/r/29468/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/29468/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12514687">KAFKA-50</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12606436">KAFKA-498</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12606438">KAFKA-499</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310060">
                    <name>Container</name>
                                            <outwardlinks description="contains">
                                        <issuelink>
            <issuekey id="12608896">KAFKA-525</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12656198" name="KAFKA-42.patch" size="0" author="bongster" created="Thu, 17 Jul 2014 01:35:34 +0000"/>
                            <attachment id="12657620" name="KAFKA-42_2014-07-24_15:36:04.patch" size="1298426" author="edgefox" created="Thu, 24 Jul 2014 15:36:39 +0000"/>
                            <attachment id="12657621" name="KAFKA-42_2014-07-24_15:37:26.patch" size="1298426" author="edgefox" created="Thu, 24 Jul 2014 15:37:54 +0000"/>
                            <attachment id="12689410" name="KAFKA-42_2014-12-29_16:36:41.patch" size="1904" author="parth.brahmbhatt" created="Tue, 30 Dec 2014 00:36:47 +0000"/>
                            <attachment id="12546556" name="kafka-42-v1.patch" size="105640" author="nehanarkhede" created="Tue, 25 Sep 2012 17:34:54 +0000"/>
                            <attachment id="12547350" name="kafka-42-v2.patch" size="121725" author="nehanarkhede" created="Tue, 2 Oct 2012 06:43:54 +0000"/>
                            <attachment id="12548167" name="kafka-42-v3.patch" size="118261" author="nehanarkhede" created="Sun, 7 Oct 2012 18:11:09 +0000"/>
                            <attachment id="12548352" name="kafka-42-v4.patch" size="118379" author="nehanarkhede" created="Tue, 9 Oct 2012 02:29:32 +0000"/>
                            <attachment id="12548489" name="kafka-42-v5.patch" size="156067" author="nehanarkhede" created="Tue, 9 Oct 2012 22:54:50 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>67074</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 47 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i014e7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4481</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>