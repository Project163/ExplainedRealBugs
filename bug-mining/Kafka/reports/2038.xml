<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:14:15 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7333] Protocol changes for KIP-320</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7333</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Implement protocol changes for &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-320%3A+Allow+fetchers+to+detect+and+handle+log+truncation&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-320%3A+Allow+fetchers+to+detect+and+handle+log+truncation&lt;/a&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13180803">KAFKA-7333</key>
            <summary>Protocol changes for KIP-320</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hachikuji">Jason Gustafson</assignee>
                                    <reporter username="hachikuji">Jason Gustafson</reporter>
                        <labels>
                    </labels>
                <created>Thu, 23 Aug 2018 18:49:35 +0000</created>
                <updated>Sun, 9 Sep 2018 07:19:35 +0000</updated>
                            <resolved>Sun, 9 Sep 2018 07:18:25 +0000</resolved>
                                                    <fixVersion>2.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16590763" author="githubbot" created="Thu, 23 Aug 2018 20:13:12 +0000"  >&lt;p&gt;hachikuji opened a new pull request #5564: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7333&quot; title=&quot;Protocol changes for KIP-320&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7333&quot;&gt;&lt;del&gt;KAFKA-7333&lt;/del&gt;&lt;/a&gt;; Protocol changes for KIP-320&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5564&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5564&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   This patch contains the protocol updates needed for KIP-320 as well as some of the basic consumer APIs (e.g. `OffsetAndMetadata` and `ConsumerRecord`). The inter-broker format version has not been changed and the brokers will continue to use the current version.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16608366" author="githubbot" created="Sun, 9 Sep 2018 07:19:35 +0000"  >&lt;p&gt;lindong28 closed pull request #5564: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7333&quot; title=&quot;Protocol changes for KIP-320&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7333&quot;&gt;&lt;del&gt;KAFKA-7333&lt;/del&gt;&lt;/a&gt;; Protocol changes for KIP-320&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5564&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5564&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java&lt;br/&gt;
index 904cd0601e5..5759d632837 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java&lt;br/&gt;
@@ -134,6 +134,7 @@&lt;br/&gt;
 import java.util.LinkedList;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.concurrent.TimeUnit;&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicInteger;&lt;br/&gt;
@@ -2643,12 +2644,14 @@ void handleResponse(AbstractResponse abstractResponse) {&lt;br/&gt;
                             for (Map.Entry&amp;lt;TopicPartition, OffsetFetchResponse.PartitionData&amp;gt; entry :&lt;br/&gt;
                                     response.responseData().entrySet()) {&lt;br/&gt;
                                 final TopicPartition topicPartition = entry.getKey();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Errors error = entry.getValue().error;&lt;br/&gt;
+                                OffsetFetchResponse.PartitionData partitionData = entry.getValue();&lt;br/&gt;
+                                final Errors error = partitionData.error;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                                 if (error == Errors.NONE) &lt;/p&gt;
{
-                                    final Long offset = entry.getValue().offset;
-                                    final String metadata = entry.getValue().metadata;
-                                    groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, metadata));
+                                    final Long offset = partitionData.offset;
+                                    final String metadata = partitionData.metadata;
+                                    final Optional&amp;lt;Integer&amp;gt; leaderEpoch = partitionData.leaderEpoch;
+                                    groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, leaderEpoch, metadata));
                                 }
&lt;p&gt; else {&lt;br/&gt;
                                     log.warn(&quot;Skipping return offset for {} due to error {}.&quot;, topicPartition, error);&lt;br/&gt;
                                 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecord.java b/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecord.java&lt;br/&gt;
index 7f852461844..0413d5b0776 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecord.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecord.java&lt;br/&gt;
@@ -22,6 +22,8 @@&lt;br/&gt;
 import org.apache.kafka.common.record.RecordBatch;&lt;br/&gt;
 import org.apache.kafka.common.record.TimestampType;&lt;/p&gt;

&lt;p&gt;+import java.util.Optional;&lt;br/&gt;
+&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A key/value pair to be received from Kafka. This also consists of a topic name and&lt;/li&gt;
	&lt;li&gt;a partition number from which the record is being received, an offset that points&lt;br/&gt;
@@ -42,6 +44,7 @@&lt;br/&gt;
     private final Headers headers;&lt;br/&gt;
     private final K key;&lt;br/&gt;
     private final V value;&lt;br/&gt;
+    private final Optional&amp;lt;Integer&amp;gt; leaderEpoch;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private volatile Long checksum;&lt;/p&gt;

&lt;p&gt;@@ -120,8 +123,41 @@ public ConsumerRecord(String topic,&lt;br/&gt;
                           K key,&lt;br/&gt;
                           V value,&lt;br/&gt;
                           Headers headers) &lt;/p&gt;
{
+        this(topic, partition, offset, timestamp, timestampType, checksum, serializedKeySize, serializedValueSize,
+                key, value, headers, Optional.empty());
+    }
&lt;p&gt;+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * Creates a record to be received from a specified topic and partition&lt;br/&gt;
+     *&lt;br/&gt;
+     * @param topic The topic this record is received from&lt;br/&gt;
+     * @param partition The partition of the topic this record is received from&lt;br/&gt;
+     * @param offset The offset of this record in the corresponding Kafka partition&lt;br/&gt;
+     * @param timestamp The timestamp of the record.&lt;br/&gt;
+     * @param timestampType The timestamp type&lt;br/&gt;
+     * @param checksum The checksum (CRC32) of the full record&lt;br/&gt;
+     * @param serializedKeySize The length of the serialized key&lt;br/&gt;
+     * @param serializedValueSize The length of the serialized value&lt;br/&gt;
+     * @param key The key of the record, if one exists (null is allowed)&lt;br/&gt;
+     * @param value The record contents&lt;br/&gt;
+     * @param headers The headers of the record&lt;br/&gt;
+     * @param leaderEpoch Optional leader epoch of the record (may be empty for legacy record formats)&lt;br/&gt;
+     */&lt;br/&gt;
+    public ConsumerRecord(String topic,&lt;br/&gt;
+                          int partition,&lt;br/&gt;
+                          long offset,&lt;br/&gt;
+                          long timestamp,&lt;br/&gt;
+                          TimestampType timestampType,&lt;br/&gt;
+                          Long checksum,&lt;br/&gt;
+                          int serializedKeySize,&lt;br/&gt;
+                          int serializedValueSize,&lt;br/&gt;
+                          K key,&lt;br/&gt;
+                          V value,&lt;br/&gt;
+                          Headers headers,&lt;br/&gt;
+                          Optional&amp;lt;Integer&amp;gt; leaderEpoch) &lt;/p&gt;
{
         if (topic == null)
             throw new IllegalArgumentException(&quot;Topic cannot be null&quot;);
+
         this.topic = topic;
         this.partition = partition;
         this.offset = offset;
@@ -133,6 +169,7 @@ public ConsumerRecord(String topic,
         this.key = key;
         this.value = value;
         this.headers = headers;
+        this.leaderEpoch = leaderEpoch;
     }

&lt;p&gt;     /**&lt;br/&gt;
@@ -225,13 +262,26 @@ public int serializedValueSize() &lt;/p&gt;
{
         return this.serializedValueSize;
     }

&lt;p&gt;+    /**&lt;br/&gt;
+     * Get the leader epoch for the record if available&lt;br/&gt;
+     *&lt;br/&gt;
+     * @return the leader epoch or empty for legacy record formats&lt;br/&gt;
+     */&lt;br/&gt;
+    public Optional&amp;lt;Integer&amp;gt; leaderEpoch() &lt;/p&gt;
{
+        return leaderEpoch;
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() {
-        return &quot;ConsumerRecord(topic = &quot; + topic() + &quot;, partition = &quot; + partition() + &quot;, offset = &quot; + offset()
+        return &quot;ConsumerRecord(topic = &quot; + topic
+               + &quot;, partition = &quot; + partition
+               + &quot;, leaderEpoch = &quot; + leaderEpoch.orElse(null)
+               + &quot;, offset = &quot; + offset
                + &quot;, &quot; + timestampType + &quot; = &quot; + timestamp
                + &quot;, serialized key size = &quot;  + serializedKeySize
                + &quot;, serialized value size = &quot; + serializedValueSize
                + &quot;, headers = &quot; + headers
-               + &quot;, key = &quot; + key + &quot;, value = &quot; + value + &quot;)&quot;;
+               + &quot;, key = &quot; + key
+               + &quot;, value = &quot; + value + &quot;)&quot;;
     }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndMetadata.java b/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndMetadata.java&lt;br/&gt;
index 262d8f8fc6c..aa91e509b87 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndMetadata.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndMetadata.java&lt;br/&gt;
@@ -19,6 +19,8 @@&lt;br/&gt;
 import org.apache.kafka.common.requests.OffsetFetchResponse;&lt;br/&gt;
 &lt;br/&gt;
 import java.io.Serializable;&lt;br/&gt;
+import java.util.Objects;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * The Kafka offset commit API allows users to provide additional metadata (in the form of a string)&lt;br/&gt;
@@ -26,16 +28,30 @@&lt;br/&gt;
  * node made the commit, what time the commit was made, etc.&lt;br/&gt;
  */&lt;br/&gt;
 public class OffsetAndMetadata implements Serializable {&lt;br/&gt;
+    private static final long serialVersionUID = 2019555404968089681L;&lt;br/&gt;
+&lt;br/&gt;
     private final long offset;&lt;br/&gt;
     private final String metadata;&lt;br/&gt;
 &lt;br/&gt;
+    // We use null to represent the absence of a leader epoch to simplify serialization.&lt;br/&gt;
+    // I.e., older serializations of this class which do not have this field will automatically&lt;br/&gt;
+    // initialize its value to null.&lt;br/&gt;
+    private final Integer leaderEpoch;&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * Construct a new OffsetAndMetadata object for committing through {@link KafkaConsumer}.&lt;br/&gt;
+     *&lt;br/&gt;
      * @param offset The offset to be committed&lt;br/&gt;
+     * @param leaderEpoch Optional leader epoch of the last consumed record&lt;br/&gt;
      * @param metadata Non-null metadata&lt;br/&gt;
      */&lt;br/&gt;
-    public OffsetAndMetadata(long offset, String metadata) {&lt;br/&gt;
+    public OffsetAndMetadata(long offset, Optional&amp;lt;Integer&amp;gt; leaderEpoch, String metadata) {&lt;br/&gt;
+        if (offset &amp;lt; 0)&lt;br/&gt;
+            throw new IllegalArgumentException(&quot;Invalid negative offset&quot;);&lt;br/&gt;
+&lt;br/&gt;
         this.offset = offset;&lt;br/&gt;
+        this.leaderEpoch = leaderEpoch.orElse(null);&lt;br/&gt;
+&lt;br/&gt;
         // The server converts null metadata to an empty string. So we store it as an empty string as well on the client&lt;br/&gt;
         // to be consistent.&lt;br/&gt;
         if (metadata == null)&lt;br/&gt;
@@ -44,6 +60,15 @@ public OffsetAndMetadata(long offset, String metadata) {
             this.metadata = metadata;
     }&lt;br/&gt;
 &lt;br/&gt;
+    /**&lt;br/&gt;
+     * Construct a new OffsetAndMetadata object for committing through {@link KafkaConsumer}.&lt;br/&gt;
+     * @param offset The offset to be committed&lt;br/&gt;
+     * @param metadata Non-null metadata&lt;br/&gt;
+     */&lt;br/&gt;
+    public OffsetAndMetadata(long offset, String metadata) {
+        this(offset, Optional.empty(), metadata);
+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * Construct a new OffsetAndMetadata object for committing through {@link KafkaConsumer}. The metadata&lt;br/&gt;
      * associated with the commit will be empty.&lt;br/&gt;
@@ -61,29 +86,39 @@ public String metadata() {
         return metadata;
     }&lt;br/&gt;
 &lt;br/&gt;
+    /**&lt;br/&gt;
+     * Get the leader epoch of the previously consumed record (if one is known). Log truncation is detected&lt;br/&gt;
+     * if there exists a leader epoch which is larger than this epoch and begins at an offset earlier than&lt;br/&gt;
+     * the committed offset.&lt;br/&gt;
+     *&lt;br/&gt;
+     * @return the leader epoch or empty if not known&lt;br/&gt;
+     */&lt;br/&gt;
+    public Optional&amp;lt;Integer&amp;gt; leaderEpoch() {
+        return Optional.ofNullable(leaderEpoch);
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Override&lt;br/&gt;
     public boolean equals(Object o) {
         if (this == o) return true;
         if (o == null || getClass() != o.getClass()) return false;
-
         OffsetAndMetadata that = (OffsetAndMetadata) o;
-
-        if (offset != that.offset) return false;
-        return metadata.equals(that.metadata);
+        return offset == that.offset &amp;amp;&amp;amp;
+                Objects.equals(metadata, that.metadata) &amp;amp;&amp;amp;
+                Objects.equals(leaderEpoch, that.leaderEpoch);
     }&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public int hashCode() {
-        int result = (int) (offset ^ (offset &amp;gt;&amp;gt;&amp;gt; 32));
-        result = 31 * result + metadata.hashCode();
-        return result;
+        return Objects.hash(offset, metadata, leaderEpoch);
     }&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() {&lt;br/&gt;
         return &quot;OffsetAndMetadata{&quot; +
                 &quot;offset=&quot; + offset +
+                &quot;, leaderEpoch=&quot; + leaderEpoch +
                 &quot;, metadata=&apos;&quot; + metadata + &apos;\&apos;&apos; +
                 &apos;}&apos;;&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndTimestamp.java b/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndTimestamp.java&lt;br/&gt;
index 3af057f9ce4..40d993074a2 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndTimestamp.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndTimestamp.java&lt;br/&gt;
@@ -16,7 +16,8 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.clients.consumer;&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import java.util.Objects;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * A container class for offset and timestamp.&lt;br/&gt;
@@ -24,12 +25,22 @@&lt;br/&gt;
 public final class OffsetAndTimestamp {&lt;br/&gt;
     private final long timestamp;&lt;br/&gt;
     private final long offset;&lt;br/&gt;
+    private final Optional&amp;lt;Integer&amp;gt; leaderEpoch;&lt;br/&gt;
 &lt;br/&gt;
     public OffsetAndTimestamp(long offset, long timestamp) {
+        this(offset, timestamp, Optional.empty());
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public OffsetAndTimestamp(long offset, long timestamp, Optional&amp;lt;Integer&amp;gt; leaderEpoch) {
+        if (offset &amp;lt; 0)
+            throw new IllegalArgumentException(&quot;Invalid negative offset&quot;);
+
+        if (timestamp &amp;lt; 0)
+            throw new IllegalArgumentException(&quot;Invalid negative timestamp&quot;);
+
         this.offset = offset;
-        assert this.offset &amp;gt;= 0;
         this.timestamp = timestamp;
-        assert this.timestamp &amp;gt;= 0;
+        this.leaderEpoch = leaderEpoch;
     }&lt;br/&gt;
 &lt;br/&gt;
     public long timestamp() {&lt;br/&gt;
@@ -40,21 +51,35 @@ public long offset() {
         return offset;
     }&lt;br/&gt;
 &lt;br/&gt;
+    /**&lt;br/&gt;
+     * Get the leader epoch corresponding to the offset that was found (if one exists).&lt;br/&gt;
+     * This can be provided to seek() to ensure that the log hasn&apos;t been truncated prior to fetching.&lt;br/&gt;
+     *&lt;br/&gt;
+     * @return The leader epoch or empty if it is not known&lt;br/&gt;
+     */&lt;br/&gt;
+    public Optional&amp;lt;Integer&amp;gt; leaderEpoch() {+        return leaderEpoch;+    }
&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public String toString() &lt;/p&gt;
{
-        return &quot;(timestamp=&quot; + timestamp + &quot;, offset=&quot; + offset + &quot;)&quot;;
+        return &quot;(timestamp=&quot; + timestamp +
+                &quot;, leaderEpoch=&quot; + leaderEpoch.orElse(null) +
+                &quot;, offset=&quot; + offset + &quot;)&quot;;
     }

&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public int hashCode() {&lt;/li&gt;
	&lt;li&gt;return 31 * Utils.longHashcode(timestamp) + Utils.longHashcode(offset);&lt;br/&gt;
+    public boolean equals(Object o) 
{
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+        OffsetAndTimestamp that = (OffsetAndTimestamp) o;
+        return timestamp == that.timestamp &amp;amp;&amp;amp;
+                offset == that.offset &amp;amp;&amp;amp;
+                Objects.equals(leaderEpoch, that.leaderEpoch);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public boolean equals(Object o) {&lt;/li&gt;
	&lt;li&gt;if (o == null || !(o instanceof OffsetAndTimestamp))&lt;/li&gt;
	&lt;li&gt;return false;&lt;/li&gt;
	&lt;li&gt;OffsetAndTimestamp other = (OffsetAndTimestamp) o;&lt;/li&gt;
	&lt;li&gt;return this.timestamp == other.timestamp() &amp;amp;&amp;amp; this.offset == other.offset();&lt;br/&gt;
+    public int hashCode() 
{
+        return Objects.hash(timestamp, offset, leaderEpoch);
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java b/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java&lt;br/&gt;
index 12da7e5367d..0d74eed7835 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java&lt;br/&gt;
@@ -671,7 +671,7 @@ boolean isSticky() {&lt;br/&gt;
     static ByteBuffer serializeTopicPartitionAssignment(List&amp;lt;TopicPartition&amp;gt; partitions) {&lt;br/&gt;
         Struct struct = new Struct(STICKY_ASSIGNOR_USER_DATA);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicAssignments = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicEntry : CollectionUtils.groupDataByTopic(partitions).entrySet()) {&lt;br/&gt;
+        for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicEntry : CollectionUtils.groupPartitionsByTopic(partitions).entrySet()) {&lt;br/&gt;
             Struct topicAssignment = new Struct(TOPIC_ASSIGNMENT);&lt;br/&gt;
             topicAssignment.set(TOPIC_KEY_NAME, topicEntry.getKey());&lt;br/&gt;
             topicAssignment.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java&lt;br/&gt;
index ce2db3566e9..3b0a5fa9c0a 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java&lt;br/&gt;
@@ -760,8 +760,8 @@ public void onComplete(Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets, Exception&lt;br/&gt;
             if (offsetAndMetadata.offset() &amp;lt; 0) 
{
                 return RequestFuture.failure(new IllegalArgumentException(&quot;Invalid offset: &quot; + offsetAndMetadata.offset()));
             }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;offsetData.put(entry.getKey(), new OffsetCommitRequest.PartitionData(&lt;/li&gt;
	&lt;li&gt;offsetAndMetadata.offset(), offsetAndMetadata.metadata()));&lt;br/&gt;
+            offsetData.put(entry.getKey(), new OffsetCommitRequest.PartitionData(offsetAndMetadata.offset(),&lt;br/&gt;
+                    offsetAndMetadata.leaderEpoch(), offsetAndMetadata.metadata()));&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Generation generation;&lt;br/&gt;
@@ -913,7 +913,7 @@ public void handle(OffsetFetchResponse response, RequestFuture&amp;lt;Map&amp;lt;TopicPartitio&lt;br/&gt;
                     return;&lt;br/&gt;
                 } else if (data.offset &amp;gt;= 0) &lt;/p&gt;
{
                     // record the position with the offset (-1 indicates no committed offset to fetch)
-                    offsets.put(tp, new OffsetAndMetadata(data.offset, data.metadata));
+                    offsets.put(tp, new OffsetAndMetadata(data.offset, data.leaderEpoch, data.metadata));
                 }
&lt;p&gt; else {&lt;br/&gt;
                     log.debug(&quot;Found no committed offset for partition {}&quot;, tp);&lt;br/&gt;
                 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocol.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocol.java&lt;br/&gt;
index 920c2957c4d..8a4aef8b334 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocol.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocol.java&lt;br/&gt;
@@ -123,7 +123,7 @@ public static ByteBuffer serializeAssignment(PartitionAssignor.Assignment assign&lt;br/&gt;
         Struct struct = new Struct(ASSIGNMENT_V0);&lt;br/&gt;
         struct.set(USER_DATA_KEY_NAME, assignment.userData());&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicAssignments = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; partitionsByTopic = CollectionUtils.groupDataByTopic(assignment.partitions());&lt;br/&gt;
+        Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; partitionsByTopic = CollectionUtils.groupPartitionsByTopic(assignment.partitions());&lt;br/&gt;
         for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicEntry : partitionsByTopic.entrySet()) {&lt;br/&gt;
             Struct topicAssignment = new Struct(TOPIC_ASSIGNMENT_V0);&lt;br/&gt;
             topicAssignment.set(TOPIC_KEY_NAME, topicEntry.getKey());&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index 36a3314e698..dc0daa233ab 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -82,6 +82,7 @@&lt;br/&gt;
 import java.util.LinkedHashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.PriorityQueue;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.concurrent.ConcurrentLinkedQueue;&lt;br/&gt;
@@ -168,10 +169,12 @@ public Fetcher(LogContext logContext,&lt;br/&gt;
     private static class OffsetData {&lt;br/&gt;
         final long offset;&lt;br/&gt;
         final Long timestamp; //  null if the broker does not support returning timestamps&lt;br/&gt;
+        final Optional&amp;lt;Integer&amp;gt; leaderEpoch; // empty if the leader epoch is not known&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;OffsetData(long offset, Long timestamp) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        OffsetData(long offset, Long timestamp, Optional&amp;lt;Integer&amp;gt; leaderEpoch) {
             this.offset = offset;
             this.timestamp = timestamp;
+            this.leaderEpoch = leaderEpoch;
         }     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -383,7 +386,8 @@ public void resetOffsetsIfNeeded() &lt;/p&gt;
{
             // &apos;entry.getValue().timestamp&apos; will not be null since we are guaranteed
             // to work with a v1 (or later) ListOffset request
             OffsetData offsetData = entry.getValue();
-            offsetsByTimes.put(entry.getKey(), new OffsetAndTimestamp(offsetData.offset, offsetData.timestamp));
+            offsetsByTimes.put(entry.getKey(), new OffsetAndTimestamp(offsetData.offset, offsetData.timestamp,
+                    offsetData.leaderEpoch));
         }

&lt;p&gt;         return offsetsByTimes;&lt;br/&gt;
@@ -570,10 +574,11 @@ private void resetOffsetsAsync(Map&amp;lt;TopicPartition, Long&amp;gt; partitionResetTimestamp&lt;br/&gt;
         for (TopicPartition tp : partitionResetTimestamps.keySet())&lt;br/&gt;
             metadata.add(tp.topic());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; timestampsToSearchByNode = groupListOffsetRequests(partitionResetTimestamps);&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; entry : timestampsToSearchByNode.entrySet()) {&lt;br/&gt;
+        Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; timestampsToSearchByNode =&lt;br/&gt;
+                groupListOffsetRequests(partitionResetTimestamps);&lt;br/&gt;
+        for (Map.Entry&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; entry : timestampsToSearchByNode.entrySet()) {&lt;br/&gt;
             Node node = entry.getKey();&lt;/li&gt;
	&lt;li&gt;final Map&amp;lt;TopicPartition, Long&amp;gt; resetTimestamps = entry.getValue();&lt;br/&gt;
+            final Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; resetTimestamps = entry.getValue();&lt;br/&gt;
             subscriptions.setResetPending(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             RequestFuture&amp;lt;ListOffsetResult&amp;gt; future = sendListOffsetRequest(node, resetTimestamps, false);&lt;br/&gt;
@@ -588,8 +593,8 @@ public void onSuccess(ListOffsetResult result) {&lt;br/&gt;
                     for (Map.Entry&amp;lt;TopicPartition, OffsetData&amp;gt; fetchedOffset : result.fetchedOffsets.entrySet()) &lt;/p&gt;
{
                         TopicPartition partition = fetchedOffset.getKey();
                         OffsetData offsetData = fetchedOffset.getValue();
-                        Long requestedResetTimestamp = resetTimestamps.get(partition);
-                        resetOffsetIfNeeded(partition, requestedResetTimestamp, offsetData);
+                        ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition);
+                        resetOffsetIfNeeded(partition, requestedReset.timestamp, offsetData);
                     }
&lt;p&gt;                 }&lt;/p&gt;

&lt;p&gt;@@ -619,7 +624,8 @@ public void onFailure(RuntimeException e) {&lt;br/&gt;
         for (TopicPartition tp : timestampsToSearch.keySet())&lt;br/&gt;
             metadata.add(tp.topic());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; timestampsToSearchByNode = groupListOffsetRequests(timestampsToSearch);&lt;br/&gt;
+        Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; timestampsToSearchByNode =&lt;br/&gt;
+                groupListOffsetRequests(timestampsToSearch);&lt;br/&gt;
         if (timestampsToSearchByNode.isEmpty())&lt;br/&gt;
             return RequestFuture.failure(new StaleMetadataException());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -628,7 +634,7 @@ public void onFailure(RuntimeException e) {&lt;br/&gt;
         final Set&amp;lt;TopicPartition&amp;gt; partitionsToRetry = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         final AtomicInteger remainingResponses = new AtomicInteger(timestampsToSearchByNode.size());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; entry : timestampsToSearchByNode.entrySet()) {&lt;br/&gt;
+        for (Map.Entry&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; entry : timestampsToSearchByNode.entrySet()) {&lt;br/&gt;
             RequestFuture&amp;lt;ListOffsetResult&amp;gt; future =&lt;br/&gt;
                     sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);&lt;br/&gt;
             future.addListener(new RequestFutureListener&amp;lt;ListOffsetResult&amp;gt;() {&lt;br/&gt;
@@ -657,8 +663,9 @@ public void onFailure(RuntimeException e) 
{
         return listOffsetRequestsFuture;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Map&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; groupListOffsetRequests(Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSearch) {&lt;/li&gt;
	&lt;li&gt;final Map&amp;lt;Node, Map&amp;lt;TopicPartition, Long&amp;gt;&amp;gt; timestampsToSearchByNode = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; groupListOffsetRequests(&lt;br/&gt;
+            Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSearch) {&lt;br/&gt;
+        final Map&amp;lt;Node, Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt;&amp;gt; timestampsToSearchByNode = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry: timestampsToSearch.entrySet()) {&lt;br/&gt;
             TopicPartition tp  = entry.getKey();&lt;br/&gt;
             PartitionInfo info = metadata.fetch().partition(tp);&lt;br/&gt;
@@ -679,12 +686,11 @@ public void onFailure(RuntimeException e) 
{
                         info.leader(), tp);
             }
&lt;p&gt; else {&lt;br/&gt;
                 Node node = info.leader();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, Long&amp;gt; topicData = timestampsToSearchByNode.get(node);&lt;/li&gt;
	&lt;li&gt;if (topicData == null) 
{
-                    topicData = new HashMap&amp;lt;&amp;gt;();
-                    timestampsToSearchByNode.put(node, topicData);
-                }&lt;/li&gt;
	&lt;li&gt;topicData.put(entry.getKey(), entry.getValue());&lt;br/&gt;
+                Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; topicData =&lt;br/&gt;
+                        timestampsToSearchByNode.computeIfAbsent(node, n -&amp;gt; new HashMap&amp;lt;&amp;gt;());&lt;br/&gt;
+                ListOffsetRequest.PartitionData partitionData = new ListOffsetRequest.PartitionData(&lt;br/&gt;
+                        entry.getValue(), Optional.empty());&lt;br/&gt;
+                topicData.put(entry.getKey(), partitionData);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
         return timestampsToSearchByNode;&lt;br/&gt;
@@ -699,7 +705,7 @@ public void onFailure(RuntimeException e) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return A response which can be polled to obtain the corresponding timestamps and offsets.&lt;br/&gt;
      */&lt;br/&gt;
     private RequestFuture&amp;lt;ListOffsetResult&amp;gt; sendListOffsetRequest(final Node node,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSearch,&lt;br/&gt;
+                                                                  final Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; timestampsToSearch,&lt;br/&gt;
                                                                   boolean requireTimestamp) {&lt;br/&gt;
         ListOffsetRequest.Builder builder = ListOffsetRequest.Builder&lt;br/&gt;
                 .forConsumer(requireTimestamp, isolationLevel)&lt;br/&gt;
@@ -729,14 +735,14 @@ public void onSuccess(ClientResponse response, RequestFuture&amp;lt;ListOffsetResult&amp;gt; f&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;return a null timestamp (-1 is returned instead when necessary).&lt;br/&gt;
      */&lt;br/&gt;
     @SuppressWarnings(&quot;deprecation&quot;)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void handleListOffsetResponse(Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSearch,&lt;br/&gt;
+    private void handleListOffsetResponse(Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; timestampsToSearch,&lt;br/&gt;
                                           ListOffsetResponse listOffsetResponse,&lt;br/&gt;
                                           RequestFuture&amp;lt;ListOffsetResult&amp;gt; future) {&lt;br/&gt;
         Map&amp;lt;TopicPartition, OffsetData&amp;gt; fetchedOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         Set&amp;lt;TopicPartition&amp;gt; partitionsToRetry = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         Set&amp;lt;String&amp;gt; unauthorizedTopics = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : timestampsToSearch.entrySet()) {&lt;br/&gt;
+        for (Map.Entry&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; entry : timestampsToSearch.entrySet()) {&lt;br/&gt;
             TopicPartition topicPartition = entry.getKey();&lt;br/&gt;
             ListOffsetResponse.PartitionData partitionData = listOffsetResponse.responseData().get(topicPartition);&lt;br/&gt;
             Errors error = partitionData.error;&lt;br/&gt;
@@ -756,7 +762,7 @@ private void handleListOffsetResponse(Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSear&lt;br/&gt;
                     log.debug(&quot;Handling v0 ListOffsetResponse response for {}. Fetched offset {}&quot;,&lt;br/&gt;
                             topicPartition, offset);&lt;br/&gt;
                     if (offset != ListOffsetResponse.UNKNOWN_OFFSET) 
{
-                        OffsetData offsetData = new OffsetData(offset, null);
+                        OffsetData offsetData = new OffsetData(offset, null, Optional.empty());
                         fetchedOffsets.put(topicPartition, offsetData);
                     }
&lt;p&gt;                 } else {&lt;br/&gt;
@@ -764,7 +770,8 @@ private void handleListOffsetResponse(Map&amp;lt;TopicPartition, Long&amp;gt; timestampsToSear&lt;br/&gt;
                     log.debug(&quot;Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}&quot;,&lt;br/&gt;
                             topicPartition, partitionData.offset, partitionData.timestamp);&lt;br/&gt;
                     if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) &lt;/p&gt;
{
-                        OffsetData offsetData = new OffsetData(partitionData.offset, partitionData.timestamp);
+                        OffsetData offsetData = new OffsetData(partitionData.offset, partitionData.timestamp,
+                                partitionData.leaderEpoch);
                         fetchedOffsets.put(topicPartition, offsetData);
                     }
&lt;p&gt;                 }&lt;br/&gt;
@@ -857,7 +864,7 @@ public ListOffsetResult() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 long position = this.subscriptions.position(partition);&lt;br/&gt;
                 builder.add(partition, new FetchRequest.PartitionData(position, FetchRequest.INVALID_LOG_START_OFFSET,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.fetchSize));&lt;br/&gt;
+                    this.fetchSize, Optional.empty()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 log.debug(&quot;Added {} fetch request for partition {} at offset {} to node {}&quot;, isolationLevel,&lt;br/&gt;
                     partition, position, node);&lt;br/&gt;
@@ -979,6 +986,7 @@ private PartitionRecords parseCompletedFetch(CompletedFetch completedFetch) {&lt;br/&gt;
         try {&lt;br/&gt;
             long offset = record.offset();&lt;br/&gt;
             long timestamp = record.timestamp();&lt;br/&gt;
+            Optional&amp;lt;Integer&amp;gt; leaderEpoch = maybeLeaderEpoch(batch.partitionLeaderEpoch());&lt;br/&gt;
             TimestampType timestampType = batch.timestampType();&lt;br/&gt;
             Headers headers = new RecordHeaders(record.headers());&lt;br/&gt;
             ByteBuffer keyBytes = record.key();&lt;br/&gt;
@@ -991,13 +999,17 @@ private PartitionRecords parseCompletedFetch(CompletedFetch completedFetch) &lt;/p&gt;
{
                                         timestamp, timestampType, record.checksumOrNull(),
                                         keyByteArray == null ? ConsumerRecord.NULL_SIZE : keyByteArray.length,
                                         valueByteArray == null ? ConsumerRecord.NULL_SIZE : valueByteArray.length,
-                                        key, value, headers);
+                                        key, value, headers, leaderEpoch);
         }
&lt;p&gt; catch (RuntimeException e) &lt;/p&gt;
{
             throw new SerializationException(&quot;Error deserializing key/value for partition &quot; + partition +
                     &quot; at offset &quot; + record.offset() + &quot;. If needed, please seek past the record to continue consumption.&quot;, e);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;+    private Optional&amp;lt;Integer&amp;gt; maybeLeaderEpoch(int leaderEpoch) &lt;/p&gt;
{
+        return leaderEpoch == RecordBatch.NO_PARTITION_LEADER_EPOCH ? Optional.empty() : Optional.of(leaderEpoch);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Override&lt;br/&gt;
     public void onAssignment(Set&amp;lt;TopicPartition&amp;gt; assignment) {&lt;br/&gt;
         sensors.updatePartitionLagAndLeadSensors(assignment);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java b/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java&lt;br/&gt;
index c0685c9cb2d..2cbd1e97d1a 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java&lt;br/&gt;
@@ -839,7 +839,8 @@ private TxnOffsetCommitHandler txnOffsetCommitHandler(TransactionalRequestResult&lt;br/&gt;
                                                           String consumerGroupId) {&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; entry : offsets.entrySet()) &lt;/p&gt;
{
             OffsetAndMetadata offsetAndMetadata = entry.getValue();
-            CommittedOffset committedOffset = new CommittedOffset(offsetAndMetadata.offset(), offsetAndMetadata.metadata());
+            CommittedOffset committedOffset = new CommittedOffset(offsetAndMetadata.offset(),
+                    offsetAndMetadata.metadata(), offsetAndMetadata.leaderEpoch());
             pendingTxnOffsetCommits.put(entry.getKey(), committedOffset);
         }
&lt;p&gt;         TxnOffsetCommitRequest.Builder builder = new TxnOffsetCommitRequest.Builder(transactionalId, consumerGroupId,&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/PartitionInfo.java b/clients/src/main/java/org/apache/kafka/common/PartitionInfo.java&lt;br/&gt;
index 38e4f6788e9..44cd4f4d215 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/PartitionInfo.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/PartitionInfo.java&lt;br/&gt;
@@ -20,7 +20,6 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This is used to describe per-partition state in the MetadataResponse.&lt;br/&gt;
  */&lt;br/&gt;
 public class PartitionInfo 
{
-
     private final String topic;
     private final int partition;
     private final Node leader;
@@ -33,7 +32,12 @@ public PartitionInfo(String topic, int partition, Node leader, Node[] replicas,
         this(topic, partition, leader, replicas, inSyncReplicas, new Node[0]);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public PartitionInfo(String topic, int partition, Node leader, Node[] replicas, Node[] inSyncReplicas, Node[] offlineReplicas) {&lt;br/&gt;
+    public PartitionInfo(String topic,&lt;br/&gt;
+                         int partition,&lt;br/&gt;
+                         Node leader,&lt;br/&gt;
+                         Node[] replicas,&lt;br/&gt;
+                         Node[] inSyncReplicas,&lt;br/&gt;
+                         Node[] offlineReplicas) {&lt;br/&gt;
         this.topic = topic;&lt;br/&gt;
         this.partition = partition;&lt;br/&gt;
         this.leader = leader;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/TopicPartition.java b/clients/src/main/java/org/apache/kafka/common/TopicPartition.java&lt;br/&gt;
index dc79c2e13dc..08b2a51d589 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/TopicPartition.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/TopicPartition.java&lt;br/&gt;
@@ -22,6 +22,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;A topic name and partition number&lt;br/&gt;
  */&lt;br/&gt;
 public final class TopicPartition implements Serializable 
{
+    private static final long serialVersionUID = -613627415771699627L;
 
     private int hash = 0;
     private final int partition;
diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java b/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java
index 9eddf2b17e6..708500c65e6 100644
--- a/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java
@@ -27,7 +27,12 @@
     public static final Field.Int32 PARTITION_ID = new Field.Int32(&quot;partition&quot;, &quot;Topic partition id&quot;);
     public static final Field.Int16 ERROR_CODE = new Field.Int16(&quot;error_code&quot;, &quot;Response error code&quot;);
     public static final Field.NullableStr ERROR_MESSAGE = new Field.NullableStr(&quot;error_message&quot;, &quot;Response error message&quot;);
-    public static final Field.Int32 LEADER_EPOCH = new Field.Int32(&quot;leader_epoch&quot;, &quot;The epoch&quot;);
+    public static final Field.Int32 LEADER_EPOCH = new Field.Int32(&quot;leader_epoch&quot;, &quot;The leader epoch&quot;);
+    public static final Field.Int32 CURRENT_LEADER_EPOCH = new Field.Int32(&quot;current_leader_epoch&quot;,
+            &quot;The current leader epoch, if provided, is used to fence consumers/replicas with old metadata. &quot; +
+                    &quot;If the epoch provided by the client is larger than the current epoch known to the broker, then &quot; +
+                    &quot;the UNKNOWN_LEADER_EPOCH error code will be returned. If the provided epoch is smaller, then &quot; +
+                    &quot;the FENCED_LEADER_EPOCH error code will be returned.&quot;);
 
     // Group APIs
     public static final Field.Str GROUP_ID = new Field.Str(&quot;group_id&quot;, &quot;The unique group identifier&quot;);
@@ -58,4 +63,13 @@
     public static final Field.Str PRINCIPAL_TYPE = new Field.Str(&quot;principal_type&quot;, &quot;principalType of the Kafka principal&quot;);
     public static final Field.Str PRINCIPAL_NAME = new Field.Str(&quot;name&quot;, &quot;name of the Kafka principal&quot;);
 
+    public static final Field.Int64 COMMITTED_OFFSET = new Field.Int64(&quot;offset&quot;,
+            &quot;Message offset to be committed&quot;);
+    public static final Field.NullableStr COMMITTED_METADATA = new Field.NullableStr(&quot;metadata&quot;,
+            &quot;Any associated metadata the client wants to keep.&quot;);
+    public static final Field.Int32 COMMITTED_LEADER_EPOCH = new Field.Int32(&quot;leader_epoch&quot;,
+            &quot;The leader epoch, if provided is derived from the last consumed record. &quot; +
+                    &quot;This is used by the consumer to check for log truncation and to ensure partition &quot; +
+                    &quot;metadata is up to date following a group rebalance.&quot;);
+
 }
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java b/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java&lt;br/&gt;
index 5c170017454..72e051c179e 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java&lt;br/&gt;
@@ -92,4 +92,41 @@ public NullableStr(String name, String docString) 
{
             super(name, Type.NULLABLE_STRING, docString, false, null);
         }
&lt;p&gt;     }&lt;br/&gt;
+&lt;br/&gt;
+    public static class Bool extends Field &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        public Bool(String name, String docString) {
+            super(name, Type.BOOLEAN, docString, false, null);
+        }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+    public static class Array extends Field &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        public Array(String name, Type elementType, String docString) {
+            super(name, new ArrayOf(elementType), docString, false, null);
+        }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+    public static class ComplexArray &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        public final String name;+        public final String docString;++        public ComplexArray(String name, String docString) {
+            this.name = name;
+            this.docString = docString;
+        }++        public Field withFields(Field... fields) {
+            Schema elementType = new Schema(fields);
+            return new Field(name, new ArrayOf(elementType), docString, false, null);
+        }++        public Field nullableWithFields(Field... fields) {
+            Schema elementType = new Schema(fields);
+            return new Field(name, ArrayOf.nullable(elementType), docString, false, null);
+        }++        public Field withFields(String docStringOverride, Field... fields) {
+            Schema elementType = new Schema(fields);
+            return new Field(name, new ArrayOf(elementType), docStringOverride, false, null);
+        }+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java b/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
index 7183aedbd95..94d5ae1a797 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java&lt;br/&gt;
@@ -99,6 +99,14 @@ public String get(Field.NullableStr field) 
{
         return getString(field.name);
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    public Object[] get(Field.Array field) &lt;/p&gt;
{
+        return getArray(field.name);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public Object[] get(Field.ComplexArray field) {+        return getArray(field.name);+    }
&lt;p&gt;+&lt;br/&gt;
     public Long getOrElse(Field.Int64 field, long alternative) {&lt;br/&gt;
         if (hasField(field.name))&lt;br/&gt;
             return getLong(field.name);&lt;br/&gt;
@@ -135,6 +143,24 @@ public String getOrElse(Field.Str field, String alternative) &lt;/p&gt;
{
         return alternative;
     }

&lt;p&gt;+    public boolean getOrElse(Field.Bool field, boolean alternative) &lt;/p&gt;
{
+        if (hasField(field.name))
+            return getBoolean(field.name);
+        return alternative;
+    }
&lt;p&gt;+&lt;br/&gt;
+    public Object[] getOrEmpty(Field.Array field) &lt;/p&gt;
{
+        if (hasField(field.name))
+            return getArray(field.name);
+        return new Object[0];
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public Object[] getOrEmpty(Field.ComplexArray field) {+        if (hasField(field.name))+            return getArray(field.name);+        return new Object[0];+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Get the record value for the field with the given name by doing a hash table lookup (slower!)&lt;br/&gt;
      *&lt;br/&gt;
@@ -162,6 +188,10 @@ public boolean hasField(Field def) 
{
         return schema.get(def.name) != null;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    public boolean hasField(Field.ComplexArray def) &lt;/p&gt;
{
+        return schema.get(def.name) != null;
+    }
&lt;p&gt;+&lt;br/&gt;
     public Struct getStruct(BoundField field) &lt;/p&gt;
{
         return (Struct) get(field);
     }
&lt;p&gt;@@ -300,6 +330,22 @@ public Struct set(Field.Int16 def, short value) &lt;/p&gt;
{
         return set(def.name, value);
     }

&lt;p&gt;+    public Struct set(Field.Array def, Object[] value) &lt;/p&gt;
{
+        return set(def.name, value);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public Struct set(Field.ComplexArray def, Object[] value) {+        return set(def.name, value);+    }
&lt;p&gt;+&lt;br/&gt;
+    public Struct setIfExists(Field.Array def, Object[] value) &lt;/p&gt;
{
+        return set(def.name, value);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public Struct setIfExists(Field.ComplexArray def, Object[] value) {+        return set(def.name, value);+    }
&lt;p&gt;+&lt;br/&gt;
     public Struct setIfExists(Field def, Object value) &lt;/p&gt;
{
         return setIfExists(def.name, value);
     }
&lt;p&gt;@@ -343,6 +389,14 @@ public Struct instance(String field) &lt;/p&gt;
{
         return instance(schema.get(field));
     }

&lt;p&gt;+    public Struct instance(Field field) &lt;/p&gt;
{
+        return instance(schema.get(field.name));
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public Struct instance(Field.ComplexArray field) {+        return instance(schema.get(field.name));+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Empty all the values from this record&lt;br/&gt;
      */&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java&lt;br/&gt;
index d2b93c4590e..d16e60f84db 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java&lt;br/&gt;
@@ -16,6 +16,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.requests;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import org.apache.kafka.common.errors.UnsupportedVersionException;&lt;br/&gt;
 import org.apache.kafka.common.network.NetworkSend;&lt;br/&gt;
 import org.apache.kafka.common.network.Send;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
@@ -76,7 +77,9 @@ public T build() {&lt;/p&gt;

&lt;p&gt;     private final short version;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public AbstractRequest(short version) {&lt;br/&gt;
+    public AbstractRequest(ApiKeys api, short version) 
{
+        if (!api.isVersionSupported(version))
+            throw new UnsupportedVersionException(&quot;The &quot; + api + &quot; protocol does not support version &quot; + version);
         this.version = version;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnRequest.java&lt;br/&gt;
index 6fb9441ace6..2668ae1db57 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnRequest.java&lt;br/&gt;
@@ -86,7 +86,7 @@ public String toString() {&lt;br/&gt;
     private final String consumerGroupId;&lt;/p&gt;

&lt;p&gt;     private AddOffsetsToTxnRequest(short version, String transactionalId, long producerId, short producerEpoch, String consumerGroupId) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.ADD_OFFSETS_TO_TXN, version);
         this.transactionalId = transactionalId;
         this.producerId = producerId;
         this.producerEpoch = producerEpoch;
@@ -94,7 +94,7 @@ private AddOffsetsToTxnRequest(short version, String transactionalId, long produ
     }

&lt;p&gt;     public AddOffsetsToTxnRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.ADD_OFFSETS_TO_TXN, version);&lt;br/&gt;
         this.transactionalId = struct.get(TRANSACTIONAL_ID);&lt;br/&gt;
         this.producerId = struct.get(PRODUCER_ID);&lt;br/&gt;
         this.producerEpoch = struct.get(PRODUCER_EPOCH);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java&lt;br/&gt;
index 4a87289bbb0..3be7c9836ab 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java&lt;br/&gt;
@@ -102,7 +102,7 @@ public String toString() {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private AddPartitionsToTxnRequest(short version, String transactionalId, long producerId, short producerEpoch,&lt;br/&gt;
                                       List&amp;lt;TopicPartition&amp;gt; partitions) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.ADD_PARTITIONS_TO_TXN, version);
         this.transactionalId = transactionalId;
         this.producerId = producerId;
         this.producerEpoch = producerEpoch;
@@ -110,7 +110,7 @@ private AddPartitionsToTxnRequest(short version, String transactionalId, long pr
     }

&lt;p&gt;     public AddPartitionsToTxnRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.ADD_PARTITIONS_TO_TXN, version);&lt;br/&gt;
         this.transactionalId = struct.get(TRANSACTIONAL_ID);&lt;br/&gt;
         this.producerId = struct.get(PRODUCER_ID);&lt;br/&gt;
         this.producerEpoch = struct.get(PRODUCER_EPOCH);&lt;br/&gt;
@@ -150,7 +150,7 @@ protected Struct toStruct() {&lt;br/&gt;
         struct.set(PRODUCER_ID, producerId);&lt;br/&gt;
         struct.set(PRODUCER_EPOCH, producerEpoch);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupDataByTopic(partitions);&lt;br/&gt;
+        Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupPartitionsByTopic(partitions);&lt;br/&gt;
         Object[] partitionsArray = new Object&lt;span class=&quot;error&quot;&gt;&amp;#91;mappedPartitions.size()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         int i = 0;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicAndPartitions : mappedPartitions.entrySet()) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponse.java&lt;br/&gt;
index 977bd595b27..ea8a073e017 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponse.java&lt;br/&gt;
@@ -110,7 +110,7 @@ protected Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.ADD_PARTITIONS_TO_TXN.responseSchema(version));&lt;br/&gt;
         struct.set(THROTTLE_TIME_MS, throttleTimeMs);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; errorsByTopic = CollectionUtils.groupDataByTopic(errors);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; errorsByTopic = CollectionUtils.groupPartitionDataByTopic(errors);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topics = new ArrayList&amp;lt;&amp;gt;(errorsByTopic.size());&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; entry : errorsByTopic.entrySet()) {&lt;br/&gt;
             Struct topicErrorCodes = struct.instance(ERRORS_KEY_NAME);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java&lt;br/&gt;
index ff1d0625c4d..963ad065e43 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java&lt;br/&gt;
@@ -123,13 +123,13 @@ public AlterConfigsRequest build(short version) {&lt;br/&gt;
     private final boolean validateOnly;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public AlterConfigsRequest(short version, Map&amp;lt;ConfigResource, Config&amp;gt; configs, boolean validateOnly) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.ALTER_CONFIGS, version);
         this.configs = Objects.requireNonNull(configs, &quot;configs&quot;);
         this.validateOnly = validateOnly;
     }

&lt;p&gt;     public AlterConfigsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.ALTER_CONFIGS, version);&lt;br/&gt;
         validateOnly = struct.getBoolean(VALIDATE_ONLY_KEY_NAME);&lt;br/&gt;
         Object[] resourcesArray = struct.getArray(RESOURCES_KEY_NAME);&lt;br/&gt;
         configs = new HashMap&amp;lt;&amp;gt;(resourcesArray.length);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java&lt;br/&gt;
index 0bc7cd0c615..03bc0987eff 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java&lt;br/&gt;
@@ -91,7 +91,7 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public AlterReplicaLogDirsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);&lt;br/&gt;
         partitionDirs = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {&lt;br/&gt;
             Struct logDirStruct = (Struct) logDirStructObj;&lt;br/&gt;
@@ -108,7 +108,7 @@ public AlterReplicaLogDirsRequest(Struct struct, short version) {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public AlterReplicaLogDirsRequest(Map&amp;lt;TopicPartition, String&amp;gt; partitionDirs, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);
         this.partitionDirs = partitionDirs;
     }

&lt;p&gt;@@ -128,7 +128,7 @@ protected Struct toStruct() {&lt;br/&gt;
             logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());&lt;/p&gt;

&lt;p&gt;             List&amp;lt;Struct&amp;gt; topicStructArray = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicEntry: CollectionUtils.groupDataByTopic(logDirEntry.getValue()).entrySet()) {&lt;br/&gt;
+            for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {&lt;br/&gt;
                 Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
                 topicStruct.set(TOPIC_NAME, topicEntry.getKey());&lt;br/&gt;
                 topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java&lt;br/&gt;
index c8f6e4df20f..7a73275d745 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java&lt;br/&gt;
@@ -101,7 +101,7 @@ public AlterReplicaLogDirsResponse(int throttleTimeMs, Map&amp;lt;TopicPartition, Error&lt;br/&gt;
     protected Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.responseSchema(version));&lt;br/&gt;
         struct.set(THROTTLE_TIME_MS, throttleTimeMs);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; responsesByTopic = CollectionUtils.groupDataByTopic(responses);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; responsesByTopic = CollectionUtils.groupPartitionDataByTopic(responses);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicStructArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; responsesByTopicEntry : responsesByTopic.entrySet()) {&lt;br/&gt;
             Struct topicStruct = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsRequest.java&lt;br/&gt;
index 347e3558fd2..e154ac92bb7 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsRequest.java&lt;br/&gt;
@@ -67,7 +67,7 @@ public ApiVersionsRequest(short version) {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public ApiVersionsRequest(short version, Short unsupportedRequestVersion) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.API_VERSIONS, version);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // Unlike other request types, the broker handles ApiVersion requests with higher versions than&lt;br/&gt;
         // supported. It does so by treating the request as if it were v0 and returns a response using&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownRequest.java&lt;br/&gt;
index e6e873429c0..d6db1e1dc0a 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownRequest.java&lt;br/&gt;
@@ -64,12 +64,12 @@ public String toString() {&lt;br/&gt;
     private final int brokerId;&lt;/p&gt;

&lt;p&gt;     private ControlledShutdownRequest(int brokerId, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.CONTROLLED_SHUTDOWN, version);
         this.brokerId = brokerId;
     }

&lt;p&gt;     public ControlledShutdownRequest(Struct struct, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.CONTROLLED_SHUTDOWN, version);
         brokerId = struct.getInt(BROKER_ID_KEY_NAME);
     }

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java&lt;br/&gt;
index a77a373e296..29ecd016f4b 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java&lt;br/&gt;
@@ -123,14 +123,14 @@ public String toString() {&lt;br/&gt;
     private final List&amp;lt;AclCreation&amp;gt; aclCreations;&lt;/p&gt;

&lt;p&gt;     CreateAclsRequest(short version, List&amp;lt;AclCreation&amp;gt; aclCreations) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.CREATE_ACLS, version);
         this.aclCreations = aclCreations;
 
         validate(aclCreations);
     }

&lt;p&gt;     public CreateAclsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.CREATE_ACLS, version);&lt;br/&gt;
         this.aclCreations = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object creationStructObj : struct.getArray(CREATIONS_KEY_NAME)) {&lt;br/&gt;
             Struct creationStruct = (Struct) creationStructObj;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenRequest.java&lt;br/&gt;
index 68e480faba8..3277f100382 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenRequest.java&lt;br/&gt;
@@ -53,13 +53,13 @@&lt;br/&gt;
     private final long maxLifeTime;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private CreateDelegationTokenRequest(short version, List&amp;lt;KafkaPrincipal&amp;gt; renewers, long maxLifeTime) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.CREATE_DELEGATION_TOKEN, version);
         this.maxLifeTime = maxLifeTime;
         this.renewers = renewers;
     }

&lt;p&gt;     public CreateDelegationTokenRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.CREATE_DELEGATION_TOKEN, version);&lt;br/&gt;
         maxLifeTime = struct.getLong(MAX_LIFE_TIME_KEY_NAME);&lt;br/&gt;
         Object[] renewerArray = struct.getArray(RENEWERS_KEY_NAME);&lt;br/&gt;
         renewers = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsRequest.java&lt;br/&gt;
index 5e776bb4dd3..795a66a9ea6 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsRequest.java&lt;br/&gt;
@@ -107,7 +107,7 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     CreatePartitionsRequest(Map&amp;lt;String, NewPartitions&amp;gt; newPartitions, int timeout, boolean validateOnly, short apiVersion) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(apiVersion);&lt;br/&gt;
+        super(ApiKeys.CREATE_PARTITIONS, apiVersion);&lt;br/&gt;
         this.newPartitions = newPartitions;&lt;br/&gt;
         this.duplicates = Collections.emptySet();&lt;br/&gt;
         this.timeout = timeout;&lt;br/&gt;
@@ -115,7 +115,7 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public CreatePartitionsRequest(Struct struct, short apiVersion) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(apiVersion);&lt;br/&gt;
+        super(ApiKeys.CREATE_PARTITIONS, apiVersion);&lt;br/&gt;
         Object[] topicCountArray = struct.getArray(TOPIC_PARTITIONS_KEY_NAME);&lt;br/&gt;
         Map&amp;lt;String, NewPartitions&amp;gt; counts = new HashMap&amp;lt;&amp;gt;(topicCountArray.length);&lt;br/&gt;
         Set&amp;lt;String&amp;gt; dupes = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsRequest.java&lt;br/&gt;
index aa346f5a260..9f05c5a5c0a 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsRequest.java&lt;br/&gt;
@@ -197,7 +197,7 @@ public String toString() {&lt;br/&gt;
     public static final short NO_REPLICATION_FACTOR = -1;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private CreateTopicsRequest(Map&amp;lt;String, TopicDetails&amp;gt; topics, Integer timeout, boolean validateOnly, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.CREATE_TOPICS, version);
         this.topics = topics;
         this.timeout = timeout;
         this.validateOnly = validateOnly;
@@ -205,7 +205,7 @@ private CreateTopicsRequest(Map&amp;lt;String, TopicDetails&amp;gt; topics, Integer timeout, b
     }

&lt;p&gt;     public CreateTopicsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.CREATE_TOPICS, version);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Object[] requestStructs = struct.getArray(REQUESTS_KEY_NAME);&lt;br/&gt;
         Map&amp;lt;String, TopicDetails&amp;gt; topics = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java&lt;br/&gt;
index 4c19a4adbed..c3fc194cd37 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java&lt;br/&gt;
@@ -96,14 +96,14 @@ public String toString() {&lt;br/&gt;
     private final List&amp;lt;AclBindingFilter&amp;gt; filters;&lt;/p&gt;

&lt;p&gt;     DeleteAclsRequest(short version, List&amp;lt;AclBindingFilter&amp;gt; filters) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DELETE_ACLS, version);
         this.filters = filters;
 
         validate(version, filters);
     }

&lt;p&gt;     public DeleteAclsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DELETE_ACLS, version);&lt;br/&gt;
         this.filters = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object filterStructObj : struct.getArray(FILTERS)) {&lt;br/&gt;
             Struct filterStruct = (Struct) filterStructObj;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsRequest.java&lt;br/&gt;
index 29604a505f8..f2a5d92e988 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsRequest.java&lt;br/&gt;
@@ -74,12 +74,12 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private DeleteGroupsRequest(Set&amp;lt;String&amp;gt; groups, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DELETE_GROUPS, version);
         this.groups = groups;
     }

&lt;p&gt;     public DeleteGroupsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DELETE_GROUPS, version);&lt;br/&gt;
         Object[] groupsArray = struct.getArray(GROUPS_KEY_NAME);&lt;br/&gt;
         Set&amp;lt;String&amp;gt; groups = new HashSet&amp;lt;&amp;gt;(groupsArray.length);&lt;br/&gt;
         for (Object group : groupsArray)&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsRequest.java&lt;br/&gt;
index ad3db607886..7ea55534f69 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsRequest.java&lt;br/&gt;
@@ -104,7 +104,7 @@ public String toString() {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;     public DeleteRecordsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DELETE_RECORDS, version);&lt;br/&gt;
         partitionOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object topicStructObj : struct.getArray(TOPICS_KEY_NAME)) {&lt;br/&gt;
             Struct topicStruct = (Struct) topicStructObj;&lt;br/&gt;
@@ -120,14 +120,14 @@ public DeleteRecordsRequest(Struct struct, short version) {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public DeleteRecordsRequest(int timeout, Map&amp;lt;TopicPartition, Long&amp;gt; partitionOffsets, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DELETE_RECORDS, version);
         this.timeout = timeout;
         this.partitionOffsets = partitionOffsets;
     }
&lt;p&gt;     @Override&lt;br/&gt;
     protected Struct toStruct() {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.DELETE_RECORDS.requestSchema(version()));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Long&amp;gt;&amp;gt; offsetsByTopic = CollectionUtils.groupDataByTopic(partitionOffsets);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, Long&amp;gt;&amp;gt; offsetsByTopic = CollectionUtils.groupPartitionDataByTopic(partitionOffsets);&lt;br/&gt;
         struct.set(TIMEOUT_KEY_NAME, timeout);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicStructArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Long&amp;gt;&amp;gt; offsetsByTopicEntry : offsetsByTopic.entrySet()) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java&lt;br/&gt;
index 0b494ba8c30..311be1f73e6 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java&lt;br/&gt;
@@ -136,7 +136,7 @@ public DeleteRecordsResponse(int throttleTimeMs, Map&amp;lt;TopicPartition, PartitionRe&lt;br/&gt;
     protected Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.DELETE_RECORDS.responseSchema(version));&lt;br/&gt;
         struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, PartitionResponse&amp;gt;&amp;gt; responsesByTopic = CollectionUtils.groupDataByTopic(responses);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, PartitionResponse&amp;gt;&amp;gt; responsesByTopic = CollectionUtils.groupPartitionDataByTopic(responses);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicStructArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, PartitionResponse&amp;gt;&amp;gt; responsesByTopicEntry : responsesByTopic.entrySet()) {&lt;br/&gt;
             Struct topicStruct = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsRequest.java&lt;br/&gt;
index 87f14b4aa63..facb55e69f2 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsRequest.java&lt;br/&gt;
@@ -92,13 +92,13 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private DeleteTopicsRequest(Set&amp;lt;String&amp;gt; topics, Integer timeout, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DELETE_TOPICS, version);
         this.topics = topics;
         this.timeout = timeout;
     }

&lt;p&gt;     public DeleteTopicsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DELETE_TOPICS, version);&lt;br/&gt;
         Object[] topicsArray = struct.getArray(TOPICS_KEY_NAME);&lt;br/&gt;
         Set&amp;lt;String&amp;gt; topics = new HashSet&amp;lt;&amp;gt;(topicsArray.length);&lt;br/&gt;
         for (Object topic : topicsArray)&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java&lt;br/&gt;
index d2198397f6d..04bfee8d740 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java&lt;br/&gt;
@@ -86,14 +86,14 @@ public String toString() {&lt;br/&gt;
     private final AclBindingFilter filter;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     DescribeAclsRequest(AclBindingFilter filter, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DELETE_ACLS, version);
         this.filter = filter;
 
         validate(filter, version);
     }

&lt;p&gt;     public DescribeAclsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DELETE_ACLS, version);&lt;br/&gt;
         ResourcePatternFilter resourceFilter = RequestUtils.resourcePatternFilterFromStructFields(struct);&lt;br/&gt;
         AccessControlEntryFilter entryFilter = RequestUtils.aceFilterFromStructFields(struct);&lt;br/&gt;
         this.filter = new AclBindingFilter(resourceFilter, entryFilter);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsRequest.java&lt;br/&gt;
index 781cd451b98..0ee256ff534 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsRequest.java&lt;br/&gt;
@@ -100,13 +100,13 @@ public DescribeConfigsRequest build(short version) {&lt;br/&gt;
     private final boolean includeSynonyms;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public DescribeConfigsRequest(short version, Map&amp;lt;ConfigResource, Collection&amp;lt;String&amp;gt;&amp;gt; resourceToConfigNames, boolean includeSynonyms) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DESCRIBE_CONFIGS, version);
         this.resourceToConfigNames = Objects.requireNonNull(resourceToConfigNames, &quot;resourceToConfigNames&quot;);
         this.includeSynonyms = includeSynonyms;
     }

&lt;p&gt;     public DescribeConfigsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DESCRIBE_CONFIGS, version);&lt;br/&gt;
         Object[] resourcesArray = struct.getArray(RESOURCES_KEY_NAME);&lt;br/&gt;
         resourceToConfigNames = new HashMap&amp;lt;&amp;gt;(resourcesArray.length);&lt;br/&gt;
         for (Object resourceObj : resourcesArray) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenRequest.java&lt;br/&gt;
index 574bbcc6460..21e14608c36 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenRequest.java&lt;br/&gt;
@@ -69,12 +69,12 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private DescribeDelegationTokenRequest(short version, List&amp;lt;KafkaPrincipal&amp;gt; owners) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DESCRIBE_DELEGATION_TOKEN, version);
         this.owners = owners;
     }

&lt;p&gt;     public DescribeDelegationTokenRequest(Struct struct, short versionId) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(versionId);&lt;br/&gt;
+        super(ApiKeys.DESCRIBE_DELEGATION_TOKEN, versionId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Object[] ownerArray = struct.getArray(OWNER_KEY_NAME);&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsRequest.java&lt;br/&gt;
index 8ea4a8c2176..006af4ff79c 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsRequest.java&lt;br/&gt;
@@ -72,12 +72,12 @@ public String toString() {&lt;br/&gt;
     private final List&amp;lt;String&amp;gt; groupIds;&lt;/p&gt;

&lt;p&gt;     private DescribeGroupsRequest(List&amp;lt;String&amp;gt; groupIds, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DESCRIBE_GROUPS, version);
         this.groupIds = groupIds;
     }

&lt;p&gt;     public DescribeGroupsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DESCRIBE_GROUPS, version);&lt;br/&gt;
         this.groupIds = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object groupId : struct.getArray(GROUP_IDS_KEY_NAME))&lt;br/&gt;
             this.groupIds.add((String) groupId);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsRequest.java&lt;br/&gt;
index 3728991040e..e16cc187332 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsRequest.java&lt;br/&gt;
@@ -86,7 +86,7 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public DescribeLogDirsRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.DESCRIBE_LOG_DIRS, version);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (struct.getArray(TOPICS_KEY_NAME) == null) {&lt;br/&gt;
             topicPartitions = null;&lt;br/&gt;
@@ -105,7 +105,7 @@ public DescribeLogDirsRequest(Struct struct, short version) {&lt;/p&gt;

&lt;p&gt;     // topicPartitions == null indicates requesting all partitions, and an empty list indicates requesting no partitions.&lt;br/&gt;
     public DescribeLogDirsRequest(Set&amp;lt;TopicPartition&amp;gt; topicPartitions, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.DESCRIBE_LOG_DIRS, version);
         this.topicPartitions = topicPartitions;
     }

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java&lt;br/&gt;
index 41c26173e3e..ee6d95cfff8 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java&lt;br/&gt;
@@ -137,7 +137,7 @@ protected Struct toStruct(short version) {&lt;br/&gt;
             logDirStruct.set(ERROR_CODE, logDirInfo.error.code());&lt;br/&gt;
             logDirStruct.set(LOG_DIR_KEY_NAME, logDirInfosEntry.getKey());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, ReplicaInfo&amp;gt;&amp;gt; replicaInfosByTopic = CollectionUtils.groupDataByTopic(logDirInfo.replicaInfos);&lt;br/&gt;
+            Map&amp;lt;String, Map&amp;lt;Integer, ReplicaInfo&amp;gt;&amp;gt; replicaInfosByTopic = CollectionUtils.groupPartitionDataByTopic(logDirInfo.replicaInfos);&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; topicStructArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, ReplicaInfo&amp;gt;&amp;gt; replicaInfosByTopicEntry : replicaInfosByTopic.entrySet()) {&lt;br/&gt;
                 Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/EndTxnRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/EndTxnRequest.java&lt;br/&gt;
index 0ec22dc2831..833a7fe5aed 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/EndTxnRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/EndTxnRequest.java&lt;br/&gt;
@@ -89,7 +89,7 @@ public String toString() {&lt;br/&gt;
     private final TransactionResult result;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private EndTxnRequest(short version, String transactionalId, long producerId, short producerEpoch, TransactionResult result) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.END_TXN, version);
         this.transactionalId = transactionalId;
         this.producerId = producerId;
         this.producerEpoch = producerEpoch;
@@ -97,7 +97,7 @@ private EndTxnRequest(short version, String transactionalId, long producerId, sh
     }

&lt;p&gt;     public EndTxnRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.END_TXN, version);&lt;br/&gt;
         this.transactionalId = struct.get(TRANSACTIONAL_ID);&lt;br/&gt;
         this.producerId = struct.get(PRODUCER_ID);&lt;br/&gt;
         this.producerEpoch = struct.get(PRODUCER_EPOCH);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java b/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java&lt;br/&gt;
index ce938aad4f1..06dfef946d2 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java&lt;br/&gt;
@@ -25,7 +25,6 @@&lt;br/&gt;
 /**&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;The offset, fetched from a leader, for a particular partition.&lt;br/&gt;
  */&lt;br/&gt;
-&lt;br/&gt;
 public class EpochEndOffset {&lt;br/&gt;
     public static final long UNDEFINED_EPOCH_OFFSET = NO_PARTITION_LEADER_EPOCH;&lt;br/&gt;
     public static final int UNDEFINED_EPOCH = NO_PARTITION_LEADER_EPOCH;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenRequest.java&lt;br/&gt;
index 21edb5e2f30..5b996763cca 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenRequest.java&lt;br/&gt;
@@ -44,14 +44,14 @@&lt;br/&gt;
     private static final Schema TOKEN_EXPIRE_REQUEST_V1 = TOKEN_EXPIRE_REQUEST_V0;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private ExpireDelegationTokenRequest(short version, ByteBuffer hmac, long renewTimePeriod) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.EXPIRE_DELEGATION_TOKEN, version);
 
         this.hmac = hmac;
         this.expiryTimePeriod = renewTimePeriod;
     }

&lt;p&gt;     public ExpireDelegationTokenRequest(Struct struct, short versionId) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(versionId);&lt;br/&gt;
+        super(ApiKeys.EXPIRE_DELEGATION_TOKEN, versionId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         hmac = struct.getBytes(HMAC_KEY_NAME);&lt;br/&gt;
         expiryTimePeriod = struct.getLong(EXPIRY_TIME_PERIOD_KEY_NAME);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java&lt;br/&gt;
index e013f5ef0d2..32eb24d7de2 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -36,154 +35,166 @@&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.Objects;&lt;br/&gt;
+import java.util.Optional;&lt;/p&gt;

&lt;p&gt;+import static org.apache.kafka.common.protocol.CommonFields.CURRENT_LEADER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT32;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT8;&lt;br/&gt;
 import static org.apache.kafka.common.requests.FetchMetadata.FINAL_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.requests.FetchMetadata.INVALID_SESSION_ID;&lt;/p&gt;

&lt;p&gt; public class FetchRequest extends AbstractRequest {&lt;br/&gt;
     public static final int CONSUMER_REPLICA_ID = -1;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String REPLICA_ID_KEY_NAME = &quot;replica_id&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String MAX_WAIT_KEY_NAME = &quot;max_wait_time&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String MIN_BYTES_KEY_NAME = &quot;min_bytes&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String ISOLATION_LEVEL_KEY_NAME = &quot;isolation_level&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String FORGOTTEN_TOPICS_DATA = &quot;forgotten_topics_data&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// request and partition level name&lt;/li&gt;
	&lt;li&gt;private static final String MAX_BYTES_KEY_NAME = &quot;max_bytes&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// topic level field names&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// partition level field names&lt;/li&gt;
	&lt;li&gt;private static final String FETCH_OFFSET_KEY_NAME = &quot;fetch_offset&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String LOG_START_OFFSET_KEY_NAME = &quot;log_start_offset&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema FETCH_REQUEST_PARTITION_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(FETCH_OFFSET_KEY_NAME, INT64, &quot;Message offset.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_BYTES_KEY_NAME, INT32, &quot;Maximum bytes to fetch.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// FETCH_REQUEST_PARTITION_V5 added log_start_offset field - the earliest available offset of partition data that can be consumed.&lt;/li&gt;
	&lt;li&gt;private static final Schema FETCH_REQUEST_PARTITION_V5 = new Schema(&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;Topics to fetch in the order provided.&quot;);&lt;br/&gt;
+    private static final Field.ComplexArray FORGOTTEN_TOPICS = new Field.ComplexArray(&quot;forgotten_topics_data&quot;,&lt;br/&gt;
+            &quot;Topics to remove from the fetch session.&quot;);&lt;br/&gt;
+    private static final Field.Int32 MAX_BYTES = new Field.Int32(&quot;max_bytes&quot;,&lt;br/&gt;
+            &quot;Maximum bytes to accumulate in the response. Note that this is not an absolute maximum, &quot; +&lt;br/&gt;
+                    &quot;if the first message in the first non-empty partition of the fetch is larger than this &quot; +&lt;br/&gt;
+                    &quot;value, the message will still be returned to ensure that progress can be made.&quot;);&lt;br/&gt;
+    private static final Field.Int8 ISOLATION_LEVEL = new Field.Int8(&quot;isolation_level&quot;,&lt;br/&gt;
+            &quot;This setting controls the visibility of transactional records. Using READ_UNCOMMITTED &quot; +&lt;br/&gt;
+                    &quot;(isolation_level = 0) makes all records visible. With READ_COMMITTED (isolation_level = 1), &quot; +&lt;br/&gt;
+                    &quot;non-transactional and COMMITTED transactional records are visible. To be more concrete, &quot; +&lt;br/&gt;
+                    &quot;READ_COMMITTED returns all data from offsets smaller than the current LSO (last stable offset), &quot; +&lt;br/&gt;
+                    &quot;and enables the inclusion of the list of aborted transactions in the result, which allows &quot; +&lt;br/&gt;
+                    &quot;consumers to discard ABORTED transactional records&quot;);&lt;br/&gt;
+    private static final Field.Int32 SESSION_ID = new Field.Int32(&quot;session_id&quot;, &quot;The fetch session ID&quot;);&lt;br/&gt;
+    private static final Field.Int32 SESSION_EPOCH = new Field.Int32(&quot;session_epoch&quot;, &quot;The fetch session epoch&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;Partitions to fetch.&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    // partition level fields&lt;br/&gt;
+    private static final Field.Int32 REPLICA_ID = new Field.Int32(&quot;replica_id&quot;,&lt;br/&gt;
+            &quot;Broker id of the follower. For normal consumers, use -1.&quot;);&lt;br/&gt;
+    private static final Field.Int64 FETCH_OFFSET = new Field.Int64(&quot;fetch_offset&quot;, &quot;Message offset.&quot;);&lt;br/&gt;
+    private static final Field.Int32 PARTITION_MAX_BYTES = new Field.Int32(&quot;partition_max_bytes&quot;,&lt;br/&gt;
+            &quot;Maximum bytes to fetch.&quot;);&lt;br/&gt;
+    private static final Field.Int32 MAX_WAIT_TIME = new Field.Int32(&quot;max_wait_time&quot;,&lt;br/&gt;
+            &quot;Maximum time in ms to wait for the response.&quot;);&lt;br/&gt;
+    private static final Field.Int32 MIN_BYTES = new Field.Int32(&quot;min_bytes&quot;,&lt;br/&gt;
+            &quot;Minimum bytes to accumulate in the response.&quot;);&lt;br/&gt;
+    private static final Field.Int64 LOG_START_OFFSET = new Field.Int64(&quot;log_start_offset&quot;,&lt;br/&gt;
+            &quot;Earliest available offset of the follower replica. &quot; +&lt;br/&gt;
+                    &quot;The field is only used when request is sent by follower. &quot;);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(FETCH_OFFSET_KEY_NAME, INT64, &quot;Message offset.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(LOG_START_OFFSET_KEY_NAME, INT64, &quot;Earliest available offset of the follower replica. &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;The field is only used when request is sent by follower. &quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_BYTES_KEY_NAME, INT32, &quot;Maximum bytes to fetch.&quot;));&lt;br/&gt;
+            FETCH_OFFSET,&lt;br/&gt;
+            PARTITION_MAX_BYTES);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema FETCH_REQUEST_TOPIC_V0 = new Schema(&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(FETCH_REQUEST_PARTITION_V0), &quot;Partitions to fetch.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema FETCH_REQUEST_TOPIC_V5 = new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(FETCH_REQUEST_PARTITION_V5), &quot;Partitions to fetch.&quot;));&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema FETCH_REQUEST_V0 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_WAIT_KEY_NAME, INT32, &quot;Maximum time in ms to wait for the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MIN_BYTES_KEY_NAME, INT32, &quot;Minimum bytes to accumulate in the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(FETCH_REQUEST_TOPIC_V0), &quot;Topics to fetch.&quot;));&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            MAX_WAIT_TIME,&lt;br/&gt;
+            MIN_BYTES,&lt;br/&gt;
+            TOPICS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // The V1 Fetch Request body is the same as V0.&lt;br/&gt;
     // Only the version number is incremented to indicate a newer client&lt;br/&gt;
     private static final Schema FETCH_REQUEST_V1 = FETCH_REQUEST_V0;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// The V2 Fetch Request body is the same as V1.&lt;/li&gt;
	&lt;li&gt;// Only the version number is incremented to indicate the client support message format V1 which uses&lt;/li&gt;
	&lt;li&gt;// relative offset and has timestamp.&lt;br/&gt;
+&lt;br/&gt;
+    // V2 bumped to indicate the client support message format V1 which uses relative offset and has timestamp.&lt;br/&gt;
     private static final Schema FETCH_REQUEST_V2 = FETCH_REQUEST_V1;&lt;/li&gt;
	&lt;li&gt;// Fetch Request V3 added top level max_bytes field - the total size of partition data to accumulate in response.&lt;br/&gt;
+&lt;br/&gt;
+    // V3 added top level max_bytes field - the total size of partition data to accumulate in response.&lt;br/&gt;
     // The partition ordering is now relevant - partitions will be processed in order they appear in request.&lt;br/&gt;
     private static final Schema FETCH_REQUEST_V3 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_WAIT_KEY_NAME, INT32, &quot;Maximum time in ms to wait for the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MIN_BYTES_KEY_NAME, INT32, &quot;Minimum bytes to accumulate in the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_BYTES_KEY_NAME, INT32, &quot;Maximum bytes to accumulate in the response. Note that this is not an absolute maximum, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;if the first message in the first non-empty partition of the fetch is larger than this &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;value, the message will still be returned to ensure that progress can be made.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(FETCH_REQUEST_TOPIC_V0), &quot;Topics to fetch in the order provided.&quot;));&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            MAX_WAIT_TIME,&lt;br/&gt;
+            MIN_BYTES,&lt;br/&gt;
+            MAX_BYTES,&lt;br/&gt;
+            TOPICS_V0);&lt;br/&gt;
+&lt;br/&gt;
+    // V4 adds the fetch isolation level and exposes magic v2 (via the response).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// The V4 Fetch Request adds the fetch isolation level and exposes magic v2 (via the response).&lt;br/&gt;
     private static final Schema FETCH_REQUEST_V4 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_WAIT_KEY_NAME, INT32, &quot;Maximum time in ms to wait for the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MIN_BYTES_KEY_NAME, INT32, &quot;Minimum bytes to accumulate in the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_BYTES_KEY_NAME, INT32, &quot;Maximum bytes to accumulate in the response. Note that this is not an absolute maximum, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;if the first message in the first non-empty partition of the fetch is larger than this &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;value, the message will still be returned to ensure that progress can be made.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(ISOLATION_LEVEL_KEY_NAME, INT8, &quot;This setting controls the visibility of transactional records. Using READ_UNCOMMITTED &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;(isolation_level = 0) makes all records visible. With READ_COMMITTED (isolation_level = 1), &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;non-transactional and COMMITTED transactional records are visible. To be more concrete, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;READ_COMMITTED returns all data from offsets smaller than the current LSO (last stable offset), &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;and enables the inclusion of the list of aborted transactions in the result, which allows &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;consumers to discard ABORTED transactional records&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(FETCH_REQUEST_TOPIC_V0), &quot;Topics to fetch in the order provided.&quot;));&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            MAX_WAIT_TIME,&lt;br/&gt;
+            MIN_BYTES,&lt;br/&gt;
+            MAX_BYTES,&lt;br/&gt;
+            ISOLATION_LEVEL,&lt;br/&gt;
+            TOPICS_V0);&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+    // V5 added log_start_offset field - the earliest available offset of partition data that can be consumed.&lt;br/&gt;
+    private static final Field PARTITIONS_V5 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            FETCH_OFFSET,&lt;br/&gt;
+            LOG_START_OFFSET,&lt;br/&gt;
+            PARTITION_MAX_BYTES);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V5 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V5);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// FETCH_REQUEST_V5 added a per-partition log_start_offset field - the earliest available offset of partition data that can be consumed.&lt;br/&gt;
     private static final Schema FETCH_REQUEST_V5 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_WAIT_KEY_NAME, INT32, &quot;Maximum time in ms to wait for the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MIN_BYTES_KEY_NAME, INT32, &quot;Minimum bytes to accumulate in the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_BYTES_KEY_NAME, INT32, &quot;Maximum bytes to accumulate in the response. Note that this is not an absolute maximum, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;if the first message in the first non-empty partition of the fetch is larger than this &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;value, the message will still be returned to ensure that progress can be made.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(ISOLATION_LEVEL_KEY_NAME, INT8, &quot;This setting controls the visibility of transactional records. Using READ_UNCOMMITTED &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;(isolation_level = 0) makes all records visible. With READ_COMMITTED (isolation_level = 1), &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;non-transactional and COMMITTED transactional records are visible. To be more concrete, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;READ_COMMITTED returns all data from offsets smaller than the current LSO (last stable offset), &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;and enables the inclusion of the list of aborted transactions in the result, which allows &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;consumers to discard ABORTED transactional records&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(FETCH_REQUEST_TOPIC_V5), &quot;Topics to fetch in the order provided.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The body of FETCH_REQUEST_V6 is the same as FETCH_REQUEST_V5.&lt;/li&gt;
	&lt;li&gt;* The version number is bumped up to indicate that the client supports KafkaStorageException.&lt;/li&gt;
	&lt;li&gt;* The KafkaStorageException will be translated to NotLeaderForPartitionException in the response if version &amp;lt;= 5&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            MAX_WAIT_TIME,&lt;br/&gt;
+            MIN_BYTES,&lt;br/&gt;
+            MAX_BYTES,&lt;br/&gt;
+            ISOLATION_LEVEL,&lt;br/&gt;
+            TOPICS_V5);&lt;br/&gt;
+&lt;br/&gt;
+    // V6 bumped up to indicate that the client supports KafkaStorageException. The KafkaStorageException will be&lt;br/&gt;
+    // translated to NotLeaderForPartitionException in the response if version &amp;lt;= 5&lt;br/&gt;
     private static final Schema FETCH_REQUEST_V6 = FETCH_REQUEST_V5;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// FETCH_REQUEST_V7 added incremental fetch requests.&lt;/li&gt;
	&lt;li&gt;public static final Field.Int32 SESSION_ID = new Field.Int32(&quot;session_id&quot;, &quot;The fetch session ID&quot;);&lt;/li&gt;
	&lt;li&gt;public static final Field.Int32 EPOCH = new Field.Int32(&quot;epoch&quot;, &quot;The fetch epoch&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema FORGOTTEN_TOPIC_DATA = new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(Type.INT32),&lt;/li&gt;
	&lt;li&gt;&quot;Partitions to remove from the fetch session.&quot;));&lt;br/&gt;
+    // V7 added incremental fetch requests.&lt;br/&gt;
+    private static final Field.Array FORGOTTEN_PARTITIONS = new Field.Array(&quot;partitions&quot;, Type.INT32,&lt;br/&gt;
+            &quot;Partitions to remove from the fetch session.&quot;);&lt;br/&gt;
+    private static final Field FORGOTTEN_TOPIC_DATA_V7 = FORGOTTEN_TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            FORGOTTEN_PARTITIONS);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema FETCH_REQUEST_V7 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_WAIT_KEY_NAME, INT32, &quot;Maximum time in ms to wait for the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MIN_BYTES_KEY_NAME, INT32, &quot;Minimum bytes to accumulate in the response.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_BYTES_KEY_NAME, INT32, &quot;Maximum bytes to accumulate in the response. Note that this is not an absolute maximum, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;if the first message in the first non-empty partition of the fetch is larger than this &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;value, the message will still be returned to ensure that progress can be made.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(ISOLATION_LEVEL_KEY_NAME, INT8, &quot;This setting controls the visibility of transactional records. Using READ_UNCOMMITTED &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;(isolation_level = 0) makes all records visible. With READ_COMMITTED (isolation_level = 1), &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;non-transactional and COMMITTED transactional records are visible. To be more concrete, &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;READ_COMMITTED returns all data from offsets smaller than the current LSO (last stable offset), &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;and enables the inclusion of the list of aborted transactions in the result, which allows &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;consumers to discard ABORTED transactional records&quot;),&lt;/li&gt;
	&lt;li&gt;SESSION_ID,&lt;/li&gt;
	&lt;li&gt;EPOCH,&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(FETCH_REQUEST_TOPIC_V5), &quot;Topics to fetch in the order provided.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(FORGOTTEN_TOPICS_DATA, new ArrayOf(FORGOTTEN_TOPIC_DATA), &quot;Topics to remove from the fetch session.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            MAX_WAIT_TIME,&lt;br/&gt;
+            MIN_BYTES,&lt;br/&gt;
+            MAX_BYTES,&lt;br/&gt;
+            ISOLATION_LEVEL,&lt;br/&gt;
+            SESSION_ID,&lt;br/&gt;
+            SESSION_EPOCH,&lt;br/&gt;
+            TOPICS_V5,&lt;br/&gt;
+            FORGOTTEN_TOPIC_DATA_V7);&lt;br/&gt;
+&lt;br/&gt;
+    // V8 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema FETCH_REQUEST_V8 = FETCH_REQUEST_V7;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V9 adds the current leader epoch (see KIP-320)&lt;br/&gt;
+    private static final Field FETCH_REQUEST_PARTITION_V9 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            CURRENT_LEADER_EPOCH,&lt;br/&gt;
+            FETCH_OFFSET,&lt;br/&gt;
+            LOG_START_OFFSET,&lt;br/&gt;
+            PARTITION_MAX_BYTES);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field FETCH_REQUEST_TOPIC_V9 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            FETCH_REQUEST_PARTITION_V9);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Schema FETCH_REQUEST_V9 = new Schema(&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            MAX_WAIT_TIME,&lt;br/&gt;
+            MIN_BYTES,&lt;br/&gt;
+            MAX_BYTES,&lt;br/&gt;
+            ISOLATION_LEVEL,&lt;br/&gt;
+            SESSION_ID,&lt;br/&gt;
+            SESSION_EPOCH,&lt;br/&gt;
+            FETCH_REQUEST_TOPIC_V9,&lt;br/&gt;
+            FORGOTTEN_TOPIC_DATA_V7);&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[]&lt;/p&gt;
{FETCH_REQUEST_V0, FETCH_REQUEST_V1, FETCH_REQUEST_V2, FETCH_REQUEST_V3, FETCH_REQUEST_V4,
-            FETCH_REQUEST_V5, FETCH_REQUEST_V6, FETCH_REQUEST_V7, FETCH_REQUEST_V8}
&lt;p&gt;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;};&lt;br/&gt;
+            FETCH_REQUEST_V5, FETCH_REQUEST_V6, FETCH_REQUEST_V7, FETCH_REQUEST_V8, FETCH_REQUEST_V9};&lt;br/&gt;
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // default values for older versions where a request level limit did not exist&lt;br/&gt;
     public static final int DEFAULT_RESPONSE_MAX_BYTES = Integer.MAX_VALUE;&lt;br/&gt;
@@ -207,21 +218,27 @@&lt;br/&gt;
         public final long fetchOffset;&lt;br/&gt;
         public final long logStartOffset;&lt;br/&gt;
         public final int maxBytes;&lt;br/&gt;
+        public final Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public PartitionData(long fetchOffset, long logStartOffset, int maxBytes) {&lt;br/&gt;
+        public PartitionData(long fetchOffset, long logStartOffset, int maxBytes, Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch) 
{
             this.fetchOffset = fetchOffset;
             this.logStartOffset = logStartOffset;
             this.maxBytes = maxBytes;
+            this.currentLeaderEpoch = currentLeaderEpoch;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public String toString() &lt;/p&gt;
{
-            return &quot;(offset=&quot; + fetchOffset + &quot;, logStartOffset=&quot; + logStartOffset + &quot;, maxBytes=&quot; + maxBytes + &quot;)&quot;;
+            return &quot;(offset=&quot; + fetchOffset +
+                    &quot;, logStartOffset=&quot; + logStartOffset +
+                    &quot;, maxBytes=&quot; + maxBytes +
+                    &quot;, currentLeaderEpoch=&quot; + currentLeaderEpoch +
+                    &quot;)&quot;;
         }

&lt;p&gt;         @Override&lt;br/&gt;
         public int hashCode() &lt;/p&gt;
{
-            return Objects.hash(fetchOffset, logStartOffset, maxBytes);
+            return Objects.hash(fetchOffset, logStartOffset, maxBytes, currentLeaderEpoch);
         }

&lt;p&gt;         @Override&lt;br/&gt;
@@ -231,7 +248,8 @@ public boolean equals(Object o) &lt;/p&gt;
{
             PartitionData that = (PartitionData) o;
             return Objects.equals(fetchOffset, that.fetchOffset) &amp;amp;&amp;amp;
                 Objects.equals(logStartOffset, that.logStartOffset) &amp;amp;&amp;amp;
-                Objects.equals(maxBytes, that.maxBytes);
+                Objects.equals(maxBytes, that.maxBytes) &amp;amp;&amp;amp;
+                Objects.equals(currentLeaderEpoch, that.currentLeaderEpoch);
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;@@ -346,7 +364,7 @@ public String toString() {&lt;br/&gt;
     private FetchRequest(short version, int replicaId, int maxWait, int minBytes, int maxBytes,&lt;br/&gt;
                          Map&amp;lt;TopicPartition, PartitionData&amp;gt; fetchData, IsolationLevel isolationLevel,&lt;br/&gt;
                          List&amp;lt;TopicPartition&amp;gt; toForget, FetchMetadata metadata) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.FETCH, version);
         this.replicaId = replicaId;
         this.maxWait = maxWait;
         this.minBytes = minBytes;
@@ -358,44 +376,44 @@ private FetchRequest(short version, int replicaId, int maxWait, int minBytes, in
     }

&lt;p&gt;     public FetchRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;/li&gt;
	&lt;li&gt;replicaId = struct.getInt(REPLICA_ID_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;maxWait = struct.getInt(MAX_WAIT_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;minBytes = struct.getInt(MIN_BYTES_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(MAX_BYTES_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;maxBytes = struct.getInt(MAX_BYTES_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;maxBytes = DEFAULT_RESPONSE_MAX_BYTES;&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(ISOLATION_LEVEL_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;isolationLevel = IsolationLevel.forId(struct.getByte(ISOLATION_LEVEL_KEY_NAME));&lt;br/&gt;
+        super(ApiKeys.FETCH, version);&lt;br/&gt;
+        replicaId = struct.get(REPLICA_ID);&lt;br/&gt;
+        maxWait = struct.get(MAX_WAIT_TIME);&lt;br/&gt;
+        minBytes = struct.get(MIN_BYTES);&lt;br/&gt;
+        maxBytes = struct.getOrElse(MAX_BYTES, DEFAULT_RESPONSE_MAX_BYTES);&lt;br/&gt;
+&lt;br/&gt;
+        if (struct.hasField(ISOLATION_LEVEL))&lt;br/&gt;
+            isolationLevel = IsolationLevel.forId(struct.get(ISOLATION_LEVEL));&lt;br/&gt;
         else&lt;br/&gt;
             isolationLevel = IsolationLevel.READ_UNCOMMITTED;&lt;br/&gt;
         toForget = new ArrayList&amp;lt;&amp;gt;(0);&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(FORGOTTEN_TOPICS_DATA)) {&lt;/li&gt;
	&lt;li&gt;for (Object forgottenTopicObj : struct.getArray(FORGOTTEN_TOPICS_DATA)) {&lt;br/&gt;
+        if (struct.hasField(FORGOTTEN_TOPICS)) {&lt;br/&gt;
+            for (Object forgottenTopicObj : struct.get(FORGOTTEN_TOPICS)) {&lt;br/&gt;
                 Struct forgottenTopic = (Struct) forgottenTopicObj;&lt;br/&gt;
                 String topicName = forgottenTopic.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partObj : forgottenTopic.getArray(PARTITIONS_KEY_NAME)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                for (Object partObj }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;         }&lt;br/&gt;
         metadata = new FetchMetadata(struct.getOrElse(SESSION_ID, INVALID_SESSION_ID),&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;struct.getOrElse(EPOCH, FINAL_EPOCH));&lt;br/&gt;
+            struct.getOrElse(SESSION_EPOCH, FINAL_EPOCH));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         fetchData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Object topicResponseObj : struct.getArray(TOPICS_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicResponseObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicResponse = (Struct) topicResponseObj;&lt;br/&gt;
             String topic = topicResponse.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (Object partitionResponseObj }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -410,14 +428,14 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {&lt;br/&gt;
         // may not be any partitions at all in the response.  For this reason, the top-level error code&lt;br/&gt;
         // is essential for them.&lt;br/&gt;
         Errors error = Errors.forException(e);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, PartitionData&amp;gt; entry : fetchData.entrySet()) 
{
-            FetchResponse.PartitionData partitionResponse = new FetchResponse.PartitionData(error,
+            FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt; partitionResponse = new FetchResponse.PartitionData&amp;lt;&amp;gt;(error,
                 FetchResponse.INVALID_HIGHWATERMARK, FetchResponse.INVALID_LAST_STABLE_OFFSET,
                 FetchResponse.INVALID_LOG_START_OFFSET, null, MemoryRecords.EMPTY);
             responseData.put(entry.getKey(), partitionResponse);
         }&lt;/li&gt;
	&lt;li&gt;return new FetchResponse(error, responseData, throttleTimeMs, metadata.sessionId());&lt;br/&gt;
+        return new FetchResponse&amp;lt;&amp;gt;(error, responseData, throttleTimeMs, metadata.sessionId());&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public int replicaId() {&lt;br/&gt;
@@ -466,53 +484,47 @@ protected Struct toStruct() {&lt;br/&gt;
         List&amp;lt;TopicAndPartitionData&amp;lt;PartitionData&amp;gt;&amp;gt; topicsData =&lt;br/&gt;
             TopicAndPartitionData.batchByTopic(fetchData.entrySet().iterator());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;struct.set(REPLICA_ID_KEY_NAME, replicaId);&lt;/li&gt;
	&lt;li&gt;struct.set(MAX_WAIT_KEY_NAME, maxWait);&lt;/li&gt;
	&lt;li&gt;struct.set(MIN_BYTES_KEY_NAME, minBytes);&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(MAX_BYTES_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;struct.set(MAX_BYTES_KEY_NAME, maxBytes);&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(ISOLATION_LEVEL_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;struct.set(ISOLATION_LEVEL_KEY_NAME, isolationLevel.id());&lt;br/&gt;
+        struct.set(REPLICA_ID, replicaId);&lt;br/&gt;
+        struct.set(MAX_WAIT_TIME, maxWait);&lt;br/&gt;
+        struct.set(MIN_BYTES, minBytes);&lt;br/&gt;
+        struct.setIfExists(MAX_BYTES, maxBytes);&lt;br/&gt;
+        struct.setIfExists(ISOLATION_LEVEL, isolationLevel.id());&lt;br/&gt;
         struct.setIfExists(SESSION_ID, metadata.sessionId());&lt;/li&gt;
	&lt;li&gt;struct.setIfExists(EPOCH, metadata.epoch());&lt;br/&gt;
+        struct.setIfExists(SESSION_EPOCH, metadata.epoch());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (TopicAndPartitionData&amp;lt;PartitionData&amp;gt; topicEntry : topicsData) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Struct topicData = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+            Struct topicData = struct.instance(TOPICS);&lt;br/&gt;
             topicData.set(TOPIC_NAME, topicEntry.topic);&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, PartitionData&amp;gt; partitionEntry : topicEntry.partitions.entrySet()) 
{
                 PartitionData fetchPartitionData = partitionEntry.getValue();
-                Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);
+                Struct partitionData = topicData.instance(PARTITIONS);
                 partitionData.set(PARTITION_ID, partitionEntry.getKey());
-                partitionData.set(FETCH_OFFSET_KEY_NAME, fetchPartitionData.fetchOffset);
-                if (partitionData.hasField(LOG_START_OFFSET_KEY_NAME))
-                    partitionData.set(LOG_START_OFFSET_KEY_NAME, fetchPartitionData.logStartOffset);
-                partitionData.set(MAX_BYTES_KEY_NAME, fetchPartitionData.maxBytes);
+                partitionData.set(FETCH_OFFSET, fetchPartitionData.fetchOffset);
+                partitionData.set(PARTITION_MAX_BYTES, fetchPartitionData.maxBytes);
+                partitionData.setIfExists(LOG_START_OFFSET, fetchPartitionData.logStartOffset);
+                RequestUtils.setLeaderEpochIfExists(partitionData, CURRENT_LEADER_EPOCH, fetchPartitionData.currentLeaderEpoch);
                 partitionArray.add(partitionData);
             }&lt;/li&gt;
	&lt;li&gt;topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());&lt;br/&gt;
+            topicData.set(PARTITIONS, partitionArray.toArray());&lt;br/&gt;
             topicArray.add(topicData);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;struct.set(TOPICS_KEY_NAME, topicArray.toArray());&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(FORGOTTEN_TOPICS_DATA)) {&lt;br/&gt;
+        struct.set(TOPICS, topicArray.toArray());&lt;br/&gt;
+        if (struct.hasField(FORGOTTEN_TOPICS)) {&lt;br/&gt;
             Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicsToPartitions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
             for (TopicPartition part : toForget) {&lt;/li&gt;
	&lt;li&gt;List&amp;lt;Integer&amp;gt; partitions = topicsToPartitions.get(part.topic());&lt;/li&gt;
	&lt;li&gt;if (partitions == null) 
{
-                    partitions = new ArrayList&amp;lt;&amp;gt;();
-                    topicsToPartitions.put(part.topic(), partitions);
-                }
&lt;p&gt;+                List&amp;lt;Integer&amp;gt; partitions = topicsToPartitions.computeIfAbsent(part.topic(), topic -&amp;gt; new ArrayList&amp;lt;&amp;gt;());&lt;br/&gt;
                 partitions.add(part.partition());&lt;br/&gt;
             }&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; toForgetStructs = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; entry : topicsToPartitions.entrySet()) &lt;/p&gt;
{
-                Struct toForgetStruct = struct.instance(FORGOTTEN_TOPICS_DATA);
+                Struct toForgetStruct = struct.instance(FORGOTTEN_TOPICS);
                 toForgetStruct.set(TOPIC_NAME, entry.getKey());
-                toForgetStruct.set(PARTITIONS_KEY_NAME, entry.getValue().toArray());
+                toForgetStruct.set(FORGOTTEN_PARTITIONS, entry.getValue().toArray());
                 toForgetStructs.add(toForgetStruct);
             }&lt;/li&gt;
	&lt;li&gt;struct.set(FORGOTTEN_TOPICS_DATA, toForgetStructs.toArray());&lt;br/&gt;
+            struct.set(FORGOTTEN_TOPICS, toForgetStructs.toArray());&lt;br/&gt;
         }&lt;br/&gt;
         return struct;&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java&lt;br/&gt;
index 16e33965e9e..2e0eaf2fc28 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java&lt;br/&gt;
@@ -18,7 +18,6 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.network.ByteBufferSend;&lt;br/&gt;
-import org.apache.kafka.common.record.MultiRecordsSend;&lt;br/&gt;
 import org.apache.kafka.common.network.Send;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
@@ -28,6 +27,7 @@&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
 import org.apache.kafka.common.record.BaseRecords;&lt;br/&gt;
 import org.apache.kafka.common.record.MemoryRecords;&lt;br/&gt;
+import org.apache.kafka.common.record.MultiRecordsSend;&lt;/p&gt;

&lt;p&gt; import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.ArrayDeque;&lt;br/&gt;
@@ -43,13 +43,20 @@&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.types.Type.RECORDS;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.types.Type.STRING;&lt;br/&gt;
 import static org.apache.kafka.common.requests.FetchMetadata.INVALID_SESSION_ID;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This wrapper supports all versions of the Fetch API&lt;br/&gt;
+ *&lt;br/&gt;
+ * Possible error codes:&lt;br/&gt;
+ *&lt;br/&gt;
+ *  OFFSET_OUT_OF_RANGE (1)&lt;br/&gt;
+ *  UNKNOWN_TOPIC_OR_PARTITION (3)&lt;br/&gt;
+ *  NOT_LEADER_FOR_PARTITION (6)&lt;br/&gt;
+ *  REPLICA_NOT_AVAILABLE (9)&lt;br/&gt;
+ *  UNKNOWN (-1)&lt;br/&gt;
  */&lt;br/&gt;
 public class FetchResponse&amp;lt;T extends BaseRecords&amp;gt; extends AbstractResponse {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -58,22 +65,20 @@&lt;br/&gt;
     // topic level field names&lt;br/&gt;
     private static final String PARTITIONS_KEY_NAME = &quot;partition_responses&quot;;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// partition level field names&lt;br/&gt;
+    // partition level fields&lt;br/&gt;
+    private static final Field.Int64 HIGH_WATERMARK = new Field.Int64(&quot;high_watermark&quot;,&lt;br/&gt;
+            &quot;Last committed offset.&quot;);&lt;br/&gt;
+    private static final Field.Int64 LOG_START_OFFSET = new Field.Int64(&quot;log_start_offset&quot;,&lt;br/&gt;
+            &quot;Earliest available offset.&quot;);&lt;br/&gt;
+&lt;br/&gt;
     private static final String PARTITION_HEADER_KEY_NAME = &quot;partition_header&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String HIGH_WATERMARK_KEY_NAME = &quot;high_watermark&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String LAST_STABLE_OFFSET_KEY_NAME = &quot;last_stable_offset&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String LOG_START_OFFSET_KEY_NAME = &quot;log_start_offset&quot;;&lt;br/&gt;
     private static final String ABORTED_TRANSACTIONS_KEY_NAME = &quot;aborted_transactions&quot;;&lt;br/&gt;
     private static final String RECORD_SET_KEY_NAME = &quot;record_set&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// aborted transaction field names&lt;/li&gt;
	&lt;li&gt;private static final String PRODUCER_ID_KEY_NAME = &quot;producer_id&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String FIRST_OFFSET_KEY_NAME = &quot;first_offset&quot;;&lt;br/&gt;
-&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_PARTITION_HEADER_V0 = new Schema(&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             ERROR_CODE,&lt;/li&gt;
	&lt;li&gt;new Field(HIGH_WATERMARK_KEY_NAME, INT64, &quot;Last committed offset.&quot;));&lt;br/&gt;
+            HIGH_WATERMARK);&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_PARTITION_V0 = new Schema(&lt;br/&gt;
             new Field(PARTITION_HEADER_KEY_NAME, FETCH_RESPONSE_PARTITION_HEADER_V0),&lt;br/&gt;
             new Field(RECORD_SET_KEY_NAME, RECORDS));&lt;br/&gt;
@@ -85,42 +90,47 @@&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_V0 = new Schema(&lt;br/&gt;
             new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V0)));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V1 bumped for the addition of the throttle time&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_V1 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;br/&gt;
             new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V0)));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Even though fetch response v2 has the same protocol as v1, the record set in the response is different. In v1,&lt;/li&gt;
	&lt;li&gt;// record set only includes messages of v0 (magic byte 0). In v2, record set can include messages of v0 and v1&lt;/li&gt;
	&lt;li&gt;// (magic byte 0 and 1). For details, see Records, RecordBatch and Record.&lt;br/&gt;
+&lt;br/&gt;
+    // V2 bumped to indicate the client support message format V1 which uses relative offset and has timestamp.&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_V2 = FETCH_RESPONSE_V1;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// The partition ordering is now relevant - partitions will be processed in order they appear in request.&lt;br/&gt;
+    // V3 bumped for addition of top-levl max_bytes field and to indicate that partition ordering is relevant&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_V3 = FETCH_RESPONSE_V2;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// The v4 Fetch Response adds features for transactional consumption (the aborted transaction list and the&lt;br/&gt;
+    // V4 adds features for transactional consumption (the aborted transaction list and the&lt;br/&gt;
     // last stable offset). It also exposes messages with magic v2 (along with older formats).&lt;/li&gt;
	&lt;li&gt;private static final Schema FETCH_RESPONSE_ABORTED_TRANSACTION_V4 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(PRODUCER_ID_KEY_NAME, INT64, &quot;The producer id associated with the aborted transactions&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(FIRST_OFFSET_KEY_NAME, INT64, &quot;The first offset in the aborted transaction&quot;));&lt;br/&gt;
+    // aborted transaction field names&lt;br/&gt;
+    private static final Field.Int64 LAST_STABLE_OFFSET = new Field.Int64(&quot;last_stable_offset&quot;,&lt;br/&gt;
+            &quot;The last stable offset (or LSO) of the partition. This is the last offset such that the state &quot; +&lt;br/&gt;
+                    &quot;of all transactional records prior to this offset have been decided (ABORTED or COMMITTED)&quot;);&lt;br/&gt;
+    private static final Field.Int64 PRODUCER_ID = new Field.Int64(&quot;producer_id&quot;,&lt;br/&gt;
+            &quot;The producer id associated with the aborted transactions&quot;);&lt;br/&gt;
+    private static final Field.Int64 FIRST_OFFSET = new Field.Int64(&quot;first_offset&quot;,&lt;br/&gt;
+            &quot;The first offset in the aborted transaction&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema FETCH_RESPONSE_ABORTED_TRANSACTION_V5 = FETCH_RESPONSE_ABORTED_TRANSACTION_V4;&lt;br/&gt;
+    private static final Schema FETCH_RESPONSE_ABORTED_TRANSACTION_V4 = new Schema(&lt;br/&gt;
+            PRODUCER_ID,&lt;br/&gt;
+            FIRST_OFFSET);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema FETCH_RESPONSE_PARTITION_HEADER_V4 = new Schema(&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             ERROR_CODE,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(HIGH_WATERMARK_KEY_NAME, INT64, &quot;Last committed offset.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(LAST_STABLE_OFFSET_KEY_NAME, INT64, &quot;The last stable offset (or LSO) of the partition. This is the last offset such that the state &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;of all transactional records prior to this offset have been decided (ABORTED or COMMITTED)&quot;),&lt;br/&gt;
+            HIGH_WATERMARK,&lt;br/&gt;
+            LAST_STABLE_OFFSET,&lt;br/&gt;
             new Field(ABORTED_TRANSACTIONS_KEY_NAME, ArrayOf.nullable(FETCH_RESPONSE_ABORTED_TRANSACTION_V4)));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// FETCH_RESPONSE_PARTITION_HEADER_V5 added log_start_offset field - the earliest available offset of partition data that can be consumed.&lt;br/&gt;
+    // V5 added log_start_offset field - the earliest available offset of partition data that can be consumed.&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_PARTITION_HEADER_V5 = new Schema(&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             ERROR_CODE,&lt;/li&gt;
	&lt;li&gt;new Field(HIGH_WATERMARK_KEY_NAME, INT64, &quot;Last committed offset.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(LAST_STABLE_OFFSET_KEY_NAME, INT64, &quot;The last stable offset (or LSO) of the partition. This is the last offset such that the state &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;of all transactional records prior to this offset have been decided (ABORTED or COMMITTED)&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(LOG_START_OFFSET_KEY_NAME, INT64, &quot;Earliest available offset.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(ABORTED_TRANSACTIONS_KEY_NAME, ArrayOf.nullable(FETCH_RESPONSE_ABORTED_TRANSACTION_V5)));&lt;br/&gt;
+            HIGH_WATERMARK,&lt;br/&gt;
+            LAST_STABLE_OFFSET,&lt;br/&gt;
+            LOG_START_OFFSET,&lt;br/&gt;
+            new Field(ABORTED_TRANSACTIONS_KEY_NAME, ArrayOf.nullable(FETCH_RESPONSE_ABORTED_TRANSACTION_V4)));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema FETCH_RESPONSE_PARTITION_V4 = new Schema(&lt;br/&gt;
             new Field(PARTITION_HEADER_KEY_NAME, FETCH_RESPONSE_PARTITION_HEADER_V4),&lt;br/&gt;
@@ -146,15 +156,12 @@&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;br/&gt;
             new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V5)));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The body of FETCH_RESPONSE_V6 is the same as FETCH_RESPONSE_V5.&lt;/li&gt;
	&lt;li&gt;* The version number is bumped up to indicate that the client supports KafkaStorageException.&lt;/li&gt;
	&lt;li&gt;* The KafkaStorageException will be translated to NotLeaderForPartitionException in the response if version &amp;lt;= 5&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V6 bumped up to indicate that the client supports KafkaStorageException. The KafkaStorageException will&lt;br/&gt;
+    // be translated to NotLeaderForPartitionException in the response if version &amp;lt;= 5&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_V6 = FETCH_RESPONSE_V5;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// FETCH_RESPONSE_V7 added incremental fetch responses and a top-level error code.&lt;/li&gt;
	&lt;li&gt;public static final Field.Int32 SESSION_ID = new Field.Int32(&quot;session_id&quot;, &quot;The fetch session ID&quot;);&lt;br/&gt;
+    // V7 added incremental fetch responses and a top-level error code.&lt;br/&gt;
+    private static final Field.Int32 SESSION_ID = new Field.Int32(&quot;session_id&quot;, &quot;The fetch session ID&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema FETCH_RESPONSE_V7 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;br/&gt;
@@ -162,32 +169,22 @@&lt;br/&gt;
             SESSION_ID,&lt;br/&gt;
             new Field(RESPONSES_KEY_NAME, new ArrayOf(FETCH_RESPONSE_TOPIC_V5)));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V8 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema FETCH_RESPONSE_V8 = FETCH_RESPONSE_V7;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V9 adds the current leader epoch (see KIP-320)&lt;br/&gt;
+    private static final Schema FETCH_RESPONSE_V9 = FETCH_RESPONSE_V8;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{FETCH_RESPONSE_V0, FETCH_RESPONSE_V1, FETCH_RESPONSE_V2,
             FETCH_RESPONSE_V3, FETCH_RESPONSE_V4, FETCH_RESPONSE_V5, FETCH_RESPONSE_V6,
-            FETCH_RESPONSE_V7, FETCH_RESPONSE_V8}
&lt;p&gt;;&lt;br/&gt;
+            FETCH_RESPONSE_V7, FETCH_RESPONSE_V8, FETCH_RESPONSE_V9};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     public static final long INVALID_HIGHWATERMARK = -1L;&lt;br/&gt;
     public static final long INVALID_LAST_STABLE_OFFSET = -1L;&lt;br/&gt;
     public static final long INVALID_LOG_START_OFFSET = -1L;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Possible error codes:&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*  OFFSET_OUT_OF_RANGE (1)&lt;/li&gt;
	&lt;li&gt;*  UNKNOWN_TOPIC_OR_PARTITION (3)&lt;/li&gt;
	&lt;li&gt;*  NOT_LEADER_FOR_PARTITION (6)&lt;/li&gt;
	&lt;li&gt;*  REPLICA_NOT_AVAILABLE (9)&lt;/li&gt;
	&lt;li&gt;*  UNKNOWN (-1)&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;br/&gt;
     private final int throttleTimeMs;&lt;br/&gt;
     private final Errors error;&lt;br/&gt;
     private final int sessionId;&lt;br/&gt;
@@ -318,13 +315,9 @@ public FetchResponse(Errors error,&lt;br/&gt;
                 Struct partitionResponseHeader = partitionResponse.getStruct(PARTITION_HEADER_KEY_NAME);&lt;br/&gt;
                 int partition = partitionResponseHeader.get(PARTITION_ID);&lt;br/&gt;
                 Errors error = Errors.forCode(partitionResponseHeader.get(ERROR_CODE));&lt;/li&gt;
	&lt;li&gt;long highWatermark = partitionResponseHeader.getLong(HIGH_WATERMARK_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;long lastStableOffset = INVALID_LAST_STABLE_OFFSET;&lt;/li&gt;
	&lt;li&gt;if (partitionResponseHeader.hasField(LAST_STABLE_OFFSET_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;lastStableOffset = partitionResponseHeader.getLong(LAST_STABLE_OFFSET_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;long logStartOffset = INVALID_LOG_START_OFFSET;&lt;/li&gt;
	&lt;li&gt;if (partitionResponseHeader.hasField(LOG_START_OFFSET_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;logStartOffset = partitionResponseHeader.getLong(LOG_START_OFFSET_KEY_NAME);&lt;br/&gt;
+                long highWatermark = partitionResponseHeader.get(HIGH_WATERMARK);&lt;br/&gt;
+                long lastStableOffset = partitionResponseHeader.getOrElse(LAST_STABLE_OFFSET, INVALID_LAST_STABLE_OFFSET);&lt;br/&gt;
+                long logStartOffset = partitionResponseHeader.getOrElse(LOG_START_OFFSET, INVALID_LOG_START_OFFSET);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);&lt;br/&gt;
                 if (!(baseRecords instanceof MemoryRecords))&lt;br/&gt;
@@ -338,8 +331,8 @@ public FetchResponse(Errors error,&lt;br/&gt;
                         abortedTransactions = new ArrayList&amp;lt;&amp;gt;(abortedTransactionsArray.length);&lt;br/&gt;
                         for (Object abortedTransactionObj : abortedTransactionsArray) &lt;/p&gt;
{
                             Struct abortedTransactionStruct = (Struct) abortedTransactionObj;
-                            long producerId = abortedTransactionStruct.getLong(PRODUCER_ID_KEY_NAME);
-                            long firstOffset = abortedTransactionStruct.getLong(FIRST_OFFSET_KEY_NAME);
+                            long producerId = abortedTransactionStruct.get(PRODUCER_ID);
+                            long firstOffset = abortedTransactionStruct.get(FIRST_OFFSET);
                             abortedTransactions.add(new AbortedTransaction(producerId, firstOffset));
                         }
&lt;p&gt;                     }&lt;br/&gt;
@@ -490,10 +483,10 @@ private static void addPartitionData(String dest, Queue&amp;lt;Send&amp;gt; sends, Struct part&lt;br/&gt;
                 Struct partitionDataHeader = partitionData.instance(PARTITION_HEADER_KEY_NAME);&lt;br/&gt;
                 partitionDataHeader.set(PARTITION_ID, partitionEntry.getKey());&lt;br/&gt;
                 partitionDataHeader.set(ERROR_CODE, errorCode);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;partitionDataHeader.set(HIGH_WATERMARK_KEY_NAME, fetchPartitionData.highWatermark);&lt;br/&gt;
+                partitionDataHeader.set(HIGH_WATERMARK, fetchPartitionData.highWatermark);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (partitionDataHeader.hasField(LAST_STABLE_OFFSET_KEY_NAME)) {&lt;/li&gt;
	&lt;li&gt;partitionDataHeader.set(LAST_STABLE_OFFSET_KEY_NAME, fetchPartitionData.lastStableOffset);&lt;br/&gt;
+                if (partitionDataHeader.hasField(LAST_STABLE_OFFSET)) {&lt;br/&gt;
+                    partitionDataHeader.set(LAST_STABLE_OFFSET, fetchPartitionData.lastStableOffset);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     if (fetchPartitionData.abortedTransactions == null) {&lt;br/&gt;
                         partitionDataHeader.set(ABORTED_TRANSACTIONS_KEY_NAME, null);&lt;br/&gt;
@@ -501,16 +494,14 @@ private static void addPartitionData(String dest, Queue&amp;lt;Send&amp;gt; sends, Struct part&lt;br/&gt;
                         List&amp;lt;Struct&amp;gt; abortedTransactionStructs = new ArrayList&amp;lt;&amp;gt;(fetchPartitionData.abortedTransactions.size());&lt;br/&gt;
                         for (AbortedTransaction abortedTransaction : fetchPartitionData.abortedTransactions) &lt;/p&gt;
{
                             Struct abortedTransactionStruct = partitionDataHeader.instance(ABORTED_TRANSACTIONS_KEY_NAME);
-                            abortedTransactionStruct.set(PRODUCER_ID_KEY_NAME, abortedTransaction.producerId);
-                            abortedTransactionStruct.set(FIRST_OFFSET_KEY_NAME, abortedTransaction.firstOffset);
+                            abortedTransactionStruct.set(PRODUCER_ID, abortedTransaction.producerId);
+                            abortedTransactionStruct.set(FIRST_OFFSET, abortedTransaction.firstOffset);
                             abortedTransactionStructs.add(abortedTransactionStruct);
                         }
&lt;p&gt;                         partitionDataHeader.set(ABORTED_TRANSACTIONS_KEY_NAME, abortedTransactionStructs.toArray());&lt;br/&gt;
                     }&lt;br/&gt;
                 }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (partitionDataHeader.hasField(LOG_START_OFFSET_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;partitionDataHeader.set(LOG_START_OFFSET_KEY_NAME, fetchPartitionData.logStartOffset);&lt;br/&gt;
-&lt;br/&gt;
+                partitionDataHeader.setIfExists(LOG_START_OFFSET, fetchPartitionData.logStartOffset);&lt;br/&gt;
                 partitionData.set(PARTITION_HEADER_KEY_NAME, partitionDataHeader);&lt;br/&gt;
                 partitionData.set(RECORD_SET_KEY_NAME, fetchPartitionData.records);&lt;br/&gt;
                 partitionArray.add(partitionData);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorRequest.java&lt;br/&gt;
index 8fb71a87c07..2d44ab3015b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorRequest.java&lt;br/&gt;
@@ -94,13 +94,13 @@ public String toString() {&lt;br/&gt;
     private final CoordinatorType coordinatorType;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private FindCoordinatorRequest(CoordinatorType coordinatorType, String coordinatorKey, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.FIND_COORDINATOR, version);
         this.coordinatorType = coordinatorType;
         this.coordinatorKey = coordinatorKey;
     }

&lt;p&gt;     public FindCoordinatorRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.FIND_COORDINATOR, version);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (struct.hasField(COORDINATOR_TYPE_KEY_NAME))&lt;br/&gt;
             this.coordinatorType = CoordinatorType.forId(struct.getByte(COORDINATOR_TYPE_KEY_NAME));&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/HeartbeatRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/HeartbeatRequest.java&lt;br/&gt;
index 5a70ed87dcd..9a131478df7 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/HeartbeatRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/HeartbeatRequest.java&lt;br/&gt;
@@ -80,14 +80,14 @@ public String toString() {&lt;br/&gt;
     private final String memberId;&lt;/p&gt;

&lt;p&gt;     private HeartbeatRequest(String groupId, int groupGenerationId, String memberId, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.HEARTBEAT, version);
         this.groupId = groupId;
         this.groupGenerationId = groupGenerationId;
         this.memberId = memberId;
     }

&lt;p&gt;     public HeartbeatRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.HEARTBEAT, version);&lt;br/&gt;
         groupId = struct.get(GROUP_ID);&lt;br/&gt;
         groupGenerationId = struct.get(GENERATION_ID);&lt;br/&gt;
         memberId = struct.get(MEMBER_ID);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdRequest.java&lt;br/&gt;
index c3505997102..aab7c72cf43 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdRequest.java&lt;br/&gt;
@@ -82,13 +82,13 @@ public String toString() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public InitProducerIdRequest(Struct struct, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.INIT_PRODUCER_ID, version);
         this.transactionalId = struct.get(NULLABLE_TRANSACTIONAL_ID);
         this.transactionTimeoutMs = struct.getInt(TRANSACTION_TIMEOUT_KEY_NAME);
     }

&lt;p&gt;     private InitProducerIdRequest(short version, String transactionalId, int transactionTimeoutMs) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.INIT_PRODUCER_ID, version);
         this.transactionalId = transactionalId;
         this.transactionTimeoutMs = transactionTimeoutMs;
     }
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java&lt;br/&gt;
index ba009a1ae2b..366509dbab1 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java&lt;br/&gt;
@@ -160,7 +160,7 @@ public String toString() {&lt;br/&gt;
     private JoinGroupRequest(short version, String groupId, int sessionTimeout,&lt;br/&gt;
             int rebalanceTimeout, String memberId, String protocolType,&lt;br/&gt;
             List&amp;lt;ProtocolMetadata&amp;gt; groupProtocols) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.JOIN_GROUP, version);
         this.groupId = groupId;
         this.sessionTimeout = sessionTimeout;
         this.rebalanceTimeout = rebalanceTimeout;
@@ -170,7 +170,7 @@ private JoinGroupRequest(short version, String groupId, int sessionTimeout,
     }

&lt;p&gt;     public JoinGroupRequest(Struct struct, short versionId) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(versionId);&lt;br/&gt;
+        super(ApiKeys.JOIN_GROUP, versionId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         groupId = struct.get(GROUP_ID);&lt;br/&gt;
         sessionTimeout = struct.getInt(SESSION_TIMEOUT_KEY_NAME);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java&lt;br/&gt;
index 4fa53371884..b821c0e948f 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java&lt;br/&gt;
@@ -145,7 +145,7 @@ public String toString() {&lt;/p&gt;

&lt;p&gt;     private LeaderAndIsrRequest(int controllerId, int controllerEpoch, Map&amp;lt;TopicPartition, PartitionState&amp;gt; partitionStates,&lt;br/&gt;
                                 Set&amp;lt;Node&amp;gt; liveLeaders, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.LEADER_AND_ISR, version);
         this.controllerId = controllerId;
         this.controllerEpoch = controllerEpoch;
         this.partitionStates = partitionStates;
@@ -153,7 +153,7 @@ private LeaderAndIsrRequest(int controllerId, int controllerEpoch, Map&amp;lt;TopicPart
     }

&lt;p&gt;     public LeaderAndIsrRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.LEADER_AND_ISR, version);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;TopicPartition, PartitionState&amp;gt; partitionStates = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object partitionStateDataObj : struct.getArray(PARTITION_STATES_KEY_NAME)) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupRequest.java&lt;br/&gt;
index 2b4acf876ff..2d0b448262f 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupRequest.java&lt;br/&gt;
@@ -74,13 +74,13 @@ public String toString() {&lt;br/&gt;
     private final String memberId;&lt;/p&gt;

&lt;p&gt;     private LeaveGroupRequest(String groupId, String memberId, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.LEAVE_GROUP, version);
         this.groupId = groupId;
         this.memberId = memberId;
     }

&lt;p&gt;     public LeaveGroupRequest(Struct struct, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.LEAVE_GROUP, version);
         groupId = struct.get(GROUP_ID);
         memberId = struct.get(MEMBER_ID);
     }
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ListGroupsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ListGroupsRequest.java&lt;br/&gt;
index f64f71a56e3..27254cb1f24 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/ListGroupsRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ListGroupsRequest.java&lt;br/&gt;
@@ -59,11 +59,11 @@ public String toString() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public ListGroupsRequest(short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.LIST_GROUPS, version);
     }

&lt;p&gt;     public ListGroupsRequest(Struct struct, short versionId) &lt;/p&gt;
{
-        super(versionId);
+        super(ApiKeys.LIST_GROUPS, versionId);
     }

&lt;p&gt;     @Override&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java&lt;br/&gt;
index be094feec55..5107c4e9140 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -32,13 +31,12 @@&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;/p&gt;

&lt;p&gt;+import static org.apache.kafka.common.protocol.CommonFields.CURRENT_LEADER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT32;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT8;&lt;/p&gt;

&lt;p&gt; public class ListOffsetRequest extends AbstractRequest {&lt;br/&gt;
     public static final long EARLIEST_TIMESTAMP = -2L;&lt;br/&gt;
@@ -47,70 +45,93 @@&lt;br/&gt;
     public static final int CONSUMER_REPLICA_ID = -1;&lt;br/&gt;
     public static final int DEBUGGING_REPLICA_ID = -2;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String REPLICA_ID_KEY_NAME = &quot;replica_id&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String ISOLATION_LEVEL_KEY_NAME = &quot;isolation_level&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;br/&gt;
+    // top level fields&lt;br/&gt;
+    private static final Field.Int32 REPLICA_ID = new Field.Int32(&quot;replica_id&quot;,&lt;br/&gt;
+            &quot;Broker id of the follower. For normal consumers, use -1.&quot;);&lt;br/&gt;
+    private static final Field.Int8 ISOLATION_LEVEL = new Field.Int8(&quot;isolation_level&quot;,&lt;br/&gt;
+            &quot;This setting controls the visibility of transactional records. &quot; +&lt;br/&gt;
+                    &quot;Using READ_UNCOMMITTED (isolation_level = 0) makes all records visible. With READ_COMMITTED &quot; +&lt;br/&gt;
+                    &quot;(isolation_level = 1), non-transactional and COMMITTED transactional records are visible. &quot; +&lt;br/&gt;
+                    &quot;To be more concrete, READ_COMMITTED returns all data from offsets smaller than the current &quot; +&lt;br/&gt;
+                    &quot;LSO (last stable offset), and enables the inclusion of the list of aborted transactions in the &quot; +&lt;br/&gt;
+                    &quot;result, which allows consumers to discard ABORTED transactional records&quot;);&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;Topics to list offsets.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// topic level field names&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;Partitions to list offsets.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// partition level field names&lt;/li&gt;
	&lt;li&gt;private static final String TIMESTAMP_KEY_NAME = &quot;timestamp&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String MAX_NUM_OFFSETS_KEY_NAME = &quot;max_num_offsets&quot;;&lt;br/&gt;
+    // partition level fields&lt;br/&gt;
+    private static final Field.Int64 TIMESTAMP = new Field.Int64(&quot;timestamp&quot;,&lt;br/&gt;
+            &quot;The target timestamp for the partition.&quot;);&lt;br/&gt;
+    private static final Field.Int32 MAX_NUM_OFFSETS = new Field.Int32(&quot;max_num_offsets&quot;,&lt;br/&gt;
+            &quot;Maximum offsets to return.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_REQUEST_PARTITION_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(TIMESTAMP_KEY_NAME, INT64, &quot;Timestamp.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(MAX_NUM_OFFSETS_KEY_NAME, INT32, &quot;Maximum offsets to return.&quot;));&lt;/li&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_REQUEST_PARTITION_V1 = new Schema(&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(TIMESTAMP_KEY_NAME, INT64, &quot;The target timestamp for the partition.&quot;));&lt;br/&gt;
+            TIMESTAMP,&lt;br/&gt;
+            MAX_NUM_OFFSETS);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_REQUEST_TOPIC_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(LIST_OFFSET_REQUEST_PARTITION_V0), &quot;Partitions to list offset.&quot;));&lt;/li&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_REQUEST_TOPIC_V1 = new Schema(&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(LIST_OFFSET_REQUEST_PARTITION_V1), &quot;Partitions to list offset.&quot;));&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema LIST_OFFSET_REQUEST_V0 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(LIST_OFFSET_REQUEST_TOPIC_V0), &quot;Topics to list offsets.&quot;));&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            TOPICS_V0);&lt;br/&gt;
+&lt;br/&gt;
+    // V1 removes max_num_offsets&lt;br/&gt;
+    private static final Field PARTITIONS_V1 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            TIMESTAMP);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V1 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V1);&lt;br/&gt;
+&lt;br/&gt;
     private static final Schema LIST_OFFSET_REQUEST_V1 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(LIST_OFFSET_REQUEST_TOPIC_V1), &quot;Topics to list offsets.&quot;));&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            TOPICS_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V2 adds a field for the isolation level&lt;br/&gt;
     private static final Schema LIST_OFFSET_REQUEST_V2 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(REPLICA_ID_KEY_NAME, INT32, &quot;Broker id of the follower. For normal consumers, use -1.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(ISOLATION_LEVEL_KEY_NAME, INT8, &quot;This setting controls the visibility of transactional records. &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;Using READ_UNCOMMITTED (isolation_level = 0) makes all records visible. With READ_COMMITTED &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;(isolation_level = 1), non-transactional and COMMITTED transactional records are visible. &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;To be more concrete, READ_COMMITTED returns all data from offsets smaller than the current &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;LSO (last stable offset), and enables the inclusion of the list of aborted transactions in the &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;result, which allows consumers to discard ABORTED transactional records&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(LIST_OFFSET_REQUEST_TOPIC_V1), &quot;Topics to list offsets.&quot;));;&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            ISOLATION_LEVEL,&lt;br/&gt;
+            TOPICS_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V3 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema LIST_OFFSET_REQUEST_V3 = LIST_OFFSET_REQUEST_V2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V4 introduces the current leader epoch, which is used for fencing&lt;br/&gt;
+    private static final Field PARTITIONS_V4 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            CURRENT_LEADER_EPOCH,&lt;br/&gt;
+            TIMESTAMP);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V4 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V4);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Schema LIST_OFFSET_REQUEST_V4 = new Schema(&lt;br/&gt;
+            REPLICA_ID,&lt;br/&gt;
+            ISOLATION_LEVEL,&lt;br/&gt;
+            TOPICS_V4);&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{LIST_OFFSET_REQUEST_V0, LIST_OFFSET_REQUEST_V1, LIST_OFFSET_REQUEST_V2,
-            LIST_OFFSET_REQUEST_V3}
&lt;p&gt;;&lt;br/&gt;
+            LIST_OFFSET_REQUEST_V3, LIST_OFFSET_REQUEST_V4};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private final int replicaId;&lt;br/&gt;
     private final IsolationLevel isolationLevel;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Map&amp;lt;TopicPartition, PartitionData&amp;gt; offsetData;&lt;/li&gt;
	&lt;li&gt;private final Map&amp;lt;TopicPartition, Long&amp;gt; partitionTimestamps;&lt;br/&gt;
+    private final Map&amp;lt;TopicPartition, PartitionData&amp;gt; partitionTimestamps;&lt;br/&gt;
     private final Set&amp;lt;TopicPartition&amp;gt; duplicatePartitions;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static class Builder extends AbstractRequest.Builder&amp;lt;ListOffsetRequest&amp;gt; {&lt;br/&gt;
         private final int replicaId;&lt;br/&gt;
         private final IsolationLevel isolationLevel;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Map&amp;lt;TopicPartition, PartitionData&amp;gt; offsetData = null;&lt;/li&gt;
	&lt;li&gt;private Map&amp;lt;TopicPartition, Long&amp;gt; partitionTimestamps = null;&lt;br/&gt;
+        private Map&amp;lt;TopicPartition, PartitionData&amp;gt; partitionTimestamps = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         public static Builder forReplica(short allowedVersion, int replicaId) &lt;/p&gt;
{
             return new Builder((short) 0, allowedVersion, replicaId, IsolationLevel.READ_UNCOMMITTED);
@@ -125,49 +146,23 @@ else if (requireTimestamp)
             return new Builder(minVersion, ApiKeys.LIST_OFFSETS.latestVersion(), CONSUMER_REPLICA_ID, isolationLevel);
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Builder(short oldestAllowedVersion, short latestAllowedVersion, int replicaId, IsolationLevel isolationLevel) {&lt;br/&gt;
+        private Builder(short oldestAllowedVersion,&lt;br/&gt;
+                        short latestAllowedVersion,&lt;br/&gt;
+                        int replicaId,&lt;br/&gt;
+                        IsolationLevel isolationLevel) 
{
             super(ApiKeys.LIST_OFFSETS, oldestAllowedVersion, latestAllowedVersion);
             this.replicaId = replicaId;
             this.isolationLevel = isolationLevel;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Builder setOffsetData(Map&amp;lt;TopicPartition, PartitionData&amp;gt; offsetData) 
{
-            this.offsetData = offsetData;
-            return this;
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public Builder setTargetTimes(Map&amp;lt;TopicPartition, Long&amp;gt; partitionTimestamps) {&lt;br/&gt;
+        public Builder setTargetTimes(Map&amp;lt;TopicPartition, PartitionData&amp;gt; partitionTimestamps) 
{
             this.partitionTimestamps = partitionTimestamps;
             return this;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public ListOffsetRequest build(short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (version == 0) {&lt;/li&gt;
	&lt;li&gt;if (offsetData == null) {&lt;/li&gt;
	&lt;li&gt;if (partitionTimestamps == null) 
{
-                        throw new RuntimeException(&quot;Must set partitionTimestamps or offsetData when creating a v0 &quot; +
-                            &quot;ListOffsetRequest&quot;);
-                    }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;offsetData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry: partitionTimestamps.entrySet()) 
{
-                            offsetData.put(entry.getKey(),
-                                    new PartitionData(entry.getValue(), 1));
-                        }&lt;/li&gt;
	&lt;li&gt;this.partitionTimestamps = null;&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;if (offsetData != null) 
{
-                    throw new RuntimeException(&quot;Cannot create a v&quot; + version + &quot; ListOffsetRequest with v0 &quot; +
-                        &quot;PartitionData.&quot;);
-                }
&lt;p&gt; else if (partitionTimestamps == null) &lt;/p&gt;
{
-                    throw new RuntimeException(&quot;Must set partitionTimestamps when creating a v&quot; +
-                            version + &quot; ListOffsetRequest&quot;);
-                }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, ?&amp;gt; m = (version == 0) ?  offsetData : partitionTimestamps;&lt;/li&gt;
	&lt;li&gt;return new ListOffsetRequest(replicaId, m, isolationLevel, version);&lt;br/&gt;
+            return new ListOffsetRequest(replicaId, partitionTimestamps, isolationLevel, version);&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
@@ -175,9 +170,6 @@ public String toString() {&lt;br/&gt;
             StringBuilder bld = new StringBuilder();&lt;br/&gt;
             bld.append(&quot;(type=ListOffsetRequest&quot;)&lt;br/&gt;
                .append(&quot;, replicaId=&quot;).append(replicaId);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (offsetData != null) 
{
-                bld.append(&quot;, offsetData=&quot;).append(offsetData);
-            }
&lt;p&gt;             if (partitionTimestamps != null) &lt;/p&gt;
{
                 bld.append(&quot;, partitionTimestamps=&quot;).append(partitionTimestamps);
             }
&lt;p&gt;@@ -187,25 +179,34 @@ public String toString() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* This class is only used by ListOffsetRequest v0 which has been deprecated.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;@Deprecated&lt;br/&gt;
     public static final class PartitionData {&lt;br/&gt;
         public final long timestamp;&lt;/li&gt;
	&lt;li&gt;public final int maxNumOffsets;&lt;br/&gt;
+        @Deprecated&lt;br/&gt;
+        public final int maxNumOffsets; // only supported in v0&lt;br/&gt;
+        public final Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public PartitionData(long timestamp, int maxNumOffsets) {&lt;br/&gt;
+        private PartitionData(long timestamp, int maxNumOffsets, Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch) 
{
             this.timestamp = timestamp;
             this.maxNumOffsets = maxNumOffsets;
+            this.currentLeaderEpoch = currentLeaderEpoch;
+        }
&lt;p&gt;+&lt;br/&gt;
+        @Deprecated&lt;br/&gt;
+        public PartitionData(long timestamp, int maxNumOffsets) &lt;/p&gt;
{
+            this(timestamp, maxNumOffsets, Optional.empty());
+        }
&lt;p&gt;+&lt;br/&gt;
+        public PartitionData(long timestamp, Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch) &lt;/p&gt;
{
+            this(timestamp, 1, currentLeaderEpoch);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public String toString() {&lt;br/&gt;
             StringBuilder bld = new StringBuilder();&lt;br/&gt;
             bld.append(&quot;&lt;/p&gt;
{timestamp: &quot;).append(timestamp).
-                append(&quot;, maxNumOffsets: &quot;).append(maxNumOffsets).
-                append(&quot;}
&lt;p&gt;&quot;);&lt;br/&gt;
+                    append(&quot;, maxNumOffsets: &quot;).append(maxNumOffsets).&lt;br/&gt;
+                    append(&quot;, currentLeaderEpoch: &quot;).append(currentLeaderEpoch).&lt;br/&gt;
+                    append(&quot;}&quot;);&lt;br/&gt;
             return bld.toString();&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
@@ -214,40 +215,39 @@ public String toString() {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Private constructor with a specified version.&lt;br/&gt;
      */&lt;br/&gt;
     @SuppressWarnings(&quot;unchecked&quot;)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private ListOffsetRequest(int replicaId, Map&amp;lt;TopicPartition, ?&amp;gt; targetTimes, IsolationLevel isolationLevel, short version) {&lt;/li&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+    private ListOffsetRequest(int replicaId,&lt;br/&gt;
+                              Map&amp;lt;TopicPartition, PartitionData&amp;gt; targetTimes,&lt;br/&gt;
+                              IsolationLevel isolationLevel,&lt;br/&gt;
+                              short version) 
{
+        super(ApiKeys.LIST_OFFSETS, version);
         this.replicaId = replicaId;
         this.isolationLevel = isolationLevel;
-        this.offsetData = version == 0 ? (Map&amp;lt;TopicPartition, PartitionData&amp;gt;) targetTimes : null;
-        this.partitionTimestamps = version &amp;gt;= 1 ? (Map&amp;lt;TopicPartition, Long&amp;gt;) targetTimes : null;
+        this.partitionTimestamps = targetTimes;
         this.duplicatePartitions = Collections.emptySet();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public ListOffsetRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.LIST_OFFSETS, version);&lt;br/&gt;
         Set&amp;lt;TopicPartition&amp;gt; duplicatePartitions = new HashSet&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;replicaId = struct.getInt(REPLICA_ID_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;isolationLevel = struct.hasField(ISOLATION_LEVEL_KEY_NAME) ?&lt;/li&gt;
	&lt;li&gt;IsolationLevel.forId(struct.getByte(ISOLATION_LEVEL_KEY_NAME)) :&lt;br/&gt;
+        replicaId = struct.get(REPLICA_ID);&lt;br/&gt;
+        isolationLevel = struct.hasField(ISOLATION_LEVEL) ?&lt;br/&gt;
+                IsolationLevel.forId(struct.get(ISOLATION_LEVEL)) :&lt;br/&gt;
                 IsolationLevel.READ_UNCOMMITTED;&lt;/li&gt;
	&lt;li&gt;offsetData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         partitionTimestamps = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Object topicResponseObj : struct.getArray(TOPICS_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicResponseObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicResponse = (Struct) topicResponseObj;&lt;br/&gt;
             String topic = topicResponse.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {&lt;br/&gt;
+            for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {&lt;br/&gt;
                 Struct partitionResponse = (Struct) partitionResponseObj;&lt;br/&gt;
                 int partition = partitionResponse.get(PARTITION_ID);&lt;/li&gt;
	&lt;li&gt;long timestamp = partitionResponse.getLong(TIMESTAMP_KEY_NAME);&lt;br/&gt;
+                long timestamp = partitionResponse.get(TIMESTAMP);&lt;br/&gt;
                 TopicPartition tp = new TopicPartition(topic, partition);&lt;/li&gt;
	&lt;li&gt;if (partitionResponse.hasField(MAX_NUM_OFFSETS_KEY_NAME)) 
{
-                    int maxNumOffsets = partitionResponse.getInt(MAX_NUM_OFFSETS_KEY_NAME);
-                    PartitionData partitionData = new PartitionData(timestamp, maxNumOffsets);
-                    offsetData.put(tp, partitionData);
-                }
&lt;p&gt; else &lt;/p&gt;
{
-                    if (partitionTimestamps.put(tp, timestamp) != null)
-                        duplicatePartitions.add(tp);
-                }
&lt;p&gt;+&lt;br/&gt;
+                int maxNumOffsets = partitionResponse.getOrElse(MAX_NUM_OFFSETS, 1);&lt;br/&gt;
+                Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch = RequestUtils.getLeaderEpoch(partitionResponse, CURRENT_LEADER_EPOCH);&lt;br/&gt;
+                PartitionData partitionData = new PartitionData(timestamp, maxNumOffsets, currentLeaderEpoch);&lt;br/&gt;
+                if (partitionTimestamps.put(tp, partitionData) != null)&lt;br/&gt;
+                    duplicatePartitions.add(tp);&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;br/&gt;
         this.duplicatePartitions = duplicatePartitions;&lt;br/&gt;
@@ -257,27 +257,21 @@ public ListOffsetRequest(Struct struct, short version) {&lt;br/&gt;
     @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
     public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {&lt;br/&gt;
         Map&amp;lt;TopicPartition, ListOffsetResponse.PartitionData&amp;gt; responseData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;br/&gt;
         short versionId = version();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (versionId == 0) {&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, PartitionData&amp;gt; entry : offsetData.entrySet()) 
{
-                ListOffsetResponse.PartitionData partitionResponse = new ListOffsetResponse.PartitionData(
-                        Errors.forException(e), Collections.&amp;lt;Long&amp;gt;emptyList());
-                responseData.put(entry.getKey(), partitionResponse);
-            }&lt;/li&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : partitionTimestamps.entrySet()) 
{
-                ListOffsetResponse.PartitionData partitionResponse = new ListOffsetResponse.PartitionData(
-                        Errors.forException(e), -1L, -1L);
-                responseData.put(entry.getKey(), partitionResponse);
-            }
&lt;p&gt;+&lt;br/&gt;
+        ListOffsetResponse.PartitionData partitionError = versionId == 0 ?&lt;br/&gt;
+                new ListOffsetResponse.PartitionData(Errors.forException(e), Collections.emptyList()) :&lt;br/&gt;
+                new ListOffsetResponse.PartitionData(Errors.forException(e), -1L, -1L, Optional.empty());&lt;br/&gt;
+        for (TopicPartition partition : partitionTimestamps.keySet()) &lt;/p&gt;
{
+            responseData.put(partition, partitionError);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;switch (versionId) {&lt;br/&gt;
+        switch (version()) {&lt;br/&gt;
             case 0:&lt;br/&gt;
             case 1:&lt;br/&gt;
             case 2:&lt;br/&gt;
             case 3:&lt;br/&gt;
+            case 4:&lt;br/&gt;
                 return new ListOffsetResponse(throttleTimeMs, responseData);&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalArgumentException(String.format(&quot;Version %d is not valid. Valid versions for %s are 0 to %d&quot;,&lt;br/&gt;
@@ -293,12 +287,7 @@ public IsolationLevel isolationLevel() 
{
         return isolationLevel;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Deprecated&lt;/li&gt;
	&lt;li&gt;public Map&amp;lt;TopicPartition, PartitionData&amp;gt; offsetData() 
{
-        return offsetData;
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public Map&amp;lt;TopicPartition, Long&amp;gt; partitionTimestamps() {&lt;br/&gt;
+    public Map&amp;lt;TopicPartition, PartitionData&amp;gt; partitionTimestamps() 
{
         return partitionTimestamps;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -314,39 +303,30 @@ public static ListOffsetRequest parse(ByteBuffer buffer, short version) {&lt;br/&gt;
     protected Struct toStruct() {&lt;br/&gt;
         short version = version();&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.LIST_OFFSETS.requestSchema(version));&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsData = CollectionUtils.groupPartitionDataByTopic(partitionTimestamps);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, ?&amp;gt; targetTimes = partitionTimestamps == null ? offsetData : partitionTimestamps;&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Object&amp;gt;&amp;gt; topicsData = CollectionUtils.groupDataByTopic(targetTimes);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;struct.set(REPLICA_ID_KEY_NAME, replicaId);&lt;br/&gt;
+        struct.set(REPLICA_ID, replicaId);&lt;br/&gt;
+        struct.setIfExists(ISOLATION_LEVEL, isolationLevel.id());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (struct.hasField(ISOLATION_LEVEL_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;struct.set(ISOLATION_LEVEL_KEY_NAME, isolationLevel.id());&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Object&amp;gt;&amp;gt; topicEntry: topicsData.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct topicData = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+        for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicEntry: topicsData.entrySet()) {&lt;br/&gt;
+            Struct topicData = struct.instance(TOPICS);&lt;br/&gt;
             topicData.set(TOPIC_NAME, topicEntry.getKey());&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Integer, Object&amp;gt; partitionEntry : topicEntry.getValue().entrySet()) {&lt;/li&gt;
	&lt;li&gt;if (version == 0) 
{
-                    PartitionData offsetPartitionData = (PartitionData) partitionEntry.getValue();
-                    Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);
-                    partitionData.set(PARTITION_ID, partitionEntry.getKey());
-                    partitionData.set(TIMESTAMP_KEY_NAME, offsetPartitionData.timestamp);
-                    partitionData.set(MAX_NUM_OFFSETS_KEY_NAME, offsetPartitionData.maxNumOffsets);
-                    partitionArray.add(partitionData);
-                }
&lt;p&gt; else &lt;/p&gt;
{
-                    Long timestamp = (Long) partitionEntry.getValue();
-                    Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);
-                    partitionData.set(PARTITION_ID, partitionEntry.getKey());
-                    partitionData.set(TIMESTAMP_KEY_NAME, timestamp);
-                    partitionArray.add(partitionData);
-                }
&lt;p&gt;+            for (Map.Entry&amp;lt;Integer, PartitionData&amp;gt; partitionEntry : topicEntry.getValue().entrySet()) &lt;/p&gt;
{
+                PartitionData offsetPartitionData = partitionEntry.getValue();
+                Struct partitionData = topicData.instance(PARTITIONS);
+                partitionData.set(PARTITION_ID, partitionEntry.getKey());
+                partitionData.set(TIMESTAMP, offsetPartitionData.timestamp);
+                partitionData.setIfExists(MAX_NUM_OFFSETS, offsetPartitionData.maxNumOffsets);
+                RequestUtils.setLeaderEpochIfExists(partitionData, CURRENT_LEADER_EPOCH,
+                        offsetPartitionData.currentLeaderEpoch);
+                partitionArray.add(partitionData);
             }&lt;/li&gt;
	&lt;li&gt;topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());&lt;br/&gt;
+            topicData.set(PARTITIONS, partitionArray.toArray());&lt;br/&gt;
             topicArray.add(topicData);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;struct.set(TOPICS_KEY_NAME, topicArray.toArray());&lt;br/&gt;
+        struct.set(TOPICS, topicArray.toArray());&lt;br/&gt;
         return struct;&lt;br/&gt;
     }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java&lt;br/&gt;
index e719dbbdbfd..9f3ce73a004 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -31,73 +30,98 @@&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.LEADER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;/p&gt;

&lt;p&gt;+/**&lt;br/&gt;
+ * Possible error code:&lt;br/&gt;
+ *&lt;br/&gt;
+ *  UNKNOWN_TOPIC_OR_PARTITION (3)&lt;br/&gt;
+ *  NOT_LEADER_FOR_PARTITION (6)&lt;br/&gt;
+ *  UNSUPPORTED_FOR_MESSAGE_FORMAT (43)&lt;br/&gt;
+ *  UNKNOWN (-1)&lt;br/&gt;
+ */&lt;br/&gt;
 public class ListOffsetResponse extends AbstractResponse {&lt;br/&gt;
     public static final long UNKNOWN_TIMESTAMP = -1L;&lt;br/&gt;
     public static final long UNKNOWN_OFFSET = -1L;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String RESPONSES_KEY_NAME = &quot;responses&quot;;&lt;br/&gt;
+    // top level fields&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;responses&quot;,&lt;br/&gt;
+            &quot;The listed offsets by topic&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// topic level field names&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partition_responses&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Possible error code:&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*  UNKNOWN_TOPIC_OR_PARTITION (3)&lt;/li&gt;
	&lt;li&gt;*  NOT_LEADER_FOR_PARTITION (6)&lt;/li&gt;
	&lt;li&gt;*  UNSUPPORTED_FOR_MESSAGE_FORMAT (43)&lt;/li&gt;
	&lt;li&gt;*  UNKNOWN (-1)&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partition_responses&quot;,&lt;br/&gt;
+            &quot;The listed offsets by partition&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // partition level fields&lt;br/&gt;
     // This key is only used by ListOffsetResponse v0&lt;br/&gt;
     @Deprecated&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String OFFSETS_KEY_NAME = &quot;offsets&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String TIMESTAMP_KEY_NAME = &quot;timestamp&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String OFFSET_KEY_NAME = &quot;offset&quot;;&lt;br/&gt;
+    private static final Field.Array OFFSETS = new Field.Array(&quot;offsets&apos;&quot;, INT64, &quot;A list of offsets.&quot;);&lt;br/&gt;
+    private static final Field.Int64 TIMESTAMP = new Field.Int64(&quot;timestamp&quot;,&lt;br/&gt;
+            &quot;The timestamp associated with the returned offset&quot;);&lt;br/&gt;
+    private static final Field.Int64 OFFSET = new Field.Int64(&quot;offset&quot;,&lt;br/&gt;
+            &quot;The offset found&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_RESPONSE_PARTITION_V0 = new Schema(&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             ERROR_CODE,&lt;/li&gt;
	&lt;li&gt;new Field(OFFSETS_KEY_NAME, new ArrayOf(INT64), &quot;A list of offsets.&quot;));&lt;br/&gt;
+            OFFSETS);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_RESPONSE_PARTITION_V1 = new Schema(&lt;br/&gt;
+    private static final Schema LIST_OFFSET_RESPONSE_V0 = new Schema(&lt;br/&gt;
+            TOPICS_V0);&lt;br/&gt;
+&lt;br/&gt;
+    // V1 bumped for the removal of the offsets array&lt;br/&gt;
+    private static final Field PARTITIONS_V1 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             ERROR_CODE,&lt;/li&gt;
	&lt;li&gt;new Field(TIMESTAMP_KEY_NAME, INT64, &quot;The timestamp associated with the returned offset&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(OFFSET_KEY_NAME, INT64, &quot;offset found&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_RESPONSE_TOPIC_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(LIST_OFFSET_RESPONSE_PARTITION_V0)));&lt;br/&gt;
+            TIMESTAMP,&lt;br/&gt;
+            OFFSET);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_RESPONSE_TOPIC_V1 = new Schema(&lt;br/&gt;
+    private static final Field TOPICS_V1 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(LIST_OFFSET_RESPONSE_PARTITION_V1)));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema LIST_OFFSET_RESPONSE_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(LIST_OFFSET_RESPONSE_TOPIC_V0)));&lt;br/&gt;
+            PARTITIONS_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema LIST_OFFSET_RESPONSE_V1 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(LIST_OFFSET_RESPONSE_TOPIC_V1)));&lt;br/&gt;
+            TOPICS_V1);&lt;br/&gt;
+&lt;br/&gt;
+    // V2 bumped for the addition of the throttle time&lt;br/&gt;
     private static final Schema LIST_OFFSET_RESPONSE_V2 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;/li&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(LIST_OFFSET_RESPONSE_TOPIC_V1)));&lt;br/&gt;
+            TOPICS_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V3 bumped to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema LIST_OFFSET_RESPONSE_V3 = LIST_OFFSET_RESPONSE_V2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V4 bumped for the addition of the current leader epoch in the request schema and the&lt;br/&gt;
+    // leader epoch in the response partition data&lt;br/&gt;
+    private static final Field PARTITIONS_V4 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            ERROR_CODE,&lt;br/&gt;
+            TIMESTAMP,&lt;br/&gt;
+            OFFSET,&lt;br/&gt;
+            LEADER_EPOCH);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V4 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V4);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Schema LIST_OFFSET_RESPONSE_V4 = new Schema(&lt;br/&gt;
+            THROTTLE_TIME_MS,&lt;br/&gt;
+            TOPICS_V4);&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{LIST_OFFSET_RESPONSE_V0, LIST_OFFSET_RESPONSE_V1, LIST_OFFSET_RESPONSE_V2,
-            LIST_OFFSET_RESPONSE_V3}
&lt;p&gt;;&lt;br/&gt;
+            LIST_OFFSET_RESPONSE_V3, LIST_OFFSET_RESPONSE_V4};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static final class PartitionData {&lt;br/&gt;
@@ -107,6 +131,7 @@&lt;br/&gt;
         public final List&amp;lt;Long&amp;gt; offsets;&lt;br/&gt;
         public final Long timestamp;&lt;br/&gt;
         public final Long offset;&lt;br/&gt;
+        public final Optional&amp;lt;Integer&amp;gt; leaderEpoch;&lt;/p&gt;

&lt;p&gt;         /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Constructor for ListOffsetResponse v0&lt;br/&gt;
@@ -117,32 +142,37 @@ public PartitionData(Errors error, List&amp;lt;Long&amp;gt; offsets) 
{
             this.offsets = offsets;
             this.timestamp = null;
             this.offset = null;
+            this.leaderEpoch = Optional.empty();
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Constructor for ListOffsetResponse v1&lt;br/&gt;
          */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public PartitionData(Errors error, long timestamp, long offset) {&lt;br/&gt;
+        public PartitionData(Errors error, long timestamp, long offset, Optional&amp;lt;Integer&amp;gt; leaderEpoch) 
{
             this.error = error;
             this.timestamp = timestamp;
             this.offset = offset;
             this.offsets = null;
+            this.leaderEpoch = leaderEpoch;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public String toString() {&lt;br/&gt;
             StringBuilder bld = new StringBuilder();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;bld.append(&quot;PartitionData{&quot;).&lt;/li&gt;
	&lt;li&gt;append(&quot;errorCode: &quot;).append((int) error.code()).&lt;/li&gt;
	&lt;li&gt;append(&quot;, timestamp: &quot;).append(timestamp).&lt;/li&gt;
	&lt;li&gt;append(&quot;, offset: &quot;).append(offset).&lt;/li&gt;
	&lt;li&gt;append(&quot;, offsets: &quot;);&lt;br/&gt;
+            bld.append(&quot;PartitionData(&quot;).&lt;br/&gt;
+                    append(&quot;errorCode: &quot;).append((int) error.code());&lt;br/&gt;
+&lt;br/&gt;
             if (offsets == null) 
{
-                bld.append(offsets);
+                bld.append(&quot;, timestamp: &quot;).append(timestamp).
+                        append(&quot;, offset: &quot;).append(offset).
+                        append(&quot;, leaderEpoch: &quot;).append(leaderEpoch);
             }
&lt;p&gt; else &lt;/p&gt;
{
-                bld.append(&quot;[&quot;).append(Utils.join(this.offsets, &quot;,&quot;)).append(&quot;]&quot;);
+                bld.append(&quot;, offsets: &quot;).
+                        append(&quot;[&quot;).
+                        append(Utils.join(this.offsets, &quot;,&quot;)).
+                        append(&quot;]&quot;);
             }&lt;/li&gt;
	&lt;li&gt;bld.append(&quot;}&quot;);&lt;br/&gt;
+            bld.append(&quot;)&quot;);&lt;br/&gt;
             return bld.toString();&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
@@ -165,24 +195,25 @@ public ListOffsetResponse(int throttleTimeMs, Map&amp;lt;TopicPartition, PartitionData&amp;gt;&lt;br/&gt;
     public ListOffsetResponse(Struct struct) {&lt;br/&gt;
         this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);&lt;br/&gt;
         responseData = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicResponseObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicResponse = (Struct) topicResponseObj;&lt;br/&gt;
             String topic = topicResponse.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {&lt;br/&gt;
+            for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {&lt;br/&gt;
                 Struct partitionResponse = (Struct) partitionResponseObj;&lt;br/&gt;
                 int partition = partitionResponse.get(PARTITION_ID);&lt;br/&gt;
                 Errors error = Errors.forCode(partitionResponse.get(ERROR_CODE));&lt;br/&gt;
                 PartitionData partitionData;&lt;/li&gt;
	&lt;li&gt;if (partitionResponse.hasField(OFFSETS_KEY_NAME)) {&lt;/li&gt;
	&lt;li&gt;Object[] offsets = partitionResponse.getArray(OFFSETS_KEY_NAME);&lt;br/&gt;
+                if (partitionResponse.hasField(OFFSETS)) 
{
+                    Object[] offsets = partitionResponse.get(OFFSETS);
                     List&amp;lt;Long&amp;gt; offsetsList = new ArrayList&amp;lt;&amp;gt;();
                     for (Object offset : offsets)
                         offsetsList.add((Long) offset);
                     partitionData = new PartitionData(error, offsetsList);
                 }
&lt;p&gt; else &lt;/p&gt;
{
-                    long timestamp = partitionResponse.getLong(TIMESTAMP_KEY_NAME);
-                    long offset = partitionResponse.getLong(OFFSET_KEY_NAME);
-                    partitionData = new PartitionData(error, timestamp, offset);
+                    long timestamp = partitionResponse.get(TIMESTAMP);
+                    long offset = partitionResponse.get(OFFSET);
+                    Optional&amp;lt;Integer&amp;gt; leaderEpoch = RequestUtils.getLeaderEpoch(partitionResponse, LEADER_EPOCH);
+                    partitionData = new PartitionData(error, timestamp, offset, leaderEpoch);
                 }
&lt;p&gt;                 responseData.put(new TopicPartition(topic, partition), partitionData);&lt;br/&gt;
             }&lt;br/&gt;
@@ -214,30 +245,31 @@ public static ListOffsetResponse parse(ByteBuffer buffer, short version) {&lt;br/&gt;
     protected Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.LIST_OFFSETS.responseSchema(version));&lt;br/&gt;
         struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsData = CollectionUtils.groupDataByTopic(responseData);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsData = CollectionUtils.groupPartitionDataByTopic(responseData);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicEntry: topicsData.entrySet()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Struct topicData = struct.instance(RESPONSES_KEY_NAME);&lt;br/&gt;
+            Struct topicData = struct.instance(TOPICS);&lt;br/&gt;
             topicData.set(TOPIC_NAME, topicEntry.getKey());&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, PartitionData&amp;gt; partitionEntry : topicEntry.getValue().entrySet()) {&lt;br/&gt;
                 PartitionData offsetPartitionData = partitionEntry.getValue();&lt;/li&gt;
	&lt;li&gt;Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);&lt;br/&gt;
+                Struct partitionData = topicData.instance(PARTITIONS);&lt;br/&gt;
                 partitionData.set(PARTITION_ID, partitionEntry.getKey());&lt;br/&gt;
                 partitionData.set(ERROR_CODE, offsetPartitionData.error.code());&lt;/li&gt;
	&lt;li&gt;if (version == 0)&lt;/li&gt;
	&lt;li&gt;partitionData.set(OFFSETS_KEY_NAME, offsetPartitionData.offsets.toArray());&lt;/li&gt;
	&lt;li&gt;else {&lt;/li&gt;
	&lt;li&gt;partitionData.set(TIMESTAMP_KEY_NAME, offsetPartitionData.timestamp);&lt;/li&gt;
	&lt;li&gt;partitionData.set(OFFSET_KEY_NAME, offsetPartitionData.offset);&lt;br/&gt;
+                if (version == 0) 
{
+                    partitionData.set(OFFSETS, offsetPartitionData.offsets.toArray());
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    partitionData.set(TIMESTAMP, offsetPartitionData.timestamp);
+                    partitionData.set(OFFSET, offsetPartitionData.offset);
+                    RequestUtils.setLeaderEpochIfExists(partitionData, LEADER_EPOCH, offsetPartitionData.leaderEpoch);
                 }
&lt;p&gt;                 partitionArray.add(partitionData);&lt;br/&gt;
             }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());&lt;br/&gt;
+            topicData.set(PARTITIONS, partitionArray.toArray());&lt;br/&gt;
             topicArray.add(topicData);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;struct.set(RESPONSES_KEY_NAME, topicArray.toArray());&lt;br/&gt;
+        struct.set(TOPICS, topicArray.toArray());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return struct;&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java&lt;br/&gt;
index 67dbe94af40..89a6e69359c 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java&lt;br/&gt;
@@ -16,7 +16,6 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.common.requests;&lt;/p&gt;

&lt;p&gt;-import org.apache.kafka.common.Node;&lt;br/&gt;
 import org.apache.kafka.common.errors.UnsupportedVersionException;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
@@ -31,13 +30,11 @@&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
 import java.util.List;&lt;/p&gt;

&lt;p&gt;-import static org.apache.kafka.common.protocol.types.Type.BOOLEAN;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.types.Type.STRING;&lt;/p&gt;

&lt;p&gt; public class MetadataRequest extends AbstractRequest {&lt;/p&gt;

&lt;p&gt;     private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String ALLOW_AUTO_TOPIC_CREATION_KEY_NAME = &quot;allow_auto_topic_creation&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema METADATA_REQUEST_V0 = new Schema(&lt;br/&gt;
             new Field(TOPICS_KEY_NAME, new ArrayOf(STRING), &quot;An array of topics to fetch metadata for. If no topics are specified fetch metadata for all topics.&quot;));&lt;br/&gt;
@@ -52,12 +49,14 @@&lt;br/&gt;
     private static final Schema METADATA_REQUEST_V3 = METADATA_REQUEST_V2;&lt;/p&gt;

&lt;p&gt;     /* The v4 metadata request has an additional field for allowing auto topic creation. The response is the same as v3. */&lt;br/&gt;
+    private static final Field.Bool ALLOW_AUTO_TOPIC_CREATION = new Field.Bool(&quot;allow_auto_topic_creation&quot;,&lt;br/&gt;
+            &quot;If this and the broker config &amp;lt;code&amp;gt;auto.create.topics.enable&amp;lt;/code&amp;gt; are true, topics that &quot; +&lt;br/&gt;
+                    &quot;don&apos;t exist will be created by the broker. Otherwise, no topics will be created by the broker.&quot;);&lt;br/&gt;
+&lt;br/&gt;
     private static final Schema METADATA_REQUEST_V4 = new Schema(&lt;br/&gt;
             new Field(TOPICS_KEY_NAME, ArrayOf.nullable(STRING), &quot;An array of topics to fetch metadata for. &quot; +&lt;br/&gt;
                     &quot;If the topics array is null fetch metadata for all topics.&quot;),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(ALLOW_AUTO_TOPIC_CREATION_KEY_NAME, BOOLEAN, &quot;If this and the broker config &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;&apos;auto.create.topics.enable&apos; are true, topics that don&apos;t exist will be created by the broker. &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;Otherwise, no topics will be created by the broker.&quot;));&lt;br/&gt;
+            ALLOW_AUTO_TOPIC_CREATION);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /* The v5 metadata request is the same as v4. An additional field for offline_replicas has been added to the v5 metadata response */&lt;br/&gt;
     private static final Schema METADATA_REQUEST_V5 = METADATA_REQUEST_V4;&lt;br/&gt;
@@ -67,9 +66,14 @@&lt;br/&gt;
      */&lt;br/&gt;
     private static final Schema METADATA_REQUEST_V6 = METADATA_REQUEST_V5;&lt;/p&gt;

&lt;p&gt;+    /**&lt;br/&gt;
+     * Bumped for the addition of the current leader epoch in the metadata response.&lt;br/&gt;
+     */&lt;br/&gt;
+    private static final Schema METADATA_REQUEST_V7 = METADATA_REQUEST_V6;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{METADATA_REQUEST_V0, METADATA_REQUEST_V1, METADATA_REQUEST_V2, METADATA_REQUEST_V3,
-            METADATA_REQUEST_V4, METADATA_REQUEST_V5, METADATA_REQUEST_V6}
&lt;p&gt;;&lt;br/&gt;
+            METADATA_REQUEST_V4, METADATA_REQUEST_V5, METADATA_REQUEST_V6, METADATA_REQUEST_V7};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static class Builder extends AbstractRequest.Builder&amp;lt;MetadataRequest&amp;gt; {&lt;br/&gt;
@@ -133,13 +137,13 @@ public String toString() {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;In v1 null indicates requesting all topics, and an empty list indicates requesting no topics.&lt;br/&gt;
      */&lt;br/&gt;
     public MetadataRequest(List&amp;lt;String&amp;gt; topics, boolean allowAutoTopicCreation, short version) 
{
-        super(version);
+        super(ApiKeys.METADATA, version);
         this.topics = topics;
         this.allowAutoTopicCreation = allowAutoTopicCreation;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public MetadataRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.METADATA, version);&lt;br/&gt;
         Object[] topicArray = struct.getArray(TOPICS_KEY_NAME);&lt;br/&gt;
         if (topicArray != null) {&lt;br/&gt;
             topics = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -149,10 +153,8 @@ public MetadataRequest(Struct struct, short version) {&lt;br/&gt;
         } else 
{
             topics = null;
         }&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(ALLOW_AUTO_TOPIC_CREATION_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;allowAutoTopicCreation = struct.getBoolean(ALLOW_AUTO_TOPIC_CREATION_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;allowAutoTopicCreation = true;&lt;br/&gt;
+&lt;br/&gt;
+        allowAutoTopicCreation = struct.getOrElse(ALLOW_AUTO_TOPIC_CREATION, true);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
@@ -171,12 +173,13 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {&lt;br/&gt;
             case 0:&lt;br/&gt;
             case 1:&lt;br/&gt;
             case 2:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new MetadataResponse(Collections.&amp;lt;Node&amp;gt;emptyList(), null, MetadataResponse.NO_CONTROLLER_ID, topicMetadatas);&lt;br/&gt;
+                return new MetadataResponse(Collections.emptyList(), null, MetadataResponse.NO_CONTROLLER_ID, topicMetadatas);&lt;br/&gt;
             case 3:&lt;br/&gt;
             case 4:&lt;br/&gt;
             case 5:&lt;br/&gt;
             case 6:&lt;/li&gt;
	&lt;li&gt;return new MetadataResponse(throttleTimeMs, Collections.&amp;lt;Node&amp;gt;emptyList(), null, MetadataResponse.NO_CONTROLLER_ID, topicMetadatas);&lt;br/&gt;
+            case 7:&lt;br/&gt;
+                return new MetadataResponse(throttleTimeMs, Collections.emptyList(), null, MetadataResponse.NO_CONTROLLER_ID, topicMetadatas);&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalArgumentException(String.format(&quot;Version %d is not valid. Valid versions for %s are 0 to %d&quot;,&lt;br/&gt;
                         versionId, this.getClass().getSimpleName(), ApiKeys.METADATA.latestVersion()));&lt;br/&gt;
@@ -206,8 +209,7 @@ protected Struct toStruct() 
{
             struct.set(TOPICS_KEY_NAME, null);
         else
             struct.set(TOPICS_KEY_NAME, topics.toArray());
-        if (struct.hasField(ALLOW_AUTO_TOPIC_CREATION_KEY_NAME))
-            struct.set(ALLOW_AUTO_TOPIC_CREATION_KEY_NAME, allowAutoTopicCreation);
+        struct.setIfExists(ALLOW_AUTO_TOPIC_CREATION, allowAutoTopicCreation);
         return struct;
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java&lt;br/&gt;
index 09a04e55603..c78066f93bb 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java&lt;br/&gt;
@@ -22,7 +22,6 @@&lt;br/&gt;
 import org.apache.kafka.common.errors.InvalidMetadataException;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -35,143 +34,170 @@&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.LEADER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.BOOLEAN;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.types.Type.INT32;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.NULLABLE_STRING;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.STRING;&lt;/p&gt;

&lt;p&gt;-public class MetadataResponse extends AbstractResponse {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String BROKERS_KEY_NAME = &quot;brokers&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String TOPIC_METADATA_KEY_NAME = &quot;topic_metadata&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// broker level field names&lt;/li&gt;
	&lt;li&gt;private static final String NODE_ID_KEY_NAME = &quot;node_id&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String HOST_KEY_NAME = &quot;host&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String PORT_KEY_NAME = &quot;port&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String RACK_KEY_NAME = &quot;rack&quot;;&lt;br/&gt;
+/**&lt;br/&gt;
+ * Possible topic-level error codes:&lt;br/&gt;
+ *  UnknownTopic (3)&lt;br/&gt;
+ *  LeaderNotAvailable (5)&lt;br/&gt;
+ *  InvalidTopic (17)&lt;br/&gt;
+ *  TopicAuthorizationFailed (29)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String CONTROLLER_ID_KEY_NAME = &quot;controller_id&quot;;&lt;br/&gt;
+ * Possible partition-level error codes:&lt;br/&gt;
+ *  LeaderNotAvailable (5)&lt;br/&gt;
+ *  ReplicaNotAvailable (9)&lt;br/&gt;
+ */&lt;br/&gt;
+public class MetadataResponse extends AbstractResponse {&lt;br/&gt;
     public static final int NO_CONTROLLER_ID = -1;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String CLUSTER_ID_KEY_NAME = &quot;cluster_id&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Possible error codes:&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* UnknownTopic (3)&lt;/li&gt;
	&lt;li&gt;* LeaderNotAvailable (5)&lt;/li&gt;
	&lt;li&gt;* InvalidTopic (17)&lt;/li&gt;
	&lt;li&gt;* TopicAuthorizationFailed (29)&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final String IS_INTERNAL_KEY_NAME = &quot;is_internal&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String PARTITION_METADATA_KEY_NAME = &quot;partition_metadata&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Possible error codes:&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* LeaderNotAvailable (5)&lt;/li&gt;
	&lt;li&gt;* ReplicaNotAvailable (9)&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final String LEADER_KEY_NAME = &quot;leader&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String REPLICAS_KEY_NAME = &quot;replicas&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String ISR_KEY_NAME = &quot;isr&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String OFFLINE_REPLICAS_KEY_NAME = &quot;offline_replicas&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema METADATA_BROKER_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(NODE_ID_KEY_NAME, INT32, &quot;The broker id.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(HOST_KEY_NAME, STRING, &quot;The hostname of the broker.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(PORT_KEY_NAME, INT32, &quot;The port on which the broker accepts requests.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema PARTITION_METADATA_V0 = new Schema(&lt;br/&gt;
+    private static final Field.ComplexArray BROKERS = new Field.ComplexArray(&quot;brokers&quot;,&lt;br/&gt;
+            &quot;Host and port information for all brokers.&quot;);&lt;br/&gt;
+    private static final Field.ComplexArray TOPIC_METADATA = new Field.ComplexArray(&quot;topic_metadata&quot;,&lt;br/&gt;
+            &quot;Metadata for requested topics&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    // cluster level fields&lt;br/&gt;
+    private static final Field.NullableStr CLUSTER_ID = new Field.NullableStr(&quot;cluster_id&quot;,&lt;br/&gt;
+            &quot;The cluster id that this broker belongs to.&quot;);&lt;br/&gt;
+    private static final Field.Int32 CONTROLLER_ID = new Field.Int32(&quot;controller_id&quot;,&lt;br/&gt;
+            &quot;The broker id of the controller broker.&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    // broker level fields&lt;br/&gt;
+    private static final Field.Int32 NODE_ID = new Field.Int32(&quot;node_id&quot;, &quot;The broker id.&quot;);&lt;br/&gt;
+    private static final Field.Str HOST = new Field.Str(&quot;host&quot;, &quot;The hostname of the broker.&quot;);&lt;br/&gt;
+    private static final Field.Int32 PORT = new Field.Int32(&quot;port&quot;, &quot;The port on which the broker accepts requests.&quot;);&lt;br/&gt;
+    private static final Field.NullableStr RACK = new Field.NullableStr(&quot;rack&quot;, &quot;The rack of the broker.&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITION_METADATA = new Field.ComplexArray(&quot;partition_metadata&quot;,&lt;br/&gt;
+            &quot;Metadata for each partition of the topic.&quot;);&lt;br/&gt;
+    private static final Field.Bool IS_INTERNAL = new Field.Bool(&quot;is_internal&quot;,&lt;br/&gt;
+            &quot;Indicates if the topic is considered a Kafka internal topic&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    // partition level fields&lt;br/&gt;
+    private static final Field.Int32 LEADER = new Field.Int32(&quot;leader&quot;,&lt;br/&gt;
+            &quot;The id of the broker acting as leader for this partition.&quot;);&lt;br/&gt;
+    private static final Field.Array REPLICAS = new Field.Array(&quot;replicas&quot;, INT32,&lt;br/&gt;
+            &quot;The set of all nodes that host this partition.&quot;);&lt;br/&gt;
+    private static final Field.Array ISR = new Field.Array(&quot;isr&quot;, INT32,&lt;br/&gt;
+            &quot;The set of nodes that are in sync with the leader for this partition.&quot;);&lt;br/&gt;
+    private static final Field.Array OFFLINE_REPLICAS = new Field.Array(&quot;offline_replicas&quot;, INT32,&lt;br/&gt;
+            &quot;The set of offline replicas of this partition.&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field METADATA_BROKER_V0 = BROKERS.withFields(&lt;br/&gt;
+            NODE_ID,&lt;br/&gt;
+            HOST,&lt;br/&gt;
+            PORT);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field PARTITION_METADATA_V0 = PARTITION_METADATA.withFields(&lt;br/&gt;
             ERROR_CODE,&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(LEADER_KEY_NAME, INT32, &quot;The id of the broker acting as leader for this partition.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(REPLICAS_KEY_NAME, new ArrayOf(INT32), &quot;The set of all nodes that host this partition.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(ISR_KEY_NAME, new ArrayOf(INT32), &quot;The set of nodes that are in sync with the leader for this partition.&quot;));&lt;br/&gt;
+            LEADER,&lt;br/&gt;
+            REPLICAS,&lt;br/&gt;
+            ISR);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema TOPIC_METADATA_V0 = new Schema(&lt;br/&gt;
+    private static final Field TOPIC_METADATA_V0 = TOPIC_METADATA.withFields(&lt;br/&gt;
             ERROR_CODE,&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITION_METADATA_KEY_NAME, new ArrayOf(PARTITION_METADATA_V0), &quot;Metadata for each partition of the topic.&quot;));&lt;br/&gt;
+            PARTITION_METADATA_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema METADATA_RESPONSE_V0 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(BROKERS_KEY_NAME, new ArrayOf(METADATA_BROKER_V0), &quot;Host and port information for all brokers.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPIC_METADATA_KEY_NAME, new ArrayOf(TOPIC_METADATA_V0)));&lt;br/&gt;
+            METADATA_BROKER_V0,&lt;br/&gt;
+            TOPIC_METADATA_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema METADATA_BROKER_V1 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(NODE_ID_KEY_NAME, INT32, &quot;The broker id.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(HOST_KEY_NAME, STRING, &quot;The hostname of the broker.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(PORT_KEY_NAME, INT32, &quot;The port on which the broker accepts requests.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(RACK_KEY_NAME, NULLABLE_STRING, &quot;The rack of the broker.&quot;));&lt;br/&gt;
+    // V1 adds fields for the rack of each broker, the controller id, and whether or not the topic is internal&lt;br/&gt;
+    private static final Field METADATA_BROKER_V1 = BROKERS.withFields(&lt;br/&gt;
+            NODE_ID,&lt;br/&gt;
+            HOST,&lt;br/&gt;
+            PORT,&lt;br/&gt;
+            RACK);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema PARTITION_METADATA_V1 = PARTITION_METADATA_V0;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// PARTITION_METADATA_V2 added a per-partition offline_replicas field. This field specifies the list of replicas that are offline.&lt;/li&gt;
	&lt;li&gt;private static final Schema PARTITION_METADATA_V2 = new Schema(&lt;/li&gt;
	&lt;li&gt;ERROR_CODE,&lt;/li&gt;
	&lt;li&gt;PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(LEADER_KEY_NAME, INT32, &quot;The id of the broker acting as leader for this partition.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(REPLICAS_KEY_NAME, new ArrayOf(INT32), &quot;The set of all nodes that host this partition.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(ISR_KEY_NAME, new ArrayOf(INT32), &quot;The set of nodes that are in sync with the leader for this partition.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(OFFLINE_REPLICAS_KEY_NAME, new ArrayOf(INT32), &quot;The set of offline replicas of this partition.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema TOPIC_METADATA_V1 = new Schema(&lt;br/&gt;
+    private static final Field TOPIC_METADATA_V1 = TOPIC_METADATA.withFields(&lt;br/&gt;
             ERROR_CODE,&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(IS_INTERNAL_KEY_NAME, BOOLEAN, &quot;Indicates if the topic is considered a Kafka internal topic&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(PARTITION_METADATA_KEY_NAME, new ArrayOf(PARTITION_METADATA_V1), &quot;Metadata for each partition of the topic.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// TOPIC_METADATA_V2 added a per-partition offline_replicas field. This field specifies the list of replicas that are offline.&lt;/li&gt;
	&lt;li&gt;private static final Schema TOPIC_METADATA_V2 = new Schema(&lt;/li&gt;
	&lt;li&gt;ERROR_CODE,&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(IS_INTERNAL_KEY_NAME, BOOLEAN, &quot;Indicates if the topic is considered a Kafka internal topic&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(PARTITION_METADATA_KEY_NAME, new ArrayOf(PARTITION_METADATA_V2), &quot;Metadata for each partition of the topic.&quot;));&lt;br/&gt;
+            IS_INTERNAL,&lt;br/&gt;
+            PARTITION_METADATA_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema METADATA_RESPONSE_V1 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(BROKERS_KEY_NAME, new ArrayOf(METADATA_BROKER_V1), &quot;Host and port information for all brokers.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(CONTROLLER_ID_KEY_NAME, INT32, &quot;The broker id of the controller broker.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPIC_METADATA_KEY_NAME, new ArrayOf(TOPIC_METADATA_V1)));&lt;br/&gt;
+            METADATA_BROKER_V1,&lt;br/&gt;
+            CONTROLLER_ID,&lt;br/&gt;
+            TOPIC_METADATA_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V2 added a field for the cluster id&lt;br/&gt;
     private static final Schema METADATA_RESPONSE_V2 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(BROKERS_KEY_NAME, new ArrayOf(METADATA_BROKER_V1), &quot;Host and port information for all brokers.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(CLUSTER_ID_KEY_NAME, NULLABLE_STRING, &quot;The cluster id that this broker belongs to.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(CONTROLLER_ID_KEY_NAME, INT32, &quot;The broker id of the controller broker.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPIC_METADATA_KEY_NAME, new ArrayOf(TOPIC_METADATA_V1)));&lt;br/&gt;
+            METADATA_BROKER_V1,&lt;br/&gt;
+            CLUSTER_ID,&lt;br/&gt;
+            CONTROLLER_ID,&lt;br/&gt;
+            TOPIC_METADATA_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V3 adds the throttle time to the response&lt;br/&gt;
     private static final Schema METADATA_RESPONSE_V3 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(BROKERS_KEY_NAME, new ArrayOf(METADATA_BROKER_V1), &quot;Host and port information for all brokers.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(CLUSTER_ID_KEY_NAME, NULLABLE_STRING, &quot;The cluster id that this broker belongs to.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(CONTROLLER_ID_KEY_NAME, INT32, &quot;The broker id of the controller broker.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPIC_METADATA_KEY_NAME, new ArrayOf(TOPIC_METADATA_V1)));&lt;br/&gt;
+            METADATA_BROKER_V1,&lt;br/&gt;
+            CLUSTER_ID,&lt;br/&gt;
+            CONTROLLER_ID,&lt;br/&gt;
+            TOPIC_METADATA_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema METADATA_RESPONSE_V4 = METADATA_RESPONSE_V3;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// METADATA_RESPONSE_V5 added a per-partition offline_replicas field. This field specifies the list of replicas that are offline.&lt;br/&gt;
+    // V5 added a per-partition offline_replicas field. This field specifies the list of replicas that are offline.&lt;br/&gt;
+    private static final Field PARTITION_METADATA_V5 = PARTITION_METADATA.withFields(&lt;br/&gt;
+            ERROR_CODE,&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            LEADER,&lt;br/&gt;
+            REPLICAS,&lt;br/&gt;
+            ISR,&lt;br/&gt;
+            OFFLINE_REPLICAS);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPIC_METADATA_V5 = TOPIC_METADATA.withFields(&lt;br/&gt;
+            ERROR_CODE,&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            IS_INTERNAL,&lt;br/&gt;
+            PARTITION_METADATA_V5);&lt;br/&gt;
+&lt;br/&gt;
     private static final Schema METADATA_RESPONSE_V5 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;/li&gt;
	&lt;li&gt;new Field(BROKERS_KEY_NAME, new ArrayOf(METADATA_BROKER_V1), &quot;Host and port information for all brokers.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(CLUSTER_ID_KEY_NAME, NULLABLE_STRING, &quot;The cluster id that this broker belongs to.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(CONTROLLER_ID_KEY_NAME, INT32, &quot;The broker id of the controller broker.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPIC_METADATA_KEY_NAME, new ArrayOf(TOPIC_METADATA_V2)));&lt;br/&gt;
+            METADATA_BROKER_V1,&lt;br/&gt;
+            CLUSTER_ID,&lt;br/&gt;
+            CONTROLLER_ID,&lt;br/&gt;
+            TOPIC_METADATA_V5);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V6 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema METADATA_RESPONSE_V6 = METADATA_RESPONSE_V5;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V7 adds the leader epoch to the partition metadata&lt;br/&gt;
+    private static final Field PARTITION_METADATA_V7 = PARTITION_METADATA.withFields(&lt;br/&gt;
+            ERROR_CODE,&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            LEADER,&lt;br/&gt;
+            LEADER_EPOCH,&lt;br/&gt;
+            REPLICAS,&lt;br/&gt;
+            ISR,&lt;br/&gt;
+            OFFLINE_REPLICAS);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPIC_METADATA_V7 = TOPIC_METADATA.withFields(&lt;br/&gt;
+            ERROR_CODE,&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            IS_INTERNAL,&lt;br/&gt;
+            PARTITION_METADATA_V7);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Schema METADATA_RESPONSE_V7 = new Schema(&lt;br/&gt;
+            THROTTLE_TIME_MS,&lt;br/&gt;
+            METADATA_BROKER_V1,&lt;br/&gt;
+            CLUSTER_ID,&lt;br/&gt;
+            CONTROLLER_ID,&lt;br/&gt;
+            TOPIC_METADATA_V7);&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{METADATA_RESPONSE_V0, METADATA_RESPONSE_V1, METADATA_RESPONSE_V2, METADATA_RESPONSE_V3,
-            METADATA_RESPONSE_V4, METADATA_RESPONSE_V5, METADATA_RESPONSE_V6}
&lt;p&gt;;&lt;br/&gt;
+            METADATA_RESPONSE_V4, METADATA_RESPONSE_V5, METADATA_RESPONSE_V6, METADATA_RESPONSE_V7};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private final int throttleTimeMs;&lt;br/&gt;
@@ -198,62 +224,56 @@ public MetadataResponse(int throttleTimeMs, List&amp;lt;Node&amp;gt; brokers, String clusterId&lt;br/&gt;
     public MetadataResponse(Struct struct) {&lt;br/&gt;
         this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);&lt;br/&gt;
         Map&amp;lt;Integer, Node&amp;gt; brokers = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object[] brokerStructs = (Object[]) struct.get(BROKERS_KEY_NAME);&lt;br/&gt;
+        Object[] brokerStructs = struct.get(BROKERS);&lt;br/&gt;
         for (Object brokerStruct : brokerStructs) 
{
             Struct broker = (Struct) brokerStruct;
-            int nodeId = broker.getInt(NODE_ID_KEY_NAME);
-            String host = broker.getString(HOST_KEY_NAME);
-            int port = broker.getInt(PORT_KEY_NAME);
+            int nodeId = broker.get(NODE_ID);
+            String host = broker.get(HOST);
+            int port = broker.get(PORT);
             // This field only exists in v1+
             // When we can&apos;t know if a rack exists in a v0 response we default to null
-            String rack = broker.hasField(RACK_KEY_NAME) ? broker.getString(RACK_KEY_NAME) : null;
+            String rack = broker.getOrElse(RACK, null);
             brokers.put(nodeId, new Node(nodeId, host, port, rack));
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // This field only exists in v1+&lt;br/&gt;
         // When we can&apos;t know the controller id in a v0 response we default to NO_CONTROLLER_ID&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int controllerId = NO_CONTROLLER_ID;&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(CONTROLLER_ID_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;controllerId = struct.getInt(CONTROLLER_ID_KEY_NAME);&lt;br/&gt;
+        int controllerId = struct.getOrElse(CONTROLLER_ID, NO_CONTROLLER_ID);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // This field only exists in v2+&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (struct.hasField(CLUSTER_ID_KEY_NAME)) 
{
-            this.clusterId = struct.getString(CLUSTER_ID_KEY_NAME);
-        }
&lt;p&gt; else &lt;/p&gt;
{
-            this.clusterId = null;
-        }
&lt;p&gt;+        this.clusterId = struct.getOrElse(CLUSTER_ID, null);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;TopicMetadata&amp;gt; topicMetadata = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object[] topicInfos = (Object[]) struct.get(TOPIC_METADATA_KEY_NAME);&lt;br/&gt;
+        Object[] topicInfos = struct.get(TOPIC_METADATA);&lt;br/&gt;
         for (Object topicInfoObj : topicInfos) {&lt;br/&gt;
             Struct topicInfo = (Struct) topicInfoObj;&lt;br/&gt;
             Errors topicError = Errors.forCode(topicInfo.get(ERROR_CODE));&lt;br/&gt;
             String topic = topicInfo.get(TOPIC_NAME);&lt;br/&gt;
             // This field only exists in v1+&lt;br/&gt;
             // When we can&apos;t know if a topic is internal or not in a v0 response we default to false&lt;/li&gt;
	&lt;li&gt;boolean isInternal = topicInfo.hasField(IS_INTERNAL_KEY_NAME) ? topicInfo.getBoolean(IS_INTERNAL_KEY_NAME) : false;&lt;br/&gt;
-&lt;br/&gt;
+            boolean isInternal = topicInfo.getOrElse(IS_INTERNAL, false);&lt;br/&gt;
             List&amp;lt;PartitionMetadata&amp;gt; partitionMetadata = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object[] partitionInfos = (Object[]) topicInfo.get(PARTITION_METADATA_KEY_NAME);&lt;br/&gt;
+            Object[] partitionInfos = topicInfo.get(PARTITION_METADATA);&lt;br/&gt;
             for (Object partitionInfoObj : partitionInfos) 
{
                 Struct partitionInfo = (Struct) partitionInfoObj;
                 Errors partitionError = Errors.forCode(partitionInfo.get(ERROR_CODE));
                 int partition = partitionInfo.get(PARTITION_ID);
-                int leader = partitionInfo.getInt(LEADER_KEY_NAME);
+                int leader = partitionInfo.get(LEADER);
+                Optional&amp;lt;Integer&amp;gt; leaderEpoch = RequestUtils.getLeaderEpoch(partitionInfo, LEADER_EPOCH);
                 Node leaderNode = leader == -1 ? null : brokers.get(leader);
 
-                Object[] replicas = (Object[]) partitionInfo.get(REPLICAS_KEY_NAME);
+                Object[] replicas = partitionInfo.get(REPLICAS);
                 List&amp;lt;Node&amp;gt; replicaNodes = convertToNodes(brokers, replicas);
 
-                Object[] isr = (Object[]) partitionInfo.get(ISR_KEY_NAME);
+                Object[] isr = partitionInfo.get(ISR);
                 List&amp;lt;Node&amp;gt; isrNodes = convertToNodes(brokers, isr);
 
-                Object[] offlineReplicas = partitionInfo.hasField(OFFLINE_REPLICAS_KEY_NAME) ?
-                    (Object[]) partitionInfo.get(OFFLINE_REPLICAS_KEY_NAME) : new Object[0];
+                Object[] offlineReplicas = partitionInfo.getOrEmpty(OFFLINE_REPLICAS);
                 List&amp;lt;Node&amp;gt; offlineNodes = convertToNodes(brokers, offlineReplicas);
 
-                partitionMetadata.add(new PartitionMetadata(partitionError, partition, leaderNode, replicaNodes, isrNodes, offlineNodes));
+                partitionMetadata.add(new PartitionMetadata(partitionError, partition, leaderNode, leaderEpoch,
+                        replicaNodes, isrNodes, offlineNodes));
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             topicMetadata.add(new TopicMetadata(topicError, topic, isInternal, partitionMetadata));&lt;br/&gt;
@@ -354,7 +374,7 @@ public Cluster cluster() {&lt;br/&gt;
             if (metadata.error == Errors.NONE) {&lt;br/&gt;
                 if (metadata.isInternal)&lt;br/&gt;
                     internalTopics.add(metadata.topic);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (PartitionMetadata partitionMetadata : metadata.partitionMetadata)&lt;br/&gt;
+                for (PartitionMetadata partitionMetadata : metadata.partitionMetadata) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                     partitions.add(new PartitionInfo(                             metadata.topic,                             partitionMetadata.partition,@@ -362,6 +382,7 @@ public Cluster cluster() {
                             partitionMetadata.replicas.toArray(new Node[0]),
                             partitionMetadata.isr.toArray(new Node[0]),
                             partitionMetadata.offlineReplicas.toArray(new Node[0])));
+                }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -452,6 +473,7 @@ public String toString() {&lt;br/&gt;
         private final Errors error;&lt;br/&gt;
         private final int partition;&lt;br/&gt;
         private final Node leader;&lt;br/&gt;
+        private final Optional&amp;lt;Integer&amp;gt; leaderEpoch;&lt;br/&gt;
         private final List&amp;lt;Node&amp;gt; replicas;&lt;br/&gt;
         private final List&amp;lt;Node&amp;gt; isr;&lt;br/&gt;
         private final List&amp;lt;Node&amp;gt; offlineReplicas;&lt;br/&gt;
@@ -459,12 +481,14 @@ public String toString() {&lt;br/&gt;
         public PartitionMetadata(Errors error,&lt;br/&gt;
                                  int partition,&lt;br/&gt;
                                  Node leader,&lt;br/&gt;
+                                 Optional&amp;lt;Integer&amp;gt; leaderEpoch,&lt;br/&gt;
                                  List&amp;lt;Node&amp;gt; replicas,&lt;br/&gt;
                                  List&amp;lt;Node&amp;gt; isr,&lt;br/&gt;
                                  List&amp;lt;Node&amp;gt; offlineReplicas) {&lt;br/&gt;
             this.error = error;&lt;br/&gt;
             this.partition = partition;&lt;br/&gt;
             this.leader = leader;&lt;br/&gt;
+            this.leaderEpoch = leaderEpoch;&lt;br/&gt;
             this.replicas = replicas;&lt;br/&gt;
             this.isr = isr;&lt;br/&gt;
             this.offlineReplicas = offlineReplicas;&lt;br/&gt;
@@ -482,6 +506,10 @@ public int leaderId() &lt;/p&gt;
{
             return leader == null ? -1 : leader.id();
         }

&lt;p&gt;+        public Optional&amp;lt;Integer&amp;gt; leaderEpoch() &lt;/p&gt;
{
+            return leaderEpoch;
+        }
&lt;p&gt;+&lt;br/&gt;
         public Node leader() &lt;/p&gt;
{
             return leader;
         }
&lt;p&gt;@@ -504,6 +532,7 @@ public String toString() {&lt;br/&gt;
                     &quot;, error=&quot; + error +&lt;br/&gt;
                     &quot;, partition=&quot; + partition +&lt;br/&gt;
                     &quot;, leader=&quot; + leader +&lt;br/&gt;
+                    &quot;, leaderEpoch=&quot; + leaderEpoch +&lt;br/&gt;
                     &quot;, replicas=&quot; + Utils.join(replicas, &quot;,&quot;) +&lt;br/&gt;
                     &quot;, isr=&quot; + Utils.join(isr, &quot;,&quot;) +&lt;br/&gt;
                     &quot;, offlineReplicas=&quot; + Utils.join(offlineReplicas, &quot;,&quot;) + &apos;)&apos;;&lt;br/&gt;
@@ -516,61 +545,61 @@ protected Struct toStruct(short version) {&lt;br/&gt;
         struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; brokerArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Node node : brokers) &lt;/p&gt;
{
-            Struct broker = struct.instance(BROKERS_KEY_NAME);
-            broker.set(NODE_ID_KEY_NAME, node.id());
-            broker.set(HOST_KEY_NAME, node.host());
-            broker.set(PORT_KEY_NAME, node.port());
+            Struct broker = struct.instance(BROKERS);
+            broker.set(NODE_ID, node.id());
+            broker.set(HOST, node.host());
+            broker.set(PORT, node.port());
             // This field only exists in v1+
-            if (broker.hasField(RACK_KEY_NAME))
-                broker.set(RACK_KEY_NAME, node.rack());
+            broker.setIfExists(RACK, node.rack());
             brokerArray.add(broker);
         }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;struct.set(BROKERS_KEY_NAME, brokerArray.toArray());&lt;br/&gt;
+        struct.set(BROKERS, brokerArray.toArray());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // This field only exists in v1+&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (struct.hasField(CONTROLLER_ID_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;struct.set(CONTROLLER_ID_KEY_NAME, controller == null ? NO_CONTROLLER_ID : controller.id());&lt;br/&gt;
+        struct.setIfExists(CONTROLLER_ID, controller == null ? NO_CONTROLLER_ID : controller.id());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // This field only exists in v2+&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (struct.hasField(CLUSTER_ID_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;struct.set(CLUSTER_ID_KEY_NAME, clusterId);&lt;br/&gt;
+        struct.setIfExists(CLUSTER_ID, clusterId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;Struct&amp;gt; topicMetadataArray = new ArrayList&amp;lt;&amp;gt;(topicMetadata.size());&lt;br/&gt;
         for (TopicMetadata metadata : topicMetadata) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Struct topicData = struct.instance(TOPIC_METADATA_KEY_NAME);&lt;br/&gt;
+            Struct topicData = struct.instance(TOPIC_METADATA);&lt;br/&gt;
             topicData.set(TOPIC_NAME, metadata.topic);&lt;br/&gt;
             topicData.set(ERROR_CODE, metadata.error.code());&lt;br/&gt;
             // This field only exists in v1+&lt;/li&gt;
	&lt;li&gt;if (topicData.hasField(IS_INTERNAL_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;topicData.set(IS_INTERNAL_KEY_NAME, metadata.isInternal());&lt;br/&gt;
+            topicData.setIfExists(IS_INTERNAL, metadata.isInternal());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             List&amp;lt;Struct&amp;gt; partitionMetadataArray = new ArrayList&amp;lt;&amp;gt;(metadata.partitionMetadata.size());&lt;br/&gt;
             for (PartitionMetadata partitionMetadata : metadata.partitionMetadata()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Struct partitionData = topicData.instance(PARTITION_METADATA_KEY_NAME);&lt;br/&gt;
+                Struct partitionData = topicData.instance(PARTITION_METADATA);&lt;br/&gt;
                 partitionData.set(ERROR_CODE, partitionMetadata.error.code());&lt;br/&gt;
                 partitionData.set(PARTITION_ID, partitionMetadata.partition);&lt;/li&gt;
	&lt;li&gt;partitionData.set(LEADER_KEY_NAME, partitionMetadata.leaderId());&lt;br/&gt;
+                partitionData.set(LEADER, partitionMetadata.leaderId());&lt;br/&gt;
+&lt;br/&gt;
+                // Leader epoch exists in v7 forward&lt;br/&gt;
+                RequestUtils.setLeaderEpochIfExists(partitionData, LEADER_EPOCH, partitionMetadata.leaderEpoch);&lt;br/&gt;
+&lt;br/&gt;
                 ArrayList&amp;lt;Integer&amp;gt; replicas = new ArrayList&amp;lt;&amp;gt;(partitionMetadata.replicas.size());&lt;br/&gt;
                 for (Node node : partitionMetadata.replicas)&lt;br/&gt;
                     replicas.add(node.id());&lt;/li&gt;
	&lt;li&gt;partitionData.set(REPLICAS_KEY_NAME, replicas.toArray());&lt;br/&gt;
+                partitionData.set(REPLICAS, replicas.toArray());&lt;br/&gt;
                 ArrayList&amp;lt;Integer&amp;gt; isr = new ArrayList&amp;lt;&amp;gt;(partitionMetadata.isr.size());&lt;br/&gt;
                 for (Node node : partitionMetadata.isr)&lt;br/&gt;
                     isr.add(node.id());&lt;/li&gt;
	&lt;li&gt;partitionData.set(ISR_KEY_NAME, isr.toArray());&lt;/li&gt;
	&lt;li&gt;if (partitionData.hasField(OFFLINE_REPLICAS_KEY_NAME)) {&lt;br/&gt;
+                partitionData.set(ISR, isr.toArray());&lt;br/&gt;
+                if (partitionData.hasField(OFFLINE_REPLICAS)) 
{
                     ArrayList&amp;lt;Integer&amp;gt; offlineReplicas = new ArrayList&amp;lt;&amp;gt;(partitionMetadata.offlineReplicas.size());
                     for (Node node : partitionMetadata.offlineReplicas)
                         offlineReplicas.add(node.id());
-                    partitionData.set(OFFLINE_REPLICAS_KEY_NAME, offlineReplicas.toArray());
+                    partitionData.set(OFFLINE_REPLICAS, offlineReplicas.toArray());
                 }
&lt;p&gt;                 partitionMetadataArray.add(partitionData);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;topicData.set(PARTITION_METADATA_KEY_NAME, partitionMetadataArray.toArray());&lt;br/&gt;
+            topicData.set(PARTITION_METADATA, partitionMetadataArray.toArray());&lt;br/&gt;
             topicMetadataArray.add(topicData);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;struct.set(TOPIC_METADATA_KEY_NAME, topicMetadataArray.toArray());&lt;br/&gt;
+        struct.set(TOPIC_METADATA, topicMetadataArray.toArray());&lt;br/&gt;
         return struct;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java&lt;br/&gt;
index 8a51e84e76a..fdb5b108199 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java&lt;br/&gt;
@@ -17,10 +17,8 @@&lt;br/&gt;
 package org.apache.kafka.common.requests;&lt;/p&gt;

&lt;p&gt; import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
-import org.apache.kafka.common.errors.UnsupportedVersionException;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -31,95 +29,112 @@&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;/p&gt;

&lt;p&gt;+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_LEADER_EPOCH;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_METADATA;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_OFFSET;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.GENERATION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.GROUP_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.MEMBER_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.NULLABLE_STRING;&lt;/p&gt;

&lt;p&gt;-/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* This wrapper supports both v0 and v1 of OffsetCommitRequest.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
 public class OffsetCommitRequest extends AbstractRequest {&lt;/li&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String RETENTION_TIME_KEY_NAME = &quot;retention_time&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// topic level field names&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;br/&gt;
+    // top level fields&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;Topics to commit offsets&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// partition level field names&lt;/li&gt;
	&lt;li&gt;private static final String COMMIT_OFFSET_KEY_NAME = &quot;offset&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String METADATA_KEY_NAME = &quot;metadata&quot;;&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;Partitions to commit offsets&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // partition level fields&lt;br/&gt;
     @Deprecated&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TIMESTAMP_KEY_NAME = &quot;timestamp&quot;;         // for v0, v1&lt;br/&gt;
+    private static final Field.Int64 COMMIT_TIMESTAMP = new Field.Int64(&quot;timestamp&quot;, &quot;Timestamp of the commit&quot;);&lt;br/&gt;
+    private static final Field.Int64 RETENTION_TIME = new Field.Int64(&quot;retention_time&quot;,&lt;br/&gt;
+            &quot;Time period in ms to retain the offset.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* Offset commit api */&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_REQUEST_PARTITION_V0 = new Schema(&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(COMMIT_OFFSET_KEY_NAME, INT64, &quot;Message offset to be committed.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(METADATA_KEY_NAME, NULLABLE_STRING, &quot;Any associated metadata the client wants to keep.&quot;));&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMITTED_METADATA);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_REQUEST_PARTITION_V1 = new Schema(&lt;/li&gt;
	&lt;li&gt;PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(COMMIT_OFFSET_KEY_NAME, INT64, &quot;Message offset to be committed.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TIMESTAMP_KEY_NAME, INT64, &quot;Timestamp of the commit&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(METADATA_KEY_NAME, NULLABLE_STRING, &quot;Any associated metadata the client wants to keep.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_REQUEST_PARTITION_V2 = new Schema(&lt;/li&gt;
	&lt;li&gt;PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(COMMIT_OFFSET_KEY_NAME, INT64, &quot;Message offset to be committed.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(METADATA_KEY_NAME, NULLABLE_STRING, &quot;Any associated metadata the client wants to keep.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_REQUEST_TOPIC_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_PARTITION_V0), &quot;Partitions to commit offsets.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_REQUEST_TOPIC_V1 = new Schema(&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_PARTITION_V1), &quot;Partitions to commit offsets.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_REQUEST_TOPIC_V2 = new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_PARTITION_V2), &quot;Partitions to commit offsets.&quot;));&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema OFFSET_COMMIT_REQUEST_V0 = new Schema(&lt;br/&gt;
             GROUP_ID,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_TOPIC_V0), &quot;Topics to commit offsets.&quot;));&lt;br/&gt;
+            TOPICS_V0);&lt;br/&gt;
+&lt;br/&gt;
+    // V1 adds timestamp and group membership information (generation and memberId)&lt;br/&gt;
+    private static final Field PARTITIONS_V1 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMIT_TIMESTAMP,&lt;br/&gt;
+            COMMITTED_METADATA);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V1 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema OFFSET_COMMIT_REQUEST_V1 = new Schema(&lt;br/&gt;
             GROUP_ID,&lt;br/&gt;
             GENERATION_ID,&lt;br/&gt;
             MEMBER_ID,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_TOPIC_V1), &quot;Topics to commit offsets.&quot;));&lt;br/&gt;
+            TOPICS_V1);&lt;br/&gt;
+&lt;br/&gt;
+    // V2 adds retention time&lt;br/&gt;
+    private static final Field PARTITIONS_V2 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMITTED_METADATA);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V2 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V2);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema OFFSET_COMMIT_REQUEST_V2 = new Schema(&lt;br/&gt;
             GROUP_ID,&lt;br/&gt;
             GENERATION_ID,&lt;br/&gt;
             MEMBER_ID,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(RETENTION_TIME_KEY_NAME, INT64, &quot;Time period in ms to retain the offset.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_TOPIC_V2), &quot;Topics to commit offsets.&quot;));&lt;br/&gt;
+            RETENTION_TIME,&lt;br/&gt;
+            TOPICS_V2);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* v3 request is same as v2. Throttle time has been added to response */&lt;br/&gt;
+    // V3 adds throttle time&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_REQUEST_V3 = OFFSET_COMMIT_REQUEST_V2;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V4 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_REQUEST_V4 = OFFSET_COMMIT_REQUEST_V3;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V5 removes the retention time which is now controlled only by a broker configuration&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_REQUEST_V5 = new Schema(&lt;br/&gt;
             GROUP_ID,&lt;br/&gt;
             GENERATION_ID,&lt;br/&gt;
             MEMBER_ID,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_REQUEST_TOPIC_V2), &quot;Topics to commit offsets.&quot;));&lt;br/&gt;
+            TOPICS_V2);&lt;br/&gt;
+&lt;br/&gt;
+    // V6 adds the leader epoch to the partition data&lt;br/&gt;
+    private static final Field PARTITIONS_V6 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMITTED_LEADER_EPOCH,&lt;br/&gt;
+            COMMITTED_METADATA);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V6 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V6);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Schema OFFSET_COMMIT_REQUEST_V6 = new Schema(&lt;br/&gt;
+            GROUP_ID,&lt;br/&gt;
+            GENERATION_ID,&lt;br/&gt;
+            MEMBER_ID,&lt;br/&gt;
+            TOPICS_V6);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{OFFSET_COMMIT_REQUEST_V0, OFFSET_COMMIT_REQUEST_V1, OFFSET_COMMIT_REQUEST_V2,
-            OFFSET_COMMIT_REQUEST_V3, OFFSET_COMMIT_REQUEST_V4, OFFSET_COMMIT_REQUEST_V5}
&lt;p&gt;;&lt;br/&gt;
+            OFFSET_COMMIT_REQUEST_V3, OFFSET_COMMIT_REQUEST_V4, OFFSET_COMMIT_REQUEST_V5, OFFSET_COMMIT_REQUEST_V6};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     // default values for the current version&lt;br/&gt;
@@ -144,25 +159,32 @@&lt;/p&gt;

&lt;p&gt;         public final long offset;&lt;br/&gt;
         public final String metadata;&lt;br/&gt;
+        public final Optional&amp;lt;Integer&amp;gt; leaderEpoch;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Deprecated&lt;/li&gt;
	&lt;li&gt;public PartitionData(long offset, long timestamp, String metadata) {&lt;br/&gt;
+        private PartitionData(long offset, Optional&amp;lt;Integer&amp;gt; leaderEpoch, long timestamp, String metadata) 
{
             this.offset = offset;
+            this.leaderEpoch = leaderEpoch;
             this.timestamp = timestamp;
             this.metadata = metadata;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public PartitionData(long offset, String metadata) {&lt;/li&gt;
	&lt;li&gt;this(offset, DEFAULT_TIMESTAMP, metadata);&lt;br/&gt;
+        @Deprecated&lt;br/&gt;
+        public PartitionData(long offset, long timestamp, String metadata) 
{
+            this(offset, Optional.empty(), timestamp, metadata);
+        }
&lt;p&gt;+&lt;br/&gt;
+        public PartitionData(long offset, Optional&amp;lt;Integer&amp;gt; leaderEpoch, String metadata) &lt;/p&gt;
{
+            this(offset, leaderEpoch, DEFAULT_TIMESTAMP, metadata);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public String toString() &lt;/p&gt;
{
             StringBuilder bld = new StringBuilder();
             bld.append(&quot;(timestamp=&quot;).append(timestamp).
-                append(&quot;, offset=&quot;).append(offset).
-                append(&quot;, metadata=&quot;).append(metadata).
-                append(&quot;)&quot;);
+                    append(&quot;, offset=&quot;).append(offset).
+                    append(&quot;, leaderEpoch=&quot;).append(leaderEpoch).
+                    append(&quot;, metadata=&quot;).append(metadata).
+                    append(&quot;)&quot;);
             return bld.toString();
         }
&lt;p&gt;     }&lt;br/&gt;
@@ -191,18 +213,12 @@ public Builder setGenerationId(int generationId) {&lt;/p&gt;

&lt;p&gt;         @Override&lt;br/&gt;
         public OffsetCommitRequest build(short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;switch (version) {&lt;/li&gt;
	&lt;li&gt;case 0:&lt;/li&gt;
	&lt;li&gt;return new OffsetCommitRequest(groupId, DEFAULT_GENERATION_ID, DEFAULT_MEMBER_ID,&lt;/li&gt;
	&lt;li&gt;DEFAULT_RETENTION_TIME, offsetData, version);&lt;/li&gt;
	&lt;li&gt;case 1:&lt;/li&gt;
	&lt;li&gt;case 2:&lt;/li&gt;
	&lt;li&gt;case 3:&lt;/li&gt;
	&lt;li&gt;case 4:&lt;/li&gt;
	&lt;li&gt;case 5:&lt;/li&gt;
	&lt;li&gt;return new OffsetCommitRequest(groupId, generationId, memberId, DEFAULT_RETENTION_TIME, offsetData, version);&lt;/li&gt;
	&lt;li&gt;default:&lt;/li&gt;
	&lt;li&gt;throw new UnsupportedVersionException(&quot;Unsupported version &quot; + version);&lt;br/&gt;
+            if (version == 0) 
{
+                return new OffsetCommitRequest(groupId, DEFAULT_GENERATION_ID, DEFAULT_MEMBER_ID,
+                        DEFAULT_RETENTION_TIME, offsetData, version);
+            }
&lt;p&gt; else &lt;/p&gt;
{
+                return new OffsetCommitRequest(groupId, generationId, memberId, DEFAULT_RETENTION_TIME,
+                        offsetData, version);
             }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -221,7 +237,7 @@ public String toString() {&lt;/p&gt;

&lt;p&gt;     private OffsetCommitRequest(String groupId, int generationId, String memberId, long retentionTime,&lt;br/&gt;
                                 Map&amp;lt;TopicPartition, PartitionData&amp;gt; offsetData, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.OFFSET_COMMIT, version);
         this.groupId = groupId;
         this.generationId = generationId;
         this.memberId = memberId;
@@ -230,7 +246,7 @@ private OffsetCommitRequest(String groupId, int generationId, String memberId, l
     }

&lt;p&gt;     public OffsetCommitRequest(Struct struct, short versionId) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(versionId);&lt;br/&gt;
+        super(ApiKeys.OFFSET_COMMIT, versionId);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         groupId = struct.get(GROUP_ID);&lt;/p&gt;

&lt;p&gt;@@ -239,27 +255,26 @@ public OffsetCommitRequest(Struct struct, short versionId) {&lt;br/&gt;
         memberId = struct.getOrElse(MEMBER_ID, DEFAULT_MEMBER_ID);&lt;/p&gt;

&lt;p&gt;         // This field only exists in v2&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (struct.hasField(RETENTION_TIME_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;retentionTime = struct.getLong(RETENTION_TIME_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;retentionTime = DEFAULT_RETENTION_TIME;&lt;br/&gt;
+        retentionTime = struct.getOrElse(RETENTION_TIME, DEFAULT_RETENTION_TIME);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         offsetData = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Object topicDataObj : struct.getArray(TOPICS_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicDataObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicData = (Struct) topicDataObj;&lt;br/&gt;
             String topic = topicData.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionDataObj : topicData.getArray(PARTITIONS_KEY_NAME)) {&lt;br/&gt;
+            for (Object partitionDataObj : topicData.get(PARTITIONS)) {&lt;br/&gt;
                 Struct partitionDataStruct = (Struct) partitionDataObj;&lt;br/&gt;
                 int partition = partitionDataStruct.get(PARTITION_ID);&lt;/li&gt;
	&lt;li&gt;long offset = partitionDataStruct.getLong(COMMIT_OFFSET_KEY_NAME);&lt;/li&gt;
	&lt;li&gt;String metadata = partitionDataStruct.getString(METADATA_KEY_NAME);&lt;br/&gt;
+                long offset = partitionDataStruct.get(COMMITTED_OFFSET);&lt;br/&gt;
+                String metadata = partitionDataStruct.get(COMMITTED_METADATA);&lt;br/&gt;
                 PartitionData partitionOffset;&lt;br/&gt;
                 // This field only exists in v1&lt;/li&gt;
	&lt;li&gt;if (partitionDataStruct.hasField(TIMESTAMP_KEY_NAME)) {&lt;/li&gt;
	&lt;li&gt;long timestamp = partitionDataStruct.getLong(TIMESTAMP_KEY_NAME);&lt;br/&gt;
+                if (partitionDataStruct.hasField(COMMIT_TIMESTAMP)) 
{
+                    long timestamp = partitionDataStruct.get(COMMIT_TIMESTAMP);
                     partitionOffset = new PartitionData(offset, timestamp, metadata);
                 }
&lt;p&gt; else &lt;/p&gt;
{
-                    partitionOffset = new PartitionData(offset, metadata);
+                    Optional&amp;lt;Integer&amp;gt; leaderEpochOpt = RequestUtils.getLeaderEpoch(partitionDataStruct,
+                            COMMITTED_LEADER_EPOCH);
+                    partitionOffset = new PartitionData(offset, leaderEpochOpt, metadata);
                 }
&lt;p&gt;                 offsetData.put(new TopicPartition(topic, partition), partitionOffset);&lt;br/&gt;
             }&lt;br/&gt;
@@ -272,31 +287,31 @@ public Struct toStruct() {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.OFFSET_COMMIT.requestSchema(version));&lt;br/&gt;
         struct.set(GROUP_ID, groupId);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsData = CollectionUtils.groupDataByTopic(offsetData);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsData = CollectionUtils.groupPartitionDataByTopic(offsetData);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicEntry: topicsData.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct topicData = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+            Struct topicData = struct.instance(TOPICS);&lt;br/&gt;
             topicData.set(TOPIC_NAME, topicEntry.getKey());&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, PartitionData&amp;gt; partitionEntry : topicEntry.getValue().entrySet()) 
{
                 PartitionData fetchPartitionData = partitionEntry.getValue();
-                Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);
+                Struct partitionData = topicData.instance(PARTITIONS);
                 partitionData.set(PARTITION_ID, partitionEntry.getKey());
-                partitionData.set(COMMIT_OFFSET_KEY_NAME, fetchPartitionData.offset);
+                partitionData.set(COMMITTED_OFFSET, fetchPartitionData.offset);
                 // Only for v1
-                if (partitionData.hasField(TIMESTAMP_KEY_NAME))
-                    partitionData.set(TIMESTAMP_KEY_NAME, fetchPartitionData.timestamp);
-                partitionData.set(METADATA_KEY_NAME, fetchPartitionData.metadata);
+                partitionData.setIfExists(COMMIT_TIMESTAMP, fetchPartitionData.timestamp);
+                // Only for v6
+                RequestUtils.setLeaderEpochIfExists(partitionData, COMMITTED_LEADER_EPOCH, fetchPartitionData.leaderEpoch);
+                partitionData.set(COMMITTED_METADATA, fetchPartitionData.metadata);
                 partitionArray.add(partitionData);
             }&lt;/li&gt;
	&lt;li&gt;topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());&lt;br/&gt;
+            topicData.set(PARTITIONS, partitionArray.toArray());&lt;br/&gt;
             topicArray.add(topicData);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;struct.set(TOPICS_KEY_NAME, topicArray.toArray());&lt;br/&gt;
+        struct.set(TOPICS, topicArray.toArray());&lt;br/&gt;
         struct.setIfExists(GENERATION_ID, generationId);&lt;br/&gt;
         struct.setIfExists(MEMBER_ID, memberId);&lt;/li&gt;
	&lt;li&gt;if (struct.hasField(RETENTION_TIME_KEY_NAME))&lt;/li&gt;
	&lt;li&gt;struct.set(RETENTION_TIME_KEY_NAME, retentionTime);&lt;br/&gt;
+        struct.setIfExists(RETENTION_TIME, retentionTime);&lt;br/&gt;
         return struct;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -316,6 +331,7 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {&lt;br/&gt;
             case 3:&lt;br/&gt;
             case 4:&lt;br/&gt;
             case 5:&lt;br/&gt;
+            case 6:&lt;br/&gt;
                 return new OffsetCommitResponse(throttleTimeMs, responseData);&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalArgumentException(String.format(&quot;Version %d is not valid. Valid versions for %s are 0 to %d&quot;,&lt;br/&gt;
@@ -335,6 +351,7 @@ public String memberId() &lt;/p&gt;
{
         return memberId;
     }

&lt;p&gt;+    @Deprecated&lt;br/&gt;
     public long retentionTime() &lt;/p&gt;
{
         return retentionTime;
     }
&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java&lt;br/&gt;
index c79bc57885b..4d724a32813 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -36,60 +35,64 @@&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;/p&gt;

&lt;p&gt;+/**&lt;br/&gt;
+ * Possible error codes:&lt;br/&gt;
+ *&lt;br/&gt;
+ * UNKNOWN_TOPIC_OR_PARTITION (3)&lt;br/&gt;
+ * REQUEST_TIMED_OUT (7)&lt;br/&gt;
+ * OFFSET_METADATA_TOO_LARGE (12)&lt;br/&gt;
+ * COORDINATOR_LOAD_IN_PROGRESS (14)&lt;br/&gt;
+ * GROUP_COORDINATOR_NOT_AVAILABLE (15)&lt;br/&gt;
+ * NOT_COORDINATOR (16)&lt;br/&gt;
+ * ILLEGAL_GENERATION (22)&lt;br/&gt;
+ * UNKNOWN_MEMBER_ID (25)&lt;br/&gt;
+ * REBALANCE_IN_PROGRESS (27)&lt;br/&gt;
+ * INVALID_COMMIT_OFFSET_SIZE (28)&lt;br/&gt;
+ * TOPIC_AUTHORIZATION_FAILED (29)&lt;br/&gt;
+ * GROUP_AUTHORIZATION_FAILED (30)&lt;br/&gt;
+ */&lt;br/&gt;
 public class OffsetCommitResponse extends AbstractResponse {&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String RESPONSES_KEY_NAME = &quot;responses&quot;;&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;responses&quot;,&lt;br/&gt;
+            &quot;Responses by topic for committed partitions&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // topic level fields&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partition_responses&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Possible error codes:&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* UNKNOWN_TOPIC_OR_PARTITION (3)&lt;/li&gt;
	&lt;li&gt;* REQUEST_TIMED_OUT (7)&lt;/li&gt;
	&lt;li&gt;* OFFSET_METADATA_TOO_LARGE (12)&lt;/li&gt;
	&lt;li&gt;* COORDINATOR_LOAD_IN_PROGRESS (14)&lt;/li&gt;
	&lt;li&gt;* GROUP_COORDINATOR_NOT_AVAILABLE (15)&lt;/li&gt;
	&lt;li&gt;* NOT_COORDINATOR (16)&lt;/li&gt;
	&lt;li&gt;* ILLEGAL_GENERATION (22)&lt;/li&gt;
	&lt;li&gt;* UNKNOWN_MEMBER_ID (25)&lt;/li&gt;
	&lt;li&gt;* REBALANCE_IN_PROGRESS (27)&lt;/li&gt;
	&lt;li&gt;* INVALID_COMMIT_OFFSET_SIZE (28)&lt;/li&gt;
	&lt;li&gt;* TOPIC_AUTHORIZATION_FAILED (29)&lt;/li&gt;
	&lt;li&gt;* GROUP_AUTHORIZATION_FAILED (30)&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_RESPONSE_PARTITION_V0 = new Schema(&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partition_responses&quot;,&lt;br/&gt;
+            &quot;Responses for committed partitions&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             ERROR_CODE);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema OFFSET_COMMIT_RESPONSE_TOPIC_V0 = new Schema(&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_COMMIT_RESPONSE_PARTITION_V0)));&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema OFFSET_COMMIT_RESPONSE_V0 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(OFFSET_COMMIT_RESPONSE_TOPIC_V0)));&lt;br/&gt;
+            TOPICS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* The response types for V0, V1 and V2 of OFFSET_COMMIT_REQUEST are the same. */&lt;br/&gt;
+    // V1 adds timestamp and group membership information (generation and memberId) to the request&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_RESPONSE_V1 = OFFSET_COMMIT_RESPONSE_V0;&lt;br/&gt;
+&lt;br/&gt;
+    // V2 adds retention time to the request&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_RESPONSE_V2 = OFFSET_COMMIT_RESPONSE_V0;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V3 adds throttle time&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_RESPONSE_V3 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(OFFSET_COMMIT_RESPONSE_TOPIC_V0)));&lt;br/&gt;
+            TOPICS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V4 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_RESPONSE_V4 = OFFSET_COMMIT_RESPONSE_V3;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V5 removes retention time from the request&lt;br/&gt;
     private static final Schema OFFSET_COMMIT_RESPONSE_V5 = OFFSET_COMMIT_RESPONSE_V4;&lt;/p&gt;

&lt;p&gt;+    // V6 adds leader epoch to the request&lt;br/&gt;
+    private static final Schema OFFSET_COMMIT_RESPONSE_V6 = OFFSET_COMMIT_RESPONSE_V5;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{OFFSET_COMMIT_RESPONSE_V0, OFFSET_COMMIT_RESPONSE_V1, OFFSET_COMMIT_RESPONSE_V2,
-            OFFSET_COMMIT_RESPONSE_V3, OFFSET_COMMIT_RESPONSE_V4, OFFSET_COMMIT_RESPONSE_V5}
&lt;p&gt;;&lt;br/&gt;
+            OFFSET_COMMIT_RESPONSE_V3, OFFSET_COMMIT_RESPONSE_V4, OFFSET_COMMIT_RESPONSE_V5, OFFSET_COMMIT_RESPONSE_V6};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private final Map&amp;lt;TopicPartition, Errors&amp;gt; responseData;&lt;br/&gt;
@@ -107,10 +110,10 @@ public OffsetCommitResponse(int throttleTimeMs, Map&amp;lt;TopicPartition, Errors&amp;gt; resp&lt;br/&gt;
     public OffsetCommitResponse(Struct struct) {&lt;br/&gt;
         this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);&lt;br/&gt;
         responseData = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicResponseObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicResponse = (Struct) topicResponseObj;&lt;br/&gt;
             String topic = topicResponse.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {&lt;br/&gt;
+            for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {&lt;br/&gt;
                 Struct partitionResponse = (Struct) partitionResponseObj;&lt;br/&gt;
                 int partition = partitionResponse.get(PARTITION_ID);&lt;br/&gt;
                 Errors error = Errors.forCode(partitionResponse.get(ERROR_CODE));&lt;br/&gt;
@@ -124,22 +127,22 @@ public Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.OFFSET_COMMIT.responseSchema(version));&lt;br/&gt;
         struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; topicsData = CollectionUtils.groupDataByTopic(responseData);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; topicsData = CollectionUtils.groupPartitionDataByTopic(responseData);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; entries: topicsData.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct topicData = struct.instance(RESPONSES_KEY_NAME);&lt;br/&gt;
+            Struct topicData = struct.instance(TOPICS);&lt;br/&gt;
             topicData.set(TOPIC_NAME, entries.getKey());&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, Errors&amp;gt; partitionEntry : entries.getValue().entrySet()) 
{
-                Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);
+                Struct partitionData = topicData.instance(PARTITIONS);
                 partitionData.set(PARTITION_ID, partitionEntry.getKey());
                 partitionData.set(ERROR_CODE, partitionEntry.getValue().code());
                 partitionArray.add(partitionData);
             }&lt;/li&gt;
	&lt;li&gt;topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());&lt;br/&gt;
+            topicData.set(PARTITIONS, partitionArray.toArray());&lt;br/&gt;
             topicArray.add(topicData);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;struct.set(RESPONSES_KEY_NAME, topicArray.toArray());&lt;br/&gt;
+        struct.set(TOPICS, topicArray.toArray());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return struct;&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java&lt;br/&gt;
index e90c6ebbe91..d2f5c888cec 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java&lt;br/&gt;
@@ -20,7 +20,6 @@&lt;br/&gt;
 import org.apache.kafka.common.errors.UnsupportedVersionException;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -32,16 +31,20 @@&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;/p&gt;

&lt;p&gt; import static org.apache.kafka.common.protocol.CommonFields.GROUP_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;/p&gt;

&lt;p&gt; public class OffsetFetchRequest extends AbstractRequest {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;br/&gt;
+    // top level fields&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;Topics to fetch offsets. If the topic array is null fetch offsets for all topics.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// topic level field names&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;Partitions to fetch offsets.&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /*&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Wire formats of version 0 and 1 are the same, but with different functionality.&lt;br/&gt;
@@ -54,32 +57,41 @@&lt;/li&gt;
	&lt;li&gt;a &apos;null&apos; is passed instead of a list of specific topic partitions. It also returns a top level error code&lt;/li&gt;
	&lt;li&gt;for group or coordinator level errors.&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema OFFSET_FETCH_REQUEST_PARTITION_V0 = new Schema(&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema OFFSET_FETCH_REQUEST_TOPIC_V0 = new Schema(&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&quot;Topics to fetch offsets.&quot;,&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_FETCH_REQUEST_PARTITION_V0), &quot;Partitions to fetch offsets.&quot;));&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema OFFSET_FETCH_REQUEST_V0 = new Schema(&lt;br/&gt;
             GROUP_ID,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_FETCH_REQUEST_TOPIC_V0), &quot;Topics to fetch offsets.&quot;));&lt;br/&gt;
+            TOPICS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V1 begins support for fetching offsets from the internal __consumer_offsets topic&lt;br/&gt;
     private static final Schema OFFSET_FETCH_REQUEST_V1 = OFFSET_FETCH_REQUEST_V0;&lt;/p&gt;

&lt;p&gt;+    // V2 adds top-level error code to the response as well as allowing a null offset array to indicate fetch&lt;br/&gt;
+    // of all committed offsets for a group&lt;br/&gt;
+    private static final Field TOPICS_V2 = TOPICS.nullableWithFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V0);&lt;br/&gt;
     private static final Schema OFFSET_FETCH_REQUEST_V2 = new Schema(&lt;br/&gt;
             GROUP_ID,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, ArrayOf.nullable(OFFSET_FETCH_REQUEST_TOPIC_V0), &quot;Topics to fetch offsets. If the &quot; +&lt;/li&gt;
	&lt;li&gt;&quot;topic array is null fetch offsets for all topics.&quot;));&lt;br/&gt;
+            TOPICS_V2);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* v3 request is the same as v2. Throttle time has been added to v3 response */&lt;br/&gt;
+    // V3 request is the same as v2. Throttle time has been added to v3 response&lt;br/&gt;
     private static final Schema OFFSET_FETCH_REQUEST_V3 = OFFSET_FETCH_REQUEST_V2;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V4 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema OFFSET_FETCH_REQUEST_V4 = OFFSET_FETCH_REQUEST_V3;&lt;/p&gt;

&lt;p&gt;+    // V5 adds the leader epoch of the committed offset in the response&lt;br/&gt;
+    private static final Schema OFFSET_FETCH_REQUEST_V5 = OFFSET_FETCH_REQUEST_V4;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{OFFSET_FETCH_REQUEST_V0, OFFSET_FETCH_REQUEST_V1, OFFSET_FETCH_REQUEST_V2,
-            OFFSET_FETCH_REQUEST_V3, OFFSET_FETCH_REQUEST_V4}
&lt;p&gt;;&lt;br/&gt;
+            OFFSET_FETCH_REQUEST_V3, OFFSET_FETCH_REQUEST_V4, OFFSET_FETCH_REQUEST_V5};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static class Builder extends AbstractRequest.Builder&amp;lt;OffsetFetchRequest&amp;gt; {&lt;br/&gt;
@@ -128,23 +140,22 @@ public static OffsetFetchRequest forAllPartitions(String groupId) &lt;/p&gt;
{
         return new OffsetFetchRequest.Builder(groupId, null).build((short) 2);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// v0, v1, and v2 have the same fields.&lt;br/&gt;
     private OffsetFetchRequest(String groupId, List&amp;lt;TopicPartition&amp;gt; partitions, short version) 
{
-        super(version);
+        super(ApiKeys.OFFSET_FETCH, version);
         this.groupId = groupId;
         this.partitions = partitions;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public OffsetFetchRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.OFFSET_FETCH, version);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object[] topicArray = struct.getArray(TOPICS_KEY_NAME);&lt;br/&gt;
+        Object[] topicArray = struct.get(TOPICS);&lt;br/&gt;
         if (topicArray != null) {&lt;br/&gt;
             partitions = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Object topicResponseObj : struct.getArray(TOPICS_KEY_NAME)) {&lt;br/&gt;
+            for (Object topicResponseObj : topicArray) {&lt;br/&gt;
                 Struct topicResponse = (Struct) topicResponseObj;&lt;br/&gt;
                 String topic = topicResponse.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {&lt;br/&gt;
+                for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {&lt;br/&gt;
                     Struct partitionResponse = (Struct) partitionResponseObj;&lt;br/&gt;
                     int partition = partitionResponse.get(PARTITION_ID);&lt;br/&gt;
                     partitions.add(new TopicPartition(topic, partition));&lt;br/&gt;
@@ -166,12 +177,14 @@ public OffsetFetchResponse getErrorResponse(int throttleTimeMs, Errors error) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;TopicPartition, OffsetFetchResponse.PartitionData&amp;gt; responsePartitions = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         if (versionId &amp;lt; 2) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (TopicPartition partition : this.partitions) 
{
-                responsePartitions.put(partition, new OffsetFetchResponse.PartitionData(
-                        OffsetFetchResponse.INVALID_OFFSET,
-                        OffsetFetchResponse.NO_METADATA,
-                        error));
-            }
&lt;p&gt;+            OffsetFetchResponse.PartitionData partitionError = new OffsetFetchResponse.PartitionData(&lt;br/&gt;
+                    OffsetFetchResponse.INVALID_OFFSET,&lt;br/&gt;
+                    Optional.empty(),&lt;br/&gt;
+                    OffsetFetchResponse.NO_METADATA,&lt;br/&gt;
+                    error);&lt;br/&gt;
+&lt;br/&gt;
+            for (TopicPartition partition : this.partitions)&lt;br/&gt;
+                responsePartitions.put(partition, partitionError);&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         switch (versionId) {&lt;br/&gt;
@@ -181,6 +194,7 @@ public OffsetFetchResponse getErrorResponse(int throttleTimeMs, Errors error) {&lt;br/&gt;
                 return new OffsetFetchResponse(error, responsePartitions);&lt;br/&gt;
             case 3:&lt;br/&gt;
             case 4:&lt;br/&gt;
+            case 5:&lt;br/&gt;
                 return new OffsetFetchResponse(throttleTimeMs, error, responsePartitions);&lt;br/&gt;
             default:&lt;br/&gt;
                 throw new IllegalArgumentException(String.format(&quot;Version %d is not valid. Valid versions for %s are 0 to %d&quot;,&lt;br/&gt;
@@ -214,25 +228,26 @@ protected Struct toStruct() {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.OFFSET_FETCH.requestSchema(version()));&lt;br/&gt;
         struct.set(GROUP_ID, groupId);&lt;br/&gt;
         if (partitions != null) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicsData = CollectionUtils.groupDataByTopic(partitions);&lt;br/&gt;
+            Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicsData = CollectionUtils.groupPartitionsByTopic(partitions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; entries : topicsData.entrySet()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Struct topicData = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+                Struct topicData = struct.instance(TOPICS);&lt;br/&gt;
                 topicData.set(TOPIC_NAME, entries.getKey());&lt;br/&gt;
                 List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
                 for (Integer partitionId : entries.getValue()) 
{
-                    Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);
+                    Struct partitionData = topicData.instance(PARTITIONS);
                     partitionData.set(PARTITION_ID, partitionId);
                     partitionArray.add(partitionData);
                 }&lt;/li&gt;
	&lt;li&gt;topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());&lt;br/&gt;
+                topicData.set(PARTITIONS, partitionArray.toArray());&lt;br/&gt;
                 topicArray.add(topicData);&lt;br/&gt;
             }&lt;/li&gt;
	&lt;li&gt;struct.set(TOPICS_KEY_NAME, topicArray.toArray());&lt;br/&gt;
+            struct.set(TOPICS, topicArray.toArray());&lt;br/&gt;
         } else&lt;/li&gt;
	&lt;li&gt;struct.set(TOPICS_KEY_NAME, null);&lt;br/&gt;
+            struct.set(TOPICS, null);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return struct;&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java&lt;br/&gt;
index 613695bf09d..2022bb77910 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -32,79 +31,94 @@&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;/p&gt;

&lt;p&gt;+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_LEADER_EPOCH;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_METADATA;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_OFFSET;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.NULLABLE_STRING;&lt;/p&gt;

&lt;p&gt;+/**&lt;br/&gt;
+ * Possible error codes:&lt;br/&gt;
+ *&lt;br/&gt;
+ * - Partition errors:&lt;br/&gt;
+ *   - UNKNOWN_TOPIC_OR_PARTITION (3)&lt;br/&gt;
+ *&lt;br/&gt;
+ * - Group or coordinator errors:&lt;br/&gt;
+ *   - COORDINATOR_LOAD_IN_PROGRESS (14)&lt;br/&gt;
+ *   - COORDINATOR_NOT_AVAILABLE (15)&lt;br/&gt;
+ *   - NOT_COORDINATOR (16)&lt;br/&gt;
+ *   - GROUP_AUTHORIZATION_FAILED (30)&lt;br/&gt;
+ */&lt;br/&gt;
 public class OffsetFetchResponse extends AbstractResponse {&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String RESPONSES_KEY_NAME = &quot;responses&quot;;&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;responses&quot;,&lt;br/&gt;
+            &quot;Responses by topic for fetched offsets&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // topic level fields&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partition_responses&quot;;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// partition level fields&lt;/li&gt;
	&lt;li&gt;private static final String COMMIT_OFFSET_KEY_NAME = &quot;offset&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String METADATA_KEY_NAME = &quot;metadata&quot;;&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partition_responses&quot;,&lt;br/&gt;
+            &quot;Responses by partition for fetched offsets&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema OFFSET_FETCH_RESPONSE_PARTITION_V0 = new Schema(&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(COMMIT_OFFSET_KEY_NAME, INT64, &quot;Last committed message offset.&quot;),&lt;/li&gt;
	&lt;li&gt;new Field(METADATA_KEY_NAME, NULLABLE_STRING, &quot;Any associated metadata the client wants to keep.&quot;),&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMITTED_METADATA,&lt;br/&gt;
             ERROR_CODE);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema OFFSET_FETCH_RESPONSE_TOPIC_V0 = new Schema(&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_FETCH_RESPONSE_PARTITION_V0)));&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema OFFSET_FETCH_RESPONSE_V0 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(OFFSET_FETCH_RESPONSE_TOPIC_V0)));&lt;br/&gt;
+            TOPICS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V1 begins support for fetching offsets from the internal __consumer_offsets topic&lt;br/&gt;
     private static final Schema OFFSET_FETCH_RESPONSE_V1 = OFFSET_FETCH_RESPONSE_V0;&lt;/p&gt;

&lt;p&gt;+    // V2 adds top-level error code&lt;br/&gt;
     private static final Schema OFFSET_FETCH_RESPONSE_V2 = new Schema(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(OFFSET_FETCH_RESPONSE_TOPIC_V0)),&lt;br/&gt;
+            TOPICS_V0,&lt;br/&gt;
             ERROR_CODE);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* v3 request is the same as v2. Throttle time has been added to v3 response */&lt;br/&gt;
+    // V3 request includes throttle time&lt;br/&gt;
     private static final Schema OFFSET_FETCH_RESPONSE_V3 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;/li&gt;
	&lt;li&gt;new Field(RESPONSES_KEY_NAME, new ArrayOf(OFFSET_FETCH_RESPONSE_TOPIC_V0)),&lt;br/&gt;
+            TOPICS_V0,&lt;br/&gt;
             ERROR_CODE);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    // V4 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema OFFSET_FETCH_RESPONSE_V4 = OFFSET_FETCH_RESPONSE_V3;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V5 adds the leader epoch to the committed offset&lt;br/&gt;
+    private static final Field PARTITIONS_V5 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMITTED_LEADER_EPOCH,&lt;br/&gt;
+            COMMITTED_METADATA,&lt;br/&gt;
+            ERROR_CODE);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V5 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V5);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Schema OFFSET_FETCH_RESPONSE_V5 = new Schema(&lt;br/&gt;
+            THROTTLE_TIME_MS,&lt;br/&gt;
+            TOPICS_V5,&lt;br/&gt;
+            ERROR_CODE);&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;br/&gt;
         return new Schema[] &lt;/p&gt;
{OFFSET_FETCH_RESPONSE_V0, OFFSET_FETCH_RESPONSE_V1, OFFSET_FETCH_RESPONSE_V2,
-            OFFSET_FETCH_RESPONSE_V3, OFFSET_FETCH_RESPONSE_V4}
&lt;p&gt;;&lt;br/&gt;
+            OFFSET_FETCH_RESPONSE_V3, OFFSET_FETCH_RESPONSE_V4, OFFSET_FETCH_RESPONSE_V5};&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static final long INVALID_OFFSET = -1L;&lt;br/&gt;
     public static final String NO_METADATA = &quot;&quot;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final PartitionData UNKNOWN_PARTITION = new PartitionData(INVALID_OFFSET, NO_METADATA,&lt;/li&gt;
	&lt;li&gt;Errors.UNKNOWN_TOPIC_OR_PARTITION);&lt;/li&gt;
	&lt;li&gt;public static final PartitionData UNAUTHORIZED_PARTITION = new PartitionData(INVALID_OFFSET, NO_METADATA,&lt;/li&gt;
	&lt;li&gt;Errors.TOPIC_AUTHORIZATION_FAILED);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Possible error codes:&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* - Partition errors:&lt;/li&gt;
	&lt;li&gt;*   - UNKNOWN_TOPIC_OR_PARTITION (3)&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* - Group or coordinator errors:&lt;/li&gt;
	&lt;li&gt;*   - COORDINATOR_LOAD_IN_PROGRESS (14)&lt;/li&gt;
	&lt;li&gt;*   - COORDINATOR_NOT_AVAILABLE (15)&lt;/li&gt;
	&lt;li&gt;*   - NOT_COORDINATOR (16)&lt;/li&gt;
	&lt;li&gt;*   - GROUP_AUTHORIZATION_FAILED (30)&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+    public static final PartitionData UNKNOWN_PARTITION = new PartitionData(INVALID_OFFSET,&lt;br/&gt;
+            Optional.empty(), NO_METADATA, Errors.UNKNOWN_TOPIC_OR_PARTITION);&lt;br/&gt;
+    public static final PartitionData UNAUTHORIZED_PARTITION = new PartitionData(INVALID_OFFSET,&lt;br/&gt;
+            Optional.empty(), NO_METADATA, Errors.TOPIC_AUTHORIZATION_FAILED);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final List&amp;lt;Errors&amp;gt; PARTITION_ERRORS = Collections.singletonList(Errors.UNKNOWN_TOPIC_OR_PARTITION);&lt;/p&gt;

&lt;p&gt;@@ -116,9 +130,14 @@&lt;br/&gt;
         public final long offset;&lt;br/&gt;
         public final String metadata;&lt;br/&gt;
         public final Errors error;&lt;br/&gt;
+        public final Optional&amp;lt;Integer&amp;gt; leaderEpoch;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public PartitionData(long offset, String metadata, Errors error) {&lt;br/&gt;
+        public PartitionData(long offset,&lt;br/&gt;
+                             Optional&amp;lt;Integer&amp;gt; leaderEpoch,&lt;br/&gt;
+                             String metadata,&lt;br/&gt;
+                             Errors error) 
{
             this.offset = offset;
+            this.leaderEpoch = leaderEpoch;
             this.metadata = metadata;
             this.error = error;
         }
&lt;p&gt;@@ -153,18 +172,21 @@ public OffsetFetchResponse(Struct struct) {&lt;br/&gt;
         this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);&lt;br/&gt;
         Errors topLevelError = Errors.NONE;&lt;br/&gt;
         this.responseData = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicResponseObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicResponse = (Struct) topicResponseObj;&lt;br/&gt;
             String topic = topicResponse.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (Object partitionResponseObj }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -215,25 +237,26 @@ protected Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.OFFSET_FETCH.responseSchema(version));&lt;br/&gt;
         struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsData = CollectionUtils.groupDataByTopic(responseData);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsData = CollectionUtils.groupPartitionDataByTopic(responseData);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; entries : topicsData.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct topicData = struct.instance(RESPONSES_KEY_NAME);&lt;br/&gt;
+            Struct topicData = struct.instance(TOPICS);&lt;br/&gt;
             topicData.set(TOPIC_NAME, entries.getKey());&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitionArray = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, PartitionData&amp;gt; partitionEntry : entries.getValue().entrySet()) 
{
                 PartitionData fetchPartitionData = partitionEntry.getValue();
-                Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);
+                Struct partitionData = topicData.instance(PARTITIONS);
                 partitionData.set(PARTITION_ID, partitionEntry.getKey());
-                partitionData.set(COMMIT_OFFSET_KEY_NAME, fetchPartitionData.offset);
-                partitionData.set(METADATA_KEY_NAME, fetchPartitionData.metadata);
+                partitionData.set(COMMITTED_OFFSET, fetchPartitionData.offset);
+                RequestUtils.setLeaderEpochIfExists(partitionData, COMMITTED_LEADER_EPOCH, fetchPartitionData.leaderEpoch);
+                partitionData.set(COMMITTED_METADATA, fetchPartitionData.metadata);
                 partitionData.set(ERROR_CODE, fetchPartitionData.error.code());
                 partitionArray.add(partitionData);
             }&lt;/li&gt;
	&lt;li&gt;topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());&lt;br/&gt;
+            topicData.set(PARTITIONS, partitionArray.toArray());&lt;br/&gt;
             topicArray.add(topicData);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;struct.set(RESPONSES_KEY_NAME, topicArray.toArray());&lt;br/&gt;
+        struct.set(TOPICS, topicArray.toArray());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (version &amp;gt; 1)&lt;br/&gt;
             struct.set(ERROR_CODE, this.error.code());&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java&lt;br/&gt;
index 651416d97a7..9de5d02d4af 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -30,53 +29,69 @@&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;/p&gt;

&lt;p&gt;+import static org.apache.kafka.common.protocol.CommonFields.CURRENT_LEADER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT32;&lt;/p&gt;

&lt;p&gt; public class OffsetsForLeaderEpochRequest extends AbstractRequest {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String LEADER_EPOCH = &quot;leader_epoch&quot;;&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;An array of topics to get epochs for&quot;);&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;An array of partitions to get epochs for&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* Offsets for Leader Epoch api */&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_FOR_LEADER_EPOCH_REQUEST_PARTITION_V0 = new Schema(&lt;br/&gt;
+    private static final Field.Int32 LEADER_EPOCH = new Field.Int32(&quot;leader_epoch&quot;,&lt;br/&gt;
+            &quot;The epoch to lookup an offset for.&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(LEADER_EPOCH, INT32, &quot;The epoch&quot;));&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_FOR_LEADER_EPOCH_REQUEST_TOPIC_V0 = new Schema(&lt;br/&gt;
+            LEADER_EPOCH);&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_REQUEST_PARTITION_V0)));&lt;br/&gt;
+            PARTITIONS_V0);&lt;br/&gt;
     private static final Schema OFFSET_FOR_LEADER_EPOCH_REQUEST_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_REQUEST_TOPIC_V0), &quot;An array of topics to get epochs for&quot;));&lt;br/&gt;
+            TOPICS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/* v1 request is the same as v0. Per-partition leader epoch has been added to response */&lt;br/&gt;
+    // V1 request is the same as v0. Per-partition leader epoch has been added to response&lt;br/&gt;
     private static final Schema OFFSET_FOR_LEADER_EPOCH_REQUEST_V1 = OFFSET_FOR_LEADER_EPOCH_REQUEST_V0;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V2 adds the current leader epoch to support fencing&lt;br/&gt;
+    private static final Field PARTITIONS_V2 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            CURRENT_LEADER_EPOCH,&lt;br/&gt;
+            LEADER_EPOCH);&lt;br/&gt;
+    private static final Field TOPICS_V2 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V2);&lt;br/&gt;
+    private static final Schema OFFSET_FOR_LEADER_EPOCH_REQUEST_V2 = new Schema(&lt;br/&gt;
+            TOPICS_V2);&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new Schema[]
{OFFSET_FOR_LEADER_EPOCH_REQUEST_V0, OFFSET_FOR_LEADER_EPOCH_REQUEST_V1}
&lt;p&gt;;&lt;br/&gt;
+        return new Schema[]&lt;/p&gt;
{OFFSET_FOR_LEADER_EPOCH_REQUEST_V0, OFFSET_FOR_LEADER_EPOCH_REQUEST_V1,
+            OFFSET_FOR_LEADER_EPOCH_REQUEST_V2}
&lt;p&gt;;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition;&lt;br/&gt;
+    private Map&amp;lt;TopicPartition, PartitionData&amp;gt; epochsByPartition;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByTopicPartition() {&lt;br/&gt;
+    public Map&amp;lt;TopicPartition, PartitionData&amp;gt; epochsByTopicPartition() 
{
         return epochsByPartition;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static class Builder extends AbstractRequest.Builder&amp;lt;OffsetsForLeaderEpochRequest&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        private Map&amp;lt;TopicPartition, PartitionData&amp;gt; epochsByPartition;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         public Builder(short version) &lt;/p&gt;
{
-            super(ApiKeys.OFFSET_FOR_LEADER_EPOCH, version);
+            this(version, new HashMap&amp;lt;&amp;gt;());
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Builder(short version, Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition) {&lt;br/&gt;
+        public Builder(short version, Map&amp;lt;TopicPartition, PartitionData&amp;gt; epochsByPartition) 
{
             super(ApiKeys.OFFSET_FOR_LEADER_EPOCH, version);
             this.epochsByPartition = epochsByPartition;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Builder add(TopicPartition topicPartition, Integer epoch) {&lt;/li&gt;
	&lt;li&gt;epochsByPartition.put(topicPartition, epoch);&lt;br/&gt;
+        public Builder add(TopicPartition topicPartition, Optional&amp;lt;Integer&amp;gt; currentEpoch, int leaderEpoch) 
{
+            epochsByPartition.put(topicPartition, new PartitionData(currentEpoch, leaderEpoch));
             return this;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -99,23 +114,24 @@ public String toString() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public OffsetsForLeaderEpochRequest(Map&amp;lt;TopicPartition, Integer&amp;gt; epochsByPartition, short version) {&lt;/li&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+    public OffsetsForLeaderEpochRequest(Map&amp;lt;TopicPartition, PartitionData&amp;gt; epochsByPartition, short version) 
{
+        super(ApiKeys.OFFSET_FOR_LEADER_EPOCH, version);
         this.epochsByPartition = epochsByPartition;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public OffsetsForLeaderEpochRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.OFFSET_FOR_LEADER_EPOCH, version);&lt;br/&gt;
         epochsByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Object topicAndEpochsObj : struct.getArray(TOPICS_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicAndEpochsObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicAndEpochs = (Struct) topicAndEpochsObj;&lt;br/&gt;
             String topic = topicAndEpochs.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionAndEpochObj : topicAndEpochs.getArray(PARTITIONS_KEY_NAME)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (Object partitionAndEpochObj }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     }&lt;br/&gt;
@@ -128,23 +144,28 @@ public static OffsetsForLeaderEpochRequest parse(ByteBuffer buffer, short versio&lt;br/&gt;
     protected Struct toStruct() {&lt;br/&gt;
         Struct requestStruct = new Struct(ApiKeys.OFFSET_FOR_LEADER_EPOCH.requestSchema(version()));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Integer&amp;gt;&amp;gt; topicsToPartitionEpochs = CollectionUtils.groupDataByTopic(epochsByPartition);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicsToPartitionEpochs = CollectionUtils.groupPartitionDataByTopic(epochsByPartition);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;Struct&amp;gt; topics = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Integer&amp;gt;&amp;gt; topicToEpochs : topicsToPartitionEpochs.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct topicsStruct = requestStruct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+        for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, PartitionData&amp;gt;&amp;gt; topicToEpochs : topicsToPartitionEpochs.entrySet()) {&lt;br/&gt;
+            Struct topicsStruct = requestStruct.instance(TOPICS);&lt;br/&gt;
             topicsStruct.set(TOPIC_NAME, topicToEpochs.getKey());&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitions = new ArrayList&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;Integer, Integer&amp;gt; partitionEpoch : topicToEpochs.getValue().entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct partitionStruct = topicsStruct.instance(PARTITIONS_KEY_NAME);&lt;br/&gt;
+            for (Map.Entry&amp;lt;Integer, PartitionData&amp;gt; partitionEpoch : topicToEpochs.getValue().entrySet()) 
{
+                Struct partitionStruct = topicsStruct.instance(PARTITIONS);
                 partitionStruct.set(PARTITION_ID, partitionEpoch.getKey());
-                partitionStruct.set(LEADER_EPOCH, partitionEpoch.getValue());
+
+                PartitionData partitionData = partitionEpoch.getValue();
+                partitionStruct.set(LEADER_EPOCH, partitionData.leaderEpoch);
+
+                // Current leader epoch introduced in v2
+                RequestUtils.setLeaderEpochIfExists(partitionStruct, CURRENT_LEADER_EPOCH, partitionData.currentLeaderEpoch);
                 partitions.add(partitionStruct);
             }&lt;/li&gt;
	&lt;li&gt;topicsStruct.set(PARTITIONS_KEY_NAME, partitions.toArray());&lt;br/&gt;
+            topicsStruct.set(PARTITIONS, partitions.toArray());&lt;br/&gt;
             topics.add(topicsStruct);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;requestStruct.set(TOPICS_KEY_NAME, topics.toArray());&lt;br/&gt;
+        requestStruct.set(TOPICS, topics.toArray());&lt;br/&gt;
         return requestStruct;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -158,4 +179,15 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {&lt;br/&gt;
         }&lt;br/&gt;
         return new OffsetsForLeaderEpochResponse(errorResponse);&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
+    public static class PartitionData {&lt;br/&gt;
+        public final Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch;&lt;br/&gt;
+        public final int leaderEpoch;&lt;br/&gt;
+&lt;br/&gt;
+        public PartitionData(Optional&amp;lt;Integer&amp;gt; currentLeaderEpoch, int leaderEpoch) &lt;/p&gt;
{
+            this.currentLeaderEpoch = currentLeaderEpoch;
+            this.leaderEpoch = leaderEpoch;
+        }
&lt;p&gt;+&lt;br/&gt;
+    }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java&lt;br/&gt;
index 4da876704b7..324a2ed5f23 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -33,59 +32,61 @@&lt;br/&gt;
 import java.util.Map;&lt;/p&gt;

&lt;p&gt; import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.LEADER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.CommonFields.LEADER_EPOCH;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;/p&gt;

&lt;p&gt; public class OffsetsForLeaderEpochResponse extends AbstractResponse {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String END_OFFSET_KEY_NAME = &quot;end_offset&quot;;&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;An array of topics for which we have leader offsets for some requested partition leader epoch&quot;);&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;An array of offsets by partition&quot;);&lt;br/&gt;
+    private static final Field.Int64 END_OFFSET = new Field.Int64(&quot;end_offset&quot;, &quot;The end offset&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V0 = new Schema(&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             ERROR_CODE,&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(END_OFFSET_KEY_NAME, INT64, &quot;The end offset&quot;));&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_TOPIC_V0 = new Schema(&lt;br/&gt;
+            END_OFFSET);&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V0)));&lt;br/&gt;
+            PARTITIONS_V0);&lt;br/&gt;
     private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_V0 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_RESPONSE_TOPIC_V0),&lt;/li&gt;
	&lt;li&gt;&quot;An array of topics for which we have leader offsets for some requested Partition Leader Epoch&quot;));&lt;br/&gt;
+            TOPICS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V1 added a per-partition leader epoch field,&lt;/li&gt;
	&lt;li&gt;// which specifies which leader epoch the end offset belongs to&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V1 = new Schema(&lt;br/&gt;
+    // V1 added a per-partition leader epoch field which specifies which leader epoch the end offset belongs to&lt;br/&gt;
+    private static final Field PARTITIONS_V1 = PARTITIONS.withFields(&lt;br/&gt;
             ERROR_CODE,&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             LEADER_EPOCH,&lt;/li&gt;
	&lt;li&gt;new Field(END_OFFSET_KEY_NAME, INT64, &quot;The end offset&quot;));&lt;/li&gt;
	&lt;li&gt;private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_TOPIC_V1 = new Schema(&lt;br/&gt;
+            END_OFFSET);&lt;br/&gt;
+    private static final Field TOPICS_V1 = TOPICS.withFields(&lt;br/&gt;
             TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_RESPONSE_PARTITION_V1)));&lt;br/&gt;
+            PARTITIONS_V1);&lt;br/&gt;
     private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_V1 = new Schema(&lt;/li&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(OFFSET_FOR_LEADER_EPOCH_RESPONSE_TOPIC_V1),&lt;/li&gt;
	&lt;li&gt;&quot;An array of topics for which we have leader offsets for some requested Partition Leader Epoch&quot;));&lt;br/&gt;
+            TOPICS_V1);&lt;br/&gt;
+&lt;br/&gt;
+    // V2 bumped for addition of current leader epoch to the request schema.&lt;br/&gt;
+    private static final Schema OFFSET_FOR_LEADER_EPOCH_RESPONSE_V2 = OFFSET_FOR_LEADER_EPOCH_RESPONSE_V1;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static Schema[] schemaVersions() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new Schema[]
{OFFSET_FOR_LEADER_EPOCH_RESPONSE_V0, OFFSET_FOR_LEADER_EPOCH_RESPONSE_V1}
&lt;p&gt;;&lt;br/&gt;
+        return new Schema[]&lt;/p&gt;
{OFFSET_FOR_LEADER_EPOCH_RESPONSE_V0, OFFSET_FOR_LEADER_EPOCH_RESPONSE_V1,
+            OFFSET_FOR_LEADER_EPOCH_RESPONSE_V2}
&lt;p&gt;;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private Map&amp;lt;TopicPartition, EpochEndOffset&amp;gt; epochEndOffsetsByPartition;&lt;/p&gt;

&lt;p&gt;     public OffsetsForLeaderEpochResponse(Struct struct) {&lt;br/&gt;
         epochEndOffsetsByPartition = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (Object topicAndEpocsObj : struct.getArray(TOPICS_KEY_NAME)) {&lt;br/&gt;
+        for (Object topicAndEpocsObj : struct.get(TOPICS)) {&lt;br/&gt;
             Struct topicAndEpochs = (Struct) topicAndEpocsObj;&lt;br/&gt;
             String topic = topicAndEpochs.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionAndEpochObj : topicAndEpochs.getArray(PARTITIONS_KEY_NAME)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (Object partitionAndEpochObj }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -115,26 +116,26 @@ public static OffsetsForLeaderEpochResponse parse(ByteBuffer buffer, short versi&lt;br/&gt;
     protected Struct toStruct(short version) {&lt;br/&gt;
         Struct responseStruct = new Struct(ApiKeys.OFFSET_FOR_LEADER_EPOCH.responseSchema(version));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, EpochEndOffset&amp;gt;&amp;gt; endOffsetsByTopic = CollectionUtils.groupDataByTopic(epochEndOffsetsByPartition);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, EpochEndOffset&amp;gt;&amp;gt; endOffsetsByTopic = CollectionUtils.groupPartitionDataByTopic(epochEndOffsetsByPartition);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         List&amp;lt;Struct&amp;gt; topics = new ArrayList&amp;lt;&amp;gt;(endOffsetsByTopic.size());&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, EpochEndOffset&amp;gt;&amp;gt; topicToPartitionEpochs : endOffsetsByTopic.entrySet()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Struct topicStruct = responseStruct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+            Struct topicStruct = responseStruct.instance(TOPICS);&lt;br/&gt;
             topicStruct.set(TOPIC_NAME, topicToPartitionEpochs.getKey());&lt;br/&gt;
             Map&amp;lt;Integer, EpochEndOffset&amp;gt; partitionEpochs = topicToPartitionEpochs.getValue();&lt;br/&gt;
             List&amp;lt;Struct&amp;gt; partitions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, EpochEndOffset&amp;gt; partitionEndOffset : partitionEpochs.entrySet()) 
{
-                Struct partitionStruct = topicStruct.instance(PARTITIONS_KEY_NAME);
+                Struct partitionStruct = topicStruct.instance(PARTITIONS);
                 partitionStruct.set(ERROR_CODE, partitionEndOffset.getValue().error().code());
                 partitionStruct.set(PARTITION_ID, partitionEndOffset.getKey());
                 partitionStruct.setIfExists(LEADER_EPOCH, partitionEndOffset.getValue().leaderEpoch());
-                partitionStruct.set(END_OFFSET_KEY_NAME, partitionEndOffset.getValue().endOffset());
+                partitionStruct.set(END_OFFSET, partitionEndOffset.getValue().endOffset());
                 partitions.add(partitionStruct);
             }&lt;/li&gt;
	&lt;li&gt;topicStruct.set(PARTITIONS_KEY_NAME, partitions.toArray());&lt;br/&gt;
+            topicStruct.set(PARTITIONS, partitions.toArray());&lt;br/&gt;
             topics.add(topicStruct);&lt;br/&gt;
         }&lt;/li&gt;
	&lt;li&gt;responseStruct.set(TOPICS_KEY_NAME, topics.toArray());&lt;br/&gt;
+        responseStruct.set(TOPICS, topics.toArray());&lt;br/&gt;
         return responseStruct;&lt;br/&gt;
     }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java&lt;br/&gt;
index 67745cbb4cc..4f1d766b8bf 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java&lt;br/&gt;
@@ -196,7 +196,7 @@ public String toString() {&lt;br/&gt;
     private boolean idempotent = false;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private ProduceRequest(short version, short acks, int timeout, Map&amp;lt;TopicPartition, MemoryRecords&amp;gt; partitionRecords, String transactionalId) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.PRODUCE, version);
         this.acks = acks;
         this.timeout = timeout;
 
@@ -216,7 +216,7 @@ private ProduceRequest(short version, short acks, int timeout, Map&amp;lt;TopicPartitio
     }

&lt;p&gt;     public ProduceRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.PRODUCE, version);&lt;br/&gt;
         partitionRecords = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object topicDataObj : struct.getArray(TOPIC_DATA_KEY_NAME)) {&lt;br/&gt;
             Struct topicData = (Struct) topicDataObj;&lt;br/&gt;
@@ -268,7 +268,7 @@ public Struct toStruct() {&lt;br/&gt;
         Map&amp;lt;TopicPartition, MemoryRecords&amp;gt; partitionRecords = partitionRecordsOrFail();&lt;br/&gt;
         short version = version();&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.PRODUCE.requestSchema(version));&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, MemoryRecords&amp;gt;&amp;gt; recordsByTopic = CollectionUtils.groupDataByTopic(partitionRecords);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, MemoryRecords&amp;gt;&amp;gt; recordsByTopic = CollectionUtils.groupPartitionDataByTopic(partitionRecords);&lt;br/&gt;
         struct.set(ACKS_KEY_NAME, acks);&lt;br/&gt;
         struct.set(TIMEOUT_KEY_NAME, timeout);&lt;br/&gt;
         struct.setIfExists(NULLABLE_TRANSACTIONAL_ID, transactionalId);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java&lt;br/&gt;
index 467c9804c3c..fb15813ccf1 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java&lt;br/&gt;
@@ -196,7 +196,7 @@ public ProduceResponse(Struct struct) {&lt;br/&gt;
     protected Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.PRODUCE.responseSchema(version));&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, PartitionResponse&amp;gt;&amp;gt; responseByTopic = CollectionUtils.groupDataByTopic(responses);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, PartitionResponse&amp;gt;&amp;gt; responseByTopic = CollectionUtils.groupPartitionDataByTopic(responses);&lt;br/&gt;
         List&amp;lt;Struct&amp;gt; topicDatas = new ArrayList&amp;lt;&amp;gt;(responseByTopic.size());&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, PartitionResponse&amp;gt;&amp;gt; entry : responseByTopic.entrySet()) {&lt;br/&gt;
             Struct topicData = struct.instance(RESPONSES_KEY_NAME);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenRequest.java&lt;br/&gt;
index 57c310014f5..d73561abbb8 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenRequest.java&lt;br/&gt;
@@ -44,14 +44,14 @@&lt;br/&gt;
     public static final Schema TOKEN_RENEW_REQUEST_V1 = TOKEN_RENEW_REQUEST_V0;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private RenewDelegationTokenRequest(short version, ByteBuffer hmac, long renewTimePeriod) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.RENEW_DELEGATION_TOKEN, version);
 
         this.hmac = hmac;
         this.renewTimePeriod = renewTimePeriod;
     }

&lt;p&gt;     public RenewDelegationTokenRequest(Struct struct, short versionId) &lt;/p&gt;
{
-        super(versionId);
+        super(ApiKeys.RENEW_DELEGATION_TOKEN, versionId);
 
         hmac = struct.getBytes(HMAC_KEY_NAME);
         renewTimePeriod = struct.getLong(RENEW_TIME_PERIOD_KEY_NAME);
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java b/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java
index 7638c6c20a1..24c2fbe4416 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java
@@ -20,12 +20,16 @@
 import org.apache.kafka.common.acl.AccessControlEntryFilter;
 import org.apache.kafka.common.acl.AclOperation;
 import org.apache.kafka.common.acl.AclPermissionType;
+import org.apache.kafka.common.protocol.types.Field;
+import org.apache.kafka.common.record.RecordBatch;
 import org.apache.kafka.common.resource.PatternType;
 import org.apache.kafka.common.resource.ResourcePattern;
 import org.apache.kafka.common.resource.ResourcePatternFilter;
 import org.apache.kafka.common.protocol.types.Struct;
 import org.apache.kafka.common.resource.ResourceType;
 
+import java.util.Optional;
+
 import static org.apache.kafka.common.protocol.CommonFields.HOST;
 import static org.apache.kafka.common.protocol.CommonFields.HOST_FILTER;
 import static org.apache.kafka.common.protocol.CommonFields.OPERATION;
@@ -101,4 +105,16 @@ static void aceFilterSetStructFields(AccessControlEntryFilter filter, Struct str
         struct.set(OPERATION, filter.operation().code());
         struct.set(PERMISSION_TYPE, filter.permissionType().code());
     }
&lt;p&gt;+&lt;br/&gt;
+    static void setLeaderEpochIfExists(Struct struct, Field.Int32 leaderEpochField, Optional&amp;lt;Integer&amp;gt; leaderEpoch) &lt;/p&gt;
{
+        struct.setIfExists(leaderEpochField, leaderEpoch.orElse(RecordBatch.NO_PARTITION_LEADER_EPOCH));
+    }
&lt;p&gt;+&lt;br/&gt;
+    static Optional&amp;lt;Integer&amp;gt; getLeaderEpoch(Struct struct, Field.Int32 leaderEpochField) &lt;/p&gt;
{
+        int leaderEpoch = struct.getOrElse(leaderEpochField, RecordBatch.NO_PARTITION_LEADER_EPOCH);
+        Optional&amp;lt;Integer&amp;gt; leaderEpochOpt = leaderEpoch == RecordBatch.NO_PARTITION_LEADER_EPOCH ?
+                Optional.empty() : Optional.of(leaderEpoch);
+        return leaderEpochOpt;
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateRequest.java&lt;br/&gt;
index 74d31a688de..2ce144e5920 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateRequest.java&lt;br/&gt;
@@ -72,12 +72,12 @@ public SaslAuthenticateRequest(ByteBuffer saslAuthBytes) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public SaslAuthenticateRequest(ByteBuffer saslAuthBytes, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.SASL_AUTHENTICATE, version);
         this.saslAuthBytes = saslAuthBytes;
     }

&lt;p&gt;     public SaslAuthenticateRequest(Struct struct, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.SASL_AUTHENTICATE, version);
         saslAuthBytes = struct.getBytes(SASL_AUTH_BYTES_KEY_NAME);
     }

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeRequest.java&lt;br/&gt;
index a06a4db870c..7225eb7b8f8 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeRequest.java&lt;br/&gt;
@@ -80,12 +80,12 @@ public SaslHandshakeRequest(String mechanism) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public SaslHandshakeRequest(String mechanism, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.SASL_HANDSHAKE, version);
         this.mechanism = mechanism;
     }

&lt;p&gt;     public SaslHandshakeRequest(Struct struct, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.SASL_HANDSHAKE, version);
         mechanism = struct.getString(MECHANISM_KEY_NAME);
     }

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java&lt;br/&gt;
index d79c938d794..a296c8059a8 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java&lt;br/&gt;
@@ -98,7 +98,7 @@ public String toString() {&lt;/p&gt;

&lt;p&gt;     private StopReplicaRequest(int controllerId, int controllerEpoch, boolean deletePartitions,&lt;br/&gt;
                                Set&amp;lt;TopicPartition&amp;gt; partitions, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.STOP_REPLICA, version);
         this.controllerId = controllerId;
         this.controllerEpoch = controllerEpoch;
         this.deletePartitions = deletePartitions;
@@ -106,7 +106,7 @@ private StopReplicaRequest(int controllerId, int controllerEpoch, boolean delete
     }

&lt;p&gt;     public StopReplicaRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.STOP_REPLICA, version);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         partitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object partitionDataObj : struct.getArray(PARTITIONS_KEY_NAME)) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/SyncGroupRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/SyncGroupRequest.java&lt;br/&gt;
index 962bc77884d..237320f9cd2 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/SyncGroupRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/SyncGroupRequest.java&lt;br/&gt;
@@ -101,7 +101,7 @@ public String toString() {&lt;/p&gt;

&lt;p&gt;     private SyncGroupRequest(String groupId, int generationId, String memberId,&lt;br/&gt;
                              Map&amp;lt;String, ByteBuffer&amp;gt; groupAssignment, short version) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.SYNC_GROUP, version);
         this.groupId = groupId;
         this.generationId = generationId;
         this.memberId = memberId;
@@ -109,7 +109,7 @@ private SyncGroupRequest(String groupId, int generationId, String memberId,
     }

&lt;p&gt;     public SyncGroupRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.SYNC_GROUP, version);&lt;br/&gt;
         this.groupId = struct.get(GROUP_ID);&lt;br/&gt;
         this.generationId = struct.get(GENERATION_ID);&lt;br/&gt;
         this.memberId = struct.get(MEMBER_ID);&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java&lt;br/&gt;
index 25245be814f..1c922e1dd52 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -28,44 +27,66 @@&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_LEADER_EPOCH;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_METADATA;&lt;br/&gt;
+import static org.apache.kafka.common.protocol.CommonFields.COMMITTED_OFFSET;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.GROUP_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PRODUCER_EPOCH;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.PRODUCER_ID;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TRANSACTIONAL_ID;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.INT64;&lt;br/&gt;
-import static org.apache.kafka.common.protocol.types.Type.NULLABLE_STRING;&lt;/p&gt;

&lt;p&gt; public class TxnOffsetCommitRequest extends AbstractRequest {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String OFFSET_KEY_NAME = &quot;offset&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String METADATA_KEY_NAME = &quot;metadata&quot;;&lt;br/&gt;
+    // top level fields&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;Topics to commit offsets&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema TXN_OFFSET_COMMIT_PARTITION_OFFSET_METADATA_REQUEST_V0 = new Schema(&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;Partitions to commit offsets&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;/li&gt;
	&lt;li&gt;new Field(OFFSET_KEY_NAME, INT64),&lt;/li&gt;
	&lt;li&gt;new Field(METADATA_KEY_NAME, NULLABLE_STRING));&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMITTED_METADATA);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private static final Schema TXN_OFFSET_COMMIT_REQUEST_V0 = new Schema(&lt;br/&gt;
             TRANSACTIONAL_ID,&lt;br/&gt;
             GROUP_ID,&lt;br/&gt;
             PRODUCER_ID,&lt;br/&gt;
             PRODUCER_EPOCH,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(TXN_OFFSET_COMMIT_PARTITION_OFFSET_METADATA_REQUEST_V0)))),&lt;/li&gt;
	&lt;li&gt;&quot;The partitions to write markers for.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+            TOPICS_V0);&lt;br/&gt;
+&lt;br/&gt;
+    // V1 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema TXN_OFFSET_COMMIT_REQUEST_V1 = TXN_OFFSET_COMMIT_REQUEST_V0;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V2 adds the leader epoch to the partition data&lt;br/&gt;
+    private static final Field PARTITIONS_V2 = PARTITIONS.withFields(&lt;br/&gt;
+            PARTITION_ID,&lt;br/&gt;
+            COMMITTED_OFFSET,&lt;br/&gt;
+            COMMITTED_LEADER_EPOCH,&lt;br/&gt;
+            COMMITTED_METADATA);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field TOPICS_V2 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V2);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Schema TXN_OFFSET_COMMIT_REQUEST_V2 = new Schema(&lt;br/&gt;
+            TRANSACTIONAL_ID,&lt;br/&gt;
+            GROUP_ID,&lt;br/&gt;
+            PRODUCER_ID,&lt;br/&gt;
+            PRODUCER_EPOCH,&lt;br/&gt;
+            TOPICS_V2);&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new Schema[]
{TXN_OFFSET_COMMIT_REQUEST_V0, TXN_OFFSET_COMMIT_REQUEST_V1}
&lt;p&gt;;&lt;br/&gt;
+        return new Schema[]&lt;/p&gt;
{TXN_OFFSET_COMMIT_REQUEST_V0, TXN_OFFSET_COMMIT_REQUEST_V1, TXN_OFFSET_COMMIT_REQUEST_V2}
&lt;p&gt;;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public static class Builder extends AbstractRequest.Builder&amp;lt;TxnOffsetCommitRequest&amp;gt; {&lt;br/&gt;
@@ -120,7 +141,7 @@ public String toString() {&lt;/p&gt;

&lt;p&gt;     public TxnOffsetCommitRequest(short version, String transactionalId, String consumerGroupId, long producerId,&lt;br/&gt;
                                   short producerEpoch, Map&amp;lt;TopicPartition, CommittedOffset&amp;gt; offsets) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.TXN_OFFSET_COMMIT, version);
         this.transactionalId = transactionalId;
         this.consumerGroupId = consumerGroupId;
         this.producerId = producerId;
@@ -129,23 +150,24 @@ public TxnOffsetCommitRequest(short version, String transactionalId, String cons
     }

&lt;p&gt;     public TxnOffsetCommitRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.TXN_OFFSET_COMMIT, version);&lt;br/&gt;
         this.transactionalId = struct.get(TRANSACTIONAL_ID);&lt;br/&gt;
         this.consumerGroupId = struct.get(GROUP_ID);&lt;br/&gt;
         this.producerId = struct.get(PRODUCER_ID);&lt;br/&gt;
         this.producerEpoch = struct.get(PRODUCER_EPOCH);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;TopicPartition, CommittedOffset&amp;gt; offsets = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object[] topicPartitionsArray = struct.getArray(TOPICS_KEY_NAME);&lt;br/&gt;
+        Object[] topicPartitionsArray = struct.get(TOPICS);&lt;br/&gt;
         for (Object topicPartitionObj : topicPartitionsArray) {&lt;br/&gt;
             Struct topicPartitionStruct = (Struct) topicPartitionObj;&lt;br/&gt;
             String topic = topicPartitionStruct.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionObj : topicPartitionStruct.getArray(PARTITIONS_KEY_NAME)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            for (Object partitionObj }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;         this.offsets = offsets;&lt;br/&gt;
@@ -179,29 +201,31 @@ protected Struct toStruct() {&lt;br/&gt;
         struct.set(PRODUCER_ID, producerId);&lt;br/&gt;
         struct.set(PRODUCER_EPOCH, producerEpoch);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, CommittedOffset&amp;gt;&amp;gt; mappedPartitionOffsets = CollectionUtils.groupDataByTopic(offsets);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, CommittedOffset&amp;gt;&amp;gt; mappedPartitionOffsets = CollectionUtils.groupPartitionDataByTopic(offsets);&lt;br/&gt;
         Object[] partitionsArray = new Object&lt;span class=&quot;error&quot;&gt;&amp;#91;mappedPartitionOffsets.size()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         int i = 0;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, CommittedOffset&amp;gt;&amp;gt; topicAndPartitions : mappedPartitionOffsets.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct topicPartitionsStruct = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+            Struct topicPartitionsStruct = struct.instance(TOPICS);&lt;br/&gt;
             topicPartitionsStruct.set(TOPIC_NAME, topicAndPartitions.getKey());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             Map&amp;lt;Integer, CommittedOffset&amp;gt; partitionOffsets = topicAndPartitions.getValue();&lt;br/&gt;
             Object[] partitionOffsetsArray = new Object&lt;span class=&quot;error&quot;&gt;&amp;#91;partitionOffsets.size()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
             int j = 0;&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, CommittedOffset&amp;gt; partitionOffset : partitionOffsets.entrySet()) &lt;/p&gt;
{
-                Struct partitionOffsetStruct = topicPartitionsStruct.instance(PARTITIONS_KEY_NAME);
+                Struct partitionOffsetStruct = topicPartitionsStruct.instance(PARTITIONS);
                 partitionOffsetStruct.set(PARTITION_ID, partitionOffset.getKey());
                 CommittedOffset committedOffset = partitionOffset.getValue();
-                partitionOffsetStruct.set(OFFSET_KEY_NAME, committedOffset.offset);
-                partitionOffsetStruct.set(METADATA_KEY_NAME, committedOffset.metadata);
+                partitionOffsetStruct.set(COMMITTED_OFFSET, committedOffset.offset);
+                partitionOffsetStruct.set(COMMITTED_METADATA, committedOffset.metadata);
+                RequestUtils.setLeaderEpochIfExists(partitionOffsetStruct, COMMITTED_LEADER_EPOCH,
+                        committedOffset.leaderEpoch);
                 partitionOffsetsArray[j++] = partitionOffsetStruct;
             }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;topicPartitionsStruct.set(PARTITIONS_KEY_NAME, partitionOffsetsArray);&lt;br/&gt;
+            topicPartitionsStruct.set(PARTITIONS, partitionOffsetsArray);&lt;br/&gt;
             partitionsArray&lt;span class=&quot;error&quot;&gt;&amp;#91;i++&amp;#93;&lt;/span&gt; = topicPartitionsStruct;&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;struct.set(TOPICS_KEY_NAME, partitionsArray);&lt;br/&gt;
+        struct.set(TOPICS, partitionsArray);&lt;br/&gt;
         return struct;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -219,28 +243,23 @@ public static TxnOffsetCommitRequest parse(ByteBuffer buffer, short version) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public static class CommittedOffset {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final long offset;&lt;/li&gt;
	&lt;li&gt;private final String metadata;&lt;br/&gt;
+        public final long offset;&lt;br/&gt;
+        public final String metadata;&lt;br/&gt;
+        public final Optional&amp;lt;Integer&amp;gt; leaderEpoch;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public CommittedOffset(long offset, String metadata) {&lt;br/&gt;
+        public CommittedOffset(long offset, String metadata, Optional&amp;lt;Integer&amp;gt; leaderEpoch) 
{
             this.offset = offset;
             this.metadata = metadata;
+            this.leaderEpoch = leaderEpoch;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @Override&lt;br/&gt;
         public String toString() &lt;/p&gt;
{
             return &quot;CommittedOffset(&quot; +
                     &quot;offset=&quot; + offset +
+                    &quot;, leaderEpoch=&quot; + leaderEpoch +
                     &quot;, metadata=&apos;&quot; + metadata + &quot;&apos;)&quot;;
         }
&lt;p&gt;-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public long offset() 
{
-            return offset;
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public String metadata() 
{
-            return metadata;
-        }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; }&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitResponse.java&lt;br/&gt;
index c34fd400232..ba8f7d6be2e 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitResponse.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
-import org.apache.kafka.common.protocol.types.ArrayOf;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Field;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Schema;&lt;br/&gt;
 import org.apache.kafka.common.protocol.types.Struct;&lt;br/&gt;
@@ -34,41 +33,49 @@&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;&lt;br/&gt;
 import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;&lt;/p&gt;

&lt;p&gt;+/**&lt;br/&gt;
+ *&lt;br/&gt;
+ * Possible error codes:&lt;br/&gt;
+ *   InvalidProducerEpoch&lt;br/&gt;
+ *   NotCoordinator&lt;br/&gt;
+ *   CoordinatorNotAvailable&lt;br/&gt;
+ *   CoordinatorLoadInProgress&lt;br/&gt;
+ *   OffsetMetadataTooLarge&lt;br/&gt;
+ *   GroupAuthorizationFailed&lt;br/&gt;
+ *   InvalidCommitOffsetSize&lt;br/&gt;
+ *   TransactionalIdAuthorizationFailed&lt;br/&gt;
+ *   RequestTimedOut&lt;br/&gt;
+ */&lt;br/&gt;
 public class TxnOffsetCommitResponse extends AbstractResponse {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String TOPICS_KEY_NAME = &quot;topics&quot;;&lt;/li&gt;
	&lt;li&gt;private static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;&lt;br/&gt;
+    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(&quot;topics&quot;,&lt;br/&gt;
+            &quot;Responses by topic for committed offsets&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Schema TXN_OFFSET_COMMIT_PARTITION_ERROR_RESPONSE_V0 = new Schema(&lt;br/&gt;
+    // topic level fields&lt;br/&gt;
+    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(&quot;partitions&quot;,&lt;br/&gt;
+            &quot;Responses by partition for committed offsets&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(&lt;br/&gt;
             PARTITION_ID,&lt;br/&gt;
             ERROR_CODE);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private static final Field TOPICS_V0 = TOPICS.withFields(&lt;br/&gt;
+            TOPIC_NAME,&lt;br/&gt;
+            PARTITIONS_V0);&lt;br/&gt;
+&lt;br/&gt;
     private static final Schema TXN_OFFSET_COMMIT_RESPONSE_V0 = new Schema(&lt;br/&gt;
             THROTTLE_TIME_MS,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new Field(TOPICS_KEY_NAME, new ArrayOf(new Schema(&lt;/li&gt;
	&lt;li&gt;TOPIC_NAME,&lt;/li&gt;
	&lt;li&gt;new Field(PARTITIONS_KEY_NAME, new ArrayOf(TXN_OFFSET_COMMIT_PARTITION_ERROR_RESPONSE_V0)))),&lt;/li&gt;
	&lt;li&gt;&quot;Errors per partition from writing markers.&quot;));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The version number is bumped to indicate that on quota violation brokers send out responses before throttling.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
+            TOPICS_V0);&lt;br/&gt;
+&lt;br/&gt;
+    // V1 bump used to indicate that on quota violation brokers send out responses before throttling.&lt;br/&gt;
     private static final Schema TXN_OFFSET_COMMIT_RESPONSE_V1 = TXN_OFFSET_COMMIT_RESPONSE_V0;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // V2 adds the leader epoch to the partition data&lt;br/&gt;
+    private static final Schema TXN_OFFSET_COMMIT_RESPONSE_V2 = TXN_OFFSET_COMMIT_RESPONSE_V1;&lt;br/&gt;
+&lt;br/&gt;
     public static Schema[] schemaVersions() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return new Schema[]
{TXN_OFFSET_COMMIT_RESPONSE_V0, TXN_OFFSET_COMMIT_RESPONSE_V1}
&lt;p&gt;;&lt;br/&gt;
+        return new Schema[]&lt;/p&gt;
{TXN_OFFSET_COMMIT_RESPONSE_V0, TXN_OFFSET_COMMIT_RESPONSE_V1, TXN_OFFSET_COMMIT_RESPONSE_V2}
&lt;p&gt;;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Possible error codes:&lt;/li&gt;
	&lt;li&gt;//   InvalidProducerEpoch&lt;/li&gt;
	&lt;li&gt;//   NotCoordinator&lt;/li&gt;
	&lt;li&gt;//   CoordinatorNotAvailable&lt;/li&gt;
	&lt;li&gt;//   CoordinatorLoadInProgress&lt;/li&gt;
	&lt;li&gt;//   OffsetMetadataTooLarge&lt;/li&gt;
	&lt;li&gt;//   GroupAuthorizationFailed&lt;/li&gt;
	&lt;li&gt;//   InvalidCommitOffsetSize&lt;/li&gt;
	&lt;li&gt;//   TransactionalIdAuthorizationFailed&lt;/li&gt;
	&lt;li&gt;//   RequestTimedOut&lt;br/&gt;
-&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, Errors&amp;gt; errors;&lt;br/&gt;
     private final int throttleTimeMs;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -80,11 +87,11 @@ public TxnOffsetCommitResponse(int throttleTimeMs, Map&amp;lt;TopicPartition, Errors&amp;gt; e&lt;br/&gt;
     public TxnOffsetCommitResponse(Struct struct) {&lt;br/&gt;
         this.throttleTimeMs = struct.get(THROTTLE_TIME_MS);&lt;br/&gt;
         Map&amp;lt;TopicPartition, Errors&amp;gt; errors = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object[] topicPartitionsArray = struct.getArray(TOPICS_KEY_NAME);&lt;br/&gt;
+        Object[] topicPartitionsArray = struct.get(TOPICS);&lt;br/&gt;
         for (Object topicPartitionObj : topicPartitionsArray) {&lt;br/&gt;
             Struct topicPartitionStruct = (Struct) topicPartitionObj;&lt;br/&gt;
             String topic = topicPartitionStruct.get(TOPIC_NAME);&lt;/li&gt;
	&lt;li&gt;for (Object partitionObj : topicPartitionStruct.getArray(PARTITIONS_KEY_NAME)) {&lt;br/&gt;
+            for (Object partitionObj : topicPartitionStruct.get(PARTITIONS)) {&lt;br/&gt;
                 Struct partitionStruct = (Struct) partitionObj;&lt;br/&gt;
                 Integer partition = partitionStruct.get(PARTITION_ID);&lt;br/&gt;
                 Errors error = Errors.forCode(partitionStruct.get(ERROR_CODE));&lt;br/&gt;
@@ -98,27 +105,27 @@ public TxnOffsetCommitResponse(Struct struct) {&lt;br/&gt;
     protected Struct toStruct(short version) {&lt;br/&gt;
         Struct struct = new Struct(ApiKeys.TXN_OFFSET_COMMIT.responseSchema(version));&lt;br/&gt;
         struct.set(THROTTLE_TIME_MS, throttleTimeMs);&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupDataByTopic(errors);&lt;br/&gt;
+        Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupPartitionDataByTopic(errors);&lt;br/&gt;
         Object[] partitionsArray = new Object&lt;span class=&quot;error&quot;&gt;&amp;#91;mappedPartitions.size()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
         int i = 0;&lt;br/&gt;
         for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; topicAndPartitions : mappedPartitions.entrySet()) {&lt;/li&gt;
	&lt;li&gt;Struct topicPartitionsStruct = struct.instance(TOPICS_KEY_NAME);&lt;br/&gt;
+            Struct topicPartitionsStruct = struct.instance(TOPICS);&lt;br/&gt;
             topicPartitionsStruct.set(TOPIC_NAME, topicAndPartitions.getKey());&lt;br/&gt;
             Map&amp;lt;Integer, Errors&amp;gt; partitionAndErrors = topicAndPartitions.getValue();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             Object[] partitionAndErrorsArray = new Object&lt;span class=&quot;error&quot;&gt;&amp;#91;partitionAndErrors.size()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
             int j = 0;&lt;br/&gt;
             for (Map.Entry&amp;lt;Integer, Errors&amp;gt; partitionAndError : partitionAndErrors.entrySet()) &lt;/p&gt;
{
-                Struct partitionAndErrorStruct = topicPartitionsStruct.instance(PARTITIONS_KEY_NAME);
+                Struct partitionAndErrorStruct = topicPartitionsStruct.instance(PARTITIONS);
                 partitionAndErrorStruct.set(PARTITION_ID, partitionAndError.getKey());
                 partitionAndErrorStruct.set(ERROR_CODE, partitionAndError.getValue().code());
                 partitionAndErrorsArray[j++] = partitionAndErrorStruct;
             }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;topicPartitionsStruct.set(PARTITIONS_KEY_NAME, partitionAndErrorsArray);&lt;br/&gt;
+            topicPartitionsStruct.set(PARTITIONS, partitionAndErrorsArray);&lt;br/&gt;
             partitionsArray&lt;span class=&quot;error&quot;&gt;&amp;#91;i++&amp;#93;&lt;/span&gt; = topicPartitionsStruct;&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;struct.set(TOPICS_KEY_NAME, partitionsArray);&lt;br/&gt;
+        struct.set(TOPICS, partitionsArray);&lt;br/&gt;
         return struct;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataRequest.java&lt;br/&gt;
index a273765704d..5c7b6f7eb73 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataRequest.java&lt;br/&gt;
@@ -284,7 +284,7 @@ public String toString() {&lt;/p&gt;

&lt;p&gt;     private UpdateMetadataRequest(short version, int controllerId, int controllerEpoch,&lt;br/&gt;
                                   Map&amp;lt;TopicPartition, PartitionState&amp;gt; partitionStates, Set&amp;lt;Broker&amp;gt; liveBrokers) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.UPDATE_METADATA, version);
         this.controllerId = controllerId;
         this.controllerEpoch = controllerEpoch;
         this.partitionStates = partitionStates;
@@ -292,7 +292,7 @@ private UpdateMetadataRequest(short version, int controllerId, int controllerEpo
     }

&lt;p&gt;     public UpdateMetadataRequest(Struct struct, short versionId) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(versionId);&lt;br/&gt;
+        super(ApiKeys.UPDATE_METADATA, versionId);&lt;br/&gt;
         Map&amp;lt;TopicPartition, PartitionState&amp;gt; partitionStates = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Object partitionStateDataObj : struct.getArray(PARTITION_STATES_KEY_NAME)) {&lt;br/&gt;
             Struct partitionStateData = (Struct) partitionStateDataObj;&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersRequest.java&lt;br/&gt;
index 3f7a0c9fc9b..33f9bb586d9 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersRequest.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersRequest.java&lt;br/&gt;
@@ -153,13 +153,13 @@ public WriteTxnMarkersRequest build(short version) {&lt;br/&gt;
     private final List&amp;lt;TxnMarkerEntry&amp;gt; markers;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private WriteTxnMarkersRequest(short version, List&amp;lt;TxnMarkerEntry&amp;gt; markers) &lt;/p&gt;
{
-        super(version);
+        super(ApiKeys.WRITE_TXN_MARKERS, version);
 
         this.markers = markers;
     }

&lt;p&gt;     public WriteTxnMarkersRequest(Struct struct, short version) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;super(version);&lt;br/&gt;
+        super(ApiKeys.WRITE_TXN_MARKERS, version);&lt;br/&gt;
         List&amp;lt;TxnMarkerEntry&amp;gt; markers = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
         Object[] markersArray = struct.getArray(TXN_MARKERS_KEY_NAME);&lt;br/&gt;
         for (Object markerObj : markersArray) {&lt;br/&gt;
@@ -204,7 +204,7 @@ protected Struct toStruct() {&lt;br/&gt;
             markerStruct.set(COORDINATOR_EPOCH_KEY_NAME, entry.coordinatorEpoch);&lt;br/&gt;
             markerStruct.set(TRANSACTION_RESULT_KEY_NAME, entry.result.id);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupDataByTopic(entry.partitions);&lt;br/&gt;
+            Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupPartitionsByTopic(entry.partitions);&lt;br/&gt;
             Object[] partitionsArray = new Object&lt;span class=&quot;error&quot;&gt;&amp;#91;mappedPartitions.size()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
             int j = 0;&lt;br/&gt;
             for (Map.Entry&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; topicAndPartitions : mappedPartitions.entrySet()) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersResponse.java&lt;br/&gt;
index f307760e123..92b5fc0b29d 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersResponse.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersResponse.java&lt;br/&gt;
@@ -118,7 +118,7 @@ protected Struct toStruct(short version) {&lt;br/&gt;
             responseStruct.set(PRODUCER_ID_KEY_NAME, responseEntry.getKey());&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             Map&amp;lt;TopicPartition, Errors&amp;gt; partitionAndErrors = responseEntry.getValue();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupDataByTopic(partitionAndErrors);&lt;br/&gt;
+            Map&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; mappedPartitions = CollectionUtils.groupPartitionDataByTopic(partitionAndErrors);&lt;br/&gt;
             Object[] partitionsArray = new Object&lt;span class=&quot;error&quot;&gt;&amp;#91;mappedPartitions.size()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
             int i = 0;&lt;br/&gt;
             for (Map.Entry&amp;lt;String, Map&amp;lt;Integer, Errors&amp;gt;&amp;gt; topicAndPartitions : mappedPartitions.entrySet()) {&lt;br/&gt;
diff --git a/clients/src/main/java/org/apache/kafka/common/utils/CollectionUtils.java b/clients/src/main/java/org/apache/kafka/common/utils/CollectionUtils.java&lt;br/&gt;
index 04fce647063..3489728c3f8 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/main/java/org/apache/kafka/common/utils/CollectionUtils.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/common/utils/CollectionUtils.java&lt;br/&gt;
@@ -39,41 +39,36 @@ private CollectionUtils() {}&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;group data by topic&lt;br/&gt;
+     *&lt;/li&gt;
	&lt;li&gt;@param data Data to be partitioned&lt;/li&gt;
	&lt;li&gt;@param &amp;lt;T&amp;gt; Partition data type&lt;/li&gt;
	&lt;li&gt;@return partitioned data&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static &amp;lt;T&amp;gt; Map&amp;lt;String, Map&amp;lt;Integer, T&amp;gt;&amp;gt; groupDataByTopic(Map&amp;lt;TopicPartition, ? extends T&amp;gt; data) {&lt;br/&gt;
+    public static &amp;lt;T&amp;gt; Map&amp;lt;String, Map&amp;lt;Integer, T&amp;gt;&amp;gt; groupPartitionDataByTopic(Map&amp;lt;TopicPartition, ? extends T&amp;gt; data) {&lt;br/&gt;
         Map&amp;lt;String, Map&amp;lt;Integer, T&amp;gt;&amp;gt; dataByTopic = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (Map.Entry&amp;lt;TopicPartition, ? extends T&amp;gt; entry: data.entrySet()) {&lt;br/&gt;
+        for (Map.Entry&amp;lt;TopicPartition, ? extends T&amp;gt; entry : data.entrySet()) {&lt;br/&gt;
             String topic = entry.getKey().topic();&lt;br/&gt;
             int partition = entry.getKey().partition();&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;Integer, T&amp;gt; topicData = dataByTopic.get(topic);&lt;/li&gt;
	&lt;li&gt;if (topicData == null) 
{
-                topicData = new HashMap&amp;lt;&amp;gt;();
-                dataByTopic.put(topic, topicData);
-            }
&lt;p&gt;+            Map&amp;lt;Integer, T&amp;gt; topicData = dataByTopic.computeIfAbsent(topic, t -&amp;gt; new HashMap&amp;lt;&amp;gt;());&lt;br/&gt;
             topicData.put(partition, entry.getValue());&lt;br/&gt;
         }&lt;br/&gt;
         return dataByTopic;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* group partitions by topic&lt;/li&gt;
	&lt;li&gt;* @param partitions&lt;br/&gt;
+     * Group a list of partitions by the topic name.&lt;br/&gt;
+     *&lt;br/&gt;
+     * @param partitions The partitions to collect&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return partitions per topic&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; groupDataByTopic(List&amp;lt;TopicPartition&amp;gt; partitions) {&lt;br/&gt;
+    public static Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; groupPartitionsByTopic(List&amp;lt;TopicPartition&amp;gt; partitions) {&lt;br/&gt;
         Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; partitionsByTopic = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (TopicPartition tp: partitions) {&lt;br/&gt;
+        for (TopicPartition tp : partitions) {&lt;br/&gt;
             String topic = tp.topic();&lt;/li&gt;
	&lt;li&gt;List&amp;lt;Integer&amp;gt; topicData = partitionsByTopic.get(topic);&lt;/li&gt;
	&lt;li&gt;if (topicData == null) 
{
-                topicData = new ArrayList&amp;lt;&amp;gt;();
-                partitionsByTopic.put(topic, topicData);
-            }
&lt;p&gt;+            List&amp;lt;Integer&amp;gt; topicData = partitionsByTopic.computeIfAbsent(topic, t -&amp;gt; new ArrayList&amp;lt;&amp;gt;());&lt;br/&gt;
             topicData.add(tp.partition());&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;return  partitionsByTopic;&lt;br/&gt;
+        return partitionsByTopic;&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java b/clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java&lt;br/&gt;
index 3095717e8dd..4c12fc68bab 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java&lt;br/&gt;
@@ -18,13 +18,13 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
+import org.apache.kafka.common.record.MemoryRecords;&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchResponse;&lt;br/&gt;
 import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
 import org.junit.Rule;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 import org.junit.rules.Timeout;&lt;br/&gt;
-import org.slf4j.Logger;&lt;/p&gt;

&lt;p&gt; import java.util.ArrayList;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
@@ -33,6 +33,7 @@&lt;br/&gt;
 import java.util.LinkedHashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.TreeSet;&lt;/p&gt;

&lt;p&gt;@@ -52,13 +53,11 @@&lt;/p&gt;

&lt;p&gt;     private static final LogContext LOG_CONTEXT = new LogContext(&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;FetchSessionHandler&amp;#93;&lt;/span&gt;=&quot;);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final Logger log = LOG_CONTEXT.logger(FetchSessionHandler.class);&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Create a set of TopicPartitions.  We use a TreeSet, in order to get a deterministic&lt;/li&gt;
	&lt;li&gt;ordering for test purposes.&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final static Set&amp;lt;TopicPartition&amp;gt; toSet(TopicPartition... arr) {&lt;br/&gt;
+    private static Set&amp;lt;TopicPartition&amp;gt; toSet(TopicPartition... arr) {&lt;br/&gt;
         TreeSet&amp;lt;TopicPartition&amp;gt; set = new TreeSet&amp;lt;&amp;gt;(new Comparator&amp;lt;TopicPartition&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public int compare(TopicPartition o1, TopicPartition o2) {&lt;br/&gt;
@@ -70,7 +69,7 @@ public int compare(TopicPartition o1, TopicPartition o2) {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testFindMissing() throws Exception {&lt;br/&gt;
+    public void testFindMissing() {&lt;br/&gt;
         TopicPartition foo0 = new TopicPartition(&quot;foo&quot;, 0);&lt;br/&gt;
         TopicPartition foo1 = new TopicPartition(&quot;foo&quot;, 1);&lt;br/&gt;
         TopicPartition bar0 = new TopicPartition(&quot;bar&quot;, 0);&lt;br/&gt;
@@ -95,7 +94,7 @@ public void testFindMissing() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         ReqEntry(String topic, int partition, long fetchOffset, long logStartOffset, int maxBytes) &lt;/p&gt;
{
             this.part = new TopicPartition(topic, partition);
-            this.data = new FetchRequest.PartitionData(fetchOffset, logStartOffset, maxBytes);
+            this.data = new FetchRequest.PartitionData(fetchOffset, logStartOffset, maxBytes, Optional.empty());
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;@@ -153,11 +152,11 @@ private static void assertListEquals(List&amp;lt;TopicPartition&amp;gt; expected, List&amp;lt;TopicPa&lt;/p&gt;

&lt;p&gt;     private static final class RespEntry {&lt;br/&gt;
         final TopicPartition part;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final FetchResponse.PartitionData data;&lt;br/&gt;
+        final FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt; data;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         RespEntry(String topic, int partition, long highWatermark, long lastStableOffset) &lt;/p&gt;
{
             this.part = new TopicPartition(topic, partition);
-            this.data = new FetchResponse.PartitionData(
+            this.data = new FetchResponse.PartitionData&amp;lt;&amp;gt;(
                 Errors.NONE,
                 highWatermark,
                 lastStableOffset,
@@ -167,8 +166,8 @@ private static void assertListEquals(List&amp;lt;TopicPartition&amp;gt; expected, List&amp;lt;TopicPa
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; respMap(RespEntry... entries) {&lt;/li&gt;
	&lt;li&gt;LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; map = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private static LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; respMap(RespEntry... entries) {&lt;br/&gt;
+        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; map = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (RespEntry entry : entries) 
{
             map.put(entry.part, entry.data);
         }
&lt;p&gt;@@ -180,13 +179,13 @@ private static void assertListEquals(List&amp;lt;TopicPartition&amp;gt; expected, List&amp;lt;TopicPa&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Pre-KIP-227 brokers always supply this kind of response.&lt;br/&gt;
      */&lt;br/&gt;
     @Test&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testSessionless() throws Exception {&lt;br/&gt;
+    public void testSessionless() {&lt;br/&gt;
         FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);&lt;br/&gt;
         FetchSessionHandler.Builder builder = handler.newBuilder();&lt;br/&gt;
         builder.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         builder.add(new TopicPartition(&quot;foo&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(10, 110, 210));&lt;br/&gt;
+            new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data = builder.build();&lt;br/&gt;
         assertMapsEqual(reqMap(new ReqEntry(&quot;foo&quot;, 0, 0, 100, 200),&lt;br/&gt;
                                new ReqEntry(&quot;foo&quot;, 1, 10, 110, 210)),&lt;br/&gt;
@@ -194,7 +193,7 @@ public void testSessionless() throws Exception {&lt;br/&gt;
         assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());&lt;br/&gt;
         assertEquals(INITIAL_EPOCH, data.metadata().epoch());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse resp = new FetchResponse(Errors.NONE,&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; resp = new FetchResponse&amp;lt;&amp;gt;(Errors.NONE,&lt;br/&gt;
             respMap(new RespEntry(&quot;foo&quot;, 0, 0, 0),&lt;br/&gt;
                     new RespEntry(&quot;foo&quot;, 1, 0, 0)),&lt;br/&gt;
             0, INVALID_SESSION_ID);&lt;br/&gt;
@@ -202,7 +201,7 @@ public void testSessionless() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         FetchSessionHandler.Builder builder2 = handler.newBuilder();&lt;br/&gt;
         builder2.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data2 = builder2.build();&lt;br/&gt;
         assertEquals(INVALID_SESSION_ID, data2.metadata().sessionId());&lt;br/&gt;
         assertEquals(INITIAL_EPOCH, data2.metadata().epoch());&lt;br/&gt;
@@ -214,13 +213,13 @@ public void testSessionless() throws Exception {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Test handling an incremental fetch session.&lt;br/&gt;
      */&lt;br/&gt;
     @Test&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testIncrementals() throws Exception {&lt;br/&gt;
+    public void testIncrementals() {&lt;br/&gt;
         FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);&lt;br/&gt;
         FetchSessionHandler.Builder builder = handler.newBuilder();&lt;br/&gt;
         builder.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         builder.add(new TopicPartition(&quot;foo&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(10, 110, 210));&lt;br/&gt;
+            new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data = builder.build();&lt;br/&gt;
         assertMapsEqual(reqMap(new ReqEntry(&quot;foo&quot;, 0, 0, 100, 200),&lt;br/&gt;
             new ReqEntry(&quot;foo&quot;, 1, 10, 110, 210)),&lt;br/&gt;
@@ -228,7 +227,7 @@ public void testIncrementals() throws Exception {&lt;br/&gt;
         assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());&lt;br/&gt;
         assertEquals(INITIAL_EPOCH, data.metadata().epoch());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse resp = new FetchResponse(Errors.NONE,&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; resp = new FetchResponse&amp;lt;&amp;gt;(Errors.NONE,&lt;br/&gt;
             respMap(new RespEntry(&quot;foo&quot;, 0, 10, 20),&lt;br/&gt;
                     new RespEntry(&quot;foo&quot;, 1, 10, 20)),&lt;br/&gt;
             0, 123);&lt;br/&gt;
@@ -237,11 +236,11 @@ public void testIncrementals() throws Exception {&lt;br/&gt;
         // Test an incremental fetch request which adds one partition and modifies another.&lt;br/&gt;
         FetchSessionHandler.Builder builder2 = handler.newBuilder();&lt;br/&gt;
         builder2.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         builder2.add(new TopicPartition(&quot;foo&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(10, 120, 210));&lt;br/&gt;
+            new FetchRequest.PartitionData(10, 120, 210, Optional.empty()));&lt;br/&gt;
         builder2.add(new TopicPartition(&quot;bar&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(20, 200, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(20, 200, 200, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data2 = builder2.build();&lt;br/&gt;
         assertFalse(data2.metadata().isFull());&lt;br/&gt;
         assertMapEquals(reqMap(new ReqEntry(&quot;foo&quot;, 0, 0, 100, 200),&lt;br/&gt;
@@ -252,24 +251,24 @@ public void testIncrementals() throws Exception {&lt;br/&gt;
                 new ReqEntry(&quot;foo&quot;, 1, 10, 120, 210)),&lt;br/&gt;
             data2.toSend());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse resp2 = new FetchResponse(Errors.NONE,&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; resp2 = new FetchResponse&amp;lt;&amp;gt;(Errors.NONE,&lt;br/&gt;
             respMap(new RespEntry(&quot;foo&quot;, 1, 20, 20)),&lt;br/&gt;
             0, 123);&lt;br/&gt;
         handler.handleResponse(resp2);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // Skip building a new request.  Test that handling an invalid fetch session epoch response results&lt;br/&gt;
         // in a request which closes the session.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse resp3 = new FetchResponse(Errors.INVALID_FETCH_SESSION_EPOCH, respMap(),&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; resp3 = new FetchResponse&amp;lt;&amp;gt;(Errors.INVALID_FETCH_SESSION_EPOCH, respMap(),&lt;br/&gt;
             0, INVALID_SESSION_ID);&lt;br/&gt;
         handler.handleResponse(resp3);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         FetchSessionHandler.Builder builder4 = handler.newBuilder();&lt;br/&gt;
         builder4.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         builder4.add(new TopicPartition(&quot;foo&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(10, 120, 210));&lt;br/&gt;
+            new FetchRequest.PartitionData(10, 120, 210, Optional.empty()));&lt;br/&gt;
         builder4.add(new TopicPartition(&quot;bar&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(20, 200, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(20, 200, 200, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data4 = builder4.build();&lt;br/&gt;
         assertTrue(data4.metadata().isFull());&lt;br/&gt;
         assertEquals(data2.metadata().sessionId(), data4.metadata().sessionId());&lt;br/&gt;
@@ -284,11 +283,11 @@ public void testIncrementals() throws Exception {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Test that calling FetchSessionHandler#Builder#build twice fails.&lt;br/&gt;
      */&lt;br/&gt;
     @Test&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testDoubleBuild() throws Exception {&lt;br/&gt;
+    public void testDoubleBuild() {&lt;br/&gt;
         FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);&lt;br/&gt;
         FetchSessionHandler.Builder builder = handler.newBuilder();&lt;br/&gt;
         builder.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         builder.build();&lt;br/&gt;
         try {&lt;br/&gt;
             builder.build();&lt;br/&gt;
@@ -299,15 +298,15 @@ public void testDoubleBuild() throws Exception {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testIncrementalPartitionRemoval() throws Exception {&lt;br/&gt;
+    public void testIncrementalPartitionRemoval() {&lt;br/&gt;
         FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);&lt;br/&gt;
         FetchSessionHandler.Builder builder = handler.newBuilder();&lt;br/&gt;
         builder.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         builder.add(new TopicPartition(&quot;foo&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(10, 110, 210));&lt;br/&gt;
+            new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));&lt;br/&gt;
         builder.add(new TopicPartition(&quot;bar&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(20, 120, 220));&lt;br/&gt;
+            new FetchRequest.PartitionData(20, 120, 220, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data = builder.build();&lt;br/&gt;
         assertMapsEqual(reqMap(new ReqEntry(&quot;foo&quot;, 0, 0, 100, 200),&lt;br/&gt;
             new ReqEntry(&quot;foo&quot;, 1, 10, 110, 210),&lt;br/&gt;
@@ -315,7 +314,7 @@ public void testIncrementalPartitionRemoval() throws Exception {&lt;br/&gt;
             data.toSend(), data.sessionPartitions());&lt;br/&gt;
         assertTrue(data.metadata().isFull());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse resp = new FetchResponse(Errors.NONE,&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; resp = new FetchResponse&amp;lt;&amp;gt;(Errors.NONE,&lt;br/&gt;
             respMap(new RespEntry(&quot;foo&quot;, 0, 10, 20),&lt;br/&gt;
                     new RespEntry(&quot;foo&quot;, 1, 10, 20),&lt;br/&gt;
                     new RespEntry(&quot;bar&quot;, 0, 10, 20)),&lt;br/&gt;
@@ -325,7 +324,7 @@ public void testIncrementalPartitionRemoval() throws Exception {&lt;br/&gt;
         // Test an incremental fetch request which removes two partitions.&lt;br/&gt;
         FetchSessionHandler.Builder builder2 = handler.newBuilder();&lt;br/&gt;
         builder2.add(new TopicPartition(&quot;foo&quot;, 1),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(10, 110, 210));&lt;br/&gt;
+            new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data2 = builder2.build();&lt;br/&gt;
         assertFalse(data2.metadata().isFull());&lt;br/&gt;
         assertEquals(123, data2.metadata().sessionId());&lt;br/&gt;
@@ -340,12 +339,12 @@ public void testIncrementalPartitionRemoval() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // A FETCH_SESSION_ID_NOT_FOUND response triggers us to close the session.&lt;br/&gt;
         // The next request is a session establishing FULL request.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse resp2 = new FetchResponse(Errors.FETCH_SESSION_ID_NOT_FOUND,&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; resp2 = new FetchResponse&amp;lt;&amp;gt;(Errors.FETCH_SESSION_ID_NOT_FOUND,&lt;br/&gt;
             respMap(), 0, INVALID_SESSION_ID);&lt;br/&gt;
         handler.handleResponse(resp2);&lt;br/&gt;
         FetchSessionHandler.Builder builder3 = handler.newBuilder();&lt;br/&gt;
         builder3.add(new TopicPartition(&quot;foo&quot;, 0),&lt;/li&gt;
	&lt;li&gt;new FetchRequest.PartitionData(0, 100, 200));&lt;br/&gt;
+            new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));&lt;br/&gt;
         FetchSessionHandler.FetchRequestData data3 = builder3.build();&lt;br/&gt;
         assertTrue(data3.metadata().isFull());&lt;br/&gt;
         assertEquals(INVALID_SESSION_ID, data3.metadata().sessionId());&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java&lt;br/&gt;
index c0dc542b159..34432c31034 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java&lt;br/&gt;
@@ -94,6 +94,7 @@&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.concurrent.ExecutionException;&lt;br/&gt;
 import java.util.concurrent.TimeUnit;&lt;br/&gt;
@@ -462,12 +463,13 @@ public void testMetadataRetries() throws Exception {&lt;br/&gt;
             env.kafkaClient().prepareResponse(new MetadataResponse(initializedCluster.nodes(),&lt;br/&gt;
                     initializedCluster.clusterResource().clusterId(),&lt;br/&gt;
                     initializedCluster.controller().id(),&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Collections.&amp;lt;MetadataResponse.TopicMetadata&amp;gt;emptyList()));&lt;br/&gt;
+                    Collections.emptyList()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             // Then we respond to the DescribeTopic request&lt;br/&gt;
             Node leader = initializedCluster.nodes().get(0);&lt;br/&gt;
             MetadataResponse.PartitionMetadata partitionMetadata = new MetadataResponse.PartitionMetadata(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Errors.NONE, 0, leader, singletonList(leader), singletonList(leader), singletonList(leader));&lt;br/&gt;
+                    Errors.NONE, 0, leader, Optional.of(10), singletonList(leader),&lt;br/&gt;
+                    singletonList(leader), singletonList(leader));&lt;br/&gt;
             env.kafkaClient().prepareResponse(new MetadataResponse(initializedCluster.nodes(),&lt;br/&gt;
                     initializedCluster.clusterResource().clusterId(), 1,&lt;br/&gt;
                     singletonList(new MetadataResponse.TopicMetadata(Errors.NONE, topic, false,&lt;br/&gt;
@@ -788,16 +790,17 @@ public void testDeleteRecords() throws Exception {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             List&amp;lt;MetadataResponse.TopicMetadata&amp;gt; t = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
             List&amp;lt;MetadataResponse.PartitionMetadata&amp;gt; p = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 0, nodes.get(0),&lt;/li&gt;
	&lt;li&gt;singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.&amp;lt;Node&amp;gt;emptyList()));&lt;/li&gt;
	&lt;li&gt;p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 1, nodes.get(0),&lt;/li&gt;
	&lt;li&gt;singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.&amp;lt;Node&amp;gt;emptyList()));&lt;br/&gt;
+            p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 0, nodes.get(0), Optional.of(5),&lt;br/&gt;
+                    singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.emptyList()));&lt;br/&gt;
+            p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 1, nodes.get(0), Optional.of(5),&lt;br/&gt;
+                    singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.emptyList()));&lt;br/&gt;
             p.add(new MetadataResponse.PartitionMetadata(Errors.LEADER_NOT_AVAILABLE, 2, null,&lt;/li&gt;
	&lt;li&gt;singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.&amp;lt;Node&amp;gt;emptyList()));&lt;/li&gt;
	&lt;li&gt;p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 3, nodes.get(0),&lt;/li&gt;
	&lt;li&gt;singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.&amp;lt;Node&amp;gt;emptyList()));&lt;/li&gt;
	&lt;li&gt;p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 4, nodes.get(0),&lt;/li&gt;
	&lt;li&gt;singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.&amp;lt;Node&amp;gt;emptyList()));&lt;br/&gt;
+                    Optional.empty(), singletonList(nodes.get(0)), singletonList(nodes.get(0)),&lt;br/&gt;
+                    Collections.emptyList()));&lt;br/&gt;
+            p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 3, nodes.get(0), Optional.of(5),&lt;br/&gt;
+                    singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.emptyList()));&lt;br/&gt;
+            p.add(new MetadataResponse.PartitionMetadata(Errors.NONE, 4, nodes.get(0), Optional.of(5),&lt;br/&gt;
+                    singletonList(nodes.get(0)), singletonList(nodes.get(0)), Collections.emptyList()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             t.add(new MetadataResponse.TopicMetadata(Errors.NONE, &quot;my_topic&quot;, false, p));&lt;/p&gt;

&lt;p&gt;@@ -1068,9 +1071,9 @@ public void testDescribeConsumerGroupOffsets() throws Exception {&lt;br/&gt;
             new Cluster(&lt;br/&gt;
                 &quot;mockClusterId&quot;,&lt;br/&gt;
                 nodes.values(),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collections.&amp;lt;PartitionInfo&amp;gt;emptyList(),&lt;/li&gt;
	&lt;li&gt;Collections.&amp;lt;String&amp;gt;emptySet(),&lt;/li&gt;
	&lt;li&gt;Collections.&amp;lt;String&amp;gt;emptySet(), nodes.get(0));&lt;br/&gt;
+                Collections.emptyList(),&lt;br/&gt;
+                Collections.emptySet(),&lt;br/&gt;
+                Collections.emptySet(), nodes.get(0));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {&lt;br/&gt;
             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());&lt;br/&gt;
@@ -1083,9 +1086,12 @@ public void testDescribeConsumerGroupOffsets() throws Exception {&lt;br/&gt;
             TopicPartition myTopicPartition2 = new TopicPartition(&quot;my_topic&quot;, 2);&lt;/p&gt;

&lt;p&gt;             final Map&amp;lt;TopicPartition, OffsetFetchResponse.PartitionData&amp;gt; responseData = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;responseData.put(myTopicPartition0, new OffsetFetchResponse.PartitionData(10, &quot;&quot;, Errors.NONE));&lt;/li&gt;
	&lt;li&gt;responseData.put(myTopicPartition1, new OffsetFetchResponse.PartitionData(0, &quot;&quot;, Errors.NONE));&lt;/li&gt;
	&lt;li&gt;responseData.put(myTopicPartition2, new OffsetFetchResponse.PartitionData(20, &quot;&quot;, Errors.NONE));&lt;br/&gt;
+            responseData.put(myTopicPartition0, new OffsetFetchResponse.PartitionData(10,&lt;br/&gt;
+                    Optional.empty(), &quot;&quot;, Errors.NONE));&lt;br/&gt;
+            responseData.put(myTopicPartition1, new OffsetFetchResponse.PartitionData(0,&lt;br/&gt;
+                    Optional.empty(), &quot;&quot;, Errors.NONE));&lt;br/&gt;
+            responseData.put(myTopicPartition2, new OffsetFetchResponse.PartitionData(20,&lt;br/&gt;
+                    Optional.empty(), &quot;&quot;, Errors.NONE));&lt;br/&gt;
             env.kafkaClient().prepareResponse(new OffsetFetchResponse(Errors.NONE, responseData));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final ListConsumerGroupOffsetsResult result = env.adminClient().listConsumerGroupOffsets(&quot;group-0&quot;);&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordTest.java&lt;br/&gt;
index 32736452004..aae269adf2b 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordTest.java&lt;br/&gt;
@@ -21,6 +21,8 @@&lt;br/&gt;
 import org.apache.kafka.common.record.TimestampType;&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt;+import java.util.Optional;&lt;br/&gt;
+&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;/p&gt;

&lt;p&gt; public class ConsumerRecordTest {&lt;br/&gt;
@@ -45,6 +47,7 @@ public void testOldConstructor() &lt;/p&gt;
{
         assertEquals(ConsumerRecord.NULL_CHECKSUM, record.checksum());
         assertEquals(ConsumerRecord.NULL_SIZE, record.serializedKeySize());
         assertEquals(ConsumerRecord.NULL_SIZE, record.serializedValueSize());
+        assertEquals(Optional.empty(), record.leaderEpoch());
         assertEquals(new RecordHeaders(), record.headers());
     }

&lt;p&gt;diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java&lt;br/&gt;
index a0f95c4aabf..8b8cea6cce1 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java&lt;br/&gt;
@@ -16,7 +16,6 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.clients.consumer;&lt;/p&gt;

&lt;p&gt;-import java.util.ArrayList;&lt;br/&gt;
 import org.apache.kafka.clients.ClientRequest;&lt;br/&gt;
 import org.apache.kafka.clients.KafkaClient;&lt;br/&gt;
 import org.apache.kafka.clients.Metadata;&lt;br/&gt;
@@ -83,6 +82,7 @@&lt;/p&gt;

&lt;p&gt; import java.nio.ByteBuffer;&lt;br/&gt;
 import java.time.Duration;&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
 import java.util.Collection;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
@@ -91,6 +91,7 @@&lt;br/&gt;
 import java.util.LinkedHashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.Queue;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
@@ -499,7 +500,7 @@ public void testFetchProgressWithMissingPartitionPosition() {&lt;br/&gt;
         Node node = cluster.nodes().get(0);&lt;/p&gt;

&lt;p&gt;         Metadata metadata = createMetadata();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;metadata.update(cluster, Collections.&amp;lt;String&amp;gt;emptySet(), time.milliseconds());&lt;br/&gt;
+        metadata.update(cluster, Collections.emptySet(), time.milliseconds());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         MockClient client = new MockClient(time, metadata);&lt;br/&gt;
         client.setNode(node);&lt;br/&gt;
@@ -514,10 +515,9 @@ public void testFetchProgressWithMissingPartitionPosition() {&lt;br/&gt;
                     @Override&lt;br/&gt;
                     public boolean matches(AbstractRequest body) &lt;/p&gt;
{
                         ListOffsetRequest request = (ListOffsetRequest) body;
-                        Map&amp;lt;TopicPartition, Long&amp;gt; expectedTimestamps = new HashMap&amp;lt;&amp;gt;();
-                        expectedTimestamps.put(tp0, ListOffsetRequest.LATEST_TIMESTAMP);
-                        expectedTimestamps.put(tp1, ListOffsetRequest.EARLIEST_TIMESTAMP);
-                        return expectedTimestamps.equals(request.partitionTimestamps());
+                        Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; timestamps = request.partitionTimestamps();
+                        return timestamps.get(tp0).timestamp == ListOffsetRequest.LATEST_TIMESTAMP &amp;amp;&amp;amp;
+                                timestamps.get(tp1).timestamp == ListOffsetRequest.EARLIEST_TIMESTAMP;
                     }
&lt;p&gt;                 }, listOffsetsResponse(Collections.singletonMap(tp0, 50L),&lt;br/&gt;
                         Collections.singletonMap(tp1, Errors.NOT_LEADER_FOR_PARTITION)));&lt;br/&gt;
@@ -1644,7 +1644,7 @@ private OffsetCommitResponse offsetCommitResponse(Map&amp;lt;TopicPartition, Errors&amp;gt; re&lt;/p&gt;

&lt;p&gt;     private JoinGroupResponse joinGroupFollowerResponse(PartitionAssignor assignor, int generationId, String memberId, String leaderId, Errors error) &lt;/p&gt;
{
         return new JoinGroupResponse(error, generationId, assignor.name(), memberId, leaderId,
-                Collections.&amp;lt;String, ByteBuffer&amp;gt;emptyMap());
+                Collections.emptyMap());
     }

&lt;p&gt;     private SyncGroupResponse syncGroupResponse(List&amp;lt;TopicPartition&amp;gt; partitions, Errors error) {&lt;br/&gt;
@@ -1655,7 +1655,8 @@ private SyncGroupResponse syncGroupResponse(List&amp;lt;TopicPartition&amp;gt; partitions, Err&lt;br/&gt;
     private OffsetFetchResponse offsetResponse(Map&amp;lt;TopicPartition, Long&amp;gt; offsets, Errors error) {&lt;br/&gt;
         Map&amp;lt;TopicPartition, OffsetFetchResponse.PartitionData&amp;gt; partitionData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : offsets.entrySet()) &lt;/p&gt;
{
-            partitionData.put(entry.getKey(), new OffsetFetchResponse.PartitionData(entry.getValue(), &quot;&quot;, error));
+            partitionData.put(entry.getKey(), new OffsetFetchResponse.PartitionData(entry.getValue(),
+                    Optional.empty(), &quot;&quot;, error));
         }
&lt;p&gt;         return new OffsetFetchResponse(Errors.NONE, partitionData);&lt;br/&gt;
     }&lt;br/&gt;
@@ -1669,13 +1670,14 @@ private ListOffsetResponse listOffsetsResponse(Map&amp;lt;TopicPartition, Long&amp;gt; partiti&lt;br/&gt;
         Map&amp;lt;TopicPartition, ListOffsetResponse.PartitionData&amp;gt; partitionData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (Map.Entry&amp;lt;TopicPartition, Long&amp;gt; partitionOffset : partitionOffsets.entrySet()) &lt;/p&gt;
{
             partitionData.put(partitionOffset.getKey(), new ListOffsetResponse.PartitionData(Errors.NONE,
-                    ListOffsetResponse.UNKNOWN_TIMESTAMP, partitionOffset.getValue()));
+                    ListOffsetResponse.UNKNOWN_TIMESTAMP, partitionOffset.getValue(),
+                    Optional.empty()));
         }

&lt;p&gt;         for (Map.Entry&amp;lt;TopicPartition, Errors&amp;gt; partitionError : partitionErrors.entrySet()) &lt;/p&gt;
{
             partitionData.put(partitionError.getKey(), new ListOffsetResponse.PartitionData(
                     partitionError.getValue(), ListOffsetResponse.UNKNOWN_TIMESTAMP,
-                    ListOffsetResponse.UNKNOWN_OFFSET));
+                    ListOffsetResponse.UNKNOWN_OFFSET, Optional.empty()));
         }

&lt;p&gt;         return new ListOffsetResponse(partitionData);&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/SerializeCompatibilityOffsetAndMetadataTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/OffsetAndMetadataTest.java&lt;br/&gt;
similarity index 50%&lt;br/&gt;
rename from clients/src/test/java/org/apache/kafka/clients/consumer/SerializeCompatibilityOffsetAndMetadataTest.java&lt;br/&gt;
rename to clients/src/test/java/org/apache/kafka/clients/consumer/OffsetAndMetadataTest.java&lt;br/&gt;
index 324aeafd886..5bdbf7c7a32 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/SerializeCompatibilityOffsetAndMetadataTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/OffsetAndMetadataTest.java&lt;br/&gt;
@@ -20,9 +20,8 @@&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt; import java.io.IOException;&lt;br/&gt;
+import java.util.Optional;&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
-import static org.junit.Assert.assertTrue;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;/p&gt;

&lt;p&gt; /**&lt;br/&gt;
@@ -30,35 +29,38 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Note: this ensures that the current code can deserialize data serialized with older versions of the code, but not the reverse.&lt;/li&gt;
	&lt;li&gt;That is, older code won&apos;t necessarily be able to deserialize data serialized with newer code.&lt;br/&gt;
  */&lt;br/&gt;
-public class SerializeCompatibilityOffsetAndMetadataTest {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private String metadata = &quot;test commit metadata&quot;;&lt;/li&gt;
	&lt;li&gt;private String fileName = &quot;serializedData/offsetAndMetadataSerializedfile&quot;;&lt;/li&gt;
	&lt;li&gt;private long offset = 10;&lt;br/&gt;
+public class OffsetAndMetadataTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void checkValues(OffsetAndMetadata deSerOAM) {&lt;/li&gt;
	&lt;li&gt;//assert deserialized values are same as original&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;Offset should be &quot; + offset + &quot; but got &quot; + deSerOAM.offset(), offset, deSerOAM.offset());&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;metadata should be &quot; + metadata + &quot; but got &quot; + deSerOAM.metadata(), metadata, deSerOAM.metadata());&lt;br/&gt;
+    @Test(expected = IllegalArgumentException.class)&lt;br/&gt;
+    public void testInvalidNegativeOffset() 
{
+        new OffsetAndMetadata(-239L, Optional.of(15), &quot;&quot;);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void testSerializationRoundtrip() throws IOException, ClassNotFoundException &lt;/p&gt;
{
-        //assert OffsetAndMetadata is serializable
-        OffsetAndMetadata origOAM = new OffsetAndMetadata(offset, metadata);
-        byte[] byteArray =  Serializer.serialize(origOAM);
+        checkSerde(new OffsetAndMetadata(239L, Optional.of(15), &quot;blah&quot;));
+        checkSerde(new OffsetAndMetadata(239L, &quot;blah&quot;));
+        checkSerde(new OffsetAndMetadata(239L));
+    }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;//deserialize the byteArray and check if the values are same as original&lt;/li&gt;
	&lt;li&gt;Object deserializedObject = Serializer.deserialize(byteArray);&lt;/li&gt;
	&lt;li&gt;assertTrue(deserializedObject instanceof OffsetAndMetadata);&lt;/li&gt;
	&lt;li&gt;checkValues((OffsetAndMetadata) deserializedObject);&lt;br/&gt;
+    private void checkSerde(OffsetAndMetadata offsetAndMetadata) throws IOException, ClassNotFoundException 
{
+        byte[] bytes =  Serializer.serialize(offsetAndMetadata);
+        OffsetAndMetadata deserialized = (OffsetAndMetadata) Serializer.deserialize(bytes);
+        assertEquals(offsetAndMetadata, deserialized);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testOffsetMetadataSerializationCompatibility() throws IOException, ClassNotFoundException {&lt;/li&gt;
	&lt;li&gt;// assert serialized OffsetAndMetadata object in file (oamserializedfile under resources folder) is&lt;/li&gt;
	&lt;li&gt;// deserializable into OffsetAndMetadata and is compatible&lt;br/&gt;
+    public void testDeserializationCompatibilityBeforeLeaderEpoch() throws IOException, ClassNotFoundException 
{
+        String fileName = &quot;serializedData/offsetAndMetadataBeforeLeaderEpoch&quot;;
         Object deserializedObject = Serializer.deserialize(fileName);
-        assertTrue(deserializedObject instanceof OffsetAndMetadata);
-        checkValues((OffsetAndMetadata) deserializedObject);
+        assertEquals(new OffsetAndMetadata(10, &quot;test commit metadata&quot;), deserializedObject);
     }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void testDeserializationCompatibilityWithLeaderEpoch() throws IOException, ClassNotFoundException &lt;/p&gt;
{
+        String fileName = &quot;serializedData/offsetAndMetadataWithLeaderEpoch&quot;;
+        Object deserializedObject = Serializer.deserialize(fileName);
+        assertEquals(new OffsetAndMetadata(10, Optional.of(235), &quot;test commit metadata&quot;), deserializedObject);
+    }
&lt;p&gt;+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java&lt;br/&gt;
index 90e3b99870c..31cee7edbf0 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java&lt;br/&gt;
@@ -711,8 +711,8 @@ private static void verifyValidityAndBalance(Map&amp;lt;String, Subscription&amp;gt; subscript&lt;br/&gt;
                 if (Math.abs(len - otherLen) &amp;lt;= 1)&lt;br/&gt;
                     continue;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; map = CollectionUtils.groupDataByTopic(partitions);&lt;/li&gt;
	&lt;li&gt;Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; otherMap = CollectionUtils.groupDataByTopic(otherPartitions);&lt;br/&gt;
+                Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; map = CollectionUtils.groupPartitionsByTopic(partitions);&lt;br/&gt;
+                Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt; otherMap = CollectionUtils.groupPartitionsByTopic(otherPartitions);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 if (len &amp;gt; otherLen) {&lt;br/&gt;
                     for (String topic: map.keySet())&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java&lt;br/&gt;
index 2b6d3037728..af073f1d560 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java&lt;br/&gt;
@@ -42,6 +42,7 @@&lt;br/&gt;
 import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics;&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors;&lt;br/&gt;
+import org.apache.kafka.common.record.RecordBatch;&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.FindCoordinatorResponse;&lt;br/&gt;
 import org.apache.kafka.common.requests.HeartbeatResponse;&lt;br/&gt;
@@ -72,6 +73,7 @@&lt;br/&gt;
 import java.util.HashSet;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.concurrent.ExecutorService;&lt;br/&gt;
 import java.util.concurrent.Executors;&lt;br/&gt;
@@ -88,6 +90,7 @@&lt;br/&gt;
 import static java.util.Collections.singletonMap;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertFalse;&lt;br/&gt;
+import static org.junit.Assert.assertNotNull;&lt;br/&gt;
 import static org.junit.Assert.assertNull;&lt;br/&gt;
 import static org.junit.Assert.assertTrue;&lt;br/&gt;
 import static org.junit.Assert.fail;&lt;br/&gt;
@@ -245,7 +248,8 @@ public void testCoordinatorUnknownInUnsentCallbacksAfterCoordinatorDead() throws&lt;/p&gt;

&lt;p&gt;         final AtomicBoolean asyncCallbackInvoked = new AtomicBoolean(false);&lt;br/&gt;
         Map&amp;lt;TopicPartition, OffsetCommitRequest.PartitionData&amp;gt; offsets = singletonMap(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new TopicPartition(&quot;foo&quot;, 0), new OffsetCommitRequest.PartitionData(13L, &quot;&quot;));&lt;br/&gt;
+                new TopicPartition(&quot;foo&quot;, 0), new OffsetCommitRequest.PartitionData(13L,&lt;br/&gt;
+                        RecordBatch.NO_PARTITION_LEADER_EPOCH, &quot;&quot;));&lt;br/&gt;
         consumerClient.send(coordinator.checkAndGetCoordinator(), new OffsetCommitRequest.Builder(groupId, offsets))&lt;br/&gt;
                 .compose(new RequestFutureAdapter&amp;lt;ClientResponse, Object&amp;gt;() {&lt;br/&gt;
                     @Override&lt;br/&gt;
@@ -1571,22 +1575,6 @@ public void testCommitOffsetSyncCallbackWithNonRetriableException() 
{
         coordinator.commitOffsetsSync(singletonMap(t1p, new OffsetAndMetadata(100L)), time.timer(Long.MAX_VALUE));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test(expected = IllegalArgumentException.class)&lt;/li&gt;
	&lt;li&gt;public void testCommitSyncNegativeOffset() 
{
-        client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));
-        coordinator.commitOffsetsSync(singletonMap(t1p, new OffsetAndMetadata(-1L)), time.timer(Long.MAX_VALUE));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testCommitAsyncNegativeOffset() 
{
-        int invokedBeforeTest = mockOffsetCommitCallback.invoked;
-        client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));
-        coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(-1L)), mockOffsetCommitCallback);
-        coordinator.invokeCompletedOffsetCommitCallbacks();
-        assertEquals(invokedBeforeTest + 1, mockOffsetCommitCallback.invoked);
-        assertTrue(mockOffsetCommitCallback.exception instanceof IllegalArgumentException);
-    }
&lt;p&gt;-&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testCommitOffsetSyncWithoutFutureGetsCompleted() {&lt;br/&gt;
         client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));&lt;br/&gt;
@@ -1608,6 +1596,25 @@ public void testRefreshOffset() &lt;/p&gt;
{
         assertEquals(100L, subscriptions.position(t1p).longValue());
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testFetchCommittedOffsets() &lt;/p&gt;
{
+        client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));
+        coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));
+
+        long offset = 500L;
+        String metadata = &quot;blahblah&quot;;
+        Optional&amp;lt;Integer&amp;gt; leaderEpoch = Optional.of(15);
+        OffsetFetchResponse.PartitionData data = new OffsetFetchResponse.PartitionData(offset, leaderEpoch,
+                metadata, Errors.NONE);
+
+        client.prepareResponse(new OffsetFetchResponse(Errors.NONE, singletonMap(t1p, data)));
+        Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; fetchedOffsets = coordinator.fetchCommittedOffsets(singleton(t1p),
+                time.timer(Long.MAX_VALUE));
+
+        assertNotNull(fetchedOffsets);
+        assertEquals(new OffsetAndMetadata(offset, leaderEpoch, metadata), fetchedOffsets.get(t1p));
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testRefreshOffsetLoadInProgress() {&lt;br/&gt;
         client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));&lt;br/&gt;
@@ -2074,7 +2081,8 @@ private OffsetFetchResponse offsetFetchResponse(Errors topLevelError) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private OffsetFetchResponse offsetFetchResponse(TopicPartition tp, Errors partitionLevelError, String metadata, long offset) &lt;/p&gt;
{
-        OffsetFetchResponse.PartitionData data = new OffsetFetchResponse.PartitionData(offset, metadata, partitionLevelError);
+        OffsetFetchResponse.PartitionData data = new OffsetFetchResponse.PartitionData(offset,
+                Optional.empty(), metadata, partitionLevelError);
         return new OffsetFetchResponse(Errors.NONE, singletonMap(tp, data));
     }

&lt;p&gt;diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 1a82faa0a3d..69c4b58986d 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -62,11 +62,10 @@&lt;br/&gt;
 import org.apache.kafka.common.record.Records;&lt;br/&gt;
 import org.apache.kafka.common.record.SimpleRecord;&lt;br/&gt;
 import org.apache.kafka.common.record.TimestampType;&lt;br/&gt;
-import org.apache.kafka.common.requests.FetchResponse;&lt;br/&gt;
 import org.apache.kafka.common.requests.AbstractRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.ApiVersionsResponse;&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest;&lt;br/&gt;
-import org.apache.kafka.common.requests.FetchRequest.PartitionData;&lt;br/&gt;
+import org.apache.kafka.common.requests.FetchResponse;&lt;br/&gt;
 import org.apache.kafka.common.requests.IsolationLevel;&lt;br/&gt;
 import org.apache.kafka.common.requests.ListOffsetRequest;&lt;br/&gt;
 import org.apache.kafka.common.requests.ListOffsetResponse;&lt;br/&gt;
@@ -100,6 +99,7 @@&lt;br/&gt;
 import java.util.LinkedHashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;/p&gt;

&lt;p&gt; import static java.util.Collections.singleton;&lt;br/&gt;
@@ -208,6 +208,85 @@ public void testFetchNormal() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testMissingLeaderEpochInRecords() {&lt;br/&gt;
+        subscriptions.assignFromUser(singleton(tp0));&lt;br/&gt;
+        subscriptions.seek(tp0, 0);&lt;br/&gt;
+&lt;br/&gt;
+        ByteBuffer buffer = ByteBuffer.allocate(1024);&lt;br/&gt;
+        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V0,&lt;br/&gt;
+                CompressionType.NONE, TimestampType.CREATE_TIME, 0L, System.currentTimeMillis(),&lt;br/&gt;
+                RecordBatch.NO_PARTITION_LEADER_EPOCH);&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), &quot;1&quot;.getBytes());&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), &quot;2&quot;.getBytes());&lt;br/&gt;
+        MemoryRecords records = builder.build();&lt;br/&gt;
+&lt;br/&gt;
+        assertEquals(1, fetcher.sendFetches());&lt;br/&gt;
+        assertFalse(fetcher.hasCompletedFetches());&lt;br/&gt;
+&lt;br/&gt;
+        client.prepareResponse(fullFetchResponse(tp0, records, Errors.NONE, 100L, 0));&lt;br/&gt;
+        consumerClient.poll(time.timer(0));&lt;br/&gt;
+        assertTrue(fetcher.hasCompletedFetches());&lt;br/&gt;
+&lt;br/&gt;
+        Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; partitionRecords = fetcher.fetchedRecords();&lt;br/&gt;
+        assertTrue(partitionRecords.containsKey(tp0));&lt;br/&gt;
+        assertEquals(2, partitionRecords.get(tp0).size());&lt;br/&gt;
+&lt;br/&gt;
+        for (ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : partitionRecords.get(tp0)) &lt;/p&gt;
{
+            assertEquals(Optional.empty(), record.leaderEpoch());
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void testLeaderEpochInConsumerRecord() {&lt;br/&gt;
+        subscriptions.assignFromUser(singleton(tp0));&lt;br/&gt;
+        subscriptions.seek(tp0, 0);&lt;br/&gt;
+&lt;br/&gt;
+        Integer partitionLeaderEpoch = 1;&lt;br/&gt;
+&lt;br/&gt;
+        ByteBuffer buffer = ByteBuffer.allocate(1024);&lt;br/&gt;
+        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE,&lt;br/&gt;
+                CompressionType.NONE, TimestampType.CREATE_TIME, 0L, System.currentTimeMillis(),&lt;br/&gt;
+                partitionLeaderEpoch);&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), partitionLeaderEpoch.toString().getBytes());&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), partitionLeaderEpoch.toString().getBytes());&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+&lt;br/&gt;
+        partitionLeaderEpoch += 7;&lt;br/&gt;
+&lt;br/&gt;
+        builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE,&lt;br/&gt;
+                TimestampType.CREATE_TIME, 2L, System.currentTimeMillis(), partitionLeaderEpoch);&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), partitionLeaderEpoch.toString().getBytes());&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+&lt;br/&gt;
+        partitionLeaderEpoch += 5;&lt;br/&gt;
+        builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE,&lt;br/&gt;
+                TimestampType.CREATE_TIME, 3L, System.currentTimeMillis(), partitionLeaderEpoch);&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), partitionLeaderEpoch.toString().getBytes());&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), partitionLeaderEpoch.toString().getBytes());&lt;br/&gt;
+        builder.append(0L, &quot;key&quot;.getBytes(), partitionLeaderEpoch.toString().getBytes());&lt;br/&gt;
+        builder.close();&lt;br/&gt;
+&lt;br/&gt;
+        buffer.flip();&lt;br/&gt;
+        MemoryRecords records = MemoryRecords.readableRecords(buffer);&lt;br/&gt;
+&lt;br/&gt;
+        assertEquals(1, fetcher.sendFetches());&lt;br/&gt;
+        assertFalse(fetcher.hasCompletedFetches());&lt;br/&gt;
+&lt;br/&gt;
+        client.prepareResponse(fullFetchResponse(tp0, records, Errors.NONE, 100L, 0));&lt;br/&gt;
+        consumerClient.poll(time.timer(0));&lt;br/&gt;
+        assertTrue(fetcher.hasCompletedFetches());&lt;br/&gt;
+&lt;br/&gt;
+        Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; partitionRecords = fetcher.fetchedRecords();&lt;br/&gt;
+        assertTrue(partitionRecords.containsKey(tp0));&lt;br/&gt;
+        assertEquals(6, partitionRecords.get(tp0).size());&lt;br/&gt;
+&lt;br/&gt;
+        for (ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : partitionRecords.get(tp0)) &lt;/p&gt;
{
+            int expectedLeaderEpoch = Integer.parseInt(Utils.utf8(record.value()));
+            assertEquals(Optional.of(expectedLeaderEpoch), record.leaderEpoch());
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void testFetchSkipsBlackedOutNodes() {&lt;br/&gt;
         subscriptions.assignFromUser(singleton(tp0));&lt;br/&gt;
@@ -1403,6 +1482,7 @@ public void testGetTopicMetadataOfflinePartitions() {&lt;br/&gt;
                     p.error(),&lt;br/&gt;
                     p.partition(),&lt;br/&gt;
                     null, //no leader&lt;br/&gt;
+                    Optional.empty(),&lt;br/&gt;
                     p.replicas(),&lt;br/&gt;
                     p.isr(),&lt;br/&gt;
                     p.offlineReplicas())&lt;br/&gt;
@@ -1843,9 +1923,11 @@ public void testGetOffsetsForTimes() {&lt;br/&gt;
     public void testBatchedListOffsetsMetadataErrors() {&lt;br/&gt;
         Map&amp;lt;TopicPartition, ListOffsetResponse.PartitionData&amp;gt; partitionData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         partitionData.put(tp0, new ListOffsetResponse.PartitionData(Errors.NOT_LEADER_FOR_PARTITION,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET));&lt;br/&gt;
+                ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET,&lt;br/&gt;
+                Optional.empty()));&lt;br/&gt;
         partitionData.put(tp1, new ListOffsetResponse.PartitionData(Errors.UNKNOWN_TOPIC_OR_PARTITION,&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET));&lt;br/&gt;
+                ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET,&lt;br/&gt;
+                Optional.empty()));&lt;br/&gt;
         client.prepareResponse(new ListOffsetResponse(0, partitionData));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         Map&amp;lt;TopicPartition, Long&amp;gt; offsetsToSearch = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -2454,7 +2536,8 @@ private void testGetOffsetsForTimesWithUnknownOffset() {&lt;/p&gt;

&lt;p&gt;         Map&amp;lt;TopicPartition, ListOffsetResponse.PartitionData&amp;gt; partitionData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         partitionData.put(tp0, new ListOffsetResponse.PartitionData(Errors.NONE,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET));&lt;br/&gt;
+                ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET,&lt;br/&gt;
+                Optional.empty()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         client.prepareResponseFrom(new ListOffsetResponse(0, partitionData), cluster.leaderFor(tp0));&lt;/p&gt;

&lt;p&gt;@@ -2473,7 +2556,7 @@ private void testGetOffsetsForTimesWithUnknownOffset() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public boolean matches(AbstractRequest body) &lt;/p&gt;
{
                 ListOffsetRequest req = (ListOffsetRequest) body;
-                return timestamp == req.partitionTimestamps().get(tp0);
+                return timestamp == req.partitionTimestamps().get(tp0).timestamp;
             }
&lt;p&gt;         };&lt;br/&gt;
     }&lt;br/&gt;
@@ -2483,7 +2566,8 @@ private ListOffsetResponse listOffsetResponse(Errors error, long timestamp, long&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private ListOffsetResponse listOffsetResponse(TopicPartition tp, Errors error, long timestamp, long offset) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ListOffsetResponse.PartitionData partitionData = new ListOffsetResponse.PartitionData(error, timestamp, offset);&lt;br/&gt;
+        ListOffsetResponse.PartitionData partitionData = new ListOffsetResponse.PartitionData(error, timestamp, offset,&lt;br/&gt;
+                Optional.empty());&lt;br/&gt;
         Map&amp;lt;TopicPartition, ListOffsetResponse.PartitionData&amp;gt; allPartitionData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         allPartitionData.put(tp, partitionData);&lt;br/&gt;
         return new ListOffsetResponse(allPartitionData);&lt;br/&gt;
@@ -2526,6 +2610,7 @@ private MetadataResponse newMetadataResponse(String topic, Errors error) {&lt;br/&gt;
                         Errors.NONE,&lt;br/&gt;
                         partitionInfo.partition(),&lt;br/&gt;
                         partitionInfo.leader(),&lt;br/&gt;
+                        Optional.empty(),&lt;br/&gt;
                         Arrays.asList(partitionInfo.replicas()),&lt;br/&gt;
                         Arrays.asList(partitionInfo.inSyncReplicas()),&lt;br/&gt;
                         Arrays.asList(partitionInfo.offlineReplicas())));&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java&lt;br/&gt;
index 606fa714a34..cf730b9cb12 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java&lt;br/&gt;
@@ -2429,19 +2429,17 @@ public boolean matches(AbstractRequest body) {&lt;br/&gt;
         }, new AddOffsetsToTxnResponse(0, error));&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void prepareTxnOffsetCommitResponse(final String consumerGroupId, final long producerId,&lt;/li&gt;
	&lt;li&gt;final short producerEpoch, Map&amp;lt;TopicPartition, Errors&amp;gt; txnOffsetCommitResponse) {&lt;/li&gt;
	&lt;li&gt;client.prepareResponse(new MockClient.RequestMatcher() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public boolean matches(AbstractRequest body) 
{
-                TxnOffsetCommitRequest txnOffsetCommitRequest = (TxnOffsetCommitRequest) body;
-                assertEquals(consumerGroupId, txnOffsetCommitRequest.consumerGroupId());
-                assertEquals(producerId, txnOffsetCommitRequest.producerId());
-                assertEquals(producerEpoch, txnOffsetCommitRequest.producerEpoch());
-                return true;
-            }
&lt;p&gt;+    private void prepareTxnOffsetCommitResponse(final String consumerGroupId,&lt;br/&gt;
+                                                final long producerId,&lt;br/&gt;
+                                                final short producerEpoch,&lt;br/&gt;
+                                                Map&amp;lt;TopicPartition, Errors&amp;gt; txnOffsetCommitResponse) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        client.prepareResponse(request -&amp;gt; {
+            TxnOffsetCommitRequest txnOffsetCommitRequest = (TxnOffsetCommitRequest) request;
+            assertEquals(consumerGroupId, txnOffsetCommitRequest.consumerGroupId());
+            assertEquals(producerId, txnOffsetCommitRequest.producerId());
+            assertEquals(producerEpoch, txnOffsetCommitRequest.producerEpoch());
+            return true;
         }, new TxnOffsetCommitResponse(0, txnOffsetCommitResponse));-     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private ProduceResponse produceResponse(TopicPartition tp, long offset, Errors error, int throttleTimeMs) {&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/SerializeCompatibilityTopicPartitionTest.java b/clients/src/test/java/org/apache/kafka/common/TopicPartitionTest.java&lt;br/&gt;
similarity index 98%&lt;br/&gt;
rename from clients/src/test/java/org/apache/kafka/common/SerializeCompatibilityTopicPartitionTest.java&lt;br/&gt;
rename to clients/src/test/java/org/apache/kafka/common/TopicPartitionTest.java&lt;br/&gt;
index 8b4df5ff01f..2a90338c64e 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/common/SerializeCompatibilityTopicPartitionTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/TopicPartitionTest.java&lt;br/&gt;
@@ -29,8 +29,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Note: this ensures that the current code can deserialize data serialized with older versions of the code, but not the reverse.&lt;/li&gt;
	&lt;li&gt;That is, older code won&apos;t necessarily be able to deserialize data serialized with newer code.&lt;br/&gt;
  */&lt;br/&gt;
-public class SerializeCompatibilityTopicPartitionTest {&lt;br/&gt;
-&lt;br/&gt;
+public class TopicPartitionTest {&lt;br/&gt;
     private String topicName = &quot;mytopic&quot;;&lt;br/&gt;
     private String fileName = &quot;serializedData/topicPartitionSerializedfile&quot;;&lt;br/&gt;
     private int partNum = 5;&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java b/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
index da613982816..05b9926d6ca 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java&lt;br/&gt;
@@ -72,6 +72,7 @@&lt;br/&gt;
 import java.util.LinkedList;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.Optional;&lt;br/&gt;
 import java.util.Set;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static java.util.Arrays.asList;&lt;br/&gt;
@@ -469,14 +470,15 @@ public void produceResponseVersionTest() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void fetchResponseVersionTest() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;responseData.put(new TopicPartition(&quot;test&quot;, 0), new FetchResponse.PartitionData(Errors.NONE, 1000000,&lt;/li&gt;
	&lt;li&gt;FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));&lt;br/&gt;
+        responseData.put(new TopicPartition(&quot;test&quot;, 0), new FetchResponse.PartitionData&amp;lt;&amp;gt;(&lt;br/&gt;
+                Errors.NONE, 1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET,&lt;br/&gt;
+                0L, null, records));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FetchResponse v0Response = new FetchResponse(Errors.NONE, responseData, 0, INVALID_SESSION_ID);&lt;/li&gt;
	&lt;li&gt;FetchResponse v1Response = new FetchResponse(Errors.NONE, responseData, 10, INVALID_SESSION_ID);&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; v0Response = new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, responseData, 0, INVALID_SESSION_ID);&lt;br/&gt;
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; v1Response = new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, responseData, 10, INVALID_SESSION_ID);&lt;br/&gt;
         assertEquals(&quot;Throttle time must be zero&quot;, 0, v0Response.throttleTimeMs());&lt;br/&gt;
         assertEquals(&quot;Throttle time must be 10&quot;, 10, v1Response.throttleTimeMs());&lt;br/&gt;
         assertEquals(&quot;Should use schema version 0&quot;, ApiKeys.FETCH.responseSchema((short) 0),&lt;br/&gt;
@@ -489,22 +491,21 @@ public void fetchResponseVersionTest() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void testFetchResponseV4() &lt;/p&gt;
{
-        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();
-
+        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();
         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));
 
         List&amp;lt;FetchResponse.AbortedTransaction&amp;gt; abortedTransactions = asList(
                 new FetchResponse.AbortedTransaction(10, 100),
                 new FetchResponse.AbortedTransaction(15, 50)
         );
-        responseData.put(new TopicPartition(&quot;bar&quot;, 0), new FetchResponse.PartitionData(Errors.NONE, 100000,
+        responseData.put(new TopicPartition(&quot;bar&quot;, 0), new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE, 100000,
                 FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, abortedTransactions, records));
-        responseData.put(new TopicPartition(&quot;bar&quot;, 1), new FetchResponse.PartitionData(Errors.NONE, 900000,
+        responseData.put(new TopicPartition(&quot;bar&quot;, 1), new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE, 900000,
                 5, FetchResponse.INVALID_LOG_START_OFFSET, null, records));
-        responseData.put(new TopicPartition(&quot;foo&quot;, 0), new FetchResponse.PartitionData(Errors.NONE, 70000,
-                6, FetchResponse.INVALID_LOG_START_OFFSET, Collections.&amp;lt;FetchResponse.AbortedTransaction&amp;gt;emptyList(), records));
+        responseData.put(new TopicPartition(&quot;foo&quot;, 0), new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE, 70000,
+                6, FetchResponse.INVALID_LOG_START_OFFSET, Collections.emptyList(), records));
 
-        FetchResponse response = new FetchResponse(Errors.NONE, responseData, 10, INVALID_SESSION_ID);
+        FetchResponse&amp;lt;MemoryRecords&amp;gt; response = new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, responseData, 10, INVALID_SESSION_ID);
         FetchResponse deserialized = FetchResponse.parse(toBuffer(response.toStruct((short) 4)), (short) 4);
         assertEquals(responseData, deserialized.responseData());
     }
&lt;p&gt;@@ -595,7 +596,7 @@ public void testFetchRequestWithMetadata() throws Exception {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void testJoinGroupRequestVersion0RebalanceTimeout() throws Exception {&lt;br/&gt;
+    public void testJoinGroupRequestVersion0RebalanceTimeout() {&lt;br/&gt;
         final short version = 0;&lt;br/&gt;
         JoinGroupRequest jgr = createJoinGroupRequest(version);&lt;br/&gt;
         JoinGroupRequest jgr2 = new JoinGroupRequest(jgr.toStruct(), version);&lt;br/&gt;
@@ -627,56 +628,61 @@ private FindCoordinatorResponse createFindCoordinatorResponse() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private FetchRequest createFetchRequest(int version, FetchMetadata metadata, List&amp;lt;TopicPartition&amp;gt; toForget) &lt;/p&gt;
{
         LinkedHashMap&amp;lt;TopicPartition, FetchRequest.PartitionData&amp;gt; fetchData = new LinkedHashMap&amp;lt;&amp;gt;();
-        fetchData.put(new TopicPartition(&quot;test1&quot;, 0), new FetchRequest.PartitionData(100, 0L, 1000000));
-        fetchData.put(new TopicPartition(&quot;test2&quot;, 0), new FetchRequest.PartitionData(200, 0L, 1000000));
+        fetchData.put(new TopicPartition(&quot;test1&quot;, 0), new FetchRequest.PartitionData(100, 0L,
+                1000000, Optional.of(15)));
+        fetchData.put(new TopicPartition(&quot;test2&quot;, 0), new FetchRequest.PartitionData(200, 0L,
+                1000000, Optional.of(25)));
         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).
             metadata(metadata).setMaxBytes(1000).toForget(toForget).build((short) version);
     }

&lt;p&gt;     private FetchRequest createFetchRequest(int version, IsolationLevel isolationLevel) &lt;/p&gt;
{
         LinkedHashMap&amp;lt;TopicPartition, FetchRequest.PartitionData&amp;gt; fetchData = new LinkedHashMap&amp;lt;&amp;gt;();
-        fetchData.put(new TopicPartition(&quot;test1&quot;, 0), new FetchRequest.PartitionData(100, 0L, 1000000));
-        fetchData.put(new TopicPartition(&quot;test2&quot;, 0), new FetchRequest.PartitionData(200, 0L, 1000000));
+        fetchData.put(new TopicPartition(&quot;test1&quot;, 0), new FetchRequest.PartitionData(100, 0L,
+                1000000, Optional.of(15)));
+        fetchData.put(new TopicPartition(&quot;test2&quot;, 0), new FetchRequest.PartitionData(200, 0L,
+                1000000, Optional.of(25)));
         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).
             isolationLevel(isolationLevel).setMaxBytes(1000).build((short) version);
     }

&lt;p&gt;     private FetchRequest createFetchRequest(int version) &lt;/p&gt;
{
         LinkedHashMap&amp;lt;TopicPartition, FetchRequest.PartitionData&amp;gt; fetchData = new LinkedHashMap&amp;lt;&amp;gt;();
-        fetchData.put(new TopicPartition(&quot;test1&quot;, 0), new FetchRequest.PartitionData(100, 0L, 1000000));
-        fetchData.put(new TopicPartition(&quot;test2&quot;, 0), new FetchRequest.PartitionData(200, 0L, 1000000));
+        fetchData.put(new TopicPartition(&quot;test1&quot;, 0), new FetchRequest.PartitionData(100, 0L,
+                1000000, Optional.of(15)));
+        fetchData.put(new TopicPartition(&quot;test2&quot;, 0), new FetchRequest.PartitionData(200, 0L,
+                1000000, Optional.of(25)));
         return FetchRequest.Builder.forConsumer(100, 100000, fetchData).setMaxBytes(1000).build((short) version);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse createFetchResponse(Errors error, int sessionId) {&lt;/li&gt;
	&lt;li&gt;return new FetchResponse(error, new LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt;(),&lt;/li&gt;
	&lt;li&gt;25, sessionId);&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; createFetchResponse(Errors error, int sessionId) 
{
+        return new FetchResponse&amp;lt;&amp;gt;(error, new LinkedHashMap&amp;lt;&amp;gt;(), 25, sessionId);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse createFetchResponse(int sessionId) {&lt;/li&gt;
	&lt;li&gt;LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; createFetchResponse(int sessionId) 
{
+        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();
         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(&quot;blah&quot;.getBytes()));
-        responseData.put(new TopicPartition(&quot;test&quot;, 0), new FetchResponse.PartitionData(Errors.NONE,
+        responseData.put(new TopicPartition(&quot;test&quot;, 0), new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE,
             1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));
         List&amp;lt;FetchResponse.AbortedTransaction&amp;gt; abortedTransactions = Collections.singletonList(
             new FetchResponse.AbortedTransaction(234L, 999L));
-        responseData.put(new TopicPartition(&quot;test&quot;, 1), new FetchResponse.PartitionData(Errors.NONE,
+        responseData.put(new TopicPartition(&quot;test&quot;, 1), new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE,
             1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, abortedTransactions, MemoryRecords.EMPTY));
-        return new FetchResponse(Errors.NONE, responseData, 25, sessionId);
+        return new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, responseData, 25, sessionId);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private FetchResponse createFetchResponse() {&lt;/li&gt;
	&lt;li&gt;LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+    private FetchResponse&amp;lt;MemoryRecords&amp;gt; createFetchResponse() 
{
+        LinkedHashMap&amp;lt;TopicPartition, FetchResponse.PartitionData&amp;lt;MemoryRecords&amp;gt;&amp;gt; responseData = new LinkedHashMap&amp;lt;&amp;gt;();
         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(&quot;blah&quot;.getBytes()));
-        responseData.put(new TopicPartition(&quot;test&quot;, 0), new FetchResponse.PartitionData(Errors.NONE,
+        responseData.put(new TopicPartition(&quot;test&quot;, 0), new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE,
                 1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));
 
         List&amp;lt;FetchResponse.AbortedTransaction&amp;gt; abortedTransactions = Collections.singletonList(
                 new FetchResponse.AbortedTransaction(234L, 999L));
-        responseData.put(new TopicPartition(&quot;test&quot;, 1), new FetchResponse.PartitionData(Errors.NONE,
+        responseData.put(new TopicPartition(&quot;test&quot;, 1), new FetchResponse.PartitionData&amp;lt;&amp;gt;(Errors.NONE,
                 1000000, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, abortedTransactions, MemoryRecords.EMPTY));
 
-        return new FetchResponse(Errors.NONE, responseData, 25, INVALID_SESSION_ID);
+        return new FetchResponse&amp;lt;&amp;gt;(Errors.NONE, responseData, 25, INVALID_SESSION_ID);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private HeartbeatRequest createHeartBeatRequest() {&lt;br/&gt;
@@ -757,18 +763,20 @@ private ListOffsetRequest createListOffsetRequest(int version) &lt;/p&gt;
{
                     new ListOffsetRequest.PartitionData(1000000L, 10));
             return ListOffsetRequest.Builder
                     .forConsumer(false, IsolationLevel.READ_UNCOMMITTED)
-                    .setOffsetData(offsetData)
+                    .setTargetTimes(offsetData)
                     .build((short) version);
         }
&lt;p&gt; else if (version == 1) &lt;/p&gt;
{
-            Map&amp;lt;TopicPartition, Long&amp;gt; offsetData = Collections.singletonMap(
-                    new TopicPartition(&quot;test&quot;, 0), 1000000L);
+            Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; offsetData = Collections.singletonMap(
+                    new TopicPartition(&quot;test&quot;, 0),
+                    new ListOffsetRequest.PartitionData(1000000L, Optional.empty()));
             return ListOffsetRequest.Builder
                     .forConsumer(true, IsolationLevel.READ_UNCOMMITTED)
                     .setTargetTimes(offsetData)
                     .build((short) version);
         }
&lt;p&gt; else if (version == 2) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map&amp;lt;TopicPartition, Long&amp;gt; offsetData = Collections.singletonMap(&lt;/li&gt;
	&lt;li&gt;new TopicPartition(&quot;test&quot;, 0), 1000000L);&lt;br/&gt;
+            Map&amp;lt;TopicPartition, ListOffsetRequest.PartitionData&amp;gt; offsetData = Collections.singletonMap(&lt;br/&gt;
+                    new TopicPartition(&quot;test&quot;, 0),&lt;br/&gt;
+                    new ListOffsetRequest.PartitionData(1000000L, Optional.of(5)));&lt;br/&gt;
             return ListOffsetRequest.Builder&lt;br/&gt;
                     .forConsumer(true, IsolationLevel.READ_COMMITTED)&lt;br/&gt;
                     .setTargetTimes(offsetData)&lt;br/&gt;
@@ -788,7 +796,7 @@ private ListOffsetResponse createListOffsetResponse(int version) {&lt;br/&gt;
         } else if (version == 1 || version == 2) 
{
             Map&amp;lt;TopicPartition, ListOffsetResponse.PartitionData&amp;gt; responseData = new HashMap&amp;lt;&amp;gt;();
             responseData.put(new TopicPartition(&quot;test&quot;, 0),
-                    new ListOffsetResponse.PartitionData(Errors.NONE, 10000L, 100L));
+                    new ListOffsetResponse.PartitionData(Errors.NONE, 10000L, 100L, Optional.of(27)));
             return new ListOffsetResponse(responseData);
         }
&lt;p&gt; else {&lt;br/&gt;
             throw new IllegalArgumentException(&quot;Illegal ListOffsetResponse version &quot; + version);&lt;br/&gt;
@@ -807,20 +815,23 @@ private MetadataResponse createMetadataResponse() &lt;/p&gt;
{
 
         List&amp;lt;MetadataResponse.TopicMetadata&amp;gt; allTopicMetadata = new ArrayList&amp;lt;&amp;gt;();
         allTopicMetadata.add(new MetadataResponse.TopicMetadata(Errors.NONE, &quot;__consumer_offsets&quot;, true,
-                asList(new MetadataResponse.PartitionMetadata(Errors.NONE, 1, node, replicas, isr, offlineReplicas))));
+                asList(new MetadataResponse.PartitionMetadata(Errors.NONE, 1, node,
+                        Optional.of(5), replicas, isr, offlineReplicas))));
         allTopicMetadata.add(new MetadataResponse.TopicMetadata(Errors.LEADER_NOT_AVAILABLE, &quot;topic2&quot;, false,
-                Collections.&amp;lt;MetadataResponse.PartitionMetadata&amp;gt;emptyList()));
+                Collections.emptyList()));
         allTopicMetadata.add(new MetadataResponse.TopicMetadata(Errors.NONE, &quot;topic3&quot;, false,
             asList(new MetadataResponse.PartitionMetadata(Errors.LEADER_NOT_AVAILABLE, 0, null,
-                replicas, isr, offlineReplicas))));
+                Optional.empty(), replicas, isr, offlineReplicas))));
 
         return new MetadataResponse(asList(node), null, MetadataResponse.NO_CONTROLLER_ID, allTopicMetadata);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private OffsetCommitRequest createOffsetCommitRequest(int version) {&lt;br/&gt;
         Map&amp;lt;TopicPartition, OffsetCommitRequest.PartitionData&amp;gt; commitData = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;commitData.put(new TopicPartition(&quot;test&quot;, 0), new OffsetCommitRequest.PartitionData(100, &quot;&quot;));&lt;/li&gt;
	&lt;li&gt;commitData.put(new TopicPartition(&quot;test&quot;, 1), new OffsetCommitRequest.PartitionData(200, null));&lt;br/&gt;
+        commitData.put(new TopicPartition(&quot;test&quot;, 0), new OffsetCommitRequest.PartitionData(100,&lt;br/&gt;
+                RecordBatch.NO_PARTITION_LEADER_EPOCH, &quot;&quot;));&lt;br/&gt;
+        commitData.put(new TopicPartition(&quot;test&quot;, 1), new OffsetCommitRequest.PartitionData(200,&lt;br/&gt;
+                RecordBatch.NO_PARTITION_LEADER_EPOCH, null));&lt;br/&gt;
         return new OffsetCommitRequest.Builder(&quot;group1&quot;, commitData)&lt;br/&gt;
                 .setGenerationId(100)&lt;br/&gt;
                 .setMemberId(&quot;consumer1&quot;)&lt;br/&gt;
@@ -840,8 +851,10 @@ private OffsetFetchRequest createOffsetFetchRequest(int version) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private OffsetFetchResponse createOffsetFetchResponse() &lt;/p&gt;
{
         Map&amp;lt;TopicPartition, OffsetFetchResponse.PartitionData&amp;gt; responseData = new HashMap&amp;lt;&amp;gt;();
-        responseData.put(new TopicPartition(&quot;test&quot;, 0), new OffsetFetchResponse.PartitionData(100L, &quot;&quot;, Errors.NONE));
-        responseData.put(new TopicPartition(&quot;test&quot;, 1), new OffsetFetchResponse.PartitionData(100L, null, Errors.NONE));
+        responseData.put(new TopicPartition(&quot;test&quot;, 0), new OffsetFetchResponse.PartitionData(
+                100L, Optional.empty(), &quot;&quot;, Errors.NONE));
+        responseData.put(new TopicPartition(&quot;test&quot;, 1), new OffsetFetchResponse.PartitionData(
+                100L, Optional.of(10), null, Errors.NONE));
         return new OffsetFetchResponse(Errors.NONE, responseData);
     }

&lt;p&gt;@@ -1021,11 +1034,14 @@ private InitProducerIdResponse createInitPidResponse() {&lt;/p&gt;


&lt;p&gt;     private OffsetsForLeaderEpochRequest createLeaderEpochRequest() &lt;/p&gt;
{
-        Map&amp;lt;TopicPartition, Integer&amp;gt; epochs = new HashMap&amp;lt;&amp;gt;();
+        Map&amp;lt;TopicPartition, OffsetsForLeaderEpochRequest.PartitionData&amp;gt; epochs = new HashMap&amp;lt;&amp;gt;();
 
-        epochs.put(new TopicPartition(&quot;topic1&quot;, 0), 1);
-        epochs.put(new TopicPartition(&quot;topic1&quot;, 1), 1);
-        epochs.put(new TopicPartition(&quot;topic2&quot;, 2), 3);
+        epochs.put(new TopicPartition(&quot;topic1&quot;, 0),
+                new OffsetsForLeaderEpochRequest.PartitionData(Optional.of(0), 1));
+        epochs.put(new TopicPartition(&quot;topic1&quot;, 1),
+                new OffsetsForLeaderEpochRequest.PartitionData(Optional.of(0), 1));
+        epochs.put(new TopicPartition(&quot;topic2&quot;, 2),
+                new OffsetsForLeaderEpochRequest.PartitionData(Optional.empty(), 3));
 
         return new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion(), epochs).build();
     }
&lt;p&gt;@@ -1082,7 +1098,9 @@ private WriteTxnMarkersResponse createWriteTxnMarkersResponse() {&lt;br/&gt;
     private TxnOffsetCommitRequest createTxnOffsetCommitRequest() &lt;/p&gt;
{
         final Map&amp;lt;TopicPartition, TxnOffsetCommitRequest.CommittedOffset&amp;gt; offsets = new HashMap&amp;lt;&amp;gt;();
         offsets.put(new TopicPartition(&quot;topic&quot;, 73),
-                    new TxnOffsetCommitRequest.CommittedOffset(100, null));
+                    new TxnOffsetCommitRequest.CommittedOffset(100, null, Optional.empty()));
+        offsets.put(new TopicPartition(&quot;topic&quot;, 74),
+                new TxnOffsetCommitRequest.CommittedOffset(100, &quot;blah&quot;, Optional.of(27)));
         return new TxnOffsetCommitRequest.Builder(&quot;transactionalId&quot;, &quot;groupId&quot;, 21L, (short) 42, offsets).build();
     }

&lt;p&gt;diff --git a/clients/src/test/resources/serializedData/offsetAndMetadataSerializedfile b/clients/src/test/resources/serializedData/offsetAndMetadataBeforeLeaderEpoch&lt;br/&gt;
similarity index 100%&lt;br/&gt;
rename from clients/src/test/resources/serializedData/offsetAndMetadataSerializedfile&lt;br/&gt;
rename to clients/src/test/resources/serializedData/offsetAndMetadataBeforeLeaderEpoch&lt;br/&gt;
diff --git a/clients/src/test/resources/serializedData/offsetAndMetadataWithLeaderEpoch b/clients/src/test/resources/serializedData/offsetAndMetadataWithLeaderEpoch&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..ddf3956a0ce&lt;br/&gt;
Binary files /dev/null and b/clients/src/test/resources/serializedData/offsetAndMetadataWithLeaderEpoch differ&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/api/ApiVersion.scala b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
index fd37367a7fc..b145adfa11f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/api/ApiVersion.scala&lt;br/&gt;
@@ -75,7 +75,9 @@ object ApiVersion {&lt;br/&gt;
     // Several request versions were bumped due to KIP-219 (Improve quota communication)&lt;br/&gt;
     KAFKA_2_0_IV1,&lt;br/&gt;
     // Introduced new schemas for group offset (v2) and group metadata (v2) (KIP-211)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KAFKA_2_1_IV0&lt;br/&gt;
+    KAFKA_2_1_IV0,&lt;br/&gt;
+    // New Fetch, OffsetsForLeaderEpoch, and ListOffsets schemas (KIP-320)&lt;br/&gt;
+    KAFKA_2_1_IV1&lt;br/&gt;
   )&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   // Map keys are the union of the short and full versions&lt;br/&gt;
@@ -256,5 +258,12 @@ case object KAFKA_2_1_IV0 extends DefaultApiVersion &lt;/p&gt;
{
   val shortVersion: String = &quot;2.1&quot;
   val subVersion = &quot;IV0&quot;
   val recordVersion = RecordVersion.V2
+  val id: Int = 17
+}
&lt;p&gt;+&lt;br/&gt;
+case object KAFKA_2_1_IV1 extends DefaultApiVersion &lt;/p&gt;
{
+  val shortVersion: String = &quot;2.1&quot;
+  val subVersion = &quot;IV1&quot;
+  val recordVersion = RecordVersion.V2
   val id: Int = 18
 }
&lt;p&gt;diff --git a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
index 6bd0a5a0d52..a86c95b6908 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
@@ -20,6 +20,7 @@ package kafka.coordinator.group&lt;br/&gt;
 import java.io.PrintStream&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
 import java.nio.charset.StandardCharsets&lt;br/&gt;
+import java.util.Optional&lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicBoolean&lt;br/&gt;
 import java.util.concurrent.locks.ReentrantLock&lt;br/&gt;
@@ -440,13 +441,17 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
     val group = groupMetadataCache.get(groupId)&lt;br/&gt;
     if (group == null) {&lt;br/&gt;
       topicPartitionsOpt.getOrElse(Seq.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;).map &lt;/p&gt;
{ topicPartition =&amp;gt;
-        (topicPartition, new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET, &quot;&quot;, Errors.NONE))
+        val partitionData = new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET,
+          Optional.empty(), &quot;&quot;, Errors.NONE)
+        topicPartition -&amp;gt; partitionData
       }
&lt;p&gt;.toMap&lt;br/&gt;
     } else {&lt;br/&gt;
       group.inLock {&lt;br/&gt;
         if (group.is(Dead)) {&lt;br/&gt;
           topicPartitionsOpt.getOrElse(Seq.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;).map &lt;/p&gt;
{ topicPartition =&amp;gt;
-            (topicPartition, new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET, &quot;&quot;, Errors.NONE))
+            val partitionData = new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET,
+              Optional.empty(), &quot;&quot;, Errors.NONE)
+            topicPartition -&amp;gt; partitionData
           }
&lt;p&gt;.toMap&lt;br/&gt;
         } else {&lt;br/&gt;
           topicPartitionsOpt match {&lt;br/&gt;
@@ -454,16 +459,19 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
               // Return offsets for all partitions owned by this consumer group. (this only applies to consumers&lt;br/&gt;
               // that commit offsets to Kafka.)&lt;br/&gt;
               group.allOffsets.map &lt;/p&gt;
{ case (topicPartition, offsetAndMetadata) =&amp;gt;
-                topicPartition -&amp;gt; new OffsetFetchResponse.PartitionData(offsetAndMetadata.offset, offsetAndMetadata.metadata, Errors.NONE)
+                topicPartition -&amp;gt; new OffsetFetchResponse.PartitionData(offsetAndMetadata.offset,
+                  Optional.empty(), offsetAndMetadata.metadata, Errors.NONE)
               }

&lt;p&gt;             case Some(topicPartitions) =&amp;gt;&lt;br/&gt;
               topicPartitions.map { topicPartition =&amp;gt;&lt;br/&gt;
                 val partitionData = group.offset(topicPartition) match &lt;/p&gt;
{
                   case None =&amp;gt;
-                    new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET, &quot;&quot;, Errors.NONE)
+                    new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET,
+                      Optional.empty(), &quot;&quot;, Errors.NONE)
                   case Some(offsetAndMetadata) =&amp;gt;
-                    new OffsetFetchResponse.PartitionData(offsetAndMetadata.offset, offsetAndMetadata.metadata, Errors.NONE)
+                    new OffsetFetchResponse.PartitionData(offsetAndMetadata.offset,
+                      Optional.empty(), offsetAndMetadata.metadata, Errors.NONE)
                 }
&lt;p&gt;                 topicPartition -&amp;gt; partitionData&lt;br/&gt;
               }.toMap&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/FetchSession.scala b/core/src/main/scala/kafka/server/FetchSession.scala&lt;br/&gt;
index 64bc773f0bb..16ee8728246 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/FetchSession.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/FetchSession.scala&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;br/&gt;
 package kafka.server&lt;/p&gt;

&lt;p&gt; import java.util&lt;br/&gt;
+import java.util.Optional&lt;br/&gt;
 import java.util.concurrent.&lt;/p&gt;
{ThreadLocalRandom, TimeUnit}

&lt;p&gt; import com.yammer.metrics.core.Gauge&lt;br/&gt;
@@ -107,7 +108,7 @@ class CachedPartition(val topic: String,&lt;/p&gt;

&lt;p&gt;   def topicPartition = new TopicPartition(topic, partition)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def reqData = new FetchRequest.PartitionData(fetchOffset, fetcherLogStartOffset, maxBytes)&lt;br/&gt;
+  def reqData = new FetchRequest.PartitionData(fetchOffset, fetcherLogStartOffset, maxBytes, Optional.empty())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def updateRequestParams(reqData: FetchRequest.PartitionData): Unit = {&lt;br/&gt;
     // Update our cached request parameters.&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
index 3a81b89f61a..24c13cd0c2f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala&lt;br/&gt;
@@ -22,7 +22,7 @@ import java.nio.ByteBuffer&lt;br/&gt;
 import java.util&lt;br/&gt;
 import java.util.concurrent.ConcurrentHashMap&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicInteger&lt;br/&gt;
-import java.util.&lt;/p&gt;
{Collections, Properties}
&lt;p&gt;+import java.util.&lt;/p&gt;
{Collections, Optional, Properties}

&lt;p&gt; import kafka.admin.&lt;/p&gt;
{AdminUtils, RackAwareMode}
&lt;p&gt; import kafka.api.&lt;/p&gt;
{ApiVersion, KAFKA_0_11_0_IV0}
&lt;p&gt;@@ -735,7 +735,7 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
     val clientId = request.header.clientId&lt;br/&gt;
     val offsetRequest = request.body&lt;span class=&quot;error&quot;&gt;&amp;#91;ListOffsetRequest&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val (authorizedRequestInfo, unauthorizedRequestInfo) = offsetRequest.offsetData.asScala.partition {&lt;br/&gt;
+    val (authorizedRequestInfo, unauthorizedRequestInfo) = offsetRequest.partitionTimestamps.asScala.partition 
{
       case (topicPartition, _) =&amp;gt; authorize(request.session, Describe, Resource(Topic, topicPartition.topic, LITERAL))
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -794,17 +794,19 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;/p&gt;

&lt;p&gt;     val unauthorizedResponseStatus = unauthorizedRequestInfo.mapValues(_ =&amp;gt; &lt;/p&gt;
{
       new ListOffsetResponse.PartitionData(Errors.TOPIC_AUTHORIZATION_FAILED,
-                                           ListOffsetResponse.UNKNOWN_TIMESTAMP,
-                                           ListOffsetResponse.UNKNOWN_OFFSET)
+        ListOffsetResponse.UNKNOWN_TIMESTAMP,
+        ListOffsetResponse.UNKNOWN_OFFSET,
+        Optional.empty())
     }
&lt;p&gt;)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val responseMap = authorizedRequestInfo.map { case (topicPartition, timestamp) =&amp;gt;&lt;br/&gt;
+    val responseMap = authorizedRequestInfo.map { case (topicPartition, partitionData) =&amp;gt;&lt;br/&gt;
       if (offsetRequest.duplicatePartitions().contains(topicPartition)) 
{
         debug(s&quot;OffsetRequest with correlation id $correlationId from client $clientId on partition $topicPartition &quot; +
             s&quot;failed because the partition is duplicated in the request.&quot;)
         (topicPartition, new ListOffsetResponse.PartitionData(Errors.INVALID_REQUEST,
-                                                              ListOffsetResponse.UNKNOWN_TIMESTAMP,
-                                                              ListOffsetResponse.UNKNOWN_OFFSET))
+          ListOffsetResponse.UNKNOWN_TIMESTAMP,
+          ListOffsetResponse.UNKNOWN_OFFSET,
+          Optional.empty()))
       }
&lt;p&gt; else {&lt;br/&gt;
         try &lt;/p&gt;
{
           // ensure leader exists
@@ -820,21 +822,22 @@ class KafkaApis(val requestChannel: RequestChannel,
               case IsolationLevel.READ_UNCOMMITTED =&amp;gt; localReplica.highWatermark.messageOffset
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (timestamp == ListOffsetRequest.LATEST_TIMESTAMP)&lt;br/&gt;
+            if (partitionData.timestamp == ListOffsetRequest.LATEST_TIMESTAMP)&lt;br/&gt;
               TimestampOffset(RecordBatch.NO_TIMESTAMP, lastFetchableOffset)&lt;br/&gt;
             else 
{
               def allowed(timestampOffset: TimestampOffset): Boolean =
-                timestamp == ListOffsetRequest.EARLIEST_TIMESTAMP || timestampOffset.offset &amp;lt; lastFetchableOffset
+                partitionData.timestamp == ListOffsetRequest.EARLIEST_TIMESTAMP || timestampOffset.offset &amp;lt; lastFetchableOffset
 
-              fetchOffsetForTimestamp(topicPartition, timestamp)
+              fetchOffsetForTimestamp(topicPartition, partitionData.timestamp)
                 .filter(allowed).getOrElse(TimestampOffset.Unknown)
             }
&lt;p&gt;           } else &lt;/p&gt;
{
-            fetchOffsetForTimestamp(topicPartition, timestamp)
+            fetchOffsetForTimestamp(topicPartition, partitionData.timestamp)
               .getOrElse(TimestampOffset.Unknown)
           }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;(topicPartition, new ListOffsetResponse.PartitionData(Errors.NONE, found.timestamp, found.offset))&lt;br/&gt;
+          (topicPartition, new ListOffsetResponse.PartitionData(Errors.NONE, found.timestamp, found.offset,&lt;br/&gt;
+            Optional.empty()))&lt;br/&gt;
         } catch {&lt;br/&gt;
           // NOTE: These exceptions are special cased since these error messages are typically transient or the client&lt;br/&gt;
           // would have received a clear exception and there is no value in logging the entire stack trace for the same&lt;br/&gt;
@@ -845,13 +848,15 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
             debug(s&quot;Offset request with correlation id $correlationId from client $clientId on &quot; +&lt;br/&gt;
                 s&quot;partition $topicPartition failed due to ${e.getMessage}&quot;)&lt;br/&gt;
             (topicPartition, new ListOffsetResponse.PartitionData(Errors.forException(e),&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_TIMESTAMP,&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_OFFSET))&lt;br/&gt;
+              ListOffsetResponse.UNKNOWN_TIMESTAMP,&lt;br/&gt;
+              ListOffsetResponse.UNKNOWN_OFFSET,&lt;br/&gt;
+              Optional.empty()))&lt;br/&gt;
           case e: Throwable =&amp;gt;&lt;br/&gt;
             error(&quot;Error while responding to offset request&quot;, e)&lt;br/&gt;
             (topicPartition, new ListOffsetResponse.PartitionData(Errors.forException(e),&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_TIMESTAMP,&lt;/li&gt;
	&lt;li&gt;ListOffsetResponse.UNKNOWN_OFFSET))&lt;br/&gt;
+              ListOffsetResponse.UNKNOWN_TIMESTAMP,&lt;br/&gt;
+              ListOffsetResponse.UNKNOWN_OFFSET,&lt;br/&gt;
+              Optional.empty()))&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
@@ -1118,16 +1123,16 @@ class KafkaApis(val requestChannel: RequestChannel,&lt;br/&gt;
                   val payloadOpt = zkClient.getConsumerOffset(offsetFetchRequest.groupId, topicPartition)&lt;br/&gt;
                   payloadOpt match 
{
                     case Some(payload) =&amp;gt;
-                      (topicPartition, new OffsetFetchResponse.PartitionData(
-                          payload.toLong, OffsetFetchResponse.NO_METADATA, Errors.NONE))
+                      (topicPartition, new OffsetFetchResponse.PartitionData(payload.toLong,
+                        Optional.empty(), OffsetFetchResponse.NO_METADATA, Errors.NONE))
                     case None =&amp;gt;
                       (topicPartition, OffsetFetchResponse.UNKNOWN_PARTITION)
                   }
&lt;p&gt;                 }&lt;br/&gt;
               } catch &lt;/p&gt;
{
                 case e: Throwable =&amp;gt;
-                  (topicPartition, new OffsetFetchResponse.PartitionData(
-                      OffsetFetchResponse.INVALID_OFFSET, OffsetFetchResponse.NO_METADATA, Errors.forException(e)))
+                  (topicPartition, new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET,
+                    Optional.empty(), OffsetFetchResponse.NO_METADATA, Errors.forException(e)))
               }
&lt;p&gt;             }.toMap&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/server/MetadataCache.scala b/core/src/main/scala/kafka/server/MetadataCache.scala&lt;br/&gt;
index 25967b30637..31146636ae0 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/MetadataCache.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/MetadataCache.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;/p&gt;

&lt;p&gt; package kafka.server&lt;/p&gt;

&lt;p&gt;-import java.util.Collections&lt;br/&gt;
+import java.util.&lt;/p&gt;
{Collections, Optional}&lt;br/&gt;
 import java.util.concurrent.locks.ReentrantReadWriteLock&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.{Seq, Set, mutable}&lt;br/&gt;
@@ -74,6 +74,7 @@ class MetadataCache(brokerId: Int) extends Logging {&lt;br/&gt;
       partitions.map { case (partitionId, partitionState) =&amp;gt;&lt;br/&gt;
         val topicPartition = new TopicPartition(topic, partitionId.toInt)&lt;br/&gt;
         val leaderBrokerId = partitionState.basePartitionState.leader&lt;br/&gt;
+        val leaderEpoch = partitionState.basePartitionState.leaderEpoch&lt;br/&gt;
         val maybeLeader = getAliveEndpoint(snapshot, leaderBrokerId, listenerName)&lt;br/&gt;
         val replicas = partitionState.basePartitionState.replicas.asScala.map(_.toInt)&lt;br/&gt;
         val replicaInfo = getEndpoints(snapshot, replicas, listenerName, errorUnavailableEndpoints)&lt;br/&gt;
@@ -89,7 +90,8 @@ class MetadataCache(brokerId: Int) extends Logging {
               if (errorUnavailableListeners) Errors.LISTENER_NOT_FOUND else Errors.LEADER_NOT_AVAILABLE
             }&lt;br/&gt;
             new MetadataResponse.PartitionMetadata(error, partitionId.toInt, Node.noNode(),&lt;br/&gt;
-              replicaInfo.asJava, java.util.Collections.emptyList(), offlineReplicaInfo.asJava)&lt;br/&gt;
+              Optional.empty(), replicaInfo.asJava, java.util.Collections.emptyList(),&lt;br/&gt;
+              offlineReplicaInfo.asJava)&lt;br/&gt;
 &lt;br/&gt;
           case Some(leader) =&amp;gt;&lt;br/&gt;
             val isr = partitionState.basePartitionState.isr.asScala.map(_.toInt)&lt;br/&gt;
@@ -100,15 +102,15 @@ class MetadataCache(brokerId: Int) extends Logging {&lt;br/&gt;
                 s&quot;following brokers ${replicas.filterNot(replicaInfo.map(_.id).contains).mkString(&quot;,&quot;)}&quot;)&lt;br/&gt;
 &lt;br/&gt;
               new MetadataResponse.PartitionMetadata(Errors.REPLICA_NOT_AVAILABLE, partitionId.toInt, leader,&lt;br/&gt;
-                replicaInfo.asJava, isrInfo.asJava, offlineReplicaInfo.asJava)&lt;br/&gt;
+                Optional.empty(), replicaInfo.asJava, isrInfo.asJava, offlineReplicaInfo.asJava)&lt;br/&gt;
             } else if (isrInfo.size &amp;lt; isr.size) {&lt;br/&gt;
               debug(s&quot;Error while fetching metadata for $topicPartition: in sync replica information not available for &quot; +&lt;br/&gt;
                 s&quot;following brokers ${isr.filterNot(isrInfo.map(_.id).contains).mkString(&quot;,&quot;)}&quot;)&lt;br/&gt;
               new MetadataResponse.PartitionMetadata(Errors.REPLICA_NOT_AVAILABLE, partitionId.toInt, leader,&lt;br/&gt;
-                replicaInfo.asJava, isrInfo.asJava, offlineReplicaInfo.asJava)&lt;br/&gt;
+                Optional.empty(), replicaInfo.asJava, isrInfo.asJava, offlineReplicaInfo.asJava)&lt;br/&gt;
             } else {
-              new MetadataResponse.PartitionMetadata(Errors.NONE, partitionId.toInt, leader, replicaInfo.asJava,
-                isrInfo.asJava, offlineReplicaInfo.asJava)
+              new MetadataResponse.PartitionMetadata(Errors.NONE, partitionId.toInt, leader, Optional.of(leaderEpoch),
+                replicaInfo.asJava, isrInfo.asJava, offlineReplicaInfo.asJava)
             }&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
index 5aec7a90d29..dc585ebd926 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.util&lt;br/&gt;
+import java.util.Optional&lt;br/&gt;
 &lt;br/&gt;
 import kafka.api.Request&lt;br/&gt;
 import kafka.cluster.BrokerEndPoint&lt;br/&gt;
@@ -196,7 +197,8 @@ class ReplicaAlterLogDirsThread(name: String,&lt;br/&gt;
       val (topicPartition, partitionFetchState) = maxPartitionOpt.get&lt;br/&gt;
       try {
         val logStartOffset = replicaMgr.getReplicaOrException(topicPartition, Request.FutureLocalReplicaId).logStartOffset
-        requestMap.put(topicPartition, new FetchRequest.PartitionData(partitionFetchState.fetchOffset, logStartOffset, fetchSize))
+        requestMap.put(topicPartition, new FetchRequest.PartitionData(partitionFetchState.fetchOffset, logStartOffset,
+          fetchSize, Optional.empty()))
       } catch {&lt;br/&gt;
         case _: KafkaStorageException =&amp;gt;&lt;br/&gt;
           partitionsWithError += topicPartition&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index 1848eb741c3..5dcd29b473d 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -17,6 +17,8 @@&lt;br/&gt;
 &lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
+import java.util.Optional&lt;br/&gt;
+&lt;br/&gt;
 import kafka.api._&lt;br/&gt;
 import kafka.cluster.BrokerEndPoint&lt;br/&gt;
 import kafka.log.{LogAppendInfo, LogConfig}&lt;br/&gt;
@@ -61,7 +63,8 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
 &lt;br/&gt;
   // Visible for testing&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; val fetchRequestVersion: Short =&lt;br/&gt;
-    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV1) 8&lt;br/&gt;
+    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_1_IV1) 9&lt;br/&gt;
+    else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV1) 8&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_1_1_IV0) 7&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_11_0_IV1) 5&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_11_0_IV0) 4&lt;br/&gt;
@@ -72,12 +75,14 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
 &lt;br/&gt;
   // Visible for testing&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; val offsetForLeaderEpochRequestVersion: Short =&lt;br/&gt;
-    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV0) 1&lt;br/&gt;
+    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_1_IV1) 2&lt;br/&gt;
+    else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV0) 1&lt;br/&gt;
     else 0&lt;br/&gt;
 &lt;br/&gt;
   // Visible for testing&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; val listOffsetRequestVersion: Short =&lt;br/&gt;
-    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV1) 3&lt;br/&gt;
+    if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_1_IV1) 4&lt;br/&gt;
+    else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_2_0_IV1) 3&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_11_0_IV0) 2&lt;br/&gt;
     else if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_10_1_IV2) 1&lt;br/&gt;
     else 0&lt;br/&gt;
@@ -199,22 +204,21 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def fetchOffsetFromLeader(topicPartition: TopicPartition, earliestOrLatest: Long): Long = {&lt;br/&gt;
-    val requestBuilder = if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_10_1_IV2) {
-        val partitions = Map(topicPartition -&amp;gt; (earliestOrLatest: java.lang.Long))
-        ListOffsetRequest.Builder.forReplica(listOffsetRequestVersion, replicaId).setTargetTimes(partitions.asJava)
-      } else {
-        val partitions = Map(topicPartition -&amp;gt; new ListOffsetRequest.PartitionData(earliestOrLatest, 1))
-        ListOffsetRequest.Builder.forReplica(listOffsetRequestVersion, replicaId).setOffsetData(partitions.asJava)
-      }&lt;br/&gt;
+    val requestPartitionData = new ListOffsetRequest.PartitionData(earliestOrLatest, Optional.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;())&lt;br/&gt;
+    val requestPartitions = Map(topicPartition -&amp;gt; requestPartitionData)&lt;br/&gt;
+    val requestBuilder = ListOffsetRequest.Builder.forReplica(listOffsetRequestVersion, replicaId)&lt;br/&gt;
+      .setTargetTimes(requestPartitions.asJava)&lt;br/&gt;
+&lt;br/&gt;
     val clientResponse = leaderEndpoint.sendRequest(requestBuilder)&lt;br/&gt;
     val response = clientResponse.responseBody.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ListOffsetResponse&amp;#93;&lt;/span&gt;&lt;br/&gt;
-    val partitionData = response.responseData.get(topicPartition)&lt;br/&gt;
-    partitionData.error match {&lt;br/&gt;
+&lt;br/&gt;
+    val responsePartitionData = response.responseData.get(topicPartition)&lt;br/&gt;
+    responsePartitionData.error match {
       case Errors.NONE =&amp;gt;
         if (brokerConfig.interBrokerProtocolVersion &amp;gt;= KAFKA_0_10_1_IV2)
-          partitionData.offset
+          responsePartitionData.offset
         else
-          partitionData.offsets.get(0)
+          responsePartitionData.offsets.get(0)
       case error =&amp;gt; throw error.exception
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -229,7 +233,7 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
         try {
           val logStartOffset = replicaMgr.getReplicaOrException(topicPartition).logStartOffset
           builder.add(topicPartition, new FetchRequest.PartitionData(
-            partitionFetchState.fetchOffset, logStartOffset, fetchSize))
+            partitionFetchState.fetchOffset, logStartOffset, fetchSize, Optional.empty()))
         } catch {
           case _: KafkaStorageException =&amp;gt;
             // The replica has already been marked offline due to log directory failure and the original failure should have already been logged.
@@ -289,7 +293,9 @@ class ReplicaFetcherThread(name: String,
         return resultWithoutEpoch
       }&lt;br/&gt;
 &lt;br/&gt;
-      val partitionsAsJava = partitionsWithEpoch.map { case (tp, epoch) =&amp;gt; tp -&amp;gt; epoch.asInstanceOf[Integer] }.toMap.asJava&lt;br/&gt;
+      val partitionsAsJava = partitions.map { case (tp, epoch) =&amp;gt; tp -&amp;gt;
+        new OffsetsForLeaderEpochRequest.PartitionData(Optional.empty(), epoch.asInstanceOf[Integer])
+      }.toMap.asJava&lt;br/&gt;
       val epochRequest = new OffsetsForLeaderEpochRequest.Builder(offsetForLeaderEpochRequestVersion, partitionsAsJava)&lt;br/&gt;
       try {
         val response = leaderEndpoint.sendRequest(epochRequest)
diff --git a/core/src/main/scala/kafka/server/ReplicaManager.scala b/core/src/main/scala/kafka/server/ReplicaManager.scala
index 2393daad003..d61240e3af6 100644
--- a/core/src/main/scala/kafka/server/ReplicaManager.scala
+++ b/core/src/main/scala/kafka/server/ReplicaManager.scala
@@ -1475,14 +1475,14 @@ class ReplicaManager(val config: KafkaConfig,
     new ReplicaAlterLogDirsManager(config, this, quotaManager, brokerTopicStats)
   }&lt;br/&gt;
 &lt;br/&gt;
-  def lastOffsetForLeaderEpoch(requestedEpochInfo: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Integer&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    requestedEpochInfo.map { case (tp, leaderEpoch) =&amp;gt;&lt;br/&gt;
+  def lastOffsetForLeaderEpoch(requestedEpochInfo: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, OffsetsForLeaderEpochRequest.PartitionData&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    requestedEpochInfo.map { case (tp, partitionData) =&amp;gt;&lt;br/&gt;
       val epochEndOffset = getPartition(tp) match {&lt;br/&gt;
         case Some(partition) =&amp;gt;&lt;br/&gt;
           if (partition eq ReplicaManager.OfflinePartition)&lt;br/&gt;
             new EpochEndOffset(Errors.KAFKA_STORAGE_ERROR, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
           else&lt;br/&gt;
-            partition.lastOffsetForLeaderEpoch(leaderEpoch)&lt;br/&gt;
+            partition.lastOffsetForLeaderEpoch(partitionData.leaderEpoch)&lt;br/&gt;
 &lt;br/&gt;
         case None if metadataCache.contains(tp) =&amp;gt;&lt;br/&gt;
           new EpochEndOffset(Errors.NOT_LEADER_FOR_PARTITION, UNDEFINED_EPOCH, UNDEFINED_EPOCH_OFFSET)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala b/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala&lt;br/&gt;
index 1ecea09b994..4758764be65 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala&lt;br/&gt;
@@ -23,7 +23,7 @@ import java.util&lt;br/&gt;
 import java.util.concurrent.CountDownLatch&lt;br/&gt;
 import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}&lt;br/&gt;
 import java.util.regex.{Pattern, PatternSyntaxException}&lt;br/&gt;
-import java.util.{Date, Properties}&lt;br/&gt;
+import java.util.{Date, Optional, Properties}&lt;br/&gt;
 &lt;br/&gt;
 import joptsimple.OptionParser&lt;br/&gt;
 import kafka.api._&lt;br/&gt;
@@ -389,7 +389,8 @@ private class ReplicaFetcher(name: String, sourceBroker: Node, topicPartitions:&lt;br/&gt;
 &lt;br/&gt;
     val requestMap = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, JFetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
     for (topicPartition &amp;lt;- topicPartitions)&lt;br/&gt;
-      requestMap.put(topicPartition, new JFetchRequest.PartitionData(replicaBuffer.getOffset(topicPartition), 0L, fetchSize))&lt;br/&gt;
+      requestMap.put(topicPartition, new JFetchRequest.PartitionData(replicaBuffer.getOffset(topicPartition),&lt;br/&gt;
+        0L, fetchSize, Optional.empty()))&lt;br/&gt;
 &lt;br/&gt;
     val fetchRequestBuilder = JFetchRequest.Builder.&lt;br/&gt;
       forReplica(ApiKeys.FETCH.latestVersion, Request.DebuggingConsumerId, maxWait, minBytes, requestMap)&lt;br/&gt;
diff --git a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
index 7c341e6fb9b..cfc6b5e43d7 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala&lt;br/&gt;
@@ -16,7 +16,7 @@ import java.nio.ByteBuffer&lt;br/&gt;
 import java.util&lt;br/&gt;
 import java.util.concurrent.ExecutionException&lt;br/&gt;
 import java.util.regex.Pattern&lt;br/&gt;
-import java.util.{ArrayList, Collections, Properties}&lt;br/&gt;
+import java.util.{ArrayList, Collections, Optional, Properties}&lt;br/&gt;
 import java.time.Duration&lt;br/&gt;
 &lt;br/&gt;
 import kafka.admin.ConsumerGroupCommand.{ConsumerGroupCommandOptions, ConsumerGroupService}&lt;br/&gt;
@@ -260,25 +260,26 @@ class AuthorizerIntegrationTest extends BaseRequestTest {&lt;br/&gt;
 &lt;br/&gt;
   private def createFetchRequest = {
     val partitionMap = new util.LinkedHashMap[TopicPartition, requests.FetchRequest.PartitionData]
-    partitionMap.put(tp, new requests.FetchRequest.PartitionData(0, 0, 100))
+    partitionMap.put(tp, new requests.FetchRequest.PartitionData(0, 0, 100, Optional.of(27)))
     requests.FetchRequest.Builder.forConsumer(100, Int.MaxValue, partitionMap).build()
   }&lt;br/&gt;
 &lt;br/&gt;
   private def createFetchFollowerRequest = {
     val partitionMap = new util.LinkedHashMap[TopicPartition, requests.FetchRequest.PartitionData]
-    partitionMap.put(tp, new requests.FetchRequest.PartitionData(0, 0, 100))
+    partitionMap.put(tp, new requests.FetchRequest.PartitionData(0, 0, 100, Optional.of(27)))
     val version = ApiKeys.FETCH.latestVersion
     requests.FetchRequest.Builder.forReplica(version, 5000, 100, Int.MaxValue, partitionMap).build()
   }&lt;br/&gt;
 &lt;br/&gt;
   private def createListOffsetsRequest = {
     requests.ListOffsetRequest.Builder.forConsumer(false, IsolationLevel.READ_UNCOMMITTED).setTargetTimes(
-      Map(tp -&amp;gt; (0L: java.lang.Long)).asJava).
+      Map(tp -&amp;gt; new ListOffsetRequest.PartitionData(0L, Optional.of[Integer](27))).asJava).
       build()
   }&lt;br/&gt;
 &lt;br/&gt;
   private def offsetsForLeaderEpochRequest = {
-    new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion()).add(tp, 7).build()
+    new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion)
+      .add(tp, Optional.of(27), 7).build()
   }&lt;br/&gt;
 &lt;br/&gt;
   private def createOffsetFetchRequest = {&lt;br/&gt;
@@ -316,7 +317,7 @@ class AuthorizerIntegrationTest extends BaseRequestTest {&lt;br/&gt;
 &lt;br/&gt;
   private def createOffsetCommitRequest = {
     new requests.OffsetCommitRequest.Builder(
-      group, Map(tp -&amp;gt; new requests.OffsetCommitRequest.PartitionData(0, &quot;metadata&quot;)).asJava).
+      group, Map(tp -&amp;gt; new requests.OffsetCommitRequest.PartitionData(0, 27, &quot;metadata&quot;)).asJava).
       setMemberId(&quot;&quot;).setGenerationId(1).
       build()
   }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala b/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
index 2befc8fdf3b..d2d115b2cfa 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/api/ApiVersionTest.scala&lt;br/&gt;
@@ -80,8 +80,9 @@ class ApiVersionTest {
     assertEquals(KAFKA_2_0_IV0, ApiVersion(&quot;2.0-IV0&quot;))
     assertEquals(KAFKA_2_0_IV1, ApiVersion(&quot;2.0-IV1&quot;))
 
-    assertEquals(KAFKA_2_1_IV0, ApiVersion(&quot;2.1&quot;))
+    assertEquals(KAFKA_2_1_IV1, ApiVersion(&quot;2.1&quot;))
     assertEquals(KAFKA_2_1_IV0, ApiVersion(&quot;2.1-IV0&quot;))
+    assertEquals(KAFKA_2_1_IV1, ApiVersion(&quot;2.1-IV1&quot;))
   }&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala b/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala&lt;br/&gt;
index c456433b6e3..8c1d95a4356 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
+import java.util.Optional&lt;br/&gt;
 &lt;br/&gt;
 import AbstractFetcherThread._&lt;br/&gt;
 import com.yammer.metrics.Metrics&lt;br/&gt;
@@ -371,7 +372,8 @@ class AbstractFetcherThreadTest {&lt;br/&gt;
       partitionMap.foreach { case (partition, state) =&amp;gt;&lt;br/&gt;
         if (state.isReadyForFetch) {
           val replicaState = replicaPartitionState(partition)
-          fetchData.put(partition, new FetchRequest.PartitionData(state.fetchOffset, replicaState.logStartOffset, 1024 * 1024))
+          fetchData.put(partition, new FetchRequest.PartitionData(state.fetchOffset, replicaState.logStartOffset,
+            1024 * 1024, Optional.empty()))
         }&lt;br/&gt;
       }&lt;br/&gt;
       val fetchRequest = FetchRequest.Builder.forReplica(ApiKeys.FETCH.latestVersion, replicaId, 0, 1, fetchData.asJava)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala b/core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala&lt;br/&gt;
index 1bf6f28eec5..673abe693c9 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.util&lt;br/&gt;
-import java.util.Properties&lt;br/&gt;
+import java.util.{Optional, Properties}&lt;br/&gt;
 &lt;br/&gt;
 import kafka.log.LogConfig&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
@@ -72,7 +72,8 @@ class FetchRequestDownConversionConfigTest extends BaseRequestTest {&lt;br/&gt;
                                  offsetMap: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt; = Map.empty): util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val partitionMap = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
     topicPartitions.foreach { tp =&amp;gt;
-      partitionMap.put(tp, new FetchRequest.PartitionData(offsetMap.getOrElse(tp, 0), 0L, maxPartitionBytes))
+      partitionMap.put(tp, new FetchRequest.PartitionData(offsetMap.getOrElse(tp, 0), 0L,
+        maxPartitionBytes, Optional.empty()))
     }&lt;br/&gt;
     partitionMap&lt;br/&gt;
   }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala b/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala&lt;br/&gt;
index 694b19d378c..f8c02cf5318 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/FetchRequestTest.scala&lt;br/&gt;
@@ -18,7 +18,7 @@ package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.io.DataInputStream&lt;br/&gt;
 import java.util&lt;br/&gt;
-import java.util.Properties&lt;br/&gt;
+import java.util.{Optional, Properties}&lt;br/&gt;
 &lt;br/&gt;
 import kafka.api.KAFKA_0_11_0_IV2&lt;br/&gt;
 import kafka.log.LogConfig&lt;br/&gt;
@@ -58,7 +58,8 @@ class FetchRequestTest extends BaseRequestTest {&lt;br/&gt;
                                  offsetMap: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt; = Map.empty): util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val partitionMap = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;br/&gt;
     topicPartitions.foreach { tp =&amp;gt;
-      partitionMap.put(tp, new FetchRequest.PartitionData(offsetMap.getOrElse(tp, 0), 0L, maxPartitionBytes))
+      partitionMap.put(tp, new FetchRequest.PartitionData(offsetMap.getOrElse(tp, 0), 0L, maxPartitionBytes,
+        Optional.empty()))
     }&lt;br/&gt;
     partitionMap&lt;br/&gt;
   }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala b/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala&lt;br/&gt;
index c4a96254eda..74dae7c5303 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.util&lt;br/&gt;
-import java.util.Collections&lt;br/&gt;
+import java.util.{Collections, Optional}

&lt;p&gt; import kafka.utils.MockTime&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -141,8 +141,10 @@ class FetchSessionTest {&lt;/p&gt;

&lt;p&gt;     // Create a new fetch session with a FULL fetch request&lt;br/&gt;
     val reqData2 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reqData2.put(new TopicPartition(&quot;foo&quot;, 0), new FetchRequest.PartitionData(0, 0, 100))&lt;/li&gt;
	&lt;li&gt;reqData2.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100))&lt;br/&gt;
+    reqData2.put(new TopicPartition(&quot;foo&quot;, 0), new FetchRequest.PartitionData(0, 0, 100,&lt;br/&gt;
+      Optional.empty()))&lt;br/&gt;
+    reqData2.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100,&lt;br/&gt;
+      Optional.empty()))&lt;br/&gt;
     val context2 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData2, EMPTY_PART_LIST, false)&lt;br/&gt;
     assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FullFetchContext&amp;#93;&lt;/span&gt;, context2.getClass)&lt;br/&gt;
     val reqData2Iter = reqData2.entrySet().iterator()&lt;br/&gt;
@@ -215,8 +217,10 @@ class FetchSessionTest {&lt;br/&gt;
     var nextSessionId = prevSessionId&lt;br/&gt;
     do {&lt;br/&gt;
       val reqData8 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;reqData8.put(new TopicPartition(&quot;bar&quot;, 0), new FetchRequest.PartitionData(0, 0, 100))&lt;/li&gt;
	&lt;li&gt;reqData8.put(new TopicPartition(&quot;bar&quot;, 1), new FetchRequest.PartitionData(10, 0, 100))&lt;br/&gt;
+      reqData8.put(new TopicPartition(&quot;bar&quot;, 0), new FetchRequest.PartitionData(0, 0, 100,&lt;br/&gt;
+        Optional.empty()))&lt;br/&gt;
+      reqData8.put(new TopicPartition(&quot;bar&quot;, 1), new FetchRequest.PartitionData(10, 0, 100,&lt;br/&gt;
+        Optional.empty()))&lt;br/&gt;
       val context8 = fetchManager.newContext(&lt;br/&gt;
         new JFetchMetadata(prevSessionId, FINAL_EPOCH), reqData8, EMPTY_PART_LIST, false)&lt;br/&gt;
       assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;SessionlessFetchContext&amp;#93;&lt;/span&gt;, context8.getClass)&lt;br/&gt;
@@ -240,8 +244,10 @@ class FetchSessionTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Create a new fetch session with foo-0 and foo-1&lt;br/&gt;
     val reqData1 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reqData1.put(new TopicPartition(&quot;foo&quot;, 0), new FetchRequest.PartitionData(0, 0, 100))&lt;/li&gt;
	&lt;li&gt;reqData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100))&lt;br/&gt;
+    reqData1.put(new TopicPartition(&quot;foo&quot;, 0), new FetchRequest.PartitionData(0, 0, 100,&lt;br/&gt;
+      Optional.empty()))&lt;br/&gt;
+    reqData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100,&lt;br/&gt;
+      Optional.empty()))&lt;br/&gt;
     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData1, EMPTY_PART_LIST, false)&lt;br/&gt;
     assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FullFetchContext&amp;#93;&lt;/span&gt;, context1.getClass)&lt;br/&gt;
     val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
@@ -256,7 +262,8 @@ class FetchSessionTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Create an incremental fetch request that removes foo-0 and adds bar-0&lt;br/&gt;
     val reqData2 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reqData2.put(new TopicPartition(&quot;bar&quot;, 0), new FetchRequest.PartitionData(15, 0, 0))&lt;br/&gt;
+    reqData2.put(new TopicPartition(&quot;bar&quot;, 0), new FetchRequest.PartitionData(15, 0, 0,&lt;br/&gt;
+      Optional.empty()))&lt;br/&gt;
     val removed2 = new util.ArrayList&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition&amp;#93;&lt;/span&gt;&lt;br/&gt;
     removed2.add(new TopicPartition(&quot;foo&quot;, 0))&lt;br/&gt;
     val context2 = fetchManager.newContext(&lt;br/&gt;
@@ -290,8 +297,10 @@ class FetchSessionTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Create a new fetch session with foo-0 and foo-1&lt;br/&gt;
     val reqData1 = new util.LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reqData1.put(new TopicPartition(&quot;foo&quot;, 0), new FetchRequest.PartitionData(0, 0, 100))&lt;/li&gt;
	&lt;li&gt;reqData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100))&lt;br/&gt;
+    reqData1.put(new TopicPartition(&quot;foo&quot;, 0), new FetchRequest.PartitionData(0, 0, 100,&lt;br/&gt;
+      Optional.empty()))&lt;br/&gt;
+    reqData1.put(new TopicPartition(&quot;foo&quot;, 1), new FetchRequest.PartitionData(10, 0, 100,&lt;br/&gt;
+      Optional.empty()))&lt;br/&gt;
     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData1, EMPTY_PART_LIST, false)&lt;br/&gt;
     assertEquals(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FullFetchContext&amp;#93;&lt;/span&gt;, context1.getClass)&lt;br/&gt;
     val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponse.PartitionData&lt;span class=&quot;error&quot;&gt;&amp;#91;Records&amp;#93;&lt;/span&gt;]&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
index 18b18fdc355..eb3f28e60e7 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala&lt;br/&gt;
@@ -20,7 +20,7 @@ package kafka.server&lt;br/&gt;
 import java.lang.
{Long =&amp;gt; JLong}&lt;br/&gt;
 import java.net.InetAddress&lt;br/&gt;
 import java.util&lt;br/&gt;
-import java.util.Collections&lt;br/&gt;
+import java.util.{Collections, Optional}&lt;br/&gt;
 &lt;br/&gt;
 import kafka.api.{ApiVersion, KAFKA_0_10_2_IV0}&lt;br/&gt;
 import kafka.cluster.Replica&lt;br/&gt;
@@ -118,7 +118,7 @@ class KafkaApisTest {&lt;br/&gt;
       EasyMock.reset(replicaManager, clientRequestQuotaManager, requestChannel)&lt;br/&gt;
 &lt;br/&gt;
       val invalidTopicPartition = new TopicPartition(topic, invalidPartitionId)&lt;br/&gt;
-      val partitionOffsetCommitData = new OffsetCommitRequest.PartitionData(15L, &quot;&quot;)&lt;br/&gt;
+      val partitionOffsetCommitData = new OffsetCommitRequest.PartitionData(15L, 23, &quot;&quot;)&lt;br/&gt;
       val (offsetCommitRequest, request) = buildRequest(new OffsetCommitRequest.Builder(&quot;groupId&quot;,&lt;br/&gt;
         Map(invalidTopicPartition -&amp;gt; partitionOffsetCommitData).asJava))&lt;br/&gt;
 &lt;br/&gt;
@@ -144,7 +144,7 @@ class KafkaApisTest {&lt;br/&gt;
       EasyMock.reset(replicaManager, clientRequestQuotaManager, requestChannel)&lt;br/&gt;
 &lt;br/&gt;
       val invalidTopicPartition = new TopicPartition(topic, invalidPartitionId)&lt;br/&gt;
-      val partitionOffsetCommitData = new TxnOffsetCommitRequest.CommittedOffset(15L, &quot;&quot;)&lt;br/&gt;
+      val partitionOffsetCommitData = new TxnOffsetCommitRequest.CommittedOffset(15L, &quot;&quot;, Optional.empty())&lt;br/&gt;
       val (offsetCommitRequest, request) = buildRequest(new TxnOffsetCommitRequest.Builder(&quot;txnlId&quot;, &quot;groupId&quot;,&lt;br/&gt;
         15L, 0.toShort, Map(invalidTopicPartition -&amp;gt; partitionOffsetCommitData).asJava))&lt;br/&gt;
 &lt;br/&gt;
@@ -377,8 +377,10 @@ class KafkaApisTest {&lt;br/&gt;
     val capturedResponse = expectNoThrottling()&lt;br/&gt;
     EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel, replica, log)&lt;br/&gt;
 &lt;br/&gt;
+&lt;br/&gt;
+    val targetTimes = Map(tp -&amp;gt; new ListOffsetRequest.PartitionData(timestamp, Optional.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;()))&lt;br/&gt;
     val builder = ListOffsetRequest.Builder.forConsumer(true, isolationLevel)&lt;br/&gt;
-      .setTargetTimes(Map(tp -&amp;gt; timestamp).asJava)&lt;br/&gt;
+      .setTargetTimes(targetTimes.asJava)&lt;br/&gt;
     val (listOffsetRequest, request) = buildRequest(builder)&lt;br/&gt;
     createKafkaApis().handleListOffsetRequest(request)&lt;br/&gt;
 &lt;br/&gt;
@@ -418,8 +420,10 @@ class KafkaApisTest {&lt;br/&gt;
     val capturedResponse = expectNoThrottling()&lt;br/&gt;
     EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel, replica, log)&lt;br/&gt;
 &lt;br/&gt;
+    val targetTimes = Map(tp -&amp;gt; new ListOffsetRequest.PartitionData(ListOffsetRequest.EARLIEST_TIMESTAMP,&lt;br/&gt;
+      Optional.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;()))&lt;br/&gt;
     val builder = ListOffsetRequest.Builder.forConsumer(true, isolationLevel)&lt;br/&gt;
-      .setTargetTimes(Map(tp -&amp;gt; (ListOffsetRequest.EARLIEST_TIMESTAMP: JLong)).asJava)&lt;br/&gt;
+      .setTargetTimes(targetTimes.asJava)&lt;br/&gt;
     val (listOffsetRequest, request) = buildRequest(builder)&lt;br/&gt;
     createKafkaApis().handleListOffsetRequest(request)&lt;br/&gt;
 &lt;br/&gt;
@@ -508,8 +512,10 @@ class KafkaApisTest {&lt;br/&gt;
 &lt;br/&gt;
     EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel, replica, log)&lt;br/&gt;
 &lt;br/&gt;
+    val targetTimes = Map(tp -&amp;gt; new ListOffsetRequest.PartitionData(ListOffsetRequest.LATEST_TIMESTAMP,&lt;br/&gt;
+      Optional.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;()))&lt;br/&gt;
     val builder = ListOffsetRequest.Builder.forConsumer(true, isolationLevel)&lt;br/&gt;
-      .setTargetTimes(Map(tp -&amp;gt; (ListOffsetRequest.LATEST_TIMESTAMP: JLong)).asJava)&lt;br/&gt;
+      .setTargetTimes(targetTimes.asJava)&lt;br/&gt;
     val (listOffsetRequest, request) = buildRequest(builder)&lt;br/&gt;
     createKafkaApis().handleListOffsetRequest(request)&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala b/core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala&lt;br/&gt;
index 6ee47eecda2..965413eb612 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala&lt;br/&gt;
@@ -16,7 +16,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
-import java.lang.{Long =&amp;gt; JLong}
&lt;p&gt;+import java.util.Optional&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -33,20 +33,22 @@ class ListOffsetsRequestTest extends BaseRequestTest {&lt;br/&gt;
   def testListOffsetsErrorCodes(): Unit = {&lt;br/&gt;
     val topic = &quot;topic&quot;&lt;br/&gt;
     val partition = new TopicPartition(topic, 0)&lt;br/&gt;
+    val targetTimes = Map(partition -&amp;gt; new ListOffsetRequest.PartitionData(&lt;br/&gt;
+      ListOffsetRequest.EARLIEST_TIMESTAMP, Optional.of&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(0))).asJava&lt;/p&gt;

&lt;p&gt;     val consumerRequest = ListOffsetRequest.Builder&lt;br/&gt;
       .forConsumer(false, IsolationLevel.READ_UNCOMMITTED)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.setTargetTimes(Map(partition -&amp;gt; ListOffsetRequest.EARLIEST_TIMESTAMP.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;JLong&amp;#93;&lt;/span&gt;).asJava)&lt;br/&gt;
+      .setTargetTimes(targetTimes)&lt;br/&gt;
       .build()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val replicaRequest = ListOffsetRequest.Builder&lt;br/&gt;
       .forReplica(ApiKeys.LIST_OFFSETS.latestVersion, servers.head.config.brokerId)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.setTargetTimes(Map(partition -&amp;gt; ListOffsetRequest.EARLIEST_TIMESTAMP.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;JLong&amp;#93;&lt;/span&gt;).asJava)&lt;br/&gt;
+      .setTargetTimes(targetTimes)&lt;br/&gt;
       .build()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val debugReplicaRequest = ListOffsetRequest.Builder&lt;br/&gt;
       .forReplica(ApiKeys.LIST_OFFSETS.latestVersion, ListOffsetRequest.DEBUGGING_REPLICA_ID)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.setTargetTimes(Map(partition -&amp;gt; ListOffsetRequest.EARLIEST_TIMESTAMP.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;JLong&amp;#93;&lt;/span&gt;).asJava)&lt;br/&gt;
+      .setTargetTimes(targetTimes)&lt;br/&gt;
       .build()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Unknown topic&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/LogOffsetTest.scala b/core/src/test/scala/unit/kafka/server/LogOffsetTest.scala&lt;br/&gt;
index e371f7f587c..45096cc61e3 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/LogOffsetTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/LogOffsetTest.scala&lt;br/&gt;
@@ -19,7 +19,7 @@ package kafka.server&lt;/p&gt;

&lt;p&gt; import java.io.File&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicInteger&lt;br/&gt;
-import java.util.&lt;/p&gt;
{Properties, Random}
&lt;p&gt;+import java.util.&lt;/p&gt;
{Optional, Properties, Random}

&lt;p&gt; import kafka.log.&lt;/p&gt;
{Log, LogSegment}
&lt;p&gt; import kafka.network.SocketServer&lt;br/&gt;
@@ -54,7 +54,7 @@ class LogOffsetTest extends BaseRequestTest {&lt;br/&gt;
   def testGetOffsetsForUnknownTopic() {&lt;br/&gt;
     val topicPartition = new TopicPartition(&quot;foo&quot;, 0)&lt;br/&gt;
     val request = ListOffsetRequest.Builder.forConsumer(false, IsolationLevel.READ_UNCOMMITTED)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.setOffsetData(Map(topicPartition -&amp;gt;&lt;br/&gt;
+      .setTargetTimes(Map(topicPartition -&amp;gt;&lt;br/&gt;
         new ListOffsetRequest.PartitionData(ListOffsetRequest.LATEST_TIMESTAMP, 10)).asJava).build(0)&lt;br/&gt;
     val response = sendListOffsetsRequest(request)&lt;br/&gt;
     assertEquals(Errors.UNKNOWN_TOPIC_OR_PARTITION, response.responseData.get(topicPartition).error)&lt;br/&gt;
@@ -86,7 +86,7 @@ class LogOffsetTest extends BaseRequestTest {&lt;br/&gt;
     TestUtils.waitUntilTrue(() =&amp;gt; TestUtils.isLeaderLocalOnBroker(topic, topicPartition.partition, server),&lt;br/&gt;
       &quot;Leader should be elected&quot;)&lt;br/&gt;
     val request = ListOffsetRequest.Builder.forReplica(0, 0)&lt;/li&gt;
	&lt;li&gt;.setOffsetData(Map(topicPartition -&amp;gt;&lt;br/&gt;
+      .setTargetTimes(Map(topicPartition -&amp;gt;&lt;br/&gt;
         new ListOffsetRequest.PartitionData(ListOffsetRequest.LATEST_TIMESTAMP, 15)).asJava).build()&lt;br/&gt;
     val consumerOffsets = sendListOffsetsRequest(request).responseData.get(topicPartition).offsets.asScala&lt;br/&gt;
     assertEquals(Seq(20L, 18L, 16L, 14L, 12L, 10L, 8L, 6L, 4L, 3L), consumerOffsets)&lt;br/&gt;
@@ -114,7 +114,7 @@ class LogOffsetTest extends BaseRequestTest {&lt;br/&gt;
     TestUtils.waitUntilTrue(() =&amp;gt; TestUtils.isLeaderLocalOnBroker(topic, topicPartition.partition, server),&lt;br/&gt;
       &quot;Leader should be elected&quot;)&lt;br/&gt;
     val request = ListOffsetRequest.Builder.forReplica(0, 0)&lt;/li&gt;
	&lt;li&gt;.setOffsetData(Map(topicPartition -&amp;gt;&lt;br/&gt;
+      .setTargetTimes(Map(topicPartition -&amp;gt;&lt;br/&gt;
         new ListOffsetRequest.PartitionData(ListOffsetRequest.LATEST_TIMESTAMP, 15)).asJava).build()&lt;br/&gt;
     val consumerOffsets = sendListOffsetsRequest(request).responseData.get(topicPartition).offsets.asScala&lt;br/&gt;
     assertEquals(Seq(20L, 18L, 16L, 14L, 12L, 10L, 8L, 6L, 4L, 2L, 0L), consumerOffsets)&lt;br/&gt;
@@ -122,7 +122,7 @@ class LogOffsetTest extends BaseRequestTest 
{
     // try to fetch using latest offset
     val fetchRequest = FetchRequest.Builder.forConsumer(0, 1,
       Map(topicPartition -&amp;gt; new FetchRequest.PartitionData(consumerOffsets.head, FetchRequest.INVALID_LOG_START_OFFSET,
-        300 * 1024)).asJava).build()
+        300 * 1024, Optional.empty())).asJava).build()
     val fetchResponse = sendFetchRequest(fetchRequest)
     assertFalse(fetchResponse.responseData.get(topicPartition).records.batches.iterator.hasNext)
   }
&lt;p&gt;@@ -142,7 +142,7 @@ class LogOffsetTest extends BaseRequestTest {&lt;br/&gt;
     for (_ &amp;lt;- 1 to 14) {&lt;br/&gt;
       val topicPartition = new TopicPartition(topic, 0)&lt;br/&gt;
       val request = ListOffsetRequest.Builder.forReplica(0, 0)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;.setOffsetData(Map(topicPartition -&amp;gt;&lt;br/&gt;
+        .setTargetTimes(Map(topicPartition -&amp;gt;&lt;br/&gt;
           new ListOffsetRequest.PartitionData(ListOffsetRequest.EARLIEST_TIMESTAMP, 1)).asJava).build()&lt;br/&gt;
       val consumerOffsets = sendListOffsetsRequest(request).responseData.get(topicPartition).offsets.asScala&lt;br/&gt;
       if (consumerOffsets.head == 1)&lt;br/&gt;
@@ -174,7 +174,7 @@ class LogOffsetTest extends BaseRequestTest {&lt;br/&gt;
     TestUtils.waitUntilTrue(() =&amp;gt; TestUtils.isLeaderLocalOnBroker(topic, topicPartition.partition, server),&lt;br/&gt;
       &quot;Leader should be elected&quot;)&lt;br/&gt;
     val request = ListOffsetRequest.Builder.forReplica(0, 0)&lt;/li&gt;
	&lt;li&gt;.setOffsetData(Map(topicPartition -&amp;gt;&lt;br/&gt;
+      .setTargetTimes(Map(topicPartition -&amp;gt;&lt;br/&gt;
         new ListOffsetRequest.PartitionData(now, 15)).asJava).build()&lt;br/&gt;
     val consumerOffsets = sendListOffsetsRequest(request).responseData.get(topicPartition).offsets.asScala&lt;br/&gt;
     assertEquals(Seq(20L, 18L, 16L, 14L, 12L, 10L, 8L, 6L, 4L, 2L, 0L), consumerOffsets)&lt;br/&gt;
@@ -201,7 +201,7 @@ class LogOffsetTest extends BaseRequestTest {&lt;br/&gt;
     TestUtils.waitUntilTrue(() =&amp;gt; TestUtils.isLeaderLocalOnBroker(topic, topicPartition.partition, server),&lt;br/&gt;
       &quot;Leader should be elected&quot;)&lt;br/&gt;
     val request = ListOffsetRequest.Builder.forReplica(0, 0)&lt;/li&gt;
	&lt;li&gt;.setOffsetData(Map(topicPartition -&amp;gt;&lt;br/&gt;
+      .setTargetTimes(Map(topicPartition -&amp;gt;&lt;br/&gt;
         new ListOffsetRequest.PartitionData(ListOffsetRequest.EARLIEST_TIMESTAMP, 10)).asJava).build()&lt;br/&gt;
     val consumerOffsets = sendListOffsetsRequest(request).responseData.get(topicPartition).offsets.asScala&lt;br/&gt;
     assertEquals(Seq(0L), consumerOffsets)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala b/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala&lt;br/&gt;
index 93ac62d562d..ec3534f2786 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala&lt;br/&gt;
@@ -17,6 +17,7 @@&lt;br/&gt;
 package kafka.server&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.util&lt;br/&gt;
+import java.util.Optional&lt;br/&gt;
 import util.Arrays.asList&lt;/p&gt;

&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -45,7 +46,6 @@ class MetadataCacheTest {&lt;br/&gt;
     val topic0 = &quot;topic-0&quot;&lt;br/&gt;
     val topic1 = &quot;topic-1&quot;&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     val cache = new MetadataCache(1)&lt;/p&gt;

&lt;p&gt;     val zkVersion = 3&lt;br/&gt;
@@ -95,6 +95,7 @@ class MetadataCacheTest {&lt;br/&gt;
           val leader = partitionMetadata.leader&lt;br/&gt;
           val partitionState = topicPartitionStates(new TopicPartition(topic, partitionId))&lt;br/&gt;
           assertEquals(partitionState.basePartitionState.leader, leader.id)&lt;br/&gt;
+          assertEquals(Optional.of(partitionState.basePartitionState.leaderEpoch), partitionMetadata.leaderEpoch)&lt;br/&gt;
           assertEquals(partitionState.basePartitionState.isr, partitionMetadata.isr.asScala.map(_.id).asJava)&lt;br/&gt;
           assertEquals(partitionState.basePartitionState.replicas, partitionMetadata.replicas.asScala.map(_.id).asJava)&lt;br/&gt;
           val endPoint = endPoints(partitionMetadata.leader.id).find(_.listenerName == listenerName).get&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/OffsetsForLeaderEpochRequestTest.scala b/core/src/test/scala/unit/kafka/server/OffsetsForLeaderEpochRequestTest.scala&lt;br/&gt;
index c6385f325ec..e314b443925 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/OffsetsForLeaderEpochRequestTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/OffsetsForLeaderEpochRequestTest.scala&lt;br/&gt;
@@ -16,6 +16,8 @@&lt;br/&gt;
  */&lt;br/&gt;
 package kafka.server&lt;/p&gt;

&lt;p&gt;+import java.util.Optional&lt;br/&gt;
+&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.&lt;/p&gt;
{ApiKeys, Errors}
&lt;p&gt;@@ -33,7 +35,7 @@ class OffsetsForLeaderEpochRequestTest extends BaseRequestTest {&lt;br/&gt;
     val partition = new TopicPartition(topic, 0)&lt;/p&gt;

&lt;p&gt;     val request = new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.add(partition, 0)&lt;br/&gt;
+      .add(partition, Optional.of(5), 0)&lt;br/&gt;
       .build()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Unknown topic&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
index 9c759cebc3f..520801c9bab 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
@@ -486,7 +486,7 @@ class ReplicaFetcherThreadTest {&lt;br/&gt;
     assertEquals(2, mockNetwork.epochFetchCount)&lt;br/&gt;
     assertEquals(1, mockNetwork.fetchCount)&lt;br/&gt;
     assertEquals(&quot;OffsetsForLeaderEpochRequest version.&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;1, mockNetwork.lastUsedOffsetForLeaderEpochVersion)&lt;br/&gt;
+      2, mockNetwork.lastUsedOffsetForLeaderEpochVersion)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Loop 3 we should not fetch epochs&lt;br/&gt;
     thread.doWork()&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala&lt;br/&gt;
index 66a2c8e5fec..ea0a8ef8000 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;br/&gt;
 package kafka.server&lt;/p&gt;

&lt;p&gt; import java.io.File&lt;br/&gt;
-import java.util.Properties&lt;br/&gt;
+import java.util.&lt;/p&gt;
{Optional, Properties}&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicBoolean&lt;br/&gt;
 &lt;br/&gt;
 import kafka.cluster.Replica&lt;br/&gt;
@@ -43,7 +43,9 @@ class ReplicaManagerQuotasTest {&lt;br/&gt;
   val record = new SimpleRecord(&quot;some-data-in-a-message&quot;.getBytes())&lt;br/&gt;
   val topicPartition1 = new TopicPartition(&quot;test-topic&quot;, 1)&lt;br/&gt;
   val topicPartition2 = new TopicPartition(&quot;test-topic&quot;, 2)&lt;br/&gt;
-  val fetchInfo = Seq(topicPartition1 -&amp;gt; new PartitionData(0, 0, 100), topicPartition2 -&amp;gt; new PartitionData(0, 0, 100))&lt;br/&gt;
+  val fetchInfo = Seq(&lt;br/&gt;
+    topicPartition1 -&amp;gt; new PartitionData(0, 0, 100, Optional.empty()),&lt;br/&gt;
+    topicPartition2 -&amp;gt; new PartitionData(0, 0, 100, Optional.empty()))&lt;br/&gt;
   var replicaManager: ReplicaManager = _&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
@@ -164,9 +166,9 @@ class ReplicaManagerQuotasTest {&lt;br/&gt;
       EasyMock.replay(replicaManager)&lt;br/&gt;
 &lt;br/&gt;
       val tp = new TopicPartition(&quot;t1&quot;, 0)&lt;br/&gt;
-      val fetchParititonStatus = new FetchPartitionStatus(new LogOffsetMetadata(messageOffset = 50L, segmentBaseOffset = 0L,&lt;br/&gt;
-        relativePositionInSegment = 250), new PartitionData(50, 0, 1))&lt;br/&gt;
-      val fetchMetadata = new FetchMetadata(fetchMinBytes = 1, fetchMaxBytes = 1000, hardMaxBytesLimit = true, fetchOnlyLeader = true,&lt;br/&gt;
+      val fetchParititonStatus = FetchPartitionStatus(new LogOffsetMetadata(messageOffset = 50L, segmentBaseOffset = 0L,&lt;br/&gt;
+         relativePositionInSegment = 250), new PartitionData(50, 0, 1, Optional.empty()))&lt;br/&gt;
+      val fetchMetadata = FetchMetadata(fetchMinBytes = 1, fetchMaxBytes = 1000, hardMaxBytesLimit = true, fetchOnlyLeader = true,&lt;br/&gt;
         fetchOnlyCommitted = false, isFromFollower = true, replicaId = 1, fetchPartitionStatus = List((tp, fetchParititonStatus)))&lt;br/&gt;
       new DelayedFetch(delayMs = 600, fetchMetadata = fetchMetadata, replicaManager = replicaManager,&lt;br/&gt;
         quota = null, isolationLevel = IsolationLevel.READ_UNCOMMITTED, responseCallback = null) {&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
index 41564a54b05..08440528638 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala&lt;br/&gt;
@@ -18,7 +18,7 @@&lt;br/&gt;
 package kafka.server&lt;br/&gt;
 &lt;br/&gt;
 import java.io.File&lt;br/&gt;
-import java.util.Properties&lt;br/&gt;
+import java.util.{Optional, Properties}
&lt;p&gt; import java.util.concurrent.&lt;/p&gt;
{CountDownLatch, TimeUnit}
&lt;p&gt; import java.util.concurrent.atomic.AtomicBoolean&lt;/p&gt;

&lt;p&gt;@@ -162,7 +162,8 @@ class ReplicaManagerTest {&lt;br/&gt;
       partition.getOrCreateReplica(0)&lt;br/&gt;
       // Make this replica the leader.&lt;br/&gt;
       val leaderAndIsrRequest1 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt; new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, false)).asJava,&lt;br/&gt;
+        collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt;&lt;br/&gt;
+          new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, false)).asJava,&lt;br/&gt;
         Set(new Node(0, &quot;host1&quot;, 0), new Node(1, &quot;host2&quot;, 1)).asJava).build()&lt;br/&gt;
       rm.becomeLeaderOrFollower(0, leaderAndIsrRequest1, (_, _) =&amp;gt; ())&lt;br/&gt;
       rm.getLeaderReplicaIfLocal(new TopicPartition(topic, 0))&lt;br/&gt;
@@ -173,13 +174,15 @@ class ReplicaManagerTest {&lt;br/&gt;
       }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Fetch some messages&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val fetchResult = fetchAsConsumer(rm, new TopicPartition(topic, 0), new PartitionData(0, 0, 100000),&lt;br/&gt;
+      val fetchResult = fetchAsConsumer(rm, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(0, 0, 100000, Optional.empty()),&lt;br/&gt;
         minBytes = 100000)&lt;br/&gt;
       assertFalse(fetchResult.isFired)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Make this replica the follower&lt;br/&gt;
       val leaderAndIsrRequest2 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt; new LeaderAndIsrRequest.PartitionState(0, 1, 1, brokerList, 0, brokerList, false)).asJava,&lt;br/&gt;
+        collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt;&lt;br/&gt;
+          new LeaderAndIsrRequest.PartitionState(0, 1, 1, brokerList, 0, brokerList, false)).asJava,&lt;br/&gt;
         Set(new Node(0, &quot;host1&quot;, 0), new Node(1, &quot;host2&quot;, 1)).asJava).build()&lt;br/&gt;
       rm.becomeLeaderOrFollower(1, leaderAndIsrRequest2, (_, _) =&amp;gt; ())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -203,7 +206,8 @@ class ReplicaManagerTest {&lt;/p&gt;

&lt;p&gt;       // Make this replica the leader.&lt;br/&gt;
       val leaderAndIsrRequest1 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt; new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, true)).asJava,&lt;br/&gt;
+        collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt;&lt;br/&gt;
+          new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, true)).asJava,&lt;br/&gt;
         Set(new Node(0, &quot;host1&quot;, 0), new Node(1, &quot;host2&quot;, 1)).asJava).build()&lt;br/&gt;
       replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest1, (_, _) =&amp;gt; ())&lt;br/&gt;
       replicaManager.getLeaderReplicaIfLocal(new TopicPartition(topic, 0))&lt;br/&gt;
@@ -252,7 +256,8 @@ class ReplicaManagerTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Make this replica the leader.&lt;br/&gt;
       val leaderAndIsrRequest1 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt; new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, true)).asJava,&lt;br/&gt;
+        collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt;&lt;br/&gt;
+          new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, true)).asJava,&lt;br/&gt;
         Set(new Node(0, &quot;host1&quot;, 0), new Node(1, &quot;host2&quot;, 1)).asJava).build()&lt;br/&gt;
       replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest1, (_, _) =&amp;gt; ())&lt;br/&gt;
       replicaManager.getLeaderReplicaIfLocal(new TopicPartition(topic, 0))&lt;br/&gt;
@@ -272,12 +277,14 @@ class ReplicaManagerTest {&lt;br/&gt;
       }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // fetch as follower to advance the high watermark&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetchAsFollower(replicaManager, new TopicPartition(topic, 0), new PartitionData(numRecords, 0, 100000),&lt;br/&gt;
+      fetchAsFollower(replicaManager, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(numRecords, 0, 100000, Optional.empty()),&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_UNCOMMITTED)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // fetch should return empty since LSO should be stuck at 0&lt;br/&gt;
       var consumerFetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new PartitionData(0, 0, 100000), isolationLevel = IsolationLevel.READ_COMMITTED)&lt;br/&gt;
+        new PartitionData(0, 0, 100000, Optional.empty()),&lt;br/&gt;
+        isolationLevel = IsolationLevel.READ_COMMITTED)&lt;br/&gt;
       var fetchData = consumerFetchResult.assertFired&lt;br/&gt;
       assertEquals(Errors.NONE, fetchData.error)&lt;br/&gt;
       assertTrue(fetchData.records.batches.asScala.isEmpty)&lt;br/&gt;
@@ -285,7 +292,8 @@ class ReplicaManagerTest {&lt;br/&gt;
       assertEquals(Some(List.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;AbortedTransaction&amp;#93;&lt;/span&gt;), fetchData.abortedTransactions)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // delayed fetch should timeout and return nothing&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumerFetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0), new PartitionData(0, 0, 100000),&lt;br/&gt;
+      consumerFetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(0, 0, 100000, Optional.empty()),&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_COMMITTED, minBytes = 1000)&lt;br/&gt;
       assertFalse(consumerFetchResult.isFired)&lt;br/&gt;
       timer.advanceClock(1001)&lt;br/&gt;
@@ -304,7 +312,8 @@ class ReplicaManagerTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // the LSO has advanced, but the appended commit marker has not been replicated, so&lt;br/&gt;
       // none of the data from the transaction should be visible yet&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumerFetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0), new PartitionData(0, 0, 100000),&lt;br/&gt;
+      consumerFetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(0, 0, 100000, Optional.empty()),&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_COMMITTED)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       fetchData = consumerFetchResult.assertFired&lt;br/&gt;
@@ -312,11 +321,13 @@ class ReplicaManagerTest {&lt;br/&gt;
       assertTrue(fetchData.records.batches.asScala.isEmpty)&lt;/p&gt;

&lt;p&gt;       // fetch as follower to advance the high watermark&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetchAsFollower(replicaManager, new TopicPartition(topic, 0), new PartitionData(numRecords + 1, 0, 100000),&lt;br/&gt;
+      fetchAsFollower(replicaManager, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(numRecords + 1, 0, 100000, Optional.empty()),&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_UNCOMMITTED)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // now all of the records should be fetchable&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumerFetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0), new PartitionData(0, 0, 100000),&lt;br/&gt;
+      consumerFetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(0, 0, 100000, Optional.empty()),&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_COMMITTED)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       fetchData = consumerFetchResult.assertFired&lt;br/&gt;
@@ -341,7 +352,8 @@ class ReplicaManagerTest {&lt;/p&gt;

&lt;p&gt;       // Make this replica the leader.&lt;br/&gt;
       val leaderAndIsrRequest1 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt; new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, true)).asJava,&lt;br/&gt;
+        collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt;&lt;br/&gt;
+          new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, true)).asJava,&lt;br/&gt;
         Set(new Node(0, &quot;host1&quot;, 0), new Node(1, &quot;host2&quot;, 1)).asJava).build()&lt;br/&gt;
       replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest1, (_, _) =&amp;gt; ())&lt;br/&gt;
       replicaManager.getLeaderReplicaIfLocal(new TopicPartition(topic, 0))&lt;br/&gt;
@@ -366,12 +378,14 @@ class ReplicaManagerTest {&lt;br/&gt;
         .onFire 
{ response =&amp;gt; assertEquals(Errors.NONE, response.error) }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // fetch as follower to advance the high watermark&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fetchAsFollower(replicaManager, new TopicPartition(topic, 0), new PartitionData(numRecords + 1, 0, 100000),&lt;br/&gt;
+      fetchAsFollower(replicaManager, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(numRecords + 1, 0, 100000, Optional.empty()),&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_UNCOMMITTED)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Set the minBytes in order force this request to enter purgatory. When it returns, we should still&lt;br/&gt;
       // see the newly aborted transaction.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val fetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0), new PartitionData(0, 0, 100000),&lt;br/&gt;
+      val fetchResult = fetchAsConsumer(replicaManager, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(0, 0, 100000, Optional.empty()),&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_COMMITTED, minBytes = 10000)&lt;br/&gt;
       assertFalse(fetchResult.isFired)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -403,7 +417,8 @@ class ReplicaManagerTest {&lt;/p&gt;

&lt;p&gt;       // Make this replica the leader.&lt;br/&gt;
       val leaderAndIsrRequest1 = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt; new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, false)).asJava,&lt;br/&gt;
+        collection.immutable.Map(new TopicPartition(topic, 0) -&amp;gt;&lt;br/&gt;
+          new LeaderAndIsrRequest.PartitionState(0, 0, 0, brokerList, 0, brokerList, false)).asJava,&lt;br/&gt;
         Set(new Node(0, &quot;host1&quot;, 0), new Node(1, &quot;host2&quot;, 1), new Node(2, &quot;host2&quot;, 2)).asJava).build()&lt;br/&gt;
       rm.becomeLeaderOrFollower(0, leaderAndIsrRequest1, (_, _) =&amp;gt; ())&lt;br/&gt;
       rm.getLeaderReplicaIfLocal(new TopicPartition(topic, 0))&lt;br/&gt;
@@ -417,13 +432,15 @@ class ReplicaManagerTest {&lt;br/&gt;
       }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Fetch a message above the high watermark as a follower&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val followerFetchResult = fetchAsFollower(rm, new TopicPartition(topic, 0), new PartitionData(1, 0, 100000))&lt;br/&gt;
+      val followerFetchResult = fetchAsFollower(rm, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(1, 0, 100000, Optional.empty()))&lt;br/&gt;
       val followerFetchData = followerFetchResult.assertFired&lt;br/&gt;
       assertEquals(&quot;Should not give an exception&quot;, Errors.NONE, followerFetchData.error)&lt;br/&gt;
       assertTrue(&quot;Should return some data&quot;, followerFetchData.records.batches.iterator.hasNext)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Fetch a message above the high watermark as a consumer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val consumerFetchResult = fetchAsConsumer(rm, new TopicPartition(topic, 0), new PartitionData(1, 0, 100000))&lt;br/&gt;
+      val consumerFetchResult = fetchAsConsumer(rm, new TopicPartition(topic, 0),&lt;br/&gt;
+        new PartitionData(1, 0, 100000, Optional.empty()))&lt;br/&gt;
       val consumerFetchData = consumerFetchResult.assertFired&lt;br/&gt;
       assertEquals(&quot;Should not give an exception&quot;, Errors.NONE, consumerFetchData.error)&lt;br/&gt;
       assertEquals(&quot;Should return empty response&quot;, MemoryRecords.EMPTY, consumerFetchData.records)&lt;br/&gt;
@@ -495,8 +512,8 @@ class ReplicaManagerTest {&lt;br/&gt;
         fetchMaxBytes = Int.MaxValue,&lt;br/&gt;
         hardMaxBytesLimit = false,&lt;br/&gt;
         fetchInfos = Seq(&lt;/li&gt;
	&lt;li&gt;tp0 -&amp;gt; new PartitionData(1, 0, 100000),&lt;/li&gt;
	&lt;li&gt;tp1 -&amp;gt; new PartitionData(1, 0, 100000)),&lt;br/&gt;
+          tp0 -&amp;gt; new PartitionData(1, 0, 100000, Optional.empty()),&lt;br/&gt;
+          tp1 -&amp;gt; new PartitionData(1, 0, 100000, Optional.empty())),&lt;br/&gt;
         responseCallback = fetchCallback,&lt;br/&gt;
         isolationLevel = IsolationLevel.READ_UNCOMMITTED&lt;br/&gt;
       )&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
index d91e008fe3b..3b7ecfb5195 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala&lt;br/&gt;
@@ -15,7 +15,7 @@&lt;br/&gt;
 package kafka.server&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.nio.ByteBuffer&lt;br/&gt;
-import java.util.&lt;/p&gt;
{Collections, LinkedHashMap, Properties}
&lt;p&gt;+import java.util.&lt;/p&gt;
{Collections, LinkedHashMap, Optional, Properties}
&lt;p&gt; import java.util.concurrent.&lt;/p&gt;
{Executors, Future, TimeUnit}

&lt;p&gt; import kafka.log.LogConfig&lt;br/&gt;
@@ -207,7 +207,7 @@ class RequestQuotaTest extends BaseRequestTest {&lt;/p&gt;

&lt;p&gt;         case ApiKeys.FETCH =&amp;gt;&lt;br/&gt;
           val partitionMap = new LinkedHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, FetchRequest.PartitionData&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;partitionMap.put(tp, new FetchRequest.PartitionData(0, 0, 100))&lt;br/&gt;
+          partitionMap.put(tp, new FetchRequest.PartitionData(0, 0, 100, Optional.of(15)))&lt;br/&gt;
           FetchRequest.Builder.forConsumer(0, 0, partitionMap)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case ApiKeys.METADATA =&amp;gt;&lt;br/&gt;
@@ -215,11 +215,13 @@ class RequestQuotaTest extends BaseRequestTest {&lt;/p&gt;

&lt;p&gt;         case ApiKeys.LIST_OFFSETS =&amp;gt;&lt;br/&gt;
           ListOffsetRequest.Builder.forConsumer(false, IsolationLevel.READ_UNCOMMITTED)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.setTargetTimes(Map(tp -&amp;gt; (0L: java.lang.Long)).asJava)&lt;br/&gt;
+            .setTargetTimes(Map(tp -&amp;gt; new ListOffsetRequest.PartitionData(&lt;br/&gt;
+              0L, Optional.of&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;(15))).asJava)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case ApiKeys.LEADER_AND_ISR =&amp;gt;&lt;br/&gt;
           new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, brokerId, Int.MaxValue,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map(tp -&amp;gt; new LeaderAndIsrRequest.PartitionState(Int.MaxValue, brokerId, Int.MaxValue, List(brokerId).asJava, 2, Seq(brokerId).asJava, true)).asJava,&lt;br/&gt;
+            Map(tp -&amp;gt; new LeaderAndIsrRequest.PartitionState(Int.MaxValue, brokerId, Int.MaxValue, List(brokerId).asJava,&lt;br/&gt;
+              2, Seq(brokerId).asJava, true)).asJava,&lt;br/&gt;
             Set(new Node(brokerId, &quot;localhost&quot;, 0)).asJava)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case ApiKeys.STOP_REPLICA =&amp;gt;&lt;br/&gt;
@@ -239,7 +241,7 @@ class RequestQuotaTest extends BaseRequestTest {&lt;/p&gt;

&lt;p&gt;         case ApiKeys.OFFSET_COMMIT =&amp;gt;&lt;br/&gt;
           new OffsetCommitRequest.Builder(&quot;test-group&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Map(tp -&amp;gt; new OffsetCommitRequest.PartitionData(0, &quot;metadata&quot;)).asJava).&lt;br/&gt;
+            Map(tp -&amp;gt; new OffsetCommitRequest.PartitionData(0, 15, &quot;metadata&quot;)).asJava).&lt;br/&gt;
             setMemberId(&quot;&quot;).setGenerationId(1)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case ApiKeys.OFFSET_FETCH =&amp;gt;&lt;br/&gt;
@@ -290,7 +292,8 @@ class RequestQuotaTest extends BaseRequestTest {&lt;br/&gt;
           new InitProducerIdRequest.Builder(&quot;abc&quot;)&lt;/p&gt;

&lt;p&gt;         case ApiKeys.OFFSET_FOR_LEADER_EPOCH =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion()).add(tp, 0)&lt;br/&gt;
+          new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion)&lt;br/&gt;
+            .add(tp, Optional.of(15), 0)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         case ApiKeys.ADD_PARTITIONS_TO_TXN =&amp;gt;&lt;br/&gt;
           new AddPartitionsToTxnRequest.Builder(&quot;test-transactional-id&quot;, 1, 0, List(tp).asJava)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala b/core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala&lt;br/&gt;
index 0797b7b3d11..d2f1c0a0ddd 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala&lt;br/&gt;
@@ -27,7 +27,7 @@ import kafka.zk.KafkaZkClient&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.requests.FetchRequest.PartitionData&lt;br/&gt;
 import org.junit.&lt;/p&gt;
{After, Before, Test}
&lt;p&gt;-import java.util.Properties&lt;br/&gt;
+import java.util.&lt;/p&gt;
{Optional, Properties}
&lt;p&gt; import java.util.concurrent.atomic.AtomicBoolean&lt;/p&gt;

&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
@@ -63,7 +63,8 @@ class SimpleFetchTest {&lt;br/&gt;
   val partitionId = 0&lt;br/&gt;
   val topicPartition = new TopicPartition(topic, partitionId)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val fetchInfo = Seq(topicPartition -&amp;gt; new PartitionData(0, 0, fetchSize))&lt;br/&gt;
+  val fetchInfo = Seq(topicPartition -&amp;gt; new PartitionData(0, 0, fetchSize,&lt;br/&gt;
+    Optional.empty()))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   var replicaManager: ReplicaManager = _&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
index 17683f4f3fd..5ad641f11a0 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala&lt;br/&gt;
@@ -16,7 +16,7 @@&lt;br/&gt;
   */&lt;br/&gt;
 package kafka.server.epoch&lt;/p&gt;

&lt;p&gt;-import java.util.&lt;/p&gt;
{Map =&amp;gt; JMap}
&lt;p&gt;+import java.util.&lt;/p&gt;
{Optional, Map =&amp;gt; JMap}

&lt;p&gt; import kafka.server.KafkaConfig._&lt;br/&gt;
 import kafka.server.&lt;/p&gt;
{BlockingSend, KafkaServer, ReplicaFetcherBlockingSend}
&lt;p&gt;@@ -31,7 +31,6 @@ import org.apache.kafka.common.serialization.StringSerializer&lt;br/&gt;
 import org.apache.kafka.common.utils.&lt;/p&gt;
{LogContext, SystemTime}
&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.protocol.ApiKeys&lt;br/&gt;
-&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.&lt;/p&gt;
{After, Test}
&lt;p&gt; import org.apache.kafka.common.requests.&lt;/p&gt;
{EpochEndOffset, OffsetsForLeaderEpochRequest, OffsetsForLeaderEpochResponse}
&lt;p&gt;@@ -266,13 +265,13 @@ class LeaderEpochIntegrationTest extends ZooKeeperTestHarness with Logging {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;epoch&amp;#93;&lt;/span&gt; class TestFetcherThread(sender: BlockingSend) extends Logging {&lt;/p&gt;

&lt;p&gt;     def leaderOffsetsFor(partitions: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Int&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, EpochEndOffset&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
-      val request = new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion(), toJavaFormat(partitions))
+      val partitionData = partitions.mapValues(
+        new OffsetsForLeaderEpochRequest.PartitionData(Optional.empty(), _))
+      val request = new OffsetsForLeaderEpochRequest.Builder(ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion,
+        partitionData.asJava)
       val response = sender.sendRequest(request)
       response.responseBody.asInstanceOf[OffsetsForLeaderEpochResponse].responses.asScala
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def toJavaFormat(partitions: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Int&amp;#93;&lt;/span&gt;): JMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Integer&amp;#93;&lt;/span&gt; = {&lt;/li&gt;
	&lt;li&gt;partitions.map 
{ case (tp, epoch) =&amp;gt; tp -&amp;gt; epoch.asInstanceOf[Integer] }
&lt;p&gt;.toMap.asJava&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
index 5c60c0017db..4fdc4d26992 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala&lt;br/&gt;
@@ -17,6 +17,7 @@&lt;br/&gt;
 package kafka.server.epoch&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.io.File&lt;br/&gt;
+import java.util.Optional&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicBoolean&lt;/p&gt;

&lt;p&gt; import kafka.cluster.Replica&lt;br/&gt;
@@ -25,13 +26,12 @@ import kafka.utils.&lt;/p&gt;
{MockTime, TestUtils}
&lt;p&gt; import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.protocol.Errors&lt;br/&gt;
-import org.apache.kafka.common.requests.EpochEndOffset&lt;br/&gt;
+import org.apache.kafka.common.requests.&lt;/p&gt;
{EpochEndOffset, OffsetsForLeaderEpochRequest}
&lt;p&gt; import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import org.easymock.EasyMock._&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.Test&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
 class OffsetsForLeaderEpochTest {&lt;br/&gt;
   private val config = TestUtils.createBrokerConfigs(1, TestUtils.MockZkConnect).map(KafkaConfig.fromProps).head&lt;br/&gt;
   private val time = new MockTime&lt;br/&gt;
@@ -43,7 +43,7 @@ class OffsetsForLeaderEpochTest {&lt;br/&gt;
     //Given&lt;br/&gt;
     val epochAndOffset = (5, 42L)&lt;br/&gt;
     val epochRequested: Integer = 5&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val request = Map(tp -&amp;gt; epochRequested)&lt;br/&gt;
+    val request = Map(tp -&amp;gt; new OffsetsForLeaderEpochRequest.PartitionData(Optional.empty(), epochRequested))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //Stubs&lt;br/&gt;
     val mockLog = createNiceMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka.log.Log&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -84,7 +84,7 @@ class OffsetsForLeaderEpochTest {&lt;/p&gt;

&lt;p&gt;     //Given&lt;br/&gt;
     val epochRequested: Integer = 5&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val request = Map(tp -&amp;gt; epochRequested)&lt;br/&gt;
+    val request = Map(tp -&amp;gt; new OffsetsForLeaderEpochRequest.PartitionData(Optional.empty(), epochRequested))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     val response = replicaManager.lastOffsetForLeaderEpoch(request)&lt;br/&gt;
@@ -106,7 +106,7 @@ class OffsetsForLeaderEpochTest {&lt;/p&gt;

&lt;p&gt;     //Given&lt;br/&gt;
     val epochRequested: Integer = 5&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val request = Map(tp -&amp;gt; epochRequested)&lt;br/&gt;
+    val request = Map(tp -&amp;gt; new OffsetsForLeaderEpochRequest.PartitionData(Optional.empty(), epochRequested))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     //When&lt;br/&gt;
     val response = replicaManager.lastOffsetForLeaderEpoch(request)&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 10 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3xd3j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>