<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:01:00 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-5099] Replica Deletion Regression from KIP-101</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-5099</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;It appears that replica deletion regressed from KIP-101. Replica deletion happens when a broker receives a StopReplicaRequest with delete=true. Ever since &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1911&quot; title=&quot;Log deletion on stopping replicas should be async&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1911&quot;&gt;&lt;del&gt;KAFKA-1911&lt;/del&gt;&lt;/a&gt;, replica deletion has been async, meaning the broker responds with a StopReplicaResponse simply after marking the replica directory as staged for deletion. This marking happens by moving a data log directory and its contents such as /tmp/kafka-logs1/t1-0 to a marked directory like /tmp/kafka-logs1/t1-0.8c9c4c0c61c44cc59ebeb00075a2a07f-delete, acting as a soft-delete. A scheduled thread later actually deletes the data. It appears that the regression occurs while the scheduled thread is actually trying to delete the data, which means the controller considers operations such as partition reassignment and topic deletion complete. But if you look at the log4j logs and data logs, you&apos;ll find that the soft-deleted data logs haven&apos;t actually won&apos;t get deleted. It seems that restarting the broker actually allows for the soft-deleted directories to get deleted.&lt;/p&gt;

&lt;p&gt;Here&apos;s the setup:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt; ./bin/zookeeper-server-start.sh config/zookeeper.properties
&amp;gt; export LOG_DIR=logs0 &amp;amp;&amp;amp; ./bin/kafka-server-start.sh config/server0.properties
&amp;gt; export LOG_DIR=logs1 &amp;amp;&amp;amp; ./bin/kafka-server-start.sh config/server1.properties
&amp;gt; ./bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic t0 --replica-assignment 1:0
&amp;gt; ./bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic t1 --replica-assignment 1:0
&amp;gt; ./bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic t0
&amp;gt; cat p.txt
{&lt;span class=&quot;code-quote&quot;&gt;&quot;partitions&quot;&lt;/span&gt;:
 [
  {&lt;span class=&quot;code-quote&quot;&gt;&quot;topic&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;t1&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;partition&quot;&lt;/span&gt;: 0, &lt;span class=&quot;code-quote&quot;&gt;&quot;replicas&quot;&lt;/span&gt;: [0] }
 ],
&lt;span class=&quot;code-quote&quot;&gt;&quot;version&quot;&lt;/span&gt;:1
}
&amp;gt; ./bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file p.txt --execute
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here are sample logs:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2017-04-20 17:46:54,801] INFO [ReplicaFetcherManager on broker 1] Removed fetcher &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions t0-0 (kafka.server.ReplicaFetcherManager)
[2017-04-20 17:46:54,814] INFO Log &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition t0-0 is renamed to /tmp/kafka-logs1/t0-0.bbc8fa126e3e4ff787f6b68d158ab771-delete and is scheduled &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; deletion (kafka.log.LogManager)
[2017-04-20 17:47:27,585] INFO Deleting index /tmp/kafka-logs1/t0-0.bbc8fa126e3e4ff787f6b68d158ab771-delete/00000000000000000000.index (kafka.log.OffsetIndex)
[2017-04-20 17:47:27,586] INFO Deleting index /tmp/kafka-logs1/t0-0/00000000000000000000.timeindex (kafka.log.TimeIndex)
[2017-04-20 17:47:27,587] ERROR Exception in deleting Log(/tmp/kafka-logs1/t0-0.bbc8fa126e3e4ff787f6b68d158ab771-delete). Moving it to the end of the queue. (kafka.log.LogManager)
java.io.FileNotFoundException: /tmp/kafka-logs1/t0-0/leader-epoch-checkpoint.tmp (No such file or directory)
  at java.io.FileOutputStream.open0(Native Method)
  at java.io.FileOutputStream.open(FileOutputStream.java:270)
  at java.io.FileOutputStream.&amp;lt;init&amp;gt;(FileOutputStream.java:213)
  at java.io.FileOutputStream.&amp;lt;init&amp;gt;(FileOutputStream.java:162)
  at kafka.server.checkpoints.CheckpointFile.write(CheckpointFile.scala:41)
  at kafka.server.checkpoints.LeaderEpochCheckpointFile.write(LeaderEpochCheckpointFile.scala:61)
  at kafka.server.epoch.LeaderEpochFileCache.kafka$server$epoch$LeaderEpochFileCache$$flush(LeaderEpochFileCache.scala:178)
  at kafka.server.epoch.LeaderEpochFileCache$$anonfun$clear$1.apply$mcV$sp(LeaderEpochFileCache.scala:161)
  at kafka.server.epoch.LeaderEpochFileCache$$anonfun$clear$1.apply(LeaderEpochFileCache.scala:159)
  at kafka.server.epoch.LeaderEpochFileCache$$anonfun$clear$1.apply(LeaderEpochFileCache.scala:159)
  at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:213)
  at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:221)
  at kafka.server.epoch.LeaderEpochFileCache.clear(LeaderEpochFileCache.scala:159)
  at kafka.log.Log.delete(Log.scala:1051)
  at kafka.log.LogManager.kafka$log$LogManager$$deleteLogs(LogManager.scala:442)
  at kafka.log.LogManager$$anonfun$startup$5.apply$mcV$sp(LogManager.scala:241)
  at kafka.utils.KafkaScheduler$$anonfun$1.apply$mcV$sp(KafkaScheduler.scala:110)
  at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:57)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The same appears on broker 1 for t1-0 after partition reassignment moves the replica off broker 1.&lt;/p&gt;

&lt;p&gt;Here&apos;s what the data logs show:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt; l /tmp/kafka-logs*/t*
/tmp/kafka-logs0/t0-0.166137ca8c28475d8c9fbf6f423a937e-delete:
total 0
-rw-r--r--  1 okaraman  wheel   0 Apr 20 17:46 00000000000000000000.timeindex
-rw-r--r--  1 okaraman  wheel   0 Apr 20 17:46 leader-epoch-checkpoint
drwxr-xr-x  2 okaraman  wheel  68 Apr 20 17:46 pid-mapping

/tmp/kafka-logs0/t1-0:
total 40960
-rw-r--r--  1 okaraman  wheel  10485760 Apr 20 17:46 00000000000000000000.index
-rw-r--r--  1 okaraman  wheel         0 Apr 20 17:46 00000000000000000000.log
-rw-r--r--  1 okaraman  wheel  10485756 Apr 20 17:46 00000000000000000000.timeindex
-rw-r--r--  1 okaraman  wheel         0 Apr 20 17:46 leader-epoch-checkpoint
drwxr-xr-x  2 okaraman  wheel        68 Apr 20 17:46 pid-mapping

/tmp/kafka-logs1/t0-0.bbc8fa126e3e4ff787f6b68d158ab771-delete:
total 0
-rw-r--r--  1 okaraman  wheel   0 Apr 20 17:46 00000000000000000000.timeindex
-rw-r--r--  1 okaraman  wheel   0 Apr 20 17:46 leader-epoch-checkpoint
drwxr-xr-x  2 okaraman  wheel  68 Apr 20 17:46 pid-mapping

/tmp/kafka-logs1/t1-0.8c9c4c0c61c44cc59ebeb00075a2a07f-delete:
total 0
-rw-r--r--  1 okaraman  wheel   0 Apr 20 17:48 00000000000000000000.timeindex
-rw-r--r--  1 okaraman  wheel   0 Apr 20 17:46 leader-epoch-checkpoint
drwxr-xr-x  2 okaraman  wheel  68 Apr 20 17:46 pid-mapping
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I ran the above test on the checkin preceding KIP-101 and it worked just fine.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13065729">KAFKA-5099</key>
            <summary>Replica Deletion Regression from KIP-101</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="onurkaraman">Onur Karaman</assignee>
                                    <reporter username="onurkaraman">Onur Karaman</reporter>
                        <labels>
                    </labels>
                <created>Fri, 21 Apr 2017 01:13:27 +0000</created>
                <updated>Wed, 10 May 2017 20:46:51 +0000</updated>
                            <resolved>Wed, 10 May 2017 20:45:47 +0000</resolved>
                                                    <fixVersion>0.11.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15977902" author="onurkaraman" created="Fri, 21 Apr 2017 01:14:59 +0000"  >&lt;p&gt;My initial guess is that the new epoch checkpoint file isn&apos;t being moved to the soft-deleted directory correctly.&lt;/p&gt;</comment>
                            <comment id="15977937" author="ijuma" created="Fri, 21 Apr 2017 01:57:58 +0000"  >&lt;p&gt;Great catch! Seems like our tests need to verify that the space is reclaimed eventually.&lt;/p&gt;</comment>
                            <comment id="15999104" author="junrao" created="Fri, 5 May 2017 23:22:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=onurkaraman&quot; class=&quot;user-hover&quot; rel=&quot;onurkaraman&quot;&gt;onurkaraman&lt;/a&gt;, thanks for reporting this. It seem the issue is that when log.delete is called, we try to flush the leader epoch file, which requires the creation of a tmp file in the original log dir. Since the original log dir has been renamed, the creation with throw a FileNotFoundException. One way to fix this is to rename LeaderEpochCache.clear to LeaderEpochCache.clearAndFlush, and introduce a new method LeaderEpochCache.clear that just clears without flush. Log.delete will call LeaderEpochCache.clear. All other cases will call LeaderEpochCache.clearAndFlush.&lt;/p&gt;</comment>
                            <comment id="15999200" author="onurkaraman" created="Sat, 6 May 2017 00:58:01 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;. Thanks for figuring it out. I gave your solution a shot and it does seem to delete the data now. However, I noticed something which I&apos;m not sure is normal. While the files get deleted successfully now, it looks like the process holds onto the file descriptors corresponding to the index and timeindex files in their soft-delete directories for a few minutes. Is this somehow related to them being mmap&apos;d?&lt;/p&gt;

&lt;p&gt;So there&apos;s a window of time where I see:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt; l /tmp/kafka-logs*/t*
ls: /tmp/kafka-logs*/t*: No such file or directory
&amp;gt; lsof -a -p 42214 | grep &lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/kafka-logs&quot;&lt;/span&gt;
java    42214 okaraman  txt      REG                1,4         0 86940741 /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/tmp/kafka-logs0/t0-0.f87568e7c6db40c7a898719376a79f8b-delete/00000000000000000000.index
java    42214 okaraman  txt      REG                1,4         0 86940744 /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/tmp/kafka-logs0/t0-0.f87568e7c6db40c7a898719376a79f8b-delete/00000000000000000000.timeindex
java    42214 okaraman  123u     REG                1,4         0 86940706 /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/tmp/kafka-logs0/.lock
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The file descriptor goes away after a few minutes:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt; l /tmp/kafka-logs*/t*
ls: /tmp/kafka-logs*/t*: No such file or directory
&amp;gt; lsof -a -p 42214 | grep &lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/kafka-logs&quot;&lt;/span&gt;
java    42214 okaraman  123u     REG                1,4         0 86940706 /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/tmp/kafka-logs0/.lock
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15999211" author="junrao" created="Sat, 6 May 2017 01:21:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=onurkaraman&quot; class=&quot;user-hover&quot; rel=&quot;onurkaraman&quot;&gt;onurkaraman&lt;/a&gt;, that&apos;s probably expected. The issue is that java doesn&apos;t have an unmap method. We try to call AbstractIndex.forceUnmap(), but it&apos;s not guaranteed to be supported in all jvms. For those unsupported jvms, those mmap files will only be cleaned after GC, which is not deterministic. That&apos;s probably not a new issue due to kip-101. Could you verify?&lt;/p&gt;</comment>
                            <comment id="15999218" author="junrao" created="Sat, 6 May 2017 01:52:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=onurkaraman&quot; class=&quot;user-hover&quot; rel=&quot;onurkaraman&quot;&gt;onurkaraman&lt;/a&gt;, that&apos;s probably expected. The issue is that java doesn&apos;t have an unmap method. We try to call AbstractIndex.forceUnmap(), but it&apos;s not guaranteed to be supported in all jvms. For those unsupported jvms, those mmap files will only be cleaned after GC, which is not deterministic. That&apos;s probably not a new issue due to kip-101. Could you verify?&lt;/p&gt;</comment>
                            <comment id="15999250" author="onurkaraman" created="Sat, 6 May 2017 03:38:19 +0000"  >&lt;p&gt;I ran the file descriptor experiment against both my patch as well as against git hash b611cfa5c0f4941a491781424afd9b699bdb894e (the commit before KIP-101: 0baea2ac13532981f3fea11e5dfc6da5aafaeaa8) several times now.&lt;/p&gt;

&lt;p&gt;I&apos;ve now witnessed file descriptor reclamation for my patch take as little as 4 minutes and as much as unbounded (I eventually gave up waiting after 20 minutes). I&apos;ve only ever witnessed file descriptor reclamation for b611cfa5c0f4941a491781424afd9b699bdb894e be unbounded (I eventually gave up waiting after 20 minutes).&lt;/p&gt;

&lt;p&gt;Not sure what to make of this. b611cfa5c0f4941a491781424afd9b699bdb894e is more recent than the fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-4614&quot; title=&quot;Long GC pause harming broker performance which is caused by mmap objects created for OffsetIndex&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-4614&quot;&gt;&lt;del&gt;KAFKA-4614&lt;/del&gt;&lt;/a&gt; (5fc530bc483db145e0cba3b63a57d6d6a7c547f2).&lt;/p&gt;

&lt;p&gt;I&apos;m just going to submit the PR.&lt;/p&gt;</comment>
                            <comment id="15999253" author="githubbot" created="Sat, 6 May 2017 03:46:13 +0000"  >&lt;p&gt;GitHub user onurkaraman opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/2986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/2986&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5099&quot; title=&quot;Replica Deletion Regression from KIP-101&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5099&quot;&gt;&lt;del&gt;KAFKA-5099&lt;/del&gt;&lt;/a&gt;: Replica Deletion Regression from KIP-101&lt;/p&gt;

&lt;p&gt;    Replica deletion regressed from KIP-101. Replica deletion happens when a broker receives a StopReplicaRequest with delete=true. Ever since &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1911&quot; title=&quot;Log deletion on stopping replicas should be async&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1911&quot;&gt;&lt;del&gt;KAFKA-1911&lt;/del&gt;&lt;/a&gt;, replica deletion has been async, meaning the broker responds with a StopReplicaResponse simply after marking the replica directory as staged for deletion. This marking happens by moving a data log directory and its contents such as /tmp/kafka-logs1/t1-0 to a marked directory like /tmp/kafka-logs1/t1-0.8c9c4c0c61c44cc59ebeb00075a2a07f-delete, acting as a soft-delete. A scheduled thread later actually deletes the data. It appears that the regression occurs while the scheduled thread is actually trying to delete the data, which means the controller considers operations such as partition reassignment and topic deletion complete. But if you look at the log4j logs and data logs, you&apos;ll find that the soft-deleted data logs actually won&apos;t get deleted.&lt;/p&gt;

&lt;p&gt;    The bug is that upon log deletion, we attempt to flush the LeaderEpochFileCache to the original file location instead of the moved file location. Restarting the broker actually allows for the soft-deleted directories to get deleted.&lt;/p&gt;

&lt;p&gt;    This patch avoids the issue by simply not flushing the LeaderEpochFileCache upon log deletion.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/onurkaraman/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/onurkaraman/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5099&quot; title=&quot;Replica Deletion Regression from KIP-101&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5099&quot;&gt;&lt;del&gt;KAFKA-5099&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/2986.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/2986.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2986&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 081e09be262dea261f7faf9b5d81b50995600f68&lt;br/&gt;
Author: Onur Karaman &amp;lt;okaraman@linkedin.com&amp;gt;&lt;br/&gt;
Date:   2017-05-06T00:59:36Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5099&quot; title=&quot;Replica Deletion Regression from KIP-101&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5099&quot;&gt;&lt;del&gt;KAFKA-5099&lt;/del&gt;&lt;/a&gt;: Replica Deletion Regression from KIP-101&lt;/p&gt;

&lt;p&gt;    Replica deletion regressed from KIP-101. Replica deletion happens when a broker receives a StopReplicaRequest with delete=true. Ever since &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1911&quot; title=&quot;Log deletion on stopping replicas should be async&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1911&quot;&gt;&lt;del&gt;KAFKA-1911&lt;/del&gt;&lt;/a&gt;, replica deletion has been async, meaning the broker responds with a StopReplicaResponse simply after marking the replica directory as staged for deletion. This marking happens by moving a data log directory and its contents such as /tmp/kafka-logs1/t1-0 to a marked directory like /tmp/kafka-logs1/t1-0.8c9c4c0c61c44cc59ebeb00075a2a07f-delete, acting as a soft-delete. A scheduled thread later actually deletes the data. It appears that the regression occurs while the scheduled thread is actually trying to delete the data, which means the controller considers operations such as partition reassignment and topic deletion complete. But if you look at the log4j logs and data logs, you&apos;ll find that the soft-deleted data logs actually won&apos;t get deleted.&lt;/p&gt;

&lt;p&gt;    The bug is that upon log deletion, we attempt to flush the LeaderEpochFileCache to the original file location instead of the moved file location. Restarting the broker actually allows for the soft-deleted directories to get deleted.&lt;/p&gt;

&lt;p&gt;    This patch avoids the issue by simply not flushing the LeaderEpochFileCache upon log deletion.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16000261" author="onurkaraman" created="Mon, 8 May 2017 04:50:18 +0000"  >&lt;p&gt;By the way, I reran the file descriptor experiment against both my patch as well as against git hash b611cfa5c0f4941a491781424afd9b699bdb894e (the commit before KIP-101: 0baea2ac13532981f3fea11e5dfc6da5aafaeaa8) on both mac and linux. The file descriptors stick around while the index files have been removed for over 10 minutes so I gave up waiting.&lt;/p&gt;</comment>
                            <comment id="16005373" author="junrao" created="Wed, 10 May 2017 20:45:47 +0000"  >&lt;p&gt;Issue resolved by pull request 2986&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/2986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/2986&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16005374" author="githubbot" created="Wed, 10 May 2017 20:46:51 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/2986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/2986&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 27 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3dwkf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>