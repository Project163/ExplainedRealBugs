<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:13:34 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-3514] Stream timestamp computation needs some further thoughts</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-3514</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;KIP-353: &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-353%3A+Improve+Kafka+Streams+Timestamp+Synchronization&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-353%3A+Improve+Kafka+Streams+Timestamp+Synchronization&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Our current stream task&apos;s timestamp is used for punctuate function as well as selecting which stream to process next (i.e. best effort stream synchronization). And it is defined as the smallest timestamp over all partitions in the task&apos;s partition group. This results in two unintuitive corner cases:&lt;/p&gt;

&lt;p&gt;1) observing a late arrived record would keep that stream&apos;s timestamp low for a period of time, and hence keep being process until that late record. For example take two partitions within the same task annotated by their timestamps:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Stream A: 5, 6, 7, 8, 9, 1, 10
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Stream B: 2, 3, 4, 5
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The late arrived record with timestamp &quot;1&quot; will cause stream A to be selected continuously in the thread loop, i.e. messages with timestamp 5, 6, 7, 8, 9 until the record itself is dequeued and processed, then stream B will be selected starting with timestamp 2.&lt;/p&gt;

&lt;p&gt;2) an empty buffered partition will cause its timestamp to be not advanced, and hence the task timestamp as well since it is the smallest among all partitions. This may not be a severe problem compared with 1) above though.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Update&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;There is one more thing to consider (full discussion found here: &lt;a href=&quot;http://search-hadoop.com/m/Kafka/uyzND1iKZJN1yz0E5?subj=Order+of+punctuate+and+process+in+a+stream+processor&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://search-hadoop.com/m/Kafka/uyzND1iKZJN1yz0E5?subj=Order+of+punctuate+and+process+in+a+stream+processor&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Let&apos;s assume the following case.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;a stream processor that uses the Processor API&lt;/li&gt;
	&lt;li&gt;context.schedule(1000) is called in the init()&lt;/li&gt;
	&lt;li&gt;the processor reads only one topic that has one partition&lt;/li&gt;
	&lt;li&gt;using custom timestamp extractor, but that timestamp is just a wall&lt;br/&gt;
 clock time&lt;br/&gt;
 Image the following events:&lt;br/&gt;
 1., for 10 seconds I send in 5 messages / second&lt;br/&gt;
 2., does not send any messages for 3 seconds&lt;br/&gt;
 3., starts the 5 messages / second again&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I see that punctuate() is not called during the 3 seconds when I do not &lt;br/&gt;
 send any messages. This is ok according to the documentation, because &lt;br/&gt;
 there is not any new messages to trigger the punctuate() call. When the &lt;br/&gt;
 first few messages arrives after a restart the sending (point 3. above) I &lt;br/&gt;
 see the following sequence of method calls:&lt;/p&gt;

&lt;p&gt;1., process() on the 1st message&lt;br/&gt;
 2., punctuate() is called 3 times&lt;br/&gt;
 3., process() on the 2nd message&lt;br/&gt;
 4., process() on each following message&lt;/p&gt;

&lt;p&gt;What I would expect instead is that punctuate() is called first and then &lt;br/&gt;
 process() is called on the messages, because the first message&apos;s timestamp &lt;br/&gt;
 is already 3 seconds older then the last punctuate() was called, so the &lt;br/&gt;
 first message belongs after the 3 punctuate() calls.&lt;/p&gt;&lt;/blockquote&gt;</description>
                <environment></environment>
        <key id="12956307">KAFKA-3514</key>
            <summary>Stream timestamp computation needs some further thoughts</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="guozhang">Guozhang Wang</assignee>
                                    <reporter username="guozhang">Guozhang Wang</reporter>
                        <labels>
                            <label>architecture</label>
                            <label>kip</label>
                    </labels>
                <created>Tue, 5 Apr 2016 22:08:53 +0000</created>
                <updated>Fri, 18 Oct 2019 17:59:24 +0000</updated>
                            <resolved>Sat, 29 Sep 2018 00:01:44 +0000</resolved>
                                                    <fixVersion>2.1.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>16</watches>
                                                                                                                <comments>
                            <comment id="15562785" author="djchooy" created="Mon, 10 Oct 2016 16:49:15 +0000"  >&lt;p&gt;maybe just use Max(all_paritions_ts) instead of min (for the punctuate logic)?&lt;/p&gt;</comment>
                            <comment id="15683056" author="mihbor" created="Mon, 21 Nov 2016 09:55:06 +0000"  >&lt;p&gt;IMO, 2) &lt;b&gt;is&lt;/b&gt; a severe problem. Punctuate methods (as described by their API) are meant to perform periodic operations. As it currently stands, if any of the input topics doesn&apos;t receive messages regularly, the punctuate method won&apos;t be called regularly either (due to the min offset across all partitions not advancing), which violates what the API promises. &lt;br/&gt;
We&apos;ve worked around it in our app by creating an independent stream and a scheduler sending ticks regularly to an input topic to a Transformer, so that it&apos;s punctuate method is called predictably but this is far from ideal.&lt;/p&gt;</comment>
                            <comment id="15897290" author="enothereska" created="Mon, 6 Mar 2017 13:46:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mihbor&quot; class=&quot;user-hover&quot; rel=&quot;mihbor&quot;&gt;mihbor&lt;/a&gt; 2) above is slightly different from the problem you are describing (it seems to me). I think the issue you are describing is that punctuate is based on event time, not system time.&lt;/p&gt;</comment>
                            <comment id="15897371" author="mihbor" created="Mon, 6 Mar 2017 14:12:08 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enothereska&quot; class=&quot;user-hover&quot; rel=&quot;enothereska&quot;&gt;enothereska&lt;/a&gt;,&lt;br/&gt;
I have to disagree. It is perfectly clear to me (from the documentation) that punctuate is based on event time, not system time. However, the problem is event time is not advanced reliably, since a single input stream that doesn&apos;t receive messages will cause the event time to not be advanced. In an extreme case of a poorly partitioned topic, I can imagine some partition may never get a message. That would cause a topology that has that partition as input to not advance event time ever, hence not fire punctuate ever, regardless of the presence of messages on its other input topics. In my opinion, if the purpose of punctuate is to perform periodic operations, then this flaw makes it unfit for that purpose. &lt;/p&gt;</comment>
                            <comment id="15897381" author="enothereska" created="Mon, 6 Mar 2017 14:18:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mihbor&quot; class=&quot;user-hover&quot; rel=&quot;mihbor&quot;&gt;mihbor&lt;/a&gt; I understand now. I had thought the effect was limited, but what you say makes sense. Thanks.&lt;/p&gt;</comment>
                            <comment id="15897413" author="mihbor" created="Mon, 6 Mar 2017 14:37:06 +0000"  >&lt;p&gt;Thank you for responding.&lt;br/&gt;
Just now I had a thought about the semantics of event time.&lt;br/&gt;
It is already possible to provide a TimestampExtractor that determines what the event time is, given a message.&lt;br/&gt;
It&apos;s not far fetched to assume user should also want a way to specify what the event time is, given the absence of messages (on one or more input partitions).&lt;br/&gt;
Possibly by providing an implementation other than what PartitionGroup.timestamp() is doing based on the timestamps of its partitions.&lt;/p&gt;</comment>
                            <comment id="15897729" author="mjsax" created="Mon, 6 Mar 2017 17:52:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mihbor&quot; class=&quot;user-hover&quot; rel=&quot;mihbor&quot;&gt;mihbor&lt;/a&gt; I am not sure about this. To me it seems to be a cleaner solution, to change punctuate semantics back to system time.&lt;/p&gt;</comment>
                            <comment id="15897771" author="mihbor" created="Mon, 6 Mar 2017 18:24:33 +0000"  >&lt;p&gt;Oh, I wouldn&apos;t mind that at all. Just thought that you wanted to stick to event time semantics for this, but if you&apos;re not precious about that then I&apos;m all for it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15897891" author="guozhang" created="Mon, 6 Mar 2017 19:30:54 +0000"  >&lt;p&gt;There are some discussions on pros and cons of changing the punctuate semantics: &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/pull/1689&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1689&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15926036" author="arunmathew88" created="Wed, 15 Mar 2017 12:15:37 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mjsax&quot; class=&quot;user-hover&quot; rel=&quot;mjsax&quot;&gt;mjsax&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mihbor&quot; class=&quot;user-hover&quot; rel=&quot;mihbor&quot;&gt;mihbor&lt;/a&gt; . We were building an audit trail for kafka based on kafka stream and encountered similar issue.&lt;/p&gt;

&lt;p&gt;Our work around was to use a hybrid of event time and system time. During regular operation we use event time. But when we create the punctuation schedules (object representing the next punctuation time) we also record the system time at which the punctuation schedule was created. The punctuate code was modified to punctuate anyway, in case the interval specified in the punctuation schedule has elapsed in terms of system time (current time - time of punctuation schedule creation), a new punctuation schedule corresponding to the next expected punctuation time (current punctuation time + punctuation interval) is also created. &lt;/p&gt;

&lt;p&gt;In an earlier version of kafka the above logic sufficed as the mayBePunctuate was called as part of the polling for events (in the absence of events). But current version doesn&apos;t seem to call it so we had to patch that portion a bit too.&lt;/p&gt;

&lt;p&gt;Please let me know your thoughts.&lt;/p&gt;</comment>
                            <comment id="15926839" author="mjsax" created="Wed, 15 Mar 2017 19:51:29 +0000"  >&lt;p&gt;Thanks for you input. It&apos;s an interesting approach. We plan to do a KIP for this change, as it will be tricky to get right. It would be great if you would participate in the discussion on the dev list &amp;#8211; not sure, when the KIP will be pushed at the moment. (see &lt;a href=&quot;https://github.com/apache/kafka/pull/1689#issuecomment-286523692&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/1689#issuecomment-286523692&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="15990176" author="mihbor" created="Sun, 30 Apr 2017 08:56:36 +0000"  >&lt;p&gt;I think the description of this ticket is missing an important detail.&lt;br/&gt;
If my understanding is correct, it will behave as described if all the records arrive in a single batch.&lt;br/&gt;
However, if the records preceding the record with timestamp &quot;1&quot; come in a separate batch (I&apos;ll use brackets to depict batch boundaries):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Stream A: [5, 6, 7, 8, 9], [1, 10]

Stream B: [2, 3, 4, 5]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;then initially the timestamp for stream A is going to be set to &quot;5&quot; (minimum of the first batch) and since it&apos;s not allowed to move back, the second batch containing the late arriving record &quot;1&quot; is not going to change that. Stream B is going to be drained first until &quot;5&quot;.&lt;br/&gt;
However, if the batch boundaries are different by just one record and the late arriving &quot;1&quot; is in the first batch:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Stream A: [5, 6, 7, 8, 9, 1], [10]

Stream B: [2, 3, 4, 5]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; then it&apos;s going to behave as currently described.&lt;/p&gt;

&lt;p&gt;Please correct me if I got this wrong.&lt;br/&gt;
But if that is the case, it feels all too non-deterministic and I think the timestamp computation deserves further thought beyond the scope of &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-138%3A+Change+punctuate+semantics&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KIP-138&lt;/a&gt;, which is limited to punctuate semantics, but not stream time semantics in general.&lt;/p&gt;</comment>
                            <comment id="15990281" author="mjsax" created="Sun, 30 Apr 2017 16:25:35 +0000"  >&lt;p&gt;Your observation is correct &amp;#8211; and this ticket is exactly about the issue you describe. There are two problems with the current design (1) it&apos;s hard to reason about processing order and (2) there is some non-determinism. But I think it does not affect KIP-138 (I agree that it&apos;s somehow connected). KIP-138 is about when/how to punctuate and the punctuation strategy should be agnostic to the way &quot;stream time&quot; gets advanced.&lt;/p&gt;</comment>
                            <comment id="15990288" author="mihbor" created="Sun, 30 Apr 2017 16:51:03 +0000"  >&lt;p&gt;Agreed. That&apos;s what I meant. Beyond the scope of KIP-138 and let&apos;s keep it that way.&lt;/p&gt;</comment>
                            <comment id="16008459" author="mjsax" created="Fri, 12 May 2017 17:44:10 +0000"  >&lt;p&gt;This JIRA also relates to this user question: &lt;a href=&quot;http://search-hadoop.com/m/uyzND1iKZJN1yz0E5&amp;amp;subj=Order+of+punctuate+and+process+in+a+stream+processor&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://search-hadoop.com/m/uyzND1iKZJN1yz0E5&amp;amp;subj=Order+of+punctuate+and+process+in+a+stream+processor&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We should consider scheduled punctuations, when advancing &quot;stream time&quot; &amp;#8211; not just advance &quot;stream time&quot; coarse grain by record timestamps only.&lt;/p&gt;</comment>
                            <comment id="16009365" author="mihbor" created="Sat, 13 May 2017 15:08:35 +0000"  >&lt;p&gt;I&apos;ve created &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5233&quot; title=&quot;Changes to punctuate semantics (KIP-138)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-5233&quot;&gt;&lt;del&gt;KAFKA-5233&lt;/del&gt;&lt;/a&gt; to track work related to KIP-138. As noted above, the considerations on this ticket span beyond the scope of KIP-138, which is agnostic to how the stream time gets advanced.&lt;/p&gt;</comment>
                            <comment id="16010813" author="guozhang" created="Mon, 15 May 2017 16:37:46 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mihbor&quot; class=&quot;user-hover&quot; rel=&quot;mihbor&quot;&gt;mihbor&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16015410" author="sini" created="Thu, 18 May 2017 08:14:43 +0000"  >&lt;p&gt;Considering the above ideas, I would prefer to see stream time &amp;amp; punctuation to work in the following way:&lt;br/&gt;
1., stream time is advanced before the message is processed (not after as it happens now), so the message will be processed in the appropriate punctuation call&lt;br/&gt;
2., call punctuate() with not the current stream time, but with the timestamp calculated with first_punctuation_timestamp + X * scheduled_time at each iteration, so that each punctuation will see a time increase difference that equals to the scheduled time interval, even those that is called just after each other because the lack of messages for awhile. (Now the real difference varies between (0 - 2x interval) in case of low message traffic).&lt;br/&gt;
3., consider running the missed punctuation when the application starts up, may be an additional option for context.schedule()&lt;/p&gt;

&lt;p&gt;What is your opinion on this?&lt;/p&gt;</comment>
                            <comment id="16106086" author="mihbor" created="Sat, 29 Jul 2017 09:53:29 +0000"  >&lt;p&gt;1. and 2. make perfect sense to me and I think I&apos;d have time to work on it. 3. is a bit more involved as we&apos;d have to persist the last punctuation timestamp, right?&lt;br/&gt;
Would 1.&amp;amp;2. require a KIP or is that considered minor/bugfix? Would 3. require a KIP?&lt;/p&gt;</comment>
                            <comment id="16547251" author="githubbot" created="Wed, 18 Jul 2018 00:27:42 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5382: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Remove min timestamp tracker&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5382&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5382&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   1. Remove MinTimestampTracker and its TimestampTracker interface.&lt;br/&gt;
   2. In RecordQueue, keep track of the head record (deserialized) while put the rest raw bytes records in the fifo queue, the head record as well as the partition timestamp will be updated accordingly.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16549398" author="githubbot" created="Thu, 19 Jul 2018 15:06:20 +0000"  >&lt;p&gt;guozhangwang closed pull request #5382: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Remove min timestamp tracker&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5382&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5382&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index df35c3d69f1..00000000000&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,84 +0,0 @@&lt;br/&gt;
-/*&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements. See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License. You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-package org.apache.kafka.streams.processor.internals;&lt;br/&gt;
-&lt;br/&gt;
-import java.util.LinkedList;&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/li&gt;
	&lt;li&gt;* MinTimestampTracker implements 
{@link TimestampTracker}
&lt;p&gt; that maintains the min&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* timestamp of the maintained stamped elements.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-public class MinTimestampTracker&amp;lt;E&amp;gt; implements TimestampTracker&amp;lt;E&amp;gt; {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// first element has the lowest timestamp and last element the highest&lt;/li&gt;
	&lt;li&gt;private final LinkedList&amp;lt;Stamped&amp;lt;E&amp;gt;&amp;gt; ascendingSubsequence = new LinkedList&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// in the case that incoming traffic is very small, the records maybe put and polled&lt;/li&gt;
	&lt;li&gt;// within a single iteration, in this case we need to remember the last polled&lt;/li&gt;
	&lt;li&gt;// record&apos;s timestamp&lt;/li&gt;
	&lt;li&gt;private long lastKnownTime = NOT_KNOWN;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* @throws NullPointerException if the element is null&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public void addElement(final Stamped&amp;lt;E&amp;gt; elem) {&lt;/li&gt;
	&lt;li&gt;if (elem == null) throw new NullPointerException();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Stamped&amp;lt;E&amp;gt; maxElem = ascendingSubsequence.peekLast();&lt;/li&gt;
	&lt;li&gt;while (maxElem != null &amp;amp;&amp;amp; maxElem.timestamp &amp;gt;= elem.timestamp) 
{
-            ascendingSubsequence.removeLast();
-            maxElem = ascendingSubsequence.peekLast();
-        }&lt;/li&gt;
	&lt;li&gt;ascendingSubsequence.offerLast(elem); //lower timestamps have been retained and all greater/equal removed&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;public void removeElement(final Stamped&amp;lt;E&amp;gt; elem) {&lt;/li&gt;
	&lt;li&gt;if (elem == null) 
{
-            return;
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (ascendingSubsequence.peekFirst() == elem) 
{
-            ascendingSubsequence.removeFirst();
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (ascendingSubsequence.isEmpty()) 
{
-            lastKnownTime = elem.timestamp;
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;public int size() 
{
-        return ascendingSubsequence.size();
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* @return the lowest tracked timestamp&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public long get() 
{
-        Stamped&amp;lt;E&amp;gt; stamped = ascendingSubsequence.peekFirst();
-
-        if (stamped == null)
-            return lastKnownTime;
-        else
-            return stamped.timestamp;
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public void clear() 
{
-        lastKnownTime = NOT_KNOWN;
-        ascendingSubsequence.clear();
-    }
&lt;p&gt;-}&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
index c809da9b4cb..34252bf2b4c 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
@@ -58,7 +58,7 @@ RecordQueue queue() 
{
         nonEmptyQueuesByTime = new PriorityQueue&amp;lt;&amp;gt;(partitionQueues.size(), Comparator.comparingLong(RecordQueue::timestamp));
         this.partitionQueues = partitionQueues;
         totalBuffered = 0;
-        streamTime = -1;
+        streamTime = RecordQueue.NOT_KNOWN;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java&lt;br/&gt;
index 22ef4d63f8c..86340bb82c1 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java&lt;br/&gt;
@@ -35,16 +35,19 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;timestamp is monotonically increasing such that once it is advanced, it will not be decremented.&lt;br/&gt;
  */&lt;br/&gt;
 public class RecordQueue {&lt;br/&gt;
+&lt;br/&gt;
+    static final long NOT_KNOWN = -1L;&lt;br/&gt;
+&lt;br/&gt;
+    private final Logger log;&lt;br/&gt;
     private final SourceNode source;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final TimestampExtractor timestampExtractor;&lt;br/&gt;
     private final TopicPartition partition;&lt;/li&gt;
	&lt;li&gt;private final ArrayDeque&amp;lt;StampedRecord&amp;gt; fifoQueue;&lt;/li&gt;
	&lt;li&gt;private final TimestampTracker&amp;lt;ConsumerRecord&amp;lt;Object, Object&amp;gt;&amp;gt; timeTracker;&lt;/li&gt;
	&lt;li&gt;private final RecordDeserializer recordDeserializer;&lt;br/&gt;
     private final ProcessorContext processorContext;&lt;/li&gt;
	&lt;li&gt;private final Logger log;&lt;br/&gt;
+    private final TimestampExtractor timestampExtractor;&lt;br/&gt;
+    private final RecordDeserializer recordDeserializer;&lt;br/&gt;
+    private final ArrayDeque&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; fifoQueue;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long partitionTime = TimestampTracker.NOT_KNOWN;&lt;br/&gt;
+    private long partitionTime = NOT_KNOWN;&lt;br/&gt;
+    private StampedRecord headRecord = null;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     RecordQueue(final TopicPartition partition,&lt;br/&gt;
                 final SourceNode source,&lt;br/&gt;
@@ -52,11 +55,10 @@&lt;br/&gt;
                 final DeserializationExceptionHandler deserializationExceptionHandler,&lt;br/&gt;
                 final InternalProcessorContext processorContext,&lt;br/&gt;
                 final LogContext logContext) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this.partition = partition;&lt;br/&gt;
         this.source = source;&lt;/li&gt;
	&lt;li&gt;this.timestampExtractor = timestampExtractor;&lt;br/&gt;
+        this.partition = partition;&lt;br/&gt;
         this.fifoQueue = new ArrayDeque&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;this.timeTracker = new MinTimestampTracker&amp;lt;&amp;gt;();&lt;br/&gt;
+        this.timestampExtractor = timestampExtractor;&lt;br/&gt;
         this.recordDeserializer = new RecordDeserializer(&lt;br/&gt;
             source,&lt;br/&gt;
             deserializationExceptionHandler,&lt;br/&gt;
@@ -93,48 +95,10 @@ public TopicPartition partition() {&lt;br/&gt;
      */&lt;br/&gt;
     int addRawRecords(final Iterable&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; rawRecords) {&lt;br/&gt;
         for (final ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; rawRecord : rawRecords) {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final ConsumerRecord&amp;lt;Object, Object&amp;gt; record = recordDeserializer.deserialize(processorContext, rawRecord);&lt;/li&gt;
	&lt;li&gt;if (record == null) 
{
-                // this only happens if the deserializer decides to skip. It has already logged the reason.
-                continue;
-            }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;final long timestamp;&lt;/li&gt;
	&lt;li&gt;try 
{
-                timestamp = timestampExtractor.extract(record, timeTracker.get());
-            }
&lt;p&gt; catch (final StreamsException internalFatalExtractorException) &lt;/p&gt;
{
-                throw internalFatalExtractorException;
-            }
&lt;p&gt; catch (final Exception fatalUserException) &lt;/p&gt;
{
-                throw new StreamsException(
-                    String.format(&quot;Fatal user code error in TimestampExtractor callback for record %s.&quot;, record),
-                    fatalUserException);
-            }&lt;/li&gt;
	&lt;li&gt;log.trace(&quot;Source node {} extracted timestamp {} for record {}&quot;, source.name(), timestamp, record);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// drop message if TS is invalid, i.e., negative&lt;/li&gt;
	&lt;li&gt;if (timestamp &amp;lt; 0) {&lt;/li&gt;
	&lt;li&gt;log.warn(&lt;/li&gt;
	&lt;li&gt;&quot;Skipping record due to negative extracted timestamp. topic=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; partition=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; offset=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; extractedTimestamp=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; extractor=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt;&quot;,&lt;/li&gt;
	&lt;li&gt;record.topic(), record.partition(), record.offset(), timestamp, timestampExtractor.getClass().getCanonicalName()&lt;/li&gt;
	&lt;li&gt;);&lt;/li&gt;
	&lt;li&gt;((StreamsMetricsImpl) processorContext.metrics()).skippedRecordsSensor().record();&lt;/li&gt;
	&lt;li&gt;continue;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final StampedRecord stampedRecord = new StampedRecord(record, timestamp);&lt;/li&gt;
	&lt;li&gt;fifoQueue.addLast(stampedRecord);&lt;/li&gt;
	&lt;li&gt;timeTracker.addElement(stampedRecord);&lt;br/&gt;
+            fifoQueue.addLast(rawRecord);&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// update the partition timestamp if its currently&lt;/li&gt;
	&lt;li&gt;// tracked min timestamp has exceed its value; this will&lt;/li&gt;
	&lt;li&gt;// usually only take effect for the first added batch&lt;/li&gt;
	&lt;li&gt;final long timestamp = timeTracker.get();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;if (timestamp &amp;gt; partitionTime) 
{
-            partitionTime = timestamp;
-        }&lt;br/&gt;
+        maybeUpdateTimestamp();&lt;br/&gt;
 &lt;br/&gt;
         return size();&lt;br/&gt;
     }&lt;br/&gt;
@@ -145,23 +109,12 @@ int addRawRecords(final Iterable&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; rawRecords) {&lt;br/&gt;
      * @return StampedRecord&lt;br/&gt;
      */&lt;br/&gt;
     public StampedRecord poll() {&lt;br/&gt;
-        final StampedRecord elem = fifoQueue.pollFirst();&lt;br/&gt;
-&lt;br/&gt;
-        if (elem == null) {
-            return null;
-        }&lt;br/&gt;
-&lt;br/&gt;
-        timeTracker.removeElement(elem);&lt;br/&gt;
+        final StampedRecord recordToReturn = headRecord;&lt;br/&gt;
+        headRecord = null;&lt;br/&gt;
 &lt;br/&gt;
-        // only advance the partition timestamp if its currently&lt;br/&gt;
-        // tracked min timestamp has exceeded its value&lt;br/&gt;
-        final long timestamp = timeTracker.get();&lt;br/&gt;
+        maybeUpdateTimestamp();&lt;br/&gt;
 &lt;br/&gt;
-        if (timestamp &amp;gt; partitionTime) {-            partitionTime = timestamp;-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;return elem;&lt;br/&gt;
+        return recordToReturn;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -170,7 +123,8 @@ public StampedRecord poll() {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return the number of records&lt;br/&gt;
      */&lt;br/&gt;
     public int size() 
{
-        return fifoQueue.size();
+        // plus one deserialized head record for timestamp tracking
+        return fifoQueue.size() + (headRecord == null ? 0 : 1);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -179,7 +133,7 @@ public int size() {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return true if the queue is empty, otherwise false&lt;br/&gt;
      */&lt;br/&gt;
     public boolean isEmpty() 
{
-        return fifoQueue.isEmpty();
+        return fifoQueue.isEmpty() &amp;amp;&amp;amp; headRecord == null;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -196,16 +150,48 @@ public long timestamp() {&lt;br/&gt;
      */&lt;br/&gt;
     public void clear() &lt;/p&gt;
{
         fifoQueue.clear();
-        timeTracker.clear();
-        partitionTime = TimestampTracker.NOT_KNOWN;
+        headRecord = null;
+        partitionTime = NOT_KNOWN;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/*&lt;/li&gt;
	&lt;li&gt;* Returns the timestamp tracker of the record queue&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* This is only used for testing&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;TimestampTracker&amp;lt;ConsumerRecord&amp;lt;Object, Object&amp;gt;&amp;gt; timeTracker() {&lt;/li&gt;
	&lt;li&gt;return timeTracker;&lt;br/&gt;
+    private void maybeUpdateTimestamp() {&lt;br/&gt;
+        while (headRecord == null &amp;amp;&amp;amp; !fifoQueue.isEmpty()) {&lt;br/&gt;
+            final ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; raw = fifoQueue.pollFirst();&lt;br/&gt;
+            final ConsumerRecord&amp;lt;Object, Object&amp;gt; deserialized = recordDeserializer.deserialize(processorContext, raw);&lt;br/&gt;
+&lt;br/&gt;
+            if (deserialized == null) 
{
+                // this only happens if the deserializer decides to skip. It has already logged the reason.
+                continue;
+            }
&lt;p&gt;+&lt;br/&gt;
+            final long timestamp;&lt;br/&gt;
+            try &lt;/p&gt;
{
+                timestamp = timestampExtractor.extract(deserialized, partitionTime);
+            }
&lt;p&gt; catch (final StreamsException internalFatalExtractorException) &lt;/p&gt;
{
+                throw internalFatalExtractorException;
+            }
&lt;p&gt; catch (final Exception fatalUserException) &lt;/p&gt;
{
+                throw new StreamsException(
+                        String.format(&quot;Fatal user code error in TimestampExtractor callback for record %s.&quot;, deserialized),
+                        fatalUserException);
+            }
&lt;p&gt;+            log.trace(&quot;Source node {} extracted timestamp {} for record {}&quot;, source.name(), timestamp, deserialized);&lt;br/&gt;
+&lt;br/&gt;
+            // drop message if TS is invalid, i.e., negative&lt;br/&gt;
+            if (timestamp &amp;lt; 0) {&lt;br/&gt;
+                log.warn(&lt;br/&gt;
+                        &quot;Skipping record due to negative extracted timestamp. topic=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; partition=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; offset=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; extractedTimestamp=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt; extractor=&lt;span class=&quot;error&quot;&gt;&amp;#91;{}&amp;#93;&lt;/span&gt;&quot;,&lt;br/&gt;
+                        deserialized.topic(), deserialized.partition(), deserialized.offset(), timestamp, timestampExtractor.getClass().getCanonicalName()&lt;br/&gt;
+                );&lt;br/&gt;
+                ((StreamsMetricsImpl) processorContext.metrics()).skippedRecordsSensor().record();&lt;br/&gt;
+                continue;&lt;br/&gt;
+            }&lt;br/&gt;
+&lt;br/&gt;
+            headRecord = new StampedRecord(deserialized, timestamp);&lt;br/&gt;
+&lt;br/&gt;
+            // update the partition timestamp if the current head record&apos;s timestamp has exceed its value&lt;br/&gt;
+            if (timestamp &amp;gt; partitionTime) &lt;/p&gt;
{
+                partitionTime = timestamp;
+            }
&lt;p&gt;+        }&lt;br/&gt;
     }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
index b01fd5b35ea..4c06c391fda 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
@@ -69,7 +69,8 @@ public void close() {}&lt;br/&gt;
             return Collections.emptyMap();&lt;br/&gt;
         }&lt;br/&gt;
     };&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;private long streamTime = TimestampTracker.NOT_KNOWN;&lt;br/&gt;
+&lt;br/&gt;
+    private long streamTime = RecordQueue.NOT_KNOWN;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     StandbyContextImpl(final TaskId id,&lt;br/&gt;
                        final StreamsConfig config,&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index 6af4c4bb132..7f121fe0df4 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -716,7 +716,7 @@ public boolean maybePunctuateStreamTime() {&lt;/p&gt;

&lt;p&gt;         // if the timestamp is not known yet, meaning there is not enough data accumulated&lt;br/&gt;
         // to reason stream partition time, then skip.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (timestamp == TimestampTracker.NOT_KNOWN) {&lt;br/&gt;
+        if (timestamp == RecordQueue.NOT_KNOWN) 
{
             return false;
         }
&lt;p&gt; else {&lt;br/&gt;
             return streamTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.STREAM_TIME, this);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TimestampTracker.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TimestampTracker.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 30c816dd526..00000000000&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TimestampTracker.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,61 +0,0 @@&lt;br/&gt;
-/*&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements. See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License. You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-package org.apache.kafka.streams.processor.internals;&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/li&gt;
	&lt;li&gt;* TimestampTracker is a helper class for a sliding window implementation.&lt;/li&gt;
	&lt;li&gt;* It is assumed that stamped elements are added or removed in a FIFO manner.&lt;/li&gt;
	&lt;li&gt;* It maintains the timestamp, such as the min timestamp, the max timestamp, etc.&lt;/li&gt;
	&lt;li&gt;* of stamped elements that were added but not yet removed.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-public interface TimestampTracker&amp;lt;E&amp;gt; {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;long NOT_KNOWN = -1L;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Adds a stamped elements to this tracker.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param elem the added element&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;void addElement(Stamped&amp;lt;E&amp;gt; elem);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Removed a stamped elements to this tracker.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param elem the removed element&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;void removeElement(Stamped&amp;lt;E&amp;gt; elem);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Returns the current tracked timestamp&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @return timestamp, or 
{@link #NOT_KNOWN}
&lt;p&gt; when empty&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;long get();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Returns the size of internal structure. The meaning of &quot;size&quot; depends on the implementation.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @return size&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;int size();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Empty the tracker by removing any tracked stamped elements&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;void clear();&lt;br/&gt;
-}&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 24653e684c7..00000000000
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,78 +0,0 @@&lt;br/&gt;
-/*&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements. See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License. You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-package org.apache.kafka.streams.processor.internals;&lt;br/&gt;
-&lt;br/&gt;
-import static org.hamcrest.CoreMatchers.equalTo;&lt;br/&gt;
-import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
-&lt;br/&gt;
-import org.junit.Test;&lt;br/&gt;
-&lt;br/&gt;
-public class MinTimestampTrackerTest {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private MinTimestampTracker&amp;lt;String&amp;gt; tracker = new MinTimestampTracker&amp;lt;&amp;gt;();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldReturnNotKnownTimestampWhenNoRecordsEverAdded() 
{
-        assertThat(tracker.get(), equalTo(TimestampTracker.NOT_KNOWN));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldReturnTimestampOfOnlyRecord() 
{
-        tracker.addElement(elem(100));
-        assertThat(tracker.get(), equalTo(100L));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldReturnLowestAvailableTimestampFromAllInputs() 
{
-        tracker.addElement(elem(100));
-        tracker.addElement(elem(99));
-        tracker.addElement(elem(102));
-        assertThat(tracker.get(), equalTo(99L));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldReturnLowestAvailableTimestampAfterPreviousLowestRemoved() 
{
-        final Stamped&amp;lt;String&amp;gt; lowest = elem(88);
-        tracker.addElement(lowest);
-        tracker.addElement(elem(101));
-        tracker.addElement(elem(99));
-        tracker.removeElement(lowest);
-        assertThat(tracker.get(), equalTo(99L));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldReturnLastKnownTimestampWhenAllElementsHaveBeenRemoved() 
{
-        final Stamped&amp;lt;String&amp;gt; record = elem(98);
-        tracker.addElement(record);
-        tracker.removeElement(record);
-        assertThat(tracker.get(), equalTo(98L));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldIgnoreNullRecordOnRemove() 
{
-        tracker.removeElement(null);
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test(expected = NullPointerException.class)&lt;/li&gt;
	&lt;li&gt;public void shouldThrowNullPointerExceptionWhenTryingToAddNullElement() 
{
-        tracker.addElement(null);
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private Stamped&amp;lt;String&amp;gt; elem(final long timestamp) 
{
-        return new Stamped&amp;lt;&amp;gt;(&quot;&quot;, timestamp);
-    }
&lt;p&gt;-}&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
index 9823ae1bc3a..b3123e46343 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
@@ -140,49 +140,49 @@ public void testTimeTracking() {&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
         // 1:&lt;span class=&quot;error&quot;&gt;&amp;#91;5, 2, 4&amp;#93;&lt;/span&gt;&lt;br/&gt;
         // 2:&lt;span class=&quot;error&quot;&gt;&amp;#91;4, 6&amp;#93;&lt;/span&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;// st: 3 (2&apos;s presence prevents it from advancing to 4)&lt;br/&gt;
+        // st: 4 as partition st is now 
{5, 4}
&lt;p&gt;         assertEquals(partition1, info.partition());&lt;br/&gt;
         assertEquals(3L, record.timestamp);&lt;br/&gt;
         assertEquals(5, group.numBuffered());&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition2));&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertEquals(3L, group.timestamp());&lt;br/&gt;
+        assertEquals(4L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// 1:&lt;span class=&quot;error&quot;&gt;&amp;#91;2, 4&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;// 2:&lt;span class=&quot;error&quot;&gt;&amp;#91;4, 6&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;// st: 3 (2&apos;s presence prevents it from advancing to 4)&lt;/li&gt;
	&lt;li&gt;assertEquals(partition1, info.partition());&lt;/li&gt;
	&lt;li&gt;assertEquals(5L, record.timestamp);&lt;br/&gt;
+        // 1:&lt;span class=&quot;error&quot;&gt;&amp;#91;5, 2, 4&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        // 2:&lt;span class=&quot;error&quot;&gt;&amp;#91;6&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        // st: 5 as partition st is now 
{5, 6}
&lt;p&gt;+        assertEquals(partition2, info.partition());&lt;br/&gt;
+        assertEquals(4L, record.timestamp);&lt;br/&gt;
         assertEquals(4, group.numBuffered());&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertEquals(2, group.numBuffered(partition1));&lt;/li&gt;
	&lt;li&gt;assertEquals(2, group.numBuffered(partition2));&lt;/li&gt;
	&lt;li&gt;assertEquals(3L, group.timestamp());&lt;br/&gt;
+        assertEquals(3, group.numBuffered(partition1));&lt;br/&gt;
+        assertEquals(1, group.numBuffered(partition2));&lt;br/&gt;
+        assertEquals(5L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one more record, now time should be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// 1:&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;// 2:&lt;span class=&quot;error&quot;&gt;&amp;#91;4, 6&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;// st: 4&lt;br/&gt;
+        // 1:&lt;span class=&quot;error&quot;&gt;&amp;#91;2, 4&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        // 2:&lt;span class=&quot;error&quot;&gt;&amp;#91;6&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        // st: 5&lt;br/&gt;
         assertEquals(partition1, info.partition());&lt;/li&gt;
	&lt;li&gt;assertEquals(2L, record.timestamp);&lt;br/&gt;
+        assertEquals(5L, record.timestamp);&lt;br/&gt;
         assertEquals(3, group.numBuffered());&lt;/li&gt;
	&lt;li&gt;assertEquals(1, group.numBuffered(partition1));&lt;/li&gt;
	&lt;li&gt;assertEquals(2, group.numBuffered(partition2));&lt;/li&gt;
	&lt;li&gt;assertEquals(4L, group.timestamp());&lt;br/&gt;
+        assertEquals(2, group.numBuffered(partition1));&lt;br/&gt;
+        assertEquals(1, group.numBuffered(partition2));&lt;br/&gt;
+        assertEquals(5L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one more record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
         // 1:&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt;&lt;br/&gt;
         // 2:&lt;span class=&quot;error&quot;&gt;&amp;#91;6&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// st: 4&lt;/li&gt;
	&lt;li&gt;assertEquals(partition2, info.partition());&lt;/li&gt;
	&lt;li&gt;assertEquals(4L, record.timestamp);&lt;br/&gt;
+        // st: 5&lt;br/&gt;
+        assertEquals(partition1, info.partition());&lt;br/&gt;
+        assertEquals(2L, record.timestamp);&lt;br/&gt;
         assertEquals(2, group.numBuffered());&lt;br/&gt;
         assertEquals(1, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(1, group.numBuffered(partition2));&lt;/li&gt;
	&lt;li&gt;assertEquals(4L, group.timestamp());&lt;br/&gt;
+        assertEquals(5L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one more record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -194,7 +194,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(1, group.numBuffered());&lt;br/&gt;
         assertEquals(0, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(1, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(4L, group.timestamp());&lt;br/&gt;
+        assertEquals(5L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one more record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -206,7 +206,7 @@ public void testTimeTracking() &lt;/p&gt;
{
         assertEquals(0, group.numBuffered());
         assertEquals(0, group.numBuffered(partition1));
         assertEquals(0, group.numBuffered(partition2));
-        assertEquals(4L, group.timestamp());
+        assertEquals(5L, group.timestamp());
 
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java&lt;br/&gt;
index 3ed9e3b61a0..cf1d63f1e1e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java&lt;br/&gt;
@@ -102,7 +102,7 @@ public void testTimeTracking() {&lt;/p&gt;

&lt;p&gt;         assertTrue(queue.isEmpty());&lt;br/&gt;
         assertEquals(0, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(TimestampTracker.NOT_KNOWN, queue.timestamp());&lt;br/&gt;
+        assertEquals(RecordQueue.NOT_KNOWN, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add three 3 out-of-order records with timestamp 2, 1, 3&lt;br/&gt;
         final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; list1 = Arrays.asList(&lt;br/&gt;
@@ -113,20 +113,17 @@ public void testTimeTracking() {&lt;br/&gt;
         queue.addRawRecords(list1);&lt;/p&gt;

&lt;p&gt;         assertEquals(3, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1L, queue.timestamp());&lt;/li&gt;
	&lt;li&gt;assertEquals(2, queue.timeTracker().size());&lt;br/&gt;
+        assertEquals(2L, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // poll the first record, now with 1, 3&lt;br/&gt;
         assertEquals(2L, queue.poll().timestamp);&lt;br/&gt;
         assertEquals(2, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1L, queue.timestamp());&lt;/li&gt;
	&lt;li&gt;assertEquals(2, queue.timeTracker().size());&lt;br/&gt;
+        assertEquals(2L, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // poll the second record, now with 3&lt;br/&gt;
         assertEquals(1L, queue.poll().timestamp);&lt;br/&gt;
         assertEquals(1, queue.size());&lt;br/&gt;
         assertEquals(3L, queue.timestamp());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1, queue.timeTracker().size());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add three 3 out-of-order records with timestamp 4, 1, 2&lt;br/&gt;
         // now with 3, 4, 1, 2&lt;br/&gt;
@@ -139,28 +136,23 @@ public void testTimeTracking() {&lt;/p&gt;

&lt;p&gt;         assertEquals(4, queue.size());&lt;br/&gt;
         assertEquals(3L, queue.timestamp());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(2, queue.timeTracker().size());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // poll the third record, now with 4, 1, 2&lt;br/&gt;
         assertEquals(3L, queue.poll().timestamp);&lt;br/&gt;
         assertEquals(3, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, queue.timestamp());&lt;/li&gt;
	&lt;li&gt;assertEquals(2, queue.timeTracker().size());&lt;br/&gt;
+        assertEquals(4L, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // poll the rest records&lt;br/&gt;
         assertEquals(4L, queue.poll().timestamp);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, queue.timestamp());&lt;/li&gt;
	&lt;li&gt;assertEquals(2, queue.timeTracker().size());&lt;br/&gt;
+        assertEquals(4L, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(1L, queue.poll().timestamp);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, queue.timestamp());&lt;/li&gt;
	&lt;li&gt;assertEquals(1, queue.timeTracker().size());&lt;br/&gt;
+        assertEquals(4L, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(2L, queue.poll().timestamp);&lt;br/&gt;
         assertTrue(queue.isEmpty());&lt;br/&gt;
         assertEquals(0, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, queue.timestamp());&lt;/li&gt;
	&lt;li&gt;assertEquals(0, queue.timeTracker().size());&lt;br/&gt;
+        assertEquals(4L, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add three more records with 4, 5, 6&lt;br/&gt;
         final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; list3 = Arrays.asList(&lt;br/&gt;
@@ -177,14 +169,12 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(4L, queue.poll().timestamp);&lt;br/&gt;
         assertEquals(2, queue.size());&lt;br/&gt;
         assertEquals(5L, queue.timestamp());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(2, queue.timeTracker().size());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // clear the queue&lt;br/&gt;
         queue.clear();&lt;br/&gt;
         assertTrue(queue.isEmpty());&lt;br/&gt;
         assertEquals(0, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(0, queue.timeTracker().size());&lt;/li&gt;
	&lt;li&gt;assertEquals(TimestampTracker.NOT_KNOWN, queue.timestamp());&lt;br/&gt;
+        assertEquals(RecordQueue.NOT_KNOWN, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // re-insert the three records with 4, 5, 6&lt;br/&gt;
         queue.addRawRecords(list3);&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16549942" author="githubbot" created="Thu, 19 Jul 2018 22:12:34 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5398: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Part II, Choose tasks with data on all partitions to process&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5398&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5398&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   1. In each iteration, decide if a task is processable if all of its partitions contains data, so it can decide which record to process next.&lt;/p&gt;

&lt;p&gt;   1.a Add one exception that, if the task indeed have data on some but not all of its partitions, we only consider as not processable for some finite round of iterations.&lt;/p&gt;

&lt;p&gt;   2. Break the main loop on put-raw-data and process-them. Since now not all data put into the queue would be processed completely within a single iteration.&lt;/p&gt;

&lt;p&gt;   3. Found and fixed a bug in metrics recording: the taskName and sensorName parameters were exchanged.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16559039" author="githubbot" created="Thu, 26 Jul 2018 23:22:47 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5428: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Part III, Refactor StreamThread main loop&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5428&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5428&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   *More detailed description of your change,&lt;br/&gt;
   if necessary. The PR title and PR message become&lt;br/&gt;
   the squashed commit message, so use a separate&lt;br/&gt;
   comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   for the feature or bug fix. Unit and/or integration&lt;br/&gt;
   tests are expected for any behaviour change and&lt;br/&gt;
   system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16561177" author="githubbot" created="Sun, 29 Jul 2018 17:04:32 +0000"  >&lt;p&gt;guozhangwang closed pull request #5428: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Part III, Refactor StreamThread main loop&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5428&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5428&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java b/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java&lt;br/&gt;
index fefeae343e0..6c504e3473f 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java&lt;br/&gt;
@@ -499,7 +499,7 @@ public ConsumerConfig(Map&amp;lt;String, Object&amp;gt; props) &lt;/p&gt;
{
         super(CONFIG, props);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ConsumerConfig(Map&amp;lt;?, ?&amp;gt; props, boolean doLog) {&lt;br/&gt;
+    protected ConsumerConfig(Map&amp;lt;?, ?&amp;gt; props, boolean doLog) 
{
         super(CONFIG, props, doLog);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java b/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java&lt;br/&gt;
index dc00b473027..52c72444091 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java&lt;br/&gt;
@@ -21,6 +21,7 @@&lt;br/&gt;
 import org.apache.kafka.clients.producer.internals.FutureRecordMetadata;&lt;br/&gt;
 import org.apache.kafka.clients.producer.internals.ProduceRequestResult;&lt;br/&gt;
 import org.apache.kafka.common.Cluster;&lt;br/&gt;
+import org.apache.kafka.common.KafkaException;&lt;br/&gt;
 import org.apache.kafka.common.Metric;&lt;br/&gt;
 import org.apache.kafka.common.MetricName;&lt;br/&gt;
 import org.apache.kafka.common.PartitionInfo;&lt;br/&gt;
@@ -244,7 +245,12 @@ private void verifyNoTransactionInFlight() {&lt;br/&gt;
      */&lt;br/&gt;
     @Override&lt;br/&gt;
     public synchronized Future&amp;lt;RecordMetadata&amp;gt; send(ProducerRecord&amp;lt;K, V&amp;gt; record, Callback callback) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;verifyProducerState();&lt;br/&gt;
+        if (this.closed) 
{
+            throw new IllegalStateException(&quot;MockProducer is already closed.&quot;);
+        }
&lt;p&gt;+        if (this.producerFenced) &lt;/p&gt;
{
+            throw new KafkaException(&quot;MockProducer is fenced.&quot;, new ProducerFencedException(&quot;Fenced&quot;));
+        }
&lt;p&gt;         int partition = 0;&lt;br/&gt;
         if (!this.cluster.partitionsForTopic(record.topic()).isEmpty())&lt;br/&gt;
             partition = partition(record, this.cluster);&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java&lt;br/&gt;
index 7a8c710b76b..a6134772571 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
 import org.apache.kafka.clients.producer.internals.DefaultPartitioner;&lt;br/&gt;
 import org.apache.kafka.common.Cluster;&lt;br/&gt;
+import org.apache.kafka.common.KafkaException;&lt;br/&gt;
 import org.apache.kafka.common.Node;&lt;br/&gt;
 import org.apache.kafka.common.PartitionInfo;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
@@ -267,18 +268,7 @@ public void shouldThrowOnSendIfProducerGotFenced() {&lt;br/&gt;
         try 
{
             producer.send(null);
             fail(&quot;Should have thrown as producer is fenced off&quot;);
-        }
&lt;p&gt; catch (ProducerFencedException e) { }&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldThrowOnFlushIfProducerGotFenced() {&lt;/li&gt;
	&lt;li&gt;buildMockProducer(true);&lt;/li&gt;
	&lt;li&gt;producer.initTransactions();&lt;/li&gt;
	&lt;li&gt;producer.fenceProducer();&lt;/li&gt;
	&lt;li&gt;try 
{
-            producer.flush();
-            fail(&quot;Should have thrown as producer is fenced off&quot;);
-        }
&lt;p&gt; catch (ProducerFencedException e) { }&lt;br/&gt;
+        } catch (KafkaException e) { }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index 94e4c71d9c2..8eb024cc670 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -51,9 +51,11 @@&lt;br/&gt;
     final boolean eosEnabled;&lt;br/&gt;
     final Logger log;&lt;br/&gt;
     final LogContext logContext;&lt;br/&gt;
+    final StateDirectory stateDirectory;&lt;br/&gt;
+&lt;br/&gt;
     boolean taskInitialized;&lt;br/&gt;
     boolean taskClosed;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final StateDirectory stateDirectory;&lt;br/&gt;
+    boolean commitNeeded;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     InternalProcessorContext processorContext;&lt;/p&gt;

&lt;p&gt;@@ -267,6 +269,10 @@ public boolean isClosed() &lt;/p&gt;
{
         return taskClosed;
     }

&lt;p&gt;+    public boolean commitNeeded() &lt;/p&gt;
{
+        return commitNeeded;
+    }
&lt;p&gt;+&lt;br/&gt;
     public boolean hasStateStores() &lt;/p&gt;
{
         return !topology.stateStores().isEmpty();
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
index f98e6356a22..844b30b59bc 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
@@ -23,14 +23,16 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
 import org.slf4j.Logger;&lt;/p&gt;

&lt;p&gt;+import java.util.Collections;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
 import java.util.Iterator;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
+import java.util.stream.Collectors;&lt;/p&gt;

&lt;p&gt; class AssignedStreamsTasks extends AssignedTasks&amp;lt;StreamTask&amp;gt; implements RestoringTasks {&lt;br/&gt;
     private final Logger log;&lt;br/&gt;
     private final TaskAction&amp;lt;StreamTask&amp;gt; maybeCommitAction;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private int committed = 0;&lt;br/&gt;
+    private Map&amp;lt;TaskId, StreamTask&amp;gt; processable = Collections.emptyMap();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     AssignedStreamsTasks(final LogContext logContext) {&lt;br/&gt;
         super(logContext, &quot;stream task&quot;);&lt;br/&gt;
@@ -45,7 +47,7 @@ public String name() {&lt;/p&gt;

&lt;p&gt;             @Override&lt;br/&gt;
             public void apply(final StreamTask task) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (task.commitNeeded()) {&lt;br/&gt;
+                if (task.commitRequested() &amp;amp;&amp;amp; task.commitNeeded()) {&lt;br/&gt;
                     committed++;&lt;br/&gt;
                     task.commit();&lt;br/&gt;
                     log.debug(&quot;Committed active task {} per user request in&quot;, task.id());&lt;br/&gt;
@@ -82,14 +84,23 @@ int maybeCommit() 
{
         return recordsToDelete;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    /**&lt;br/&gt;
+     * Update the list of processable tasks&lt;br/&gt;
+     */&lt;br/&gt;
+    void update() &lt;/p&gt;
{
+        processable = running.entrySet().stream()
+                .filter(entry -&amp;gt; entry.getValue().isProcessable())
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+    }
&lt;p&gt;+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if the task producer got fenced (EOS only)&lt;br/&gt;
      */&lt;br/&gt;
     int process() {&lt;br/&gt;
         int processed = 0;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Iterator&amp;lt;Map.Entry&amp;lt;TaskId, StreamTask&amp;gt;&amp;gt; it = running.entrySet().iterator();&lt;/li&gt;
	&lt;li&gt;while (it.hasNext()) {&lt;/li&gt;
	&lt;li&gt;final StreamTask task = it.next().getValue();&lt;br/&gt;
+&lt;br/&gt;
+        for (Iterator&amp;lt;StreamTask&amp;gt; iter = processable.values().iterator(); iter.hasNext(); ) {&lt;br/&gt;
+            StreamTask task = iter.next();&lt;br/&gt;
             try {&lt;br/&gt;
                 if (task.process()) {&lt;br/&gt;
                     processed++;&lt;br/&gt;
@@ -101,13 +112,18 @@ int process() {&lt;br/&gt;
                 if (fatalException != null) 
{
                     throw fatalException;
                 }&lt;/li&gt;
	&lt;li&gt;it.remove();&lt;br/&gt;
+                running.remove(task.id());&lt;br/&gt;
                 throw e;&lt;br/&gt;
             } catch (final RuntimeException e) {&lt;br/&gt;
                 log.error(&quot;Failed to process stream task {} due to the following error:&quot;, task.id(), e);&lt;br/&gt;
                 throw e;&lt;br/&gt;
             }&lt;br/&gt;
+&lt;br/&gt;
+            if (!task.allSourcePartitionsBuffered()) 
{
+                iter.remove();
+            }
&lt;p&gt;         }&lt;br/&gt;
+&lt;br/&gt;
         return processed;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
index 079d405cb50..cb4bde2e35a 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
@@ -45,6 +45,9 @@&lt;br/&gt;
     private final Map&amp;lt;TaskId, T&amp;gt; restoring = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Set&amp;lt;TopicPartition&amp;gt; restoredPartitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+    protected int committed = 0;&lt;br/&gt;
+&lt;br/&gt;
     // IQ may access this map.&lt;br/&gt;
     final Map&amp;lt;TaskId, T&amp;gt; running = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, T&amp;gt; runningByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -64,7 +67,10 @@ public String name() {&lt;/p&gt;

&lt;p&gt;             @Override&lt;br/&gt;
             public void apply(final T task) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;task.commit();&lt;br/&gt;
+                if (task.commitNeeded()) 
{
+                    committed++;
+                    task.commit();
+                }
&lt;p&gt;             }&lt;br/&gt;
         };&lt;br/&gt;
     }&lt;br/&gt;
@@ -349,8 +355,9 @@ void clear() {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;br/&gt;
     int commit() 
{
+        committed = 0;
         applyToRunningTasks(commitAction);
-        return running.size();
+        return committed;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     void applyToRunningTasks(final TaskAction&amp;lt;T&amp;gt; action) {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
index 34252bf2b4c..80608213941 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
@@ -32,10 +32,13 @@&lt;br/&gt;
 public class PartitionGroup {&lt;/p&gt;

&lt;p&gt;     private final Map&amp;lt;TopicPartition, RecordQueue&amp;gt; partitionQueues;&lt;br/&gt;
-&lt;br/&gt;
     private final PriorityQueue&amp;lt;RecordQueue&amp;gt; nonEmptyQueuesByTime;&lt;br/&gt;
+    private final int numQueues;&lt;br/&gt;
+&lt;br/&gt;
     private long streamTime;&lt;br/&gt;
     private int totalBuffered;&lt;br/&gt;
+    private boolean allBuffered;&lt;br/&gt;
+&lt;/p&gt;

&lt;p&gt;     public static class RecordInfo {&lt;br/&gt;
         RecordQueue queue;&lt;br/&gt;
@@ -53,12 +56,13 @@ RecordQueue queue() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     PartitionGroup(final Map&amp;lt;TopicPartition, RecordQueue&amp;gt; partitionQueues) &lt;/p&gt;
{
         nonEmptyQueuesByTime = new PriorityQueue&amp;lt;&amp;gt;(partitionQueues.size(), Comparator.comparingLong(RecordQueue::timestamp));
         this.partitionQueues = partitionQueues;
         totalBuffered = 0;
+        allBuffered = false;
         streamTime = RecordQueue.NOT_KNOWN;
+        numQueues = this.partitionQueues.keySet().size();
     }

&lt;p&gt;     /**&lt;br/&gt;
@@ -81,15 +85,14 @@ StampedRecord nextRecord(final RecordInfo info) {&lt;/p&gt;

&lt;p&gt;                 if (!queue.isEmpty()) &lt;/p&gt;
{
                     nonEmptyQueuesByTime.offer(queue);
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    // if a certain queue has been drained, reset the flag
+                    allBuffered = false;
                 }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Since this was previously a queue with min timestamp,&lt;/li&gt;
	&lt;li&gt;// streamTime could only advance if this queue&apos;s time did.&lt;/li&gt;
	&lt;li&gt;if (queue.timestamp() &amp;gt; streamTime) 
{
-                    computeStreamTime();
-                }
&lt;p&gt;+                // always update the stream time to the record&apos;s timestamp yet to be processed if it is larger&lt;br/&gt;
+                streamTime = record.timestamp &amp;gt; streamTime ? record.timestamp : streamTime;&lt;br/&gt;
             }&lt;br/&gt;
-&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return record;&lt;br/&gt;
@@ -106,17 +109,22 @@ int addRawRecords(final TopicPartition partition, final Iterable&amp;lt;ConsumerRecord&amp;lt;&lt;br/&gt;
         final RecordQueue recordQueue = partitionQueues.get(partition);&lt;/p&gt;

&lt;p&gt;         final int oldSize = recordQueue.size();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long oldTimestamp = recordQueue.timestamp();&lt;br/&gt;
         final int newSize = recordQueue.addRawRecords(rawRecords);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add this record queue to be considered for processing in the future if it was empty before&lt;br/&gt;
         if (oldSize == 0 &amp;amp;&amp;amp; newSize &amp;gt; 0) &lt;/p&gt;
{
             nonEmptyQueuesByTime.offer(recordQueue);
-        }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Adding to this queue could only advance streamTime if it was previously the queue with min timestamp (= streamTime)&lt;/li&gt;
	&lt;li&gt;if (oldTimestamp &amp;lt;= streamTime &amp;amp;&amp;amp; recordQueue.timestamp() &amp;gt; streamTime) {&lt;/li&gt;
	&lt;li&gt;computeStreamTime();&lt;br/&gt;
+            // if all partitions now are non-empty, set the flag and compute the stream time&lt;br/&gt;
+            if (nonEmptyQueuesByTime.size() == numQueues) 
{
+                allBuffered = true;
+
+                // since we may enforce processing even if some queue is empty, it is possible that after some
+                // raw data has been added to that queue the new partition&apos;s timestamp is even smaller than the current
+                // stream time, in this case we should not update.
+                final long newTimestamp = nonEmptyQueuesByTime.peek().timestamp();
+                streamTime = newTimestamp &amp;gt; streamTime ? newTimestamp : streamTime;
+            }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         totalBuffered += newSize - oldSize;&lt;br/&gt;
@@ -136,18 +144,6 @@ public long timestamp() &lt;/p&gt;
{
         return streamTime;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void computeStreamTime() {&lt;/li&gt;
	&lt;li&gt;// we should always return the smallest timestamp of all partitions&lt;/li&gt;
	&lt;li&gt;// to avoid group partition time goes backward&lt;/li&gt;
	&lt;li&gt;long timestamp = Long.MAX_VALUE;&lt;/li&gt;
	&lt;li&gt;for (final RecordQueue queue : partitionQueues.values()) {&lt;/li&gt;
	&lt;li&gt;if (queue.timestamp() &amp;lt; timestamp) 
{
-                timestamp = queue.timestamp();
-            }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;this.streamTime = timestamp;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@throws IllegalStateException if the record&apos;s partition does not belong to this partition group&lt;br/&gt;
      */&lt;br/&gt;
@@ -165,6 +161,10 @@ int numBuffered() 
{
         return totalBuffered;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    boolean allPartitionsBuffered() &lt;/p&gt;
{
+        return allBuffered;
+    }
&lt;p&gt;+&lt;br/&gt;
     public void close() &lt;/p&gt;
{
         partitionQueues.clear();
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java&lt;br/&gt;
index d753648eede..73a242e4cd8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java&lt;br/&gt;
@@ -207,18 +207,41 @@ public void onCompletion(final RecordMetadata metadata,&lt;br/&gt;
                 &quot;You can increase producer parameter `max.block.ms` to increase this timeout.&quot;, topic);&lt;br/&gt;
             throw new StreamsException(String.format(&quot;%sFailed to send record to topic %s due to timeout.&quot;, logPrefix, topic));&lt;br/&gt;
         } catch (final Exception uncaughtException) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;throw new StreamsException(&lt;/li&gt;
	&lt;li&gt;String.format(&lt;/li&gt;
	&lt;li&gt;EXCEPTION_MESSAGE,&lt;/li&gt;
	&lt;li&gt;logPrefix,&lt;/li&gt;
	&lt;li&gt;&quot;an error caught&quot;,&lt;/li&gt;
	&lt;li&gt;key,&lt;/li&gt;
	&lt;li&gt;value,&lt;/li&gt;
	&lt;li&gt;timestamp,&lt;/li&gt;
	&lt;li&gt;topic,&lt;/li&gt;
	&lt;li&gt;uncaughtException.toString()&lt;/li&gt;
	&lt;li&gt;),&lt;/li&gt;
	&lt;li&gt;uncaughtException);&lt;br/&gt;
+            if (uncaughtException instanceof KafkaException) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                final KafkaException kafkaException = (KafkaException) uncaughtException;++                if (kafkaException.getCause() instanceof ProducerFencedException) {
+                    // producer.send() call may throw a KafkaException which wraps a FencedException,
+                    // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException
+                    throw (ProducerFencedException) kafkaException.getCause();
+                } else {
+                    throw new StreamsException(
+                            String.format(
+                                    EXCEPTION_MESSAGE,
+                                    logPrefix,
+                                    &quot;an error caught&quot;,
+                                    key,
+                                    value,
+                                    timestamp,
+                                    topic,
+                                    uncaughtException.toString()
+                            ),
+                            uncaughtException);
+                }+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else &lt;/p&gt;
{
+                throw new StreamsException(
+                        String.format(
+                                EXCEPTION_MESSAGE,
+                                logPrefix,
+                                &quot;an error caught&quot;,
+                                key,
+                                value,
+                                timestamp,
+                                topic,
+                                uncaughtException.toString()
+                        ),
+                        uncaughtException);
+            }
&lt;p&gt;         }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java&lt;br/&gt;
index 72cc6295a2d..3ac64146187 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java&lt;br/&gt;
@@ -103,6 +103,8 @@ public void commit() &lt;/p&gt;
{
         flushAndCheckpointState();
         // reinitialize offset limits
         updateOffsetLimits();
+
+        commitNeeded = false;
     }

&lt;p&gt;     /**&lt;br/&gt;
@@ -185,6 +187,11 @@ public void closeSuspended(final boolean clean,&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         stateMgr.updateStandbyStates(partition, restoreRecords, lastOffset);&lt;br/&gt;
+&lt;br/&gt;
+        if (!restoreRecords.isEmpty()) &lt;/p&gt;
{
+            commitNeeded = true;
+        }
&lt;p&gt;+&lt;br/&gt;
         return remainingRecords;&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index 7f121fe0df4..648c997658e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -59,6 +59,8 @@&lt;/p&gt;

&lt;p&gt;     private static final ConsumerRecord&amp;lt;Object, Object&amp;gt; DUMMY_RECORD = new ConsumerRecord&amp;lt;&amp;gt;(ProcessorContextImpl.NONEXIST_TOPIC, -1, -1L, null, null);&lt;/p&gt;

&lt;p&gt;+    private static final int WAIT_ON_PARTIAL_INPUT = 5;&lt;br/&gt;
+&lt;br/&gt;
     private final PartitionGroup partitionGroup;&lt;br/&gt;
     private final PartitionGroup.RecordInfo recordInfo;&lt;br/&gt;
     private final PunctuationQueue streamTimePunctuationQueue;&lt;br/&gt;
@@ -69,15 +71,17 @@&lt;br/&gt;
     private final Producer&amp;lt;byte[], byte[]&amp;gt; producer;&lt;br/&gt;
     private final int maxBufferedSize;&lt;/p&gt;

&lt;p&gt;+&lt;br/&gt;
     private boolean commitRequested = false;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private boolean commitOffsetNeeded = false;&lt;br/&gt;
     private boolean transactionInFlight = false;&lt;br/&gt;
+    private int waits = WAIT_ON_PARTIAL_INPUT;&lt;br/&gt;
     private final Time time;&lt;br/&gt;
     private final TaskMetrics taskMetrics;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     protected static final class TaskMetrics &lt;/p&gt;
{
         final StreamsMetricsImpl metrics;
         final Sensor taskCommitTimeSensor;
+        final Sensor taskEnforcedProcessSensor;
         private final String taskName;
 
 
@@ -108,7 +112,7 @@
 
             // add the operation metrics with additional tags
             final Map&amp;lt;String, String&amp;gt; tagMap = metrics.tagMap(&quot;task-id&quot;, taskName);
-            taskCommitTimeSensor = metrics.taskLevelSensor(&quot;commit&quot;, taskName, Sensor.RecordingLevel.DEBUG, parent);
+            taskCommitTimeSensor = metrics.taskLevelSensor(taskName, &quot;commit&quot;, Sensor.RecordingLevel.DEBUG, parent);
             taskCommitTimeSensor.add(
                 new MetricName(&quot;commit-latency-avg&quot;, group, &quot;The average latency of commit operation.&quot;, tagMap),
                 new Avg()
@@ -125,6 +129,18 @@
                 new MetricName(&quot;commit-total&quot;, group, &quot;The total number of occurrence of commit operations.&quot;, tagMap),
                 new Count()
             );
+
+            // add the metrics for enforced processing
+            taskEnforcedProcessSensor = metrics.taskLevelSensor(taskName, &quot;enforced-process&quot;, Sensor.RecordingLevel.DEBUG, parent);
+            taskEnforcedProcessSensor.add(
+                    new MetricName(&quot;enforced-process-rate&quot;, group, &quot;The average number of occurrence of enforced-process per second.&quot;, tagMap),
+                    new Rate(TimeUnit.SECONDS, new Count())
+            );
+            taskEnforcedProcessSensor.add(
+                    new MetricName(&quot;enforced-process-total&quot;, group, &quot;The total number of occurrence of enforced-process operations.&quot;, tagMap),
+                    new Count()
+            );
+
         }

&lt;p&gt;         void removeAllSensors() {&lt;br/&gt;
@@ -263,6 +279,25 @@ public void resume() &lt;/p&gt;
{
         log.debug(&quot;Resuming&quot;);
     }

&lt;p&gt;+    /**&lt;br/&gt;
+     * An active task is processable if its buffer contains data for all of its input source topic partitions&lt;br/&gt;
+     */&lt;br/&gt;
+    public boolean isProcessable() {&lt;br/&gt;
+        if (partitionGroup.allPartitionsBuffered()) &lt;/p&gt;
{
+            return true;
+        }
&lt;p&gt; else if (partitionGroup.numBuffered() &amp;gt; 0 &amp;amp;&amp;amp; --waits &amp;lt; 0) &lt;/p&gt;
{
+            taskMetrics.taskEnforcedProcessSensor.record();
+            waits = WAIT_ON_PARTIAL_INPUT;
+            return true;
+        }
&lt;p&gt; else &lt;/p&gt;
{
+            return false;
+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    public boolean allSourcePartitionsBuffered() {
+        return partitionGroup.allPartitionsBuffered();
+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;br/&gt;
      * Process one record.&lt;br/&gt;
      *&lt;br/&gt;
@@ -293,7 +328,7 @@ public boolean process() {&lt;br/&gt;
 &lt;br/&gt;
             // update the consumed offset map after processing is done&lt;br/&gt;
             consumedOffsets.put(partition, record.offset());&lt;br/&gt;
-            commitOffsetNeeded = true;&lt;br/&gt;
+            commitNeeded = true;&lt;br/&gt;
 &lt;br/&gt;
             // after processing this record, if its partition queue&apos;s buffered size has been&lt;br/&gt;
             // decreased to the threshold, we can then resume the consumption on this partition&lt;br/&gt;
@@ -385,12 +420,33 @@ void commit(final boolean startNewTransaction) {
             stateMgr.checkpoint(activeTaskCheckpointableOffsets());
         }&lt;br/&gt;
 &lt;br/&gt;
-        commitOffsets(startNewTransaction);&lt;br/&gt;
+        final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; consumedOffsetsAndMetadata = new HashMap&amp;lt;&amp;gt;(consumedOffsets.size());&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) {
+            final TopicPartition partition = entry.getKey();
+            final long offset = entry.getValue() + 1;
+            consumedOffsetsAndMetadata.put(partition, new OffsetAndMetadata(offset));
+            stateMgr.putOffsetLimit(partition, offset);
+        }&lt;br/&gt;
 &lt;br/&gt;
-        commitRequested = false;&lt;br/&gt;
+        try {&lt;br/&gt;
+            if (eosEnabled) {&lt;br/&gt;
+                producer.sendOffsetsToTransaction(consumedOffsetsAndMetadata, applicationId);&lt;br/&gt;
+                producer.commitTransaction();&lt;br/&gt;
+                transactionInFlight = false;&lt;br/&gt;
+                if (startNewTransaction) {
+                    producer.beginTransaction();
+                    transactionInFlight = true;
+                }&lt;br/&gt;
+            } else {
+                consumer.commitSync(consumedOffsetsAndMetadata);
+            }&lt;br/&gt;
+        } catch (final CommitFailedException | ProducerFencedException fatal) {
+            throw new TaskMigratedException(this, fatal);
+        }&lt;br/&gt;
 &lt;br/&gt;
-        taskMetrics.taskCommitTimeSensor.record(time.nanoseconds() - startNs);&lt;br/&gt;
-    }&lt;br/&gt;
+        commitNeeded = false;&lt;br/&gt;
+        commitRequested = false;&lt;br/&gt;
+        taskMetrics.taskCommitTimeSensor.record(time.nanoseconds() - startNs);    }&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     protected Map&amp;lt;TopicPartition, Long&amp;gt; activeTaskCheckpointableOffsets() {&lt;br/&gt;
@@ -413,43 +469,6 @@ protected void flushState() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    /**&lt;br/&gt;
-     * @throws TaskMigratedException if committing offsets failed (non-EOS)&lt;br/&gt;
-     *                               or if the task producer got fenced (EOS)&lt;br/&gt;
-     */&lt;br/&gt;
-    private void commitOffsets(final boolean startNewTransaction) {&lt;br/&gt;
-        try {&lt;br/&gt;
-            if (commitOffsetNeeded) {&lt;br/&gt;
-                log.trace(&quot;Committing offsets&quot;);&lt;br/&gt;
-                final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; consumedOffsetsAndMetadata = new HashMap&amp;lt;&amp;gt;(consumedOffsets.size());&lt;br/&gt;
-                for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) {
-                    final TopicPartition partition = entry.getKey();
-                    final long offset = entry.getValue() + 1;
-                    consumedOffsetsAndMetadata.put(partition, new OffsetAndMetadata(offset));
-                    stateMgr.putOffsetLimit(partition, offset);
-                }&lt;br/&gt;
-&lt;br/&gt;
-                if (eosEnabled) {
-                    producer.sendOffsetsToTransaction(consumedOffsetsAndMetadata, applicationId);
-                } else {
-                    consumer.commitSync(consumedOffsetsAndMetadata);
-                }&lt;br/&gt;
-                commitOffsetNeeded = false;&lt;br/&gt;
-            }&lt;br/&gt;
-&lt;br/&gt;
-            if (eosEnabled) {&lt;br/&gt;
-                producer.commitTransaction();&lt;br/&gt;
-                transactionInFlight = false;&lt;br/&gt;
-                if (startNewTransaction) {
-                    producer.beginTransaction();
-                    transactionInFlight = true;
-                }&lt;br/&gt;
-            }&lt;br/&gt;
-        } catch (final CommitFailedException | ProducerFencedException fatal) {
-            throw new TaskMigratedException(this, fatal);
-        }&lt;br/&gt;
-    }&lt;br/&gt;
-&lt;br/&gt;
     Map&amp;lt;TopicPartition, Long&amp;gt; purgableOffsets() {&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; purgableConsumedOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) {&lt;br/&gt;
@@ -719,7 +738,14 @@ public boolean maybePunctuateStreamTime() {&lt;br/&gt;
         if (timestamp == RecordQueue.NOT_KNOWN) {
             return false;
         } else {&lt;br/&gt;
-            return streamTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.STREAM_TIME, this);&lt;br/&gt;
+            final boolean punctuated = streamTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.STREAM_TIME, this);&lt;br/&gt;
+&lt;br/&gt;
+            if (punctuated) {
+                commitNeeded = true;
+                return true;
+            } else {
+                return false;
+            }&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -733,7 +759,14 @@ public boolean maybePunctuateStreamTime() {&lt;br/&gt;
     public boolean maybePunctuateSystemTime() {&lt;br/&gt;
         final long timestamp = time.milliseconds();&lt;br/&gt;
 &lt;br/&gt;
-        return systemTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.WALL_CLOCK_TIME, this);&lt;br/&gt;
+        final boolean punctuated = systemTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.WALL_CLOCK_TIME, this);&lt;br/&gt;
+&lt;br/&gt;
+        if (punctuated) {
+            commitNeeded = true;
+            return true;
+        } else {+            return false;+        }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;     /**&lt;br/&gt;
@@ -746,7 +779,7 @@ void needCommit() {&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Whether or not a request has been made to commit the current state&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;boolean commitNeeded() {&lt;br/&gt;
+    boolean commitRequested() 
{
         return commitRequested;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index 31de839b1a1..8f89ec04734 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -36,6 +36,7 @@&lt;br/&gt;
 import org.apache.kafka.common.metrics.stats.Max;&lt;br/&gt;
 import org.apache.kafka.common.metrics.stats.Rate;&lt;br/&gt;
 import org.apache.kafka.common.metrics.stats.Total;&lt;br/&gt;
+import org.apache.kafka.common.serialization.ByteArrayDeserializer;&lt;br/&gt;
 import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
 import org.apache.kafka.common.utils.Time;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaClientSupplier;&lt;br/&gt;
@@ -69,7 +70,6 @@&lt;/p&gt;

&lt;p&gt; public class StreamThread extends Thread &lt;/p&gt;
{
 
-    private final static int UNLIMITED_RECORDS = -1;
     private final static AtomicInteger STREAM_THREAD_ID_SEQUENCE = new AtomicInteger(1);
 
     /**
@@ -561,18 +561,21 @@ StandbyTask createTask(final Consumer&amp;lt;byte[], byte[]&amp;gt; consumer,
     }

&lt;p&gt;     private final Time time;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Duration pollTime;&lt;/li&gt;
	&lt;li&gt;private final long commitTimeMs;&lt;/li&gt;
	&lt;li&gt;private final Object stateLock;&lt;br/&gt;
     private final Logger log;&lt;br/&gt;
     private final String logPrefix;&lt;br/&gt;
+    private final Object stateLock;&lt;br/&gt;
+    private final Duration pollTime;&lt;br/&gt;
+    private final long commitTimeMs;&lt;br/&gt;
+    private final int maxPollTimeMs;&lt;br/&gt;
+    private final String originalReset;&lt;br/&gt;
     private final TaskManager taskManager;&lt;br/&gt;
     private final StreamsMetricsThreadImpl streamsMetrics;&lt;br/&gt;
     private final AtomicInteger assignmentErrorCode;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private long now;&lt;br/&gt;
+    private long lastPollMs;&lt;br/&gt;
     private long lastCommitMs;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long timerStartedMs;&lt;/li&gt;
	&lt;li&gt;private final String originalReset;&lt;br/&gt;
+    private int numIterations;&lt;br/&gt;
     private Throwable rebalanceException = null;&lt;br/&gt;
     private boolean processStandbyRecords = false;&lt;br/&gt;
     private volatile State state = State.CREATED;&lt;br/&gt;
@@ -718,11 +721,21 @@ public StreamThread(final Time time,&lt;br/&gt;
         this.assignmentErrorCode = assignmentErrorCode;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         this.pollTime = Duration.ofMillis(config.getLong(StreamsConfig.POLL_MS_CONFIG));&lt;br/&gt;
+        this.maxPollTimeMs = new InternalConsumerConfig(config.getMainConsumerConfigs(&quot;dummyGroupId&quot;, &quot;dummyClientId&quot;))&lt;br/&gt;
+                .getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG);&lt;br/&gt;
         this.commitTimeMs = config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG);&lt;/p&gt;

&lt;p&gt;+        this.numIterations = 1;&lt;br/&gt;
+&lt;br/&gt;
         updateThreadMetadata(Collections.emptyMap(), Collections.emptyMap());&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    private static final class InternalConsumerConfig extends ConsumerConfig {&lt;br/&gt;
+        private InternalConsumerConfig(final Map&amp;lt;String, Object&amp;gt; props) &lt;/p&gt;
{
+            super(ConsumerConfig.addDeserializerToConfig(props, new ByteArrayDeserializer(), new ByteArrayDeserializer()), false);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Execute the stream processors&lt;br/&gt;
      *&lt;br/&gt;
@@ -764,12 +777,11 @@ private void setRebalanceException(final Throwable rebalanceException) {&lt;/li&gt;
	&lt;li&gt;@throws StreamsException      if the store&apos;s change log does not contain the partition&lt;br/&gt;
      */&lt;br/&gt;
     private void runLoop() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long recordsProcessedBeforeCommit = UNLIMITED_RECORDS;&lt;br/&gt;
         consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         while (isRunning()) {&lt;br/&gt;
             try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;recordsProcessedBeforeCommit = runOnce(recordsProcessedBeforeCommit);&lt;br/&gt;
+                runOnce();&lt;br/&gt;
                 if (assignmentErrorCode.get() == StreamsPartitionAssignor.Error.VERSION_PROBING.code()) {&lt;br/&gt;
                     log.info(&quot;Version probing detected. Triggering new rebalance.&quot;);&lt;br/&gt;
                     enforceRebalance();&lt;br/&gt;
@@ -798,12 +810,10 @@ private void enforceRebalance() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;br/&gt;
     // Visible for testing&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long runOnce(final long recordsProcessedBeforeCommit) {&lt;/li&gt;
	&lt;li&gt;long processedBeforeCommit = recordsProcessedBeforeCommit;&lt;br/&gt;
-&lt;br/&gt;
+    void runOnce() {&lt;br/&gt;
         final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;timerStartedMs = time.milliseconds();&lt;br/&gt;
+        now = time.milliseconds();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (state == State.PARTITIONS_ASSIGNED) {&lt;br/&gt;
             // try to fetch some records with zero poll millis&lt;br/&gt;
@@ -831,25 +841,45 @@ long runOnce(final long recordsProcessedBeforeCommit) {&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (records != null &amp;amp;&amp;amp; !records.isEmpty() &amp;amp;&amp;amp; taskManager.hasActiveRunningTasks()) {&lt;/li&gt;
	&lt;li&gt;streamsMetrics.pollTimeSensor.record(computeLatency(), timerStartedMs);&lt;br/&gt;
+        if (records != null &amp;amp;&amp;amp; !records.isEmpty()) {&lt;br/&gt;
+            streamsMetrics.pollTimeSensor.record(computeLatency(), now);&lt;br/&gt;
             addRecordsToTasks(records);&lt;/li&gt;
	&lt;li&gt;final long totalProcessed = processAndMaybeCommit(recordsProcessedBeforeCommit);&lt;/li&gt;
	&lt;li&gt;if (totalProcessed &amp;gt; 0) 
{
-                final long processLatency = computeLatency();
-                streamsMetrics.processTimeSensor.record(processLatency / (double) totalProcessed, timerStartedMs);
-                processedBeforeCommit = adjustRecordsProcessedBeforeCommit(
-                    recordsProcessedBeforeCommit,
-                    totalProcessed,
-                    processLatency,
-                    commitTimeMs);
-            }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;punctuate();&lt;/li&gt;
	&lt;li&gt;maybeCommit(timerStartedMs);&lt;/li&gt;
	&lt;li&gt;maybeUpdateStandbyTasks(timerStartedMs);&lt;/li&gt;
	&lt;li&gt;return processedBeforeCommit;&lt;br/&gt;
+        if (state == State.RUNNING) {&lt;br/&gt;
+            taskManager.updateProcessableTasks();&lt;br/&gt;
+&lt;br/&gt;
+            /*&lt;br/&gt;
+             * Within an iteration, after N (N initialized as 1 upon start up) round of processing one-record-each on the applicable tasks, check the current time:&lt;br/&gt;
+             *  1. If it is time to commit, do it;&lt;br/&gt;
+             *  2. If it is time to punctuate, do it;&lt;br/&gt;
+             *  3. If elapsed time is close to consumer&apos;s max.poll.interval.ms, end the current iteration immediately.&lt;br/&gt;
+             *  4. If none of the the above happens, increment N.&lt;br/&gt;
+             *  5. If one of the above happens, half the value of N.&lt;br/&gt;
+             */&lt;br/&gt;
+            long totalProcessed;&lt;br/&gt;
+            long timeSinceLastPoll;&lt;br/&gt;
+&lt;br/&gt;
+            do 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                totalProcessed = processAndMaybeCommit();+                timeSinceLastPoll = Math.max(now - lastPollMs, 0);++                if (timeSinceLastPoll / 2 &amp;gt;= maxPollTimeMs) {
+                    break;
+                } else if (maybePunctuate() || maybeCommit()) {
+                    numIterations = numIterations &amp;gt; 1 ? numIterations / 2 : numIterations;
+                } else {
+                    numIterations++;
+                }+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; while (totalProcessed &amp;gt; 0 &amp;amp;&amp;amp; timeSinceLastPoll &amp;lt; maxPollTimeMs);&lt;br/&gt;
+&lt;br/&gt;
+            // even if there is not data to process in this iteration, still need to check if commit / punctuate is needed&lt;br/&gt;
+            maybePunctuate();&lt;br/&gt;
+&lt;br/&gt;
+            maybeCommit();&lt;br/&gt;
+&lt;br/&gt;
+            maybeUpdateStandbyTasks();&lt;br/&gt;
+        }&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -862,6 +892,8 @@ long runOnce(final long recordsProcessedBeforeCommit) {&lt;br/&gt;
     private ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; pollRequests(final Duration pollTime) {&lt;br/&gt;
         ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = null;&lt;/p&gt;

&lt;p&gt;+        this.lastPollMs = now;&lt;br/&gt;
+&lt;br/&gt;
         try &lt;/p&gt;
{
             records = consumer.poll(pollTime);
         }
&lt;p&gt; catch (final InvalidOffsetException e) {&lt;br/&gt;
@@ -946,82 +978,48 @@ private void addRecordsToTasks(final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records) {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Schedule the records processing by selecting which record is processed next. Commits may&lt;/li&gt;
	&lt;li&gt;happen as records are processed.&lt;br/&gt;
      *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @param recordsProcessedBeforeCommit number of records to be processed before commit is called.&lt;/li&gt;
	&lt;li&gt;*                                     if UNLIMITED_RECORDS, then commit is never called&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return Number of records processed since last commit.&lt;/li&gt;
	&lt;li&gt;@throws TaskMigratedException if committing offsets failed (non-EOS)&lt;/li&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long processAndMaybeCommit(final long recordsProcessedBeforeCommit) {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;long processed;&lt;/li&gt;
	&lt;li&gt;long totalProcessedSinceLastMaybeCommit = 0;&lt;/li&gt;
	&lt;li&gt;// Round-robin scheduling by taking one record from each task repeatedly&lt;/li&gt;
	&lt;li&gt;// until no task has any records left&lt;/li&gt;
	&lt;li&gt;do {&lt;/li&gt;
	&lt;li&gt;processed = taskManager.process();&lt;br/&gt;
+    private long processAndMaybeCommit() {&lt;br/&gt;
+        long totalProcessed = 0;&lt;br/&gt;
+&lt;br/&gt;
+        for (int i = 0; i &amp;lt; numIterations; i++) {&lt;br/&gt;
+            int processed = taskManager.process();&lt;br/&gt;
+&lt;br/&gt;
             if (processed &amp;gt; 0) 
{
-                streamsMetrics.processTimeSensor.record(computeLatency() / (double) processed, timerStartedMs);
-            }&lt;/li&gt;
	&lt;li&gt;totalProcessedSinceLastMaybeCommit += processed;&lt;br/&gt;
+                totalProcessed += processed;&lt;br/&gt;
+                streamsMetrics.processTimeSensor.record(computeLatency() / (double) processed, now);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;punctuate();&lt;br/&gt;
+                taskManager.updateProcessableTasks();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (recordsProcessedBeforeCommit != UNLIMITED_RECORDS &amp;amp;&amp;amp;&lt;/li&gt;
	&lt;li&gt;totalProcessedSinceLastMaybeCommit &amp;gt;= recordsProcessedBeforeCommit) 
{
-                totalProcessedSinceLastMaybeCommit = 0;
-                maybeCommit(timerStartedMs);
-            }&lt;/li&gt;
	&lt;li&gt;// commit any tasks that have requested a commit&lt;/li&gt;
	&lt;li&gt;final int committed = taskManager.maybeCommitActiveTasks();&lt;/li&gt;
	&lt;li&gt;if (committed &amp;gt; 0) {&lt;/li&gt;
	&lt;li&gt;streamsMetrics.commitTimeSensor.record(computeLatency() / (double) committed, timerStartedMs);&lt;br/&gt;
+                // commit any tasks that have requested a commit&lt;br/&gt;
+                final int committed = taskManager.maybeCommitActiveTasks();&lt;br/&gt;
+                if (committed &amp;gt; 0) 
{
+                    streamsMetrics.commitTimeSensor.record(computeLatency() / (double) committed, now);
+                }
&lt;p&gt;+            } else &lt;/p&gt;
{
+                // if there is no records to be processed, exit immediately
+                break;
             }&lt;/li&gt;
	&lt;li&gt;} while (processed != 0);&lt;br/&gt;
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return totalProcessedSinceLastMaybeCommit;&lt;br/&gt;
+        return totalProcessed;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if the task producer got fenced (EOS only)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void punctuate() {&lt;br/&gt;
+    private boolean maybePunctuate() {&lt;br/&gt;
         final int punctuated = taskManager.punctuate();&lt;br/&gt;
         if (punctuated &amp;gt; 0) 
{
-            streamsMetrics.punctuateTimeSensor.record(computeLatency() / (double) punctuated, timerStartedMs);
-        }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
+            streamsMetrics.punctuateTimeSensor.record(computeLatency() / (double) punctuated, now);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Adjust the number of records that should be processed by scheduler. This avoids&lt;/li&gt;
	&lt;li&gt;* scenarios where the processing time is higher than the commit time.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param prevRecordsProcessedBeforeCommit Previous number of records processed by scheduler.&lt;/li&gt;
	&lt;li&gt;* @param totalProcessed                   Total number of records processed in this last round.&lt;/li&gt;
	&lt;li&gt;* @param processLatency                   Total processing latency in ms processed in this last round.&lt;/li&gt;
	&lt;li&gt;* @param commitTime                       Desired commit time in ms.&lt;/li&gt;
	&lt;li&gt;* @return An adjusted number of records to be processed in the next round.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private long adjustRecordsProcessedBeforeCommit(final long prevRecordsProcessedBeforeCommit, final long totalProcessed,&lt;/li&gt;
	&lt;li&gt;final long processLatency, final long commitTime) {&lt;/li&gt;
	&lt;li&gt;long recordsProcessedBeforeCommit = UNLIMITED_RECORDS;&lt;/li&gt;
	&lt;li&gt;// check if process latency larger than commit latency&lt;/li&gt;
	&lt;li&gt;// note that once we set recordsProcessedBeforeCommit, it will never be UNLIMITED_RECORDS again, so&lt;/li&gt;
	&lt;li&gt;// we will never process all records again. This might be an issue if the initial measurement&lt;/li&gt;
	&lt;li&gt;// was off due to a slow start.&lt;/li&gt;
	&lt;li&gt;if (processLatency &amp;gt; 0 &amp;amp;&amp;amp; processLatency &amp;gt; commitTime) {&lt;/li&gt;
	&lt;li&gt;// push down&lt;/li&gt;
	&lt;li&gt;recordsProcessedBeforeCommit = Math.max(1, (commitTime * totalProcessed) / processLatency);&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;processing latency {} &amp;gt; commit time {} for {} records. Adjusting down recordsProcessedBeforeCommit={}&quot;,&lt;/li&gt;
	&lt;li&gt;processLatency, commitTime, totalProcessed, recordsProcessedBeforeCommit);&lt;/li&gt;
	&lt;li&gt;} else if (prevRecordsProcessedBeforeCommit != UNLIMITED_RECORDS &amp;amp;&amp;amp; processLatency &amp;gt; 0) {&lt;/li&gt;
	&lt;li&gt;// push up&lt;/li&gt;
	&lt;li&gt;recordsProcessedBeforeCommit = Math.max(1, (commitTime * totalProcessed) / processLatency);&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;processing latency {} &amp;lt; commit time {} for {} records. Adjusting up recordsProcessedBeforeCommit={}&quot;,&lt;/li&gt;
	&lt;li&gt;processLatency, commitTime, totalProcessed, recordsProcessedBeforeCommit);&lt;br/&gt;
+            return true;&lt;br/&gt;
+        } else 
{
+            return false;
         }&lt;br/&gt;
-&lt;br/&gt;
-        return recordsProcessedBeforeCommit;&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
@@ -1030,32 +1028,44 @@ private long adjustRecordsProcessedBeforeCommit(final long prevRecordsProcessedB&lt;br/&gt;
      * @throws TaskMigratedException if committing offsets failed (non-EOS)&lt;br/&gt;
      *                               or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;br/&gt;
-    void maybeCommit(final long now) {&lt;br/&gt;
+    boolean maybeCommit() {&lt;br/&gt;
+        int committed = taskManager.maybeCommitActiveTasks();&lt;br/&gt;
+        if (committed &amp;gt; 0) {
+            streamsMetrics.commitTimeSensor.record(computeLatency() / (double) committed, now);
+        }&lt;br/&gt;
+&lt;br/&gt;
         if (commitTimeMs &amp;gt;= 0 &amp;amp;&amp;amp; lastCommitMs + commitTimeMs &amp;lt; now) {&lt;br/&gt;
             if (log.isTraceEnabled()) {&lt;br/&gt;
                 log.trace(&quot;Committing all active tasks {} and standby tasks {} since {}ms has elapsed (commit interval is {}ms)&quot;,&lt;br/&gt;
-                    taskManager.activeTaskIds(), taskManager.standbyTaskIds(), now - lastCommitMs, commitTimeMs);&lt;br/&gt;
+                        taskManager.activeTaskIds(), taskManager.standbyTaskIds(), now - lastCommitMs, commitTimeMs);&lt;br/&gt;
             }&lt;br/&gt;
 &lt;br/&gt;
-            final int committed = taskManager.commitAll();&lt;br/&gt;
+            committed = taskManager.commitAll();&lt;br/&gt;
             if (committed &amp;gt; 0) {
-                streamsMetrics.commitTimeSensor.record(computeLatency() / (double) committed, timerStartedMs);
+                final long previous = now;
+
+                streamsMetrics.commitTimeSensor.record(computeLatency() / (double) committed, now);
 
                 // try to purge the committed records for repartition topics if possible
                 taskManager.maybePurgeCommitedRecords();
-            }&lt;br/&gt;
-            if (log.isDebugEnabled()) {&lt;br/&gt;
-                log.debug(&quot;Committed all active tasks {} and standby tasks {} in {}ms&quot;,&lt;br/&gt;
-                    taskManager.activeTaskIds(), taskManager.standbyTaskIds(), timerStartedMs - now);&lt;br/&gt;
+&lt;br/&gt;
+                if (log.isDebugEnabled()) {&lt;br/&gt;
+                    log.debug(&quot;Committed all active tasks {} and standby tasks {} in {}ms&quot;,&lt;br/&gt;
+                            taskManager.activeTaskIds(), taskManager.standbyTaskIds(), now - previous);&lt;br/&gt;
+                }&lt;br/&gt;
             }&lt;br/&gt;
 &lt;br/&gt;
             lastCommitMs = now;&lt;br/&gt;
 &lt;br/&gt;
             processStandbyRecords = true;&lt;br/&gt;
+&lt;br/&gt;
+            return committed &amp;gt; 0;&lt;br/&gt;
+        } else {+            return false;         }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void maybeUpdateStandbyTasks(final long now) {&lt;br/&gt;
+    private void maybeUpdateStandbyTasks() {&lt;br/&gt;
         if (state == State.RUNNING &amp;amp;&amp;amp; taskManager.hasStandbyRunningTasks()) {&lt;br/&gt;
             if (processStandbyRecords) {&lt;br/&gt;
                 if (!standbyRecords.isEmpty()) {&lt;br/&gt;
@@ -1084,7 +1094,9 @@ private void maybeUpdateStandbyTasks(final long now) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     standbyRecords = remainingStandbyRecords;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.debug(&quot;Updated standby tasks {} in {}ms&quot;, taskManager.standbyTaskIds(), time.milliseconds() - now);&lt;br/&gt;
+                    if (log.isDebugEnabled()) {&lt;br/&gt;
+                        log.debug(&quot;Updated standby tasks {} in {}ms&quot;, taskManager.standbyTaskIds(), time.milliseconds() - now);&lt;br/&gt;
+                    }&lt;br/&gt;
                 }&lt;br/&gt;
                 processStandbyRecords = false;&lt;br/&gt;
             }&lt;br/&gt;
@@ -1144,10 +1156,10 @@ private void maybeUpdateStandbyTasks(final long now) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return latency&lt;br/&gt;
      */&lt;br/&gt;
     private long computeLatency() 
{
-        final long previousTimeMs = timerStartedMs;
-        timerStartedMs = time.milliseconds();
+        final long previous = now;
+        now = time.milliseconds();
 
-        return Math.max(timerStartedMs - previousTimeMs, 0);
+        return Math.max(now - previous, 0);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -1246,6 +1258,10 @@ public String toString(final String indent) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     // the following are for testing only&lt;br/&gt;
+    void setNow(final long now) &lt;/p&gt;
{
+        this.now = now;
+    }
&lt;p&gt;+&lt;br/&gt;
     TaskManager taskManager() &lt;/p&gt;
{
         return taskManager;
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java&lt;br/&gt;
index 5f221e3dc02..59bcd7869f8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java&lt;br/&gt;
@@ -34,6 +34,8 @@&lt;br/&gt;
      */&lt;br/&gt;
     boolean initializeStateStores();&lt;/p&gt;

&lt;p&gt;+    boolean commitNeeded();&lt;br/&gt;
+&lt;br/&gt;
     void initializeTopology();&lt;/p&gt;

&lt;p&gt;     void commit();&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
index 9da27020c5f..9571ae190b7 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
@@ -278,6 +278,10 @@ void shutdown(final boolean clean) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    void updateProcessableTasks() &lt;/p&gt;
{
+        active.update();
+    }
&lt;p&gt;+&lt;br/&gt;
     AdminClient getAdminClient() &lt;/p&gt;
{
         return adminClient;
     }
&lt;p&gt;@@ -332,10 +336,6 @@ boolean updateNewAndRestoringTasks() &lt;/p&gt;
{
         return false;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;boolean hasActiveRunningTasks() 
{
-        return active.hasRunningTasks();
-    }
&lt;p&gt;-&lt;br/&gt;
     boolean hasStandbyRunningTasks() &lt;/p&gt;
{
         return standby.hasRunningTasks();
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java&lt;br/&gt;
index 662ded553ad..e99a5b3bb23 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java&lt;br/&gt;
@@ -88,13 +88,13 @@ public final void removeAllThreadLevelSensors() {&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public final Sensor taskLevelSensor(final String taskName,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final String sensorName,&lt;/li&gt;
	&lt;li&gt;final Sensor.RecordingLevel recordingLevel,&lt;/li&gt;
	&lt;li&gt;final Sensor... parents) {&lt;br/&gt;
+                                        final String sensorName,&lt;br/&gt;
+                                        final Sensor.RecordingLevel recordingLevel,&lt;br/&gt;
+                                        final Sensor... parents) {&lt;br/&gt;
         final String key = threadName + &quot;.&quot; + taskName;&lt;br/&gt;
         synchronized (taskLevelSensors) {&lt;br/&gt;
             if (!taskLevelSensors.containsKey(key)) 
{
-                taskLevelSensors.put(key, new LinkedList&amp;lt;String&amp;gt;());
+                taskLevelSensors.put(key, new LinkedList&amp;lt;&amp;gt;());
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final String fullSensorName = key + &quot;.&quot; + sensorName;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java&lt;br/&gt;
index 77b9c1e8560..12b4cf30240 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java&lt;br/&gt;
@@ -370,7 +370,7 @@ private NamedCacheMetrics(final StreamsMetricsImpl metrics, final String cacheNa&lt;br/&gt;
                 &quot;record-cache-id&quot;, &quot;all&quot;,&lt;br/&gt;
                 &quot;task-id&quot;, taskName&lt;br/&gt;
             );&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Sensor taskLevelHitRatioSensor = metrics.taskLevelSensor(&quot;hitRatio&quot;, taskName, Sensor.RecordingLevel.DEBUG);&lt;br/&gt;
+            final Sensor taskLevelHitRatioSensor = metrics.taskLevelSensor(taskName, &quot;hitRatio&quot;, Sensor.RecordingLevel.DEBUG);&lt;br/&gt;
             taskLevelHitRatioSensor.add(&lt;br/&gt;
                 new MetricName(&quot;hitRatio-avg&quot;, group, &quot;The average cache hit ratio.&quot;, allMetricTags),&lt;br/&gt;
                 new Avg()&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index 5681d7c045f..7f6db04ec8d 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -24,14 +24,13 @@&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
 import org.apache.kafka.common.serialization.StringDeserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.StringSerializer;&lt;br/&gt;
-import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
-import org.apache.kafka.streams.kstream.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
 import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;&lt;br/&gt;
 import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KGroupedStream;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KeyValueMapper;&lt;br/&gt;
@@ -40,9 +39,6 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Reducer;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Serialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.TimeWindows;&lt;br/&gt;
-import org.apache.kafka.streams.kstream.Windowed;&lt;br/&gt;
-import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
-import org.apache.kafka.streams.state.WindowStore;&lt;br/&gt;
 import org.apache.kafka.test.IntegrationTest;&lt;br/&gt;
 import org.apache.kafka.test.MockMapper;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
@@ -54,14 +50,9 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.io.IOException;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
-import java.util.Collections;&lt;br/&gt;
-import java.util.Comparator;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Properties;&lt;/p&gt;

&lt;p&gt;-import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
-import static org.hamcrest.core.Is.is;&lt;br/&gt;
-&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Similar to KStreamAggregationIntegrationTest but with dedupping enabled&lt;/li&gt;
	&lt;li&gt;by virtue of having a large commit interval&lt;br/&gt;
@@ -93,11 +84,9 @@ public void before() throws InterruptedException {&lt;br/&gt;
         builder = new StreamsBuilder();&lt;br/&gt;
         createTopics();&lt;br/&gt;
         streamsConfiguration = new Properties();&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;String applicationId = &quot;kgrouped-stream-test-&quot; +&lt;/li&gt;
	&lt;li&gt;testNo;&lt;br/&gt;
+        String applicationId = &quot;kgrouped-stream-test-&quot; + testNo;&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);&lt;/li&gt;
	&lt;li&gt;streamsConfiguration&lt;/li&gt;
	&lt;li&gt;.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL_MS);&lt;br/&gt;
@@ -111,12 +100,7 @@ public void before() throws InterruptedException {&lt;br/&gt;
                 mapper,&lt;br/&gt;
                 Serialized.with(Serdes.String(), Serdes.String()));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reducer = new Reducer&amp;lt;String&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public String apply(String value1, String value2) 
{
-                return value1 + &quot;:&quot; + value2;
-            }&lt;/li&gt;
	&lt;li&gt;};&lt;br/&gt;
+        reducer = (value1, value2) -&amp;gt; value1 + &quot;:&quot; + value2;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @After&lt;br/&gt;
@@ -132,7 +116,7 @@ public void whenShuttingDown() throws IOException {&lt;br/&gt;
     public void shouldReduce() throws Exception {&lt;br/&gt;
         produceMessages(System.currentTimeMillis());&lt;br/&gt;
         groupedStream&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.reduce(reducer, Materialized.&amp;lt;String, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;reduce-by-key&quot;))&lt;br/&gt;
+                .reduce(reducer, Materialized.as(&quot;reduce-by-key&quot;))&lt;br/&gt;
                 .toStream()&lt;br/&gt;
                 .to(outputTopic, Produced.with(Serdes.String(), Serdes.String()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -140,34 +124,15 @@ public void shouldReduce() throws Exception {&lt;/p&gt;

&lt;p&gt;         produceMessages(System.currentTimeMillis());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; results = receiveMessages(&lt;/li&gt;
	&lt;li&gt;new StringDeserializer(),&lt;/li&gt;
	&lt;li&gt;new StringDeserializer(),&lt;/li&gt;
	&lt;li&gt;5);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public int compare(KeyValue&amp;lt;String, String&amp;gt; o1, KeyValue&amp;lt;String, String&amp;gt; o2) 
{
-                return KStreamAggregationDedupIntegrationTest.compare(o1, o2);
-            }&lt;br/&gt;
-        });&lt;br/&gt;
-&lt;br/&gt;
-        assertThat(results, is(Arrays.asList(&lt;br/&gt;
-            KeyValue.pair(&quot;A&quot;, &quot;A:A&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;B&quot;, &quot;B:B&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;C&quot;, &quot;C:C&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;D&quot;, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;E&quot;, &quot;E:E&quot;))));&lt;br/&gt;
-    }&lt;br/&gt;
-&lt;br/&gt;
-    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
-    private static &amp;lt;K extends Comparable, V extends Comparable&amp;gt; int compare(final KeyValue&amp;lt;K, V&amp;gt; o1,&lt;br/&gt;
-                                                                            final KeyValue&amp;lt;K, V&amp;gt; o2) {&lt;br/&gt;
-        final int keyComparison = o1.key.compareTo(o2.key);&lt;br/&gt;
-        if (keyComparison == 0) {
-            return o1.value.compareTo(o2.value);
-        }&lt;br/&gt;
-        return keyComparison;&lt;br/&gt;
+        validateReceivedMessages(&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                Arrays.asList(&lt;br/&gt;
+                        KeyValue.pair(&quot;A&quot;, &quot;A:A&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;B&quot;, &quot;B:B&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;C&quot;, &quot;C:C&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;D&quot;, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;E&quot;, &quot;E:E&quot;)));&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -180,50 +145,31 @@ public void shouldReduceWindowed() throws Exception {&lt;br/&gt;
 &lt;br/&gt;
         groupedStream&lt;br/&gt;
             .windowedBy(TimeWindows.of(500L))&lt;br/&gt;
-            .reduce(reducer, Materialized.&amp;lt;String, String, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;reduce-time-windows&quot;))&lt;br/&gt;
-            .toStream(new KeyValueMapper&amp;lt;Windowed&amp;lt;String&amp;gt;, String, String&amp;gt;() {&lt;br/&gt;
-                @Override&lt;br/&gt;
-                public String apply(Windowed&amp;lt;String&amp;gt; windowedKey, String value) {
-                    return windowedKey.key() + &quot;@&quot; + windowedKey.window().start();
-                }&lt;br/&gt;
-            })&lt;br/&gt;
+            .reduce(reducer, Materialized.as(&quot;reduce-time-windows&quot;))&lt;br/&gt;
+            .toStream((windowedKey, value) -&amp;gt; windowedKey.key() + &quot;@&quot; + windowedKey.window().start())&lt;br/&gt;
             .to(outputTopic, Produced.with(Serdes.String(), Serdes.String()));&lt;br/&gt;
 &lt;br/&gt;
         startStreams();&lt;br/&gt;
 &lt;br/&gt;
-        List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; windowedOutput = receiveMessages(&lt;br/&gt;
-            new StringDeserializer(),&lt;br/&gt;
-            new StringDeserializer(),&lt;br/&gt;
-            10);&lt;br/&gt;
-&lt;br/&gt;
-        Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;&lt;br/&gt;
-            comparator =&lt;br/&gt;
-            new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;br/&gt;
-                @Override&lt;br/&gt;
-                public int compare(final KeyValue&amp;lt;String, String&amp;gt; o1,&lt;br/&gt;
-                                   final KeyValue&amp;lt;String, String&amp;gt; o2) {
-                    return KStreamAggregationDedupIntegrationTest.compare(o1, o2);
-                }&lt;br/&gt;
-            };&lt;br/&gt;
-&lt;br/&gt;
-        Collections.sort(windowedOutput, comparator);&lt;br/&gt;
         long firstBatchWindow = firstBatchTimestamp / 500 * 500;&lt;br/&gt;
         long secondBatchWindow = secondBatchTimestamp / 500 * 500;&lt;br/&gt;
 &lt;br/&gt;
-        assertThat(windowedOutput, is(&lt;br/&gt;
-            Arrays.asList(&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + firstBatchWindow, &quot;A&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + secondBatchWindow, &quot;A:A&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + firstBatchWindow, &quot;B&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + secondBatchWindow, &quot;B:B&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + firstBatchWindow, &quot;C&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + secondBatchWindow, &quot;C:C&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + firstBatchWindow, &quot;D&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + secondBatchWindow, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + firstBatchWindow, &quot;E&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + secondBatchWindow, &quot;E:E&quot;)&lt;br/&gt;
-            )&lt;br/&gt;
-        ));&lt;br/&gt;
+        validateReceivedMessages(&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                Arrays.asList(&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + firstBatchWindow, &quot;A&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + secondBatchWindow, &quot;A:A&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + firstBatchWindow, &quot;B&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + secondBatchWindow, &quot;B:B&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + firstBatchWindow, &quot;C&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + secondBatchWindow, &quot;C:C&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + firstBatchWindow, &quot;D&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + secondBatchWindow, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + firstBatchWindow, &quot;E&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + secondBatchWindow, &quot;E:E&quot;)&lt;br/&gt;
+                )&lt;br/&gt;
+        );&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -234,36 +180,25 @@ public void shouldGroupByKey() throws Exception {&lt;br/&gt;
 &lt;br/&gt;
         stream.groupByKey(Serialized.with(Serdes.Integer(), Serdes.String()))&lt;br/&gt;
             .windowedBy(TimeWindows.of(500L))&lt;br/&gt;
-            .count(Materialized.&amp;lt;Integer, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;count-windows&quot;))&lt;br/&gt;
-            .toStream(new KeyValueMapper&amp;lt;Windowed&amp;lt;Integer&amp;gt;, Long, String&amp;gt;() {&lt;br/&gt;
-                @Override&lt;br/&gt;
-                public String apply(final Windowed&amp;lt;Integer&amp;gt; windowedKey, final Long value) {-                    return windowedKey.key() + &quot;@&quot; + windowedKey.window().start();-                }&lt;br/&gt;
-            }).to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));&lt;br/&gt;
+            .count(Materialized.as(&quot;count-windows&quot;))&lt;br/&gt;
+            .toStream((windowedKey, value) -&amp;gt; windowedKey.key() + &quot;@&quot; + windowedKey.window().start())&lt;br/&gt;
+            .to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));&lt;br/&gt;
 &lt;br/&gt;
         startStreams();&lt;br/&gt;
 &lt;br/&gt;
-        final List&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
-            new StringDeserializer(),&lt;br/&gt;
-            new LongDeserializer(),&lt;br/&gt;
-            5);&lt;br/&gt;
-        Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt;() {&lt;br/&gt;
-            @Override&lt;br/&gt;
-            public int compare(final KeyValue&amp;lt;String, Long&amp;gt; o1, final KeyValue&amp;lt;String, Long&amp;gt; o2) {-                return KStreamAggregationDedupIntegrationTest.compare(o1, o2);-            }&lt;/li&gt;
	&lt;li&gt;});&lt;br/&gt;
-&lt;br/&gt;
         final long window = timestamp / 500 * 500;&lt;/li&gt;
	&lt;li&gt;assertThat(results, is(Arrays.asList(&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;1@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;2@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;3@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;4@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;5@&quot; + window, 2L)&lt;/li&gt;
	&lt;li&gt;)));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        validateReceivedMessages(&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                new LongDeserializer(),&lt;br/&gt;
+                Arrays.asList(&lt;br/&gt;
+                        KeyValue.pair(&quot;1@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;2@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;3@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;4@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;5@&quot; + window, 2L)&lt;br/&gt;
+                )&lt;br/&gt;
+        );&lt;br/&gt;
     }&lt;/p&gt;


&lt;p&gt;@@ -298,11 +233,9 @@ private void startStreams() {&lt;br/&gt;
     }&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private &amp;lt;K, V&amp;gt; List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; receiveMessages(final Deserializer&amp;lt;K&amp;gt;&lt;/li&gt;
	&lt;li&gt;keyDeserializer,&lt;/li&gt;
	&lt;li&gt;final Deserializer&amp;lt;V&amp;gt;&lt;/li&gt;
	&lt;li&gt;valueDeserializer,&lt;/li&gt;
	&lt;li&gt;final int numMessages)&lt;br/&gt;
+    private &amp;lt;K, V&amp;gt; void validateReceivedMessages(final Deserializer&amp;lt;K&amp;gt; keyDeserializer,&lt;br/&gt;
+                                                 final Deserializer&amp;lt;V&amp;gt; valueDeserializer,&lt;br/&gt;
+                                                 final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; expectedRecords)&lt;br/&gt;
         throws InterruptedException {&lt;br/&gt;
         final Properties consumerProperties = new Properties();&lt;br/&gt;
         consumerProperties&lt;br/&gt;
@@ -314,11 +247,11 @@ private void startStreams() 
{
             keyDeserializer.getClass().getName());
         consumerProperties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
             valueDeserializer.getClass().getName());
-        return IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerProperties,
-            outputTopic,
-            numMessages,
-            60 * 1000);
+        consumerProperties.setProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, String.valueOf(Integer.MAX_VALUE));
 
+        IntegrationTestUtils.waitUntilExactKeyValueRecordsReceived(consumerProperties,
+                outputTopic,
+                expectedRecords);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
index ff791be1316..b68695e1247 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
@@ -524,20 +524,24 @@ public boolean test(final String key, final Long value) {&lt;br/&gt;
             myFilterNotStore = kafkaStreams.store(&quot;queryFilterNot&quot;, QueryableStoreTypes.&amp;lt;String, Long&amp;gt;keyValueStore());&lt;/p&gt;

&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; expectedEntry : expectedBatch1) &lt;/p&gt;
{
-            assertEquals(myFilterStore.get(expectedEntry.key), expectedEntry.value);
+            TestUtils.waitForCondition(() -&amp;gt; expectedEntry.value.equals(myFilterStore.get(expectedEntry.key)),
+                    &quot;Cannot get expected result&quot;);
         }
&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; batchEntry : batch1) {&lt;br/&gt;
             if (!expectedBatch1.contains(batchEntry)) &lt;/p&gt;
{
-                assertNull(myFilterStore.get(batchEntry.key));
+                TestUtils.waitForCondition(() -&amp;gt; myFilterStore.get(batchEntry.key) == null,
+                        &quot;Cannot get null result&quot;);
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; expectedEntry : expectedBatch1) &lt;/p&gt;
{
-            assertNull(myFilterNotStore.get(expectedEntry.key));
+            TestUtils.waitForCondition(() -&amp;gt; myFilterNotStore.get(expectedEntry.key) == null,
+                    &quot;Cannot get null result&quot;);
         }
&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; batchEntry : batch1) {&lt;br/&gt;
             if (!expectedBatch1.contains(batchEntry)) &lt;/p&gt;
{
-                assertEquals(myFilterNotStore.get(batchEntry.key), batchEntry.value);
+                TestUtils.waitForCondition(() -&amp;gt; batchEntry.value.equals(myFilterNotStore.get(batchEntry.key)),
+                        &quot;Cannot get expected result&quot;);
             }
&lt;p&gt;         }&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
index 749d74887e5..0f64b8df6fc 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
@@ -350,6 +350,33 @@ public static void waitForCompletion(final KafkaStreams streams,&lt;br/&gt;
         return accumData;&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    public static &amp;lt;K, V&amp;gt; List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; waitUntilExactKeyValueRecordsReceived(final Properties consumerConfig,&lt;br/&gt;
+                                                                                    final String topic,&lt;br/&gt;
+                                                                                    final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; expectedRecords) throws InterruptedException &lt;/p&gt;
{
+        return waitUntilExactKeyValueRecordsReceived(consumerConfig, topic, expectedRecords, DEFAULT_TIMEOUT);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public static &amp;lt;K, V&amp;gt; List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; waitUntilExactKeyValueRecordsReceived(final Properties consumerConfig,&lt;br/&gt;
+                                                                                    final String topic,&lt;br/&gt;
+                                                                                    final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; expectedRecords,&lt;br/&gt;
+                                                                                    final long waitTime) throws InterruptedException {&lt;br/&gt;
+        final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; accumData = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        try (final Consumer&amp;lt;K, V&amp;gt; consumer = createConsumer(consumerConfig)) {&lt;br/&gt;
+            final TestCondition valuesRead = new TestCondition() {&lt;br/&gt;
+                @Override&lt;br/&gt;
+                public boolean conditionMet() &lt;/p&gt;
{
+                    final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; readData =
+                            readKeyValues(topic, consumer, waitTime, expectedRecords.size());
+                    accumData.addAll(readData);
+                    return accumData.containsAll(expectedRecords);
+                }
&lt;p&gt;+            };&lt;br/&gt;
+            final String conditionDetails = &quot;Did not receive all &quot; + expectedRecords + &quot; records from topic &quot; + topic;&lt;br/&gt;
+            TestUtils.waitForCondition(valuesRead, waitTime, conditionDetails);&lt;br/&gt;
+        }&lt;br/&gt;
+        return accumData;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     public static &amp;lt;K, V&amp;gt; List&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; waitUntilMinRecordsReceived(final Properties consumerConfig,&lt;br/&gt;
                                                                                 final String topic,&lt;br/&gt;
                                                                                 final int expectedNumRecords,&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
index 8a8d6255c46..3787ffec205 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
@@ -253,6 +253,7 @@ private void mockTaskInitialization() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCommitRunningTasks() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall();&lt;br/&gt;
         EasyMock.replay(t1);&lt;br/&gt;
@@ -266,6 +267,7 @@ public void shouldCommitRunningTasks() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCloseTaskOnCommitIfTaskMigratedException() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new TaskMigratedException());&lt;br/&gt;
         t1.close(false, true);&lt;br/&gt;
@@ -285,6 +287,7 @@ public void shouldCloseTaskOnCommitIfTaskMigratedException() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldThrowExceptionOnCommitWhenNotCommitFailedOrProducerFenced() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new RuntimeException(&quot;&quot;));&lt;br/&gt;
         EasyMock.replay(t1);&lt;br/&gt;
@@ -303,6 +306,7 @@ public void shouldThrowExceptionOnCommitWhenNotCommitFailedOrProducerFenced() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCommitRunningTasksIfNeeded() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitRequested()).andReturn(true);&lt;br/&gt;
         EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall();&lt;br/&gt;
@@ -317,6 +321,7 @@ public void shouldCommitRunningTasksIfNeeded() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCloseTaskOnMaybeCommitIfTaskMigratedException() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitRequested()).andReturn(true);&lt;br/&gt;
         EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new TaskMigratedException());&lt;br/&gt;
@@ -337,12 +342,14 @@ public void shouldCloseTaskOnMaybeCommitIfTaskMigratedException() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCloseTaskOnProcessesIfTaskMigratedException() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.isProcessable()).andReturn(true);&lt;br/&gt;
         t1.process();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new TaskMigratedException());&lt;br/&gt;
         t1.close(false, true);&lt;br/&gt;
         EasyMock.expectLastCall();&lt;br/&gt;
         EasyMock.replay(t1);&lt;br/&gt;
         addAndInitTask();&lt;br/&gt;
+        assignedTasks.update();&lt;/p&gt;

&lt;p&gt;         try {&lt;br/&gt;
             assignedTasks.process();&lt;br/&gt;
@@ -353,6 +360,35 @@ public void shouldCloseTaskOnProcessesIfTaskMigratedException() &lt;/p&gt;
{
         EasyMock.verify(t1);
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldNotProcessUnprocessableTasks() &lt;/p&gt;
{
+        mockTaskInitialization();
+        EasyMock.expect(t1.isProcessable()).andReturn(false);
+        EasyMock.replay(t1);
+        addAndInitTask();
+        assignedTasks.update();
+
+        assertThat(assignedTasks.process(), equalTo(0));
+
+        EasyMock.verify(t1);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldAlwaysProcessProcessableTasks() &lt;/p&gt;
{
+        mockTaskInitialization();
+        EasyMock.expect(t1.isProcessable()).andReturn(true);
+        EasyMock.expect(t1.process()).andReturn(true).once();
+        EasyMock.expect(t1.allSourcePartitionsBuffered()).andReturn(true);
+        EasyMock.replay(t1);
+
+        addAndInitTask();
+        assignedTasks.update();
+
+        assertThat(assignedTasks.process(), equalTo(1));
+
+        EasyMock.verify(t1);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldPunctuateRunningTasks() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
index b3123e46343..f7765ca0780 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
@@ -108,7 +108,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(5, group.numBuffered());&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(2L, group.timestamp());&lt;br/&gt;
+        assertEquals(1L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one record, now the time should be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -120,7 +120,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(4, group.numBuffered());&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, group.timestamp());&lt;br/&gt;
+        assertEquals(2L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add 2 more records with timestamp 2, 4 to partition-1&lt;br/&gt;
         final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; list3 = Arrays.asList(&lt;br/&gt;
@@ -134,7 +134,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(6, group.numBuffered());&lt;br/&gt;
         assertEquals(4, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, group.timestamp());&lt;br/&gt;
+        assertEquals(2L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -146,7 +146,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(5, group.numBuffered());&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(4L, group.timestamp());&lt;br/&gt;
+        assertEquals(3L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -158,7 +158,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(4, group.numBuffered());&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(1, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(5L, group.timestamp());&lt;br/&gt;
+        assertEquals(4L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one more record, now time should be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -206,7 +206,7 @@ public void testTimeTracking() &lt;/p&gt;
{
         assertEquals(0, group.numBuffered());
         assertEquals(0, group.numBuffered(partition1));
         assertEquals(0, group.numBuffered(partition2));
-        assertEquals(5L, group.timestamp());
+        assertEquals(6L, group.timestamp());
 
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
index bfbb2a00270..cdb7410fd8a 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
@@ -57,6 +57,7 @@&lt;/p&gt;

&lt;p&gt; import java.io.File;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
 import java.time.Duration;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
@@ -111,7 +112,7 @@ public void close() {&lt;/p&gt;

&lt;p&gt;     private final ProcessorTopology topology = ProcessorTopology.withSources(&lt;br/&gt;
         Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source2))&lt;br/&gt;
+        mkMap(mkEntry(topic1, source1), mkEntry(topic2, source2))&lt;br/&gt;
     );&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final MockConsumer&amp;lt;byte[], byte[]&amp;gt; consumer = new MockConsumer&amp;lt;&amp;gt;(OffsetResetStrategy.EARLIEST);&lt;br/&gt;
@@ -307,97 +308,6 @@ public void testPauseResume() &lt;/p&gt;
{
         assertEquals(0, consumer.paused().size());
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@SuppressWarnings(&quot;unchecked&quot;)&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testMaybePunctuateStreamTime() 
{
-        task = createStatelessTask(createConfig(false));
-        task.initializeStateStores();
-        task.initializeTopology();
-
-        task.addRecords(partition1, Arrays.asList(
-            getConsumerRecord(partition1, 0),
-            getConsumerRecord(partition1, 20),
-            getConsumerRecord(partition1, 32),
-            getConsumerRecord(partition1, 40),
-            getConsumerRecord(partition1, 60)
-        ));
-
-        task.addRecords(partition2, Arrays.asList(
-            getConsumerRecord(partition2, 25),
-            getConsumerRecord(partition2, 35),
-            getConsumerRecord(partition2, 45),
-            getConsumerRecord(partition2, 61)
-        ));
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(8, task.numBuffered());
-        assertEquals(1, source1.numReceived);
-        assertEquals(0, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(7, task.numBuffered());
-        assertEquals(2, source1.numReceived);
-        assertEquals(0, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(6, task.numBuffered());
-        assertEquals(2, source1.numReceived);
-        assertEquals(1, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(5, task.numBuffered());
-        assertEquals(3, source1.numReceived);
-        assertEquals(1, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(4, task.numBuffered());
-        assertEquals(3, source1.numReceived);
-        assertEquals(2, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(3, task.numBuffered());
-        assertEquals(4, source1.numReceived);
-        assertEquals(2, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(2, task.numBuffered());
-        assertEquals(4, source1.numReceived);
-        assertEquals(3, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(1, task.numBuffered());
-        assertEquals(5, source1.numReceived);
-        assertEquals(3, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(0, task.numBuffered());
-        assertEquals(5, source1.numReceived);
-        assertEquals(4, source2.numReceived);
-
-        assertFalse(task.process());
-        assertFalse(task.maybePunctuateStreamTime());
-
-        processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 0L, 20L, 32L, 40L, 60L);
-    }
&lt;p&gt;-&lt;br/&gt;
     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldPunctuateOnceStreamTimeAfterGap() {&lt;br/&gt;
@@ -419,6 +329,7 @@ public void shouldPunctuateOnceStreamTimeAfterGap() {&lt;br/&gt;
             getConsumerRecord(partition2, 161)&lt;br/&gt;
         ));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        // st: 20&lt;br/&gt;
         assertTrue(task.maybePunctuateStreamTime()); // punctuate at 20&lt;/p&gt;

&lt;p&gt;         assertTrue(task.process());&lt;br/&gt;
@@ -426,6 +337,7 @@ public void shouldPunctuateOnceStreamTimeAfterGap() {&lt;br/&gt;
         assertEquals(1, source1.numReceived);&lt;br/&gt;
         assertEquals(0, source2.numReceived);&lt;/p&gt;

&lt;p&gt;+        // st: 20&lt;br/&gt;
         assertFalse(task.maybePunctuateStreamTime());&lt;/p&gt;

&lt;p&gt;         assertTrue(task.process());&lt;br/&gt;
@@ -433,9 +345,7 @@ public void shouldPunctuateOnceStreamTimeAfterGap() {&lt;br/&gt;
         assertEquals(1, source1.numReceived);&lt;br/&gt;
         assertEquals(1, source2.numReceived);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(task.maybePunctuateStreamTime()); // punctuate at 142&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// only one punctuation after 100ms gap&lt;br/&gt;
+        // st: 25&lt;br/&gt;
         assertFalse(task.maybePunctuateStreamTime());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertTrue(task.process());&lt;br/&gt;
@@ -443,50 +353,58 @@ public void shouldPunctuateOnceStreamTimeAfterGap() &lt;/p&gt;
{
         assertEquals(2, source1.numReceived);
         assertEquals(1, source2.numReceived);
 
-        assertFalse(task.maybePunctuateStreamTime());
+        // st: 142
+        assertTrue(task.maybePunctuateStreamTime()); // punctuate at 142
 
         assertTrue(task.process());
         assertEquals(4, task.numBuffered());
         assertEquals(2, source1.numReceived);
         assertEquals(2, source2.numReceived);
 
-        assertTrue(task.maybePunctuateStreamTime()); // punctuate at 155
+        // st: 145
+        // only one punctuation after 100ms gap
+        assertFalse(task.maybePunctuateStreamTime());
 
         assertTrue(task.process());
         assertEquals(3, task.numBuffered());
         assertEquals(3, source1.numReceived);
         assertEquals(2, source2.numReceived);
 
-        assertFalse(task.maybePunctuateStreamTime());
+        // st: 155
+        assertTrue(task.maybePunctuateStreamTime()); // punctuate at 155
 
         assertTrue(task.process());
         assertEquals(2, task.numBuffered());
         assertEquals(3, source1.numReceived);
         assertEquals(3, source2.numReceived);
 
-        assertTrue(task.maybePunctuateStreamTime()); // punctuate at 160, still aligned on the initial punctuation
+        // st: 159
+        assertFalse(task.maybePunctuateStreamTime());
 
         assertTrue(task.process());
         assertEquals(1, task.numBuffered());
         assertEquals(4, source1.numReceived);
         assertEquals(3, source2.numReceived);
 
-        assertFalse(task.maybePunctuateStreamTime());
+        // st: 160, aligned at 0
+        assertTrue(task.maybePunctuateStreamTime());
 
         assertTrue(task.process());
         assertEquals(0, task.numBuffered());
         assertEquals(4, source1.numReceived);
         assertEquals(4, source2.numReceived);
 
+        // st: 161
+        assertFalse(task.maybePunctuateStreamTime());
+
         assertFalse(task.process());
         assertFalse(task.maybePunctuateStreamTime());
 
         processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L, 142L, 155L, 160L);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;/li&gt;
	&lt;li&gt;public void testCancelPunctuateStreamTime() {&lt;br/&gt;
+    public void shouldRespectPunctuateCancellationStreamTime() {&lt;br/&gt;
         task = createStatelessTask(createConfig(false));&lt;br/&gt;
         task.initializeStateStores();&lt;br/&gt;
         task.initializeTopology();&lt;br/&gt;
@@ -518,6 +436,61 @@ public void testCancelPunctuateStreamTime() 
{
         processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldRespectPunctuateCancellationSystemTime() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+        final long now = time.milliseconds();
+        time.sleep(10);
+        assertTrue(task.maybePunctuateSystemTime());
+        processorSystemTime.mockProcessor.scheduleCancellable.cancel();
+        time.sleep(10);
+        assertFalse(task.maybePunctuateSystemTime());
+        processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 10);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldBeProcessableIfAllPartitionsBuffered() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+
+        assertFalse(task.isProcessable());
+
+        final byte[] bytes = ByteBuffer.allocate(4).putInt(1).array();
+
+        task.addRecords(partition1, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, bytes, bytes)));
+
+        assertFalse(task.isProcessable());
+
+        task.addRecords(partition2, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic2, 1, 0, bytes, bytes)));
+
+        assertTrue(task.isProcessable());
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldBeProcessableIfWaitedForTooLong() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+
+        assertFalse(task.isProcessable());
+
+        final byte[] bytes = ByteBuffer.allocate(4).putInt(1).array();
+
+        task.addRecords(partition1, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, bytes, bytes)));
+
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+
+        assertTrue(task.isProcessable());
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldPunctuateSystemTimeWhenIntervalElapsed() {&lt;br/&gt;
         task = createStatelessTask(createConfig(false));&lt;br/&gt;
@@ -575,20 +548,6 @@ public void shouldPunctuateOnceSystemTimeAfterGap() &lt;/p&gt;
{
         processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 100, now + 110, now + 122, now + 130, now + 235, now + 240);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testCancelPunctuateSystemTime() 
{
-        task = createStatelessTask(createConfig(false));
-        task.initializeStateStores();
-        task.initializeTopology();
-        final long now = time.milliseconds();
-        time.sleep(10);
-        assertTrue(task.maybePunctuateSystemTime());
-        processorSystemTime.mockProcessor.scheduleCancellable.cancel();
-        time.sleep(10);
-        assertFalse(task.maybePunctuateSystemTime());
-        processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 10);
-    }
&lt;p&gt;-&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldWrapKafkaExceptionsWithStreamsExceptionAndAddContext() {&lt;br/&gt;
         task = createTaskThatThrowsException();&lt;br/&gt;
@@ -1110,7 +1069,7 @@ private StreamTask createStatefulTaskThatThrowsExceptionOnClose() {&lt;br/&gt;
     private StreamTask createStatelessTask(final StreamsConfig streamsConfig) {&lt;br/&gt;
         final ProcessorTopology topology = ProcessorTopology.withSources(&lt;br/&gt;
             Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source2))&lt;br/&gt;
+            mkMap(mkEntry(topic1, source1), mkEntry(topic2, source2))&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         source1.addChild(processorStreamTime);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
index 93d4e94a234..b2ae7e30df2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
@@ -177,7 +177,7 @@ public void testPartitionAssignmentChangeForSingleGroup() {&lt;br/&gt;
         mockConsumer.assign(assignedPartitions);&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(thread.state(), StreamThread.State.RUNNING);&lt;br/&gt;
         Assert.assertEquals(4, stateListener.numChanges);&lt;br/&gt;
         Assert.assertEquals(StreamThread.State.PARTITIONS_ASSIGNED, stateListener.oldState);&lt;br/&gt;
@@ -307,9 +307,11 @@ public void shouldNotCommitBeforeTheCommitInterval() 
{
             new LogContext(&quot;&quot;),
             new AtomicInteger()
         );
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
         mockTime.sleep(commitInterval - 10L);
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
 
         EasyMock.verify(taskManager);
     }&lt;br/&gt;
@@ -341,9 +343,11 @@ public void shouldNotCauseExceptionIfNothingCommitted() {             new LogContext(&quot;&quot;),             new AtomicInteger()         );-        thread.maybeCommit(mockTime.milliseconds());+        thread.setNow(mockTime.milliseconds());+        thread.maybeCommit();         mockTime.sleep(commitInterval - 10L);-        thread.maybeCommit(mockTime.milliseconds());+        thread.setNow(mockTime.milliseconds());+        thread.maybeCommit();          EasyMock.verify(taskManager);     }
&lt;p&gt;@@ -376,9 +380,11 @@ public void shouldCommitAfterTheCommitInterval() &lt;/p&gt;
{
             new LogContext(&quot;&quot;),
             new AtomicInteger()
         );
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
         mockTime.sleep(commitInterval + 1);
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
 
         EasyMock.verify(taskManager);
     }
&lt;p&gt;@@ -456,7 +462,7 @@ public void shouldInjectProducerPerTaskUsingClientSupplierOnCreateIfEosEnable()&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(beginOffsets);&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(new HashSet&amp;lt;&amp;gt;(assignedPartitions));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(thread.tasks().size(), clientSupplier.producers.size());&lt;br/&gt;
         assertSame(clientSupplier.consumer, thread.consumer);&lt;br/&gt;
@@ -645,7 +651,7 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerWasFencedWh&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertThat(thread.tasks().size(), equalTo(1));&lt;br/&gt;
         final MockProducer producer = clientSupplier.producers.get(0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -656,7 +662,7 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerWasFencedWh&lt;/p&gt;

&lt;p&gt;         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockTime.sleep(config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG) + 1);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertThat(producer.history().size(), equalTo(1));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertFalse(producer.transactionCommitted());&lt;br/&gt;
@@ -665,16 +671,16 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerWasFencedWh&lt;br/&gt;
             new TestCondition() {&lt;br/&gt;
                 @Override&lt;br/&gt;
                 public boolean conditionMet() &lt;/p&gt;
{
-                    return producer.commitCount() == 2;
+                    return producer.commitCount() == 1;
                 }
&lt;p&gt;             },&lt;br/&gt;
             &quot;StreamsThread did not commit transaction.&quot;);&lt;/p&gt;

&lt;p&gt;         producer.fenceProducer();&lt;br/&gt;
         mockTime.sleep(config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG) + 1L);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
+        consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         try 
{
-            thread.runOnce(-1);
+            thread.runOnce();
             fail(&quot;Should have thrown TaskMigratedException&quot;);
         } catch (final TaskMigratedException expected) { /* ignore */ }&lt;br/&gt;
         TestUtils.waitForCondition(&lt;br/&gt;
@@ -686,7 +692,7 @@ public boolean conditionMet() {&lt;br/&gt;
             },&lt;br/&gt;
             &quot;StreamsThread did not remove fenced zombie task.&quot;);&lt;br/&gt;
 &lt;br/&gt;
-        assertThat(producer.commitCount(), equalTo(2L));&lt;br/&gt;
+        assertThat(producer.commitCount(), equalTo(1L));&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -713,7 +719,7 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerGotFencedAt&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;br/&gt;
 &lt;br/&gt;
-        thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
 &lt;br/&gt;
         assertThat(thread.tasks().size(), equalTo(1));&lt;br/&gt;
 &lt;br/&gt;
@@ -721,7 +727,7 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerGotFencedAt&lt;br/&gt;
         clientSupplier.producers.get(0).fenceProducer();&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;br/&gt;
         try {-            thread.runOnce(-1);+            thread.runOnce();             fail(&quot;Should have thrown TaskMigratedException&quot;);         }
&lt;p&gt; catch (final TaskMigratedException expected) &lt;/p&gt;
{ /* ignore */ }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -772,7 +778,7 @@ public void shouldReturnActiveTaskMetadataWhileRunningState() {&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final ThreadMetadata threadMetadata = thread.threadMetadata();&lt;br/&gt;
         assertEquals(StreamThread.State.RUNNING.name(), threadMetadata.threadState());&lt;br/&gt;
@@ -816,7 +822,7 @@ public void shouldReturnStandbyTaskMetadataWhileRunningState() {&lt;/p&gt;

&lt;p&gt;         thread.rebalanceListener.onPartitionsAssigned(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final ThreadMetadata threadMetadata = thread.threadMetadata();&lt;br/&gt;
         assertEquals(StreamThread.State.RUNNING.name(), threadMetadata.threadState());&lt;br/&gt;
@@ -881,7 +887,7 @@ public void shouldUpdateStandbyTask() throws IOException {&lt;/p&gt;

&lt;p&gt;         thread.rebalanceListener.onPartitionsAssigned(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final StandbyTask standbyTask1 = thread.taskManager().standbyTask(partition1);&lt;br/&gt;
         final StandbyTask standbyTask2 = thread.taskManager().standbyTask(partition2);&lt;br/&gt;
@@ -947,7 +953,7 @@ public void close() {}&lt;br/&gt;
         clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, punctuatedStreamTime.size());&lt;br/&gt;
         assertEquals(0, punctuatedWallClockTime.size());&lt;br/&gt;
@@ -957,14 +963,14 @@ public void close() {}&lt;br/&gt;
             clientSupplier.consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, i, i * 100L, TimestampType.CREATE_TIME, ConsumerRecord.NULL_CHECKSUM, (&quot;K&quot; + i).getBytes().length, (&quot;V&quot; + i).getBytes().length, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));&lt;br/&gt;
         }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(1, punctuatedStreamTime.size());&lt;br/&gt;
         assertEquals(1, punctuatedWallClockTime.size());&lt;/p&gt;

&lt;p&gt;         mockTime.sleep(100L);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // we should skip stream time punctuation, only trigger wall-clock time punctuation&lt;br/&gt;
         assertEquals(1, punctuatedStreamTime.size());&lt;br/&gt;
@@ -1155,7 +1161,7 @@ public void shouldRecordSkippedMetricForDeserializationException() {&lt;br/&gt;
         mockConsumer.assign(Collections.singleton(t1p1));&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final MetricName skippedTotalMetric = metrics.metricName(&quot;skipped-records-total&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
         final MetricName skippedRateMetric = metrics.metricName(&quot;skipped-records-rate&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
@@ -1165,7 +1171,7 @@ public void shouldRecordSkippedMetricForDeserializationException() {&lt;br/&gt;
         long offset = -1;&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, &quot;I am not an integer.&quot;.getBytes()));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, &quot;I am not an integer.&quot;.getBytes()));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(2.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1199,7 +1205,7 @@ public void shouldReportSkippedRecordsForInvalidTimestamps() {&lt;br/&gt;
         mockConsumer.assign(Collections.singleton(t1p1));&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final MetricName skippedTotalMetric = metrics.metricName(&quot;skipped-records-total&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
         final MetricName skippedRateMetric = metrics.metricName(&quot;skipped-records-rate&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
@@ -1209,7 +1215,7 @@ public void shouldReportSkippedRecordsForInvalidTimestamps() {&lt;br/&gt;
         long offset = -1;&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(2.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1217,13 +1223,13 @@ public void shouldReportSkippedRecordsForInvalidTimestamps() {&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(6.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(6.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/resources/log4j.properties b/streams/src/test/resources/log4j.properties&lt;br/&gt;
index be36f90299a..0c0caf162c9 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/resources/log4j.properties&lt;br/&gt;
+++ b/streams/src/test/resources/log4j.properties&lt;br/&gt;
@@ -12,10 +12,10 @@&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
-log4j.rootLogger=INFO, stdout&lt;br/&gt;
+log4j.rootLogger=WARN, stdout&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; log4j.appender.stdout=org.apache.log4j.ConsoleAppender&lt;br/&gt;
 log4j.appender.stdout.layout=org.apache.log4j.PatternLayout&lt;br/&gt;
 log4j.appender.stdout.layout.ConversionPattern=&lt;span class=&quot;error&quot;&gt;&amp;#91;%d&amp;#93;&lt;/span&gt; %p %m (%c:%L)%n&lt;/p&gt;

&lt;p&gt;-log4j.logger.org.apache.kafka=INFO&lt;br/&gt;
+log4j.logger.org.apache.kafka=WARN&lt;br/&gt;
diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
index 32ad793bc84..d43c1970a14 100644&lt;br/&gt;
&amp;#8212; a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
+++ b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
@@ -24,11 +24,7 @@ import org.apache.kafka.common.serialization._&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime&lt;br/&gt;
 import org.apache.kafka.streams._&lt;br/&gt;
 import org.apache.kafka.streams.integration.utils.&lt;/p&gt;
{EmbeddedKafkaCluster, IntegrationTestUtils}
&lt;p&gt;-import org.apache.kafka.streams.processor.internals.StreamThread&lt;br/&gt;
-import org.apache.kafka.streams.scala.ImplicitConversions._&lt;br/&gt;
-import org.apache.kafka.streams.scala.kstream._&lt;br/&gt;
 import org.apache.kafka.test.TestUtils&lt;br/&gt;
-import org.junit.Assert._&lt;br/&gt;
 import org.junit._&lt;br/&gt;
 import org.junit.rules.TemporaryFolder&lt;br/&gt;
 import org.scalatest.junit.JUnitSuite&lt;br/&gt;
@@ -129,9 +125,9 @@ class StreamToTableJoinScalaIntegrationTestBase extends JUnitSuite with StreamTo&lt;br/&gt;
       // consume and verify result&lt;br/&gt;
       val consumerConfig = getConsumerConfig()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig,&lt;/li&gt;
	&lt;li&gt;outputTopic,&lt;/li&gt;
	&lt;li&gt;expectedClicksPerRegion.size)&lt;br/&gt;
+      IntegrationTestUtils.waitUntilExactKeyValueRecordsReceived(consumerConfig,&lt;br/&gt;
+                                                                 outputTopic,&lt;br/&gt;
+                                                                 expectedClicksPerRegion.asJava)&lt;br/&gt;
     } else 
{
       java.util.Collections.emptyList()
     }
&lt;p&gt;diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
index e5253f95d45..3d1bab5d086 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
+++ b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
@@ -82,9 +82,6 @@ class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends StreamToTableJ&lt;br/&gt;
     val actualClicksPerRegion: java.util.List[KeyValue&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt;] =&lt;br/&gt;
       produceNConsume(userClicksTopic, userRegionsTopic, outputTopic)&lt;br/&gt;
     streams.close()&lt;br/&gt;
-&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;import collection.JavaConverters._&lt;/li&gt;
	&lt;li&gt;assertEquals(actualClicksPerRegion.asScala.sortBy(&lt;em&gt;.key), expectedClicksPerRegion.sortBy(&lt;/em&gt;.key))&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test def testShouldCountClicksPerRegionJava(): Unit = &lt;/p&gt;
{
@@ -149,6 +146,5 @@ class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends StreamToTableJ
       produceNConsume(userClicksTopicJ, userRegionsTopicJ, outputTopicJ)
 
     streams.close()
-    assertEquals(actualClicksPerRegion.asScala.sortBy(_.key), expectedClicksPerRegion.sortBy(_.key))
   }
&lt;p&gt; }&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16561178" author="githubbot" created="Sun, 29 Jul 2018 17:04:34 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5428: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Part III, Refactor StreamThread main loop&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5428&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5428&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   *More detailed description of your change,&lt;br/&gt;
   if necessary. The PR title and PR message become&lt;br/&gt;
   the squashed commit message, so use a separate&lt;br/&gt;
   comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   for the feature or bug fix. Unit and/or integration&lt;br/&gt;
   tests are expected for any behaviour change and&lt;br/&gt;
   system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16567670" author="githubbot" created="Fri, 3 Aug 2018 01:34:56 +0000"  >&lt;p&gt;guozhangwang closed pull request #5398: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Part II, Choose tasks with data on all partitions to process&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5398&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5398&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
index f98e6356a22..0a839655748 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
@@ -87,11 +87,13 @@ int maybeCommit() {&lt;br/&gt;
      */&lt;br/&gt;
     int process() {&lt;br/&gt;
         int processed = 0;&lt;br/&gt;
+&lt;br/&gt;
         final Iterator&amp;lt;Map.Entry&amp;lt;TaskId, StreamTask&amp;gt;&amp;gt; it = running.entrySet().iterator();&lt;br/&gt;
         while (it.hasNext()) {&lt;br/&gt;
             final StreamTask task = it.next().getValue();&lt;br/&gt;
+&lt;br/&gt;
             try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (task.process()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                if (task.isProcessable() &amp;amp;&amp;amp; task.process()) {
                     processed++;
                 }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; catch (final TaskMigratedException e) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {@@ -108,6 +110,7 @@ int process() {
                 throw e;
             }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
         return processed;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
index 34252bf2b4c..f17c63acd2f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
@@ -27,15 +27,23 @@&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A PartitionGroup is composed from a set of partitions. It also maintains the timestamp of this&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* group, hence the associated task as the min timestamp across all partitions in the group.&lt;br/&gt;
+ * group, a.k.a. the stream time of the associated task. It is defined as the maximum timestamp of&lt;br/&gt;
+ * all the records having been retrieved for processing from this PartitionGroup so far.&lt;br/&gt;
+ *&lt;br/&gt;
+ * We decide from which partition to retrieve the next record to process based on partitions&apos; timestamps.&lt;br/&gt;
+ * The timestamp of a specific partition is initialized as UNKNOWN (-1), and is updated with the head record&apos;s timestamp&lt;br/&gt;
+ * if it is smaller (i.e. it should be monotonically increasing); when the partition&apos;s buffer becomes empty and there is&lt;br/&gt;
+ * no head record, the partition&apos;s timestamp will not be updated any more.&lt;br/&gt;
  */&lt;br/&gt;
 public class PartitionGroup {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final Map&amp;lt;TopicPartition, RecordQueue&amp;gt; partitionQueues;&lt;br/&gt;
-&lt;br/&gt;
     private final PriorityQueue&amp;lt;RecordQueue&amp;gt; nonEmptyQueuesByTime;&lt;br/&gt;
+&lt;br/&gt;
     private long streamTime;&lt;br/&gt;
     private int totalBuffered;&lt;br/&gt;
+    private boolean allBuffered;&lt;br/&gt;
+&lt;/p&gt;

&lt;p&gt;     public static class RecordInfo {&lt;br/&gt;
         RecordQueue queue;&lt;br/&gt;
@@ -53,11 +61,11 @@ RecordQueue queue() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     PartitionGroup(final Map&amp;lt;TopicPartition, RecordQueue&amp;gt; partitionQueues) &lt;/p&gt;
{
         nonEmptyQueuesByTime = new PriorityQueue&amp;lt;&amp;gt;(partitionQueues.size(), Comparator.comparingLong(RecordQueue::timestamp));
         this.partitionQueues = partitionQueues;
         totalBuffered = 0;
+        allBuffered = false;
         streamTime = RecordQueue.NOT_KNOWN;
     }

&lt;p&gt;@@ -79,17 +87,16 @@ StampedRecord nextRecord(final RecordInfo info) {&lt;br/&gt;
             if (record != null) {&lt;br/&gt;
                 --totalBuffered;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!queue.isEmpty()) {&lt;br/&gt;
+                if (queue.isEmpty()) 
{
+                    // if a certain queue has been drained, reset the flag
+                    allBuffered = false;
+                }
&lt;p&gt; else &lt;/p&gt;
{
                     nonEmptyQueuesByTime.offer(queue);
                 }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Since this was previously a queue with min timestamp,&lt;/li&gt;
	&lt;li&gt;// streamTime could only advance if this queue&apos;s time did.&lt;/li&gt;
	&lt;li&gt;if (queue.timestamp() &amp;gt; streamTime) 
{
-                    computeStreamTime();
-                }
&lt;p&gt;+                // always update the stream time to the record&apos;s timestamp yet to be processed if it is larger&lt;br/&gt;
+                streamTime = Math.max(streamTime, record.timestamp);&lt;br/&gt;
             }&lt;br/&gt;
-&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return record;&lt;br/&gt;
@@ -106,17 +113,18 @@ int addRawRecords(final TopicPartition partition, final Iterable&amp;lt;ConsumerRecord&amp;lt;&lt;br/&gt;
         final RecordQueue recordQueue = partitionQueues.get(partition);&lt;/p&gt;

&lt;p&gt;         final int oldSize = recordQueue.size();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long oldTimestamp = recordQueue.timestamp();&lt;br/&gt;
         final int newSize = recordQueue.addRawRecords(rawRecords);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add this record queue to be considered for processing in the future if it was empty before&lt;br/&gt;
         if (oldSize == 0 &amp;amp;&amp;amp; newSize &amp;gt; 0) &lt;/p&gt;
{
             nonEmptyQueuesByTime.offer(recordQueue);
-        }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Adding to this queue could only advance streamTime if it was previously the queue with min timestamp (= streamTime)&lt;/li&gt;
	&lt;li&gt;if (oldTimestamp &amp;lt;= streamTime &amp;amp;&amp;amp; recordQueue.timestamp() &amp;gt; streamTime) {&lt;/li&gt;
	&lt;li&gt;computeStreamTime();&lt;br/&gt;
+            // if all partitions now are non-empty, set the flag&lt;br/&gt;
+            // we do not need to update the stream time here since this task will definitely be&lt;br/&gt;
+            // processed next, and hence the stream time will be updated when we retrieved records by then&lt;br/&gt;
+            if (nonEmptyQueuesByTime.size() == this.partitionQueues.size()) 
{
+                allBuffered = true;
+            }
&lt;p&gt;         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         totalBuffered += newSize - oldSize;&lt;br/&gt;
@@ -136,18 +144,6 @@ public long timestamp() &lt;/p&gt;
{
         return streamTime;
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void computeStreamTime() {&lt;/li&gt;
	&lt;li&gt;// we should always return the smallest timestamp of all partitions&lt;/li&gt;
	&lt;li&gt;// to avoid group partition time goes backward&lt;/li&gt;
	&lt;li&gt;long timestamp = Long.MAX_VALUE;&lt;/li&gt;
	&lt;li&gt;for (final RecordQueue queue : partitionQueues.values()) {&lt;/li&gt;
	&lt;li&gt;if (queue.timestamp() &amp;lt; timestamp) 
{
-                timestamp = queue.timestamp();
-            }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;this.streamTime = timestamp;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@throws IllegalStateException if the record&apos;s partition does not belong to this partition group&lt;br/&gt;
      */&lt;br/&gt;
@@ -165,7 +161,12 @@ int numBuffered() 
{
         return totalBuffered;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    boolean allPartitionsBuffered() &lt;/p&gt;
{
+        return allBuffered;
+    }
&lt;p&gt;+&lt;br/&gt;
     public void close() &lt;/p&gt;
{
+        clear();
         partitionQueues.clear();
     }

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index 7f121fe0df4..6f3b031b8db 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -59,6 +59,8 @@&lt;/p&gt;

&lt;p&gt;     private static final ConsumerRecord&amp;lt;Object, Object&amp;gt; DUMMY_RECORD = new ConsumerRecord&amp;lt;&amp;gt;(ProcessorContextImpl.NONEXIST_TOPIC, -1, -1L, null, null);&lt;/p&gt;

&lt;p&gt;+    private static final int WAIT_ON_PARTIAL_INPUT = 5;&lt;br/&gt;
+&lt;br/&gt;
     private final PartitionGroup partitionGroup;&lt;br/&gt;
     private final PartitionGroup.RecordInfo recordInfo;&lt;br/&gt;
     private final PunctuationQueue streamTimePunctuationQueue;&lt;br/&gt;
@@ -72,12 +74,14 @@&lt;br/&gt;
     private boolean commitRequested = false;&lt;br/&gt;
     private boolean commitOffsetNeeded = false;&lt;br/&gt;
     private boolean transactionInFlight = false;&lt;br/&gt;
+    private int waits = WAIT_ON_PARTIAL_INPUT;&lt;br/&gt;
     private final Time time;&lt;br/&gt;
     private final TaskMetrics taskMetrics;&lt;/p&gt;

&lt;p&gt;     protected static final class TaskMetrics &lt;/p&gt;
{
         final StreamsMetricsImpl metrics;
         final Sensor taskCommitTimeSensor;
+        final Sensor taskEnforcedProcessSensor;
         private final String taskName;
 
 
@@ -108,7 +112,7 @@
 
             // add the operation metrics with additional tags
             final Map&amp;lt;String, String&amp;gt; tagMap = metrics.tagMap(&quot;task-id&quot;, taskName);
-            taskCommitTimeSensor = metrics.taskLevelSensor(&quot;commit&quot;, taskName, Sensor.RecordingLevel.DEBUG, parent);
+            taskCommitTimeSensor = metrics.taskLevelSensor(taskName, &quot;commit&quot;, Sensor.RecordingLevel.DEBUG, parent);
             taskCommitTimeSensor.add(
                 new MetricName(&quot;commit-latency-avg&quot;, group, &quot;The average latency of commit operation.&quot;, tagMap),
                 new Avg()
@@ -125,6 +129,18 @@
                 new MetricName(&quot;commit-total&quot;, group, &quot;The total number of occurrence of commit operations.&quot;, tagMap),
                 new Count()
             );
+
+            // add the metrics for enforced processing
+            taskEnforcedProcessSensor = metrics.taskLevelSensor(taskName, &quot;enforced-process&quot;, Sensor.RecordingLevel.DEBUG, parent);
+            taskEnforcedProcessSensor.add(
+                    new MetricName(&quot;enforced-process-rate&quot;, group, &quot;The average number of occurrence of enforced-process per second.&quot;, tagMap),
+                    new Rate(TimeUnit.SECONDS, new Count())
+            );
+            taskEnforcedProcessSensor.add(
+                    new MetricName(&quot;enforced-process-total&quot;, group, &quot;The total number of occurrence of enforced-process operations.&quot;, tagMap),
+                    new Count()
+            );
+
         }

&lt;p&gt;         void removeAllSensors() {&lt;br/&gt;
@@ -263,6 +279,21 @@ public void resume() &lt;/p&gt;
{
         log.debug(&quot;Resuming&quot;);
     }

&lt;p&gt;+    /**&lt;br/&gt;
+     * An active task is processable if its buffer contains data for all of its input source topic partitions&lt;br/&gt;
+     */&lt;br/&gt;
+    public boolean isProcessable() {&lt;br/&gt;
+        if (partitionGroup.allPartitionsBuffered()) &lt;/p&gt;
{
+            return true;
+        }
&lt;p&gt; else if (partitionGroup.numBuffered() &amp;gt; 0 &amp;amp;&amp;amp; --waits &amp;lt; 0) &lt;/p&gt;
{
+            taskMetrics.taskEnforcedProcessSensor.record();
+            waits = WAIT_ON_PARTIAL_INPUT;
+            return true;
+        }
&lt;p&gt;+&lt;br/&gt;
+        return false;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Process one record.&lt;br/&gt;
      *&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index 428aa1d938b..42f55efbf63 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -831,18 +831,21 @@ long runOnce(final long recordsProcessedBeforeCommit) {&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (records != null &amp;amp;&amp;amp; !records.isEmpty() &amp;amp;&amp;amp; taskManager.hasActiveRunningTasks()) {&lt;br/&gt;
+        if (records != null &amp;amp;&amp;amp; !records.isEmpty()) 
{
             streamsMetrics.pollTimeSensor.record(computeLatency(), timerStartedMs);
             addRecordsToTasks(records);
+        }
&lt;p&gt;+&lt;br/&gt;
+        if (taskManager.hasActiveRunningTasks()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {             final long totalProcessed = processAndMaybeCommit(recordsProcessedBeforeCommit);             if (totalProcessed &amp;gt; 0) {
                 final long processLatency = computeLatency();
                 streamsMetrics.processTimeSensor.record(processLatency / (double) totalProcessed, timerStartedMs);
                 processedBeforeCommit = adjustRecordsProcessedBeforeCommit(
-                    recordsProcessedBeforeCommit,
-                    totalProcessed,
-                    processLatency,
-                    commitTimeMs);
+                        recordsProcessedBeforeCommit,
+                        totalProcessed,
+                        processLatency,
+                        commitTimeMs);
             }         }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java&lt;br/&gt;
index 662ded553ad..e99a5b3bb23 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java&lt;br/&gt;
@@ -88,13 +88,13 @@ public final void removeAllThreadLevelSensors() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     public final Sensor taskLevelSensor(final String taskName,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final String sensorName,&lt;/li&gt;
	&lt;li&gt;final Sensor.RecordingLevel recordingLevel,&lt;/li&gt;
	&lt;li&gt;final Sensor... parents) {&lt;br/&gt;
+                                        final String sensorName,&lt;br/&gt;
+                                        final Sensor.RecordingLevel recordingLevel,&lt;br/&gt;
+                                        final Sensor... parents) {&lt;br/&gt;
         final String key = threadName + &quot;.&quot; + taskName;&lt;br/&gt;
         synchronized (taskLevelSensors) {&lt;br/&gt;
             if (!taskLevelSensors.containsKey(key)) 
{
-                taskLevelSensors.put(key, new LinkedList&amp;lt;String&amp;gt;());
+                taskLevelSensors.put(key, new LinkedList&amp;lt;&amp;gt;());
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             final String fullSensorName = key + &quot;.&quot; + sensorName;&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java&lt;br/&gt;
index 77b9c1e8560..12b4cf30240 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java&lt;br/&gt;
@@ -370,7 +370,7 @@ private NamedCacheMetrics(final StreamsMetricsImpl metrics, final String cacheNa&lt;br/&gt;
                 &quot;record-cache-id&quot;, &quot;all&quot;,&lt;br/&gt;
                 &quot;task-id&quot;, taskName&lt;br/&gt;
             );&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Sensor taskLevelHitRatioSensor = metrics.taskLevelSensor(&quot;hitRatio&quot;, taskName, Sensor.RecordingLevel.DEBUG);&lt;br/&gt;
+            final Sensor taskLevelHitRatioSensor = metrics.taskLevelSensor(taskName, &quot;hitRatio&quot;, Sensor.RecordingLevel.DEBUG);&lt;br/&gt;
             taskLevelHitRatioSensor.add(&lt;br/&gt;
                 new MetricName(&quot;hitRatio-avg&quot;, group, &quot;The average cache hit ratio.&quot;, allMetricTags),&lt;br/&gt;
                 new Avg()&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
index 749d74887e5..1a78ed356bb 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
@@ -50,7 +50,9 @@&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Collection;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
+import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.concurrent.ExecutionException;&lt;br/&gt;
 import java.util.concurrent.Future;&lt;br/&gt;
@@ -350,6 +352,45 @@ public static void waitForCompletion(final KafkaStreams streams,&lt;br/&gt;
         return accumData;&lt;br/&gt;
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    public static &amp;lt;K, V&amp;gt; List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; waitUntilFinalKeyValueRecordsReceived(final Properties consumerConfig,&lt;br/&gt;
+                                                                                    final String topic,&lt;br/&gt;
+                                                                                    final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; expectedRecords) throws InterruptedException &lt;/p&gt;
{
+        return waitUntilFinalKeyValueRecordsReceived(consumerConfig, topic, expectedRecords, DEFAULT_TIMEOUT);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public static &amp;lt;K, V&amp;gt; List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; waitUntilFinalKeyValueRecordsReceived(final Properties consumerConfig,&lt;br/&gt;
+                                                                                    final String topic,&lt;br/&gt;
+                                                                                    final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; expectedRecords,&lt;br/&gt;
+                                                                                    final long waitTime) throws InterruptedException {&lt;br/&gt;
+        final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; accumData = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        try (final Consumer&amp;lt;K, V&amp;gt; consumer = createConsumer(consumerConfig)) {&lt;br/&gt;
+            final TestCondition valuesRead = new TestCondition() {&lt;br/&gt;
+                @Override&lt;br/&gt;
+                public boolean conditionMet() {&lt;br/&gt;
+                    final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; readData =&lt;br/&gt;
+                            readKeyValues(topic, consumer, waitTime, expectedRecords.size());&lt;br/&gt;
+                    accumData.addAll(readData);&lt;br/&gt;
+&lt;br/&gt;
+                    final Map&amp;lt;K, V&amp;gt; finalData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+                    for (final KeyValue&amp;lt;K, V&amp;gt; keyValue : accumData) &lt;/p&gt;
{
+                        finalData.put(keyValue.key, keyValue.value);
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    for (final KeyValue&amp;lt;K, V&amp;gt; keyValue : expectedRecords) &lt;/p&gt;
{
+                        if (!keyValue.value.equals(finalData.get(keyValue.key)))
+                            return false;
+                    }
&lt;p&gt;+&lt;br/&gt;
+                    return true;&lt;br/&gt;
+                }&lt;br/&gt;
+            };&lt;br/&gt;
+            final String conditionDetails = &quot;Did not receive all &quot; + expectedRecords + &quot; records from topic &quot; + topic;&lt;br/&gt;
+            TestUtils.waitForCondition(valuesRead, waitTime, conditionDetails);&lt;br/&gt;
+        }&lt;br/&gt;
+        return accumData;&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     public static &amp;lt;K, V&amp;gt; List&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; waitUntilMinRecordsReceived(final Properties consumerConfig,&lt;br/&gt;
                                                                                 final String topic,&lt;br/&gt;
                                                                                 final int expectedNumRecords,&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
index 8a8d6255c46..7efe653578f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
@@ -337,6 +337,7 @@ public void shouldCloseTaskOnMaybeCommitIfTaskMigratedException() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCloseTaskOnProcessesIfTaskMigratedException() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.isProcessable()).andReturn(true);&lt;br/&gt;
         t1.process();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new TaskMigratedException());&lt;br/&gt;
         t1.close(false, true);&lt;br/&gt;
@@ -353,6 +354,32 @@ public void shouldCloseTaskOnProcessesIfTaskMigratedException() &lt;/p&gt;
{
         EasyMock.verify(t1);
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldNotProcessUnprocessableTasks() &lt;/p&gt;
{
+        mockTaskInitialization();
+        EasyMock.expect(t1.isProcessable()).andReturn(false);
+        EasyMock.replay(t1);
+        addAndInitTask();
+
+        assertThat(assignedTasks.process(), equalTo(0));
+
+        EasyMock.verify(t1);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldAlwaysProcessProcessableTasks() &lt;/p&gt;
{
+        mockTaskInitialization();
+        EasyMock.expect(t1.isProcessable()).andReturn(true);
+        EasyMock.expect(t1.process()).andReturn(true).once();
+        EasyMock.replay(t1);
+
+        addAndInitTask();
+
+        assertThat(assignedTasks.process(), equalTo(1));
+
+        EasyMock.verify(t1);
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldPunctuateRunningTasks() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
index b3123e46343..2df4f66cb85 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java&lt;br/&gt;
@@ -88,12 +88,12 @@ public void testTimeTracking() {&lt;br/&gt;
         group.addRawRecords(partition2, list2);&lt;br/&gt;
         // 1:&lt;span class=&quot;error&quot;&gt;&amp;#91;1, 3, 5&amp;#93;&lt;/span&gt;&lt;br/&gt;
         // 2:&lt;span class=&quot;error&quot;&gt;&amp;#91;2, 4, 6&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// st: 1&lt;br/&gt;
+        // st: -1 since no records was being processed yet&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(6, group.numBuffered());&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(1L, group.timestamp());&lt;br/&gt;
+        assertEquals(-1L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         StampedRecord record;&lt;br/&gt;
         final PartitionGroup.RecordInfo info = new PartitionGroup.RecordInfo();&lt;br/&gt;
@@ -108,7 +108,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(5, group.numBuffered());&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(2L, group.timestamp());&lt;br/&gt;
+        assertEquals(1L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one record, now the time should be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -120,7 +120,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(4, group.numBuffered());&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, group.timestamp());&lt;br/&gt;
+        assertEquals(2L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add 2 more records with timestamp 2, 4 to partition-1&lt;br/&gt;
         final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; list3 = Arrays.asList(&lt;br/&gt;
@@ -134,7 +134,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(6, group.numBuffered());&lt;br/&gt;
         assertEquals(4, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(3L, group.timestamp());&lt;br/&gt;
+        assertEquals(2L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -146,7 +146,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(5, group.numBuffered());&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(2, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(4L, group.timestamp());&lt;br/&gt;
+        assertEquals(3L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one record, time should not be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -158,7 +158,7 @@ public void testTimeTracking() {&lt;br/&gt;
         assertEquals(4, group.numBuffered());&lt;br/&gt;
         assertEquals(3, group.numBuffered(partition1));&lt;br/&gt;
         assertEquals(1, group.numBuffered(partition2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(5L, group.timestamp());&lt;br/&gt;
+        assertEquals(4L, group.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // get one more record, now time should be advanced&lt;br/&gt;
         record = group.nextRecord(info);&lt;br/&gt;
@@ -206,7 +206,7 @@ public void testTimeTracking() &lt;/p&gt;
{
         assertEquals(0, group.numBuffered());
         assertEquals(0, group.numBuffered(partition1));
         assertEquals(0, group.numBuffered(partition2));
-        assertEquals(5L, group.timestamp());
+        assertEquals(6L, group.timestamp());
 
     }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
index bfbb2a00270..146bcb3b54e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
@@ -57,6 +57,7 @@&lt;/p&gt;

&lt;p&gt; import java.io.File;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
 import java.time.Duration;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
@@ -111,7 +112,7 @@ public void close() {&lt;/p&gt;

&lt;p&gt;     private final ProcessorTopology topology = ProcessorTopology.withSources(&lt;br/&gt;
         Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source2))&lt;br/&gt;
+        mkMap(mkEntry(topic1, source1), mkEntry(topic2, source2))&lt;br/&gt;
     );&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final MockConsumer&amp;lt;byte[], byte[]&amp;gt; consumer = new MockConsumer&amp;lt;&amp;gt;(OffsetResetStrategy.EARLIEST);&lt;br/&gt;
@@ -307,97 +308,6 @@ public void testPauseResume() &lt;/p&gt;
{
         assertEquals(0, consumer.paused().size());
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@SuppressWarnings(&quot;unchecked&quot;)&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testMaybePunctuateStreamTime() 
{
-        task = createStatelessTask(createConfig(false));
-        task.initializeStateStores();
-        task.initializeTopology();
-
-        task.addRecords(partition1, Arrays.asList(
-            getConsumerRecord(partition1, 0),
-            getConsumerRecord(partition1, 20),
-            getConsumerRecord(partition1, 32),
-            getConsumerRecord(partition1, 40),
-            getConsumerRecord(partition1, 60)
-        ));
-
-        task.addRecords(partition2, Arrays.asList(
-            getConsumerRecord(partition2, 25),
-            getConsumerRecord(partition2, 35),
-            getConsumerRecord(partition2, 45),
-            getConsumerRecord(partition2, 61)
-        ));
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(8, task.numBuffered());
-        assertEquals(1, source1.numReceived);
-        assertEquals(0, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(7, task.numBuffered());
-        assertEquals(2, source1.numReceived);
-        assertEquals(0, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(6, task.numBuffered());
-        assertEquals(2, source1.numReceived);
-        assertEquals(1, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(5, task.numBuffered());
-        assertEquals(3, source1.numReceived);
-        assertEquals(1, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(4, task.numBuffered());
-        assertEquals(3, source1.numReceived);
-        assertEquals(2, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(3, task.numBuffered());
-        assertEquals(4, source1.numReceived);
-        assertEquals(2, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(2, task.numBuffered());
-        assertEquals(4, source1.numReceived);
-        assertEquals(3, source2.numReceived);
-
-        assertTrue(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(1, task.numBuffered());
-        assertEquals(5, source1.numReceived);
-        assertEquals(3, source2.numReceived);
-
-        assertFalse(task.maybePunctuateStreamTime());
-
-        assertTrue(task.process());
-        assertEquals(0, task.numBuffered());
-        assertEquals(5, source1.numReceived);
-        assertEquals(4, source2.numReceived);
-
-        assertFalse(task.process());
-        assertFalse(task.maybePunctuateStreamTime());
-
-        processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 0L, 20L, 32L, 40L, 60L);
-    }
&lt;p&gt;-&lt;br/&gt;
     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldPunctuateOnceStreamTimeAfterGap() {&lt;br/&gt;
@@ -419,64 +329,67 @@ public void shouldPunctuateOnceStreamTimeAfterGap() {&lt;br/&gt;
             getConsumerRecord(partition2, 161)&lt;br/&gt;
         ));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(task.maybePunctuateStreamTime()); // punctuate at 20&lt;br/&gt;
+        // st: -1&lt;br/&gt;
+        assertFalse(task.maybePunctuateStreamTime()); // punctuate at 20&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        // st: 20&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(7, task.numBuffered());&lt;br/&gt;
         assertEquals(1, source1.numReceived);&lt;br/&gt;
         assertEquals(0, source2.numReceived);&lt;br/&gt;
+        assertTrue(task.maybePunctuateStreamTime());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertFalse(task.maybePunctuateStreamTime());&lt;br/&gt;
-&lt;br/&gt;
+        // st: 25&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(6, task.numBuffered());&lt;br/&gt;
         assertEquals(1, source1.numReceived);&lt;br/&gt;
         assertEquals(1, source2.numReceived);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;assertTrue(task.maybePunctuateStreamTime()); // punctuate at 142&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// only one punctuation after 100ms gap&lt;br/&gt;
         assertFalse(task.maybePunctuateStreamTime());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        // st: 142&lt;br/&gt;
+        // punctuate at 142&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(5, task.numBuffered());&lt;br/&gt;
         assertEquals(2, source1.numReceived);&lt;br/&gt;
         assertEquals(1, source2.numReceived);&lt;br/&gt;
+        assertTrue(task.maybePunctuateStreamTime());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertFalse(task.maybePunctuateStreamTime());&lt;br/&gt;
-&lt;br/&gt;
+        // st: 145&lt;br/&gt;
+        // only one punctuation after 100ms gap&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(4, task.numBuffered());&lt;br/&gt;
         assertEquals(2, source1.numReceived);&lt;br/&gt;
         assertEquals(2, source2.numReceived);&lt;br/&gt;
+        assertFalse(task.maybePunctuateStreamTime());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(task.maybePunctuateStreamTime()); // punctuate at 155&lt;br/&gt;
-&lt;br/&gt;
+        // st: 155&lt;br/&gt;
+        // punctuate at 155&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(3, task.numBuffered());&lt;br/&gt;
         assertEquals(3, source1.numReceived);&lt;br/&gt;
         assertEquals(2, source2.numReceived);&lt;br/&gt;
+        assertTrue(task.maybePunctuateStreamTime());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertFalse(task.maybePunctuateStreamTime());&lt;br/&gt;
-&lt;br/&gt;
+        // st: 159&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(2, task.numBuffered());&lt;br/&gt;
         assertEquals(3, source1.numReceived);&lt;br/&gt;
         assertEquals(3, source2.numReceived);&lt;br/&gt;
+        assertFalse(task.maybePunctuateStreamTime());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(task.maybePunctuateStreamTime()); // punctuate at 160, still aligned on the initial punctuation&lt;br/&gt;
-&lt;br/&gt;
+        // st: 160, aligned at 0&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(1, task.numBuffered());&lt;br/&gt;
         assertEquals(4, source1.numReceived);&lt;br/&gt;
         assertEquals(3, source2.numReceived);&lt;br/&gt;
+        assertTrue(task.maybePunctuateStreamTime());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertFalse(task.maybePunctuateStreamTime());&lt;br/&gt;
-&lt;br/&gt;
+        // st: 161&lt;br/&gt;
         assertTrue(task.process());&lt;br/&gt;
         assertEquals(0, task.numBuffered());&lt;br/&gt;
         assertEquals(4, source1.numReceived);&lt;br/&gt;
         assertEquals(4, source2.numReceived);&lt;br/&gt;
+        assertFalse(task.maybePunctuateStreamTime());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertFalse(task.process());&lt;br/&gt;
         assertFalse(task.maybePunctuateStreamTime());&lt;br/&gt;
@@ -484,9 +397,8 @@ public void shouldPunctuateOnceStreamTimeAfterGap() &lt;/p&gt;
{
         processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L, 142L, 155L, 160L);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;/li&gt;
	&lt;li&gt;public void testCancelPunctuateStreamTime() {&lt;br/&gt;
+    public void shouldRespectPunctuateCancellationStreamTime() {&lt;br/&gt;
         task = createStatelessTask(createConfig(false));&lt;br/&gt;
         task.initializeStateStores();&lt;br/&gt;
         task.initializeTopology();&lt;br/&gt;
@@ -503,12 +415,19 @@ public void testCancelPunctuateStreamTime() {&lt;br/&gt;
             getConsumerRecord(partition2, 45)&lt;br/&gt;
         ));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        assertFalse(task.maybePunctuateStreamTime());&lt;br/&gt;
+&lt;br/&gt;
+        // st is now 20&lt;br/&gt;
+        assertTrue(task.process());&lt;br/&gt;
+&lt;br/&gt;
         assertTrue(task.maybePunctuateStreamTime());&lt;/p&gt;

&lt;p&gt;+        // st is now 25&lt;br/&gt;
         assertTrue(task.process());&lt;/p&gt;

&lt;p&gt;         assertFalse(task.maybePunctuateStreamTime());&lt;/p&gt;

&lt;p&gt;+        // st is now 30&lt;br/&gt;
         assertTrue(task.process());&lt;/p&gt;

&lt;p&gt;         processorStreamTime.mockProcessor.scheduleCancellable.cancel();&lt;br/&gt;
@@ -518,6 +437,61 @@ public void testCancelPunctuateStreamTime() &lt;/p&gt;
{
         processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L);
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldRespectPunctuateCancellationSystemTime() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+        final long now = time.milliseconds();
+        time.sleep(10);
+        assertTrue(task.maybePunctuateSystemTime());
+        processorSystemTime.mockProcessor.scheduleCancellable.cancel();
+        time.sleep(10);
+        assertFalse(task.maybePunctuateSystemTime());
+        processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 10);
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldBeProcessableIfAllPartitionsBuffered() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+
+        assertFalse(task.isProcessable());
+
+        final byte[] bytes = ByteBuffer.allocate(4).putInt(1).array();
+
+        task.addRecords(partition1, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, bytes, bytes)));
+
+        assertFalse(task.isProcessable());
+
+        task.addRecords(partition2, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic2, 1, 0, bytes, bytes)));
+
+        assertTrue(task.isProcessable());
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldBeProcessableIfWaitedForTooLong() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+
+        assertFalse(task.isProcessable());
+
+        final byte[] bytes = ByteBuffer.allocate(4).putInt(1).array();
+
+        task.addRecords(partition1, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, bytes, bytes)));
+
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable());
+
+        assertTrue(task.isProcessable());
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldPunctuateSystemTimeWhenIntervalElapsed() {&lt;br/&gt;
         task = createStatelessTask(createConfig(false));&lt;br/&gt;
@@ -575,20 +549,6 @@ public void shouldPunctuateOnceSystemTimeAfterGap() &lt;/p&gt;
{
         processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 100, now + 110, now + 122, now + 130, now + 235, now + 240);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testCancelPunctuateSystemTime() 
{
-        task = createStatelessTask(createConfig(false));
-        task.initializeStateStores();
-        task.initializeTopology();
-        final long now = time.milliseconds();
-        time.sleep(10);
-        assertTrue(task.maybePunctuateSystemTime());
-        processorSystemTime.mockProcessor.scheduleCancellable.cancel();
-        time.sleep(10);
-        assertFalse(task.maybePunctuateSystemTime());
-        processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 10);
-    }
&lt;p&gt;-&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldWrapKafkaExceptionsWithStreamsExceptionAndAddContext() {&lt;br/&gt;
         task = createTaskThatThrowsException();&lt;br/&gt;
@@ -1110,7 +1070,7 @@ private StreamTask createStatefulTaskThatThrowsExceptionOnClose() {&lt;br/&gt;
     private StreamTask createStatelessTask(final StreamsConfig streamsConfig) {&lt;br/&gt;
         final ProcessorTopology topology = ProcessorTopology.withSources(&lt;br/&gt;
             Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source2))&lt;br/&gt;
+            mkMap(mkEntry(topic1, source1), mkEntry(topic2, source2))&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         source1.addChild(processorStreamTime);&lt;br/&gt;
diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
index 32ad793bc84..cf87eb5e27d 100644&lt;br/&gt;
&amp;#8212; a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
+++ b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestBase.scala&lt;br/&gt;
@@ -24,11 +24,7 @@ import org.apache.kafka.common.serialization._&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime&lt;br/&gt;
 import org.apache.kafka.streams._&lt;br/&gt;
 import org.apache.kafka.streams.integration.utils.&lt;/p&gt;
{EmbeddedKafkaCluster, IntegrationTestUtils}
&lt;p&gt;-import org.apache.kafka.streams.processor.internals.StreamThread&lt;br/&gt;
-import org.apache.kafka.streams.scala.ImplicitConversions._&lt;br/&gt;
-import org.apache.kafka.streams.scala.kstream._&lt;br/&gt;
 import org.apache.kafka.test.TestUtils&lt;br/&gt;
-import org.junit.Assert._&lt;br/&gt;
 import org.junit._&lt;br/&gt;
 import org.junit.rules.TemporaryFolder&lt;br/&gt;
 import org.scalatest.junit.JUnitSuite&lt;br/&gt;
@@ -129,9 +125,9 @@ class StreamToTableJoinScalaIntegrationTestBase extends JUnitSuite with StreamTo&lt;br/&gt;
       // consume and verify result&lt;br/&gt;
       val consumerConfig = getConsumerConfig()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig,&lt;/li&gt;
	&lt;li&gt;outputTopic,&lt;/li&gt;
	&lt;li&gt;expectedClicksPerRegion.size)&lt;br/&gt;
+      IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig,&lt;br/&gt;
+                                                                 outputTopic,&lt;br/&gt;
+                                                                 expectedClicksPerRegion.asJava)&lt;br/&gt;
     } else 
{
       java.util.Collections.emptyList()
     }
&lt;p&gt;diff --git a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
index e5253f95d45..3d1bab5d086 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
+++ b/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala&lt;br/&gt;
@@ -82,9 +82,6 @@ class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends StreamToTableJ&lt;br/&gt;
     val actualClicksPerRegion: java.util.List[KeyValue&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt;] =&lt;br/&gt;
       produceNConsume(userClicksTopic, userRegionsTopic, outputTopic)&lt;br/&gt;
     streams.close()&lt;br/&gt;
-&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;import collection.JavaConverters._&lt;/li&gt;
	&lt;li&gt;assertEquals(actualClicksPerRegion.asScala.sortBy(&lt;em&gt;.key), expectedClicksPerRegion.sortBy(&lt;/em&gt;.key))&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test def testShouldCountClicksPerRegionJava(): Unit = &lt;/p&gt;
{
@@ -149,6 +146,5 @@ class StreamToTableJoinScalaIntegrationTestImplicitSerdes extends StreamToTableJ
       produceNConsume(userClicksTopicJ, userRegionsTopicJ, outputTopicJ)
 
     streams.close()
-    assertEquals(actualClicksPerRegion.asScala.sortBy(_.key), expectedClicksPerRegion.sortBy(_.key))
   }
&lt;p&gt; }&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16568763" author="githubbot" created="Fri, 3 Aug 2018 21:08:17 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5458: &lt;span class=&quot;error&quot;&gt;&amp;#91;NOT MERGE&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;, Documentations: Add out of ordering in concepts.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5458&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5458&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   *More detailed description of your change,&lt;br/&gt;
   if necessary. The PR title and PR message become&lt;br/&gt;
   the squashed commit message, so use a separate&lt;br/&gt;
   comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   for the feature or bug fix. Unit and/or integration&lt;br/&gt;
   tests are expected for any behaviour change and&lt;br/&gt;
   system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16584350" author="githubbot" created="Fri, 17 Aug 2018 20:28:29 +0000"  >&lt;p&gt;mjsax closed pull request #5458: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;, Documentations: Add out of ordering in concepts.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5458&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5458&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/docs/streams/core-concepts.html b/docs/streams/core-concepts.html&lt;br/&gt;
index 3f9eab57ecd..e84d3d362a1 100644&lt;br/&gt;
&amp;#8212; a/docs/streams/core-concepts.html&lt;br/&gt;
+++ b/docs/streams/core-concepts.html&lt;br/&gt;
@@ -172,6 +172,37 @@ &amp;lt;h2&amp;gt;&amp;lt;a id=&quot;streams_processing_guarantee&quot; href=&quot;#streams_processing_guarantee&quot;&amp;gt;Pr&lt;br/&gt;
         More details can be found in the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation#streamsconfigs&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams Configs&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; section.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;+    &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_out_of_ordering&quot; href=&quot;#streams_out_of_ordering&quot;&amp;gt;Out-of-Order Handling&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;p&amp;gt;&lt;br/&gt;
+        Besides the guarantee that each record will be processed exactly-once, another issue that many stream processing application will face is how to&lt;br/&gt;
+        handle &amp;lt;a href=&quot;tbd&quot;&amp;gt;out-of-order data&amp;lt;/a&amp;gt; that may impact their business logic. In Kafka Streams, there are two causes that could potentially&lt;br/&gt;
+        result in out-of-order data arrivals with respect to their timestamps:&lt;br/&gt;
+    &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; Within a topic-partition, a record&apos;s timestamp may not be monotonically increasing along with their offsets. Since Kafka Streams will always try to process records within a topic-partition to follow the offset order, it can cause records with larger timestamps&lt;br/&gt;
+             to be processed earlier than records with smaller timestamp in the same topic-partition.&lt;br/&gt;
+        &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; Within a &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/architecture#streams_architecture_tasks&quot;&amp;gt;stream task&amp;lt;/a&amp;gt; that may be processing multiple topic-partitions, if users configure the application to not wait for all partitions to contain some buffered data and&lt;br/&gt;
+             pick from the partition with the smallest timestamp to process the next record, then later on when some records are fetched for other topic-partitions, their timestamps may be smaller than those processed records fetched from another topic-partition.&lt;br/&gt;
+        &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;p&amp;gt;&lt;br/&gt;
+        For stateless operations, out-of-order data will not impact processing logic since only one record is considered at a time, without looking into the history of past processed records;&lt;br/&gt;
+        for stateful operations such as aggregations and joins, however, out-of-order data could cause the processing logic to be incorrect. If users want to handle such out-of-order data, generally they need to allow their applications&lt;br/&gt;
+        to wait for longer time while bookkeeping their states during the wait time, i.e. making trade-off decisions between latency, cost, and correctness.&lt;br/&gt;
+        In Kafka Streams specifically, users can configure their window operators for aggregations to achieve such trade-offs (details can be found in &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/developer-guide&quot;&amp;gt;&amp;lt;b&amp;gt;Developer Guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As for Joins, users have to be aware that some of the out-of-order data cannot be handled by increasing on latency and cost in Streams yet:&lt;br/&gt;
+    &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; For Stream-Stream joins, all three types (inner, outer, left) handle out-of-order records correctly, but the resulted stream may contain unnecessary leftRecord-null for left joins, and leftRecord-null or null-rightRecord for outer joins. &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; For Stream-Table joins, out-of-order records are not handled (i.e., Streams applications don&apos;t check for out-of-order records and just process all records in offset order), and hence it may produce unpredictable results. &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; For Table-Table joins, out-of-order records are not handled (i.e., Streams applications don&apos;t check for out-of-order records and just process all records in offset order). However, the join result is a changelog stream and hence will be eventual consistent. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
     &amp;lt;div class=&quot;pagination&quot;&amp;gt;&lt;br/&gt;
         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/tutorial&quot; class=&quot;pagination_&lt;em&gt;btn pagination&lt;/em&gt;&lt;em&gt;btn&lt;/em&gt;_prev&quot;&amp;gt;Previous&amp;lt;/a&amp;gt;&lt;br/&gt;
         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/architecture&quot; class=&quot;pagination_&lt;em&gt;btn pagination&lt;/em&gt;&lt;em&gt;btn&lt;/em&gt;_next&quot;&amp;gt;Next&amp;lt;/a&amp;gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16584351" author="githubbot" created="Fri, 17 Aug 2018 20:28:35 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5458: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;, Documentations: Add out of ordering in concepts.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5458&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5458&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   *More detailed description of your change,&lt;br/&gt;
   if necessary. The PR title and PR message become&lt;br/&gt;
   the squashed commit message, so use a separate&lt;br/&gt;
   comment to ping reviewers.*&lt;/p&gt;

&lt;p&gt;   *Summary of testing strategy (including rationale)&lt;br/&gt;
   for the feature or bug fix. Unit and/or integration&lt;br/&gt;
   tests are expected for any behaviour change and&lt;br/&gt;
   system tests should be considered for larger changes.*&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16611366" author="githubbot" created="Tue, 11 Sep 2018 23:16:56 +0000"  >&lt;p&gt;guozhangwang closed pull request #5428: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Part III, Refactor StreamThread main loop&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5428&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5428&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java b/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java&lt;br/&gt;
index 54d065ff3c2..1fcabca134c 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java&lt;br/&gt;
@@ -499,7 +499,7 @@ public ConsumerConfig(Map&amp;lt;String, Object&amp;gt; props) &lt;/p&gt;
{
         super(CONFIG, props);
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ConsumerConfig(Map&amp;lt;?, ?&amp;gt; props, boolean doLog) {&lt;br/&gt;
+    protected ConsumerConfig(Map&amp;lt;?, ?&amp;gt; props, boolean doLog) 
{
         super(CONFIG, props, doLog);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
index b9eaaa68f67..736e9cbd34f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java&lt;br/&gt;
@@ -278,7 +278,7 @@&lt;br/&gt;
     /** &lt;/p&gt;
{@code buffered.records.per.partition}
&lt;p&gt; */&lt;br/&gt;
     @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String BUFFERED_RECORDS_PER_PARTITION_CONFIG = &quot;buffered.records.per.partition&quot;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final String BUFFERED_RECORDS_PER_PARTITION_DOC = &quot;The maximum number of records to buffer per partition.&quot;;&lt;br/&gt;
+    private static final String BUFFERED_RECORDS_PER_PARTITION_DOC = &quot;Maximum number of records to buffer per partition.&quot;;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /** &lt;/p&gt;
{@code cache.max.bytes.buffering}
&lt;p&gt; */&lt;br/&gt;
     @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
@@ -298,6 +298,11 @@&lt;br/&gt;
         &quot; (Note, if &amp;lt;code&amp;gt;processing.guarantee&amp;lt;/code&amp;gt; is set to &amp;lt;code&amp;gt;&quot; + EXACTLY_ONCE + &quot;&amp;lt;/code&amp;gt;, the default value is &amp;lt;code&amp;gt;&quot; + EOS_DEFAULT_COMMIT_INTERVAL_MS + &quot;&amp;lt;/code&amp;gt;,&quot; +&lt;br/&gt;
         &quot; otherwise the default value is &amp;lt;code&amp;gt;&quot; + DEFAULT_COMMIT_INTERVAL_MS + &quot;&amp;lt;/code&amp;gt;.&quot;;&lt;/p&gt;

&lt;p&gt;+    /** &lt;/p&gt;
{@code max.task.idle.ms}
&lt;p&gt; */&lt;br/&gt;
+    public static final String MAX_TASK_IDLE_MS_CONFIG = &quot;max.task.idle.ms&quot;;&lt;br/&gt;
+    private static final String MAX_TASK_IDLE_MS_DOC = &quot;Maximum amount of time a stream task will stay idle when not all of its partition buffers contain records,&quot; +&lt;br/&gt;
+        &quot; to avoid potential out-of-order record processing across multiple input streams.&quot;;&lt;br/&gt;
+&lt;br/&gt;
     /** &lt;/p&gt;
{@code connections.max.idle.ms}
&lt;p&gt; */&lt;br/&gt;
     @SuppressWarnings(&quot;WeakerAccess&quot;)&lt;br/&gt;
     public static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = CommonClientConfigs.CONNECTIONS_MAX_IDLE_MS_CONFIG;&lt;br/&gt;
@@ -538,6 +543,11 @@&lt;br/&gt;
                     1,&lt;br/&gt;
                     Importance.MEDIUM,&lt;br/&gt;
                     NUM_STREAM_THREADS_DOC)&lt;br/&gt;
+            .define(MAX_TASK_IDLE_MS_CONFIG,&lt;br/&gt;
+                    Type.LONG,&lt;br/&gt;
+                    0L,&lt;br/&gt;
+                    Importance.MEDIUM,&lt;br/&gt;
+                    MAX_TASK_IDLE_MS_DOC)&lt;br/&gt;
             .define(PROCESSING_GUARANTEE_CONFIG,&lt;br/&gt;
                     Type.STRING,&lt;br/&gt;
                     AT_LEAST_ONCE,&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index 94e4c71d9c2..8eb024cc670 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -51,9 +51,11 @@&lt;br/&gt;
     final boolean eosEnabled;&lt;br/&gt;
     final Logger log;&lt;br/&gt;
     final LogContext logContext;&lt;br/&gt;
+    final StateDirectory stateDirectory;&lt;br/&gt;
+&lt;br/&gt;
     boolean taskInitialized;&lt;br/&gt;
     boolean taskClosed;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final StateDirectory stateDirectory;&lt;br/&gt;
+    boolean commitNeeded;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     InternalProcessorContext processorContext;&lt;/p&gt;

&lt;p&gt;@@ -267,6 +269,10 @@ public boolean isClosed() &lt;/p&gt;
{
         return taskClosed;
     }

&lt;p&gt;+    public boolean commitNeeded() &lt;/p&gt;
{
+        return commitNeeded;
+    }
&lt;p&gt;+&lt;br/&gt;
     public boolean hasStateStores() &lt;/p&gt;
{
         return !topology.stateStores().isEmpty();
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
index 0a839655748..1eb3ab97bf8 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java&lt;br/&gt;
@@ -21,37 +21,14 @@&lt;br/&gt;
 import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
 import org.apache.kafka.streams.errors.TaskMigratedException;&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
-import org.slf4j.Logger;&lt;/p&gt;

&lt;p&gt; import java.util.HashMap;&lt;br/&gt;
 import java.util.Iterator;&lt;br/&gt;
 import java.util.Map;&lt;/p&gt;

&lt;p&gt; class AssignedStreamsTasks extends AssignedTasks&amp;lt;StreamTask&amp;gt; implements RestoringTasks {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Logger log;&lt;/li&gt;
	&lt;li&gt;private final TaskAction&amp;lt;StreamTask&amp;gt; maybeCommitAction;&lt;/li&gt;
	&lt;li&gt;private int committed = 0;&lt;br/&gt;
-&lt;br/&gt;
     AssignedStreamsTasks(final LogContext logContext) {&lt;br/&gt;
         super(logContext, &quot;stream task&quot;);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;this.log = logContext.logger(getClass());&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;maybeCommitAction = new TaskAction&amp;lt;StreamTask&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public String name() 
{
-                return &quot;maybeCommit&quot;;
-            }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void apply(final StreamTask task) {&lt;/li&gt;
	&lt;li&gt;if (task.commitNeeded()) {&lt;/li&gt;
	&lt;li&gt;committed++;&lt;/li&gt;
	&lt;li&gt;task.commit();&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;Committed active task {} per user request in&quot;, task.id());&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;};&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
@@ -63,9 +40,41 @@ public StreamTask restoringTaskFor(final TopicPartition partition) {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if committing offsets failed (non-EOS)&lt;/li&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int maybeCommit() {&lt;/li&gt;
	&lt;li&gt;committed = 0;&lt;/li&gt;
	&lt;li&gt;applyToRunningTasks(maybeCommitAction);&lt;br/&gt;
+    int maybeCommitPerUserRequested() {&lt;br/&gt;
+        int committed = 0;&lt;br/&gt;
+        RuntimeException firstException = null;&lt;br/&gt;
+&lt;br/&gt;
+        for (final Iterator&amp;lt;StreamTask&amp;gt; it = running().iterator(); it.hasNext(); ) {&lt;br/&gt;
+            final StreamTask task = it.next();&lt;br/&gt;
+            try {&lt;br/&gt;
+                if (task.commitRequested() &amp;amp;&amp;amp; task.commitNeeded()) {&lt;br/&gt;
+                    task.commit();&lt;br/&gt;
+                    committed++;&lt;br/&gt;
+                    log.debug(&quot;Committed active task {} per user request in&quot;, task.id());&lt;br/&gt;
+                }&lt;br/&gt;
+            } catch (final TaskMigratedException e) {&lt;br/&gt;
+                log.info(&quot;Failed to commit {} since it got migrated to another thread already. &quot; +&lt;br/&gt;
+                        &quot;Closing it as zombie before triggering a new rebalance.&quot;, task.id());&lt;br/&gt;
+                final RuntimeException fatalException = closeZombieTask(task);&lt;br/&gt;
+                if (fatalException != null) 
{
+                    throw fatalException;
+                }
&lt;p&gt;+                it.remove();&lt;br/&gt;
+                throw e;&lt;br/&gt;
+            } catch (final RuntimeException t) {&lt;br/&gt;
+                log.error(&quot;Failed to commit StreamTask {} due to the following error:&quot;,&lt;br/&gt;
+                        task.id(),&lt;br/&gt;
+                        t);&lt;br/&gt;
+                if (firstException == null) &lt;/p&gt;
{
+                    firstException = t;
+                }
&lt;p&gt;+            }&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        if (firstException != null) &lt;/p&gt;
{
+            throw firstException;
+        }
&lt;p&gt;+&lt;br/&gt;
         return committed;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -85,15 +94,14 @@ int maybeCommit() {&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if the task producer got fenced (EOS only)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int process() {&lt;br/&gt;
+    int process(final long now) {&lt;br/&gt;
         int processed = 0;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Iterator&amp;lt;Map.Entry&amp;lt;TaskId, StreamTask&amp;gt;&amp;gt; it = running.entrySet().iterator();&lt;br/&gt;
         while (it.hasNext()) {&lt;br/&gt;
             final StreamTask task = it.next().getValue();&lt;br/&gt;
-&lt;br/&gt;
             try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (task.isProcessable() &amp;amp;&amp;amp; task.process()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                if (task.isProcessable(now) &amp;amp;&amp;amp; task.process()) {
                     processed++;
                 }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; catch (final TaskMigratedException e) {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
index 079d405cb50..3cc396df8f3 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java&lt;br/&gt;
@@ -37,14 +37,14 @@&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicReference;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; abstract class AssignedTasks&amp;lt;T extends Task&amp;gt; {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Logger log;&lt;br/&gt;
+    final Logger log;&lt;br/&gt;
     private final String taskTypeName;&lt;/li&gt;
	&lt;li&gt;private final TaskAction&amp;lt;T&amp;gt; commitAction;&lt;br/&gt;
     private final Map&amp;lt;TaskId, T&amp;gt; created = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Map&amp;lt;TaskId, T&amp;gt; suspended = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Map&amp;lt;TaskId, T&amp;gt; restoring = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Set&amp;lt;TopicPartition&amp;gt; restoredPartitions = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Set&amp;lt;TaskId&amp;gt; previousActiveTasks = new HashSet&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
     // IQ may access this map.&lt;br/&gt;
     final Map&amp;lt;TaskId, T&amp;gt; running = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, T&amp;gt; runningByPartition = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -53,20 +53,7 @@&lt;br/&gt;
     AssignedTasks(final LogContext logContext,&lt;br/&gt;
                   final String taskTypeName) {&lt;br/&gt;
         this.taskTypeName = taskTypeName;&lt;br/&gt;
-&lt;br/&gt;
         this.log = logContext.logger(getClass());&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;commitAction = new TaskAction&amp;lt;T&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public String name() 
{
-                return &quot;commit&quot;;
-            }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void apply(final T task) 
{
-                task.commit();
-            }&lt;/li&gt;
	&lt;li&gt;};&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     void addNewTask(final T task) {&lt;br/&gt;
@@ -349,17 +336,16 @@ void clear() {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;br/&gt;
     int commit() 
{
-        applyToRunningTasks(commitAction);
-        return running.size();
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void applyToRunningTasks(final TaskAction&amp;lt;T&amp;gt; action) {&lt;br/&gt;
+        int committed = 0;&lt;br/&gt;
         RuntimeException firstException = null;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         for (final Iterator&amp;lt;T&amp;gt; it = running().iterator(); it.hasNext(); ) {&lt;br/&gt;
             final T task = it.next();&lt;br/&gt;
             try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;action.apply(task);&lt;br/&gt;
+                if (task.commitNeeded()) 
{
+                    task.commit();
+                    committed++;
+                }
&lt;p&gt;             } catch (final TaskMigratedException e) {&lt;br/&gt;
                 log.info(&quot;Failed to commit {} {} since it got migrated to another thread already. &quot; +&lt;br/&gt;
                         &quot;Closing it as zombie before triggering a new rebalance.&quot;, taskTypeName, task.id());&lt;br/&gt;
@@ -370,11 +356,10 @@ void applyToRunningTasks(final TaskAction&amp;lt;T&amp;gt; action) &lt;/p&gt;
{
                 it.remove();
                 throw e;
             }
&lt;p&gt; catch (final RuntimeException t) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;log.error(&quot;Failed to {} {} {} due to the following error:&quot;,&lt;/li&gt;
	&lt;li&gt;action.name(),&lt;/li&gt;
	&lt;li&gt;taskTypeName,&lt;/li&gt;
	&lt;li&gt;task.id(),&lt;/li&gt;
	&lt;li&gt;t);&lt;br/&gt;
+                log.error(&quot;Failed to commit {} {} due to the following error:&quot;,&lt;br/&gt;
+                        taskTypeName,&lt;br/&gt;
+                        task.id(),&lt;br/&gt;
+                        t);&lt;br/&gt;
                 if (firstException == null) 
{
                     firstException = t;
                 }
&lt;p&gt;@@ -384,6 +369,8 @@ void applyToRunningTasks(final TaskAction&amp;lt;T&amp;gt; action) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         if (firstException != null) {
             throw firstException;
         }++        return committed;     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     void closeNonAssignedSuspendedTasks(final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; newAssignment) {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
index f17c63acd2f..70202534d69 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java&lt;br/&gt;
@@ -66,7 +66,7 @@ RecordQueue queue() &lt;/p&gt;
{
         this.partitionQueues = partitionQueues;
         totalBuffered = 0;
         allBuffered = false;
-        streamTime = RecordQueue.NOT_KNOWN;
+        streamTime = RecordQueue.UNKNOWN;
     }

&lt;p&gt;     /**&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java&lt;br/&gt;
index beab35f61c5..ee21379c79f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java&lt;br/&gt;
@@ -146,7 +146,7 @@ public StateStore getStateStore(final String name) {&lt;/p&gt;

&lt;p&gt;     @Override&lt;br/&gt;
     public void commit() &lt;/p&gt;
{
-        task.needCommit();
+        task.requestCommit();
     }

&lt;p&gt;     @Override&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
index 76d1a0c8f4d..15a5c212bfb 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
@@ -318,6 +318,7 @@ private BatchingStateRestoreCallback getBatchingRestoreCallback(final StateResto&lt;br/&gt;
             return (BatchingStateRestoreCallback) callback;&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;+        // TODO: avoid creating a new object for each update call?&lt;br/&gt;
         return new WrappedBatchingStateRestoreCallback(callback);&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java&lt;br/&gt;
index 86340bb82c1..d06d7f3bad6 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java&lt;br/&gt;
@@ -36,7 +36,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 public class RecordQueue {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;static final long NOT_KNOWN = -1L;&lt;br/&gt;
+    static final long UNKNOWN = -1L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final Logger log;&lt;br/&gt;
     private final SourceNode source;&lt;br/&gt;
@@ -46,7 +46,7 @@&lt;br/&gt;
     private final RecordDeserializer recordDeserializer;&lt;br/&gt;
     private final ArrayDeque&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; fifoQueue;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long partitionTime = NOT_KNOWN;&lt;br/&gt;
+    private long partitionTime = UNKNOWN;&lt;br/&gt;
     private StampedRecord headRecord = null;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     RecordQueue(final TopicPartition partition,&lt;br/&gt;
@@ -151,7 +151,7 @@ public long timestamp() {&lt;br/&gt;
     public void clear() &lt;/p&gt;
{
         fifoQueue.clear();
         headRecord = null;
-        partitionTime = NOT_KNOWN;
+        partitionTime = UNKNOWN;
     }

&lt;p&gt;     private void maybeUpdateTimestamp() {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
index 67834d7a7cd..e8631aaf85a 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java&lt;br/&gt;
@@ -74,7 +74,7 @@ public void close() {}&lt;br/&gt;
         }&lt;br/&gt;
     };&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long streamTime = RecordQueue.NOT_KNOWN;&lt;br/&gt;
+    private long streamTime = RecordQueue.UNKNOWN;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     StandbyContextImpl(final TaskId id,&lt;br/&gt;
                        final StreamsConfig config,&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java&lt;br/&gt;
index 72cc6295a2d..3ac64146187 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java&lt;br/&gt;
@@ -103,6 +103,8 @@ public void commit() &lt;/p&gt;
{
         flushAndCheckpointState();
         // reinitialize offset limits
         updateOffsetLimits();
+
+        commitNeeded = false;
     }

&lt;p&gt;     /**&lt;br/&gt;
@@ -185,6 +187,11 @@ public void closeSuspended(final boolean clean,&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;         stateMgr.updateStandbyStates(partition, restoreRecords, lastOffset);&lt;br/&gt;
+&lt;br/&gt;
+        if (!restoreRecords.isEmpty()) &lt;/p&gt;
{
+            commitNeeded = true;
+        }&lt;br/&gt;
+&lt;br/&gt;
         return remainingRecords;&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index 7f3d31fd776..2f97b7f27ba 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -60,26 +60,23 @@&lt;br/&gt;
 &lt;br/&gt;
     private static final ConsumerRecord&amp;lt;Object, Object&amp;gt; DUMMY_RECORD = new ConsumerRecord&amp;lt;&amp;gt;(ProcessorContextImpl.NONEXIST_TOPIC, -1, -1L, null, null);&lt;br/&gt;
 &lt;br/&gt;
-    private static final int WAIT_ON_PARTIAL_INPUT = 5;&lt;br/&gt;
-&lt;br/&gt;
+    private final Time time;&lt;br/&gt;
+    private final long maxTaskIdleMs;&lt;br/&gt;
+    private final int maxBufferedSize;&lt;br/&gt;
+    private final TaskMetrics taskMetrics;&lt;br/&gt;
     private final PartitionGroup partitionGroup;&lt;br/&gt;
+    private final RecordCollector recordCollector;&lt;br/&gt;
     private final PartitionGroup.RecordInfo recordInfo;&lt;br/&gt;
+    private final Map&amp;lt;TopicPartition, Long&amp;gt; consumedOffsets;&lt;br/&gt;
     private final PunctuationQueue streamTimePunctuationQueue;&lt;br/&gt;
     private final PunctuationQueue systemTimePunctuationQueue;&lt;br/&gt;
-&lt;br/&gt;
-    private final Map&amp;lt;TopicPartition, Long&amp;gt; consumedOffsets;&lt;br/&gt;
-    private final RecordCollector recordCollector;&lt;br/&gt;
     private final ProducerSupplier producerSupplier;&lt;br/&gt;
-    private Producer&amp;lt;byte[], byte[]&amp;gt; producer;&lt;br/&gt;
-    private final int maxBufferedSize;&lt;br/&gt;
 &lt;br/&gt;
+    private Sensor closeSensor;&lt;br/&gt;
+    private long idleStartTime;&lt;br/&gt;
+    private Producer&amp;lt;byte[], byte[]&amp;gt; producer;&lt;br/&gt;
     private boolean commitRequested = false;&lt;br/&gt;
-    private boolean commitOffsetNeeded = false;&lt;br/&gt;
     private boolean transactionInFlight = false;&lt;br/&gt;
-    private int waits = WAIT_ON_PARTIAL_INPUT;&lt;br/&gt;
-    private final Time time;&lt;br/&gt;
-    private final TaskMetrics taskMetrics;&lt;br/&gt;
-    private Sensor closeSensor;&lt;br/&gt;
 &lt;br/&gt;
     protected static final class TaskMetrics {&lt;br/&gt;
         final StreamsMetricsImpl metrics;&lt;br/&gt;
@@ -87,7 +84,6 @@&lt;br/&gt;
         final Sensor taskEnforcedProcessSensor;&lt;br/&gt;
         private final String taskName;&lt;br/&gt;
 &lt;br/&gt;
-&lt;br/&gt;
         TaskMetrics(final TaskId id, final StreamsMetricsImpl metrics) {&lt;br/&gt;
             taskName = id.toString();&lt;br/&gt;
             this.metrics = metrics;&lt;br/&gt;
@@ -134,13 +130,13 @@&lt;br/&gt;
             );&lt;br/&gt;
 &lt;br/&gt;
             // add the metrics for enforced processing&lt;br/&gt;
-            taskEnforcedProcessSensor = metrics.taskLevelSensor(taskName, &quot;enforced-process&quot;, Sensor.RecordingLevel.DEBUG, parent);&lt;br/&gt;
+            taskEnforcedProcessSensor = metrics.taskLevelSensor(taskName, &quot;enforced-processing&quot;, Sensor.RecordingLevel.DEBUG, parent);&lt;br/&gt;
             taskEnforcedProcessSensor.add(&lt;br/&gt;
-                    new MetricName(&quot;enforced-process-rate&quot;, group, &quot;The average number of occurrence of enforced-process per second.&quot;, tagMap),&lt;br/&gt;
+                    new MetricName(&quot;enforced-processing-rate&quot;, group, &quot;The average number of occurrence of enforced-processing operation per second.&quot;, tagMap),&lt;br/&gt;
                     new Rate(TimeUnit.SECONDS, new Count())&lt;br/&gt;
             );&lt;br/&gt;
             taskEnforcedProcessSensor.add(&lt;br/&gt;
-                    new MetricName(&quot;enforced-process-total&quot;, group, &quot;The total number of occurrence of enforced-process operations.&quot;, tagMap),&lt;br/&gt;
+                    new MetricName(&quot;enforced-processing-total&quot;, group, &quot;The total number of occurrence of enforced-processing operations.&quot;, tagMap),&lt;br/&gt;
                     new CumulativeCount()&lt;br/&gt;
             );&lt;br/&gt;
 &lt;br/&gt;
@@ -207,6 +203,7 @@ public StreamTask(final TaskId id,&lt;br/&gt;
 &lt;br/&gt;
         streamTimePunctuationQueue = new PunctuationQueue();&lt;br/&gt;
         systemTimePunctuationQueue = new PunctuationQueue();&lt;br/&gt;
+        maxTaskIdleMs = config.getLong(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);&lt;br/&gt;
         maxBufferedSize = config.getInt(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG);&lt;br/&gt;
 &lt;br/&gt;
         // initialize the consumed and committed offset cache&lt;br/&gt;
@@ -253,6 +250,7 @@ public StreamTask(final TaskId id,&lt;br/&gt;
     public boolean initializeStateStores() {
         log.trace(&quot;Initializing state stores&quot;);
         registerStateStores();
+
         return changelogPartitions().isEmpty();
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -277,7 +275,10 @@ public void initializeTopology() {&lt;br/&gt;
         }&lt;br/&gt;
 &lt;br/&gt;
         processorContext.initialized();&lt;br/&gt;
+&lt;br/&gt;
         taskInitialized = true;&lt;br/&gt;
+&lt;br/&gt;
+        idleStartTime = RecordQueue.UNKNOWN;&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
@@ -299,18 +300,27 @@ public void resume() {&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
-     * An active task is processable if its buffer contains data for all of its input source topic partitions&lt;br/&gt;
+     * An active task is processable if its buffer contains data for all of its input&lt;br/&gt;
+     * source topic partitions, or if it is enforced to be processable&lt;br/&gt;
      */&lt;br/&gt;
-    public boolean isProcessable() {&lt;br/&gt;
+    boolean isProcessable(final long now) {&lt;br/&gt;
         if (partitionGroup.allPartitionsBuffered()) {
+            idleStartTime = RecordQueue.UNKNOWN;
             return true;
-        } else if (partitionGroup.numBuffered() &amp;gt; 0 &amp;amp;&amp;amp; --waits &amp;lt; 0) {
-            taskMetrics.taskEnforcedProcessSensor.record();
-            waits = WAIT_ON_PARTIAL_INPUT;
-            return true;
-        }&lt;br/&gt;
+        } else if (partitionGroup.numBuffered() &amp;gt; 0) {&lt;br/&gt;
+            if (idleStartTime == RecordQueue.UNKNOWN) {
+                idleStartTime = now;
+            }&lt;br/&gt;
 &lt;br/&gt;
-        return false;&lt;br/&gt;
+            if (now - idleStartTime &amp;gt;= maxTaskIdleMs) {
+                taskMetrics.taskEnforcedProcessSensor.record();
+                return true;
+            } else {
+                return false;
+            }&lt;br/&gt;
+        } else {
+            return false;
+        }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     /**&lt;br/&gt;
@@ -343,7 +353,7 @@ public boolean process() {&lt;br/&gt;
 &lt;br/&gt;
             // update the consumed offset map after processing is done&lt;br/&gt;
             consumedOffsets.put(partition, record.offset());&lt;br/&gt;
-            commitOffsetNeeded = true;&lt;br/&gt;
+            commitNeeded = true;&lt;br/&gt;
 &lt;br/&gt;
             // after processing this record, if its partition queue&apos;s buffered size has been&lt;br/&gt;
             // decreased to the threshold, we can then resume the consumption on this partition&lt;br/&gt;
@@ -435,10 +445,32 @@ void commit(final boolean startNewTransaction) {
             stateMgr.checkpoint(activeTaskCheckpointableOffsets());
         }&lt;br/&gt;
 &lt;br/&gt;
-        commitOffsets(startNewTransaction);&lt;br/&gt;
+        final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; consumedOffsetsAndMetadata = new HashMap&amp;lt;&amp;gt;(consumedOffsets.size());&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) {
+            final TopicPartition partition = entry.getKey();
+            final long offset = entry.getValue() + 1;
+            consumedOffsetsAndMetadata.put(partition, new OffsetAndMetadata(offset));
+            stateMgr.putOffsetLimit(partition, offset);
+        }&lt;br/&gt;
 &lt;br/&gt;
-        commitRequested = false;&lt;br/&gt;
+        try {&lt;br/&gt;
+            if (eosEnabled) {&lt;br/&gt;
+                producer.sendOffsetsToTransaction(consumedOffsetsAndMetadata, applicationId);&lt;br/&gt;
+                producer.commitTransaction();&lt;br/&gt;
+                transactionInFlight = false;&lt;br/&gt;
+                if (startNewTransaction) {
+                    producer.beginTransaction();
+                    transactionInFlight = true;
+                }&lt;br/&gt;
+            } else {
+                consumer.commitSync(consumedOffsetsAndMetadata);
+            }&lt;br/&gt;
+        } catch (final CommitFailedException | ProducerFencedException error) {
+            throw new TaskMigratedException(this, error);
+        }&lt;br/&gt;
 &lt;br/&gt;
+        commitNeeded = false;&lt;br/&gt;
+        commitRequested = false;&lt;br/&gt;
         taskMetrics.taskCommitTimeSensor.record(time.nanoseconds() - startNs);&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -463,43 +495,6 @@ protected void flushState() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    /**&lt;br/&gt;
-     * @throws TaskMigratedException if committing offsets failed (non-EOS)&lt;br/&gt;
-     *                               or if the task producer got fenced (EOS)&lt;br/&gt;
-     */&lt;br/&gt;
-    private void commitOffsets(final boolean startNewTransaction) {&lt;br/&gt;
-        try {&lt;br/&gt;
-            if (commitOffsetNeeded) {&lt;br/&gt;
-                log.trace(&quot;Committing offsets&quot;);&lt;br/&gt;
-                final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; consumedOffsetsAndMetadata = new HashMap&amp;lt;&amp;gt;(consumedOffsets.size());&lt;br/&gt;
-                for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) {
-                    final TopicPartition partition = entry.getKey();
-                    final long offset = entry.getValue() + 1;
-                    consumedOffsetsAndMetadata.put(partition, new OffsetAndMetadata(offset));
-                    stateMgr.putOffsetLimit(partition, offset);
-                }&lt;br/&gt;
-&lt;br/&gt;
-                if (eosEnabled) {
-                    producer.sendOffsetsToTransaction(consumedOffsetsAndMetadata, applicationId);
-                } else {
-                    consumer.commitSync(consumedOffsetsAndMetadata);
-                }&lt;br/&gt;
-                commitOffsetNeeded = false;&lt;br/&gt;
-            }&lt;br/&gt;
-&lt;br/&gt;
-            if (eosEnabled) {&lt;br/&gt;
-                producer.commitTransaction();&lt;br/&gt;
-                transactionInFlight = false;&lt;br/&gt;
-                if (startNewTransaction) {
-                    producer.beginTransaction();
-                    transactionInFlight = true;
-                }&lt;br/&gt;
-            }&lt;br/&gt;
-        } catch (final CommitFailedException | ProducerFencedException fatal) {
-            throw new TaskMigratedException(this, fatal);
-        }&lt;br/&gt;
-    }&lt;br/&gt;
-&lt;br/&gt;
     Map&amp;lt;TopicPartition, Long&amp;gt; purgableOffsets() {&lt;br/&gt;
         final Map&amp;lt;TopicPartition, Long&amp;gt; purgableConsumedOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) {&lt;br/&gt;
@@ -645,13 +640,12 @@ private void closeTopology() {&lt;br/&gt;
 &lt;br/&gt;
     // helper to avoid calling suspend() twice if a suspended task is not reassigned and closed&lt;br/&gt;
     @Override&lt;br/&gt;
-    public void closeSuspended(boolean clean,&lt;br/&gt;
+    public void closeSuspended(final boolean clean,&lt;br/&gt;
                                final boolean isZombie,&lt;br/&gt;
                                RuntimeException firstException) {&lt;br/&gt;
         try {
             closeStateManager(clean);
         } catch (final RuntimeException e) {&lt;br/&gt;
-            clean = false;&lt;br/&gt;
             if (firstException == null) {
                 firstException = e;
             }&lt;br/&gt;
@@ -793,10 +787,16 @@ public boolean maybePunctuateStreamTime() {&lt;br/&gt;
 &lt;br/&gt;
         // if the timestamp is not known yet, meaning there is not enough data accumulated&lt;br/&gt;
         // to reason stream partition time, then skip.&lt;br/&gt;
-        if (timestamp == RecordQueue.NOT_KNOWN) {&lt;br/&gt;
+        if (timestamp == RecordQueue.UNKNOWN) {
             return false;
         } else {&lt;br/&gt;
-            return streamTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.STREAM_TIME, this);&lt;br/&gt;
+            final boolean punctuated = streamTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.STREAM_TIME, this);&lt;br/&gt;
+&lt;br/&gt;
+            if (punctuated) {
+                commitNeeded = true;
+            }&lt;br/&gt;
+&lt;br/&gt;
+            return punctuated;&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -810,20 +810,26 @@ public boolean maybePunctuateStreamTime() {&lt;br/&gt;
     public boolean maybePunctuateSystemTime() {&lt;br/&gt;
         final long timestamp = time.milliseconds();&lt;br/&gt;
 &lt;br/&gt;
-        return systemTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.WALL_CLOCK_TIME, this);&lt;br/&gt;
+        final boolean punctuated = systemTimePunctuationQueue.mayPunctuate(timestamp, PunctuationType.WALL_CLOCK_TIME, this);&lt;br/&gt;
+&lt;br/&gt;
+        if (punctuated) {+            commitNeeded = true;+        }
&lt;p&gt;+&lt;br/&gt;
+        return punctuated;&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Request committing the current task&apos;s state&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void needCommit() {&lt;br/&gt;
+    void requestCommit() 
{
         commitRequested = true;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Whether or not a request has been made to commit the current state&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;boolean commitNeeded() {&lt;br/&gt;
+    boolean commitRequested() 
{
         return commitRequested;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index 28cedbe4197..b43177d9d4e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -34,6 +34,7 @@&lt;br/&gt;
 import org.apache.kafka.common.metrics.stats.Count;&lt;br/&gt;
 import org.apache.kafka.common.metrics.stats.Rate;&lt;br/&gt;
 import org.apache.kafka.common.metrics.stats.Total;&lt;br/&gt;
+import org.apache.kafka.common.serialization.ByteArrayDeserializer;&lt;br/&gt;
 import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
 import org.apache.kafka.common.utils.Time;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaClientSupplier;&lt;br/&gt;
@@ -68,7 +69,6 @@&lt;/p&gt;

&lt;p&gt; public class StreamThread extends Thread &lt;/p&gt;
{
 
-    private final static int UNLIMITED_RECORDS = -1;
     private final static AtomicInteger STREAM_THREAD_ID_SEQUENCE = new AtomicInteger(1);
 
     /**
@@ -554,18 +554,21 @@ StandbyTask createTask(final Consumer&amp;lt;byte[], byte[]&amp;gt; consumer,
     }

&lt;p&gt;     private final Time time;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Duration pollTime;&lt;/li&gt;
	&lt;li&gt;private final long commitTimeMs;&lt;/li&gt;
	&lt;li&gt;private final Object stateLock;&lt;br/&gt;
     private final Logger log;&lt;br/&gt;
     private final String logPrefix;&lt;br/&gt;
+    private final Object stateLock;&lt;br/&gt;
+    private final Duration pollTime;&lt;br/&gt;
+    private final long commitTimeMs;&lt;br/&gt;
+    private final int maxPollTimeMs;&lt;br/&gt;
+    private final String originalReset;&lt;br/&gt;
     private final TaskManager taskManager;&lt;br/&gt;
     private final StreamsMetricsThreadImpl streamsMetrics;&lt;br/&gt;
     private final AtomicInteger assignmentErrorCode;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private long now;&lt;br/&gt;
+    private long lastPollMs;&lt;br/&gt;
     private long lastCommitMs;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long timerStartedMs;&lt;/li&gt;
	&lt;li&gt;private final String originalReset;&lt;br/&gt;
+    private int numIterations;&lt;br/&gt;
     private Throwable rebalanceException = null;&lt;br/&gt;
     private boolean processStandbyRecords = false;&lt;br/&gt;
     private volatile State state = State.CREATED;&lt;br/&gt;
@@ -711,11 +714,21 @@ public StreamThread(final Time time,&lt;br/&gt;
         this.assignmentErrorCode = assignmentErrorCode;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         this.pollTime = Duration.ofMillis(config.getLong(StreamsConfig.POLL_MS_CONFIG));&lt;br/&gt;
+        this.maxPollTimeMs = new InternalConsumerConfig(config.getMainConsumerConfigs(&quot;dummyGroupId&quot;, &quot;dummyClientId&quot;))&lt;br/&gt;
+                .getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG);&lt;br/&gt;
         this.commitTimeMs = config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG);&lt;/p&gt;

&lt;p&gt;+        this.numIterations = 1;&lt;br/&gt;
+&lt;br/&gt;
         updateThreadMetadata(Collections.emptyMap(), Collections.emptyMap());&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;+    private static final class InternalConsumerConfig extends ConsumerConfig {&lt;br/&gt;
+        private InternalConsumerConfig(final Map&amp;lt;String, Object&amp;gt; props) &lt;/p&gt;
{
+            super(ConsumerConfig.addDeserializerToConfig(props, new ByteArrayDeserializer(), new ByteArrayDeserializer()), false);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Execute the stream processors&lt;br/&gt;
      *&lt;br/&gt;
@@ -757,12 +770,11 @@ private void setRebalanceException(final Throwable rebalanceException) {&lt;/li&gt;
	&lt;li&gt;@throws StreamsException      if the store&apos;s change log does not contain the partition&lt;br/&gt;
      */&lt;br/&gt;
     private void runLoop() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long recordsProcessedBeforeCommit = UNLIMITED_RECORDS;&lt;br/&gt;
         consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         while (isRunning()) {&lt;br/&gt;
             try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;recordsProcessedBeforeCommit = runOnce(recordsProcessedBeforeCommit);&lt;br/&gt;
+                runOnce();&lt;br/&gt;
                 if (assignmentErrorCode.get() == StreamsPartitionAssignor.Error.VERSION_PROBING.code()) {&lt;br/&gt;
                     log.info(&quot;Version probing detected. Triggering new rebalance.&quot;);&lt;br/&gt;
                     enforceRebalance();&lt;br/&gt;
@@ -791,12 +803,10 @@ private void enforceRebalance() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;br/&gt;
     // Visible for testing&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long runOnce(final long recordsProcessedBeforeCommit) {&lt;/li&gt;
	&lt;li&gt;long processedBeforeCommit = recordsProcessedBeforeCommit;&lt;br/&gt;
-&lt;br/&gt;
+    void runOnce() {&lt;br/&gt;
         final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;timerStartedMs = time.milliseconds();&lt;br/&gt;
+        now = time.milliseconds();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (state == State.PARTITIONS_ASSIGNED) {&lt;br/&gt;
             // try to fetch some records with zero poll millis&lt;br/&gt;
@@ -816,6 +826,13 @@ long runOnce(final long recordsProcessedBeforeCommit) &lt;/p&gt;
{
             throw new StreamsException(logPrefix + &quot;Unexpected state &quot; + state + &quot; during normal iteration&quot;);
         }

&lt;p&gt;+        final long pollLatency = advanceNowAndComputeLatency();&lt;br/&gt;
+&lt;br/&gt;
+        if (records != null &amp;amp;&amp;amp; !records.isEmpty()) &lt;/p&gt;
{
+            streamsMetrics.pollTimeSensor.record(pollLatency, now);
+            addRecordsToTasks(records);
+        }
&lt;p&gt;+&lt;br/&gt;
         // only try to initialize the assigned tasks&lt;br/&gt;
         // if the state is still in PARTITION_ASSIGNED after the poll call&lt;br/&gt;
         if (state == State.PARTITIONS_ASSIGNED) {&lt;br/&gt;
@@ -824,28 +841,60 @@ long runOnce(final long recordsProcessedBeforeCommit) {&lt;br/&gt;
             }&lt;br/&gt;
         }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (records != null &amp;amp;&amp;amp; !records.isEmpty()) 
{
-            streamsMetrics.pollTimeSensor.record(computeLatency(), timerStartedMs);
-            addRecordsToTasks(records);
-        }
&lt;p&gt;+        advanceNowAndComputeLatency();&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        // TODO: we will process some tasks even if the state is not RUNNING, i.e. some other&lt;br/&gt;
+        // tasks are still being restored.&lt;br/&gt;
         if (taskManager.hasActiveRunningTasks()) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long totalProcessed = processAndMaybeCommit(recordsProcessedBeforeCommit);&lt;/li&gt;
	&lt;li&gt;if (totalProcessed &amp;gt; 0) 
{
-                final long processLatency = computeLatency();
-                streamsMetrics.processTimeSensor.record(processLatency / (double) totalProcessed, timerStartedMs);
-                processedBeforeCommit = adjustRecordsProcessedBeforeCommit(
-                        recordsProcessedBeforeCommit,
-                        totalProcessed,
-                        processLatency,
-                        commitTimeMs);
-            }
&lt;p&gt;+            /*&lt;br/&gt;
+             * Within an iteration, after N (N initialized as 1 upon start up) round of processing one-record-each on the applicable tasks, check the current time:&lt;br/&gt;
+             *  1. If it is time to commit, do it;&lt;br/&gt;
+             *  2. If it is time to punctuate, do it;&lt;br/&gt;
+             *  3. If elapsed time is close to consumer&apos;s max.poll.interval.ms, end the current iteration immediately.&lt;br/&gt;
+             *  4. If none of the the above happens, increment N.&lt;br/&gt;
+             *  5. If one of the above happens, half the value of N.&lt;br/&gt;
+             */&lt;br/&gt;
+            int processed = 0;&lt;br/&gt;
+            long timeSinceLastPoll = 0L;&lt;br/&gt;
+&lt;br/&gt;
+            do {&lt;br/&gt;
+                for (int i = 0; i &amp;lt; numIterations; i++) {&lt;br/&gt;
+                    processed = taskManager.process(now);&lt;br/&gt;
+&lt;br/&gt;
+                    if (processed &amp;gt; 0) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                        final long processLatency = advanceNowAndComputeLatency();+                        streamsMetrics.processTimeSensor.record(processLatency / (double) processed, now);++                        // commit any tasks that have requested a commit+                        final int committed = taskManager.maybeCommitActiveTasksPerUserRequested();++                        if (committed &amp;gt; 0) {
+                            final long commitLatency = advanceNowAndComputeLatency();
+                            streamsMetrics.commitTimeSensor.record(commitLatency / (double) committed, now);
+                        }+                    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else &lt;/p&gt;
{
+                        // if there is no records to be processed, exit immediately
+                        break;
+                    }
&lt;p&gt;+                }&lt;br/&gt;
+&lt;br/&gt;
+                timeSinceLastPoll = Math.max(now - lastPollMs, 0);&lt;br/&gt;
+&lt;br/&gt;
+                if (maybePunctuate() || maybeCommit()) &lt;/p&gt;
{
+                    numIterations = numIterations &amp;gt; 1 ? numIterations / 2 : numIterations;
+                }
&lt;p&gt; else if (timeSinceLastPoll &amp;gt; maxPollTimeMs / 2) &lt;/p&gt;
{
+                    numIterations = numIterations &amp;gt; 1 ? numIterations / 2 : numIterations;
+                    break;
+                }
&lt;p&gt; else if (processed &amp;gt; 0) &lt;/p&gt;
{
+                    numIterations++;
+                }
&lt;p&gt;+            } while (processed &amp;gt; 0);&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;punctuate();&lt;/li&gt;
	&lt;li&gt;maybeCommit(timerStartedMs);&lt;/li&gt;
	&lt;li&gt;maybeUpdateStandbyTasks(timerStartedMs);&lt;/li&gt;
	&lt;li&gt;return processedBeforeCommit;&lt;br/&gt;
+        // update standby tasks and maybe commit the standby tasks as well&lt;br/&gt;
+        maybeUpdateStandbyTasks();&lt;br/&gt;
+&lt;br/&gt;
+        maybeCommit();&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -858,6 +907,8 @@ long runOnce(final long recordsProcessedBeforeCommit) {&lt;br/&gt;
     private ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; pollRequests(final Duration pollTime) {&lt;br/&gt;
         ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = null;&lt;/p&gt;

&lt;p&gt;+        lastPollMs = now;&lt;br/&gt;
+&lt;br/&gt;
         try &lt;/p&gt;
{
             records = consumer.poll(pollTime);
         }
&lt;p&gt; catch (final InvalidOffsetException e) {&lt;br/&gt;
@@ -938,120 +989,65 @@ private void addRecordsToTasks(final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records) {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Schedule the records processing by selecting which record is processed next. Commits may&lt;/li&gt;
	&lt;li&gt;* happen as records are processed.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param recordsProcessedBeforeCommit number of records to be processed before commit is called.&lt;/li&gt;
	&lt;li&gt;*                                     if UNLIMITED_RECORDS, then commit is never called&lt;/li&gt;
	&lt;li&gt;* @return Number of records processed since last commit.&lt;/li&gt;
	&lt;li&gt;* @throws TaskMigratedException if committing offsets failed (non-EOS)&lt;/li&gt;
	&lt;li&gt;*                               or if the task producer got fenced (EOS)&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private long processAndMaybeCommit(final long recordsProcessedBeforeCommit) {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;long processed;&lt;/li&gt;
	&lt;li&gt;long totalProcessedSinceLastMaybeCommit = 0;&lt;/li&gt;
	&lt;li&gt;// Round-robin scheduling by taking one record from each task repeatedly&lt;/li&gt;
	&lt;li&gt;// until no task has any records left&lt;/li&gt;
	&lt;li&gt;do {&lt;/li&gt;
	&lt;li&gt;processed = taskManager.process();&lt;/li&gt;
	&lt;li&gt;if (processed &amp;gt; 0) 
{
-                streamsMetrics.processTimeSensor.record(computeLatency() / (double) processed, timerStartedMs);
-            }&lt;/li&gt;
	&lt;li&gt;totalProcessedSinceLastMaybeCommit += processed;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;punctuate();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;if (recordsProcessedBeforeCommit != UNLIMITED_RECORDS &amp;amp;&amp;amp;&lt;/li&gt;
	&lt;li&gt;totalProcessedSinceLastMaybeCommit &amp;gt;= recordsProcessedBeforeCommit) 
{
-                totalProcessedSinceLastMaybeCommit = 0;
-                maybeCommit(timerStartedMs);
-            }&lt;/li&gt;
	&lt;li&gt;// commit any tasks that have requested a commit&lt;/li&gt;
	&lt;li&gt;final int committed = taskManager.maybeCommitActiveTasks();&lt;/li&gt;
	&lt;li&gt;if (committed &amp;gt; 0) 
{
-                streamsMetrics.commitTimeSensor.record(computeLatency() / (double) committed, timerStartedMs);
-            }&lt;/li&gt;
	&lt;li&gt;} while (processed != 0);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;return totalProcessedSinceLastMaybeCommit;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
     /**&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if the task producer got fenced (EOS only)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void punctuate() {&lt;br/&gt;
+    private boolean maybePunctuate() {&lt;br/&gt;
         final int punctuated = taskManager.punctuate();&lt;br/&gt;
         if (punctuated &amp;gt; 0) 
{
-            streamsMetrics.punctuateTimeSensor.record(computeLatency() / (double) punctuated, timerStartedMs);
+            final long punctuateLatency = advanceNowAndComputeLatency();
+            streamsMetrics.punctuateTimeSensor.record(punctuateLatency / (double) punctuated, now);
         }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Adjust the number of records that should be processed by scheduler. This avoids&lt;/li&gt;
	&lt;li&gt;* scenarios where the processing time is higher than the commit time.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param prevRecordsProcessedBeforeCommit Previous number of records processed by scheduler.&lt;/li&gt;
	&lt;li&gt;* @param totalProcessed                   Total number of records processed in this last round.&lt;/li&gt;
	&lt;li&gt;* @param processLatency                   Total processing latency in ms processed in this last round.&lt;/li&gt;
	&lt;li&gt;* @param commitTime                       Desired commit time in ms.&lt;/li&gt;
	&lt;li&gt;* @return An adjusted number of records to be processed in the next round.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private long adjustRecordsProcessedBeforeCommit(final long prevRecordsProcessedBeforeCommit, final long totalProcessed,&lt;/li&gt;
	&lt;li&gt;final long processLatency, final long commitTime) {&lt;/li&gt;
	&lt;li&gt;long recordsProcessedBeforeCommit = UNLIMITED_RECORDS;&lt;/li&gt;
	&lt;li&gt;// check if process latency larger than commit latency&lt;/li&gt;
	&lt;li&gt;// note that once we set recordsProcessedBeforeCommit, it will never be UNLIMITED_RECORDS again, so&lt;/li&gt;
	&lt;li&gt;// we will never process all records again. This might be an issue if the initial measurement&lt;/li&gt;
	&lt;li&gt;// was off due to a slow start.&lt;/li&gt;
	&lt;li&gt;if (processLatency &amp;gt; 0 &amp;amp;&amp;amp; processLatency &amp;gt; commitTime) {&lt;/li&gt;
	&lt;li&gt;// push down&lt;/li&gt;
	&lt;li&gt;recordsProcessedBeforeCommit = Math.max(1, (commitTime * totalProcessed) / processLatency);&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;processing latency {} &amp;gt; commit time {} for {} records. Adjusting down recordsProcessedBeforeCommit={}&quot;,&lt;/li&gt;
	&lt;li&gt;processLatency, commitTime, totalProcessed, recordsProcessedBeforeCommit);&lt;/li&gt;
	&lt;li&gt;} else if (prevRecordsProcessedBeforeCommit != UNLIMITED_RECORDS &amp;amp;&amp;amp; processLatency &amp;gt; 0) {&lt;/li&gt;
	&lt;li&gt;// push up&lt;/li&gt;
	&lt;li&gt;recordsProcessedBeforeCommit = Math.max(1, (commitTime * totalProcessed) / processLatency);&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;processing latency {} &amp;lt; commit time {} for {} records. Adjusting up recordsProcessedBeforeCommit={}&quot;,&lt;/li&gt;
	&lt;li&gt;processLatency, commitTime, totalProcessed, recordsProcessedBeforeCommit);&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;return recordsProcessedBeforeCommit;&lt;br/&gt;
+        return punctuated &amp;gt; 0;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Commit all tasks owned by this thread if specified interval time has elapsed&lt;br/&gt;
+     * Try to commit all active tasks owned by this thread.&lt;br/&gt;
+     *&lt;br/&gt;
+     * Visible for testing.&lt;br/&gt;
      *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if committing offsets failed (non-EOS)&lt;/li&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;void maybeCommit(final long now) {&lt;/li&gt;
	&lt;li&gt;if (commitTimeMs &amp;gt;= 0 &amp;amp;&amp;amp; lastCommitMs + commitTimeMs &amp;lt; now) {&lt;br/&gt;
+    boolean maybeCommit() {&lt;br/&gt;
+        int committed = 0;&lt;br/&gt;
+&lt;br/&gt;
+        if (commitTimeMs &amp;gt;= 0 &amp;amp;&amp;amp; now - lastCommitMs &amp;gt; commitTimeMs) {&lt;br/&gt;
             if (log.isTraceEnabled()) {&lt;br/&gt;
                 log.trace(&quot;Committing all active tasks {} and standby tasks {} since {}ms has elapsed (commit interval is {}ms)&quot;,&lt;br/&gt;
                     taskManager.activeTaskIds(), taskManager.standbyTaskIds(), now - lastCommitMs, commitTimeMs);&lt;br/&gt;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int committed = taskManager.commitAll();&lt;br/&gt;
+            committed += taskManager.commitAll();&lt;br/&gt;
             if (committed &amp;gt; 0) 
{
-                streamsMetrics.commitTimeSensor.record(computeLatency() / (double) committed, timerStartedMs);
+                final long intervalCommitLatency = advanceNowAndComputeLatency();
+                streamsMetrics.commitTimeSensor.record(intervalCommitLatency / (double) committed, now);
 
                 // try to purge the committed records for repartition topics if possible
                 taskManager.maybePurgeCommitedRecords();
-            }&lt;/li&gt;
	&lt;li&gt;if (log.isDebugEnabled()) {&lt;/li&gt;
	&lt;li&gt;log.debug(&quot;Committed all active tasks {} and standby tasks {} in {}ms&quot;,&lt;/li&gt;
	&lt;li&gt;taskManager.activeTaskIds(), taskManager.standbyTaskIds(), timerStartedMs - now);&lt;br/&gt;
+&lt;br/&gt;
+                if (log.isDebugEnabled()) {&lt;br/&gt;
+                    log.debug(&quot;Committed all active tasks {} and standby tasks {} in {}ms&quot;,&lt;br/&gt;
+                        taskManager.activeTaskIds(), taskManager.standbyTaskIds(), intervalCommitLatency);&lt;br/&gt;
+                }&lt;br/&gt;
             }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             lastCommitMs = now;&lt;br/&gt;
-&lt;br/&gt;
             processStandbyRecords = true;&lt;br/&gt;
+        } else {&lt;br/&gt;
+            final int commitPerRequested = taskManager.maybeCommitActiveTasksPerUserRequested();&lt;br/&gt;
+            if (commitPerRequested &amp;gt; 0) &lt;/p&gt;
{
+                final long requestCommitLatency = advanceNowAndComputeLatency();
+                streamsMetrics.commitTimeSensor.record(requestCommitLatency / (double) committed, now);
+                committed += commitPerRequested;
+            }
&lt;p&gt;         }&lt;br/&gt;
+&lt;br/&gt;
+        return committed &amp;gt; 0;&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void maybeUpdateStandbyTasks(final long now) {&lt;br/&gt;
+    private void maybeUpdateStandbyTasks() {&lt;br/&gt;
         if (state == State.RUNNING &amp;amp;&amp;amp; taskManager.hasStandbyRunningTasks()) {&lt;br/&gt;
             if (processStandbyRecords) {&lt;br/&gt;
                 if (!standbyRecords.isEmpty()) {&lt;br/&gt;
@@ -1080,7 +1076,9 @@ private void maybeUpdateStandbyTasks(final long now) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                     standbyRecords = remainingStandbyRecords;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.debug(&quot;Updated standby tasks {} in {}ms&quot;, taskManager.standbyTaskIds(), time.milliseconds() - now);&lt;br/&gt;
+                    if (log.isDebugEnabled()) {&lt;br/&gt;
+                        log.debug(&quot;Updated standby tasks {} in {}ms&quot;, taskManager.standbyTaskIds(), time.milliseconds() - now);&lt;br/&gt;
+                    }&lt;br/&gt;
                 }&lt;br/&gt;
                 processStandbyRecords = false;&lt;br/&gt;
             }&lt;br/&gt;
@@ -1130,6 +1128,9 @@ private void maybeUpdateStandbyTasks(final long now) {&lt;br/&gt;
                 }&lt;br/&gt;
                 restoreConsumer.seekToBeginning(partitions);&lt;br/&gt;
             }&lt;br/&gt;
+&lt;br/&gt;
+            // update now if the standby restoration indeed executed&lt;br/&gt;
+            advanceNowAndComputeLatency();&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1139,11 +1140,11 @@ private void maybeUpdateStandbyTasks(final long now) {&lt;br/&gt;
      *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return latency&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private long computeLatency() {&lt;/li&gt;
	&lt;li&gt;final long previousTimeMs = timerStartedMs;&lt;/li&gt;
	&lt;li&gt;timerStartedMs = time.milliseconds();&lt;br/&gt;
+    private long advanceNowAndComputeLatency() 
{
+        final long previous = now;
+        now = time.milliseconds();
 
-        return Math.max(timerStartedMs - previousTimeMs, 0);
+        return Math.max(now - previous, 0);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -1242,6 +1243,10 @@ public String toString(final String indent) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     // the following are for testing only&lt;br/&gt;
+    void setNow(final long now) &lt;/p&gt;
{
+        this.now = now;
+    }
&lt;p&gt;+&lt;br/&gt;
     TaskManager taskManager() &lt;/p&gt;
{
         return taskManager;
     }
&lt;p&gt;@@ -1250,6 +1255,10 @@ TaskManager taskManager() &lt;/p&gt;
{
         return standbyRecords;
     }

&lt;p&gt;+    int currentNumIterations() &lt;/p&gt;
{
+        return numIterations;
+    }
&lt;p&gt;+&lt;br/&gt;
     public Map&amp;lt;MetricName, Metric&amp;gt; producerMetrics() {&lt;br/&gt;
         final LinkedHashMap&amp;lt;MetricName, Metric&amp;gt; result = new LinkedHashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         if (producer != null) {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java&lt;br/&gt;
index 2b43640f00e..812e7e1131c 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java&lt;br/&gt;
@@ -34,6 +34,8 @@&lt;br/&gt;
      */&lt;br/&gt;
     boolean initializeStateStores();&lt;/p&gt;

&lt;p&gt;+    boolean commitNeeded();&lt;br/&gt;
+&lt;br/&gt;
     void initializeTopology();&lt;/p&gt;

&lt;p&gt;     void commit();&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
index e25133737bd..9cc5a19e017 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java&lt;br/&gt;
@@ -409,8 +409,8 @@ int commitAll() {&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if the task producer got fenced (EOS only)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int process() {&lt;/li&gt;
	&lt;li&gt;return active.process();&lt;br/&gt;
+    int process(final long now) 
{
+        return active.process(now);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
@@ -424,8 +424,8 @@ int punctuate() {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@throws TaskMigratedException if committing offsets failed (non-EOS)&lt;/li&gt;
	&lt;li&gt;or if the task producer got fenced (EOS)&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int maybeCommitActiveTasks() {&lt;/li&gt;
	&lt;li&gt;return active.maybeCommit();&lt;br/&gt;
+    int maybeCommitActiveTasksPerUserRequested() 
{
+        return active.maybeCommitPerUserRequested();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     void maybePurgeCommitedRecords() {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
index 4593e59a54d..b51511e19e9 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java&lt;br/&gt;
@@ -24,14 +24,13 @@&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
 import org.apache.kafka.common.serialization.StringDeserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.StringSerializer;&lt;br/&gt;
-import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
-import org.apache.kafka.streams.kstream.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
 import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;&lt;br/&gt;
 import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KGroupedStream;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KStream;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.KeyValueMapper;&lt;br/&gt;
@@ -40,9 +39,6 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Reducer;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Serialized;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.TimeWindows;&lt;br/&gt;
-import org.apache.kafka.streams.kstream.Windowed;&lt;br/&gt;
-import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
-import org.apache.kafka.streams.state.WindowStore;&lt;br/&gt;
 import org.apache.kafka.test.IntegrationTest;&lt;br/&gt;
 import org.apache.kafka.test.MockMapper;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
@@ -54,14 +50,9 @@&lt;/p&gt;

&lt;p&gt; import java.io.IOException;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
-import java.util.Collections;&lt;br/&gt;
-import java.util.Comparator;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 import java.util.Properties;&lt;/p&gt;

&lt;p&gt;-import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
-import static org.hamcrest.core.Is.is;&lt;br/&gt;
-&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Similar to KStreamAggregationIntegrationTest but with dedupping enabled&lt;/li&gt;
	&lt;li&gt;by virtue of having a large commit interval&lt;br/&gt;
@@ -93,11 +84,9 @@ public void before() throws InterruptedException {&lt;br/&gt;
         builder = new StreamsBuilder();&lt;br/&gt;
         createTopics();&lt;br/&gt;
         streamsConfiguration = new Properties();&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final String applicationId = &quot;kgrouped-stream-test-&quot; +&lt;/li&gt;
	&lt;li&gt;testNo;&lt;br/&gt;
+        final String applicationId = &quot;kgrouped-stream-test-&quot; + testNo;&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);&lt;/li&gt;
	&lt;li&gt;streamsConfiguration&lt;/li&gt;
	&lt;li&gt;.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());&lt;br/&gt;
         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL_MS);&lt;br/&gt;
@@ -111,12 +100,7 @@ public void before() throws InterruptedException {&lt;br/&gt;
                 mapper,&lt;br/&gt;
                 Serialized.with(Serdes.String(), Serdes.String()));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reducer = new Reducer&amp;lt;String&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public String apply(final String value1, final String value2) 
{
-                return value1 + &quot;:&quot; + value2;
-            }&lt;/li&gt;
	&lt;li&gt;};&lt;br/&gt;
+        reducer = (value1, value2) -&amp;gt; value1 + &quot;:&quot; + value2;&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @After&lt;br/&gt;
@@ -132,7 +116,7 @@ public void whenShuttingDown() throws IOException {&lt;br/&gt;
     public void shouldReduce() throws Exception {&lt;br/&gt;
         produceMessages(System.currentTimeMillis());&lt;br/&gt;
         groupedStream&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.reduce(reducer, Materialized.&amp;lt;String, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;reduce-by-key&quot;))&lt;br/&gt;
+                .reduce(reducer, Materialized.as(&quot;reduce-by-key&quot;))&lt;br/&gt;
                 .toStream()&lt;br/&gt;
                 .to(outputTopic, Produced.with(Serdes.String(), Serdes.String()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -140,34 +124,15 @@ public void shouldReduce() throws Exception {&lt;/p&gt;

&lt;p&gt;         produceMessages(System.currentTimeMillis());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; results = receiveMessages(&lt;/li&gt;
	&lt;li&gt;new StringDeserializer(),&lt;/li&gt;
	&lt;li&gt;new StringDeserializer(),&lt;/li&gt;
	&lt;li&gt;5);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public int compare(final KeyValue&amp;lt;String, String&amp;gt; o1, final KeyValue&amp;lt;String, String&amp;gt; o2) 
{
-                return KStreamAggregationDedupIntegrationTest.compare(o1, o2);
-            }&lt;br/&gt;
-        });&lt;br/&gt;
-&lt;br/&gt;
-        assertThat(results, is(Arrays.asList(&lt;br/&gt;
-            KeyValue.pair(&quot;A&quot;, &quot;A:A&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;B&quot;, &quot;B:B&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;C&quot;, &quot;C:C&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;D&quot;, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
-            KeyValue.pair(&quot;E&quot;, &quot;E:E&quot;))));&lt;br/&gt;
-    }&lt;br/&gt;
-&lt;br/&gt;
-    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
-    private static &amp;lt;K extends Comparable, V extends Comparable&amp;gt; int compare(final KeyValue&amp;lt;K, V&amp;gt; o1,&lt;br/&gt;
-                                                                            final KeyValue&amp;lt;K, V&amp;gt; o2) {&lt;br/&gt;
-        final int keyComparison = o1.key.compareTo(o2.key);&lt;br/&gt;
-        if (keyComparison == 0) {
-            return o1.value.compareTo(o2.value);
-        }&lt;br/&gt;
-        return keyComparison;&lt;br/&gt;
+        validateReceivedMessages(&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                Arrays.asList(&lt;br/&gt;
+                        KeyValue.pair(&quot;A&quot;, &quot;A:A&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;B&quot;, &quot;B:B&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;C&quot;, &quot;C:C&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;D&quot;, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
+                        KeyValue.pair(&quot;E&quot;, &quot;E:E&quot;)));&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -180,50 +145,31 @@ public void shouldReduceWindowed() throws Exception {&lt;br/&gt;
 &lt;br/&gt;
         groupedStream&lt;br/&gt;
             .windowedBy(TimeWindows.of(500L))&lt;br/&gt;
-            .reduce(reducer, Materialized.&amp;lt;String, String, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;reduce-time-windows&quot;))&lt;br/&gt;
-            .toStream(new KeyValueMapper&amp;lt;Windowed&amp;lt;String&amp;gt;, String, String&amp;gt;() {&lt;br/&gt;
-                @Override&lt;br/&gt;
-                public String apply(final Windowed&amp;lt;String&amp;gt; windowedKey, final String value) {
-                    return windowedKey.key() + &quot;@&quot; + windowedKey.window().start();
-                }&lt;br/&gt;
-            })&lt;br/&gt;
+            .reduce(reducer, Materialized.as(&quot;reduce-time-windows&quot;))&lt;br/&gt;
+            .toStream((windowedKey, value) -&amp;gt; windowedKey.key() + &quot;@&quot; + windowedKey.window().start())&lt;br/&gt;
             .to(outputTopic, Produced.with(Serdes.String(), Serdes.String()));&lt;br/&gt;
 &lt;br/&gt;
         startStreams();&lt;br/&gt;
 &lt;br/&gt;
-        final List&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt; windowedOutput = receiveMessages(&lt;br/&gt;
-            new StringDeserializer(),&lt;br/&gt;
-            new StringDeserializer(),&lt;br/&gt;
-            10);&lt;br/&gt;
-&lt;br/&gt;
-        final Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;&lt;br/&gt;
-            comparator =&lt;br/&gt;
-            new Comparator&amp;lt;KeyValue&amp;lt;String, String&amp;gt;&amp;gt;() {&lt;br/&gt;
-                @Override&lt;br/&gt;
-                public int compare(final KeyValue&amp;lt;String, String&amp;gt; o1,&lt;br/&gt;
-                                   final KeyValue&amp;lt;String, String&amp;gt; o2) {
-                    return KStreamAggregationDedupIntegrationTest.compare(o1, o2);
-                }&lt;br/&gt;
-            };&lt;br/&gt;
-&lt;br/&gt;
-        Collections.sort(windowedOutput, comparator);&lt;br/&gt;
         final long firstBatchWindow = firstBatchTimestamp / 500 * 500;&lt;br/&gt;
         final long secondBatchWindow = secondBatchTimestamp / 500 * 500;&lt;br/&gt;
 &lt;br/&gt;
-        assertThat(windowedOutput, is(&lt;br/&gt;
-            Arrays.asList(&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + firstBatchWindow, &quot;A&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + secondBatchWindow, &quot;A:A&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + firstBatchWindow, &quot;B&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + secondBatchWindow, &quot;B:B&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + firstBatchWindow, &quot;C&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + secondBatchWindow, &quot;C:C&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + firstBatchWindow, &quot;D&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + secondBatchWindow, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + firstBatchWindow, &quot;E&quot;),&lt;br/&gt;
-                new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + secondBatchWindow, &quot;E:E&quot;)&lt;br/&gt;
-            )&lt;br/&gt;
-        ));&lt;br/&gt;
+        validateReceivedMessages(&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                Arrays.asList(&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + firstBatchWindow, &quot;A&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;A@&quot; + secondBatchWindow, &quot;A:A&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + firstBatchWindow, &quot;B&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;B@&quot; + secondBatchWindow, &quot;B:B&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + firstBatchWindow, &quot;C&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;C@&quot; + secondBatchWindow, &quot;C:C&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + firstBatchWindow, &quot;D&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;D@&quot; + secondBatchWindow, &quot;D&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + firstBatchWindow, &quot;E&quot;),&lt;br/&gt;
+                        new KeyValue&amp;lt;&amp;gt;(&quot;E@&quot; + secondBatchWindow, &quot;E:E&quot;)&lt;br/&gt;
+                )&lt;br/&gt;
+        );&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     @Test&lt;br/&gt;
@@ -234,36 +180,25 @@ public void shouldGroupByKey() throws Exception {&lt;br/&gt;
 &lt;br/&gt;
         stream.groupByKey(Serialized.with(Serdes.Integer(), Serdes.String()))&lt;br/&gt;
             .windowedBy(TimeWindows.of(500L))&lt;br/&gt;
-            .count(Materialized.&amp;lt;Integer, Long, WindowStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;count-windows&quot;))&lt;br/&gt;
-            .toStream(new KeyValueMapper&amp;lt;Windowed&amp;lt;Integer&amp;gt;, Long, String&amp;gt;() {&lt;br/&gt;
-                @Override&lt;br/&gt;
-                public String apply(final Windowed&amp;lt;Integer&amp;gt; windowedKey, final Long value) {-                    return windowedKey.key() + &quot;@&quot; + windowedKey.window().start();-                }&lt;br/&gt;
-            }).to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));&lt;br/&gt;
+            .count(Materialized.as(&quot;count-windows&quot;))&lt;br/&gt;
+            .toStream((windowedKey, value) -&amp;gt; windowedKey.key() + &quot;@&quot; + windowedKey.window().start())&lt;br/&gt;
+            .to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));&lt;br/&gt;
 &lt;br/&gt;
         startStreams();&lt;br/&gt;
 &lt;br/&gt;
-        final List&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt; results = receiveMessages(&lt;br/&gt;
-            new StringDeserializer(),&lt;br/&gt;
-            new LongDeserializer(),&lt;br/&gt;
-            5);&lt;br/&gt;
-        Collections.sort(results, new Comparator&amp;lt;KeyValue&amp;lt;String, Long&amp;gt;&amp;gt;() {&lt;br/&gt;
-            @Override&lt;br/&gt;
-            public int compare(final KeyValue&amp;lt;String, Long&amp;gt; o1, final KeyValue&amp;lt;String, Long&amp;gt; o2) {-                return KStreamAggregationDedupIntegrationTest.compare(o1, o2);-            }&lt;/li&gt;
	&lt;li&gt;});&lt;br/&gt;
-&lt;br/&gt;
         final long window = timestamp / 500 * 500;&lt;/li&gt;
	&lt;li&gt;assertThat(results, is(Arrays.asList(&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;1@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;2@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;3@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;4@&quot; + window, 2L),&lt;/li&gt;
	&lt;li&gt;KeyValue.pair(&quot;5@&quot; + window, 2L)&lt;/li&gt;
	&lt;li&gt;)));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        validateReceivedMessages(&lt;br/&gt;
+                new StringDeserializer(),&lt;br/&gt;
+                new LongDeserializer(),&lt;br/&gt;
+                Arrays.asList(&lt;br/&gt;
+                        KeyValue.pair(&quot;1@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;2@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;3@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;4@&quot; + window, 2L),&lt;br/&gt;
+                        KeyValue.pair(&quot;5@&quot; + window, 2L)&lt;br/&gt;
+                )&lt;br/&gt;
+        );&lt;br/&gt;
     }&lt;/p&gt;


&lt;p&gt;@@ -298,11 +233,9 @@ private void startStreams() {&lt;br/&gt;
     }&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private &amp;lt;K, V&amp;gt; List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; receiveMessages(final Deserializer&amp;lt;K&amp;gt;&lt;/li&gt;
	&lt;li&gt;keyDeserializer,&lt;/li&gt;
	&lt;li&gt;final Deserializer&amp;lt;V&amp;gt;&lt;/li&gt;
	&lt;li&gt;valueDeserializer,&lt;/li&gt;
	&lt;li&gt;final int numMessages)&lt;br/&gt;
+    private &amp;lt;K, V&amp;gt; void validateReceivedMessages(final Deserializer&amp;lt;K&amp;gt; keyDeserializer,&lt;br/&gt;
+                                                 final Deserializer&amp;lt;V&amp;gt; valueDeserializer,&lt;br/&gt;
+                                                 final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; expectedRecords)&lt;br/&gt;
         throws InterruptedException {&lt;br/&gt;
         final Properties consumerProperties = new Properties();&lt;br/&gt;
         consumerProperties&lt;br/&gt;
@@ -314,11 +247,11 @@ private void startStreams() 
{
             keyDeserializer.getClass().getName());
         consumerProperties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
             valueDeserializer.getClass().getName());
-        return IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerProperties,
-            outputTopic,
-            numMessages,
-            60 * 1000);
 
+        IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(
+            consumerProperties,
+            outputTopic,
+            expectedRecords);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
index 496ba5842a9..97d1071aaa2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java&lt;br/&gt;
@@ -524,20 +524,24 @@ public boolean test(final String key, final Long value) {&lt;br/&gt;
             myFilterNotStore = kafkaStreams.store(&quot;queryFilterNot&quot;, QueryableStoreTypes.&amp;lt;String, Long&amp;gt;keyValueStore());&lt;/p&gt;

&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; expectedEntry : expectedBatch1) &lt;/p&gt;
{
-            assertEquals(myFilterStore.get(expectedEntry.key), expectedEntry.value);
+            TestUtils.waitForCondition(() -&amp;gt; expectedEntry.value.equals(myFilterStore.get(expectedEntry.key)),
+                    &quot;Cannot get expected result&quot;);
         }
&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; batchEntry : batch1) {&lt;br/&gt;
             if (!expectedBatch1.contains(batchEntry)) &lt;/p&gt;
{
-                assertNull(myFilterStore.get(batchEntry.key));
+                TestUtils.waitForCondition(() -&amp;gt; myFilterStore.get(batchEntry.key) == null,
+                        &quot;Cannot get null result&quot;);
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; expectedEntry : expectedBatch1) &lt;/p&gt;
{
-            assertNull(myFilterNotStore.get(expectedEntry.key));
+            TestUtils.waitForCondition(() -&amp;gt; myFilterNotStore.get(expectedEntry.key) == null,
+                    &quot;Cannot get null result&quot;);
         }
&lt;p&gt;         for (final KeyValue&amp;lt;String, Long&amp;gt; batchEntry : batch1) {&lt;br/&gt;
             if (!expectedBatch1.contains(batchEntry)) &lt;/p&gt;
{
-                assertEquals(myFilterNotStore.get(batchEntry.key), batchEntry.value);
+                TestUtils.waitForCondition(() -&amp;gt; batchEntry.value.equals(myFilterNotStore.get(batchEntry.key)),
+                        &quot;Cannot get expected result&quot;);
             }
&lt;p&gt;         }&lt;br/&gt;
     }&lt;br/&gt;
@@ -568,24 +572,25 @@ public void shouldBeAbleToQueryMapValuesState() throws Exception {&lt;br/&gt;
             mockTime);&lt;/p&gt;

&lt;p&gt;         final KTable&amp;lt;String, String&amp;gt; t1 = builder.table(streamOne);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final KTable&amp;lt;String, Long&amp;gt; t2 = t1.mapValues(new ValueMapper&amp;lt;String, Long&amp;gt;() {&lt;br/&gt;
+        t1.mapValues(new ValueMapper&amp;lt;String, Long&amp;gt;() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public Long apply(final String value) 
{
                 return Long.valueOf(value);
             }&lt;/li&gt;
	&lt;li&gt;}, Materialized.&amp;lt;String, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;queryMapValues&quot;).withValueSerde(Serdes.Long()));&lt;/li&gt;
	&lt;li&gt;t2.toStream().to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));&lt;br/&gt;
+        }, Materialized.&amp;lt;String, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;queryMapValues&quot;).withValueSerde(Serdes.Long()))&lt;br/&gt;
+            .toStream()&lt;br/&gt;
+            .to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);&lt;br/&gt;
         kafkaStreams.start();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;waitUntilAtLeastNumRecordProcessed(outputTopic, 1);&lt;br/&gt;
+        waitUntilAtLeastNumRecordProcessed(outputTopic, 5);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final ReadOnlyKeyValueStore&amp;lt;String, Long&amp;gt;&lt;br/&gt;
             myMapStore = kafkaStreams.store(&quot;queryMapValues&quot;,&lt;br/&gt;
             QueryableStoreTypes.&amp;lt;String, Long&amp;gt;keyValueStore());&lt;br/&gt;
         for (final KeyValue&amp;lt;String, String&amp;gt; batchEntry : batch1) &lt;/p&gt;
{
-            assertEquals(myMapStore.get(batchEntry.key), Long.valueOf(batchEntry.value));
+            assertEquals(Long.valueOf(batchEntry.value), myMapStore.get(batchEntry.key));
         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
index d9602f35304..985b57f4cd5 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java&lt;br/&gt;
@@ -46,7 +46,9 @@&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Collection;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
+import java.util.HashMap;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.concurrent.ExecutionException;&lt;br/&gt;
 import java.util.concurrent.Future;&lt;br/&gt;
@@ -369,19 +371,22 @@ public static void waitForCompletion(final KafkaStreams streams,&lt;br/&gt;
                         readKeyValues(topic, consumer, waitTime, expectedRecords.size());&lt;br/&gt;
                 accumData.addAll(readData);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int accumLastIndex = accumData.size() - 1;&lt;/li&gt;
	&lt;li&gt;final int expectedLastIndex = expectedRecords.size() - 1;&lt;br/&gt;
-&lt;br/&gt;
                 // filter out all intermediate records we don&apos;t want&lt;br/&gt;
                 final List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt; accumulatedActual = accumData.stream().filter(expectedRecords::contains).collect(Collectors.toList());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// need this check as filtering above could have removed the last record from accumData, but it did not&lt;/li&gt;
	&lt;li&gt;// equal the last expected record&lt;/li&gt;
	&lt;li&gt;final boolean lastRecordsMatch = accumData.get(accumLastIndex).equals(expectedRecords.get(expectedLastIndex));&lt;br/&gt;
+                // still need to check that for each key, the ordering is expected&lt;br/&gt;
+                final Map&amp;lt;K, List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt;&amp;gt; finalAccumData = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+                for (final KeyValue&amp;lt;K, V&amp;gt; kv : accumulatedActual) 
{
+                    finalAccumData.computeIfAbsent(kv.key, key -&amp;gt; new ArrayList&amp;lt;&amp;gt;()).add(kv);
+                }
&lt;p&gt;+                final Map&amp;lt;K, List&amp;lt;KeyValue&amp;lt;K, V&amp;gt;&amp;gt;&amp;gt; finalExpected = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+                for (final KeyValue&amp;lt;K, V&amp;gt; kv : expectedRecords) &lt;/p&gt;
{
+                    finalExpected.computeIfAbsent(kv.key, key -&amp;gt; new ArrayList&amp;lt;&amp;gt;()).add(kv);
+                }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 // returns true only if the remaining records in both lists are the same and in the same order&lt;br/&gt;
                 // and the last record received matches the last expected record&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return accumulatedActual.equals(expectedRecords) &amp;amp;&amp;amp; lastRecordsMatch;&lt;br/&gt;
+                return finalAccumData.equals(finalExpected);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             };&lt;br/&gt;
             final String conditionDetails = &quot;Did not receive all &quot; + expectedRecords + &quot; records from topic &quot; + topic;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
index a3c47b5bdc2..fe71135f223 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java&lt;br/&gt;
@@ -61,7 +61,7 @@ public void before() {&lt;br/&gt;
     public void shouldInitializeNewTasks() {&lt;br/&gt;
         EasyMock.expect(t1.initializeStateStores()).andReturn(false);&lt;br/&gt;
         EasyMock.expect(t1.partitions()).andReturn(Collections.singleton(tp1));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.&amp;lt;TopicPartition&amp;gt;emptySet());&lt;br/&gt;
+        EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.emptySet());&lt;br/&gt;
         EasyMock.replay(t1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         addAndInitTask();&lt;br/&gt;
@@ -75,13 +75,13 @@ public void shouldMoveInitializedTasksNeedingRestoreToRestoring() {&lt;br/&gt;
         t1.initializeTopology();&lt;br/&gt;
         EasyMock.expectLastCall().once();&lt;br/&gt;
         EasyMock.expect(t1.partitions()).andReturn(Collections.singleton(tp1));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.&amp;lt;TopicPartition&amp;gt;emptySet());&lt;br/&gt;
+        EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.emptySet());&lt;br/&gt;
         EasyMock.expect(t2.initializeStateStores()).andReturn(true);&lt;br/&gt;
         t2.initializeTopology();&lt;br/&gt;
         EasyMock.expectLastCall().once();&lt;br/&gt;
         final Set&amp;lt;TopicPartition&amp;gt; t2partitions = Collections.singleton(tp2);&lt;br/&gt;
         EasyMock.expect(t2.partitions()).andReturn(t2partitions);&lt;/li&gt;
	&lt;li&gt;EasyMock.expect(t2.changelogPartitions()).andReturn(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;br/&gt;
+        EasyMock.expect(t2.changelogPartitions()).andReturn(Collections.emptyList());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         EasyMock.replay(t1, t2);&lt;/p&gt;

&lt;p&gt;@@ -101,7 +101,7 @@ public void shouldMoveInitializedTasksThatDontNeedRestoringToRunning() {&lt;br/&gt;
         t2.initializeTopology();&lt;br/&gt;
         EasyMock.expectLastCall().once();&lt;br/&gt;
         EasyMock.expect(t2.partitions()).andReturn(Collections.singleton(tp2));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(t2.changelogPartitions()).andReturn(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;br/&gt;
+        EasyMock.expect(t2.changelogPartitions()).andReturn(Collections.emptyList());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         EasyMock.replay(t2);&lt;/p&gt;

&lt;p&gt;@@ -145,7 +145,7 @@ public void shouldSuspendRunningTasks() {&lt;br/&gt;
     public void shouldCloseRestoringTasks() {&lt;br/&gt;
         EasyMock.expect(t1.initializeStateStores()).andReturn(false);&lt;br/&gt;
         EasyMock.expect(t1.partitions()).andReturn(Collections.singleton(tp1));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.&amp;lt;TopicPartition&amp;gt;emptySet());&lt;br/&gt;
+        EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.emptySet());&lt;br/&gt;
         t1.close(false, false);&lt;br/&gt;
         EasyMock.expectLastCall();&lt;br/&gt;
         EasyMock.replay(t1);&lt;br/&gt;
@@ -247,12 +247,13 @@ private void mockTaskInitialization() 
{
         t1.initializeTopology();
         EasyMock.expectLastCall().once();
         EasyMock.expect(t1.partitions()).andReturn(Collections.singleton(tp1));
-        EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());
+        EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.emptyList());
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldCommitRunningTasks() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall();&lt;br/&gt;
         EasyMock.replay(t1);&lt;br/&gt;
@@ -266,6 +267,7 @@ public void shouldCommitRunningTasks() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCloseTaskOnCommitIfTaskMigratedException() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new TaskMigratedException());&lt;br/&gt;
         t1.close(false, true);&lt;br/&gt;
@@ -285,6 +287,7 @@ public void shouldCloseTaskOnCommitIfTaskMigratedException() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldThrowExceptionOnCommitWhenNotCommitFailedOrProducerFenced() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new RuntimeException(&quot;&quot;));&lt;br/&gt;
         EasyMock.replay(t1);&lt;br/&gt;
@@ -303,6 +306,7 @@ public void shouldThrowExceptionOnCommitWhenNotCommitFailedOrProducerFenced() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCommitRunningTasksIfNeeded() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitRequested()).andReturn(true);&lt;br/&gt;
         EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall();&lt;br/&gt;
@@ -310,13 +314,14 @@ public void shouldCommitRunningTasksIfNeeded() &lt;/p&gt;
{
 
         addAndInitTask();
 
-        assertThat(assignedTasks.maybeCommit(), equalTo(1));
+        assertThat(assignedTasks.maybeCommitPerUserRequested(), equalTo(1));
         EasyMock.verify(t1);
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldCloseTaskOnMaybeCommitIfTaskMigratedException() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
+        EasyMock.expect(t1.commitRequested()).andReturn(true);&lt;br/&gt;
         EasyMock.expect(t1.commitNeeded()).andReturn(true);&lt;br/&gt;
         t1.commit();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new TaskMigratedException());&lt;br/&gt;
@@ -326,7 +331,7 @@ public void shouldCloseTaskOnMaybeCommitIfTaskMigratedException() {&lt;br/&gt;
         addAndInitTask();&lt;/p&gt;

&lt;p&gt;         try &lt;/p&gt;
{
-            assignedTasks.maybeCommit();
+            assignedTasks.maybeCommitPerUserRequested();
             fail(&quot;Should have thrown TaskMigratedException.&quot;);
         }
&lt;p&gt; catch (final TaskMigratedException expected) &lt;/p&gt;
{ /* ignore */ }&lt;br/&gt;
 &lt;br/&gt;
@@ -337,7 +342,7 @@ public void shouldCloseTaskOnMaybeCommitIfTaskMigratedException() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldCloseTaskOnProcessesIfTaskMigratedException() {&lt;br/&gt;
         mockTaskInitialization();&lt;br/&gt;
-        EasyMock.expect(t1.isProcessable()).andReturn(true);&lt;br/&gt;
+        EasyMock.expect(t1.isProcessable(0L)).andReturn(true);&lt;br/&gt;
         t1.process();&lt;br/&gt;
         EasyMock.expectLastCall().andThrow(new TaskMigratedException());&lt;br/&gt;
         t1.close(false, true);&lt;br/&gt;
@@ -346,7 +351,7 @@ public void shouldCloseTaskOnProcessesIfTaskMigratedException() {&lt;br/&gt;
         addAndInitTask();&lt;br/&gt;
 &lt;br/&gt;
         try {
-            assignedTasks.process();
+            assignedTasks.process(0L);
             fail(&quot;Should have thrown TaskMigratedException.&quot;);
         } catch (final TaskMigratedException expected) { /* ignore */ }

&lt;p&gt;@@ -357,11 +362,11 @@ public void shouldCloseTaskOnProcessesIfTaskMigratedException() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldNotProcessUnprocessableTasks() &lt;/p&gt;
{
         mockTaskInitialization();
-        EasyMock.expect(t1.isProcessable()).andReturn(false);
+        EasyMock.expect(t1.isProcessable(0L)).andReturn(false);
         EasyMock.replay(t1);
         addAndInitTask();
 
-        assertThat(assignedTasks.process(), equalTo(0));
+        assertThat(assignedTasks.process(0L), equalTo(0));
 
         EasyMock.verify(t1);
     }
&lt;p&gt;@@ -369,13 +374,14 @@ public void shouldNotProcessUnprocessableTasks() {&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldAlwaysProcessProcessableTasks() &lt;/p&gt;
{
         mockTaskInitialization();
-        EasyMock.expect(t1.isProcessable()).andReturn(true);
+        EasyMock.expect(t1.isProcessable(0L)).andReturn(true);
         EasyMock.expect(t1.process()).andReturn(true).once();
+
         EasyMock.replay(t1);
 
         addAndInitTask();
 
-        assertThat(assignedTasks.process(), equalTo(1));
+        assertThat(assignedTasks.process(0L), equalTo(1));
 
         EasyMock.verify(t1);
     }
&lt;p&gt;@@ -459,7 +465,7 @@ private void mockRunningTaskSuspension() &lt;/p&gt;
{
         EasyMock.expectLastCall().once();
         EasyMock.expect(t1.hasStateStores()).andReturn(false).anyTimes();
         EasyMock.expect(t1.partitions()).andReturn(Collections.singleton(tp1)).anyTimes();
-        EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.&amp;lt;TopicPartition&amp;gt;emptyList()).anyTimes();
+        EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.emptyList()).anyTimes();
         t1.suspend();
         EasyMock.expectLastCall();
     }
&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java&lt;br/&gt;
index a8cd2c8f357..b91aba5fdd6 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java&lt;br/&gt;
@@ -101,7 +101,7 @@ public void testTimeTracking() {&lt;/p&gt;

&lt;p&gt;         assertTrue(queue.isEmpty());&lt;br/&gt;
         assertEquals(0, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(RecordQueue.NOT_KNOWN, queue.timestamp());&lt;br/&gt;
+        assertEquals(RecordQueue.UNKNOWN, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // add three 3 out-of-order records with timestamp 2, 1, 3&lt;br/&gt;
         final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; list1 = Arrays.asList(&lt;br/&gt;
@@ -173,7 +173,7 @@ public void testTimeTracking() {&lt;br/&gt;
         queue.clear();&lt;br/&gt;
         assertTrue(queue.isEmpty());&lt;br/&gt;
         assertEquals(0, queue.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(RecordQueue.NOT_KNOWN, queue.timestamp());&lt;br/&gt;
+        assertEquals(RecordQueue.UNKNOWN, queue.timestamp());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // re-insert the three records with 4, 5, 6&lt;br/&gt;
         queue.addRawRecords(list3);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
index 848cd4f0bf8..834ab3e1ae2 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
@@ -21,16 +21,17 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.OffsetResetStrategy;&lt;br/&gt;
 import org.apache.kafka.clients.producer.MockProducer;&lt;br/&gt;
 import org.apache.kafka.common.KafkaException;&lt;br/&gt;
+import org.apache.kafka.common.MetricName;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
 import org.apache.kafka.common.errors.ProducerFencedException;&lt;br/&gt;
 import org.apache.kafka.common.metrics.KafkaMetric;&lt;br/&gt;
+import org.apache.kafka.common.metrics.MetricConfig;&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics;&lt;br/&gt;
 import org.apache.kafka.common.metrics.Sensor;&lt;br/&gt;
 import org.apache.kafka.common.record.TimestampType;&lt;br/&gt;
-import org.apache.kafka.common.serialization.ByteArraySerializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Deserializer;&lt;br/&gt;
-import org.apache.kafka.common.serialization.IntegerDeserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.IntegerSerializer;&lt;br/&gt;
+import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serializer;&lt;br/&gt;
 import org.apache.kafka.common.utils.LogContext;&lt;br/&gt;
 import org.apache.kafka.common.utils.MockTime;&lt;br/&gt;
@@ -82,9 +83,9 @@&lt;/p&gt;

&lt;p&gt; public class StreamTaskTest {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Serializer&amp;lt;Integer&amp;gt; intSerializer = new IntegerSerializer();&lt;/li&gt;
	&lt;li&gt;private final Deserializer&amp;lt;Integer&amp;gt; intDeserializer = new IntegerDeserializer();&lt;/li&gt;
	&lt;li&gt;private final Serializer&amp;lt;byte[]&amp;gt; bytesSerializer = new ByteArraySerializer();&lt;br/&gt;
+    private final Serializer&amp;lt;Integer&amp;gt; intSerializer = Serdes.Integer().serializer();&lt;br/&gt;
+    private final Serializer&amp;lt;byte[]&amp;gt; bytesSerializer = Serdes.ByteArray().serializer();&lt;br/&gt;
+    private final Deserializer&amp;lt;Integer&amp;gt; intDeserializer = Serdes.Integer().deserializer();&lt;br/&gt;
     private final String topic1 = &quot;topic1&quot;;&lt;br/&gt;
     private final String topic2 = &quot;topic2&quot;;&lt;br/&gt;
     private final TopicPartition partition1 = new TopicPartition(topic1, 1);&lt;br/&gt;
@@ -113,7 +114,7 @@ public void close() {&lt;br/&gt;
     private final Long offset = 543L;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private final ProcessorTopology topology = ProcessorTopology.withSources(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;br/&gt;
+        Utils.mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;br/&gt;
         mkMap(mkEntry(topic1, source1), mkEntry(topic2, source2))&lt;br/&gt;
     );&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -129,8 +130,7 @@ public void close() {&lt;br/&gt;
     };&lt;br/&gt;
     private final byte[] recordValue = intSerializer.serialize(null, 10);&lt;br/&gt;
     private final byte[] recordKey = intSerializer.serialize(null, 1);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final Metrics metrics = new Metrics();&lt;/li&gt;
	&lt;li&gt;private final Sensor skippedRecordsSensor = metrics.sensor(&quot;skipped-records&quot;);&lt;br/&gt;
+    private final Metrics metrics = new Metrics(new MetricConfig().recordLevel(Sensor.RecordingLevel.DEBUG));&lt;br/&gt;
     private final StreamsMetricsImpl streamsMetrics = new MockStreamsMetrics(metrics);&lt;br/&gt;
     private final TaskId taskId00 = new TaskId(0, 0);&lt;br/&gt;
     private final MockTime time = new MockTime();&lt;br/&gt;
@@ -159,7 +159,8 @@ private StreamsConfig createConfig(final boolean enableEoS) 
{
             mkEntry(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, &quot;3&quot;),
             mkEntry(StreamsConfig.STATE_DIR_CONFIG, canonicalPath),
             mkEntry(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName()),
-            mkEntry(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, enableEoS ? StreamsConfig.EXACTLY_ONCE : StreamsConfig.AT_LEAST_ONCE)
+            mkEntry(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, enableEoS ? StreamsConfig.EXACTLY_ONCE : StreamsConfig.AT_LEAST_ONCE),
+            mkEntry(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG, &quot;100&quot;)
         )));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -393,9 +394,6 @@ public void shouldPunctuateOnceStreamTimeAfterGap() &lt;/p&gt;
{
         assertEquals(4, source2.numReceived);
         assertFalse(task.maybePunctuateStreamTime());
 
-        assertFalse(task.process());
-        assertFalse(task.maybePunctuateStreamTime());
-
         processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L, 142L, 155L, 160L);
     }

&lt;p&gt;@@ -453,23 +451,62 @@ public void shouldRespectPunctuateCancellationSystemTime() &lt;/p&gt;
{
         processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 10);
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void shouldRespectCommitNeeded() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+
+        assertFalse(task.commitNeeded());
+
+        task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));
+        assertTrue(task.process());
+        assertTrue(task.commitNeeded());
+
+        task.commit();
+        assertFalse(task.commitNeeded());
+
+        assertTrue(task.maybePunctuateStreamTime());
+        assertTrue(task.commitNeeded());
+
+        task.commit();
+        assertFalse(task.commitNeeded());
+
+        time.sleep(10);
+        assertTrue(task.maybePunctuateSystemTime());
+        assertTrue(task.commitNeeded());
+
+        task.commit();
+        assertFalse(task.commitNeeded());
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRespectCommitRequested() &lt;/p&gt;
{
+        task = createStatelessTask(createConfig(false));
+        task.initializeStateStores();
+        task.initializeTopology();
+
+        task.requestCommit();
+        assertTrue(task.commitRequested());
+    }
&lt;p&gt;+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldBeProcessableIfAllPartitionsBuffered() &lt;/p&gt;
{
         task = createStatelessTask(createConfig(false));
         task.initializeStateStores();
         task.initializeTopology();
 
-        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable(0L));
 
         final byte[] bytes = ByteBuffer.allocate(4).putInt(1).array();
 
         task.addRecords(partition1, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, bytes, bytes)));
 
-        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable(0L));
 
         task.addRecords(partition2, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic2, 1, 0, bytes, bytes)));
 
-        assertTrue(task.isProcessable());
+        assertTrue(task.isProcessable(0L));
     }

&lt;p&gt;     @Test&lt;br/&gt;
@@ -478,19 +515,42 @@ public void shouldBeProcessableIfWaitedForTooLong() &lt;/p&gt;
{
         task.initializeStateStores();
         task.initializeTopology();
 
-        assertFalse(task.isProcessable());
+        final MetricName enforcedProcessMetric = metrics.metricName(&quot;enforced-processing-total&quot;, &quot;stream-task-metrics&quot;, mkMap(mkEntry(&quot;client-id&quot;, &quot;test&quot;), mkEntry(&quot;task-id&quot;, taskId00.toString())));
+
+        assertFalse(task.isProcessable(0L));
+        assertEquals(0.0, metrics.metric(enforcedProcessMetric).metricValue());
 
         final byte[] bytes = ByteBuffer.allocate(4).putInt(1).array();
 
         task.addRecords(partition1, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, bytes, bytes)));
 
-        assertFalse(task.isProcessable());
-        assertFalse(task.isProcessable());
-        assertFalse(task.isProcessable());
-        assertFalse(task.isProcessable());
-        assertFalse(task.isProcessable());
+        assertFalse(task.isProcessable(time.milliseconds()));
+
+        assertFalse(task.isProcessable(time.milliseconds() + 50L));
+
+        assertTrue(task.isProcessable(time.milliseconds() + 100L));
+        assertEquals(1.0, metrics.metric(enforcedProcessMetric).metricValue());
+
+        // once decided to enforce, continue doing that
+        assertTrue(task.isProcessable(time.milliseconds() + 101L));
+        assertEquals(2.0, metrics.metric(enforcedProcessMetric).metricValue());
+
+        task.addRecords(partition2, Collections.singleton(new ConsumerRecord&amp;lt;&amp;gt;(topic2, 1, 0, bytes, bytes)));
+
+        assertTrue(task.isProcessable(time.milliseconds() + 130L));
+        assertEquals(2.0, metrics.metric(enforcedProcessMetric).metricValue());
+
+        // one resumed to normal processing, the timer should be reset
+        task.process();
+
+        assertFalse(task.isProcessable(time.milliseconds() + 150L));
+        assertEquals(2.0, metrics.metric(enforcedProcessMetric).metricValue());
+
+        assertFalse(task.isProcessable(time.milliseconds() + 249L));
+        assertEquals(2.0, metrics.metric(enforcedProcessMetric).metricValue());
 
-        assertTrue(task.isProcessable());
+        assertTrue(task.isProcessable(time.milliseconds() + 250L));
+        assertEquals(3.0, metrics.metric(enforcedProcessMetric).metricValue());
     }


&lt;p&gt;@@ -1155,8 +1215,8 @@ public void shouldReturnOffsetsForRepartitionTopicsForPurging() {&lt;br/&gt;
         final TopicPartition repartition = new TopicPartition(&quot;repartition&quot;, 1);&lt;/p&gt;

&lt;p&gt;         final ProcessorTopology topology = ProcessorTopology.withRepartitionTopics(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2),&lt;/li&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(repartition.topic(), (SourceNode) source2)),&lt;br/&gt;
+            Utils.mkList(source1, source2),&lt;br/&gt;
+            mkMap(mkEntry(topic1, source1), mkEntry(repartition.topic(), source2)),&lt;br/&gt;
             Collections.singleton(repartition.topic())&lt;br/&gt;
         );&lt;br/&gt;
         consumer.assign(Arrays.asList(partition1, repartition));&lt;br/&gt;
@@ -1227,10 +1287,10 @@ public void punctuate(final long timestamp) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     private StreamTask createStatefulTask(final StreamsConfig config, final boolean logged) {&lt;br/&gt;
         final ProcessorTopology topology = ProcessorTopology.with(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2),&lt;/li&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source2)),&lt;br/&gt;
+            Utils.mkList(source1, source2),&lt;br/&gt;
+            mkMap(mkEntry(topic1, source1), mkEntry(topic2, source2)),&lt;br/&gt;
             singletonList(stateStore),&lt;/li&gt;
	&lt;li&gt;logged ? Collections.singletonMap(storeName, storeName + &quot;-changelog&quot;) : Collections.&amp;lt;String, String&amp;gt;emptyMap());&lt;br/&gt;
+            logged ? Collections.singletonMap(storeName, storeName + &quot;-changelog&quot;) : Collections.emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return new StreamTask(&lt;br/&gt;
             taskId00,&lt;br/&gt;
@@ -1249,10 +1309,10 @@ private StreamTask createStatefulTask(final StreamsConfig config, final boolean&lt;/p&gt;

&lt;p&gt;     private StreamTask createStatefulTaskThatThrowsExceptionOnClose() {&lt;br/&gt;
         final ProcessorTopology topology = ProcessorTopology.with(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source3),&lt;/li&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source3)),&lt;br/&gt;
+            Utils.mkList(source1, source3),&lt;br/&gt;
+            mkMap(mkEntry(topic1, source1), mkEntry(topic2, source3)),&lt;br/&gt;
             singletonList(stateStore),&lt;/li&gt;
	&lt;li&gt;Collections.&amp;lt;String, String&amp;gt;emptyMap());&lt;br/&gt;
+            Collections.emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return new StreamTask(&lt;br/&gt;
             taskId00,&lt;br/&gt;
@@ -1271,7 +1331,7 @@ private StreamTask createStatefulTaskThatThrowsExceptionOnClose() {&lt;/p&gt;

&lt;p&gt;     private StreamTask createStatelessTask(final StreamsConfig streamsConfig) {&lt;br/&gt;
         final ProcessorTopology topology = ProcessorTopology.withSources(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;br/&gt;
+            Utils.mkList(source1, source2, processorStreamTime, processorSystemTime),&lt;br/&gt;
             mkMap(mkEntry(topic1, source1), mkEntry(topic2, source2))&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1298,8 +1358,8 @@ private StreamTask createStatelessTask(final StreamsConfig streamsConfig) {&lt;br/&gt;
     // this task will throw exception when processing (on partition2), flushing, suspending and closing&lt;br/&gt;
     private StreamTask createTaskThatThrowsException(final boolean enableEos) {&lt;br/&gt;
         final ProcessorTopology topology = ProcessorTopology.withSources(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Utils.&amp;lt;ProcessorNode&amp;gt;mkList(source1, source3, processorStreamTime, processorSystemTime),&lt;/li&gt;
	&lt;li&gt;mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source3))&lt;br/&gt;
+            Utils.mkList(source1, source3, processorStreamTime, processorSystemTime),&lt;br/&gt;
+            mkMap(mkEntry(topic1, source1), mkEntry(topic2, source3))&lt;br/&gt;
         );&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         source1.addChild(processorStreamTime);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
index c1485fb8056..e691c54d8eb 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
@@ -61,8 +61,10 @@&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
 import org.apache.kafka.streams.state.internals.OffsetCheckpoint;&lt;br/&gt;
 import org.apache.kafka.test.MockClientSupplier;&lt;br/&gt;
+import org.apache.kafka.test.MockProcessor;&lt;br/&gt;
 import org.apache.kafka.test.MockStateRestoreListener;&lt;br/&gt;
 import org.apache.kafka.test.MockTimestampExtractor;&lt;br/&gt;
+import org.apache.kafka.test.StreamsTestUtils;&lt;br/&gt;
 import org.apache.kafka.test.TestCondition;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.easymock.EasyMock;&lt;br/&gt;
@@ -106,15 +108,16 @@&lt;br/&gt;
     private final MockTime mockTime = new MockTime();&lt;br/&gt;
     private final Metrics metrics = new Metrics();&lt;br/&gt;
     private final MockClientSupplier clientSupplier = new MockClientSupplier();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private UUID processId = UUID.randomUUID();&lt;br/&gt;
     private final InternalStreamsBuilder internalStreamsBuilder = new InternalStreamsBuilder(new InternalTopologyBuilder());&lt;/li&gt;
	&lt;li&gt;private InternalTopologyBuilder internalTopologyBuilder;&lt;br/&gt;
     private final StreamsConfig config = new StreamsConfig(configProps(false));&lt;br/&gt;
     private final String stateDir = TestUtils.tempDirectory().getPath();&lt;br/&gt;
     private final StateDirectory stateDirectory = new StateDirectory(config, mockTime);&lt;/li&gt;
	&lt;li&gt;private StreamsMetadataState streamsMetadataState;&lt;br/&gt;
     private final ConsumedInternal&amp;lt;Object, Object&amp;gt; consumed = new ConsumedInternal&amp;lt;&amp;gt;();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    private UUID processId = UUID.randomUUID();&lt;br/&gt;
+    private InternalTopologyBuilder internalTopologyBuilder;&lt;br/&gt;
+    private StreamsMetadataState streamsMetadataState;&lt;br/&gt;
+&lt;br/&gt;
     @Before&lt;br/&gt;
     public void setUp() {&lt;br/&gt;
         processId = UUID.randomUUID();&lt;br/&gt;
@@ -177,7 +180,7 @@ public void testPartitionAssignmentChangeForSingleGroup() {&lt;br/&gt;
         mockConsumer.assign(assignedPartitions);&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(thread.state(), StreamThread.State.RUNNING);&lt;br/&gt;
         Assert.assertEquals(4, stateListener.numChanges);&lt;br/&gt;
         Assert.assertEquals(StreamThread.State.PARTITIONS_ASSIGNED, stateListener.oldState);&lt;br/&gt;
@@ -307,13 +310,106 @@ public void shouldNotCommitBeforeTheCommitInterval() 
{
             new LogContext(&quot;&quot;),
             new AtomicInteger()
         );
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
         mockTime.sleep(commitInterval - 10L);
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
 
         EasyMock.verify(taskManager);
     }&lt;br/&gt;
 &lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRespectNumIterationsInMainLoop() {
+        final MockProcessor mockProcessor = new MockProcessor(PunctuationType.WALL_CLOCK_TIME, 10L);
+        internalTopologyBuilder.addSource(null, &quot;source1&quot;, null, null, null, topic1);
+        internalTopologyBuilder.addProcessor(&quot;processor1&quot;, () -&amp;gt; mockProcessor, &quot;source1&quot;);
+        internalTopologyBuilder.addProcessor(&quot;processor2&quot;, () -&amp;gt; new MockProcessor(PunctuationType.STREAM_TIME, 10L), &quot;source1&quot;);
+
+        final Properties properties = new Properties();
+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100L);
+        final StreamsConfig config = new StreamsConfig(StreamsTestUtils.getStreamsConfig(applicationId,
+            &quot;localhost:2171&quot;,
+            Serdes.ByteArraySerde.class.getName(),
+            Serdes.ByteArraySerde.class.getName(),
+            properties));
+        final StreamThread thread = createStreamThread(clientId, config, false);
+
+        thread.setState(StreamThread.State.RUNNING);
+        thread.setState(StreamThread.State.PARTITIONS_REVOKED);
+
+        final Set&amp;lt;TopicPartition&amp;gt; assignedPartitions = Collections.singleton(t1p1);
+        thread.taskManager().setAssignmentMetadata(
+            Collections.singletonMap(
+                new TaskId(0, t1p1.partition()),
+                assignedPartitions),
+            Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap());
+
+        final MockConsumer&amp;lt;byte[], byte[]&amp;gt; mockConsumer = (MockConsumer&amp;lt;byte[], byte[]&amp;gt;) thread.consumer;
+        mockConsumer.assign(Collections.singleton(t1p1));
+        mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
+        thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
+        thread.runOnce();
+
+        // processed one record, punctuated after the first record, and hence num.iterations is still 1
+        long offset = -1;
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 0, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        thread.runOnce();
+
+        assertThat(thread.currentNumIterations(), equalTo(1));
+
+        // processed one more record without punctuation, and bump num.iterations to 2
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 1, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        thread.runOnce();
+
+        assertThat(thread.currentNumIterations(), equalTo(2));
+
+        // processed zero records, early exit and iterations stays as 2
+        thread.runOnce();
+        assertThat(thread.currentNumIterations(), equalTo(2));
+
+        // system time based punctutation halves to 1
+        mockTime.sleep(11L);
+
+        thread.runOnce();
+        assertThat(thread.currentNumIterations(), equalTo(1));
+
+        // processed two records, bumping up iterations to 2
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 5, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 6, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        thread.runOnce();
+
+        assertThat(thread.currentNumIterations(), equalTo(2));
+
+        // stream time based punctutation halves to 1
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 11, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        thread.runOnce();
+
+        assertThat(thread.currentNumIterations(), equalTo(1));
+
+        // processed three records, bumping up iterations to 3 (1 + 2)
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 12, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 13, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 14, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        thread.runOnce();
+
+        assertThat(thread.currentNumIterations(), equalTo(3));
+
+        mockProcessor.requestCommit();
+        mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 15, TimestampType.CREATE_TIME, -1, -1, -1, new byte[0], new byte[0]));
+        thread.runOnce();
+
+        // user requested commit should not impact on iteration adjustment
+        assertThat(thread.currentNumIterations(), equalTo(3));
+
+        // time based commit, halves iterations to 3 / 2 = 1
+        mockTime.sleep(90L);
+        thread.runOnce();
+
+        assertThat(thread.currentNumIterations(), equalTo(1));
+
+    }&lt;br/&gt;
+&lt;br/&gt;
     @SuppressWarnings({&quot;unchecked&quot;, &quot;ThrowableNotThrown&quot;})&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldNotCauseExceptionIfNothingCommitted() {&lt;br/&gt;
@@ -341,9 +437,11 @@ public void shouldNotCauseExceptionIfNothingCommitted() {             new LogContext(&quot;&quot;),             new AtomicInteger()         );-        thread.maybeCommit(mockTime.milliseconds());+        thread.setNow(mockTime.milliseconds());+        thread.maybeCommit();         mockTime.sleep(commitInterval - 10L);-        thread.maybeCommit(mockTime.milliseconds());+        thread.setNow(mockTime.milliseconds());+        thread.maybeCommit();          EasyMock.verify(taskManager);     }
&lt;p&gt;@@ -376,9 +474,11 @@ public void shouldCommitAfterTheCommitInterval() &lt;/p&gt;
{
             new LogContext(&quot;&quot;),
             new AtomicInteger()
         );
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
         mockTime.sleep(commitInterval + 1);
-        thread.maybeCommit(mockTime.milliseconds());
+        thread.setNow(mockTime.milliseconds());
+        thread.maybeCommit();
 
         EasyMock.verify(taskManager);
     }
&lt;p&gt;@@ -457,7 +557,7 @@ public void shouldInjectProducerPerTaskUsingClientSupplierOnCreateIfEosEnable()&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(beginOffsets);&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(new HashSet&amp;lt;&amp;gt;(assignedPartitions));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(thread.tasks().size(), clientSupplier.producers.size());&lt;br/&gt;
         assertSame(clientSupplier.consumer, thread.consumer);&lt;br/&gt;
@@ -646,7 +746,7 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerWasFencedWh&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertThat(thread.tasks().size(), equalTo(1));&lt;br/&gt;
         final MockProducer producer = clientSupplier.producers.get(0);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -657,7 +757,7 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerWasFencedWh&lt;/p&gt;

&lt;p&gt;         consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockTime.sleep(config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG) + 1);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertThat(producer.history().size(), equalTo(1));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertFalse(producer.transactionCommitted());&lt;br/&gt;
@@ -666,16 +766,16 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerWasFencedWh&lt;br/&gt;
             new TestCondition() {&lt;br/&gt;
                 @Override&lt;br/&gt;
                 public boolean conditionMet() &lt;/p&gt;
{
-                    return producer.commitCount() == 2;
+                    return producer.commitCount() == 1;
                 }
&lt;p&gt;             },&lt;br/&gt;
             &quot;StreamsThread did not commit transaction.&quot;);&lt;/p&gt;

&lt;p&gt;         producer.fenceProducer();&lt;br/&gt;
         mockTime.sleep(config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG) + 1L);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 0, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
+        consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, 1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         try 
{
-            thread.runOnce(-1);
+            thread.runOnce();
             fail(&quot;Should have thrown TaskMigratedException&quot;);
         }
&lt;p&gt; catch (final TaskMigratedException expected) &lt;/p&gt;
{ /* ignore */ }
&lt;p&gt;         TestUtils.waitForCondition(&lt;br/&gt;
@@ -687,15 +787,16 @@ public boolean conditionMet() {&lt;br/&gt;
             },&lt;br/&gt;
             &quot;StreamsThread did not remove fenced zombie task.&quot;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertThat(producer.commitCount(), equalTo(2L));&lt;br/&gt;
+        assertThat(producer.commitCount(), equalTo(1L));&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private StreamThread setupStreamThread() {&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerGotFencedInCommitTransactionWhenSuspendingTaks() {&lt;br/&gt;
+        final StreamThread thread = createStreamThread(clientId, new StreamsConfig(configProps(true)), true);&lt;br/&gt;
+&lt;br/&gt;
         internalTopologyBuilder.addSource(null, &quot;name&quot;, null, null, null, topic1);&lt;br/&gt;
         internalTopologyBuilder.addSink(&quot;out&quot;, &quot;output&quot;, null, null, null, &quot;name&quot;);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final StreamThread thread = createStreamThread(clientId, new StreamsConfig(configProps(true)), true);&lt;br/&gt;
-&lt;br/&gt;
         thread.setState(StreamThread.State.RUNNING);&lt;br/&gt;
         thread.rebalanceListener.onPartitionsRevoked(null);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -713,19 +814,12 @@ private StreamThread setupStreamThread() &lt;/p&gt;
{
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
 
-        thread.runOnce(-1);
+        thread.runOnce();
 
         assertThat(thread.tasks().size(), equalTo(1));
-        return thread;
-    }
&lt;p&gt;-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerGotFencedInCommitTransactionWhenSuspendingTaks() 
{
-        final StreamThread thread = setupStreamThread();
 
         clientSupplier.producers.get(0).fenceProducer();
         thread.rebalanceListener.onPartitionsRevoked(null);
-
         assertTrue(clientSupplier.producers.get(0).transactionInFlight());
         assertFalse(clientSupplier.producers.get(0).transactionCommitted());
         assertTrue(clientSupplier.producers.get(0).closed());
@@ -733,8 +827,32 @@ public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerGotFencedIn
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerGotFencedInCloseTransactionWhenSuspendingTaks() {&lt;/li&gt;
	&lt;li&gt;final StreamThread thread = setupStreamThread();&lt;br/&gt;
+    public void shouldCloseTaskAsZombieAndRemoveFromActiveTasksIfProducerGotFencedInCloseTransactionWhenSuspendingTasks() {&lt;br/&gt;
+        final StreamThread thread = createStreamThread(clientId, new StreamsConfig(configProps(true)), true);&lt;br/&gt;
+&lt;br/&gt;
+        internalTopologyBuilder.addSource(null, &quot;name&quot;, null, null, null, topic1);&lt;br/&gt;
+        internalTopologyBuilder.addSink(&quot;out&quot;, &quot;output&quot;, null, null, null, &quot;name&quot;);&lt;br/&gt;
+&lt;br/&gt;
+        thread.setState(StreamThread.State.RUNNING);&lt;br/&gt;
+        thread.rebalanceListener.onPartitionsRevoked(null);&lt;br/&gt;
+&lt;br/&gt;
+        final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; activeTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        final List&amp;lt;TopicPartition&amp;gt; assignedPartitions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+        // assign single partition&lt;br/&gt;
+        assignedPartitions.add(t1p1);&lt;br/&gt;
+        activeTasks.put(task1, Collections.singleton(t1p1));&lt;br/&gt;
+&lt;br/&gt;
+        thread.taskManager().setAssignmentMetadata(activeTasks, Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap());&lt;br/&gt;
+&lt;br/&gt;
+        final MockConsumer&amp;lt;byte[], byte[]&amp;gt; mockConsumer = (MockConsumer&amp;lt;byte[], byte[]&amp;gt;) thread.consumer;&lt;br/&gt;
+        mockConsumer.assign(assignedPartitions);&lt;br/&gt;
+        mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
+        thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;br/&gt;
+&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
+&lt;br/&gt;
+        assertThat(thread.tasks().size(), equalTo(1));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         clientSupplier.producers.get(0).fenceProducerOnClose();&lt;br/&gt;
         thread.rebalanceListener.onPartitionsRevoked(null);&lt;br/&gt;
@@ -789,7 +907,7 @@ public void shouldReturnActiveTaskMetadataWhileRunningState() {&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final ThreadMetadata threadMetadata = thread.threadMetadata();&lt;br/&gt;
         assertEquals(StreamThread.State.RUNNING.name(), threadMetadata.threadState());&lt;br/&gt;
@@ -834,7 +952,7 @@ public void shouldReturnStandbyTaskMetadataWhileRunningState() {&lt;/p&gt;

&lt;p&gt;         thread.rebalanceListener.onPartitionsAssigned(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final ThreadMetadata threadMetadata = thread.threadMetadata();&lt;br/&gt;
         assertEquals(StreamThread.State.RUNNING.name(), threadMetadata.threadState());&lt;br/&gt;
@@ -900,7 +1018,7 @@ public void shouldUpdateStandbyTask() throws IOException {&lt;/p&gt;

&lt;p&gt;         thread.rebalanceListener.onPartitionsAssigned(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final StandbyTask standbyTask1 = thread.taskManager().standbyTask(partition1);&lt;br/&gt;
         final StandbyTask standbyTask2 = thread.taskManager().standbyTask(partition2);&lt;br/&gt;
@@ -967,7 +1085,7 @@ public void close() {}&lt;br/&gt;
         clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(0, punctuatedStreamTime.size());&lt;br/&gt;
         assertEquals(0, punctuatedWallClockTime.size());&lt;br/&gt;
@@ -977,14 +1095,14 @@ public void close() {}&lt;br/&gt;
             clientSupplier.consumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic1, 1, i, i * 100L, TimestampType.CREATE_TIME, ConsumerRecord.NULL_CHECKSUM, (&quot;K&quot; + i).getBytes().length, (&quot;V&quot; + i).getBytes().length, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));&lt;br/&gt;
         }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertEquals(1, punctuatedStreamTime.size());&lt;br/&gt;
         assertEquals(1, punctuatedWallClockTime.size());&lt;/p&gt;

&lt;p&gt;         mockTime.sleep(100L);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // we should skip stream time punctuation, only trigger wall-clock time punctuation&lt;br/&gt;
         assertEquals(1, punctuatedStreamTime.size());&lt;br/&gt;
@@ -1177,7 +1295,7 @@ public void shouldRecordSkippedMetricForDeserializationException() {&lt;br/&gt;
         mockConsumer.assign(Collections.singleton(t1p1));&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final MetricName skippedTotalMetric = metrics.metricName(&quot;skipped-records-total&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
         final MetricName skippedRateMetric = metrics.metricName(&quot;skipped-records-rate&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
@@ -1187,7 +1305,7 @@ public void shouldRecordSkippedMetricForDeserializationException() {&lt;br/&gt;
         long offset = -1;&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, &quot;I am not an integer.&quot;.getBytes()));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, &quot;I am not an integer.&quot;.getBytes()));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(2.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1221,7 +1339,7 @@ public void shouldReportSkippedRecordsForInvalidTimestamps() {&lt;br/&gt;
         mockConsumer.assign(Collections.singleton(t1p1));&lt;br/&gt;
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));&lt;br/&gt;
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final MetricName skippedTotalMetric = metrics.metricName(&quot;skipped-records-total&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
         final MetricName skippedRateMetric = metrics.metricName(&quot;skipped-records-rate&quot;, &quot;stream-metrics&quot;, Collections.singletonMap(&quot;client-id&quot;, thread.getName()));&lt;br/&gt;
@@ -1231,7 +1349,7 @@ public void shouldReportSkippedRecordsForInvalidTimestamps() {&lt;br/&gt;
         long offset = -1;&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(2.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -1239,13 +1357,13 @@ public void shouldReportSkippedRecordsForInvalidTimestamps() {&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(6.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;br/&gt;
         mockConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(t1p1.topic(), t1p1.partition(), ++offset, 1, TimestampType.CREATE_TIME, -1, -1, -1, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;thread.runOnce(-1);&lt;br/&gt;
+        thread.runOnce();&lt;br/&gt;
         assertEquals(6.0, metrics.metric(skippedTotalMetric).metricValue());&lt;br/&gt;
         assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java&lt;br/&gt;
index 508f2ee6dd7..b0e7fce2bbe 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java&lt;br/&gt;
@@ -590,19 +590,19 @@ public void shouldIgnorePurgeDataErrors() {&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldMaybeCommitActiveTasks() &lt;/p&gt;
{
-        EasyMock.expect(active.maybeCommit()).andReturn(5);
+        EasyMock.expect(active.maybeCommitPerUserRequested()).andReturn(5);
         replay();
 
-        assertThat(taskManager.maybeCommitActiveTasks(), equalTo(5));
+        assertThat(taskManager.maybeCommitActiveTasksPerUserRequested(), equalTo(5));
         verify(active);
     }

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldProcessActiveTasks() &lt;/p&gt;
{
-        EasyMock.expect(active.process()).andReturn(10);
+        EasyMock.expect(active.process(0L)).andReturn(10);
         replay();
 
-        assertThat(taskManager.process(), equalTo(10));
+        assertThat(taskManager.process(0L), equalTo(10));
         verify(active);
     }

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/test/MockProcessor.java b/streams/src/test/java/org/apache/kafka/test/MockProcessor.java&lt;br/&gt;
index 927be0b4911..c95f4086ef1 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/test/MockProcessor.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/test/MockProcessor.java&lt;br/&gt;
@@ -40,6 +40,8 @@&lt;br/&gt;
     private final PunctuationType punctuationType;&lt;br/&gt;
     private final long scheduleInterval;&lt;/p&gt;

&lt;p&gt;+    private boolean commitRequested = false;&lt;br/&gt;
+&lt;br/&gt;
     public MockProcessor(final PunctuationType punctuationType, final long scheduleInterval) {&lt;br/&gt;
         this.punctuationType = punctuationType;&lt;br/&gt;
         this.scheduleInterval = scheduleInterval;&lt;br/&gt;
@@ -76,6 +78,10 @@ public void process(final K key, final V value) {&lt;br/&gt;
         processed.add((key == null ? &quot;null&quot; : key) + &quot;:&quot; +&lt;br/&gt;
                 (value == null ? &quot;null&quot; : value));&lt;/p&gt;

&lt;p&gt;+        if (commitRequested) &lt;/p&gt;
{
+            context().commit();
+            commitRequested = false;
+        }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;     public void checkAndClearProcessResult(final String... expected) {&lt;br/&gt;
@@ -87,6 +93,10 @@ public void checkAndClearProcessResult(final String... expected) &lt;/p&gt;
{
         processed.clear();
     }

&lt;p&gt;+    public void requestCommit() &lt;/p&gt;
{
+        commitRequested = true;
+    }
&lt;p&gt;+&lt;br/&gt;
     public void checkEmptyAndClearProcessResult() {&lt;br/&gt;
         assertEquals(&quot;the number of outputs:&quot;, 0, processed.size());&lt;br/&gt;
         processed.clear();&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16611373" author="githubbot" created="Tue, 11 Sep 2018 23:28:55 +0000"  >&lt;p&gt;guozhangwang closed pull request #5458: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;, Documentations: Add out of ordering in concepts.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5458&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5458&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/docs/streams/core-concepts.html b/docs/streams/core-concepts.html&lt;br/&gt;
index 3f9eab57ecd..015fbb495f3 100644&lt;br/&gt;
&amp;#8212; a/docs/streams/core-concepts.html&lt;br/&gt;
+++ b/docs/streams/core-concepts.html&lt;br/&gt;
@@ -172,6 +172,37 @@ &amp;lt;h2&amp;gt;&amp;lt;a id=&quot;streams_processing_guarantee&quot; href=&quot;#streams_processing_guarantee&quot;&amp;gt;Pr&lt;br/&gt;
         More details can be found in the &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation#streamsconfigs&quot;&amp;gt;&amp;lt;b&amp;gt;Kafka Streams Configs&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt; section.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;+    &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_out_of_ordering&quot; href=&quot;#streams_out_of_ordering&quot;&amp;gt;Out-of-Order Handling&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;p&amp;gt;&lt;br/&gt;
+        Besides the guarantee that each record will be processed exactly-once, another issue that many stream processing application will face is how to&lt;br/&gt;
+        handle &amp;lt;a href=&quot;tbd&quot;&amp;gt;out-of-order data&amp;lt;/a&amp;gt; that may impact their business logic. In Kafka Streams, there are two causes that could potentially&lt;br/&gt;
+        result in out-of-order data arrivals with respect to their timestamps:&lt;br/&gt;
+    &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; Within a topic-partition, a record&apos;s timestamp may not be monotonically increasing along with their offsets. Since Kafka Streams will always try to process records within a topic-partition to follow the offset order,&lt;br/&gt;
+            it can cause records with larger timestamps (but smaller offsets) to be processed earlier than records with smaller timestamps (but larger offsets) in the same topic-partition.&lt;br/&gt;
+        &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; Within a &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/architecture#streams_architecture_tasks&quot;&amp;gt;stream task&amp;lt;/a&amp;gt; that may be processing multiple topic-partitions, if users configure the application to not wait for all partitions to contain some buffered data and&lt;br/&gt;
+             pick from the partition with the smallest timestamp to process the next record, then later on when some records are fetched for other topic-partitions, their timestamps may be smaller than those processed records fetched from another topic-partition.&lt;br/&gt;
+        &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;p&amp;gt;&lt;br/&gt;
+        For stateless operations, out-of-order data will not impact processing logic since only one record is considered at a time, without looking into the history of past processed records;&lt;br/&gt;
+        for stateful operations such as aggregations and joins, however, out-of-order data could cause the processing logic to be incorrect. If users want to handle such out-of-order data, generally they need to allow their applications&lt;br/&gt;
+        to wait for longer time while bookkeeping their states during the wait time, i.e. making trade-off decisions between latency, cost, and correctness.&lt;br/&gt;
+        In Kafka Streams specifically, users can configure their window operators for windowed aggregations to achieve such trade-offs (details can be found in &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/developer-guide&quot;&amp;gt;&amp;lt;b&amp;gt;Developer Guide&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;).&lt;br/&gt;
+        As for Joins, users have to be aware that some of the out-of-order data cannot be handled by increasing on latency and cost in Streams yet:&lt;br/&gt;
+    &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+    &amp;lt;ul&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; For Stream-Stream joins, all three types (inner, outer, left) handle out-of-order records correctly, but the resulted stream may contain unnecessary leftRecord-null for left joins, and leftRecord-null or null-rightRecord for outer joins. &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; For Stream-Table joins, out-of-order records are not handled (i.e., Streams applications don&apos;t check for out-of-order records and just process all records in offset order), and hence it may produce unpredictable results. &amp;lt;/li&amp;gt;&lt;br/&gt;
+        &amp;lt;li&amp;gt; For Table-Table joins, out-of-order records are not handled (i.e., Streams applications don&apos;t check for out-of-order records and just process all records in offset order). However, the join result is a changelog stream and hence will be eventually consistent. &amp;lt;/li&amp;gt;&lt;br/&gt;
+    &amp;lt;/ul&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
     &amp;lt;div class=&quot;pagination&quot;&amp;gt;&lt;br/&gt;
         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/tutorial&quot; class=&quot;pagination_&lt;em&gt;btn pagination&lt;/em&gt;&lt;em&gt;btn&lt;/em&gt;_prev&quot;&amp;gt;Previous&amp;lt;/a&amp;gt;&lt;br/&gt;
         &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/architecture&quot; class=&quot;pagination_&lt;em&gt;btn pagination&lt;/em&gt;&lt;em&gt;btn&lt;/em&gt;_next&quot;&amp;gt;Next&amp;lt;/a&amp;gt;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16622939" author="githubbot" created="Fri, 21 Sep 2018 01:31:04 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5669: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Modify pause logic if we being enforced processing&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5669&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5669&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   1. When we are being enforced processing (idleStartTime is not UNKNOWN), we will only resume a partition if it is drained empty, instead of below the max.partition buffer.&lt;/p&gt;

&lt;p&gt;   2. Related upgrade docs, and unit tests.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16632668" author="githubbot" created="Sat, 29 Sep 2018 00:19:42 +0000"  >&lt;p&gt;guozhangwang closed pull request #5669: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Modify pause logic if we being enforced processing&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5669&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5669&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html&lt;br/&gt;
index 35e1f77fd4c..a4e08bad479 100644&lt;br/&gt;
&amp;#8212; a/docs/streams/upgrade-guide.html&lt;br/&gt;
+++ b/docs/streams/upgrade-guide.html&lt;br/&gt;
@@ -90,6 +90,7 @@ &amp;lt;h1&amp;gt;Upgrade Guide and API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
         We have also removed some public APIs that are deprecated prior to 1.0.x in 2.0.0.&lt;br/&gt;
         See below for a detailed list of removed APIs.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_210&quot; href=&quot;#streams_api_changes_210&quot;&amp;gt;Streams API changes in 2.1.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
     &amp;lt;p&amp;gt;&lt;br/&gt;
         We updated &amp;lt;code&amp;gt;TopologyDescription&amp;lt;/code&amp;gt; API to allow for better runtime checking.&lt;br/&gt;
@@ -99,6 +100,14 @@ &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_210&quot; href=&quot;#streams_api_changes_210&quot;&amp;gt;Streams API&lt;br/&gt;
         &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-321%3A+Update+TopologyDescription+to+better+represent+Source+and+Sink+Nodes&quot;&amp;gt;KIP-321&amp;lt;/a&amp;gt;.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;+    &amp;lt;p&amp;gt;&lt;br/&gt;
+        We&apos;ve added a new config named &amp;lt;code&amp;gt;max.task.idle.ms&amp;lt;/code&amp;gt; to allow users specify how to handle out-of-order data within a task that may be processing multiple&lt;br/&gt;
+        topic-partitions (see &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/core-concepts.html#streams_out_of_ordering&quot;&amp;gt;Out-of-Order Handling&amp;lt;/a&amp;gt; section for more details).&lt;br/&gt;
+        The default value is set to &amp;lt;code&amp;gt;0&amp;lt;/code&amp;gt;, to favor minimized latency over synchronization between multiple input streams from topic-partitions.&lt;br/&gt;
+        If users would like to wait for longer time when some of the topic-partitions do not have data available to process and hence cannot determine its corresponding stream time,&lt;br/&gt;
+        they can override this config to a larger value.&lt;br/&gt;
+    &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_200&quot; href=&quot;#streams_api_changes_200&quot;&amp;gt;Streams API changes in 2.0.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
     &amp;lt;p&amp;gt;&lt;br/&gt;
         We have removed the &amp;lt;code&amp;gt;skippedDueToDeserializationError-rate&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;skippedDueToDeserializationError-total&amp;lt;/code&amp;gt; metrics.&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index 2f97b7f27ba..b77a18ef85e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -233,6 +233,7 @@ public StreamTask(final TaskId id,&lt;br/&gt;
             partitionQueues.put(partition, queue);&lt;br/&gt;
         }&lt;/p&gt;

&lt;p&gt;+        idleStartTime = RecordQueue.UNKNOWN;&lt;br/&gt;
         recordInfo = new PartitionGroup.RecordInfo();&lt;br/&gt;
         partitionGroup = new PartitionGroup(partitionQueues);&lt;br/&gt;
         processorContextImpl.setStreamTimeSupplier(partitionGroup::timestamp);&lt;br/&gt;
@@ -355,10 +356,19 @@ public boolean process() {&lt;br/&gt;
             consumedOffsets.put(partition, record.offset());&lt;br/&gt;
             commitNeeded = true;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// after processing this record, if its partition queue&apos;s buffered size has been&lt;/li&gt;
	&lt;li&gt;// decreased to the threshold, we can then resume the consumption on this partition&lt;/li&gt;
	&lt;li&gt;if (recordInfo.queue().size() == maxBufferedSize) {&lt;/li&gt;
	&lt;li&gt;consumer.resume(singleton(partition));&lt;br/&gt;
+            // if we are not in the enforced processing state, then after processing&lt;br/&gt;
+            // this record, if its partition queue&apos;s buffered size has been decreased below&lt;br/&gt;
+            // the threshold, we can then resume the consumption on this partition;&lt;br/&gt;
+            // otherwise, we only resume the consumption on this partition after it&lt;br/&gt;
+            // has been drained.&lt;br/&gt;
+            if (idleStartTime != RecordQueue.UNKNOWN) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                if (recordInfo.queue().isEmpty()) {
+                    consumer.resume(singleton(partition));
+                }&lt;br/&gt;
+            } else {&lt;br/&gt;
+                if (recordInfo.queue().size() == maxBufferedSize) {+                    consumer.resume(singleton(partition));+                }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;         } catch (final ProducerFencedException fatal) {&lt;br/&gt;
             throw new TaskMigratedException(this, fatal);&lt;br/&gt;
@@ -713,10 +723,19 @@ public void addRecords(final TopicPartition partition, final Iterable&amp;lt;ConsumerRe&lt;br/&gt;
             log.trace(&quot;Added records into the buffered queue of partition {}, new queue size is {}&quot;, partition, newQueueSize);&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// if after adding these records, its partition queue&apos;s buffered size has been&lt;/li&gt;
	&lt;li&gt;// increased beyond the threshold, we can then pause the consumption for this partition&lt;/li&gt;
	&lt;li&gt;if (newQueueSize &amp;gt; maxBufferedSize) {&lt;/li&gt;
	&lt;li&gt;consumer.pause(singleton(partition));&lt;br/&gt;
+        // if we are not in the enforced processing state, then after adding these records,&lt;br/&gt;
+        // we can then pause the consumption for this partition if its partition queue&apos;s&lt;br/&gt;
+        // buffered size has been increased beyond the threshold;&lt;br/&gt;
+        // otherwise, we will immediately pause the consumption on this partition after it&lt;br/&gt;
+        // has at least some records already&lt;br/&gt;
+        if (idleStartTime != RecordQueue.UNKNOWN) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            if (newQueueSize &amp;gt; 0) {
+                consumer.pause(singleton(partition));
+            }&lt;br/&gt;
+        } else {&lt;br/&gt;
+            if (newQueueSize &amp;gt; maxBufferedSize) {+                consumer.pause(singleton(partition));+            }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
index d332b5b8394..7447633c011 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
@@ -273,7 +273,12 @@ public void testPauseResume() {&lt;br/&gt;
             getConsumerRecord(partition2, 65)&lt;br/&gt;
         ));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(task.process());&lt;br/&gt;
+        assertEquals(6, task.numBuffered());&lt;br/&gt;
+        assertEquals(1, consumer.paused().size());&lt;br/&gt;
+        assertTrue(consumer.paused().contains(partition2));&lt;br/&gt;
+&lt;br/&gt;
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());&lt;br/&gt;
+        assertEquals(5, task.numBuffered());&lt;br/&gt;
         assertEquals(1, source1.numReceived);&lt;br/&gt;
         assertEquals(0, source2.numReceived);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -286,29 +291,92 @@ public void testPauseResume() &lt;/p&gt;
{
             getConsumerRecord(partition1, 50)
         ));
 
+        assertEquals(8, task.numBuffered());
         assertEquals(2, consumer.paused().size());
         assertTrue(consumer.paused().contains(partition1));
         assertTrue(consumer.paused().contains(partition2));
 
-        assertTrue(task.process());
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());
+        assertEquals(7, task.numBuffered());
         assertEquals(2, source1.numReceived);
         assertEquals(0, source2.numReceived);
 
         assertEquals(1, consumer.paused().size());
         assertTrue(consumer.paused().contains(partition2));
 
-        assertTrue(task.process());
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());
+        assertEquals(6, task.numBuffered());
         assertEquals(3, source1.numReceived);
         assertEquals(0, source2.numReceived);
 
         assertEquals(1, consumer.paused().size());
         assertTrue(consumer.paused().contains(partition2));
 
-        assertTrue(task.process());
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());
+        assertEquals(5, task.numBuffered());
         assertEquals(3, source1.numReceived);
         assertEquals(1, source2.numReceived);
 
         assertEquals(0, consumer.paused().size());
+
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());  // 1: 40
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());  // 2: 45
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());  // 1: 50
+        assertEquals(2, task.numBuffered());
+        assertEquals(5, source1.numReceived);
+        assertEquals(2, source2.numReceived);
+        assertEquals(0, consumer.paused().size());
+
+        assertFalse(task.isProcessable(time.milliseconds()));  // we are idle now
+
+        time.sleep(100L);
+
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());  // start enforce processing
+        assertEquals(1, task.numBuffered());
+        assertEquals(5, source1.numReceived);
+        assertEquals(3, source2.numReceived);  // 1: 55
+        assertEquals(0, consumer.paused().size());
+
+        task.addRecords(partition2, Arrays.asList(
+            getConsumerRecord(partition2, 70),
+            getConsumerRecord(partition2, 80),
+            getConsumerRecord(partition2, 90)
+        ));
+
+        assertEquals(4, task.numBuffered());
+        assertEquals(1, consumer.paused().size());
+        assertTrue(consumer.paused().contains(partition2));
+
+        // we are enforced processing now
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());
+        assertEquals(3, task.numBuffered());
+        assertEquals(1, consumer.paused().size());
+        assertTrue(consumer.paused().contains(partition2));
+
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());
+        assertEquals(2, task.numBuffered());
+        assertEquals(1, consumer.paused().size());
+        assertTrue(consumer.paused().contains(partition2));
+
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());
+        assertEquals(1, task.numBuffered());
+        assertEquals(1, consumer.paused().size());
+        assertTrue(consumer.paused().contains(partition2));
+
+        // only resume if we are enforced processing when the fetched partition is empty
+        assertTrue(task.isProcessable(time.milliseconds()) &amp;amp;&amp;amp; task.process());
+        assertEquals(0, task.numBuffered());
+        assertEquals(0, consumer.paused().size());
+
+        task.addRecords(partition2, Arrays.asList(
+            getConsumerRecord(partition2, 100),
+            getConsumerRecord(partition2, 110)
+        ));
+
+        // pause immediately if we are enforced processing when there are at least some records already
+        assertEquals(2, task.numBuffered());
+        assertEquals(1, consumer.paused().size());
+        assertTrue(consumer.paused().contains(partition2));
     }

&lt;p&gt;     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
@@ -1392,7 +1460,7 @@ protected void flushState() {&lt;br/&gt;
             topicPartition.topic(),&lt;br/&gt;
             topicPartition.partition(),&lt;br/&gt;
             offset,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;0L,&lt;br/&gt;
+            offset, // use the offset as the timestamp&lt;br/&gt;
             TimestampType.CREATE_TIME,&lt;br/&gt;
             0L,&lt;br/&gt;
             0,&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16632669" author="githubbot" created="Sat, 29 Sep 2018 00:20:07 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5714: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Upgrade Documentation&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5714&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5714&lt;/a&gt;&lt;/p&gt;




&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16633481" author="githubbot" created="Sun, 30 Sep 2018 19:01:14 +0000"  >&lt;p&gt;guozhangwang closed pull request #5714: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3514&quot; title=&quot;Stream timestamp computation needs some further thoughts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-3514&quot;&gt;&lt;del&gt;KAFKA-3514&lt;/del&gt;&lt;/a&gt;: Upgrade Documentation&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5714&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5714&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html&lt;br/&gt;
index 35e1f77fd4c..a4e08bad479 100644&lt;br/&gt;
&amp;#8212; a/docs/streams/upgrade-guide.html&lt;br/&gt;
+++ b/docs/streams/upgrade-guide.html&lt;br/&gt;
@@ -90,6 +90,7 @@ &amp;lt;h1&amp;gt;Upgrade Guide and API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
         We have also removed some public APIs that are deprecated prior to 1.0.x in 2.0.0.&lt;br/&gt;
         See below for a detailed list of removed APIs.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_210&quot; href=&quot;#streams_api_changes_210&quot;&amp;gt;Streams API changes in 2.1.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
     &amp;lt;p&amp;gt;&lt;br/&gt;
         We updated &amp;lt;code&amp;gt;TopologyDescription&amp;lt;/code&amp;gt; API to allow for better runtime checking.&lt;br/&gt;
@@ -99,6 +100,14 @@ &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_210&quot; href=&quot;#streams_api_changes_210&quot;&amp;gt;Streams API&lt;br/&gt;
         &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-321%3A+Update+TopologyDescription+to+better+represent+Source+and+Sink+Nodes&quot;&amp;gt;KIP-321&amp;lt;/a&amp;gt;.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;+    &amp;lt;p&amp;gt;&lt;br/&gt;
+        We&apos;ve added a new config named &amp;lt;code&amp;gt;max.task.idle.ms&amp;lt;/code&amp;gt; to allow users specify how to handle out-of-order data within a task that may be processing multiple&lt;br/&gt;
+        topic-partitions (see &amp;lt;a href=&quot;/&lt;tt&gt;version&lt;/tt&gt;/documentation/streams/core-concepts.html#streams_out_of_ordering&quot;&amp;gt;Out-of-Order Handling&amp;lt;/a&amp;gt; section for more details).&lt;br/&gt;
+        The default value is set to &amp;lt;code&amp;gt;0&amp;lt;/code&amp;gt;, to favor minimized latency over synchronization between multiple input streams from topic-partitions.&lt;br/&gt;
+        If users would like to wait for longer time when some of the topic-partitions do not have data available to process and hence cannot determine its corresponding stream time,&lt;br/&gt;
+        they can override this config to a larger value.&lt;br/&gt;
+    &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
     &amp;lt;h3&amp;gt;&amp;lt;a id=&quot;streams_api_changes_200&quot; href=&quot;#streams_api_changes_200&quot;&amp;gt;Streams API changes in 2.0.0&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;br/&gt;
     &amp;lt;p&amp;gt;&lt;br/&gt;
         We have removed the &amp;lt;code&amp;gt;skippedDueToDeserializationError-rate&amp;lt;/code&amp;gt; and &amp;lt;code&amp;gt;skippedDueToDeserializationError-total&amp;lt;/code&amp;gt; metrics.&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
index d332b5b8394..7ff7c708395 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java&lt;br/&gt;
@@ -1392,7 +1392,7 @@ protected void flushState() {&lt;br/&gt;
             topicPartition.topic(),&lt;br/&gt;
             topicPartition.partition(),&lt;br/&gt;
             offset,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;0L,&lt;br/&gt;
+            offset, // use the offset as the timestamp&lt;br/&gt;
             TimestampType.CREATE_TIME,&lt;br/&gt;
             0L,&lt;br/&gt;
             0,&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310060">
                    <name>Container</name>
                                            <outwardlinks description="contains">
                                        <issuelink>
            <issuekey id="12957311">KAFKA-3534</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13188268">KAFKA-7458</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13068023">KAFKA-5144</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13071763">KAFKA-5233</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 7 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2votb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>