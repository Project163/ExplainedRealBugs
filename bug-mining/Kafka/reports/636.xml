<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:42:48 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-527] Compression support does numerous byte copies</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-527</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;The data path for compressing or decompressing messages is extremely inefficient. We do something like 7 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; complete copies of the data, often for simple things like adding a 4 byte size to the front. I am not sure how this went by unnoticed.&lt;/p&gt;

&lt;p&gt;This is likely the root cause of the performance issues we saw in doing bulk recompression of data in mirror maker.&lt;/p&gt;

&lt;p&gt;The mismatch between the InputStream and OutputStream interfaces and the Message/MessageSet interfaces which are based on byte buffers is the cause of many of these.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12608923">KAFKA-527</key>
            <summary>Compression support does numerous byte copies</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yasuhiro.matsuda">Yasuhiro Matsuda</assignee>
                                    <reporter username="jkreps">Jay Kreps</reporter>
                        <labels>
                    </labels>
                <created>Mon, 24 Sep 2012 20:38:57 +0000</created>
                <updated>Sun, 29 Mar 2015 22:53:47 +0000</updated>
                            <resolved>Thu, 26 Mar 2015 22:51:36 +0000</resolved>
                                                                    <component>compression</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="13728452" author="jkreps" created="Sat, 3 Aug 2013 05:06:01 +0000"  >&lt;p&gt;To test performance checkout trunk and do&lt;br/&gt;
./sbt package test:package&lt;br/&gt;
./bin/kafka-run-class.sh kafka.TestLinearWriteSpeed --bytes 1007483648 --files 1 --log --message-size 4096 --size 109600 --compression snappy&lt;br/&gt;
./bin/kafka-run-class.sh kafka.TestLinearWriteSpeed --bytes 1007483648 --files 1 --log --message-size 4096 --size 109600 --compression none&lt;/p&gt;

&lt;p&gt;Performance (on my laptop):&lt;br/&gt;
none: 75 MB/sec&lt;br/&gt;
snappy: 18 MB/sec&lt;/p&gt;

&lt;p&gt;Why is this so slow? After all snappy claims a roundtrip performance of around 200MB/sec...&lt;/p&gt;

&lt;p&gt;If you look at the attached hprof traces without compression you see the following:&lt;br/&gt;
     1 79.87% 79.87%    2004 300968 sun.nio.ch.FileDispatcher.write0&lt;br/&gt;
   2  5.38% 85.25%     135 300978 kafka.utils.Utils$.crc32&lt;br/&gt;
   3  5.06% 90.31%     127 301074 sun.nio.ch.FileChannelImpl.force0&lt;br/&gt;
   4  1.79% 92.11%      45 301075 java.nio.MappedByteBuffer.force0&lt;br/&gt;
I.e. 80% goes to writing, 5 percent goes to computing crcs, and about 7% goes to flushing, then a long tail&lt;/p&gt;

&lt;p&gt;If you look at the same trace with compression you expect to see that all the time goes to compressing stuff, right? Wrong:&lt;br/&gt;
   1 16.44% 16.44%     807 301044 org.xerial.snappy.SnappyNative.arrayCopy&lt;br/&gt;
   2 14.81% 31.24%     727 301073 java.util.Arrays.copyOf&lt;br/&gt;
   3  9.61% 40.86%     472 301053 org.xerial.snappy.SnappyNative.rawCompress&lt;br/&gt;
   4  7.45% 48.31%     366 301084 org.xerial.snappy.SnappyNative.rawUncompress&lt;br/&gt;
   5  5.60% 53.91%     275 301063 java.io.ByteArrayOutputStream.&amp;lt;init&amp;gt;&lt;br/&gt;
   6  5.13% 59.04%     252 301090 sun.nio.ch.FileDispatcher.write0&lt;br/&gt;
   7  4.32% 63.36%     212 301074 java.nio.HeapByteBuffer.&amp;lt;init&amp;gt;&lt;br/&gt;
   8  3.97% 67.33%     195 301049 java.util.Arrays.copyOf&lt;br/&gt;
   9  3.83% 71.16%     188 301070 org.xerial.snappy.SnappyNative.arrayCopy&lt;br/&gt;
  10  3.22% 74.38%     158 301068 org.xerial.snappy.SnappyNative.rawUncompress&lt;br/&gt;
  11  2.10% 76.48%     103 301065 org.xerial.snappy.SnappyNative.arrayCopy&lt;br/&gt;
  12  1.91% 78.39%      94 301089 java.nio.HeapByteBuffer.&amp;lt;init&amp;gt;&lt;/p&gt;

&lt;p&gt;If you tally this up you see that about 50% of the time is going to array copying and allocation and a mere 22% going to compression with the actual cost of the write knocked down to 5%.&lt;/p&gt;

&lt;p&gt;Pretty sad.&lt;/p&gt;</comment>
                            <comment id="13728454" author="jkreps" created="Sat, 3 Aug 2013 05:11:52 +0000"  >&lt;p&gt;I should mention that to run the above test you need to apply the patch from &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-615&quot; title=&quot;Avoid fsync on log segment roll&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-615&quot;&gt;&lt;del&gt;KAFKA-615&lt;/del&gt;&lt;/a&gt; (v5) to get the compression option and log support in the throughput test.&lt;/p&gt;</comment>
                            <comment id="13728575" author="jkreps" created="Sat, 3 Aug 2013 16:35:54 +0000"  >&lt;p&gt;The general idea of the refactoring would be to allow directly appending to a ByteBufferMessageSet. Perhaps we could add a static method Message.write(byteBuffer, key, value) and a ByteBufferMessageSet.append that works in-place.&lt;/p&gt;

&lt;p&gt;The compression codec would likely need to change from an OutputStream and InputStream to something that worked directly with byte[].&lt;/p&gt;

&lt;p&gt;This is straight-forward for snappy but requires a little more work for gzip to get the header right since I think they only provide array access from the more generic inflate/deflate codec (see Deflater.java and GZIPOutputStream.java in the jdk).&lt;/p&gt;</comment>
                            <comment id="13741294" author="guozhang" created="Thu, 15 Aug 2013 18:15:42 +0000"  >&lt;p&gt;One alternative approach would be like this:&lt;/p&gt;

&lt;p&gt;Currently in the compression code (ByteBufferMessageSet.create), for each message we write 1) the incrementing logical offset in LONG, 2) the message byte size in INT, and 3) the message payload.&lt;/p&gt;

&lt;p&gt;The idea is that since the logical offset is just incrementing, hence with a compressed message, as long as we know the offset of the first message, we would know the offset of the rest messages without even reading the offset field.&lt;/p&gt;

&lt;p&gt;So we can ignore reading the offset of each message inside of the compressed message but only the offset of the wrapper message which is the offset of the last message + 1, and then in assignOffsets just modify the offset of the wrapper message. Another change would be at the consumer side, the iterator would need to be smart of interpreting the offsets of messages while deep-iterating the compressed message.&lt;/p&gt;

&lt;p&gt;As Jay pointed out, this method would not work with log compaction since it would break the assumption that offsets increments continuously. Two workarounds of this issue:&lt;/p&gt;

&lt;p&gt;1) In log compaction, instead of deleting the to-be-deleted-message just setting its payload to null but keep its header and hence keeping its slot in the incrementing offset.&lt;br/&gt;
2) During the compression process, instead of writing the absolute value of the logical offset of messages, write the deltas of their offset compared with the offset of the wrapper message. So -1 would mean continuously decrementing from the wrapper message offset, and -2/3/... would be skipping holes in side the compressed message.&lt;/p&gt;</comment>
                            <comment id="13784456" author="guozhang" created="Wed, 2 Oct 2013 21:30:57 +0000"  >&lt;p&gt;Attached a file summarizing the copying operations. If we count the shallow iterating over message sets then there are about 8 copy operations throughout the life time of a message. Among them Copy 3/4/5 could be avoided, and we could only apply one shallow iteration on each broker (leader and follower) and consumer.&lt;/p&gt;

&lt;p&gt;Given that the offset assignment is currently the main reason for converting between array of Message and ByteBufferMessageSet instead of appending Message to ByteBufferMessageSet, and that trimming invalid bytes on consumer complicates its logic, one proposal to refactoring the compression while still be backward compatible is:&lt;/p&gt;

&lt;p&gt;1) On serving fetch request, only returns complete Messages within the request rate so the consumer would not need to trim invalid bytes.&lt;br/&gt;
2) Use the first 6 bits of the attribute byte of Message to indicate the number of real messages it wraps. This will limit the number of compressed messages in one wrapper to be 256, which I think is good enough.&lt;br/&gt;
3) When doing dedup, only delete the payload of the message but keep its counter so that the offsets are still consecutively incremental. &lt;br/&gt;
4) For offset assignment on the broker, the ByteBufferMessageSet will have multiple wrapper Messages (since we have limit compress size to 256 one ProducerRequest now may have ByteBufferMessageSet of multiple wrapper messages); we need to use the shallowIterator to iterate them and reassign offsets without decompress/recompress.&lt;br/&gt;
5) On the consumer side, allow shallow iteration as well as deep iteration; with deep iteration, the consumer iterator needs to assign offset to each decompressed message based on its wrapper messages offset and size. Also add the compression codec to the MessageAndMetadata returned to the consumers, which will then solve &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1011&quot; title=&quot;Decompression and re-compression on MirrorMaker could result in messages being dropped in the pipeline&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1011&quot;&gt;&lt;del&gt;KAFKA-1011&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this we can completely remove the de/re-compression on brokers, and always use the compressed wrapper message unit throughout the pipeline. However cons of this approach is that it complicates logic on producer (256 compression batch limit) and consumer (re-compute message offsets).&lt;/p&gt;</comment>
                            <comment id="13784463" author="guozhang" created="Wed, 2 Oct 2013 21:33:36 +0000"  >&lt;p&gt;If we do not want to use the attribute byte and have the limit, then we can just still do decompression to get the size of messages in a wrapper Message. This is the same as my previous proposal which still saves us re-compression.&lt;/p&gt;</comment>
                            <comment id="14347441" author="yasuhiro.matsuda" created="Wed, 4 Mar 2015 19:43:54 +0000"  >&lt;p&gt;Created reviewboard &lt;a href=&quot;https://reviews.apache.org/r/31742/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/31742/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14347478" author="yasuhiro.matsuda" created="Wed, 4 Mar 2015 20:02:55 +0000"  >&lt;p&gt;This patch introduces BufferingOutputStream, an alternative for ByteArrayOutputStream. It is backed by a chain of byte arrays, so it does not copy bytes when increasing its capacity. Also, it has a method that writes the content to ByteBuffer directly, so there is no need to create an array instance to transfer the content to ByteBuffer. Lastly, it has a deferred write, which means that you reserve a number of bytes before knowing the value and fill it later. In MessageWriter (a new class), it is used for writing the CRC value and the payload length.&lt;/p&gt;

&lt;p&gt;On laptop,I tested the performance using TestLinearWriteSpeed with snappy.&lt;/p&gt;

&lt;p&gt;Previously&lt;br/&gt;
26.64786026813998 MB per sec&lt;/p&gt;

&lt;p&gt;With the patch&lt;br/&gt;
35.78401869390889 MB per sec&lt;/p&gt;

&lt;p&gt;The improvement is about 34% better throughput.&lt;/p&gt;</comment>
                            <comment id="14351177" author="guozhang" created="Sat, 7 Mar 2015 00:08:45 +0000"  >&lt;p&gt;Thanks for the patch, this is very promising.&lt;/p&gt;

&lt;p&gt;There are a couple of issues we want to resolve here:&lt;/p&gt;

&lt;p&gt;1. ByteArrayOutputStream copies data upon overflowing and resizing.&lt;/p&gt;

&lt;p&gt;2. Compressed stream needs one extra copy upon finishing reading / writing.&lt;/p&gt;

&lt;p&gt;This patch is mainly aimed at #1 above, and I have uploaded a patch for optimizing decompressed iterator, just as an example for resolving #2. In addition, I think in the end we will deprecate ByeBufferMessageSet and move to o.a.k.c.r.MemoryRecords, which will resolve both points above. We can discuss whether we want to incorporate these patches into ByeBufferMessageSet now or just wait for the migration and improve on o.a.k.c.r.MemoryRecords. &lt;/p&gt;

&lt;p&gt;For example, today MemoryRecords&apos;s write pattern is only for appending messages with pre-defined &quot;records batch size&quot;, and try to close the batch when its size is approached; in ByteBufferMessageSet.create() we are given a set of messages without a predicated batch size, but it is still possible to get the value from the estimated compression ratio as we do in Compressor, such that in the worst case only one or two buffer expansions (i.e. data copies) are needed. Just is just an alternative to the linked-list buffers as proposed in this patch.&lt;/p&gt;</comment>
                            <comment id="14351180" author="guozhang" created="Sat, 7 Mar 2015 00:09:30 +0000"  >&lt;p&gt;Created reviewboard &lt;a href=&quot;https://reviews.apache.org/r/31816/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/31816/diff/&lt;/a&gt;&lt;br/&gt;
against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14352222" author="nehanarkhede" created="Sun, 8 Mar 2015 19:53:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think in the end we will deprecate ByeBufferMessageSet and move to o.a.k.c.r.MemoryRecords&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Isn&apos;t that happening in 0.9 and will be helpful only for folks using the new clients?&lt;/p&gt;

&lt;p&gt;Since &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yasuhiro.matsuda&quot; class=&quot;user-hover&quot; rel=&quot;yasuhiro.matsuda&quot;&gt;yasuhiro.matsuda&lt;/a&gt;&apos;s patch solves #1 and shows the throughput improvement, it is worth the checkin. If you want to follow up with #2, that&apos;s great too.&lt;/p&gt;</comment>
                            <comment id="14352227" author="jkreps" created="Sun, 8 Mar 2015 20:10:38 +0000"  >&lt;p&gt;The clients already use MemoryRecords, so 0.8.2 and 0.8.3 will give the speed-up to people uses the clients. I think the question is how best to get the perf improvement to the server which should be largely independent.&lt;/p&gt;

&lt;p&gt;Guozhang is correct that moving the server to MemoryRecords should be our long term plan and is the end-state we want. However the Message interface is fairly heavily used inside kafka.log so this would be a very large change to those classes. We haven&apos;t had a real discussion about how we would go about this and I don&apos;t think there is really a timeline. Several options I see:&lt;br/&gt;
1. We could do Yasu and Guozhang&apos;s fixes now: they are limited in scope, compression is a painpoint now, and we have lots of things in flight right now.&lt;br/&gt;
2. We could do a larger conversion of kafka.log to move it off Message/MessageSet/FileMessageSet/ByteBufferMessageSet as Guozhang proposes. This would be a fairly big refactoring, as there are a number of things tied to the MessageSet interface that would all have to move, and there is a significant amount of test code so this would be a big change. However this is certainly where we want to end up.&lt;br/&gt;
3. We could decide that we actually prefer java code, and given that the a significant chunk of the common code has to be in Java we should start moving chunks of the server as well. We had talked about this before but I don&apos;t think we should start until we have a real plan to finish. But anyhow if we did that we would say instead of just migrating the server from Message/MessageSet/FileMessageSet/ByteBufferMessageSet we would also just wholesale move the log subpackage to java as the first step in a larger migration. The argument both for and against this would be that instead of doing two rewrites, one to change interfaces, and a second to move scala=&amp;gt;java we could just do both at the same time.&lt;/p&gt;</comment>
                            <comment id="14352230" author="nehanarkhede" created="Sun, 8 Mar 2015 20:21:21 +0000"  >&lt;p&gt;Agree with the long term plan. Definitely makes sense, though are very large changes. I see value in checking in Yasuhiro&apos;s and Guozhang&apos;s changes given that compression is a pain point and the patches clearly show improvement. I&apos;m +1 for moving ahead.&lt;/p&gt;</comment>
                            <comment id="14352290" author="yasuhiro.matsuda" created="Sun, 8 Mar 2015 21:52:46 +0000"  >&lt;p&gt;&amp;gt;&amp;gt;This patch is mainly aimed at #1 above&lt;/p&gt;

&lt;p&gt;If you read the patch carefully, there are more for the compression part. It avoids copies to an intermediate buffer (byte array) when we do ByteArrayOutputStream to ByteBuffer, also a copy form ByteBuffer to ByteBuffer when we create a MessageSet from a Message at the end of compression.&lt;/p&gt;

&lt;p&gt;For the decompression part, your iterator patch looks nice. It seems to make ByteBufferSessageSet.decompress obsolete if you clean up all callers by using your iterator.&lt;/p&gt;</comment>
                            <comment id="14355133" author="guozhang" created="Tue, 10 Mar 2015 16:13:29 +0000"  >&lt;p&gt;Hi Yasuhiro,&lt;/p&gt;

&lt;p&gt;I thought for compressed writes, the linked list buffers in BufferingOutputStream still need to be copied to a newly allocated buffer (in line 54/55 of ByteBufferMessageSet) whereas for MemoryRecord, it append messages to the compressed stream in-place and no extra copy is required at the end of the writes, but I may misunderstood Scala&apos;s function-parameter syntax and please let me know if I did.&lt;/p&gt;

&lt;p&gt;As for the migration plan, I agree that ByteBufferMessageSet replacement would not come in the near future, and we can definitely commit the patches now as compress / de-compress has been a pain for us.&lt;/p&gt;</comment>
                            <comment id="14364085" author="yasuhiro.matsuda" created="Mon, 16 Mar 2015 22:19:49 +0000"  >&lt;p&gt;Updated reviewboard &lt;a href=&quot;https://reviews.apache.org/r/31742/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/31742/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14370703" author="yasuhiro.matsuda" created="Fri, 20 Mar 2015 04:33:38 +0000"  >&lt;p&gt;Updated reviewboard &lt;a href=&quot;https://reviews.apache.org/r/31742/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/31742/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14380547" author="yasuhiro.matsuda" created="Wed, 25 Mar 2015 19:09:05 +0000"  >&lt;p&gt;Updated reviewboard &lt;a href=&quot;https://reviews.apache.org/r/31742/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/31742/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14380657" author="junrao" created="Wed, 25 Mar 2015 20:10:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ymatsuda&quot; class=&quot;user-hover&quot; rel=&quot;ymatsuda&quot;&gt;ymatsuda&lt;/a&gt;, thanks for the patch. +1 and committed to trunk. Leave this jira open for the other patch from Guozhang.&lt;/p&gt;</comment>
                            <comment id="14380693" author="guozhang" created="Wed, 25 Mar 2015 20:26:42 +0000"  >&lt;p&gt;Updated reviewboard &lt;a href=&quot;https://reviews.apache.org/r/31816/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/31816/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14380706" author="jkreps" created="Wed, 25 Mar 2015 20:28:50 +0000"  >&lt;p&gt;Yeah it would be great to get the &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;&apos;s patch in as well and be able to summarize the improvement from the producer&apos;s point of view: that is repeat the perf test Yasuhiro did but using the producer performance test so it is more representative of what the user will actually see. That would be a nice tidbit to have for release notes on the next release.&lt;/p&gt;</comment>
                            <comment id="14382872" author="guozhang" created="Thu, 26 Mar 2015 22:50:53 +0000"  >&lt;p&gt;Both patches have been checked in, closing this ticket for now.&lt;/p&gt;</comment>
                            <comment id="14382880" author="jkreps" created="Thu, 26 Mar 2015 22:55:25 +0000"  >&lt;p&gt;Do we have any kind of before/after performance assessment from the producer&apos;s point of view? It would be nice to be able to say that you guys made the producer X% faster. Even just a simple test over localhost would be good. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14385999" author="nehanarkhede" created="Sun, 29 Mar 2015 22:53:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt; +1. Will be great to re-run the test after your patch.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12606449" name="KAFKA-527.message-copy.history" size="2518" author="guozhang" created="Wed, 2 Oct 2013 21:30:57 +0000"/>
                            <attachment id="12702593" name="KAFKA-527.patch" size="18118" author="yasuhiro.matsuda" created="Wed, 4 Mar 2015 19:43:54 +0000"/>
                            <attachment id="12704904" name="KAFKA-527_2015-03-16_15:19:29.patch" size="20864" author="yasuhiro.matsuda" created="Mon, 16 Mar 2015 22:19:48 +0000"/>
                            <attachment id="12705832" name="KAFKA-527_2015-03-19_21:32:24.patch" size="24944" author="yasuhiro.matsuda" created="Fri, 20 Mar 2015 04:33:36 +0000"/>
                            <attachment id="12707290" name="KAFKA-527_2015-03-25_12:08:00.patch" size="27638" author="yasuhiro.matsuda" created="Wed, 25 Mar 2015 19:09:04 +0000"/>
                            <attachment id="12707304" name="KAFKA-527_2015-03-25_13:26:36.patch" size="9616" author="guozhang" created="Wed, 25 Mar 2015 20:26:42 +0000"/>
                            <attachment id="12595731" name="java.hprof.no-compression.txt" size="118307" author="jkreps" created="Sat, 3 Aug 2013 04:47:13 +0000"/>
                            <attachment id="12595732" name="java.hprof.snappy.text" size="224485" author="jkreps" created="Sat, 3 Aug 2013 04:47:24 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>241645</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 34 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i029iv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11144</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>