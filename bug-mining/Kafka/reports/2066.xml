<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:15:30 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7467] NoSuchElementException is raised because controlBatch is empty</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7467</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Somehow, log cleaner died because of NoSuchElementException when it calls onControlBatchRead:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2018-09-25 14:18:31,088] INFO Cleaner 0: Cleaning segment 0 in log __consumer_offsets-45 (largest timestamp Fri Apr 27 16:12:39 CDT 2018) into 0, discarding deletes. (kafka.log.LogCleaner)
[2018-09-25 14:18:31,092] ERROR [kafka-log-cleaner-thread-0]: Error due to (kafka.log.LogCleaner)
java.util.NoSuchElementException
  at java.util.Collections$EmptyIterator.next(Collections.java:4189)
  at kafka.log.CleanedTransactionMetadata.onControlBatchRead(LogCleaner.scala:945)
  at kafka.log.Cleaner.kafka$log$Cleaner$$shouldDiscardBatch(LogCleaner.scala:636)
  at kafka.log.Cleaner$$anon$5.checkBatchRetention(LogCleaner.scala:573)
  at org.apache.kafka.common.record.MemoryRecords.filterTo(MemoryRecords.java:157)
  at org.apache.kafka.common.record.MemoryRecords.filterTo(MemoryRecords.java:138)
  at kafka.log.Cleaner.cleanInto(LogCleaner.scala:604)
  at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:518)
  at kafka.log.Cleaner$$anonfun$doClean$4.apply(LogCleaner.scala:462)
  at kafka.log.Cleaner$$anonfun$doClean$4.apply(LogCleaner.scala:461)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at kafka.log.Cleaner.doClean(LogCleaner.scala:461)
  at kafka.log.Cleaner.clean(LogCleaner.scala:438)
  at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:305)
  at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:291)
  at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2018-09-25 14:18:31,093] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The following code does not seem to expect the controlBatch to be empty:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/kafka/blob/1.1/core/src/main/scala/kafka/log/LogCleaner.scala#L946&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/1.1/core/src/main/scala/kafka/log/LogCleaner.scala#L946&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  def onControlBatchRead(controlBatch: RecordBatch): Boolean = {
    consumeAbortedTxnsUpTo(controlBatch.lastOffset)

    val controlRecord = controlBatch.iterator.next()
    val controlType = ControlRecordType.parse(controlRecord.key)
    val producerId = controlBatch.producerId
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;MemoryRecords.filterTo copies the original control attribute for empty batches, which results in empty control batches. Trying to read the control type of an empty batch causes the error.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  else if (batchRetention == BatchRetention.RETAIN_EMPTY) {
    if (batchMagic &amp;lt; RecordBatch.MAGIC_VALUE_V2)
        throw new IllegalStateException(&quot;Empty batches are only supported for magic v2 and above&quot;);

    bufferOutputStream.ensureRemaining(DefaultRecordBatch.RECORD_BATCH_OVERHEAD);
    DefaultRecordBatch.writeEmptyHeader(bufferOutputStream.buffer(), batchMagic, batch.producerId(),
            batch.producerEpoch(), batch.baseSequence(), batch.baseOffset(), batch.lastOffset(),
            batch.partitionLeaderEpoch(), batch.timestampType(), batch.maxTimestamp(),
            batch.isTransactional(), batch.isControlBatch());
    filterResult.updateRetainedBatchMetadata(batch, 0, true);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13188689">KAFKA-7467</key>
            <summary>NoSuchElementException is raised because controlBatch is empty</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bob-barrett">Bob Barrett</assignee>
                                    <reporter username="badai">Badai Aqrandista</reporter>
                        <labels>
                    </labels>
                <created>Mon, 1 Oct 2018 19:55:55 +0000</created>
                <updated>Fri, 5 Oct 2018 19:34:46 +0000</updated>
                            <resolved>Fri, 5 Oct 2018 19:34:46 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>1.0.3</fixVersion>
                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16634878" author="ijuma" created="Tue, 2 Oct 2018 03:42:25 +0000"  >&lt;p&gt;How come this is considered minor? It seems severe since it kills the cleaner.&lt;/p&gt;</comment>
                            <comment id="16635078" author="badai" created="Tue, 2 Oct 2018 06:40:48 +0000"  >&lt;p&gt;Changed Priority to Major.&lt;/p&gt;</comment>
                            <comment id="16635692" author="githubbot" created="Tue, 2 Oct 2018 15:29:52 +0000"  >&lt;p&gt;bob-barrett opened a new pull request #5727: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7467&quot; title=&quot;NoSuchElementException is raised because controlBatch is empty&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7467&quot;&gt;&lt;del&gt;KAFKA-7467&lt;/del&gt;&lt;/a&gt;: NoSuchElementException is raised because controlBatch is &#8230;&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5727&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5727&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   &#8230;empty&lt;/p&gt;

&lt;p&gt;   This patch changes the behavior of MemoryRecords.filterTo() to always mark empty batches as non-control batches, rather than taking the attribute from the original batch. Empty control batches will cause fatal errors in segment recovery or the log cleaner due to trying access a record in an empty batch. In order to prevent errors from logs that are already in this state, this patch also adds a check to DefaultRecordBatch.isControlBatch() so that it will only return true if the batch is non-empty. Finally, it adds isControl to the output of DumpLogSegments. MemoryRecords and DefaultRecordsBatch changes were tested with unit tests, DumpLogSegments change was tested manually.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16640218" author="githubbot" created="Fri, 5 Oct 2018 18:59:03 +0000"  >&lt;p&gt;hachikuji closed pull request #5727: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7467&quot; title=&quot;NoSuchElementException is raised because controlBatch is empty&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7467&quot;&gt;&lt;del&gt;KAFKA-7467&lt;/del&gt;&lt;/a&gt;: NoSuchElementException is raised because controlBatch is &#8230;&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5727&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5727&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
index 2a6ac0cfec7..6b42d07cc3f 100644&lt;br/&gt;
&amp;#8212; a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java&lt;br/&gt;
@@ -1300,8 +1300,7 @@ private boolean containsAbortMarker(RecordBatch batch) {&lt;/p&gt;

&lt;p&gt;             Iterator&amp;lt;Record&amp;gt; batchIterator = batch.iterator();&lt;br/&gt;
             if (!batchIterator.hasNext())&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;throw new InvalidRecordException(&quot;Invalid batch for partition &quot; + partition + &quot; at offset &quot; +&lt;/li&gt;
	&lt;li&gt;batch.baseOffset() + &quot; with control sequence set, but no records&quot;);&lt;br/&gt;
+                return false;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             Record firstRecord = batchIterator.next();&lt;br/&gt;
             return ControlRecordType.ABORT == ControlRecordType.parse(firstRecord.key());&lt;br/&gt;
diff --git a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
index 75a34ccf908..42f6beb16c2 100644&lt;br/&gt;
&amp;#8212; a/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
+++ b/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java&lt;br/&gt;
@@ -2639,6 +2639,52 @@ private void verifySessionPartitions() &lt;/p&gt;
{
         assertEquals(0, future.get());
     }

&lt;p&gt;+    @Test&lt;br/&gt;
+    public void testEmptyControlBatch() {&lt;br/&gt;
+        Fetcher&amp;lt;byte[], byte[]&amp;gt; fetcher = createFetcher(subscriptions, new Metrics(), new ByteArrayDeserializer(),&lt;br/&gt;
+                new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);&lt;br/&gt;
+        ByteBuffer buffer = ByteBuffer.allocate(1024);&lt;br/&gt;
+        int currentOffset = 1;&lt;br/&gt;
+&lt;br/&gt;
+        // Empty control batch should not cause an exception&lt;br/&gt;
+        DefaultRecordBatch.writeEmptyHeader(buffer, RecordBatch.MAGIC_VALUE_V2, 1L,&lt;br/&gt;
+                (short) 0, -1, 0, 0,&lt;br/&gt;
+                RecordBatch.NO_PARTITION_LEADER_EPOCH, TimestampType.CREATE_TIME, time.milliseconds(),&lt;br/&gt;
+                true, true);&lt;br/&gt;
+&lt;br/&gt;
+        currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset,&lt;br/&gt;
+                new SimpleRecord(time.milliseconds(), &quot;key&quot;.getBytes(), &quot;value&quot;.getBytes()),&lt;br/&gt;
+                new SimpleRecord(time.milliseconds(), &quot;key&quot;.getBytes(), &quot;value&quot;.getBytes()));&lt;br/&gt;
+&lt;br/&gt;
+        commitTransaction(buffer, 1L, currentOffset);&lt;br/&gt;
+        buffer.flip();&lt;br/&gt;
+&lt;br/&gt;
+        List&amp;lt;FetchResponse.AbortedTransaction&amp;gt; abortedTransactions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+        MemoryRecords records = MemoryRecords.readableRecords(buffer);&lt;br/&gt;
+        subscriptions.assignFromUser(singleton(tp0));&lt;br/&gt;
+&lt;br/&gt;
+        subscriptions.seek(tp0, 0);&lt;br/&gt;
+&lt;br/&gt;
+        // normal fetch&lt;br/&gt;
+        assertEquals(1, fetcher.sendFetches());&lt;br/&gt;
+        assertFalse(fetcher.hasCompletedFetches());&lt;br/&gt;
+        client.prepareResponse(new MockClient.RequestMatcher() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public boolean matches(AbstractRequest body) &lt;/p&gt;
{
+                FetchRequest request = (FetchRequest) body;
+                assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());
+                return true;
+            }
&lt;p&gt;+        }, fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));&lt;br/&gt;
+&lt;br/&gt;
+        consumerClient.poll(time.timer(0));&lt;br/&gt;
+        assertTrue(fetcher.hasCompletedFetches());&lt;br/&gt;
+&lt;br/&gt;
+        Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; fetchedRecords = fetcher.fetchedRecords();&lt;br/&gt;
+        assertTrue(fetchedRecords.containsKey(tp0));&lt;br/&gt;
+        assertEquals(fetchedRecords.get(tp0).size(), 2);&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     private MemoryRecords buildRecords(long baseOffset, int count, long firstMessageId) {&lt;br/&gt;
         MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, baseOffset);&lt;br/&gt;
         for (int i = 0; i &amp;lt; count; i++)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
index 21f4a3dcad9..31cd3615675 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala&lt;br/&gt;
@@ -555,17 +555,20 @@ class GroupMetadataManager(brokerId: Int,&lt;br/&gt;
           memRecords.batches.asScala.foreach { batch =&amp;gt;&lt;br/&gt;
             val isTxnOffsetCommit = batch.isTransactional&lt;br/&gt;
             if (batch.isControlBatch) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val record = batch.iterator.next()&lt;/li&gt;
	&lt;li&gt;val controlRecord = ControlRecordType.parse(record.key)&lt;/li&gt;
	&lt;li&gt;if (controlRecord == ControlRecordType.COMMIT) {&lt;/li&gt;
	&lt;li&gt;pendingOffsets.getOrElse(batch.producerId, mutable.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;GroupTopicPartition, CommitRecordMetadataAndOffset&amp;#93;&lt;/span&gt;())&lt;/li&gt;
	&lt;li&gt;.foreach 
{
-                    case (groupTopicPartition, commitRecordMetadataAndOffset) =&amp;gt;
-                      if (!loadedOffsets.contains(groupTopicPartition) || loadedOffsets(groupTopicPartition).olderThan(commitRecordMetadataAndOffset))
-                        loadedOffsets.put(groupTopicPartition, commitRecordMetadataAndOffset)
-                  }
&lt;p&gt;+              val recordIterator = batch.iterator&lt;br/&gt;
+              if (recordIterator.hasNext) {&lt;br/&gt;
+                val record = recordIterator.next()&lt;br/&gt;
+                val controlRecord = ControlRecordType.parse(record.key)&lt;br/&gt;
+                if (controlRecord == ControlRecordType.COMMIT) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                  pendingOffsets.getOrElse(batch.producerId, mutable.Map[GroupTopicPartition, CommitRecordMetadataAndOffset]())+                    .foreach {
+                      case (groupTopicPartition, commitRecordMetadataAndOffset) =&amp;gt;
+                        if (!loadedOffsets.contains(groupTopicPartition) || loadedOffsets(groupTopicPartition).olderThan(commitRecordMetadataAndOffset))
+                          loadedOffsets.put(groupTopicPartition, commitRecordMetadataAndOffset)
+                    }+                }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+                pendingOffsets.remove(batch.producerId)&lt;br/&gt;
               }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;pendingOffsets.remove(batch.producerId)&lt;br/&gt;
             } else {&lt;br/&gt;
               var batchBaseOffset: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
               for (record &amp;lt;- batch.asScala) {&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/log/LogCleaner.scala b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
index f1ac1fc5e94..bf4f7e1fcba 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/LogCleaner.scala&lt;br/&gt;
@@ -990,24 +990,30 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class CleanedTransactionMetadata(val abortedTransactions: mutable.P&lt;br/&gt;
   def onControlBatchRead(controlBatch: RecordBatch): Boolean = {&lt;br/&gt;
     consumeAbortedTxnsUpTo(controlBatch.lastOffset)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val controlRecord = controlBatch.iterator.next()&lt;/li&gt;
	&lt;li&gt;val controlType = ControlRecordType.parse(controlRecord.key)&lt;/li&gt;
	&lt;li&gt;val producerId = controlBatch.producerId&lt;/li&gt;
	&lt;li&gt;controlType match {&lt;/li&gt;
	&lt;li&gt;case ControlRecordType.ABORT =&amp;gt;&lt;/li&gt;
	&lt;li&gt;ongoingAbortedTxns.remove(producerId) match 
{
-          // Retain the marker until all batches from the transaction have been removed
-          case Some(abortedTxnMetadata) if abortedTxnMetadata.lastObservedBatchOffset.isDefined =&amp;gt;
-            transactionIndex.foreach(_.append(abortedTxnMetadata.abortedTxn))
-            false
-          case _ =&amp;gt; true
-        }
&lt;p&gt;+    val controlRecordIterator = controlBatch.iterator&lt;br/&gt;
+    if (controlRecordIterator.hasNext) {&lt;br/&gt;
+      val controlRecord = controlRecordIterator.next()&lt;br/&gt;
+      val controlType = ControlRecordType.parse(controlRecord.key)&lt;br/&gt;
+      val producerId = controlBatch.producerId&lt;br/&gt;
+      controlType match {&lt;br/&gt;
+        case ControlRecordType.ABORT =&amp;gt;&lt;br/&gt;
+          ongoingAbortedTxns.remove(producerId) match &lt;/p&gt;
{
+            // Retain the marker until all batches from the transaction have been removed
+            case Some(abortedTxnMetadata) if abortedTxnMetadata.lastObservedBatchOffset.isDefined =&amp;gt;
+              transactionIndex.foreach(_.append(abortedTxnMetadata.abortedTxn))
+              false
+            case _ =&amp;gt; true
+          }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case ControlRecordType.COMMIT =&amp;gt;&lt;/li&gt;
	&lt;li&gt;// This marker is eligible for deletion if we didn&apos;t traverse any batches from the transaction&lt;/li&gt;
	&lt;li&gt;!ongoingCommittedTxns.remove(producerId)&lt;br/&gt;
+        case ControlRecordType.COMMIT =&amp;gt;&lt;br/&gt;
+          // This marker is eligible for deletion if we didn&apos;t traverse any batches from the transaction&lt;br/&gt;
+          !ongoingCommittedTxns.remove(producerId)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case _ =&amp;gt; false&lt;br/&gt;
+        case _ =&amp;gt; false&lt;br/&gt;
+      }&lt;br/&gt;
+    } else 
{
+      // An empty control batch was already cleaned, so it&apos;s safe to discard
+      true
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/kafka/log/ProducerStateManager.scala b/core/src/main/scala/kafka/log/ProducerStateManager.scala&lt;br/&gt;
index 2f711234bdb..a5c182c2ce3 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/log/ProducerStateManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/log/ProducerStateManager.scala&lt;br/&gt;
@@ -254,10 +254,16 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;log&amp;#93;&lt;/span&gt; class ProducerAppendInfo(val producerId: Long,&lt;/p&gt;

&lt;p&gt;   def append(batch: RecordBatch): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;CompletedTxn&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     if (batch.isControlBatch) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val record = batch.iterator.next()&lt;/li&gt;
	&lt;li&gt;val endTxnMarker = EndTransactionMarker.deserialize(record)&lt;/li&gt;
	&lt;li&gt;val completedTxn = appendEndTxnMarker(endTxnMarker, batch.producerEpoch, batch.baseOffset, record.timestamp)&lt;/li&gt;
	&lt;li&gt;Some(completedTxn)&lt;br/&gt;
+      val recordIterator = batch.iterator&lt;br/&gt;
+      if (recordIterator.hasNext) 
{
+        val record = recordIterator.next()
+        val endTxnMarker = EndTransactionMarker.deserialize(record)
+        val completedTxn = appendEndTxnMarker(endTxnMarker, batch.producerEpoch, batch.baseOffset, record.timestamp)
+        Some(completedTxn)
+      }
&lt;p&gt; else &lt;/p&gt;
{
+        // An empty control batch means the entire transaction has been cleaned from the log, so no need to append
+        None
+      }
&lt;p&gt;     } else {&lt;br/&gt;
       append(batch.producerEpoch, batch.baseSequence, batch.lastSequence, batch.maxTimestamp, batch.lastOffset,&lt;br/&gt;
         batch.isTransactional)&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/tools/DumpLogSegments.scala b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
index 1792c7bff81..b5b0e6eced5 100755&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/tools/DumpLogSegments.scala&lt;br/&gt;
@@ -424,7 +424,8 @@ object DumpLogSegments {&lt;br/&gt;
           print(&quot;baseOffset: &quot; + batch.baseOffset + &quot; lastOffset: &quot; + batch.lastOffset + &quot; count: &quot; + batch.countOrNull +&lt;br/&gt;
             &quot; baseSequence: &quot; + batch.baseSequence + &quot; lastSequence: &quot; + batch.lastSequence +&lt;br/&gt;
             &quot; producerId: &quot; + batch.producerId + &quot; producerEpoch: &quot; + batch.producerEpoch +&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;&quot; partitionLeaderEpoch: &quot; + batch.partitionLeaderEpoch + &quot; isTransactional: &quot; + batch.isTransactional)&lt;br/&gt;
+            &quot; partitionLeaderEpoch: &quot; + batch.partitionLeaderEpoch + &quot; isTransactional: &quot; + batch.isTransactional +&lt;br/&gt;
+            &quot; isControl: &quot; + batch.isControlBatch)&lt;br/&gt;
         else&lt;br/&gt;
           print(&quot;offset: &quot; + batch.lastOffset)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
index b48f297ad78..f6482573841 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala&lt;br/&gt;
@@ -35,6 +35,7 @@ import org.easymock.&lt;/p&gt;
{Capture, EasyMock, IAnswer}
&lt;p&gt; import org.junit.Assert.&lt;/p&gt;
{assertEquals, assertFalse, assertNull, assertTrue}
&lt;p&gt; import org.junit.&lt;/p&gt;
{Before, Test}
&lt;p&gt; import java.nio.ByteBuffer&lt;br/&gt;
+import java.util.Collections&lt;br/&gt;
 import java.util.Optional&lt;/p&gt;

&lt;p&gt; import com.yammer.metrics.Metrics&lt;br/&gt;
@@ -1766,6 +1767,61 @@ class GroupMetadataManagerTest &lt;/p&gt;
{
       verifySerde(version)
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testLoadOffsetsWithEmptyControlBatch() {&lt;br/&gt;
+    val groupMetadataTopicPartition = groupTopicPartition&lt;br/&gt;
+    val startOffset = 15L&lt;br/&gt;
+    val generation = 15&lt;br/&gt;
+&lt;br/&gt;
+    val committedOffsets = Map(&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 0) -&amp;gt; 23L,&lt;br/&gt;
+      new TopicPartition(&quot;foo&quot;, 1) -&amp;gt; 455L,&lt;br/&gt;
+      new TopicPartition(&quot;bar&quot;, 0) -&amp;gt; 8992L&lt;br/&gt;
+    )&lt;br/&gt;
+&lt;br/&gt;
+    val offsetCommitRecords = createCommittedOffsetRecords(committedOffsets)&lt;br/&gt;
+    val groupMetadataRecord = buildEmptyGroupRecord(generation, protocolType)&lt;br/&gt;
+    val records = MemoryRecords.withRecords(startOffset, CompressionType.NONE,&lt;br/&gt;
+      offsetCommitRecords ++ Seq(groupMetadataRecord): _*)&lt;br/&gt;
+&lt;br/&gt;
+    // Prepend empty control batch to valid records&lt;br/&gt;
+    val mockBatch = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;MutableRecordBatch&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(mockBatch.iterator).andReturn(Collections.emptyIterator&lt;span class=&quot;error&quot;&gt;&amp;#91;Record&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(mockBatch.isControlBatch).andReturn(true)&lt;br/&gt;
+    EasyMock.expect(mockBatch.isTransactional).andReturn(true)&lt;br/&gt;
+    EasyMock.expect(mockBatch.nextOffset).andReturn(16L)&lt;br/&gt;
+    EasyMock.replay(mockBatch)&lt;br/&gt;
+    val mockRecords = EasyMock.createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryRecords&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(mockRecords.batches).andReturn((Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;MutableRecordBatch&amp;#93;&lt;/span&gt;(mockBatch) ++ records.batches.asScala).asJava).anyTimes()&lt;br/&gt;
+    EasyMock.expect(mockRecords.records).andReturn(records.records()).anyTimes()&lt;br/&gt;
+    EasyMock.expect(mockRecords.sizeInBytes()).andReturn(DefaultRecordBatch.RECORD_BATCH_OVERHEAD + records.sizeInBytes()).anyTimes()&lt;br/&gt;
+    EasyMock.replay(mockRecords)&lt;br/&gt;
+&lt;br/&gt;
+    val logMock = EasyMock.mock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Log&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    EasyMock.expect(logMock.logStartOffset).andReturn(startOffset).anyTimes()&lt;br/&gt;
+    EasyMock.expect(logMock.read(EasyMock.eq(startOffset), EasyMock.anyInt(), EasyMock.eq(None),&lt;br/&gt;
+      EasyMock.eq(true), EasyMock.eq(IsolationLevel.READ_UNCOMMITTED)))&lt;br/&gt;
+      .andReturn(FetchDataInfo(LogOffsetMetadata(startOffset), mockRecords))&lt;br/&gt;
+    EasyMock.expect(replicaManager.getLog(groupMetadataTopicPartition)).andStubReturn(Some(logMock))&lt;br/&gt;
+    EasyMock.expect(replicaManager.getLogEndOffset(groupMetadataTopicPartition)).andStubReturn(Some(18))&lt;br/&gt;
+    EasyMock.replay(logMock)&lt;br/&gt;
+    EasyMock.replay(replicaManager)&lt;br/&gt;
+&lt;br/&gt;
+    groupMetadataManager.loadGroupsAndOffsets(groupMetadataTopicPartition, _ =&amp;gt; ())&lt;br/&gt;
+&lt;br/&gt;
+    // Empty control batch should not have caused the load to fail&lt;br/&gt;
+    val group = groupMetadataManager.getGroup(groupId).getOrElse(fail(&quot;Group was not loaded into the cache&quot;))&lt;br/&gt;
+    assertEquals(groupId, group.groupId)&lt;br/&gt;
+    assertEquals(Empty, group.currentState)&lt;br/&gt;
+    assertEquals(generation, group.generationId)&lt;br/&gt;
+    assertEquals(Some(protocolType), group.protocolType)&lt;br/&gt;
+    assertNull(group.leaderOrNull)&lt;br/&gt;
+    assertNull(group.protocolOrNull)&lt;br/&gt;
+    committedOffsets.foreach &lt;/p&gt;
{ case (topicPartition, offset) =&amp;gt;
+      assertEquals(Some(offset), group.offset(topicPartition).map(_.offset))
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
   private def appendAndCaptureCallback(): Capture[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionResponse&amp;#93;&lt;/span&gt; =&amp;gt; Unit] = {&lt;br/&gt;
     val capturedArgument: Capture[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, PartitionResponse&amp;#93;&lt;/span&gt; =&amp;gt; Unit] = EasyMock.newCapture()&lt;br/&gt;
     EasyMock.expect(replicaManager.appendRecords(EasyMock.anyLong(),&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
index 73dfa7e39cd..ff5af6123e2 100755&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/LogCleanerTest.scala&lt;br/&gt;
@@ -465,6 +465,37 @@ class LogCleanerTest extends JUnitSuite &lt;/p&gt;
{
     assertEquals(List(1, 5, 6, 8, 9), lastOffsetsPerBatchInLog(log))
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testCleanEmptyControlBatch(): Unit = {&lt;br/&gt;
+    val tp = new TopicPartition(&quot;test&quot;, 0)&lt;br/&gt;
+    val cleaner = makeCleaner(Int.MaxValue)&lt;br/&gt;
+    val logProps = new Properties()&lt;br/&gt;
+    logProps.put(LogConfig.SegmentBytesProp, 256: java.lang.Integer)&lt;br/&gt;
+    val log = makeLog(config = LogConfig.fromProps(logConfig.originals, logProps))&lt;br/&gt;
+&lt;br/&gt;
+    val producerEpoch = 0.toShort&lt;br/&gt;
+&lt;br/&gt;
+    // [&lt;/p&gt;
{Producer1: Commit}
&lt;p&gt;, &lt;/p&gt;
{2}, {3}]&lt;br/&gt;
+    log.appendAsLeader(commitMarker(1L, producerEpoch), leaderEpoch = 0, isFromClient = false) // offset 7&lt;br/&gt;
+    log.appendAsLeader(record(2, 2), leaderEpoch = 0) // offset 2&lt;br/&gt;
+    log.appendAsLeader(record(3, 3), leaderEpoch = 0) // offset 3&lt;br/&gt;
+    log.roll()&lt;br/&gt;
+&lt;br/&gt;
+    // first time through the control batch is retained as an empty batch&lt;br/&gt;
+    // Expected State: &lt;span class=&quot;error&quot;&gt;&amp;#91;{Producer1: EmptyBatch}&amp;#93;&lt;/span&gt;, [{2}
&lt;p&gt;, &lt;/p&gt;
{3}]&lt;br/&gt;
+    var dirtyOffset = cleaner.doClean(LogToClean(tp, log, 0L, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
+    assertEquals(List(2, 3), LogTest.keysInLog(log))&lt;br/&gt;
+    assertEquals(List(1, 2), offsetsInLog(log))&lt;br/&gt;
+    assertEquals(List(0, 1, 2), lastOffsetsPerBatchInLog(log))&lt;br/&gt;
+&lt;br/&gt;
+    // the empty control batch does not cause an exception when cleaned&lt;br/&gt;
+    // Expected State: &lt;span class=&quot;error&quot;&gt;&amp;#91;{Producer1: EmptyBatch}&amp;#93;&lt;/span&gt;, [{2}, {3}
&lt;p&gt;]&lt;br/&gt;
+    dirtyOffset = cleaner.doClean(LogToClean(tp, log, dirtyOffset, 100L), deleteHorizonMs = Long.MaxValue)._1&lt;br/&gt;
+    assertEquals(List(2, 3), LogTest.keysInLog(log))&lt;br/&gt;
+    assertEquals(List(1, 2), offsetsInLog(log))&lt;br/&gt;
+    assertEquals(List(0, 1, 2), lastOffsetsPerBatchInLog(log))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   @Test&lt;br/&gt;
   def testAbortMarkerRemoval(): Unit = {&lt;br/&gt;
     val tp = new TopicPartition(&quot;test&quot;, 0)&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala b/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala&lt;br/&gt;
index f9f4a239023..9afb145c4c6 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala&lt;br/&gt;
@@ -21,14 +21,16 @@ import java.io.File&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
 import java.nio.channels.FileChannel&lt;br/&gt;
 import java.nio.file.StandardOpenOption&lt;br/&gt;
+import java.util.Collections&lt;/p&gt;

&lt;p&gt; import kafka.server.LogOffsetMetadata&lt;br/&gt;
 import kafka.utils.TestUtils&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
 import org.apache.kafka.common.errors._&lt;br/&gt;
 import org.apache.kafka.common.internals.Topic&lt;br/&gt;
-import org.apache.kafka.common.record.&lt;/p&gt;
{ControlRecordType, EndTransactionMarker, RecordBatch}
&lt;p&gt;+import org.apache.kafka.common.record._&lt;br/&gt;
 import org.apache.kafka.common.utils.&lt;/p&gt;
{MockTime, Utils}
&lt;p&gt;+import org.easymock.EasyMock&lt;br/&gt;
 import org.junit.Assert._&lt;br/&gt;
 import org.junit.&lt;/p&gt;
{After, Before, Test}
&lt;p&gt; import org.scalatest.junit.JUnitSuite&lt;br/&gt;
@@ -746,6 +748,22 @@ class ProducerStateManagerTest extends JUnitSuite {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  @Test&lt;br/&gt;
+  def testAppendEmptyControlBatch(): Unit = &lt;/p&gt;
{
+    val producerId = 23423L
+    val producerEpoch = 145.toShort
+    val baseOffset = 15
+
+    val batch = EasyMock.createMock(classOf[RecordBatch])
+    EasyMock.expect(batch.isControlBatch).andReturn(true).once
+    EasyMock.expect(batch.iterator).andReturn(Collections.emptyIterator[Record]).once
+    EasyMock.replay(batch)
+
+    // Appending the empty control batch should not throw and a new transaction shouldn&apos;t be started
+    append(stateManager, producerId, producerEpoch, baseOffset, batch, isFromClient = true)
+    assertEquals(None, stateManager.lastEntry(producerId).get.currentTxnFirstOffset)
+  }
&lt;p&gt;+&lt;br/&gt;
   private def testLoadFromCorruptSnapshot(makeFileCorrupt: FileChannel =&amp;gt; Unit): Unit = {&lt;br/&gt;
     val epoch = 0.toShort&lt;br/&gt;
     val producerId = 1L&lt;br/&gt;
@@ -806,6 +824,18 @@ class ProducerStateManagerTest extends JUnitSuite &lt;/p&gt;
{
     stateManager.updateMapEndOffset(offset + 1)
   }

&lt;p&gt;+  private def append(stateManager: ProducerStateManager,&lt;br/&gt;
+                     producerId: Long,&lt;br/&gt;
+                     producerEpoch: Short,&lt;br/&gt;
+                     offset: Long,&lt;br/&gt;
+                     batch: RecordBatch,&lt;br/&gt;
+                     isFromClient : Boolean): Unit = &lt;/p&gt;
{
+    val producerAppendInfo = stateManager.prepareUpdate(producerId, isFromClient)
+    producerAppendInfo.append(batch)
+    stateManager.update(producerAppendInfo)
+    stateManager.updateMapEndOffset(offset + 1)
+  }
&lt;p&gt;+&lt;br/&gt;
   private def currentSnapshotOffsets =&lt;br/&gt;
     logDir.listFiles.map(Log.offsetFromFile).toSet&lt;/p&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 6 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3ypdr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>