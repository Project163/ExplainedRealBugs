<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:41:41 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-1305] Controller can hang on controlled shutdown with auto leader balance enabled</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-1305</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;This is relatively easy to reproduce especially when doing a rolling bounce.&lt;br/&gt;
What happened here is as follows:&lt;/p&gt;

&lt;p&gt;1. The previous controller was bounced and broker 265 became the new controller.&lt;br/&gt;
2. I went on to do a controlled shutdown of broker 265 (the new controller).&lt;br/&gt;
3. In the mean time the automatically scheduled preferred replica leader election process started doing its thing and starts sending LeaderAndIsrRequests/UpdateMetadataRequests to itself (and other brokers).  (t@113 below).&lt;br/&gt;
4. While that&apos;s happening, the controlled shutdown process on 265 succeeds and proceeds to deregister itself from ZooKeeper and shuts down the socket server.&lt;br/&gt;
5. (ReplicaStateMachine actually removes deregistered brokers from the controller channel manager&apos;s list of brokers to send requests to.  However, that removal cannot take place (t@18 below) because preferred replica leader election task owns the controller lock.)&lt;br/&gt;
6. So the request thread to broker 265 gets into infinite retries.&lt;br/&gt;
7. The entire broker shutdown process is blocked on controller shutdown for the same reason (it needs to acquire the controller lock).&lt;/p&gt;

&lt;p&gt;Relevant portions from the thread-dump:&lt;/p&gt;

&lt;p&gt;&quot;Controller-265-to-broker-265-send-thread&quot; - Thread t@113&lt;br/&gt;
   java.lang.Thread.State: TIMED_WAITING&lt;br/&gt;
	at java.lang.Thread.sleep(Native Method)&lt;br/&gt;
	at kafka.controller.RequestSendThread$$anonfun$liftedTree1$1$1.apply$mcV$sp(ControllerChannelManager.scala:143)&lt;br/&gt;
	at kafka.utils.Utils$.swallow(Utils.scala:167)&lt;br/&gt;
	at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)&lt;br/&gt;
	at kafka.utils.Utils$.swallowWarn(Utils.scala:46)&lt;br/&gt;
	at kafka.utils.Logging$class.swallow(Logging.scala:94)&lt;br/&gt;
	at kafka.utils.Utils$.swallow(Utils.scala:46)&lt;br/&gt;
	at kafka.controller.RequestSendThread.liftedTree1$1(ControllerChannelManager.scala:143)&lt;br/&gt;
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:131)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked java.lang.Object@6dbf14a7&lt;br/&gt;
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   Locked ownable synchronizers:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;None&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&quot;Thread-4&quot; - Thread t@17&lt;br/&gt;
   java.lang.Thread.State: WAITING on java.util.concurrent.locks.ReentrantLock$NonfairSync@4836840 owned by: kafka-scheduler-0&lt;br/&gt;
	at sun.misc.Unsafe.park(Native Method)&lt;br/&gt;
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)&lt;br/&gt;
	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)&lt;br/&gt;
	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)&lt;br/&gt;
	at kafka.utils.Utils$.inLock(Utils.scala:536)&lt;br/&gt;
	at kafka.controller.KafkaController.shutdown(KafkaController.scala:642)&lt;br/&gt;
	at kafka.server.KafkaServer$$anonfun$shutdown$9.apply$mcV$sp(KafkaServer.scala:242)&lt;br/&gt;
	at kafka.utils.Utils$.swallow(Utils.scala:167)&lt;br/&gt;
	at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)&lt;br/&gt;
	at kafka.utils.Utils$.swallowWarn(Utils.scala:46)&lt;br/&gt;
	at kafka.utils.Logging$class.swallow(Logging.scala:94)&lt;br/&gt;
	at kafka.utils.Utils$.swallow(Utils.scala:46)&lt;br/&gt;
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:242)&lt;br/&gt;
	at kafka.server.KafkaServerStartable.shutdown(KafkaServerStartable.scala:46)&lt;br/&gt;
	at kafka.Kafka$$anon$1.run(Kafka.scala:42)&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&quot;kafka-scheduler-0&quot; - Thread t@117&lt;br/&gt;
   java.lang.Thread.State: WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1dc407fc&lt;br/&gt;
	at sun.misc.Unsafe.park(Native Method)&lt;br/&gt;
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)&lt;br/&gt;
	at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:306)&lt;br/&gt;
	at kafka.controller.ControllerChannelManager.sendRequest(ControllerChannelManager.scala:57)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked java.lang.Object@578b748f&lt;br/&gt;
	at kafka.controller.KafkaController.sendRequest(KafkaController.scala:657)&lt;br/&gt;
	at kafka.controller.ControllerBrokerRequestBatch$$anonfun$sendRequestsToBrokers$2.apply(ControllerChannelManager.scala:290)&lt;br/&gt;
	at kafka.controller.ControllerBrokerRequestBatch$$anonfun$sendRequestsToBrokers$2.apply(ControllerChannelManager.scala:282)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)&lt;br/&gt;
	at kafka.controller.ControllerBrokerRequestBatch.sendRequestsToBrokers(ControllerChannelManager.scala:282)&lt;br/&gt;
	at kafka.controller.PartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:126)&lt;br/&gt;
	at kafka.controller.KafkaController.onPreferredReplicaElection(KafkaController.scala:612)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17$$anonfun$apply$5.apply$mcV$sp(KafkaController.scala:1119)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17$$anonfun$apply$5.apply(KafkaController.scala:1114)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17$$anonfun$apply$5.apply(KafkaController.scala:1114)&lt;br/&gt;
	at kafka.utils.Utils$.inLock(Utils.scala:538)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1111)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1109)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)&lt;br/&gt;
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1109)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1088)&lt;br/&gt;
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)&lt;br/&gt;
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)&lt;br/&gt;
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance(KafkaController.scala:1088)&lt;br/&gt;
	at kafka.controller.KafkaController$$anonfun$onControllerFailover$1.apply$mcV$sp(KafkaController.scala:323)&lt;br/&gt;
	at kafka.utils.KafkaScheduler$$anon$1.run(KafkaScheduler.scala:100)&lt;br/&gt;
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)&lt;br/&gt;
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)&lt;br/&gt;
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)&lt;br/&gt;
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)&lt;br/&gt;
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)&lt;br/&gt;
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:662)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   Locked ownable synchronizers:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked java.util.concurrent.locks.ReentrantLock$NonfairSync@4836840&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked java.util.concurrent.locks.ReentrantLock$NonfairSync@4918530&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&quot;ZkClient-EventThread-18-/kafka-shadow&quot; - Thread t@18&lt;br/&gt;
   java.lang.Thread.State: WAITING on java.util.concurrent.locks.ReentrantLock$NonfairSync@4836840 owned by: kafka-scheduler-0&lt;br/&gt;
	at sun.misc.Unsafe.park(Native Method)&lt;br/&gt;
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)&lt;br/&gt;
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)&lt;br/&gt;
	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)&lt;br/&gt;
	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)&lt;br/&gt;
	at kafka.utils.Utils$.inLock(Utils.scala:536)&lt;br/&gt;
	at kafka.controller.ReplicaStateMachine$BrokerChangeListener.handleChildChange(ReplicaStateMachine.scala:328)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)&lt;br/&gt;
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12701385">KAFKA-1305</key>
            <summary>Controller can hang on controlled shutdown with auto leader balance enabled</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sriharsha">Harsha</assignee>
                                    <reporter username="jjkoshy">Joel Jacob Koshy</reporter>
                        <labels>
                    </labels>
                <created>Fri, 14 Mar 2014 00:08:21 +0000</created>
                <updated>Tue, 17 May 2016 13:51:31 +0000</updated>
                            <resolved>Mon, 13 Oct 2014 19:32:01 +0000</resolved>
                                                    <fixVersion>0.8.2.0</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="13935651" author="guozhang" created="Fri, 14 Mar 2014 21:13:39 +0000"  >&lt;p&gt;This is a similar issue as for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1235&quot; title=&quot;Enable server to indefinitely retry on controlled shutdown&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1235&quot;&gt;&lt;del&gt;KAFKA-1235&lt;/del&gt;&lt;/a&gt;. One alternative solution to kill both birds is to allow the sender thread jump out of the infinite retry if it realize that the destination broker is shutting down.&lt;/p&gt;</comment>
                            <comment id="13986087" author="noslowerdna" created="Wed, 30 Apr 2014 21:10:09 +0000"  >&lt;p&gt;Is it recommended to not set both auto.leader.rebalance.enable=true and controlled.shutdown.enable=true?&lt;/p&gt;

&lt;p&gt;If this issue is encountered, killing the hung broker process sounds like the only resolution. Would that cause any issues for the other brokers in the cluster?  Also, would it be problematic if the hung broker is not killed in a timely manner?&lt;/p&gt;</comment>
                            <comment id="13986635" author="junrao" created="Thu, 1 May 2014 14:34:03 +0000"  >&lt;p&gt;Right, until this is fixed, don&apos;t turn set auto.leader.rebalance.enable and controlled.shutdown.enable to be true.&lt;/p&gt;</comment>
                            <comment id="14122033" author="guozhang" created="Thu, 4 Sep 2014 21:50:22 +0000"  >&lt;p&gt;Moving out of  0.8.2 for now.&lt;/p&gt;</comment>
                            <comment id="14142261" author="becket_qin" created="Sun, 21 Sep 2014 00:45:08 +0000"  >&lt;p&gt;I looked into this problem and it seems to me the issue is mainly because the default controller queue size was too small.&lt;br/&gt;
The problem flow is as below:&lt;br/&gt;
1. Controller 265 received controlled shutdown request&lt;br/&gt;
2. Controller 265 put leaderAndIsrRequest into controller message queue and responded to broker 265.&lt;br/&gt;
3. Broker 265 received respond from Controller 265, shutdown successfully and de-registerred itself form zk.&lt;br/&gt;
4. Controller 265 request send thread started to send leaderAndIsrRequests which are put in step 2 to the brokers. Since broker 265 has already shutdown, it will start infinite retry. At this moment, the controller message queue size will never decrease. (Thread t@113)&lt;br/&gt;
5. Scheduled preferred leader election started, grabbed the controller lock and was trying to put the LeaderAndIsr request into controller message queue. However, because the queue size is only 10, it could not finish but just blocking on the put method while still holding the controller lock. (Thread t@117)&lt;br/&gt;
6. Broker change listener on controller 265 was triggered because broker path change in step 3, it was trying to grab the controller lock and stop thread t@113, but failed to do that because thread t@117 was holding controller lock and waiting on the controller message queue.&lt;/p&gt;

&lt;p&gt;Currently the controller message queue size is 10. IMO if we can increase the number to be 100 or even bigger, this problem won&apos;t happen again. Actually, in most time, the number of messages in the queue will be small even empty because there should not be too many controller messages. So increasing the queue size won&apos;t cause memory consumption to increase.&lt;/p&gt;</comment>
                            <comment id="14143383" author="guozhang" created="Mon, 22 Sep 2014 16:38:20 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt; for the great findings. It seems to me that as long as the controller&apos;s channel manager is async, no matter how large is its queue the corner-case issue can still happen in (i.e. request blocked in the queue for brokers that is already shutdown but the ZK watcher not fired yet), and causing some chain of lock conflicts.&lt;/p&gt;

&lt;p&gt;Currently the controller has multiple threads for admin commands, ZK listeners, scheduled operations (leader electioner), etc, which complicates the locking mechanism inside controller. After going through the code I think it would be better to refactor the controller as following:&lt;/p&gt;

&lt;p&gt;1. Besides the async channel manager&apos;s sender thread, we use only a single controller thread and have a single working queue for the controller thread.&lt;br/&gt;
3. ZK fire handling logic determines the event (topic/partition/broker change, admin operation, etc), and put the task into the queue.&lt;br/&gt;
4. Scheduled task is also created periodically and put into the queue.&lt;br/&gt;
5. The controller did one task at a time, which do not need to compete locks on controller metadata.&lt;br/&gt;
6. Make the channel manager&apos;s queue size infinite and add a metric on monitoring its size.&lt;/p&gt;

&lt;p&gt;With this the controller logic would be easier to read / debug, may also help &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1558&quot; title=&quot;AdminUtils.deleteTopic does not work&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-1558&quot;&gt;&lt;del&gt;KAFKA-1558&lt;/del&gt;&lt;/a&gt;. The downside is that since a single thread is used, it loses parallelism for controller task handling, and the unbounded channel queue may also be an issue (when there is a bug). But since controller tasks are usually rare in practice, this should not be an issue.&lt;/p&gt;</comment>
                            <comment id="14151276" author="junrao" created="Mon, 29 Sep 2014 00:35:00 +0000"  >&lt;p&gt;Guozhang,&lt;/p&gt;

&lt;p&gt;Yes, I agree that the controller code is getting complicated and it&apos;s probably worth a refactoring. A single threaded controller perhaps will make the logic simpler and easier to understand. There are operations like reassigning partitions that are time consuming though. Instead of blocking on those operations, do we want to use a purgatory to track them?&lt;/p&gt;

&lt;p&gt;Since it may take time to completely fix this issue, I suggest that we just disable this feature in 0.8.2. Otherwise, people may turn it on and hit this issue. &lt;/p&gt;</comment>
                            <comment id="14152633" author="guozhang" created="Tue, 30 Sep 2014 01:00:20 +0000"  >&lt;p&gt;Yeah. Reassigning partitions would take time and hence better be handled in a purgatory, but then there are a couple more subtle issues we need to be careful with:&lt;/p&gt;

&lt;p&gt;1. Upon condition (new replica caught up, etc) satisfied, shall we execute the rest of the logic in the satisfaction checking thread, which will then be a different thread, or just put the rest of the job back into the queue?&lt;/p&gt;

&lt;p&gt;2. Related to 1), as a topic is undergoing partition reassignment or any other &quot;delayable&quot; operations, we need to disable other operations under these topics, right?&lt;/p&gt;

&lt;p&gt;I agree that this is a rather big change and maybe we should push after 0.8.2.&lt;/p&gt;</comment>
                            <comment id="14152743" author="nehanarkhede" created="Tue, 30 Sep 2014 02:48:20 +0000"  >&lt;p&gt;This is a good but potentially very large change. Basically controller functionality would need to be modeled as multi-step actions that require need arbitrary delay between each of the steps that constitute an action. As such the rest of the logic definitely needs to go back into the queue.  &lt;/p&gt;</comment>
                            <comment id="14157654" author="sriharsha" created="Fri, 3 Oct 2014 03:59:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nehanarkhede&quot; class=&quot;user-hover&quot; rel=&quot;nehanarkhede&quot;&gt;nehanarkhede&lt;/a&gt; Is this still planned for 0.8.2 release. &lt;/p&gt;</comment>
                            <comment id="14158829" author="nehanarkhede" created="Sat, 4 Oct 2014 00:56:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sriharsha&quot; class=&quot;user-hover&quot; rel=&quot;sriharsha&quot;&gt;sriharsha&lt;/a&gt; Depends &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Basically, my perspective is that if doing this correctly requires delaying 0.8.2 by a month, then let&apos;s push it to 0.8.3. If there is a small fix for the issue, then let&apos;s include it. IIRC, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; was going to take a stab at thinking if there is a small fix or not.&lt;/p&gt;</comment>
                            <comment id="14159830" author="junrao" created="Mon, 6 Oct 2014 03:14:28 +0000"  >&lt;p&gt;Thinking about this a bit more. As a short term fix, I think Jiangjie&apos;s suggestion actually has a point. Basically, if a channel queue is full (since the target broker is down), a thread that tries to put a request into the queue will block while holding the controller lock. This essentially will stall the controller since it can&apos;t process any other events, which is bad. Now, imagine what if we make the channel queue unbounded. In this case, no thread will block on putting requests into the queue. So, if a broker is down, the controller will always be able to act on it, which will clear up the queue and remove the channel to the dead broker. The only down side is that if there are outstanding requests in a queue, new important requests may be delayed. This is not a big concern because in the common case, the channel queue shouldn&apos;t build up.&lt;/p&gt;

&lt;p&gt;Sriharsha,&lt;/p&gt;

&lt;p&gt;Could you do a bit of testing on this? Basically, set the default value of controller.message.queue.size to sth large (e.g., 10K). Create a cluster with a few K partitions per broker. Enable auto leader balancing and keep doing rolling bounces of the cluster (with controlled shutdown enabled) and see there is any issue. Ideally, we want to hit the case that the auto leader balancing happens concurrently with the controlled shutdown in the controller. So, you may want to play with leader.imbalance.check.interval.seconds.&lt;/p&gt;</comment>
                            <comment id="14160789" author="jkreps" created="Mon, 6 Oct 2014 19:28:39 +0000"  >&lt;p&gt;FWIW, from my perspective being able to enable auto leader balancing would be a huge win. This is arguably the biggest operational &quot;gotcha&quot; today...&lt;/p&gt;</comment>
                            <comment id="14160815" author="noslowerdna" created="Mon, 6 Oct 2014 19:48:53 +0000"  >&lt;p&gt;&amp;gt; from my perspective being able to enable auto leader balancing would be a huge win&lt;/p&gt;

&lt;p&gt;We are running with auto leader balance enabled and controlled shutdown disabled. Given that they&apos;re currently mutually exclusive options, is controlled shutdown generally considered more valuable than auto leader balancing? If so, why is that?&lt;/p&gt;</comment>
                            <comment id="14160828" author="jkreps" created="Mon, 6 Oct 2014 19:54:49 +0000"  >&lt;p&gt;Well I guess the point is that they shouldn&apos;t be mutually exclusive. So hopefully we can make them both be enabled by default.&lt;/p&gt;</comment>
                            <comment id="14165200" author="sriharsha" created="Thu, 9 Oct 2014 14:50:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; Ran into this issue when I am testing delete topics along while simultaneously running preferred replica leader election tool. I am running the above test you suggested will update with the results.&lt;/p&gt;</comment>
                            <comment id="14165914" author="sriharsha" created="Thu, 9 Oct 2014 22:45:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;  I ran the above test you suggested with leader.imbalance.check.interval.seconds set to 30 and controller.message.queue.size set 10000. With 5 brokers and 1500 topics with 3 partitons and 3 replication factor. I am able to run into a case where a broker prints &quot;Shutdown completed&quot; but the process still hangs. Running the test by setting controller.message.queue.size  to higher number.&lt;br/&gt;
Here is thread dump&lt;/p&gt;

&lt;p&gt;Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.65-b04 mixed mode):&lt;/p&gt;

&lt;p&gt;&quot;Attach Listener&quot; daemon prio=10 tid=0x00007f8860003000 nid=0x26cc waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000000000000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;&quot;Controller-4-to-broker-3-send-thread&quot; prio=10 tid=0x00007f884c049000 nid=0x26b2 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f83c99e0000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (parking)&lt;br/&gt;
        at sun.misc.Unsafe.park(Native Method)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;parking to wait for  &amp;lt;0x00000000d2be8008&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)&lt;br/&gt;
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)&lt;br/&gt;
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)&lt;br/&gt;
        at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:121)&lt;br/&gt;
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&quot;Thread-2&quot; prio=10 tid=0x00007f8868005800 nid=0x26b1 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f83c98df000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: TIMED_WAITING (parking)&lt;br/&gt;
        at sun.misc.Unsafe.park(Native Method)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;parking to wait for  &amp;lt;0x00000000d2a34508&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)&lt;br/&gt;
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1468)&lt;br/&gt;
        at kafka.utils.KafkaScheduler.shutdown(KafkaScheduler.scala:88)&lt;br/&gt;
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply$mcV$sp(KafkaController.scala:353)&lt;br/&gt;
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:348)&lt;br/&gt;
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:348)&lt;br/&gt;
        at kafka.utils.Utils$.inLock(Utils.scala:535)&lt;br/&gt;
        at kafka.controller.KafkaController.onControllerResignation(KafkaController.scala:348)&lt;br/&gt;
        at kafka.controller.KafkaController.shutdown(KafkaController.scala:663)&lt;br/&gt;
        at kafka.server.KafkaServer$$anonfun$shutdown$9.apply$mcV$sp(KafkaServer.scala:287)&lt;br/&gt;
        at kafka.utils.Utils$.swallow(Utils.scala:172)&lt;br/&gt;
        at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)&lt;br/&gt;
        at kafka.utils.Utils$.swallowWarn(Utils.scala:45)&lt;br/&gt;
        at kafka.utils.Logging$class.swallow(Logging.scala:94)&lt;br/&gt;
        at kafka.utils.Utils$.swallow(Utils.scala:45)&lt;br/&gt;
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:287)&lt;br/&gt;
        at kafka.server.KafkaServerStartable.shutdown(KafkaServerStartable.scala:40)&lt;br/&gt;
        at kafka.Kafka$$anon$1.run(Kafka.scala:42)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&quot;SIGTERM handler&quot; daemon prio=10 tid=0x00007f8860002000 nid=0x26ae in Object.wait() &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f83c9be2000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (on object monitor)&lt;br/&gt;
        at java.lang.Object.wait(Native Method)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;waiting on &amp;lt;0x00000000d018bda8&amp;gt; (a kafka.Kafka$$anon$1)&lt;br/&gt;
        at java.lang.Thread.join(Thread.java:1281)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x00000000d018bda8&amp;gt; (a kafka.Kafka$$anon$1)&lt;br/&gt;
        at java.lang.Thread.join(Thread.java:1355)&lt;br/&gt;
        at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)&lt;br/&gt;
        at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)&lt;br/&gt;
        at java.lang.Shutdown.runHooks(Shutdown.java:123)&lt;br/&gt;
        at java.lang.Shutdown.sequence(Shutdown.java:167)&lt;br/&gt;
        at java.lang.Shutdown.exit(Shutdown.java:212)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x00000000d0080660&amp;gt; (a java.lang.Class for java.lang.Shutdown)&lt;br/&gt;
        at java.lang.Terminator$1.handle(Terminator.java:52)&lt;br/&gt;
        at sun.misc.Signal$1.run(Signal.java:212)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&quot;kafka-scheduler-0&quot; daemon prio=10 tid=0x00007f884c045000 nid=0x26aa waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f83c9ee5000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (parking)&lt;br/&gt;
        at sun.misc.Unsafe.park(Native Method)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;parking to wait for  &amp;lt;0x00000000cfb1b800&amp;gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)&lt;br/&gt;
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)&lt;br/&gt;
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)&lt;br/&gt;
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)&lt;br/&gt;
        at kafka.utils.Utils$.inLock(Utils.scala:533)&lt;br/&gt;
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1149)&lt;br/&gt;
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1147)&lt;br/&gt;
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)&lt;br/&gt;
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)&lt;br/&gt;
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)&lt;br/&gt;
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)&lt;br/&gt;
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)&lt;br/&gt;
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1147)&lt;br/&gt;
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1126)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)&lt;br/&gt;
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)&lt;br/&gt;
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance(KafkaController.scala:1126)        at kafka.controller.KafkaController$$anonfun$onControllerFailover$1.apply$mcV$sp(KafkaController.scala:326)&lt;br/&gt;
        at kafka.utils.KafkaScheduler$$anonfun$1.apply$mcV$sp(KafkaScheduler.scala:99)&lt;br/&gt;
        at kafka.utils.Utils$$anon$1.run(Utils.scala:54)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)&lt;br/&gt;
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&quot;Controller-4-to-broker-1-send-thread&quot; prio=10 tid=0x00007f884c019800 nid=0x26a9 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f83c9fe6000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (parking)&lt;br/&gt;
        at sun.misc.Unsafe.park(Native Method)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;parking to wait for  &amp;lt;0x00000000d2828b58&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)&lt;br/&gt;
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)&lt;br/&gt;
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14165967" author="junrao" created="Thu, 9 Oct 2014 23:17:27 +0000"  >&lt;p&gt;Sriharsha,&lt;/p&gt;

&lt;p&gt;It seems that you found a deadlock.&lt;/p&gt;

&lt;p&gt;In KafkaController.onControllerResignation(), it holds the controller lock while calling autoRebalanceScheduler.shutdown(). The auto rebalance scheduler couldn&apos;t shutdown since it&apos;s blocked waiting for the controller lock in checkAndTriggerPartitionRebalance().&lt;/p&gt;

&lt;p&gt;To break the deadlock, we can move autoRebalanceScheduler.shutdown() in onControllerResignation() to before acquiring the controller lock. We don&apos;t need to hold on the controller lock while shutting down the scheduler.&lt;/p&gt;</comment>
                            <comment id="14165971" author="sriharsha" created="Thu, 9 Oct 2014 23:20:23 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;. Testing with above change.&lt;/p&gt;</comment>
                            <comment id="14166153" author="sriharsha" created="Fri, 10 Oct 2014 01:47:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; ran tests with your suggested change. All of the controller shutdown went through without any issue.&lt;br/&gt;
I&apos;ll file a separate JIRA for autoRebalanceScheduler.shutdown deadlock.&lt;/p&gt;</comment>
                            <comment id="14166369" author="junrao" created="Fri, 10 Oct 2014 04:17:17 +0000"  >&lt;p&gt;Sriharsha,&lt;/p&gt;

&lt;p&gt;Thanks for testing this out. Based on your result, it seems that auto leader balancing is stable with a large controller channel queue. Could you create a patch by changing the default value of controller.message.queue.size to 10000 and auto.leader.rebalance.enable to true? Once that&apos;s done, we can resolve this jira. We can file a separate jira for controller refactoring.&lt;/p&gt;
</comment>
                            <comment id="14167017" author="sriharsha" created="Fri, 10 Oct 2014 15:51:36 +0000"  >&lt;p&gt;Created reviewboard &lt;a href=&quot;https://reviews.apache.org/r/26560/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/26560/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14167050" author="nehanarkhede" created="Fri, 10 Oct 2014 16:10:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sriharsha&quot; class=&quot;user-hover&quot; rel=&quot;sriharsha&quot;&gt;sriharsha&lt;/a&gt; What&apos;s the value in changing it from something to 10K vs unbounded?&lt;/p&gt;</comment>
                            <comment id="14167145" author="sriharsha" created="Fri, 10 Oct 2014 17:19:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nehanarkhede&quot; class=&quot;user-hover&quot; rel=&quot;nehanarkhede&quot;&gt;nehanarkhede&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt; my understanding is that we created more room for KafkaController not to get into any of the above mentioned issues by setting to 10k but yes making unbounded is a better option as there could be a chance of exhausting 10k bounded queue and run into issues. We can get rid off  controller.message.queue.size as config option and make the LinkedBlockingQueue unbounded.  &lt;/p&gt;</comment>
                            <comment id="14167179" author="junrao" created="Fri, 10 Oct 2014 17:44:06 +0000"  >&lt;p&gt;Yes, in theory, we can make the queue unbounded. However, in practice, the queue shouldn&apos;t build up. I was a bit concerned that if we make the queue unbounded and another issue that causes the queue to build up, we may hit OOME. Then, we may not be able to take a thread dump to diagnose the issue.&lt;/p&gt;</comment>
                            <comment id="14168819" author="nehanarkhede" created="Sun, 12 Oct 2014 22:25:54 +0000"  >&lt;p&gt;Increasing the queue size by a little doesn&apos;t really solve the problem. We should conduct more tests on an unbounded controller queue, if we have any doubt whether or not it will work. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sriharsha&quot; class=&quot;user-hover&quot; rel=&quot;sriharsha&quot;&gt;sriharsha&lt;/a&gt; I will help you review changes to the controller, if you are up for updating your patch.&lt;/p&gt;</comment>
                            <comment id="14168838" author="sriharsha" created="Sun, 12 Oct 2014 23:20:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nehanarkhede&quot; class=&quot;user-hover&quot; rel=&quot;nehanarkhede&quot;&gt;nehanarkhede&lt;/a&gt; so the changes you are looking for are remove the config option and make the LinkedBlockingQueue to unbounded?&lt;/p&gt;</comment>
                            <comment id="14168852" author="sriharsha" created="Sun, 12 Oct 2014 23:49:45 +0000"  >&lt;p&gt;Created reviewboard &lt;a href=&quot;https://reviews.apache.org/r/26633/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/26633/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14168934" author="junrao" created="Mon, 13 Oct 2014 04:20:19 +0000"  >&lt;p&gt;Perhaps we can just set the default queue to max int for now. If we don&apos;t see any issue with this, we can make LinkedBlockingQueue to unbounded later. Could you also change the default value for auto.leader.rebalance.enable?&lt;/p&gt;</comment>
                            <comment id="14169343" author="sriharsha" created="Mon, 13 Oct 2014 14:30:53 +0000"  >&lt;p&gt;Updated reviewboard &lt;a href=&quot;https://reviews.apache.org/r/26633/diff/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://reviews.apache.org/r/26633/diff/&lt;/a&gt;&lt;br/&gt;
 against branch origin/trunk&lt;/p&gt;</comment>
                            <comment id="14169697" author="nehanarkhede" created="Mon, 13 Oct 2014 18:25:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sriharsha&quot; class=&quot;user-hover&quot; rel=&quot;sriharsha&quot;&gt;sriharsha&lt;/a&gt; Latest patch looks good. Pushing it to trunk and 0.8.2&lt;/p&gt;</comment>
                            <comment id="14213960" author="jbrosenberg@gmail.com" created="Sun, 16 Nov 2014 16:55:06 +0000"  >&lt;p&gt;Is it safe to say then, if we are not yet on 0.8.2 (e.g. still on 0.8.1.1), we should not enable automatic preferred leader election?&lt;/p&gt;</comment>
                            <comment id="14213997" author="sriharsha" created="Sun, 16 Nov 2014 18:29:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbrosenberg%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;jbrosenberg@gmail.com&quot;&gt;jbrosenberg@gmail.com&lt;/a&gt; The fix is in config. So if you are using 0.8.1.1 you can enable automatic preferred leader election but make sure you set controller.message.queue.size to Int.MaxValue by default this is set to 10 in 0.8.1.1.&lt;/p&gt;</comment>
                            <comment id="14360253" author="dmitrybugaychenko" created="Fri, 13 Mar 2015 12:01:28 +0000"  >&lt;p&gt;With a large controller queues size we see a significant datalos during prefered replica election for a brokers leading a lot of partitions (100+). The problem is that the prefered replica handles its requests fast, empties its request queue and stop following others, but the old leader hadles request much slower and it takse few minutes before it stop considering itself as a leader for all re-elected partitions. During these few minutes old leader continue to acknowledge produce requests and at the end it recognize its not longer a leader and truncates its logs deleting all data received...&lt;/p&gt;</comment>
                            <comment id="14360638" author="becket_qin" created="Fri, 13 Mar 2015 16:57:27 +0000"  >&lt;p&gt;That&apos;s a good point. But I kind of think the right solution to that problem does not lie in the queue size. Because there will be data loss in leader migration today, more or less. The amount is actually non-deterministic. So my understanding is either user can tolerate data loss or user needs to use acks=-1.&lt;/p&gt;</comment>
                            <comment id="14361155" author="dmitrybugaychenko" created="Fri, 13 Mar 2015 21:42:53 +0000"  >&lt;p&gt;Yes, data loss is tolerated to some extend. With small queue it was about lossing less then a second or even none of data and were considered fine, but with extended queue it is about few minutes - using acks in this case will simply cause producers to crash, denie their service or drop messages (because for few minutes they basically can not produce).  In the end we decided to reduce the queue to default and to apply three patches:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Add throttling to prefered replica elections and controlled shuttdown leadership reasignement&lt;/li&gt;
	&lt;li&gt;Add timeout for adding messages to queue in order to avoid locked controller&lt;/li&gt;
	&lt;li&gt;Add separate timeout for sending controlled shutdown message - in our setup it takes about 10 minutes and this value is meaningless and dangerous for other kind of controller-to-broker communication&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Things seems to work and data are not lost, but shutdown and rebalance are slow. Instead of throttling it could be better to wait for previous leader movement to be completed by all the participatnts before moving to next one. It is also possible to do leader movements in batches (at least api seems to support that).&lt;/p&gt;</comment>
                            <comment id="14361185" author="becket_qin" created="Fri, 13 Mar 2015 22:13:44 +0000"  >&lt;p&gt;It is not clear to me how adding timeout when put messages to broker queue in controller would help. This operation is done in the controller lock and have to be infinitely retry. I would guess in a parallel shutdown, you might still see deadlock.&lt;/p&gt;</comment>
                            <comment id="14361619" author="dmitrybugaychenko" created="Sat, 14 Mar 2015 05:53:28 +0000"  >&lt;p&gt;Retry itself is in the cahnnel manager independent of controller lock. Deadlock happens because one of the threads owning controller lock trying to put message to channel manager - it waits for free space but won&apos;t evere get it. With timeout it won&apos;t wait forever and eventually fail the operation given controller a chance to handle broker failure (which includes closing corresponding channel and emptying its queue).&lt;/p&gt;</comment>
                            <comment id="14361666" author="becket_qin" created="Sat, 14 Mar 2015 08:10:11 +0000"  >&lt;p&gt;I see. The risk of this approach is that controller or broker could potentially be in a inconsistent state. Because it is not necessarily the case that timeout occurs on broker shutdown. In that case, some controller to broker messages are sent while some might not.&lt;br/&gt;
I think the key problem of current approach is that we mix the data plain and control plain, i.e. the controller message and user data are handled by same request handlers on Kafka server. So controller messages usually sitting in the queue behind many user requests. That could cause the handling of controller messages to delay for almost arbitrary time (the more leader a broker has, the worse the situation will be). The right solution is probably having a separate thread handling controller message or prioritize controller message handling. Giving priority to controller message probably has less change because we just need to insert the controller message to the head of the queue instead of the tail.&lt;/p&gt;</comment>
                            <comment id="14363129" author="dmitrybugaychenko" created="Mon, 16 Mar 2015 12:25:12 +0000"  >&lt;p&gt;Even with a fast dedicated channel there will be a race condition in switching leadership. It could be removed either by complicating the protocol (eg. the new leader shoul take leadership only after getting &quot;not a leader&quot; respone in fetcher thread from the old one, while the old leader should stop handling produce request allowing fetches only from the new leader untill it gets everything), or, may be, it is worth to consider getting rid of controller in partition leader election and use distributed elections in ZK.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12747179">KAFKA-1699</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12674439" name="KAFKA-1305.patch" size="7267" author="sriharsha" created="Sun, 12 Oct 2014 23:49:44 +0000"/>
                            <attachment id="12674192" name="KAFKA-1305.patch" size="1858" author="sriharsha" created="Fri, 10 Oct 2014 15:51:36 +0000"/>
                            <attachment id="12674511" name="KAFKA-1305_2014-10-13_07:30:45.patch" size="1865" author="sriharsha" created="Mon, 13 Oct 2014 14:30:53 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>379731</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 36 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1tfg7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>380016</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>junrao</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>