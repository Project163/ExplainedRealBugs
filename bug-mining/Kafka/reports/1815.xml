<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:06:35 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-6528] Transient failure in DynamicBrokerReconfigurationTest.testThreadPoolResize</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-6528</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.AssertionError: expected:&amp;lt;108&amp;gt; but was:&amp;lt;123&amp;gt;
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at kafka.server.DynamicBrokerReconfigurationTest.stopAndVerifyProduceConsume(DynamicBrokerReconfigurationTest.scala:755)
	at kafka.server.DynamicBrokerReconfigurationTest.verifyThreadPoolResize$1(DynamicBrokerReconfigurationTest.scala:443)
	at kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize(DynamicBrokerReconfigurationTest.scala:451)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13135837">KAFKA-6528</key>
            <summary>Transient failure in DynamicBrokerReconfigurationTest.testThreadPoolResize</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsivaram">Rajini Sivaram</assignee>
                                    <reporter username="hachikuji">Jason Gustafson</reporter>
                        <labels>
                    </labels>
                <created>Fri, 2 Feb 2018 18:30:13 +0000</created>
                <updated>Tue, 6 Feb 2018 10:21:17 +0000</updated>
                            <resolved>Tue, 6 Feb 2018 10:21:17 +0000</resolved>
                                                    <fixVersion>1.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16352449" author="githubbot" created="Mon, 5 Feb 2018 14:27:15 +0000"  >&lt;p&gt;rajinisivaram opened a new pull request #4526: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6528&quot; title=&quot;Transient failure in DynamicBrokerReconfigurationTest.testThreadPoolResize&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6528&quot;&gt;&lt;del&gt;KAFKA-6528&lt;/del&gt;&lt;/a&gt;: Fix transient test failure in testThreadPoolResize&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4526&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4526&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Add locking to access `AbstractFetcherThread#partitionStates` during dynamic thread update. Also make testing of thread updates that trigger retries more resilient.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16353653" author="githubbot" created="Tue, 6 Feb 2018 09:50:54 +0000"  >&lt;p&gt;rajinisivaram closed pull request #4526: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6528&quot; title=&quot;Transient failure in DynamicBrokerReconfigurationTest.testThreadPoolResize&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6528&quot;&gt;&lt;del&gt;KAFKA-6528&lt;/del&gt;&lt;/a&gt;: Fix transient test failure in testThreadPoolResize&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/4526&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/4526&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/server/AbstractFetcherManager.scala b/core/src/main/scala/kafka/server/AbstractFetcherManager.scala&lt;br/&gt;
index 6d88d8de8b2..312123c3ab6 100755&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/AbstractFetcherManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/AbstractFetcherManager.scala&lt;br/&gt;
@@ -70,9 +70,7 @@ abstract class AbstractFetcherManager(protected val name: String, clientId: Stri&lt;br/&gt;
   def resizeThreadPool(newSize: Int): Unit = {&lt;br/&gt;
     def migratePartitions(newSize: Int): Unit = {&lt;br/&gt;
       fetcherThreadMap.foreach { case (id, thread) =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val removedPartitions = thread.partitionStates.partitionStates.asScala.map 
{ case state =&amp;gt;
-          state.topicPartition -&amp;gt; new BrokerAndInitialOffset(thread.sourceBroker, state.value.fetchOffset)
-        }
&lt;p&gt;.toMap&lt;br/&gt;
+        val removedPartitions = thread.partitionsAndOffsets&lt;br/&gt;
         removeFetcherForPartitions(removedPartitions.keySet)&lt;br/&gt;
         if (id.fetcherId &amp;gt;= newSize)&lt;br/&gt;
           thread.shutdown()&lt;br/&gt;
diff --git a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala&lt;br/&gt;
index 925c33095a2..39a70321a6e 100755&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala&lt;br/&gt;
@@ -312,6 +312,12 @@ abstract class AbstractFetcherThread(name: String,&lt;br/&gt;
     finally partitionMapLock.unlock()&lt;br/&gt;
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;server&amp;#93;&lt;/span&gt; def partitionsAndOffsets: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, BrokerAndInitialOffset&amp;#93;&lt;/span&gt; = inLock(partitionMapLock) {&lt;br/&gt;
+    partitionStates.partitionStates.asScala.map &lt;/p&gt;
{ case state =&amp;gt;
+      state.topicPartition -&amp;gt; new BrokerAndInitialOffset(sourceBroker, state.value.fetchOffset)
+    }
&lt;p&gt;.toMap&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt; object AbstractFetcherThread {&lt;br/&gt;
diff --git a/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala b/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala&lt;br/&gt;
index b7f0ae863a2..cb2ac5244a0 100644&lt;br/&gt;
&amp;#8212; a/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala&lt;br/&gt;
@@ -69,6 +69,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;/p&gt;

&lt;p&gt;   private val servers = new ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaServer&amp;#93;&lt;/span&gt;&lt;br/&gt;
   private val numServers = 3&lt;br/&gt;
+  private val numPartitions = 10&lt;br/&gt;
   private val producers = new ArrayBuffer[KafkaProducer&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;]&lt;br/&gt;
   private val consumers = new ArrayBuffer[KafkaConsumer&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;]&lt;br/&gt;
   private val adminClients = new ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;AdminClient&amp;#93;&lt;/span&gt;()&lt;br/&gt;
@@ -122,7 +123,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;br/&gt;
     TestUtils.createTopic(zkClient, Topic.GROUP_METADATA_TOPIC_NAME, OffsetConfig.DefaultOffsetsTopicNumPartitions,&lt;br/&gt;
       replicationFactor = numServers, servers, servers.head.groupCoordinator.offsetsTopicConfigs)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TestUtils.createTopic(zkClient, topic, numPartitions = 10, replicationFactor = numServers, servers)&lt;br/&gt;
+    TestUtils.createTopic(zkClient, topic, numPartitions, replicationFactor = numServers, servers)&lt;br/&gt;
     createAdminClient(SecurityProtocol.SSL, SecureInternal)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     TestMetricsReporter.testReporters.clear()&lt;br/&gt;
@@ -203,7 +204,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;br/&gt;
   @Test&lt;br/&gt;
   def testKeyStoreAlter(): Unit = &lt;/p&gt;
{
     val topic2 = &quot;testtopic2&quot;
-    TestUtils.createTopic(zkClient, topic2, numPartitions = 10, replicationFactor = numServers, servers)
+    TestUtils.createTopic(zkClient, topic2, numPartitions, replicationFactor = numServers, servers)
 
     // Start a producer and consumer that work with the current truststore.
     // This should continue working while changes are made
@@ -241,7 +242,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet
     verifyProduceConsume(producer, consumer, 10, topic2)
 
     // Verify that all messages sent with retries=0 while keystores were being altered were consumed
-    stopAndVerifyProduceConsume(producerThread, consumerThread, mayFailRequests = false)
+    stopAndVerifyProduceConsume(producerThread, consumerThread)
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -282,7 +283,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;br/&gt;
     verifyThreads(&quot;kafka-log-cleaner-thread-&quot;, countPerBroker = 2)&lt;/p&gt;

&lt;p&gt;     // Verify that produce/consume worked throughout this test without any retries in producer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stopAndVerifyProduceConsume(producerThread, consumerThread, mayFailRequests = false)&lt;br/&gt;
+    stopAndVerifyProduceConsume(producerThread, consumerThread)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
@@ -370,7 +371,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;br/&gt;
     servers.tail.foreach &lt;/p&gt;
{ server =&amp;gt; assertEquals(Defaults.LogIndexSizeMaxBytes, server.config.values.get(KafkaConfig.LogIndexSizeMaxBytesProp)) }

&lt;p&gt;     // Verify that produce/consume worked throughout this test without any retries in producer&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stopAndVerifyProduceConsume(producerThread, consumerThread, mayFailRequests = false)&lt;br/&gt;
+    stopAndVerifyProduceConsume(producerThread, consumerThread)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
@@ -418,9 +419,9 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;br/&gt;
       reconfigureServers(props, perBrokerConfig = false, (propName, newSize.toString))&lt;br/&gt;
       maybeVerifyThreadPoolSize(propName, newSize, threadPrefix)&lt;br/&gt;
     }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def verifyThreadPoolResize(propName: String, currentSize: =&amp;gt; Int, threadPrefix: String, mayFailRequests: Boolean): Unit = {&lt;br/&gt;
+    def verifyThreadPoolResize(propName: String, currentSize: =&amp;gt; Int, threadPrefix: String, mayReceiveDuplicates: Boolean): Unit = {&lt;br/&gt;
       maybeVerifyThreadPoolSize(propName, currentSize, threadPrefix)&lt;/li&gt;
	&lt;li&gt;val numRetries = if (mayFailRequests) 100 else 0&lt;br/&gt;
+      val numRetries = if (mayReceiveDuplicates) 100 else 0&lt;br/&gt;
       val (producerThread, consumerThread) = startProduceConsume(numRetries)&lt;br/&gt;
       var threadPoolSize = currentSize&lt;br/&gt;
       (1 to 2).foreach 
{ _ =&amp;gt;
@@ -429,20 +430,20 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet
         threadPoolSize = increasePoolSize(propName, threadPoolSize, threadPrefix)
         Thread.sleep(100)
       }&lt;/li&gt;
	&lt;li&gt;stopAndVerifyProduceConsume(producerThread, consumerThread, mayFailRequests)&lt;br/&gt;
+      stopAndVerifyProduceConsume(producerThread, consumerThread, mayReceiveDuplicates)&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val config = servers.head.config&lt;br/&gt;
     verifyThreadPoolResize(KafkaConfig.NumIoThreadsProp, config.numIoThreads,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;requestHandlerPrefix, mayFailRequests = false)&lt;/li&gt;
	&lt;li&gt;verifyThreadPoolResize(KafkaConfig.NumNetworkThreadsProp, config.numNetworkThreads,&lt;/li&gt;
	&lt;li&gt;networkThreadPrefix, mayFailRequests = true)&lt;br/&gt;
+      requestHandlerPrefix, mayReceiveDuplicates = false)&lt;br/&gt;
     verifyThreadPoolResize(KafkaConfig.NumReplicaFetchersProp, config.numReplicaFetchers,&lt;/li&gt;
	&lt;li&gt;fetcherThreadPrefix, mayFailRequests = false)&lt;br/&gt;
+      fetcherThreadPrefix, mayReceiveDuplicates = false)&lt;br/&gt;
     verifyThreadPoolResize(KafkaConfig.BackgroundThreadsProp, config.backgroundThreads,&lt;/li&gt;
	&lt;li&gt;&quot;kafka-scheduler-&quot;, mayFailRequests = false)&lt;br/&gt;
+      &quot;kafka-scheduler-&quot;, mayReceiveDuplicates = false)&lt;br/&gt;
     verifyThreadPoolResize(KafkaConfig.NumRecoveryThreadsPerDataDirProp, config.numRecoveryThreadsPerDataDir,&lt;/li&gt;
	&lt;li&gt;&quot;&quot;, mayFailRequests = false)&lt;br/&gt;
+      &quot;&quot;, mayReceiveDuplicates = false)&lt;br/&gt;
+    verifyThreadPoolResize(KafkaConfig.NumNetworkThreadsProp, config.numNetworkThreads,&lt;br/&gt;
+      networkThreadPrefix, mayReceiveDuplicates = true)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
@@ -1055,17 +1056,16 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   private def stopAndVerifyProduceConsume(producerThread: ProducerThread, consumerThread: ConsumerThread,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;mayFailRequests: Boolean = false): Unit = {&lt;br/&gt;
+                                          mayReceiveDuplicates: Boolean = false): Unit = {&lt;br/&gt;
     TestUtils.waitUntilTrue(() =&amp;gt; producerThread.sent &amp;gt;= 10, &quot;Messages not sent&quot;)&lt;br/&gt;
     producerThread.shutdown()&lt;br/&gt;
     consumerThread.initiateShutdown()&lt;br/&gt;
     consumerThread.awaitShutdown()&lt;/li&gt;
	&lt;li&gt;if (!mayFailRequests)&lt;/li&gt;
	&lt;li&gt;assertEquals(producerThread.sent, consumerThread.received)&lt;/li&gt;
	&lt;li&gt;else {&lt;/li&gt;
	&lt;li&gt;assertTrue(s&quot;Some messages not received, sent=${producerThread.sent} received=${consumerThread.received}&quot;,&lt;/li&gt;
	&lt;li&gt;consumerThread.received &amp;gt;= producerThread.sent)&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
+    assertEquals(producerThread.lastSent, consumerThread.lastReceived)&lt;br/&gt;
+    assertEquals(0, consumerThread.missingRecords.size)&lt;br/&gt;
+    if (!mayReceiveDuplicates)&lt;br/&gt;
+      assertFalse(&quot;Duplicates not expected&quot;, consumerThread.duplicates)&lt;br/&gt;
+    assertFalse(&quot;Some messages received out of order&quot;, consumerThread.outOfOrder)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private def verifyConnectionFailure(producer: KafkaProducer&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;): Future&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
@@ -1128,32 +1128,58 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet&lt;/p&gt;

&lt;p&gt;   private class ProducerThread(clientId: String, retries: Int) extends ShutdownableThread(clientId, isInterruptible = false) {&lt;br/&gt;
     private val producer = createProducer(trustStoreFile1, retries, clientId)&lt;br/&gt;
+    val lastSent = new ConcurrentHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Int&amp;#93;&lt;/span&gt;()&lt;br/&gt;
     @volatile var sent = 0&lt;br/&gt;
     override def doWork(): Unit = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;while (isRunning) 
{
-                sent += 1
-                val record = new ProducerRecord(topic, s&quot;key$sent&quot;, s&quot;value$sent&quot;)
-                producer.send(record).get(10, TimeUnit.SECONDS)
-              }&lt;/li&gt;
	&lt;li&gt;} finally 
{
-            producer.close()
-          }
&lt;p&gt;+      try &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        while (isRunning) {
+          val key = sent.toString
+          val partition = sent % numPartitions
+          val record = new ProducerRecord(topic, partition, key, s&quot;value$sent&quot;)
+          producer.send(record).get(10, TimeUnit.SECONDS)
+          lastSent.put(partition, sent)
+          sent += 1
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; finally &lt;/p&gt;
{
+        producer.close()
       }
&lt;p&gt;+    }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private class ConsumerThread(producerThread: ProducerThread) extends ShutdownableThread(&quot;test-consumer&quot;, isInterruptible = false) {&lt;br/&gt;
     private val consumer = createConsumer(&quot;group1&quot;, trustStoreFile1)&lt;br/&gt;
+    val lastReceived = new ConcurrentHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Int&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    val missingRecords = new ConcurrentLinkedQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+    @volatile var outOfOrder = false&lt;br/&gt;
+    @volatile var duplicates = false&lt;br/&gt;
     @volatile var lastBatch: ConsumerRecords&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
     @volatile private var endTimeMs = Long.MaxValue&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var received = 0&lt;br/&gt;
+    @volatile var received = 0&lt;br/&gt;
     override def doWork(): Unit = {&lt;br/&gt;
       try {&lt;/li&gt;
	&lt;li&gt;while (isRunning || (received &amp;lt; producerThread.sent &amp;amp;&amp;amp; System.currentTimeMillis &amp;lt; endTimeMs)) {&lt;br/&gt;
+        while (isRunning || (lastReceived != producerThread.lastSent &amp;amp;&amp;amp; System.currentTimeMillis &amp;lt; endTimeMs)) {&lt;br/&gt;
           val records = consumer.poll(50)&lt;br/&gt;
           received += records.count&lt;/li&gt;
	&lt;li&gt;if (!records.isEmpty)&lt;br/&gt;
+          if (!records.isEmpty) {&lt;br/&gt;
             lastBatch = records&lt;br/&gt;
+            records.partitions.asScala.foreach { tp =&amp;gt;&lt;br/&gt;
+              val partition = tp.partition&lt;br/&gt;
+              records.records(tp).asScala.map(_.key.toInt).foreach 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: { key =&amp;gt;+                val prevKey = lastReceived.asScala.get(partition).getOrElse(partition - numPartitions)+                val expectedKey = prevKey + numPartitions+                if (key &amp;lt; prevKey)+                  outOfOrder = true+                else if (key == prevKey)+                  duplicates = true+                else {
+                  for (i &amp;lt;- expectedKey until key by numPartitions)
+                    missingRecords.add(expectedKey)
+                }+                lastReceived.put(partition, key)+                missingRecords.remove(key)+              }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+            }&lt;br/&gt;
+          }&lt;br/&gt;
         }&lt;br/&gt;
       } finally {&lt;br/&gt;
         consumer.close()&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 41 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3pq3j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>