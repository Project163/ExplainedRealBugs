<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:35:35 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-499] Refactor controller state machine </title>
                <link>https://issues.apache.org/jira/browse/KAFKA-499</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Currently, the controller logic is very procedural and is similar to KafkaZookeeper. Controller should have a well defined state machine with states and transitions. This will make it easier to understand and maintain the controller code. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12606438">KAFKA-499</key>
            <summary>Refactor controller state machine </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nehanarkhede">Neha Narkhede</assignee>
                                    <reporter username="nehanarkhede">Neha Narkhede</reporter>
                        <labels>
                            <label>optimization</label>
                    </labels>
                <created>Thu, 6 Sep 2012 16:57:52 +0000</created>
                <updated>Tue, 18 Sep 2012 20:35:25 +0000</updated>
                            <resolved>Tue, 18 Sep 2012 17:30:52 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                    <workratio workratioPercent="0"/>
                                    <progress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                    <timeoriginalestimate seconds="345600">96h</timeoriginalestimate>
                            <timeestimate seconds="345600">96h</timeestimate>
                                        <comments>
                            <comment id="13455939" author="nehanarkhede" created="Fri, 14 Sep 2012 16:59:45 +0000"  >&lt;p&gt;This patch refactors (almost rewrites) the controller. The controller has the following components -&lt;br/&gt;
1. A partition state machine.&lt;br/&gt;
2. A replica state machine&lt;br/&gt;
3. Controller channel manager for communication with the brokers.&lt;/p&gt;

&lt;p&gt;1. Partition state machine&lt;br/&gt;
 This represents the state machine for partitions. It defines the states that a partition can be in, and&lt;br/&gt;
 transitions to move the partition to some legal state. The different states that a partition can be in are -&lt;br/&gt;
 1. NonExistentPartition: This state indicates that the partition was either never created or was created and then&lt;br/&gt;
deleted. Valid previous state, if one exists, is OfflinePartition&lt;br/&gt;
 2. NewPartition        : After creation, the partition is in the NewPartition state. In this state, the partition should have replicas assigned to it, but no leader/isr yet. Valid previous states are NonExistentPartition&lt;br/&gt;
 3. OnlinePartition     : Once a leader is elected for a partition, it is in the OnlinePartition state. Valid previous states are NewPartition/OfflinePartition&lt;br/&gt;
 4. OfflinePartition    : If, after successful leader election, the leader for partition dies, then the partition moves to the OfflinePartition state. Valid previous states are NewPartition/OnlinePartition&lt;/p&gt;

&lt;p&gt;2. Replica state machine&lt;br/&gt;
  This class represents the state machine for replicas. It defines the states that a replica can be in, and&lt;br/&gt;
  transitions to move the replica to another legal state. The different states that a replica can be in are -&lt;br/&gt;
  1. OnlineReplica     : Once a replica is started, it is in this state. Valid previous state are OnlineReplica or OfflineReplica&lt;br/&gt;
  2. OfflineReplica    : If a replica dies, it moves to this state. Valid previous state is OnlineReplica. &lt;br/&gt;
There might be some changes to this state machine after &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-42&quot; title=&quot;Support rebalancing the partitions with replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-42&quot;&gt;&lt;del&gt;KAFKA-42&lt;/del&gt;&lt;/a&gt; is checked in. For now, these are the relevant states that a replica can be in.&lt;/p&gt;

&lt;p&gt;3. Controller channel manager&lt;/p&gt;

&lt;p&gt;  Everything related to communication between controller and broker is moved here. This includes the request channels to the brokers, the request send thread, and the request batching mechanism. Since the request batching mechanism is new, I&apos;ll explain it here -&lt;br/&gt;
ControllerBrokerRequestBatch&lt;br/&gt;
3.1. All the state changes initiated by the controller requires batching of requests per broker. Previously, the code to achieve this was copy pasted in several state changes APIs. I moved it into one class that allows the controller to -&lt;br/&gt;
3.1.1. newBatch() - start a new request batch when a set of state changes need to be initiated&lt;br/&gt;
3.1.2  sendRequestsToBrokers() - collate the state changes per broker and send them&lt;/p&gt;

&lt;p&gt;4. Testing&lt;/p&gt;

&lt;p&gt; While I was testing the controller code, I found that most of our existing unit tests already cover most of the corner cases. For example, LogRecoveryTest and LeaderElectionTest. However, I guess the controller needs more system testing where we have multiple topics and partitions and it -&lt;br/&gt;
4.1 Introduces soft (GC) as well as hard failures to trigger controller failover. Right now, our tests only cover controller failures (clean shutdown)&lt;br/&gt;
4.2 Introduce state changes for multiple partitions in quick succession.&lt;br/&gt;
I feel the above test could be cleanly done as a system test. Since this patch blocks &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-42&quot; title=&quot;Support rebalancing the partitions with replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-42&quot;&gt;&lt;del&gt;KAFKA-42&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-43&quot; title=&quot;Rebalance to preferred broke with intra-cluster replication support&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-43&quot;&gt;&lt;del&gt;KAFKA-43&lt;/del&gt;&lt;/a&gt; and some other controller related JIRAs, I thought I can push this to a separate patch or under &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-42&quot; title=&quot;Support rebalancing the partitions with replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-42&quot;&gt;&lt;del&gt;KAFKA-42&lt;/del&gt;&lt;/a&gt;. If people feel it is important to add it as part of this patch, let me know, I can get it in.&lt;/p&gt;

&lt;p&gt;5. I wanted to include a state change diagram to make it easier to understand both the partitions and replica state machines. I think I will add that to the Internal Design wiki soon.&lt;/p&gt;</comment>
                            <comment id="13456140" author="nehanarkhede" created="Fri, 14 Sep 2012 21:06:26 +0000"  >&lt;p&gt;Rebased and fixed a minor JMX bean issue&lt;/p&gt;</comment>
                            <comment id="13456641" author="junrao" created="Sun, 16 Sep 2012 19:33:16 +0000"  >&lt;p&gt;Thanks for patch v2. It&apos;s a big change. However, it&apos;s changing things in the right direction. It makes the logic cleaner and covers some of the corner cases that were missed before. Excellent job! Some comments:&lt;/p&gt;

&lt;p&gt;20. ReplicaStateMachine:&lt;br/&gt;
20.1 handleStateChanges(): There is no need to read the partition assignment from ZK. We can just use the cache version in controller context.&lt;br/&gt;
20.2 handleStateChange(): We can use leaderAndIsr in the last parameter of the following method directly.&lt;br/&gt;
                  brokerRequestBatch.addRequestForBrokers(List(replicaId), topic, partition,&lt;br/&gt;
                    controllerContext.allLeaders((topic, partition)))&lt;br/&gt;
20.3 handleStateChange(): For the OfflineReplica case, we should send new LeaderISRRequests to the leader broker. We should also increment the leader epoc. In theory, we just need to generate new LeaderISRRequests for partitions that lost a follower, not the leader (those partitions are handled in partitionStateChange already). For the latter partitions, we will be sending the same LeaderISRRequests twice, once through partitionStateTransition, and another through replicaStateTransition. However, we don&apos;t really need to optimize this right now since the makeLeader process on a broker is very cheap.&lt;br/&gt;
20.4 handleStateChange(): The OnlineReplica case is a bit special in that valid previous states include OnlineReplica itself. No other state transition allows transition from one state to itself. This seems to be due to that in initializeReplicaState(), we already initialized some replicas to online state already. However, in general, should we allow self transition for all states? This seems to be safer and will guard the case when we somehow redid the same transition again.&lt;br/&gt;
20.5 Before this patch, we will send LeaderAndISR requests with the INIT flag in 2 cases: one during controller failover and another during broker startup. The only purpose for the INIT flad is really to ensure that a broker will clean up deleted topics properly. Thinking about this more, using INIT is really a hacky way to handle topic deletion when we do have a clean state machine. Now that we have one, a better way is probably to use a separate topic deletion path and only delete it once every broker has received the stop replica request. On both controller failover and replica startup, the new controller can find out all topics that still need to be deleted and resend the stop replica requests. This seems to be a better way of handling deletion. So, in this patch, maybe we can get rid of the INIT flag and not to worry about deletion. We can add the proper delete state transition when we get to topic deletion.&lt;/p&gt;

&lt;p&gt;21. PartitionStateMachine:&lt;br/&gt;
21.1 isShuttingDown needs to be set to false in startup, not just in constructor since startup can be called multiple times. Ditto for ReplicaStateMachine.&lt;br/&gt;
21.2 initializeLeaderAndIsrForPartition(): Do we really need to read leaderAndISR from ZK before updating it? In this case, we expect that path not to exist. We can just create the path with the initial leaderAndISR. If the path already exists, we will get a ZK exception and we can log a state change failure.&lt;br/&gt;
21.3 electLeaderForPartition(): we should only update controllerContext.allLeaders if zookeeperPathUpdateSucceeded is true.&lt;br/&gt;
21.4 handleStateChange(): We should update controllerContext.allLeaders in both the OfflinePartition and the NonExistentPartition case.&lt;br/&gt;
21.5 remove unused imports&lt;/p&gt;

&lt;p&gt;22. RequestPurgatoryTest: Are the changes om testRequestExpiry() needed?&lt;/p&gt;

&lt;p&gt;23. ControllerBrokerRequestBatch.addRequestForBrokers(): The following statement seems to be unnecessarily creating a new HashMap on each call (except for the first one, which is necessary).&lt;br/&gt;
      brokerRequestMap.getOrElseUpdate(brokerId, new mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Int), LeaderAndIsr&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;24. KafkaController:&lt;br/&gt;
24.1 can startChannelManager() be part of initializeControllerContext?&lt;br/&gt;
24.2 In ControllerContext.allLeaders, we just need to cache the leader of each partition, not the ISR. This is because ISR can be changed by the leader at any time, which will invalidate the cache in the controller. Every time that the controller wants to update leaderAndISR in ZK, it always has to read the latest ISR from ZK anyway. Also, there is no need to call updateLeaderAndIsrCache in onBrokerFailure and onBrokerStartup. The leader cache is always updated when there is a leader change. ISR always has to be read from ZK before each update.&lt;br/&gt;
24.3 We need to hold a controller lock whenever we call onControllerFailover()  since it registers listeners and therefore allows the watcher handler to update cached maps in controller context concurrently. Currently, ZookeeperLeaderElector.startup can call onControllerFailover() without the controller lock.&lt;br/&gt;
24.4 It seems that all ZK listeners like BrokerChangeListener, TopicChangeListener and PartitionChangeListener should be in controller, not inside ReplicaStateMachine and PartitionStateMachine. Both new-topic and broker failure watchers are defined in controller and state change machines just need to act on state changes.&lt;/p&gt;

&lt;p&gt;25. ZookeeperLeaderElector:&lt;br/&gt;
25.1 Currently, it seems that we can call onBecomingLeader() twice, once in startup and another in LeaderChangeListener.handleDataChange. This may not cause any harm now, but it would be better if we can avoid it. One possible way is to call onBecomingLeader in elect if the election wins. Then we can get rid of the code in LeaderChangeListener.handleDataChange and the call to onBecomingLeader() in startup.&lt;br/&gt;
25.2 The name ZookeeperLeaderElector seems very generic. Should we name it to ControllerElector? Ditto for LeaderChangeListener?&lt;br/&gt;
25.3 The subscription of leaderChangeListener needs to be done in elect() after the ephemeral path is created successfully, not in startup. This is because we need to set the watcher each time the controller path is created and only the subscription sets the watcher.&lt;/p&gt;</comment>
                            <comment id="13457375" author="nehanarkhede" created="Mon, 17 Sep 2012 21:42:45 +0000"  >&lt;p&gt;Appreciate your patience in reviewing the controller code (again). Thanks a lot for the detailed review !&lt;br/&gt;
0. ReplicaStateMachine:&lt;br/&gt;
0.1 handleStateChanges(): Right. Changed it to use the cached version in controller context.&lt;br/&gt;
0.2 handleStateChange(): Changed it.&lt;br/&gt;
0.3 handleStateChange(): Good catch! I had forgotten to include the actual request send. Also, in addition to sending the LeaderAndIsr request to the leader replica, we also should update the controller&apos;s allLeaders cache. And I agree that we shouldn&apos;t try to optimize the non-follower case, at least just yet.&lt;br/&gt;
0.4 handleStateChange(): You raise a good point here. I think the OnlineReplica state description is a little fuzzy right now. I hope that after &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-42&quot; title=&quot;Support rebalancing the partitions with replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-42&quot;&gt;&lt;del&gt;KAFKA-42&lt;/del&gt;&lt;/a&gt;, it will be more clear. You are right that it is special from the others and this is due to the hacky way we use the INIT flag to resend the list of partitions to all the replicas. I actually don&apos;t think we should be handling deletes the way we do today. I think you&apos;ve agreed on this as part of 20.5 below. However, until we have that, OnlineReplica probably might stay special. Also, I&apos;m not sure the other states can be subjected to self transitions without fully knowing the triggers that can cause such self transitions. I can revisit all states and see if we can safely allow self transitions for those as well.&lt;br/&gt;
0.5 Awesome, I remember discussing the hackiness of this approach during the design discussions. Great that you agree with me here. Let&apos;s push this work to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-330&quot; title=&quot;Add delete topic support &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-330&quot;&gt;&lt;del&gt;KAFKA-330&lt;/del&gt;&lt;/a&gt;. Until then, I&apos;ve removed the INIT flag support.&lt;/p&gt;

&lt;p&gt;1. PartitionStateMachine:&lt;br/&gt;
1.1 Right, this probably happened since the startup() API was an after-thought. Fixed it.&lt;br/&gt;
1.2 initializeLeaderAndIsrForPartition(): Good point. Fixed it.&lt;br/&gt;
1.3 electLeaderForPartition(): True, fixed it&lt;br/&gt;
1.4 handleStateChange(): Actually, changing the state of the partition has no real effect on the allLeaders cache, unless a new leader is elected or ISR has changed. We remove the partition from the allLeaders cache only when it is deleted. We add a partition to this cache, when it enters the OnlinePartition state.&lt;/p&gt;

&lt;p&gt;2. RequestPurgatoryTest: This was accidental, probably during the rebase. Reverted it.&lt;/p&gt;

&lt;p&gt;3. ControllerBrokerRequestBatch.addRequestForBrokers(): It will create a new hashmap only if it executes that part of the code, which is when the brokerId doesn&apos;t exist.&lt;/p&gt;

&lt;p&gt;4. KafkaController:&lt;br/&gt;
4.1 Moved startChannelManager to initializeControllerContext.&lt;br/&gt;
4.2 Very good point. I agree that we shouldn&apos;t really be caching the ISR. I had initially changed it to cover the Online/OfflineReplica state change user case to send the LeaderAndIsr request to the affected brokers, without realizing that caching ISR after that is not useful/safe.&lt;br/&gt;
4.3 Good catch ! Fixed it.&lt;br/&gt;
4.4 They were in controller initially, but I moved it to the respective state machine on purpose. The reason is that I thought it will be good for each state machine to also have the appropriate zookeeper listeners that trigger the state changes wrapped in that state machine. The reason that the controller has the callbacks is because the controller needs to notify multiple state machines about the trigger in a certain order. So you can look at the controller as a bunch of callbacks required to handle the replication state machine transitions correctly. That is, controller offers a high level view of the all the possible meta state changes and triggers that can happen as part of replication. If one needs to dig deeper, they can look at the individual state machines for the details.&lt;/p&gt;

&lt;p&gt;5. ZookeeperLeaderElector:&lt;br/&gt;
5.1 Actually, we don&apos;t need to call onBecomingLeader in the startup(), that is redundant. It needs to be called when the leader successfully writes the new ephemeral node in zookeeper. So, this can happen only once either in the LeaderChangeListener or inside elect(). I moved it to elect()&lt;br/&gt;
5.2 It is generic for a purpose. When we include the consumer co-ordinator change, it will require to use the same ZookeeperLeaderElector component for the election of the consumer co-ordinator. Since it is written to be generic, the name is generic too.&lt;br/&gt;
5.3 Actually, the point is that you just need to register a watcher on startup and then on new session creation. Since it is a data change listener, it also includes an exists listener. The problem was that on session expiration, it wasn&apos;t re-registering the leader change listener. This is a bug. I fixed it to move the registration as the first line in elect() API.&lt;/p&gt;

&lt;p&gt;6. Removed all code that referred to the INIT flag in the LeaderAndIsrRequest&lt;/p&gt;</comment>
                            <comment id="13457519" author="nehanarkhede" created="Tue, 18 Sep 2012 00:58:52 +0000"  >&lt;p&gt;Rebased after &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-391&quot; title=&quot;Producer request and response classes should use maps&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-391&quot;&gt;&lt;del&gt;KAFKA-391&lt;/del&gt;&lt;/a&gt;&apos;s checkin. &lt;/p&gt;</comment>
                            <comment id="13457592" author="junrao" created="Tue, 18 Sep 2012 04:37:02 +0000"  >&lt;p&gt;Thanks for patch v4. Have a few minor comments. Once those are addressed, the patch can be checked in without another review.&lt;/p&gt;

&lt;p&gt;40. ControllerBrokerRequestBatch.sendRequestsToBrokers(): no need for  isInit in constructor.&lt;/p&gt;

&lt;p&gt;41. ReplicaStateMachine: The purpose of isShuttingDown is probably to guard that we don&apos;t startup (shutdown) again if it&apos;s already started (shutdown). If so, we should do that. Otherwise, there is really no need for isShuttingDown. Ditto for PartitionStateMachine.&lt;/p&gt;

&lt;p&gt;42. PartitionStateMachine: In all places that we mark a partition offline, we should remove the partition from controllerContext.allLeaders.&lt;/p&gt;

&lt;p&gt;43. ZookeeperLeaderElector: We can remove all the code in LeaderChangeListener.handleDataChange() now.&lt;/p&gt;</comment>
                            <comment id="13457964" author="nehanarkhede" created="Tue, 18 Sep 2012 17:14:21 +0000"  >&lt;p&gt;ReplicaStateMachine: The purpose of isShuttingDown is to avoid triggering state changes from the BrokerChangeListener when the state machine is in the middle of a shutdown. Ditto for PartitionStateMachine.&lt;br/&gt;
PartitionStateMachine: The allLeaders cache stores the controller&apos;s leader decision for all existing partitions in zookeeper. A partition is deleted from this list only when it is deleted.&lt;/p&gt;

&lt;p&gt;Rest of the comments are addressed.&lt;/p&gt;</comment>
                            <comment id="13457971" author="junrao" created="Tue, 18 Sep 2012 17:26:46 +0000"  >&lt;p&gt;That sounds good. +1 on the patch.&lt;/p&gt;</comment>
                            <comment id="13457974" author="nehanarkhede" created="Tue, 18 Sep 2012 17:30:52 +0000"  >&lt;p&gt;Thanks for the review ! Just committed it.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12514679">KAFKA-42</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12514680">KAFKA-43</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12545171" name="kafka-499-v1.patch" size="110058" author="nehanarkhede" created="Fri, 14 Sep 2012 17:01:31 +0000"/>
                            <attachment id="12545214" name="kafka-499-v2.patch" size="110662" author="nehanarkhede" created="Fri, 14 Sep 2012 21:06:26 +0000"/>
                            <attachment id="12545481" name="kafka-499-v3.patch" size="114006" author="nehanarkhede" created="Mon, 17 Sep 2012 21:42:45 +0000"/>
                            <attachment id="12545508" name="kafka-499-v4.patch" size="113072" author="nehanarkhede" created="Tue, 18 Sep 2012 00:58:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>299149</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 9 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i15zwf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>243116</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>