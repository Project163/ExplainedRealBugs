<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:15:47 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7464] Fail to shutdown ReplicaManager during broker cleaned shutdown</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7464</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;In 2.0 deployment, we saw the following log when shutting down the ReplicaManager in broker cleaned shutdown:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2018/09/27 08:22:18.699 WARN [CoreUtils$] [Thread-1] [kafka-server] [] null
java.lang.IllegalArgumentException: null
        at java.nio.Buffer.position(Buffer.java:244) ~[?:1.8.0_121]
        at sun.nio.ch.IOUtil.write(IOUtil.java:68) ~[?:1.8.0_121]
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.SslTransportLayer.flush(SslTransportLayer.java:214) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.network.SslTransportLayer.close(SslTransportLayer.java:164) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.utils.Utils.closeAll(Utils.java:806) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.network.KafkaChannel.close(KafkaChannel.java:107) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.network.Selector.doClose(Selector.java:751) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.network.Selector.close(Selector.java:739) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.network.Selector.close(Selector.java:701) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.network.Selector.close(Selector.java:315) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.clients.NetworkClient.close(NetworkClient.java:595) ~[kafka-clients-2.0.0.22.jar:?]
        at kafka.server.ReplicaFetcherBlockingSend.close(ReplicaFetcherBlockingSend.scala:107) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.ReplicaFetcherThread.initiateShutdown(ReplicaFetcherThread.scala:108) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherManager$$anonfun$closeAllFetchers$2.apply(AbstractFetcherManager.scala:183) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherManager$$anonfun$closeAllFetchers$2.apply(AbstractFetcherManager.scala:182) ~[kafka_2.11-2.0.0.22.jar:?]
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) ~[scala-library-2.11.12.jar:?]
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130) ~[scala-library-2.11.12.jar:?]
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130) ~[scala-library-2.11.12.jar:?]
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236) ~[scala-library-2.11.12.jar:?]
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40) ~[scala-library-2.11.12.jar:?]
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:130) ~[scala-library-2.11.12.jar:?]
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) ~[scala-library-2.11.12.jar:?]
        at kafka.server.AbstractFetcherManager.closeAllFetchers(AbstractFetcherManager.scala:182) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.ReplicaFetcherManager.shutdown(ReplicaFetcherManager.scala:37) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.ReplicaManager.shutdown(ReplicaManager.scala:1471) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.KafkaServer$$anonfun$shutdown$12.apply$mcV$sp(KafkaServer.scala:616) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:616) ~[kafka_2.11-2.0.0.22.jar:?]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;After that, we noticed that some of the replica fetcher thread fail to shutdown:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2018/09/27 08:22:46.176 ERROR [LogDirFailureChannel] [ReplicaFetcherThread-26-13085] [kafka-server] [] Error while rolling log segment for video-social-gestures-30 in dir /export/content/kafka/i001_caches
java.nio.channels.ClosedChannelException: null
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:110) ~[?:1.8.0_121]
        at sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:300) ~[?:1.8.0_121]
        at org.apache.kafka.common.record.FileRecords.truncateTo(FileRecords.java:244) ~[kafka-clients-2.0.0.22.jar:?]
        at org.apache.kafka.common.record.FileRecords.trim(FileRecords.java:206) ~[kafka-clients-2.0.0.22.jar:?]
        at kafka.log.LogSegment.onBecomeInactiveSegment(LogSegment.scala:512) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log$$anonfun$roll$2$$anonfun$apply$30.apply(Log.scala:1493) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log$$anonfun$roll$2$$anonfun$apply$30.apply(Log.scala:1493) ~[kafka_2.11-2.0.0.22.jar:?]
        at scala.Option.foreach(Option.scala:257) ~[scala-library-2.11.12.jar:?]
        at kafka.log.Log$$anonfun$roll$2.apply(Log.scala:1493) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log$$anonfun$roll$2.apply(Log.scala:1479) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log.maybeHandleIOException(Log.scala:1856) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log.roll(Log.scala:1479) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log.kafka$log$Log$$maybeRoll(Log.scala:1465) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log$$anonfun$append$2.apply(Log.scala:868) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log$$anonfun$append$2.apply(Log.scala:762) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log.maybeHandleIOException(Log.scala:1856) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log.append(Log.scala:762) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.log.Log.appendAsFollower(Log.scala:743) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.cluster.Partition$$anonfun$doAppendRecordsToFollowerOrFutureReplica$1.apply(Partition.scala:601) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.utils.CoreUtils$.inReadLock(CoreUtils.scala:257) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.cluster.Partition.doAppendRecordsToFollowerOrFutureReplica(Partition.scala:588) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.cluster.Partition.appendRecordsToFollowerOrFutureReplica(Partition.scala:608) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:130) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:43) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:188) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:174) ~[kafka_2.11-2.0.0.22.jar:?]
        at scala.Option.foreach(Option.scala:257) ~[scala-library-2.11.12.jar:?]
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:174) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:171) ~[kafka_2.11-2.0.0.22.jar:?]
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library-2.11.12.jar:?]
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library-2.11.12.jar:?]
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply$mcV$sp(AbstractFetcherThread.scala:171) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:171) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:171) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:169) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:115) ~[kafka_2.11-2.0.0.22.jar:?]
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82) ~[kafka_2.11-2.0.0.22.jar:?]&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Worse more, we found out that if there is a exception thrown in ReplicaFetcherManager shutdown, we basically will skip purgatory shutdown and HW checkpoint and in our case we didn&apos;t see the &quot;Shut down completely&quot; log:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 def shutdown(checkpointHW: &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) {
    info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Shutting down&quot;&lt;/span&gt;)
    removeMetrics()
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (logDirFailureHandler != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;)
      logDirFailureHandler.shutdown()
    replicaFetcherManager.shutdown()
    replicaAlterLogDirsManager.shutdown()
    delayedFetchPurgatory.shutdown()
    delayedProducePurgatory.shutdown()
    delayedDeleteRecordsPurgatory.shutdown()
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (checkpointHW)
      checkpointHighWatermarks()
    info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Shut down completely&quot;&lt;/span&gt;)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The reason why we see this is that after &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6051&quot; title=&quot;ReplicaFetcherThread should close the ReplicaFetcherBlockingSend earlier on shutdown&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6051&quot;&gt;&lt;del&gt;KAFKA-6051&lt;/del&gt;&lt;/a&gt;, we close leaderEndPoint in replica fetcher thread initiateShutdown to try to preempt in-progress fetch request and accelerate repica fetcher thread shutdown. However, leaderEndpoint can throw an Exception when the replica fetcher thread is still actively fetching.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I am wondering whether we should try to catch the exception thrown in &quot;leaderEndpoint.close()&quot; instead of letting it throw up in the call stack. In my opinion, it is safe to do so because ReplicaFetcherThread.initiateShutdown will be called when:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Server shutdown &#8211; In this case we will shut down the process anyway so even though we fail to close leader enpoint cleanly there is no harm.&lt;/li&gt;
	&lt;li&gt;shutdownIdleFetcherThread &#8211; In this case the fetcher thread is idle and we will not use it again anyway so there is no harm either.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <environment></environment>
        <key id="13188590">KAFKA-7464</key>
            <summary>Fail to shutdown ReplicaManager during broker cleaned shutdown</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hzxa21">Zhanxiang (Patrick) Huang</assignee>
                                    <reporter username="hzxa21">Zhanxiang (Patrick) Huang</reporter>
                        <labels>
                    </labels>
                <created>Mon, 1 Oct 2018 17:04:08 +0000</created>
                <updated>Sat, 20 Oct 2018 20:43:20 +0000</updated>
                            <resolved>Thu, 18 Oct 2018 22:01:57 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16652198" author="ijuma" created="Tue, 16 Oct 2018 18:25:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hzxa21&quot; class=&quot;user-hover&quot; rel=&quot;hzxa21&quot;&gt;hzxa21&lt;/a&gt;, are you planning to submit a fix for this? It&apos;s currently marked as a blocker for 2.0.1 and 2.1.0.&lt;/p&gt;</comment>
                            <comment id="16652394" author="githubbot" created="Tue, 16 Oct 2018 20:28:52 +0000"  >&lt;p&gt;hzxa21 opened a new pull request #5808: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7464&quot; title=&quot;Fail to shutdown ReplicaManager during broker cleaned shutdown&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7464&quot;&gt;&lt;del&gt;KAFKA-7464&lt;/del&gt;&lt;/a&gt;: catch exceptions in &quot;leaderEndpoint.close()&quot; when shutting down ReplicaFetcherThread&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5808&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5808&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   After &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6051&quot; title=&quot;ReplicaFetcherThread should close the ReplicaFetcherBlockingSend earlier on shutdown&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6051&quot;&gt;&lt;del&gt;KAFKA-6051&lt;/del&gt;&lt;/a&gt;, we close leaderEndPoint in replica fetcher thread initiateShutdown to try to preempt in-progress fetch request and accelerate repica fetcher thread shutdown. However, leaderEndpoint can throw an Exception when the replica fetcher thread is still actively fetching, which can cause ReplicaManager to fail to shutdown cleanly. This PR catches the exceptions thrown in &quot;leaderEndpoint.close()&quot; instead of letting it throw up in the call stack.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16652396" author="hzxa21" created="Tue, 16 Oct 2018 20:31:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;I was waiting to see whether there is any concern for my proposal and was not aware of this is blocking the release. I have submitted the PR for the fix. Let me know if there is any concern. Thanks!&lt;/p&gt;</comment>
                            <comment id="16652812" author="lindong" created="Wed, 17 Oct 2018 01:38:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ijuma&quot; class=&quot;user-hover&quot; rel=&quot;ijuma&quot;&gt;ijuma&lt;/a&gt; Previously I marked this issue as blocker because the effect looks very concerning: the broker will fail to shutdown cleanly. After taking a closer look at the issue, it seems that the issue happens very rarely and it should not be a blocker. The bug has been introduced since Oct 2017 . And kafka broker at LinkedIn has also been running well without requiring a fix for this issue.&lt;/p&gt;

&lt;p&gt;I have updated the JIRA to mark it as critical instead of blocker. It is still good to fix this issue in Apache Kafka.&lt;/p&gt;</comment>
                            <comment id="16655709" author="githubbot" created="Thu, 18 Oct 2018 18:27:01 +0000"  >&lt;p&gt;lindong28 closed pull request #5808: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7464&quot; title=&quot;Fail to shutdown ReplicaManager during broker cleaned shutdown&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7464&quot;&gt;&lt;del&gt;KAFKA-7464&lt;/del&gt;&lt;/a&gt;: catch exceptions in &quot;leaderEndpoint.close()&quot; when shutting down ReplicaFetcherThread&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5808&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5808&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
index aeeaf29516a..6b119308205 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala&lt;br/&gt;
@@ -115,7 +115,19 @@ class ReplicaFetcherThread(name: String,&lt;br/&gt;
   override def initiateShutdown(): Boolean = {&lt;br/&gt;
     val justShutdown = super.initiateShutdown()&lt;br/&gt;
     if (justShutdown) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;leaderEndpoint.close()&lt;br/&gt;
+      // leaderEndpoint.close can throw an exception when the replica fetcher thread is still&lt;br/&gt;
+      // actively fetching because the selector can close the channel while sending the request&lt;br/&gt;
+      // after we initiate leaderEndpoint.close and the leaderEndpoint.close itself may also close&lt;br/&gt;
+      // the channel again. When this race condition happens, an exception will be thrown.&lt;br/&gt;
+      // Throwing the exception to the caller may fail the ReplicaManager shutdown. It is safe to catch&lt;br/&gt;
+      // the exception without here causing correctness issue because we are going to shutdown the thread&lt;br/&gt;
+      // and will not re-use the leaderEndpoint anyway.&lt;br/&gt;
+      try 
{
+        leaderEndpoint.close()
+      }
&lt;p&gt; catch &lt;/p&gt;
{
+        case t: Throwable =&amp;gt;
+          debug(s&quot;Fail to close leader endpoint $leaderEndpoint after initiating replica fetcher thread shutdown&quot;, t)
+      }
&lt;p&gt;     }&lt;br/&gt;
     justShutdown&lt;br/&gt;
   }&lt;br/&gt;
diff --git a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
index 4d54c81044a..c9d9b966964 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
+++ b/core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala&lt;br/&gt;
@@ -24,16 +24,16 @@ import kafka.cluster.Partition&lt;br/&gt;
 import kafka.server.QuotaFactory.UnboundedQuota&lt;br/&gt;
 import kafka.server.epoch.LeaderEpochFileCache&lt;br/&gt;
 import kafka.server.epoch.util.ReplicaFetcherMockBlockingSend&lt;br/&gt;
-import kafka.utils.TestUtils&lt;br/&gt;
+import kafka.utils.
{LogCaptureAppender, TestUtils}
&lt;p&gt; import org.apache.kafka.clients.ClientResponse&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition&lt;br/&gt;
-import org.apache.kafka.common.internals.PartitionStates&lt;br/&gt;
 import org.apache.kafka.common.metrics.Metrics&lt;br/&gt;
 import org.apache.kafka.common.protocol.&lt;/p&gt;
{ApiKeys, Errors}
&lt;p&gt; import org.apache.kafka.common.protocol.Errors._&lt;br/&gt;
 import org.apache.kafka.common.requests.&lt;/p&gt;
{EpochEndOffset, OffsetsForLeaderEpochRequest}
&lt;p&gt; import org.apache.kafka.common.requests.EpochEndOffset._&lt;br/&gt;
 import org.apache.kafka.common.utils.SystemTime&lt;br/&gt;
+import org.apache.log4j.Level&lt;br/&gt;
 import org.easymock.EasyMock._&lt;br/&gt;
 import org.easymock.&lt;/p&gt;
{Capture, CaptureType, IAnswer}
&lt;p&gt; import org.junit.Assert._&lt;br/&gt;
@@ -793,6 +793,45 @@ class ReplicaFetcherThreadTest &lt;/p&gt;
{
     assertEquals(49, truncateToCapture.getValue)
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @Test&lt;br/&gt;
+  def shouldCatchExceptionFromBlockingSendWhenShuttingDownReplicaFetcherThread(): Unit = {&lt;br/&gt;
+    val props = TestUtils.createBrokerConfig(1, &quot;localhost:1234&quot;)&lt;br/&gt;
+    val config = KafkaConfig.fromProps(props)&lt;br/&gt;
+    val mockBlockingSend = createMock(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BlockingSend&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+&lt;br/&gt;
+    expect(mockBlockingSend.close()).andThrow(new IllegalArgumentException()).once()&lt;br/&gt;
+    replay(mockBlockingSend)&lt;br/&gt;
+&lt;br/&gt;
+    val thread = new ReplicaFetcherThread(&lt;br/&gt;
+      name = &quot;bob&quot;,&lt;br/&gt;
+      fetcherId = 0,&lt;br/&gt;
+      sourceBroker = brokerEndPoint,&lt;br/&gt;
+      brokerConfig = config,&lt;br/&gt;
+      replicaMgr = null,&lt;br/&gt;
+      metrics =  new Metrics(),&lt;br/&gt;
+      time = new SystemTime(),&lt;br/&gt;
+      quota = null,&lt;br/&gt;
+      leaderEndpointBlockingSend = Some(mockBlockingSend))&lt;br/&gt;
+&lt;br/&gt;
+    val previousLevel = LogCaptureAppender.setClassLoggerLevel(thread.getClass, Level.DEBUG)&lt;br/&gt;
+    val logCaptureAppender = LogCaptureAppender.createAndRegister()&lt;br/&gt;
+&lt;br/&gt;
+    try &lt;/p&gt;
{
+      thread.initiateShutdown()
+
+      val event = logCaptureAppender.getMessages.find(e =&amp;gt; e.getLevel == Level.DEBUG
+        &amp;amp;&amp;amp; e.getRenderedMessage.contains(s&quot;Fail to close leader endpoint $mockBlockingSend after initiating replica fetcher thread shutdown&quot;)
+        &amp;amp;&amp;amp; e.getThrowableInformation != null
+        &amp;amp;&amp;amp; e.getThrowableInformation.getThrowable.getClass.getName.equals(new IllegalArgumentException().getClass.getName))
+      assertTrue(event.isDefined)
+
+      verify(mockBlockingSend)
+    }
&lt;p&gt; finally &lt;/p&gt;
{
+      LogCaptureAppender.unregister(logCaptureAppender)
+      LogCaptureAppender.setClassLoggerLevel(thread.getClass, previousLevel)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
   def stub(replica: Replica, partition: Partition, replicaManager: ReplicaManager) = {&lt;br/&gt;
     expect(replicaManager.localReplica(t1p0)).andReturn(Some(replica)).anyTimes()&lt;br/&gt;
     expect(replicaManager.localReplicaOrException(t1p0)).andReturn(replica).anyTimes()&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 4 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3yorr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>