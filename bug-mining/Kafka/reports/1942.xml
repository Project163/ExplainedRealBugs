<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 17:11:50 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-7021] Source KTable checkpoint is not correct</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-7021</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;Kafka Streams treats source KTables,ie, table created via `builder.table()`, differently. Instead of creating a changelog topic, the original source topic is use to avoid unnecessary data redundancy.&lt;/p&gt;

&lt;p&gt;However, Kafka Streams does not write a correct local state checkpoint file. This results in unnecessary state restore after a rebalance. Instead of the latest committed offset, the latest restored offset is written into the checkpoint file in `ProcessorStateManager#close()`&lt;/p&gt;</description>
                <environment></environment>
        <key id="13164822">KAFKA-7021</key>
            <summary>Source KTable checkpoint is not correct</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="guozhang">Guozhang Wang</assignee>
                                    <reporter username="mjsax">Matthias J. Sax</reporter>
                        <labels>
                    </labels>
                <created>Thu, 7 Jun 2018 23:31:52 +0000</created>
                <updated>Fri, 15 Jun 2018 15:49:48 +0000</updated>
                            <resolved>Fri, 15 Jun 2018 15:33:01 +0000</resolved>
                                    <version>0.10.0.0</version>
                                    <fixVersion>0.10.2.2</fixVersion>
                    <fixVersion>0.11.0.3</fixVersion>
                    <fixVersion>1.0.2</fixVersion>
                    <fixVersion>1.1.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>streams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16508927" author="githubbot" created="Mon, 11 Jun 2018 23:08:27 +0000"  >&lt;p&gt;guozhangwang closed pull request #5163: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7021&quot; title=&quot;Source KTable checkpoint is not correct&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7021&quot;&gt;&lt;del&gt;KAFKA-7021&lt;/del&gt;&lt;/a&gt;: Reuse source based on config&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5163&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5163&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java b/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java&lt;br/&gt;
index d6002ff016b..6a707ff986d 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java&lt;br/&gt;
@@ -599,7 +599,7 @@ public KafkaStreams(final Topology topology,&lt;br/&gt;
     @Deprecated&lt;br/&gt;
     public KafkaStreams(final Topology topology,&lt;br/&gt;
                         final StreamsConfig config) &lt;/p&gt;
{
-        this(topology.internalTopologyBuilder, config, new DefaultKafkaClientSupplier());
+        this(topology, config, new DefaultKafkaClientSupplier());
     }

&lt;p&gt;     /**&lt;br/&gt;
@@ -635,6 +635,10 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,&lt;br/&gt;
         this.config = config;&lt;br/&gt;
         this.time = time;&lt;/p&gt;

&lt;p&gt;+        // adjust the topology if optimization is turned on.&lt;br/&gt;
+        // TODO: to be removed post 2.0&lt;br/&gt;
+        internalTopologyBuilder.adjust(config);&lt;br/&gt;
+&lt;br/&gt;
         // The application ID is a required config and hence should always have value&lt;br/&gt;
         processId = UUID.randomUUID();&lt;br/&gt;
         final String userClientId = config.getString(StreamsConfig.CLIENT_ID_CONFIG);&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java b/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java&lt;br/&gt;
index 517104da323..ae6d44c449e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java&lt;br/&gt;
@@ -302,11 +302,10 @@&lt;br/&gt;
         Objects.requireNonNull(materialized, &quot;materialized can&apos;t be null&quot;);&lt;br/&gt;
         final MaterializedInternal&amp;lt;K, V, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt; materializedInternal = new MaterializedInternal&amp;lt;&amp;gt;(materialized);&lt;br/&gt;
         materializedInternal.generateStoreNameIfNeeded(internalStreamsBuilder, topic + &quot;-&quot;);&lt;br/&gt;
+        final ConsumedInternal&amp;lt;K, V&amp;gt; consumedInternal =&lt;br/&gt;
+                new ConsumedInternal&amp;lt;&amp;gt;(Consumed.with(materializedInternal.keySerde(), materializedInternal.valueSerde()));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;return internalStreamsBuilder.table(topic,&lt;/li&gt;
	&lt;li&gt;new ConsumedInternal&amp;lt;&amp;gt;(Consumed.with(materializedInternal.keySerde(),&lt;/li&gt;
	&lt;li&gt;materializedInternal.valueSerde())),&lt;/li&gt;
	&lt;li&gt;materializedInternal);&lt;br/&gt;
+        return internalStreamsBuilder.table(topic, consumedInternal, materializedInternal);&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     /**&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/Topology.java b/streams/src/main/java/org/apache/kafka/streams/Topology.java&lt;br/&gt;
index 22f6ea8362b..753185c2164 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/Topology.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/Topology.java&lt;br/&gt;
@@ -776,5 +776,4 @@ public synchronized Topology connectProcessorAndStateStores(final String process&lt;br/&gt;
     public synchronized TopologyDescription describe() &lt;/p&gt;
{
         return internalTopologyBuilder.describe();
     }
&lt;p&gt;-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java&lt;br/&gt;
index 0a19b4eb0c0..c7bf2fac8f7 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java&lt;br/&gt;
@@ -72,11 +72,7 @@ public InternalStreamsBuilder(final InternalTopologyBuilder internalTopologyBuil&lt;br/&gt;
     public &amp;lt;K, V&amp;gt; KTable&amp;lt;K, V&amp;gt; table(final String topic,&lt;br/&gt;
                                      final ConsumedInternal&amp;lt;K, V&amp;gt; consumed,&lt;br/&gt;
                                      final MaterializedInternal&amp;lt;K, V, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt; materialized) &lt;/p&gt;
{
-        // explicitly disable logging for source table materialized stores
-        materialized.withLoggingDisabled();
-
-        final StoreBuilder&amp;lt;KeyValueStore&amp;lt;K, V&amp;gt;&amp;gt; storeBuilder = new KeyValueStoreMaterializer&amp;lt;&amp;gt;(materialized)
-                .materialize();
+        final StoreBuilder&amp;lt;KeyValueStore&amp;lt;K, V&amp;gt;&amp;gt; storeBuilder = new KeyValueStoreMaterializer&amp;lt;&amp;gt;(materialized).materialize();
 
         final String source = newProcessorName(KStreamImpl.SOURCE_NAME);
         final String name = newProcessorName(KTableImpl.SOURCE_NAME);
@@ -88,7 +84,7 @@ public InternalStreamsBuilder(final InternalTopologyBuilder internalTopologyBuil
                                                  name);
 
         internalTopologyBuilder.addStateStore(storeBuilder, name);
-        internalTopologyBuilder.connectSourceStoreAndTopic(storeBuilder.name(), topic);
+        internalTopologyBuilder.markSourceStoreAndTopic(storeBuilder, topic);
 
         return kTable;
     }
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index 02a1a066ab8..188ff473038 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -167,7 +167,7 @@ public String toString(final String indent) &lt;/p&gt;
{
         return sb.toString();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected Map&amp;lt;TopicPartition, Long&amp;gt; recordCollectorOffsets() {&lt;br/&gt;
+    protected Map&amp;lt;TopicPartition, Long&amp;gt; activeTaskCheckpointableOffsets() 
{
         return Collections.emptyMap();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -242,7 +242,7 @@ void closeStateManager(final boolean writeCheckpoint) throws ProcessorStateExcep&lt;br/&gt;
         ProcessorStateException exception = null;&lt;br/&gt;
         log.trace(&quot;Closing state manager&quot;);&lt;br/&gt;
         try &lt;/p&gt;
{
-            stateMgr.close(writeCheckpoint ? recordCollectorOffsets() : null);
+            stateMgr.close(writeCheckpoint ? activeTaskCheckpointableOffsets() : null);
         }
&lt;p&gt; catch (final ProcessorStateException e) &lt;/p&gt;
{
             exception = e;
         }
&lt;p&gt; finally {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java&lt;br/&gt;
index 7d09031d713..36a2edc6766 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java&lt;br/&gt;
@@ -19,6 +19,7 @@&lt;br/&gt;
 import org.apache.kafka.common.serialization.Deserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serializer;&lt;br/&gt;
 import org.apache.kafka.common.utils.Utils;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
 import org.apache.kafka.streams.Topology;&lt;br/&gt;
 import org.apache.kafka.streams.errors.TopologyException;&lt;br/&gt;
 import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
@@ -121,6 +122,9 @@&lt;/p&gt;

&lt;p&gt;     private Map&amp;lt;Integer, Set&amp;lt;String&amp;gt;&amp;gt; nodeGroups = null;&lt;/p&gt;

&lt;p&gt;+    // TODO: this is only temporary for 2.0 and should be removed&lt;br/&gt;
+    public final Map&amp;lt;StoreBuilder, String&amp;gt; storeToSourceChangelogTopic = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
     public interface StateStoreFactory {&lt;br/&gt;
         Set&amp;lt;String&amp;gt; users();&lt;br/&gt;
         boolean loggingEnabled();&lt;br/&gt;
@@ -498,8 +502,14 @@ public final void addProcessor(final String name,&lt;/p&gt;

&lt;p&gt;     public final void addStateStore(final StoreBuilder storeBuilder,&lt;br/&gt;
                                     final String... processorNames) &lt;/p&gt;
{
+        addStateStore(storeBuilder, false, processorNames);
+    }
&lt;p&gt;+&lt;br/&gt;
+    public final void addStateStore(final StoreBuilder storeBuilder,&lt;br/&gt;
+                                    final boolean allowOverride,&lt;br/&gt;
+                                    final String... processorNames) {&lt;br/&gt;
         Objects.requireNonNull(storeBuilder, &quot;storeBuilder can&apos;t be null&quot;);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (stateFactories.containsKey(storeBuilder.name())) {&lt;br/&gt;
+        if (!allowOverride &amp;amp;&amp;amp; stateFactories.containsKey(storeBuilder.name())) 
{
             throw new TopologyException(&quot;StateStore &quot; + storeBuilder.name() + &quot; is already added.&quot;);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -566,16 +576,22 @@ public final void connectProcessorAndStateStores(final String processorName,&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// TODO: this method is only used by DSL and we might want to refactor this part&lt;br/&gt;
     public final void connectSourceStoreAndTopic(final String sourceStoreName,&lt;/li&gt;
	&lt;li&gt;final String topic) {&lt;br/&gt;
+                                                 final String topic) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {         if (storeToChangelogTopic.containsKey(sourceStoreName)) {
             throw new TopologyException(&quot;Source store &quot; + sourceStoreName + &quot; is already added.&quot;);
         }         storeToChangelogTopic.put(sourceStoreName, topic);     }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// TODO: this method is only used by DSL and we might want to refactor this part&lt;br/&gt;
+    public final void markSourceStoreAndTopic(final StoreBuilder storeBuilder,&lt;br/&gt;
+                                              final String topic) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        if (storeToSourceChangelogTopic.containsKey(storeBuilder)) {
+            throw new TopologyException(&quot;Source store &quot; + storeBuilder.name() + &quot; is already used.&quot;);
+        }+        storeToSourceChangelogTopic.put(storeBuilder, topic);+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
     public final void connectProcessors(final String... processorNames) {&lt;br/&gt;
         if (processorNames.length &amp;lt; 2) {&lt;br/&gt;
             throw new TopologyException(&quot;At least two processors need to participate in the connection.&quot;);&lt;br/&gt;
@@ -591,13 +607,11 @@ public final void connectProcessors(final String... processorNames) &lt;/p&gt;
{
         nodeGrouper.unite(processorNames[0], Arrays.copyOfRange(processorNames, 1, processorNames.length));
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// TODO: this method is only used by DSL and we might want to refactor this part&lt;br/&gt;
     public final void addInternalTopic(final String topicName) 
{
         Objects.requireNonNull(topicName, &quot;topicName can&apos;t be null&quot;);
         internalTopicNames.add(topicName);
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// TODO: this method is only used by DSL and we might want to refactor this part&lt;br/&gt;
     public final void copartitionSources(final Collection&amp;lt;String&amp;gt; sourceNodes) 
{
         copartitionSourceGroups.add(Collections.unmodifiableSet(new HashSet&amp;lt;&amp;gt;(sourceNodes)));
     }
&lt;p&gt;@@ -1059,6 +1073,24 @@ private void buildProcessorNode(final Map&amp;lt;String, ProcessorNode&amp;gt; processorMap,&lt;br/&gt;
         return Collections.unmodifiableMap(topicGroups);&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    // Adjust the generated topology based on the configs.&lt;br/&gt;
+    // Not exposed as public API and should be removed post 2.0&lt;br/&gt;
+    public void adjust(final StreamsConfig config) {&lt;br/&gt;
+        final boolean enableOptimization20 = config.getString(StreamsConfig.TOPOLOGY_OPTIMIZATION).equals(StreamsConfig.OPTIMIZE);&lt;br/&gt;
+&lt;br/&gt;
+        if (enableOptimization20) {&lt;br/&gt;
+            for (final Map.Entry&amp;lt;StoreBuilder, String&amp;gt; entry : storeToSourceChangelogTopic.entrySet()) &lt;/p&gt;
{
+                final StoreBuilder storeBuilder = entry.getKey();
+                final String topicName = entry.getValue();
+
+                // update store map to disable logging for this store
+                storeBuilder.withLoggingDisabled();
+                addStateStore(storeBuilder, true);
+                connectSourceStoreAndTopic(storeBuilder.name(), topicName);
+            }
&lt;p&gt;+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     private void setRegexMatchedTopicsToSourceNodes() {&lt;br/&gt;
         if (subscriptionUpdates.hasUpdates()) {&lt;br/&gt;
             for (final Map.Entry&amp;lt;String, Pattern&amp;gt; stringPatternEntry : nodeToSourcePatterns.entrySet()) &lt;/p&gt;
{
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java
index e7a23bd4b5f..054333b7a8f 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java
@@ -46,7 +46,7 @@
     private final boolean isStandby;
     private final ChangelogReader changelogReader;
     private final Map&amp;lt;TopicPartition, Long&amp;gt; offsetLimits;
-    private final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets;
+    private final Map&amp;lt;TopicPartition, Long&amp;gt; standbyRestoredOffsets;
     private final Map&amp;lt;String, StateRestoreCallback&amp;gt; restoreCallbacks; // used for standby tasks, keyed by state topic name
     private final Map&amp;lt;String, String&amp;gt; storeToChangelogTopic;
     private final List&amp;lt;TopicPartition&amp;gt; changelogPartitions = new ArrayList&amp;lt;&amp;gt;();
@@ -79,7 +79,7 @@ public ProcessorStateManager(final TaskId taskId,
             partitionForTopic.put(source.topic(), source);
         }
&lt;p&gt;         offsetLimits = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;restoredOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        standbyRestoredOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         this.isStandby = isStandby;&lt;br/&gt;
         restoreCallbacks = isStandby ? new HashMap&amp;lt;String, StateRestoreCallback&amp;gt;() : null;&lt;br/&gt;
         this.storeToChangelogTopic = storeToChangelogTopic;&lt;br/&gt;
@@ -212,7 +212,7 @@ public void reinitializeStateStoresForPartitions(final Collection&amp;lt;TopicPartition&lt;br/&gt;
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // record the restored offset for its change log partition&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;restoredOffsets.put(storePartition, lastOffset + 1);&lt;br/&gt;
+        standbyRestoredOffsets.put(storePartition, lastOffset + 1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return remainingRecords;&lt;br/&gt;
     }&lt;br/&gt;
@@ -293,8 +293,8 @@ public void close(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) throws Processor&lt;/p&gt;

&lt;p&gt;     // write the checkpoint&lt;br/&gt;
     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) {&lt;/li&gt;
	&lt;li&gt;checkpointableOffsets.putAll(changelogReader.restoredOffsets());&lt;br/&gt;
+    public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; checkpointableOffsets) {&lt;br/&gt;
+        this.checkpointableOffsets.putAll(changelogReader.restoredOffsets());&lt;br/&gt;
         for (final StateStore store : stores.values()) {&lt;br/&gt;
             final String storeName = store.name();&lt;br/&gt;
             // only checkpoint the offset to the offsets file if&lt;br/&gt;
@@ -302,11 +302,11 @@ public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) {&lt;br/&gt;
             if (store.persistent() &amp;amp;&amp;amp; storeToChangelogTopic.containsKey(storeName)) {&lt;br/&gt;
                 final String changelogTopic = storeToChangelogTopic.get(storeName);&lt;br/&gt;
                 final TopicPartition topicPartition = new TopicPartition(changelogTopic, getPartition(storeName));&lt;/li&gt;
	&lt;li&gt;if (ackedOffsets.containsKey(topicPartition)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                if (checkpointableOffsets.containsKey(topicPartition)) {
                     // store the last offset + 1 (the log position after restoration)
-                    checkpointableOffsets.put(topicPartition, ackedOffsets.get(topicPartition) + 1);
-                } else if (restoredOffsets.containsKey(topicPartition)) {
-                    checkpointableOffsets.put(topicPartition, restoredOffsets.get(topicPartition));
+                    this.checkpointableOffsets.put(topicPartition, checkpointableOffsets.get(topicPartition) + 1);
+                } else if (standbyRestoredOffsets.containsKey(topicPartition)) {
+                    this.checkpointableOffsets.put(topicPartition, standbyRestoredOffsets.get(topicPartition));
                 }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;         }&lt;br/&gt;
@@ -315,9 +315,9 @@ public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) &lt;/p&gt;
{
             checkpoint = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.trace(&quot;Writing checkpoint: {}&quot;, checkpointableOffsets);&lt;br/&gt;
+        log.trace(&quot;Writing checkpoint: {}&quot;, this.checkpointableOffsets);&lt;br/&gt;
         try 
{
-            checkpoint.write(checkpointableOffsets);
+            checkpoint.write(this.checkpointableOffsets);
         }
&lt;p&gt; catch (final IOException e) {&lt;br/&gt;
             log.warn(&quot;Failed to write offset checkpoint file to {}: {}&quot;, checkpoint, e);&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
index c33ade6f36e..7623c66cd3b 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
@@ -91,7 +91,7 @@ public StateDirectory(final StreamsConfig config,&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return directory for the 
{@link TaskId}&lt;/li&gt;
	&lt;li&gt;@throws ProcessorStateException if the task directory does not exists and could not be created&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;File directoryForTask(final TaskId taskId) {&lt;br/&gt;
+    public File directoryForTask(final TaskId taskId) {&lt;br/&gt;
         final File taskDir = new File(stateDir, taskId.toString());&lt;br/&gt;
         if (!taskDir.exists() &amp;amp;&amp;amp; !taskDir.mkdir()) {&lt;br/&gt;
             throw new ProcessorStateException(&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index e2be3e29172..9493493973e 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -380,7 +380,7 @@ void commit(final boolean startNewTransaction) {&lt;br/&gt;
         flushState();&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         if (!eosEnabled) &lt;/p&gt;
{
-            stateMgr.checkpoint(recordCollectorOffsets());
+            stateMgr.checkpoint(activeTaskCheckpointableOffsets());
         }

&lt;p&gt;         commitOffsets(startNewTransaction);&lt;br/&gt;
@@ -391,8 +391,13 @@ void commit(final boolean startNewTransaction) {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected Map&amp;lt;TopicPartition, Long&amp;gt; recordCollectorOffsets() {&lt;/li&gt;
	&lt;li&gt;return recordCollector.offsets();&lt;br/&gt;
+    protected Map&amp;lt;TopicPartition, Long&amp;gt; activeTaskCheckpointableOffsets() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        final Map&amp;lt;TopicPartition, Long&amp;gt; checkpointableOffsets = recordCollector.offsets();+        for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java&lt;br/&gt;
index 37101de344a..3b8c9bd47d9 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java&lt;br/&gt;
@@ -273,11 +273,17 @@ public void shouldUseDefaultNodeAndStoreNames() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldReuseSourceTopicAsChangelogs() {&lt;br/&gt;
+    public void shouldReuseSourceTopicAsChangelogsWithOptimization20() {&lt;br/&gt;
         final String topic = &quot;topic&quot;;&lt;br/&gt;
         builder.table(topic, Materialized.&amp;lt;Long, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;store&quot;));&lt;br/&gt;
+        final Topology topology = builder.build();&lt;br/&gt;
+        final Properties props = StreamsTestUtils.minimalStreamsConfig();&lt;br/&gt;
+        props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());&lt;br/&gt;
+        final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);&lt;br/&gt;
+        internalTopologyBuilder.adjust(new StreamsConfig(props));&lt;br/&gt;
+&lt;br/&gt;
+        assertThat(internalTopologyBuilder.build().storeToChangelogTopic(), equalTo(Collections.singletonMap(&quot;store&quot;, &quot;topic&quot;)));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertThat(internalTopologyBuilder.getStateStores().keySet(), equalTo(Collections.singleton(&quot;store&quot;)));&lt;/p&gt;

&lt;p&gt;@@ -285,6 +291,23 @@ public void shouldReuseSourceTopicAsChangelogs() &lt;/p&gt;
{
 
         assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.isEmpty(), equalTo(true));
     }
&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotReuseSourceTopicAsChangelogsByDefault() &lt;/p&gt;
{
+        final String topic = &quot;topic&quot;;
+        builder.table(topic, Materialized.&amp;lt;Long, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;store&quot;));
+
+        final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());
+        internalTopologyBuilder.setApplicationId(&quot;appId&quot;);
+
+        assertThat(internalTopologyBuilder.build().storeToChangelogTopic(), equalTo(Collections.singletonMap(&quot;store&quot;, &quot;appId-store-changelog&quot;)));
+
+        assertThat(internalTopologyBuilder.getStateStores().keySet(), equalTo(Collections.singleton(&quot;store&quot;)));
+
+        assertThat(internalTopologyBuilder.getStateStores().get(&quot;store&quot;).loggingEnabled(), equalTo(true));
+
+        assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.keySet(), equalTo(Collections.singleton(&quot;appId-store-changelog&quot;)));
+    }

&lt;p&gt;     @Test(expected = TopologyException.class)&lt;br/&gt;
     public void shouldThrowExceptionWhenNoTopicPresent() {&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
index f6d36f70848..dbf85fa46cd 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
@@ -19,7 +19,6 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.Consumer;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
-import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
 import org.apache.kafka.clients.producer.KafkaProducer;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
@@ -28,6 +27,7 @@&lt;br/&gt;
 import org.apache.kafka.common.serialization.IntegerSerializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
 import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
+import org.apache.kafka.common.utils.MockTime;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
@@ -44,11 +44,14 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
 import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StateRestoreListener;&lt;br/&gt;
+import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StateDirectory;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
 import org.apache.kafka.streams.state.StoreBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.state.Stores;&lt;br/&gt;
 import org.apache.kafka.streams.state.internals.KeyValueStoreBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.state.internals.OffsetCheckpoint;&lt;br/&gt;
 import org.apache.kafka.test.IntegrationTest;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.After;&lt;br/&gt;
@@ -57,10 +60,10 @@&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 import org.junit.experimental.categories.Category;&lt;/p&gt;

&lt;p&gt;+import java.io.File;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
-import java.util.HashMap;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
-import java.util.Map;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.concurrent.CountDownLatch;&lt;br/&gt;
 import java.util.concurrent.ExecutionException;&lt;br/&gt;
@@ -76,6 +79,8 @@&lt;br/&gt;
 public class RestoreIntegrationTest {&lt;br/&gt;
     private static final int NUM_BROKERS = 1;&lt;/p&gt;

&lt;p&gt;+    private static final String APPID = &quot;restore-test&quot;;&lt;br/&gt;
+&lt;br/&gt;
     @ClassRule&lt;br/&gt;
     public static final EmbeddedKafkaCluster CLUSTER =&lt;br/&gt;
             new EmbeddedKafkaCluster(NUM_BROKERS);&lt;br/&gt;
@@ -83,24 +88,24 @@&lt;br/&gt;
     private static final String INPUT_STREAM_2 = &quot;input-stream-2&quot;;&lt;br/&gt;
     private final int numberOfKeys = 10000;&lt;br/&gt;
     private KafkaStreams kafkaStreams;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private String applicationId = &quot;restore-test&quot;;&lt;br/&gt;
-&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @BeforeClass&lt;br/&gt;
     public static void createTopics() throws InterruptedException &lt;/p&gt;
{
         CLUSTER.createTopic(INPUT_STREAM, 2, 1);
         CLUSTER.createTopic(INPUT_STREAM_2, 2, 1);
+        CLUSTER.createTopic(APPID + &quot;-store-changelog&quot;, 2, 1);
     }

&lt;p&gt;     private Properties props(final String applicationId) &lt;/p&gt;
{
         Properties streamsConfiguration = new Properties();
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
-        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
+        streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
         streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(applicationId).getPath());
         streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());
         streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());
         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
         return streamsConfiguration;
     }

&lt;p&gt;@@ -112,24 +117,106 @@ public void shutdown() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldRestoreState() throws ExecutionException, InterruptedException {&lt;br/&gt;
+    public void shouldRestoreStateFromSourceTopic() throws Exception {&lt;br/&gt;
         final AtomicInteger numReceived = new AtomicInteger(0);&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;createStateForRestoration();&lt;br/&gt;
+        final Properties props = props(APPID);&lt;br/&gt;
+        props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);&lt;br/&gt;
+&lt;br/&gt;
+        // restoring from 1000 to 4000 (committed), and then process from 4000 to 5000 on each of the two partitions&lt;br/&gt;
+        final int offsetLimitDelta = 1000;&lt;br/&gt;
+        final int offsetCheckpointed = 1000;&lt;br/&gt;
+        createStateForRestoration(INPUT_STREAM);&lt;br/&gt;
+        setCommittedOffset(INPUT_STREAM, offsetLimitDelta);&lt;br/&gt;
+&lt;br/&gt;
+        final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(props), new MockTime());&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 0)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 0), (long) offsetCheckpointed));&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 1)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 1), (long) offsetCheckpointed));&lt;br/&gt;
+&lt;br/&gt;
+        final CountDownLatch startupLatch = new CountDownLatch(1);&lt;br/&gt;
+        final CountDownLatch shutdownLatch = new CountDownLatch(1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         builder.table(INPUT_STREAM, Consumed.with(Serdes.Integer(), Serdes.Integer()))&lt;br/&gt;
                 .toStream()&lt;br/&gt;
                 .foreach(new ForeachAction&amp;lt;Integer, Integer&amp;gt;() {&lt;br/&gt;
                     @Override&lt;br/&gt;
                     public void apply(final Integer key, final Integer value) &lt;/p&gt;
{
-                        numReceived.incrementAndGet();
+                        if (numReceived.incrementAndGet() == 2 * offsetLimitDelta)
+                            shutdownLatch.countDown();
                     }
&lt;p&gt;                 });&lt;/p&gt;

&lt;p&gt;+        kafkaStreams = new KafkaStreams(builder.build(), props);&lt;br/&gt;
+        kafkaStreams.setStateListener(new KafkaStreams.StateListener() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void onChange(final KafkaStreams.State newState, final KafkaStreams.State oldState) {&lt;br/&gt;
+                if (newState == KafkaStreams.State.RUNNING &amp;amp;&amp;amp; oldState == KafkaStreams.State.REBALANCING) &lt;/p&gt;
{
+                    startupLatch.countDown();
+                }
&lt;p&gt;+            }&lt;br/&gt;
+        });&lt;br/&gt;
+&lt;br/&gt;
+        final AtomicLong restored = new AtomicLong(0);&lt;br/&gt;
+        kafkaStreams.setGlobalStateRestoreListener(new StateRestoreListener() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void onRestoreStart(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset) &lt;/p&gt;
{
+
+            }&lt;br/&gt;
+&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void onBatchRestored(final TopicPartition topicPartition, final String storeName, final long batchEndOffset, final long numRestored) {++            }
&lt;p&gt;+&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) &lt;/p&gt;
{
+                restored.addAndGet(totalRestored);
+            }
&lt;p&gt;+        });&lt;br/&gt;
+        kafkaStreams.start();&lt;br/&gt;
+&lt;br/&gt;
+        assertTrue(startupLatch.await(30, TimeUnit.SECONDS));&lt;br/&gt;
+        assertThat(restored.get(), equalTo((long) numberOfKeys - offsetLimitDelta * 2 - offsetCheckpointed * 2));&lt;br/&gt;
+&lt;br/&gt;
+        assertTrue(shutdownLatch.await(30, TimeUnit.SECONDS));&lt;br/&gt;
+        assertThat(numReceived.get(), equalTo(offsetLimitDelta * 2));&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRestoreStateFromChangelogTopic() throws Exception {&lt;br/&gt;
+        final AtomicInteger numReceived = new AtomicInteger(0);&lt;br/&gt;
+        final StreamsBuilder builder = new StreamsBuilder();&lt;br/&gt;
+&lt;br/&gt;
+        final Properties props = props(APPID);&lt;br/&gt;
+&lt;br/&gt;
+        // restoring from 1000 to 5000, and then process from 5000 to 10000 on each of the two partitions&lt;br/&gt;
+        final int offsetCheckpointed = 1000;&lt;br/&gt;
+        createStateForRestoration(APPID + &quot;-store-changelog&quot;);&lt;br/&gt;
+        createStateForRestoration(INPUT_STREAM);&lt;br/&gt;
+&lt;br/&gt;
+        final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(props), new MockTime());&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 0)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(APPID + &quot;-store-changelog&quot;, 0), (long) offsetCheckpointed));&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 1)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(APPID + &quot;-store-changelog&quot;, 1), (long) offsetCheckpointed));&lt;/p&gt;

&lt;p&gt;         final CountDownLatch startupLatch = new CountDownLatch(1);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;kafkaStreams = new KafkaStreams(builder.build(), props(applicationId));&lt;br/&gt;
+        final CountDownLatch shutdownLatch = new CountDownLatch(1);&lt;br/&gt;
+&lt;br/&gt;
+        builder.table(INPUT_STREAM, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.as(&quot;store&quot;))&lt;br/&gt;
+                .toStream()&lt;br/&gt;
+                .foreach(new ForeachAction&amp;lt;Integer, Integer&amp;gt;() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                    @Override+                    public void apply(final Integer key, final Integer value) {
+                        if (numReceived.incrementAndGet() == numberOfKeys)
+                            shutdownLatch.countDown();
+                    }+                }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;br/&gt;
+&lt;br/&gt;
+        kafkaStreams = new KafkaStreams(builder.build(), props);&lt;br/&gt;
         kafkaStreams.setStateListener(new KafkaStreams.StateListener() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public void onChange(final KafkaStreams.State newState, final KafkaStreams.State oldState) &lt;/p&gt;
{
@@ -159,8 +246,10 @@ public void onRestoreEnd(final TopicPartition topicPartition, final String store
         kafkaStreams.start();
 
         assertTrue(startupLatch.await(30, TimeUnit.SECONDS));
-        assertThat(restored.get(), equalTo((long) numberOfKeys));
-        assertThat(numReceived.get(), equalTo(0));
+        assertThat(restored.get(), equalTo((long) numberOfKeys - 2 * offsetCheckpointed));
+
+        assertTrue(shutdownLatch.await(30, TimeUnit.SECONDS));
+        assertThat(numReceived.get(), equalTo(numberOfKeys));
     }&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;@@ -178,7 +267,7 @@ public Integer apply(final Integer value1, final Integer value2) {&lt;br/&gt;
                 }, Materialized.&amp;lt;Integer, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;reduce-store&quot;).withLoggingDisabled());&lt;/p&gt;

&lt;p&gt;         final CountDownLatch startupLatch = new CountDownLatch(1);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;kafkaStreams = new KafkaStreams(builder.build(), props(applicationId));&lt;br/&gt;
+        kafkaStreams = new KafkaStreams(builder.build(), props(APPID));&lt;br/&gt;
         kafkaStreams.setStateListener(new KafkaStreams.StateListener() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public void onChange(final KafkaStreams.State newState, final KafkaStreams.State oldState) {&lt;br/&gt;
@@ -228,7 +317,7 @@ public void shouldProcessDataFromStoresWithLoggingDisabled() throws InterruptedE&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Topology topology = streamsBuilder.build();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;kafkaStreams = new KafkaStreams(topology, props(applicationId + &quot;-logging-disabled&quot;));&lt;br/&gt;
+        kafkaStreams = new KafkaStreams(topology, props(APPID + &quot;-logging-disabled&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final CountDownLatch latch = new CountDownLatch(1);&lt;br/&gt;
         kafkaStreams.setStateListener(new KafkaStreams.StateListener() {&lt;br/&gt;
@@ -279,8 +368,7 @@ public void close() {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void createStateForRestoration()&lt;/li&gt;
	&lt;li&gt;throws ExecutionException, InterruptedException {&lt;br/&gt;
+    private void createStateForRestoration(final String changelogTopic) {&lt;br/&gt;
         final Properties producerConfig = new Properties();&lt;br/&gt;
         producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -288,30 +376,33 @@ private void createStateForRestoration()&lt;br/&gt;
                      new KafkaProducer&amp;lt;&amp;gt;(producerConfig, new IntegerSerializer(), new IntegerSerializer())) {&lt;/p&gt;

&lt;p&gt;             for (int i = 0; i &amp;lt; numberOfKeys; i++) &lt;/p&gt;
{
-                producer.send(new ProducerRecord&amp;lt;&amp;gt;(INPUT_STREAM, i, i));
+                producer.send(new ProducerRecord&amp;lt;&amp;gt;(changelogTopic, i, i));
             }
&lt;p&gt;         }&lt;br/&gt;
+    }&lt;/p&gt;

&lt;p&gt;+    private void setCommittedOffset(final String topic, final int limitDelta) {&lt;br/&gt;
         final Properties consumerConfig = new Properties();&lt;br/&gt;
         consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, applicationId);&lt;br/&gt;
+        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APPID);&lt;br/&gt;
+        consumerConfig.put(ConsumerConfig.CLIENT_ID_CONFIG, &quot;commit-consumer&quot;);&lt;br/&gt;
         consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);&lt;br/&gt;
         consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Consumer consumer = new KafkaConsumer(consumerConfig);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final List&amp;lt;TopicPartition&amp;gt; partitions = Arrays.asList(new TopicPartition(INPUT_STREAM, 0),&lt;/li&gt;
	&lt;li&gt;new TopicPartition(INPUT_STREAM, 1));&lt;br/&gt;
+        final List&amp;lt;TopicPartition&amp;gt; partitions = Arrays.asList(&lt;br/&gt;
+            new TopicPartition(topic, 0),&lt;br/&gt;
+            new TopicPartition(topic, 1));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         consumer.assign(partitions);&lt;br/&gt;
         consumer.seekToEnd(partitions);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         for (TopicPartition partition : partitions) 
{
             final long position = consumer.position(partition);
-            offsets.put(partition, new OffsetAndMetadata(position + 1));
+            consumer.seek(partition, position - limitDelta);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumer.commitSync(offsets);&lt;br/&gt;
+        consumer.commitSync();&lt;br/&gt;
         consumer.close();&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java&lt;br/&gt;
index b5e6fcb63b9..5fab6660c4e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java&lt;br/&gt;
@@ -55,8 +55,8 @@ public void prepareTopology() throws InterruptedException &lt;/p&gt;
{
         appID = &quot;table-table-join-integration-test&quot;;
 
         builder = new StreamsBuilder();
-        leftTable = builder.table(INPUT_TOPIC_LEFT);
-        rightTable = builder.table(INPUT_TOPIC_RIGHT);
+        leftTable = builder.table(INPUT_TOPIC_LEFT, Materialized.&amp;lt;Long, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;left&quot;).withLoggingDisabled());
+        rightTable = builder.table(INPUT_TOPIC_RIGHT, Materialized.&amp;lt;Long, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;right&quot;).withLoggingDisabled());
     }

&lt;p&gt;     final private String expectedFinalJoinResult = &quot;D-d&quot;;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java&lt;br/&gt;
index 63432ffc439..ef3fcd6110f 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java&lt;br/&gt;
@@ -137,7 +137,7 @@ public void shouldStillMaterializeSourceKTableIfMaterializedIsntQueryable() &lt;/p&gt;
{
         assertEquals(storeName, topology.stateStores().get(0).name());
 
         assertEquals(1, topology.storeToChangelogTopic().size());
-        assertEquals(&quot;topic2&quot;, topology.storeToChangelogTopic().get(storeName));
+        assertEquals(&quot;app-id-prefix-STATE-STORE-0000000000-changelog&quot;, topology.storeToChangelogTopic().get(storeName));
         assertNull(table1.queryableStoreName());
     }

&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
index 936c67b8ef8..97d605265db 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
@@ -21,7 +21,6 @@&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.InvalidOffsetException;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.MockConsumer;&lt;br/&gt;
-import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
 import org.apache.kafka.clients.producer.MockProducer;&lt;br/&gt;
 import org.apache.kafka.clients.producer.Producer;&lt;br/&gt;
 import org.apache.kafka.common.Cluster;&lt;br/&gt;
@@ -59,6 +58,7 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.ThreadMetadata;&lt;br/&gt;
 import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
+import org.apache.kafka.streams.state.internals.OffsetCheckpoint;&lt;br/&gt;
 import org.apache.kafka.test.MockClientSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockStateRestoreListener;&lt;br/&gt;
 import org.apache.kafka.test.MockTimestampExtractor;&lt;br/&gt;
@@ -69,6 +69,8 @@&lt;br/&gt;
 import org.junit.Before;&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt;+import java.io.File;&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
@@ -84,6 +86,7 @@&lt;br/&gt;
 import static org.apache.kafka.common.utils.Utils.mkEntry;&lt;br/&gt;
 import static org.apache.kafka.common.utils.Utils.mkMap;&lt;br/&gt;
 import static org.apache.kafka.common.utils.Utils.mkProperties;&lt;br/&gt;
+import static org.apache.kafka.streams.processor.internals.AbstractStateManager.CHECKPOINT_FILE_NAME;&lt;br/&gt;
 import static org.hamcrest.CoreMatchers.equalTo;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertFalse;&lt;br/&gt;
@@ -821,12 +824,13 @@ public void shouldReturnStandbyTaskMetadataWhileRunningState() {&lt;/p&gt;

&lt;p&gt;     @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldUpdateStandbyTask() {&lt;br/&gt;
+    public void shouldUpdateStandbyTask() throws IOException {&lt;br/&gt;
         final String storeName1 = &quot;count-one&quot;;&lt;br/&gt;
         final String storeName2 = &quot;table-two&quot;;&lt;/li&gt;
	&lt;li&gt;final String changelogName = applicationId + &quot;-&quot; + storeName1 + &quot;-changelog&quot;;&lt;/li&gt;
	&lt;li&gt;final TopicPartition partition1 = new TopicPartition(changelogName, 1);&lt;/li&gt;
	&lt;li&gt;final TopicPartition partition2 = t2p1;&lt;br/&gt;
+        final String changelogName1 = applicationId + &quot;-&quot; + storeName1 + &quot;-changelog&quot;;&lt;br/&gt;
+        final String changelogName2 = applicationId + &quot;-&quot; + storeName2 + &quot;-changelog&quot;;&lt;br/&gt;
+        final TopicPartition partition1 = new TopicPartition(changelogName1, 1);&lt;br/&gt;
+        final TopicPartition partition2 = new TopicPartition(changelogName2, 1);&lt;br/&gt;
         internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)&lt;br/&gt;
             .groupByKey().count(Materialized.&amp;lt;Object, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(storeName1));&lt;br/&gt;
         final MaterializedInternal materialized = new MaterializedInternal(Materialized.as(storeName2));&lt;br/&gt;
@@ -835,10 +839,10 @@ public void shouldUpdateStandbyTask() {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final StreamThread thread = createStreamThread(clientId, config, false);&lt;br/&gt;
         final MockConsumer&amp;lt;byte[], byte[]&amp;gt; restoreConsumer = clientSupplier.restoreConsumer;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;restoreConsumer.updatePartitions(changelogName,&lt;br/&gt;
+        restoreConsumer.updatePartitions(changelogName1,&lt;br/&gt;
             singletonList(&lt;br/&gt;
                 new PartitionInfo(&lt;/li&gt;
	&lt;li&gt;changelogName,&lt;br/&gt;
+                    changelogName1,&lt;br/&gt;
                     1,&lt;br/&gt;
                     null,&lt;br/&gt;
                     new Node&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;,&lt;br/&gt;
@@ -852,13 +856,13 @@ public void shouldUpdateStandbyTask() {&lt;br/&gt;
         restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition1, 0L));&lt;br/&gt;
         restoreConsumer.updateEndOffsets(Collections.singletonMap(partition2, 10L));&lt;br/&gt;
         restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition2, 0L));&lt;/li&gt;
	&lt;li&gt;// let the store1 be restored from 0 to 10; store2 be restored from 0 to (committed offset) 5&lt;/li&gt;
	&lt;li&gt;clientSupplier.consumer.assign(Utils.mkSet(partition2));&lt;/li&gt;
	&lt;li&gt;clientSupplier.consumer.commitSync(Collections.singletonMap(partition2, new OffsetAndMetadata(5L, &quot;&quot;)));&lt;br/&gt;
+        // let the store1 be restored from 0 to 10; store2 be restored from 5 (checkpointed) to 10&lt;br/&gt;
+        OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(task3), CHECKPOINT_FILE_NAME));&lt;br/&gt;
+        checkpoint.write(Collections.singletonMap(partition2, 5L));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         for (long i = 0L; i &amp;lt; 10L; i++) &lt;/p&gt;
{
-            restoreConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(changelogName, 1, i, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));
-            restoreConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(topic2, 1, i, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));
+            restoreConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(changelogName1, 1, i, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));
+            restoreConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(changelogName2, 1, i, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));
         }

&lt;p&gt;         thread.setState(StreamThread.State.RUNNING);&lt;br/&gt;
@@ -884,9 +888,7 @@ public void shouldUpdateStandbyTask() &lt;/p&gt;
{
 
         assertEquals(10L, store1.approximateNumEntries());
         assertEquals(5L, store2.approximateNumEntries());
-        assertEquals(Collections.singleton(partition2), restoreConsumer.paused());
-        assertEquals(1, thread.standbyRecords().size());
-        assertEquals(5, thread.standbyRecords().get(partition2).size());
+        assertEquals(0, thread.standbyRecords().size());
     }

&lt;p&gt;     @Test&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
index a32d193a171..4327e8f1ee4 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java&lt;br/&gt;
@@ -799,6 +799,7 @@ public Object apply(final Object value1, final Object value2) {&lt;br/&gt;
         final Map&amp;lt;String, Integer&amp;gt; expectedCreatedInternalTopics = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         expectedCreatedInternalTopics.put(applicationId + &quot;-KTABLE-AGGREGATE-STATE-STORE-0000000006-repartition&quot;, 4);&lt;br/&gt;
         expectedCreatedInternalTopics.put(applicationId + &quot;-KTABLE-AGGREGATE-STATE-STORE-0000000006-changelog&quot;, 4);&lt;br/&gt;
+        expectedCreatedInternalTopics.put(applicationId + &quot;-topic3-STATE-STORE-0000000002-changelog&quot;, 4);&lt;br/&gt;
         expectedCreatedInternalTopics.put(applicationId + &quot;-KSTREAM-MAP-0000000001-repartition&quot;, 4);&lt;/p&gt;

&lt;p&gt;         // check if all internal topics were created as expected&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16508982" author="githubbot" created="Tue, 12 Jun 2018 00:07:34 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5195: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7021&quot; title=&quot;Source KTable checkpoint is not correct&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7021&quot;&gt;&lt;del&gt;KAFKA-7021&lt;/del&gt;&lt;/a&gt;: Update upgrade guide section for reusing source topic&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5195&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5195&lt;/a&gt;&lt;/p&gt;




&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16512813" author="githubbot" created="Thu, 14 Jun 2018 17:42:35 +0000"  >&lt;p&gt;guozhangwang closed pull request #5207: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7021&quot; title=&quot;Source KTable checkpoint is not correct&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7021&quot;&gt;&lt;del&gt;KAFKA-7021&lt;/del&gt;&lt;/a&gt;: checkpoint offsets from committed&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5207&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5207&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java&lt;br/&gt;
index 0b028e67381..0c611199d88 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java&lt;br/&gt;
@@ -107,7 +107,6 @@ public InternalStreamsBuilder(final InternalTopologyBuilder internalTopologyBuil&lt;br/&gt;
                                                  name);&lt;/p&gt;

&lt;p&gt;         internalTopologyBuilder.addStateStore(storeBuilder, name);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;internalTopologyBuilder.connectSourceStoreAndTopic(storeBuilder.name(), topic);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return kTable;&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index d9c827fff52..bf6ceded143 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -164,7 +164,7 @@ public String toString(final String indent) &lt;/p&gt;
{
         return sb.toString();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected Map&amp;lt;TopicPartition, Long&amp;gt; recordCollectorOffsets() {&lt;br/&gt;
+    protected Map&amp;lt;TopicPartition, Long&amp;gt; activeTaskCheckpointableOffsets() 
{
         return Collections.emptyMap();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -239,7 +239,7 @@ void closeStateManager(final boolean writeCheckpoint) throws ProcessorStateExcep&lt;br/&gt;
         ProcessorStateException exception = null;&lt;br/&gt;
         log.trace(&quot;Closing state manager&quot;);&lt;br/&gt;
         try &lt;/p&gt;
{
-            stateMgr.close(writeCheckpoint ? recordCollectorOffsets() : null);
+            stateMgr.close(writeCheckpoint ? activeTaskCheckpointableOffsets() : null);
         }
&lt;p&gt; catch (final ProcessorStateException e) &lt;/p&gt;
{
             exception = e;
         }
&lt;p&gt; finally {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java&lt;br/&gt;
index 7d4b592b6a6..a9d5a93f228 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java&lt;br/&gt;
@@ -122,7 +122,7 @@&lt;/p&gt;

&lt;p&gt;     private Map&amp;lt;Integer, Set&amp;lt;String&amp;gt;&amp;gt; nodeGroups = null;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;interface StateStoreFactory 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    public interface StateStoreFactory {
         Set&amp;lt;String&amp;gt; users();
         boolean loggingEnabled();
         StateStore build();
@@ -1883,4 +1883,10 @@ public void updateSubscribedTopics(final Set&amp;lt;String&amp;gt; topics, final String logPre
         subscriptionUpdates.updateTopics(topics);
         updateSubscriptions(subscriptionUpdates, logPrefix);
     }++    // following functions are for test only++    public synchronized Map&amp;lt;String, StateStoreFactory&amp;gt; getStateStores() {
+        return stateFactories;
+    } }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
index 195ea99f62d..b0761ac5507 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java&lt;br/&gt;
@@ -46,7 +46,7 @@&lt;br/&gt;
     private final boolean isStandby;&lt;br/&gt;
     private final ChangelogReader changelogReader;&lt;br/&gt;
     private final Map&amp;lt;TopicPartition, Long&amp;gt; offsetLimits;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;private final Map&amp;lt;TopicPartition, Long&amp;gt; restoredOffsets;&lt;br/&gt;
+    private final Map&amp;lt;TopicPartition, Long&amp;gt; standbyRestoredOffsets;&lt;br/&gt;
     private final Map&amp;lt;String, StateRestoreCallback&amp;gt; restoreCallbacks; // used for standby tasks, keyed by state topic name&lt;br/&gt;
     private final Map&amp;lt;String, String&amp;gt; storeToChangelogTopic;&lt;br/&gt;
     private final List&amp;lt;TopicPartition&amp;gt; changelogPartitions = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -79,9 +79,9 @@ public ProcessorStateManager(final TaskId taskId,&lt;br/&gt;
             partitionForTopic.put(source.topic(), source);&lt;br/&gt;
         }&lt;br/&gt;
         offsetLimits = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;restoredOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+        standbyRestoredOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
         this.isStandby = isStandby;&lt;/li&gt;
	&lt;li&gt;restoreCallbacks = isStandby ? new HashMap&amp;lt;&amp;gt;() : null;&lt;br/&gt;
+        restoreCallbacks = isStandby ? new HashMap&amp;lt;String, StateRestoreCallback&amp;gt;() : null;&lt;br/&gt;
         this.storeToChangelogTopic = storeToChangelogTopic;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // load the checkpoint information&lt;br/&gt;
@@ -168,7 +168,11 @@ public void reinitializeStateStoresForPartitions(final Collection&amp;lt;TopicPartition&lt;br/&gt;
             final int partition = getPartition(topicName);&lt;br/&gt;
             final TopicPartition storePartition = new TopicPartition(topicName, partition);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;partitionsAndOffsets.put(storePartition, checkpointableOffsets.getOrDefault(storePartition, -1L));&lt;br/&gt;
+            if (checkpointableOffsets.containsKey(storePartition)) 
{
+                partitionsAndOffsets.put(storePartition, checkpointableOffsets.get(storePartition));
+            }
&lt;p&gt; else &lt;/p&gt;
{
+                partitionsAndOffsets.put(storePartition, -1L);
+            }
&lt;p&gt;         }&lt;br/&gt;
         return partitionsAndOffsets;&lt;br/&gt;
     }&lt;br/&gt;
@@ -207,7 +211,7 @@ public void reinitializeStateStoresForPartitions(final Collection&amp;lt;TopicPartition&lt;br/&gt;
         }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // record the restored offset for its change log partition&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;restoredOffsets.put(storePartition, lastOffset + 1);&lt;br/&gt;
+        standbyRestoredOffsets.put(storePartition, lastOffset + 1);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         return remainingRecords;&lt;br/&gt;
     }&lt;br/&gt;
@@ -288,8 +292,8 @@ public void close(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) throws Processor&lt;/p&gt;

&lt;p&gt;     // write the checkpoint&lt;br/&gt;
     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) {&lt;/li&gt;
	&lt;li&gt;checkpointableOffsets.putAll(changelogReader.restoredOffsets());&lt;br/&gt;
+    public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; checkpointableOffsets) {&lt;br/&gt;
+        this.checkpointableOffsets.putAll(changelogReader.restoredOffsets());&lt;br/&gt;
         for (final StateStore store : stores.values()) {&lt;br/&gt;
             final String storeName = store.name();&lt;br/&gt;
             // only checkpoint the offset to the offsets file if&lt;br/&gt;
@@ -297,11 +301,11 @@ public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) {&lt;br/&gt;
             if (store.persistent() &amp;amp;&amp;amp; storeToChangelogTopic.containsKey(storeName)) {&lt;br/&gt;
                 final String changelogTopic = storeToChangelogTopic.get(storeName);&lt;br/&gt;
                 final TopicPartition topicPartition = new TopicPartition(changelogTopic, getPartition(storeName));&lt;/li&gt;
	&lt;li&gt;if (ackedOffsets.containsKey(topicPartition)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                if (checkpointableOffsets.containsKey(topicPartition)) {
                     // store the last offset + 1 (the log position after restoration)
-                    checkpointableOffsets.put(topicPartition, ackedOffsets.get(topicPartition) + 1);
-                } else if (restoredOffsets.containsKey(topicPartition)) {
-                    checkpointableOffsets.put(topicPartition, restoredOffsets.get(topicPartition));
+                    this.checkpointableOffsets.put(topicPartition, checkpointableOffsets.get(topicPartition) + 1);
+                } else if (standbyRestoredOffsets.containsKey(topicPartition)) {
+                    this.checkpointableOffsets.put(topicPartition, standbyRestoredOffsets.get(topicPartition));
                 }             }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;         }&lt;br/&gt;
@@ -310,9 +314,9 @@ public void checkpoint(final Map&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets) &lt;/p&gt;
{
             checkpoint = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;log.trace(&quot;Writing checkpoint: {}&quot;, checkpointableOffsets);&lt;br/&gt;
+        log.trace(&quot;Writing checkpoint: {}&quot;, this.checkpointableOffsets);&lt;br/&gt;
         try 
{
-            checkpoint.write(checkpointableOffsets);
+            checkpoint.write(this.checkpointableOffsets);
         }
&lt;p&gt; catch (final IOException e) {&lt;br/&gt;
             log.warn(&quot;Failed to write offset checkpoint file to {}: {}&quot;, checkpoint, e);&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
index c33ade6f36e..7623c66cd3b 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
@@ -91,7 +91,7 @@ public StateDirectory(final StreamsConfig config,&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@return directory for the 
{@link TaskId}&lt;/li&gt;
	&lt;li&gt;@throws ProcessorStateException if the task directory does not exists and could not be created&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;File directoryForTask(final TaskId taskId) {&lt;br/&gt;
+    public File directoryForTask(final TaskId taskId) {&lt;br/&gt;
         final File taskDir = new File(stateDir, taskId.toString());&lt;br/&gt;
         if (!taskDir.exists() &amp;amp;&amp;amp; !taskDir.mkdir()) {&lt;br/&gt;
             throw new ProcessorStateException(&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index 21a4750f954..1f3f0b57a6a 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -311,7 +311,7 @@ void commit(final boolean startNewTransaction) {&lt;br/&gt;
                 public void run() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                     flushState();                     if (!eosEnabled) {
-                        stateMgr.checkpoint(recordCollectorOffsets());
+                        stateMgr.checkpoint(activeTaskCheckpointableOffsets());
                     }                     commitOffsets(startNewTransaction);                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -322,8 +322,15 @@ public void run() {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected Map&amp;lt;TopicPartition, Long&amp;gt; recordCollectorOffsets() {&lt;/li&gt;
	&lt;li&gt;return recordCollector.offsets();&lt;br/&gt;
+    protected Map&amp;lt;TopicPartition, Long&amp;gt; activeTaskCheckpointableOffsets() {&lt;br/&gt;
+        final Map&amp;lt;TopicPartition, Long&amp;gt; checkpointableOffsets = recordCollector.offsets();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            if (!checkpointableOffsets.containsKey(entry.getKey())) {
+                checkpointableOffsets.put(entry.getKey(), entry.getValue());
+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+        return checkpointableOffsets;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
index 2ba66a5b1bd..76190a00e2e 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java&lt;br/&gt;
@@ -1186,4 +1186,8 @@ public String toString(final String indent) {&lt;br/&gt;
     TaskManager taskManager() &lt;/p&gt;
{
         return taskManager;
     }
&lt;p&gt;+&lt;br/&gt;
+    Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; standbyRecords() &lt;/p&gt;
{
+        return standbyRecords;
+    }
&lt;p&gt; }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java&lt;br/&gt;
index 4a496b8dd28..e7ce819677b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java&lt;br/&gt;
@@ -293,7 +293,26 @@ public void shouldUseDefaultNodeAndStoreNames() &lt;/p&gt;
{
         assertFalse(stores.hasNext());
         assertFalse(subtopologies.hasNext());
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;p&gt;+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldNotReuseSourceTopicAsChangelogsByDefault() &lt;/p&gt;
{
+        final String topic = &quot;topic&quot;;
+        builder.table(topic, Materialized.&amp;lt;Long, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;store&quot;));
+
+        final InternalTopologyBuilder internalTopologyBuilder = InternalTopologyAccessor.getInternalTopologyBuilder(builder.build());
+        internalTopologyBuilder.setApplicationId(&quot;appId&quot;);
+
+        assertThat(internalTopologyBuilder.build().storeToChangelogTopic(), equalTo(Collections.singletonMap(&quot;store&quot;, &quot;appId-store-changelog&quot;)));
+
+        assertThat(internalTopologyBuilder.getStateStores().keySet(), equalTo(Collections.singleton(&quot;store&quot;)));
+
+
+        assertThat(internalTopologyBuilder.getStateStores().get(&quot;store&quot;).loggingEnabled(), equalTo(true));
+
+        assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.keySet(), equalTo(Collections.singleton(&quot;appId-store-changelog&quot;)));
+    }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
     @Test(expected = TopologyException.class)&lt;br/&gt;
     public void shouldThrowExceptionWhenNoTopicPresent() throws Exception {&lt;br/&gt;
         builder.stream(Collections.&amp;lt;String&amp;gt;emptyList());&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
index 19ddedfdc94..9d554c5425c 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
@@ -16,18 +16,15 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.kafka.streams.integration;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import org.apache.kafka.clients.consumer.Consumer;&lt;br/&gt;
 import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
-import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
-import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
 import org.apache.kafka.clients.producer.KafkaProducer;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;
 import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
 import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
-import org.apache.kafka.common.serialization.IntegerDeserializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.IntegerSerializer;&lt;br/&gt;
 import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
 import org.apache.kafka.common.utils.Bytes;&lt;br/&gt;
+import org.apache.kafka.common.utils.MockTime;&lt;br/&gt;
 import org.apache.kafka.streams.Consumed;&lt;br/&gt;
 import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
 import org.apache.kafka.streams.KeyValue;&lt;br/&gt;
@@ -44,11 +41,14 @@&lt;br/&gt;
 import org.apache.kafka.streams.processor.ProcessorContext;&lt;br/&gt;
 import org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;
 import org.apache.kafka.streams.processor.StateRestoreListener;&lt;br/&gt;
+import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StateDirectory;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
 import org.apache.kafka.streams.state.StoreBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.state.Stores;&lt;br/&gt;
 import org.apache.kafka.streams.state.internals.KeyValueStoreBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.state.internals.OffsetCheckpoint;&lt;br/&gt;
 import org.apache.kafka.test.IntegrationTest;&lt;br/&gt;
 import org.apache.kafka.test.TestUtils;&lt;br/&gt;
 import org.junit.After;&lt;br/&gt;
@@ -57,11 +57,10 @@&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 import org.junit.experimental.categories.Category;&lt;/p&gt;

&lt;p&gt;+import java.io.File;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
-import java.util.HashMap;&lt;br/&gt;
-import java.util.List;&lt;br/&gt;
-import java.util.Map;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
 import java.util.Properties;&lt;br/&gt;
 import java.util.concurrent.CountDownLatch;&lt;br/&gt;
 import java.util.concurrent.ExecutionException;&lt;br/&gt;
@@ -76,32 +75,33 @@&lt;br/&gt;
 @Category(&lt;/p&gt;
{IntegrationTest.class}
&lt;p&gt;)&lt;br/&gt;
 public class RestoreIntegrationTest {&lt;br/&gt;
     private static final int NUM_BROKERS = 1;&lt;br/&gt;
+    private static final String APPID = &quot;restore-test&quot;;&lt;/p&gt;

&lt;p&gt;     @ClassRule&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static final EmbeddedKafkaCluster CLUSTER =&lt;/li&gt;
	&lt;li&gt;new EmbeddedKafkaCluster(NUM_BROKERS);&lt;br/&gt;
+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(NUM_BROKERS);&lt;br/&gt;
     private static final String INPUT_STREAM = &quot;input-stream&quot;;&lt;br/&gt;
     private static final String INPUT_STREAM_2 = &quot;input-stream-2&quot;;&lt;br/&gt;
     private final int numberOfKeys = 10000;&lt;br/&gt;
     private KafkaStreams kafkaStreams;&lt;/li&gt;
	&lt;li&gt;private String applicationId = &quot;restore-test&quot;;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;     @BeforeClass&lt;br/&gt;
     public static void createTopics() throws InterruptedException &lt;/p&gt;
{
         CLUSTER.createTopic(INPUT_STREAM, 2, 1);
         CLUSTER.createTopic(INPUT_STREAM_2, 2, 1);
+        CLUSTER.createTopic(APPID + &quot;-store-changelog&quot;, 2, 1);
     }

&lt;p&gt;     private Properties props(final String applicationId) &lt;/p&gt;
{
         Properties streamsConfiguration = new Properties();
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
-        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
+        streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
         streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(applicationId).getPath());
         streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());
         streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());
         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
         return streamsConfiguration;
     }

&lt;p&gt;@@ -112,26 +112,38 @@ public void shutdown() throws IOException {&lt;br/&gt;
         }&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;-&lt;br/&gt;
     @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void shouldRestoreState() throws ExecutionException, InterruptedException {&lt;br/&gt;
+    public void shouldRestoreStateFromChangelogTopic() throws Exception {&lt;br/&gt;
         final AtomicInteger numReceived = new AtomicInteger(0);&lt;br/&gt;
         final StreamsBuilder builder = new StreamsBuilder();&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;createStateForRestoration();&lt;br/&gt;
+        final Properties props = props(APPID);&lt;br/&gt;
+&lt;br/&gt;
+        // restoring from 1000 to 5000, and then process from 5000 to 10000 on each of the two partitions&lt;br/&gt;
+        final int offsetCheckpointed = 1000;&lt;br/&gt;
+        createStateForRestoration(APPID + &quot;-store-changelog&quot;);&lt;br/&gt;
+        createStateForRestoration(INPUT_STREAM);&lt;br/&gt;
+&lt;br/&gt;
+        final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(props), new MockTime());&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 0)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(APPID + &quot;-store-changelog&quot;, 0), (long) offsetCheckpointed));&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 1)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(APPID + &quot;-store-changelog&quot;, 1), (long) offsetCheckpointed));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;builder.table(INPUT_STREAM, Consumed.with(Serdes.Integer(), Serdes.Integer()))&lt;br/&gt;
+        final CountDownLatch startupLatch = new CountDownLatch(1);&lt;br/&gt;
+        final CountDownLatch shutdownLatch = new CountDownLatch(1);&lt;br/&gt;
+&lt;br/&gt;
+        builder.table(INPUT_STREAM, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.&amp;lt;Integer, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;store&quot;))&lt;br/&gt;
                 .toStream()&lt;br/&gt;
                 .foreach(new ForeachAction&amp;lt;Integer, Integer&amp;gt;() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                     @Override                     public void apply(final Integer key, final Integer value) {
-                        numReceived.incrementAndGet();
+                        if (numReceived.incrementAndGet() == numberOfKeys)
+                            shutdownLatch.countDown();
                     }                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final CountDownLatch startupLatch = new CountDownLatch(1);&lt;/li&gt;
	&lt;li&gt;kafkaStreams = new KafkaStreams(builder.build(), props(applicationId));&lt;br/&gt;
+        kafkaStreams = new KafkaStreams(builder.build(), props);&lt;br/&gt;
         kafkaStreams.setStateListener(new KafkaStreams.StateListener() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public void onChange(final KafkaStreams.State newState, final KafkaStreams.State oldState) 
{
@@ -161,10 +173,11 @@ public void onRestoreEnd(final TopicPartition topicPartition, final String store
         kafkaStreams.start();
 
         assertTrue(startupLatch.await(30, TimeUnit.SECONDS));
-        assertThat(restored.get(), equalTo((long) numberOfKeys));
-        assertThat(numReceived.get(), equalTo(0));
-    }
&lt;p&gt;+        assertThat(restored.get(), equalTo((long) numberOfKeys - 2 * offsetCheckpointed));&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+        assertTrue(shutdownLatch.await(30, TimeUnit.SECONDS));&lt;br/&gt;
+        assertThat(numReceived.get(), equalTo(numberOfKeys));&lt;br/&gt;
+    }&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void shouldSuccessfullyStartWhenLoggingDisabled() throws InterruptedException {&lt;br/&gt;
@@ -180,7 +193,7 @@ public Integer apply(final Integer value1, final Integer value2) {&lt;br/&gt;
                 }, Materialized.&amp;lt;Integer, Integer, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;reduce-store&quot;).withLoggingDisabled());&lt;/p&gt;

&lt;p&gt;         final CountDownLatch startupLatch = new CountDownLatch(1);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;kafkaStreams = new KafkaStreams(builder.build(), props(applicationId));&lt;br/&gt;
+        kafkaStreams = new KafkaStreams(builder.build(), props(APPID));&lt;br/&gt;
         kafkaStreams.setStateListener(new KafkaStreams.StateListener() {&lt;br/&gt;
             @Override&lt;br/&gt;
             public void onChange(final KafkaStreams.State newState, final KafkaStreams.State oldState) {&lt;br/&gt;
@@ -230,7 +243,7 @@ public void shouldProcessDataFromStoresWithLoggingDisabled() throws InterruptedE&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Topology topology = streamsBuilder.build();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;kafkaStreams = new KafkaStreams(topology, props(applicationId + &quot;-logging-disabled&quot;));&lt;br/&gt;
+        kafkaStreams = new KafkaStreams(topology, props(APPID + &quot;-logging-disabled&quot;));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final CountDownLatch latch = new CountDownLatch(1);&lt;br/&gt;
         kafkaStreams.setStateListener(new KafkaStreams.StateListener() &lt;/p&gt;
{
@@ -250,6 +263,7 @@ public void onChange(final KafkaStreams.State newState, final KafkaStreams.State
     }


&lt;p&gt;+&lt;br/&gt;
     public static class KeyValueStoreProcessor implements Processor&amp;lt;Integer, Integer&amp;gt; {&lt;/p&gt;

&lt;p&gt;         private String topic;&lt;br/&gt;
@@ -285,9 +299,8 @@ public void close() {&lt;/p&gt;

&lt;p&gt;         }&lt;br/&gt;
     }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;private void createStateForRestoration()&lt;/li&gt;
	&lt;li&gt;throws ExecutionException, InterruptedException {&lt;br/&gt;
+&lt;br/&gt;
+    private void createStateForRestoration(final String changelogTopic) {&lt;br/&gt;
         final Properties producerConfig = new Properties();&lt;br/&gt;
         producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -295,31 +308,8 @@ private void createStateForRestoration()&lt;br/&gt;
                      new KafkaProducer&amp;lt;&amp;gt;(producerConfig, new IntegerSerializer(), new IntegerSerializer())) {&lt;/p&gt;

&lt;p&gt;             for (int i = 0; i &amp;lt; numberOfKeys; i++) &lt;/p&gt;
{
-                producer.send(new ProducerRecord&amp;lt;&amp;gt;(INPUT_STREAM, i, i));
+                producer.send(new ProducerRecord&amp;lt;&amp;gt;(changelogTopic, i, i));
             }
&lt;p&gt;         }&lt;br/&gt;
-&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final Properties consumerConfig = new Properties();&lt;/li&gt;
	&lt;li&gt;consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;/li&gt;
	&lt;li&gt;consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, applicationId);&lt;/li&gt;
	&lt;li&gt;consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);&lt;/li&gt;
	&lt;li&gt;consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final Consumer consumer = new KafkaConsumer(consumerConfig);&lt;/li&gt;
	&lt;li&gt;final List&amp;lt;TopicPartition&amp;gt; partitions = Arrays.asList(new TopicPartition(INPUT_STREAM, 0),&lt;/li&gt;
	&lt;li&gt;new TopicPartition(INPUT_STREAM, 1));&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;consumer.assign(partitions);&lt;/li&gt;
	&lt;li&gt;consumer.seekToEnd(partitions);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets = new HashMap&amp;lt;&amp;gt;();&lt;/li&gt;
	&lt;li&gt;for (TopicPartition partition : partitions) 
{
-            final long position = consumer.position(partition);
-            offsets.put(partition, new OffsetAndMetadata(position + 1));
-        }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;consumer.commitSync(offsets);&lt;/li&gt;
	&lt;li&gt;consumer.close();&lt;br/&gt;
     }&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java&lt;br/&gt;
index b5e6fcb63b9..5fab6660c4e 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java&lt;br/&gt;
@@ -55,8 +55,8 @@ public void prepareTopology() throws InterruptedException 
{
         appID = &quot;table-table-join-integration-test&quot;;
 
         builder = new StreamsBuilder();
-        leftTable = builder.table(INPUT_TOPIC_LEFT);
-        rightTable = builder.table(INPUT_TOPIC_RIGHT);
+        leftTable = builder.table(INPUT_TOPIC_LEFT, Materialized.&amp;lt;Long, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;left&quot;).withLoggingDisabled());
+        rightTable = builder.table(INPUT_TOPIC_RIGHT, Materialized.&amp;lt;Long, String, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(&quot;right&quot;).withLoggingDisabled());
     }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     final private String expectedFinalJoinResult = &quot;D-d&quot;;&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java&lt;br/&gt;
index b9ba6089497..61a45f2898a 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java&lt;br/&gt;
@@ -153,7 +153,7 @@ public void shouldStillMaterializeSourceKTableIfMaterializedIsntQueryable() thro&lt;br/&gt;
         assertEquals(storeName, topology.stateStores().get(0).name());&lt;/p&gt;

&lt;p&gt;         assertEquals(1, topology.storeToChangelogTopic().size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(&quot;topic2&quot;, topology.storeToChangelogTopic().get(storeName));&lt;br/&gt;
+        assertEquals(&quot;app-id-prefix-STATE-STORE-0000000000-changelog&quot;, topology.storeToChangelogTopic().get(storeName));&lt;br/&gt;
         assertNull(table1.queryableStoreName());&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java&lt;br/&gt;
index 4df2bd1c6a1..2373e4c69c5 100644&lt;br/&gt;
&amp;#8212; a/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java&lt;br/&gt;
@@ -123,7 +123,7 @@ public void shouldRestoreStoreWithBatchingRestoreSpecification() throws Exceptio&lt;br/&gt;
             assertThat(batchingRestoreCallback.getRestoredRecords().size(), is(1));&lt;br/&gt;
             assertTrue(batchingRestoreCallback.getRestoredRecords().contains(expectedKeyValue));&lt;br/&gt;
         } finally &lt;/p&gt;
{
-            stateMgr.close(Collections.emptyMap());
+            stateMgr.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -141,7 +141,7 @@ public void shouldRestoreStoreWithSinglePutRestoreSpecification() throws Excepti&lt;br/&gt;
             assertThat(persistentStore.keys.size(), is(1));&lt;br/&gt;
             assertTrue(persistentStore.keys.contains(intKey));&lt;br/&gt;
         } finally {-            stateMgr.close(Collections.emptyMap());+            stateMgr.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;@@ -169,7 +169,7 @@ public void testRegisterPersistentStore() throws IOException &lt;/p&gt;
{
             stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);
             assertTrue(changelogReader.wasRegistered(new TopicPartition(persistentStoreTopicName, 2)));
         }
&lt;p&gt; finally &lt;/p&gt;
{
-            stateMgr.close(Collections.emptyMap());
+            stateMgr.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -196,7 +196,7 @@ public void testRegisterNonPersistentStore() throws IOException {
             stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);
             assertTrue(changelogReader.wasRegistered(new TopicPartition(nonPersistentStoreTopicName, 2)));
         } finally {-            stateMgr.close(Collections.emptyMap());+            stateMgr.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;@@ -257,7 +257,7 @@ public void testChangeLogOffsets() throws IOException &lt;/p&gt;
{
             assertEquals(-1L, (long) changeLogOffsets.get(partition3));
 
         }
&lt;p&gt; finally &lt;/p&gt;
{
-            stateMgr.close(Collections.emptyMap());
+            stateMgr.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());
         }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -269,7 +269,7 @@ public void testGetStore() throws IOException {&lt;br/&gt;
             noPartitions,&lt;br/&gt;
             false,&lt;br/&gt;
             stateDirectory,&lt;br/&gt;
-            Collections.emptyMap(),&lt;br/&gt;
+            Collections.&amp;lt;String, String&amp;gt;emptyMap(),&lt;br/&gt;
             changelogReader,&lt;br/&gt;
             false,&lt;br/&gt;
             logContext);&lt;br/&gt;
@@ -280,13 +280,13 @@ public void testGetStore() throws IOException {
             assertEquals(mockStateStore, stateMgr.getStore(nonPersistentStoreName));
 
         } finally {-            stateMgr.close(Collections.emptyMap());+            stateMgr.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());         }
&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;     @Test&lt;br/&gt;
     public void testFlushAndClose() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;checkpoint.write(Collections.emptyMap());&lt;br/&gt;
+        checkpoint.write(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         // set up ack&apos;ed offsets&lt;br/&gt;
         final HashMap&amp;lt;TopicPartition, Long&amp;gt; ackedOffsets = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
@@ -339,7 +339,7 @@ public void shouldRegisterStoreWithoutLoggingEnabledAndNotBackedByATopic() throw&lt;br/&gt;
             noPartitions,&lt;br/&gt;
             false,&lt;br/&gt;
             stateDirectory,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collections.emptyMap(),&lt;br/&gt;
+            Collections.&amp;lt;String, String&amp;gt;emptyMap(),&lt;br/&gt;
             changelogReader,&lt;br/&gt;
             false,&lt;br/&gt;
             logContext);&lt;br/&gt;
@@ -358,7 +358,7 @@ public void shouldNotChangeOffsetsIfAckedOffsetsIsNull() throws IOException {&lt;br/&gt;
             noPartitions,&lt;br/&gt;
             false,&lt;br/&gt;
             stateDirectory,&lt;/li&gt;
	&lt;li&gt;Collections.emptyMap(),&lt;br/&gt;
+            Collections.&amp;lt;String, String&amp;gt;emptyMap(),&lt;br/&gt;
             changelogReader,&lt;br/&gt;
             false,&lt;br/&gt;
             logContext);&lt;br/&gt;
@@ -408,7 +408,7 @@ public void shouldWriteCheckpointForStandbyReplica() throws IOException {&lt;br/&gt;
                                                                   bytes,&lt;br/&gt;
                                                                   bytes)));&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stateMgr.checkpoint(Collections.emptyMap());&lt;br/&gt;
+        stateMgr.checkpoint(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Map&amp;lt;TopicPartition, Long&amp;gt; read = checkpoint.read();&lt;br/&gt;
         assertThat(read, equalTo(Collections.singletonMap(persistentStorePartition, 889L)));&lt;br/&gt;
@@ -433,7 +433,7 @@ public void shouldNotWriteCheckpointForNonPersistent() throws IOException &lt;/p&gt;
{
         stateMgr.checkpoint(Collections.singletonMap(topicPartition, 876L));
 
         final Map&amp;lt;TopicPartition, Long&amp;gt; read = checkpoint.read();
-        assertThat(read, equalTo(Collections.emptyMap()));
+        assertThat(read, equalTo(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap()));
     }

&lt;p&gt;     @Test&lt;br/&gt;
@@ -443,7 +443,7 @@ public void shouldNotWriteCheckpointForStoresWithoutChangelogTopic() throws IOEx&lt;br/&gt;
             noPartitions,&lt;br/&gt;
             true, // standby&lt;br/&gt;
             stateDirectory,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collections.emptyMap(),&lt;br/&gt;
+            Collections.&amp;lt;String, String&amp;gt;emptyMap(),&lt;br/&gt;
             changelogReader,&lt;br/&gt;
             false,&lt;br/&gt;
             logContext);&lt;br/&gt;
@@ -453,7 +453,7 @@ public void shouldNotWriteCheckpointForStoresWithoutChangelogTopic() throws IOEx&lt;br/&gt;
         stateMgr.checkpoint(Collections.singletonMap(persistentStorePartition, 987L));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         final Map&amp;lt;TopicPartition, Long&amp;gt; read = checkpoint.read();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertThat(read, equalTo(Collections.emptyMap()));&lt;br/&gt;
+        assertThat(read, equalTo(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap()));&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Test&lt;br/&gt;
@@ -463,7 +463,7 @@ public void shouldThrowIllegalArgumentExceptionIfStoreNameIsSameAsCheckpointFile&lt;br/&gt;
             noPartitions,&lt;br/&gt;
             false,&lt;br/&gt;
             stateDirectory,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collections.emptyMap(),&lt;br/&gt;
+            Collections.&amp;lt;String, String&amp;gt;emptyMap(),&lt;br/&gt;
             changelogReader,&lt;br/&gt;
             false,&lt;br/&gt;
             logContext);&lt;br/&gt;
@@ -483,7 +483,7 @@ public void shouldThrowIllegalArgumentExceptionOnRegisterWhenStoreHasAlreadyBeen&lt;br/&gt;
             noPartitions,&lt;br/&gt;
             false,&lt;br/&gt;
             stateDirectory,&lt;/li&gt;
	&lt;li&gt;Collections.emptyMap(),&lt;br/&gt;
+            Collections.&amp;lt;String, String&amp;gt;emptyMap(),&lt;br/&gt;
             changelogReader,&lt;br/&gt;
             false,&lt;br/&gt;
             logContext);&lt;br/&gt;
@@ -550,7 +550,7 @@ public void close() {&lt;br/&gt;
         stateManager.register(stateStore, stateStore.stateRestoreCallback);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         try &lt;/p&gt;
{
-            stateManager.close(Collections.emptyMap());
+            stateManager.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());
             fail(&quot;Should throw ProcessorStateException if store close throws exception&quot;);
         }
&lt;p&gt; catch (final ProcessorStateException e) {&lt;br/&gt;
             // pass&lt;br/&gt;
@@ -622,7 +622,7 @@ public void close() {&lt;br/&gt;
         stateManager.register(stateStore2, stateStore2.stateRestoreCallback);&lt;/p&gt;

&lt;p&gt;         try &lt;/p&gt;
{
-            stateManager.close(Collections.emptyMap());
+            stateManager.close(Collections.&amp;lt;TopicPartition, Long&amp;gt;emptyMap());
         }
&lt;p&gt; catch (final ProcessorStateException expected) &lt;/p&gt;
{ /* ignode */ }
&lt;p&gt;         Assert.assertTrue(closedStore.get());&lt;br/&gt;
     }&lt;br/&gt;
@@ -639,7 +639,7 @@ public void shouldDeleteCheckpointFileOnCreationIfEosEnabled() throws IOExceptio&lt;br/&gt;
                 noPartitions,&lt;br/&gt;
                 false,&lt;br/&gt;
                 stateDirectory,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Collections.emptyMap(),&lt;br/&gt;
+                Collections.&amp;lt;String, String&amp;gt;emptyMap(),&lt;br/&gt;
                 changelogReader,&lt;br/&gt;
                 true,&lt;br/&gt;
                 logContext);&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
index a71aaad805a..73761eb6548 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java&lt;br/&gt;
@@ -40,11 +40,13 @@&lt;br/&gt;
 import org.apache.kafka.streams.kstream.internals.ConsumedInternal;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.internals.InternalStreamsBuilder;&lt;br/&gt;
 import org.apache.kafka.streams.kstream.internals.InternalStreamsBuilderTest;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.internals.MaterializedInternal;&lt;br/&gt;
 import org.apache.kafka.streams.processor.LogAndSkipOnInvalidTimestamp;&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
 import org.apache.kafka.streams.processor.TaskMetadata;&lt;br/&gt;
 import org.apache.kafka.streams.processor.ThreadMetadata;&lt;br/&gt;
 import org.apache.kafka.streams.state.KeyValueStore;&lt;br/&gt;
+import org.apache.kafka.streams.state.internals.OffsetCheckpoint;&lt;br/&gt;
 import org.apache.kafka.test.MockClientSupplier;&lt;br/&gt;
 import org.apache.kafka.test.MockStateRestoreListener;&lt;br/&gt;
 import org.apache.kafka.test.MockTimestampExtractor;&lt;br/&gt;
@@ -55,6 +57,8 @@&lt;br/&gt;
 import org.junit.Before;&lt;br/&gt;
 import org.junit.Test;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import java.io.File;&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
 import java.util.ArrayList;&lt;br/&gt;
 import java.util.Collections;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
@@ -65,6 +69,8 @@&lt;br/&gt;
 import java.util.Set;&lt;br/&gt;
 import java.util.UUID;&lt;/p&gt;

&lt;p&gt;+import static java.util.Collections.singletonList;&lt;br/&gt;
+import static org.apache.kafka.streams.processor.internals.AbstractStateManager.CHECKPOINT_FILE_NAME;&lt;br/&gt;
 import static org.hamcrest.CoreMatchers.equalTo;&lt;br/&gt;
 import static org.junit.Assert.assertEquals;&lt;br/&gt;
 import static org.junit.Assert.assertFalse;&lt;br/&gt;
@@ -100,13 +106,16 @@ public void setUp() {&lt;br/&gt;
     }&lt;/p&gt;

&lt;p&gt;     private final String topic1 = &quot;topic1&quot;;&lt;br/&gt;
+    private final String topic2 = &quot;topic2&quot;;&lt;/p&gt;

&lt;p&gt;     private final TopicPartition t1p1 = new TopicPartition(topic1, 1);&lt;br/&gt;
     private final TopicPartition t1p2 = new TopicPartition(topic1, 2);&lt;br/&gt;
+    private final TopicPartition t2p1 = new TopicPartition(topic2, 1);&lt;/p&gt;

&lt;p&gt;     // task0 is unused&lt;br/&gt;
     private final TaskId task1 = new TaskId(0, 1);&lt;br/&gt;
     private final TaskId task2 = new TaskId(0, 2);&lt;br/&gt;
+    private final TaskId task3 = new TaskId(1, 1);&lt;/p&gt;

&lt;p&gt;     private Properties configProps(final boolean enableEos) {&lt;br/&gt;
         return new Properties() {&lt;br/&gt;
@@ -789,6 +798,75 @@ public void shouldReturnStandbyTaskMetadataWhileRunningState() &lt;/p&gt;
{
         assertTrue(threadMetadata.activeTasks().isEmpty());
     }

&lt;p&gt;+    @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldUpdateStandbyTask() throws IOException {&lt;br/&gt;
+        final String storeName1 = &quot;count-one&quot;;&lt;br/&gt;
+        final String storeName2 = &quot;table-two&quot;;&lt;br/&gt;
+        final String changelogName1 = applicationId + &quot;-&quot; + storeName1 + &quot;-changelog&quot;;&lt;br/&gt;
+        final String changelogName2 = applicationId + &quot;-&quot; + storeName2 + &quot;-changelog&quot;;&lt;br/&gt;
+        final TopicPartition partition1 = new TopicPartition(changelogName1, 1);&lt;br/&gt;
+        final TopicPartition partition2 = new TopicPartition(changelogName2, 1);&lt;br/&gt;
+        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)&lt;br/&gt;
+                .groupByKey().count(Materialized.&amp;lt;Object, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(storeName1));&lt;br/&gt;
+        final MaterializedInternal materialized = new MaterializedInternal(Materialized.&amp;lt;Object, Long, KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;as(storeName2),&lt;br/&gt;
+                internalStreamsBuilder, &quot;&quot;);&lt;br/&gt;
+        internalStreamsBuilder.table(topic2, new ConsumedInternal(), materialized);&lt;br/&gt;
+&lt;br/&gt;
+        final StreamThread thread = createStreamThread(clientId, config, false);&lt;br/&gt;
+        final MockConsumer&amp;lt;byte[], byte[]&amp;gt; restoreConsumer = clientSupplier.restoreConsumer;&lt;br/&gt;
+        restoreConsumer.updatePartitions(changelogName1,&lt;br/&gt;
+                singletonList(&lt;br/&gt;
+                        new PartitionInfo(&lt;br/&gt;
+                                changelogName1,&lt;br/&gt;
+                                1,&lt;br/&gt;
+                                null,&lt;br/&gt;
+                                new Node&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+                                new Node&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;&lt;br/&gt;
+                        )&lt;br/&gt;
+                )&lt;br/&gt;
+        );&lt;br/&gt;
+&lt;br/&gt;
+        restoreConsumer.assign(Utils.mkSet(partition1, partition2));&lt;br/&gt;
+        restoreConsumer.updateEndOffsets(Collections.singletonMap(partition1, 10L));&lt;br/&gt;
+        restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition1, 0L));&lt;br/&gt;
+        restoreConsumer.updateEndOffsets(Collections.singletonMap(partition2, 10L));&lt;br/&gt;
+        restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition2, 0L));&lt;br/&gt;
+        // let the store1 be restored from 0 to 10; store2 be restored from 5 (checkpointed) to 10&lt;br/&gt;
+        OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(task3), CHECKPOINT_FILE_NAME));&lt;br/&gt;
+        checkpoint.write(Collections.singletonMap(partition2, 5L));&lt;br/&gt;
+&lt;br/&gt;
+        for (long i = 0L; i &amp;lt; 10L; i++) &lt;/p&gt;
{
+            restoreConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(changelogName1, 1, i, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));
+            restoreConsumer.addRecord(new ConsumerRecord&amp;lt;&amp;gt;(changelogName2, 1, i, (&quot;K&quot; + i).getBytes(), (&quot;V&quot; + i).getBytes()));
+        }
&lt;p&gt;+&lt;br/&gt;
+        thread.setState(StreamThread.State.RUNNING);&lt;br/&gt;
+&lt;br/&gt;
+        thread.rebalanceListener.onPartitionsRevoked(null);&lt;br/&gt;
+&lt;br/&gt;
+        final Map&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt; standbyTasks = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
+&lt;br/&gt;
+        // assign single partition&lt;br/&gt;
+        standbyTasks.put(task1, Collections.singleton(t1p1));&lt;br/&gt;
+        standbyTasks.put(task3, Collections.singleton(t2p1));&lt;br/&gt;
+&lt;br/&gt;
+        thread.taskManager().setAssignmentMetadata(Collections.&amp;lt;TaskId, Set&amp;lt;TopicPartition&amp;gt;&amp;gt;emptyMap(), standbyTasks);&lt;br/&gt;
+&lt;br/&gt;
+        thread.rebalanceListener.onPartitionsAssigned(Collections.&amp;lt;TopicPartition&amp;gt;emptyList());&lt;br/&gt;
+&lt;br/&gt;
+        thread.runOnce(-1);&lt;br/&gt;
+&lt;br/&gt;
+        final StandbyTask standbyTask1 = thread.taskManager().standbyTask(partition1);&lt;br/&gt;
+        final StandbyTask standbyTask2 = thread.taskManager().standbyTask(partition2);&lt;br/&gt;
+        final KeyValueStore&amp;lt;Object, Long&amp;gt; store1 = (KeyValueStore&amp;lt;Object, Long&amp;gt;) standbyTask1.getStore(storeName1);&lt;br/&gt;
+        final KeyValueStore&amp;lt;Object, Long&amp;gt; store2 = (KeyValueStore&amp;lt;Object, Long&amp;gt;) standbyTask2.getStore(storeName2);&lt;br/&gt;
+&lt;br/&gt;
+        assertEquals(10L, store1.approximateNumEntries());&lt;br/&gt;
+        assertEquals(5L, store2.approximateNumEntries());&lt;br/&gt;
+        assertEquals(0, thread.standbyRecords().size());&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     @Test&lt;br/&gt;
     public void shouldAlwaysUpdateTasksMetadataAfterChangingState() {&lt;br/&gt;
         final StreamThread thread = createStreamThread(clientId, config, false);&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16513013" author="githubbot" created="Thu, 14 Jun 2018 21:25:30 +0000"  >&lt;p&gt;guozhangwang opened a new pull request #5232: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7021&quot; title=&quot;Source KTable checkpoint is not correct&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7021&quot;&gt;&lt;del&gt;KAFKA-7021&lt;/del&gt;&lt;/a&gt;: checkpoint offsets from committed&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5232&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5232&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   This is a cherry-pick PR from &lt;a href=&quot;https://github.com/apache/kafka/pull/5207&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5207&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;   1) add the committed offsets to checkpointable offset map.&lt;/p&gt;

&lt;p&gt;   2) add the restoration integration test for the source KTable case.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Committer Checklist (excluded from commit message)&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Verify design and implementation&lt;/li&gt;
	&lt;li&gt;[ ] Verify test coverage and CI build status&lt;/li&gt;
	&lt;li&gt;[ ] Verify documentation (including upgrade notes)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16513358" author="githubbot" created="Fri, 15 Jun 2018 05:21:52 +0000"  >&lt;p&gt;guozhangwang closed pull request #5232: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7021&quot; title=&quot;Source KTable checkpoint is not correct&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7021&quot;&gt;&lt;del&gt;KAFKA-7021&lt;/del&gt;&lt;/a&gt;: checkpoint offsets from committed&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5232&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5232&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
index 364fbe855d1..7f6ac7ca614 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java&lt;br/&gt;
@@ -163,7 +163,7 @@ public String toString(final String indent) &lt;/p&gt;
{
         return sb.toString();
     }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected Map&amp;lt;TopicPartition, Long&amp;gt; recordCollectorOffsets() {&lt;br/&gt;
+    protected Map&amp;lt;TopicPartition, Long&amp;gt; activeTaskCheckpointableOffsets() 
{
         return Collections.emptyMap();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -234,7 +234,7 @@ void closeStateManager(final boolean writeCheckpoint) throws ProcessorStateExcep&lt;br/&gt;
         ProcessorStateException exception = null;&lt;br/&gt;
         log.trace(&quot;{} Closing state manager&quot;, logPrefix);&lt;br/&gt;
         try &lt;/p&gt;
{
-            stateMgr.close(writeCheckpoint ? recordCollectorOffsets() : null);
+            stateMgr.close(writeCheckpoint ? activeTaskCheckpointableOffsets() : null);
         }
&lt;p&gt; catch (final ProcessorStateException e) &lt;/p&gt;
{
             exception = e;
         }
&lt;p&gt; finally {&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
index 8d46da19904..a18175ac58b 100644&lt;br/&gt;
&amp;#8212; a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java&lt;br/&gt;
@@ -81,7 +81,7 @@ public StateDirectory(final String applicationId, final String stateDirConfig, f&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param taskId&lt;/li&gt;
	&lt;li&gt;@return directory for the 
{@link TaskId}
&lt;p&gt;      */&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;File directoryForTask(final TaskId taskId) {&lt;br/&gt;
+    public File directoryForTask(final TaskId taskId) {&lt;br/&gt;
         final File taskDir = new File(stateDir, taskId.toString());&lt;br/&gt;
         if (!taskDir.exists() &amp;amp;&amp;amp; !taskDir.mkdir()) {&lt;br/&gt;
             throw new ProcessorStateException(String.format(&quot;task directory &lt;span class=&quot;error&quot;&gt;&amp;#91;%s&amp;#93;&lt;/span&gt; doesn&apos;t exist and couldn&apos;t be created&quot;,&lt;br/&gt;
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
index 4b24aab65a3..86855f39c6e 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java&lt;br/&gt;
@@ -286,7 +286,7 @@ void commit(final boolean startNewTransaction) {&lt;br/&gt;
                 public void run() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {                     flushState();                     if (!eosEnabled) {
-                        stateMgr.checkpoint(recordCollectorOffsets());
+                        stateMgr.checkpoint(activeTaskCheckpointableOffsets());
                     }                     commitOffsets(startNewTransaction);                 }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -297,8 +297,17 @@ public void run() {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;protected Map&amp;lt;TopicPartition, Long&amp;gt; recordCollectorOffsets() {&lt;/li&gt;
	&lt;li&gt;return recordCollector.offsets();&lt;br/&gt;
+&lt;br/&gt;
+    protected Map&amp;lt;TopicPartition, Long&amp;gt; activeTaskCheckpointableOffsets() {&lt;br/&gt;
+        // put both producer acked offsets and consumer committed offsets as checkpointable offsets&lt;br/&gt;
+        final Map&amp;lt;TopicPartition, Long&amp;gt; checkpointableOffsets = recordCollector.offsets();&lt;br/&gt;
+        for (final Map.Entry&amp;lt;TopicPartition, Long&amp;gt; entry : consumedOffsets.entrySet()) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            if (!checkpointableOffsets.containsKey(entry.getKey())) {
+                checkpointableOffsets.put(entry.getKey(), entry.getValue());
+            }+        }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+        return checkpointableOffsets;&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     @Override&lt;br/&gt;
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..54c2bd7ede4&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java&lt;br/&gt;
@@ -0,0 +1,193 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements. See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License. You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.kafka.streams.integration;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.kafka.clients.consumer.Consumer;&lt;br/&gt;
+import org.apache.kafka.clients.consumer.ConsumerConfig;&lt;br/&gt;
+import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
+import org.apache.kafka.clients.producer.KafkaProducer;&lt;br/&gt;
+import org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;
+import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
+import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
+import org.apache.kafka.common.serialization.IntegerDeserializer;&lt;br/&gt;
+import org.apache.kafka.common.serialization.IntegerSerializer;&lt;br/&gt;
+import org.apache.kafka.common.serialization.Serdes;&lt;br/&gt;
+import org.apache.kafka.common.utils.MockTime;&lt;br/&gt;
+import org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;
+import org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;
+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.ForeachAction;&lt;br/&gt;
+import org.apache.kafka.streams.kstream.KStreamBuilder;&lt;br/&gt;
+import org.apache.kafka.streams.processor.TaskId;&lt;br/&gt;
+import org.apache.kafka.streams.processor.internals.StateDirectory;&lt;br/&gt;
+import org.apache.kafka.streams.state.QueryableStoreTypes;&lt;br/&gt;
+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;&lt;br/&gt;
+import org.apache.kafka.streams.state.internals.OffsetCheckpoint;&lt;br/&gt;
+import org.apache.kafka.test.IntegrationTest;&lt;br/&gt;
+import org.apache.kafka.test.TestUtils;&lt;br/&gt;
+import org.junit.After;&lt;br/&gt;
+import org.junit.BeforeClass;&lt;br/&gt;
+import org.junit.ClassRule;&lt;br/&gt;
+import org.junit.Test;&lt;br/&gt;
+import org.junit.experimental.categories.Category;&lt;br/&gt;
+&lt;br/&gt;
+import java.io.File;&lt;br/&gt;
+import java.util.Arrays;&lt;br/&gt;
+import java.util.Collections;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+import java.util.Properties;&lt;br/&gt;
+import java.util.concurrent.CountDownLatch;&lt;br/&gt;
+import java.util.concurrent.TimeUnit;&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicInteger;&lt;br/&gt;
+&lt;br/&gt;
+import static org.hamcrest.MatcherAssert.assertThat;&lt;br/&gt;
+import static org.hamcrest.core.IsEqual.equalTo;&lt;br/&gt;
+import static org.junit.Assert.assertTrue;&lt;br/&gt;
+&lt;br/&gt;
+@Category(&lt;/p&gt;
{IntegrationTest.class}
&lt;p&gt;)&lt;br/&gt;
+public class RestoreIntegrationTest {&lt;br/&gt;
+    private static final int NUM_BROKERS = 1;&lt;br/&gt;
+&lt;br/&gt;
+    private static final String APPID = &quot;restore-test&quot;;&lt;br/&gt;
+&lt;br/&gt;
+    @ClassRule&lt;br/&gt;
+    public static final EmbeddedKafkaCluster CLUSTER =&lt;br/&gt;
+            new EmbeddedKafkaCluster(NUM_BROKERS);&lt;br/&gt;
+    private static final String INPUT_STREAM = &quot;input-stream&quot;;&lt;br/&gt;
+    private static final String INPUT_STREAM_2 = &quot;input-stream-2&quot;;&lt;br/&gt;
+    private final int numberOfKeys = 10000;&lt;br/&gt;
+    private KafkaStreams kafkaStreams;&lt;br/&gt;
+&lt;br/&gt;
+    @BeforeClass&lt;br/&gt;
+    public static void createTopics() throws InterruptedException &lt;/p&gt;
{
+        CLUSTER.createTopic(INPUT_STREAM, 2, 1);
+        CLUSTER.createTopic(INPUT_STREAM_2, 2, 1);
+        CLUSTER.createTopic(APPID + &quot;-store-changelog&quot;, 2, 1);
+    }
&lt;p&gt;+&lt;br/&gt;
+    private Properties props(final String applicationId) &lt;/p&gt;
{
+        Properties streamsConfiguration = new Properties();
+        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
+        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(applicationId).getPath());
+        streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());
+        streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());
+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);
+        return streamsConfiguration;
+    }
&lt;p&gt;+&lt;br/&gt;
+    @After&lt;br/&gt;
+    public void shutdown() {&lt;br/&gt;
+        if (kafkaStreams != null) &lt;/p&gt;
{
+            kafkaStreams.close(30, TimeUnit.SECONDS);
+        }
&lt;p&gt;+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Test&lt;br/&gt;
+    public void shouldRestoreStateFromSourceTopic() throws Exception {&lt;br/&gt;
+        final AtomicInteger numReceived = new AtomicInteger(0);&lt;br/&gt;
+        final KStreamBuilder builder = new KStreamBuilder();&lt;br/&gt;
+&lt;br/&gt;
+        final Properties props = props(APPID);&lt;br/&gt;
+&lt;br/&gt;
+        // restoring from 1000 to 4000 (committed), and then process from 4000 to 5000 on each of the two partitions&lt;br/&gt;
+        final int offsetLimitDelta = 1000;&lt;br/&gt;
+        final int offsetCheckpointed = 1000;&lt;br/&gt;
+        createStateForRestoration(INPUT_STREAM);&lt;br/&gt;
+        setCommittedOffset(INPUT_STREAM, offsetLimitDelta);&lt;br/&gt;
+&lt;br/&gt;
+        final StateDirectory stateDirectory = new StateDirectory(APPID, props.getProperty(StreamsConfig.STATE_DIR_CONFIG), new MockTime());&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 0)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 0), (long) offsetCheckpointed));&lt;br/&gt;
+        new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 1)), &quot;.checkpoint&quot;))&lt;br/&gt;
+                .write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 1), (long) offsetCheckpointed));&lt;br/&gt;
+&lt;br/&gt;
+        final CountDownLatch startupLatch = new CountDownLatch(1);&lt;br/&gt;
+        final CountDownLatch shutdownLatch = new CountDownLatch(1);&lt;br/&gt;
+&lt;br/&gt;
+        builder.table(Serdes.Integer(), Serdes.Integer(), INPUT_STREAM, &quot;store&quot;)&lt;br/&gt;
+                .toStream()&lt;br/&gt;
+                .foreach(new ForeachAction&amp;lt;Integer, Integer&amp;gt;() {&lt;br/&gt;
+                    @Override&lt;br/&gt;
+                    public void apply(final Integer key, final Integer value) &lt;/p&gt;
{
+                        if (numReceived.incrementAndGet() == 2 * offsetLimitDelta)
+                            shutdownLatch.countDown();
+                    }
&lt;p&gt;+                });&lt;br/&gt;
+&lt;br/&gt;
+        kafkaStreams = new KafkaStreams(builder, props);&lt;br/&gt;
+        kafkaStreams.setStateListener(new KafkaStreams.StateListener() {&lt;br/&gt;
+            @Override&lt;br/&gt;
+            public void onChange(final KafkaStreams.State newState, final KafkaStreams.State oldState) {&lt;br/&gt;
+                if (newState == KafkaStreams.State.RUNNING &amp;amp;&amp;amp; oldState == KafkaStreams.State.REBALANCING) &lt;/p&gt;
{
+                    startupLatch.countDown();
+                }
&lt;p&gt;+            }&lt;br/&gt;
+        });&lt;br/&gt;
+&lt;br/&gt;
+        kafkaStreams.start();&lt;br/&gt;
+&lt;br/&gt;
+        assertTrue(startupLatch.await(30, TimeUnit.SECONDS));&lt;br/&gt;
+        ReadOnlyKeyValueStore&amp;lt;Integer, Integer&amp;gt; store = kafkaStreams.store(&quot;store&quot;, QueryableStoreTypes.&amp;lt;Integer, Integer&amp;gt;keyValueStore());&lt;br/&gt;
+        assertThat(store.approximateNumEntries(), equalTo((long) numberOfKeys - offsetLimitDelta * 2 - offsetCheckpointed * 2));&lt;br/&gt;
+&lt;br/&gt;
+        assertTrue(shutdownLatch.await(30, TimeUnit.SECONDS));&lt;br/&gt;
+        assertThat(numReceived.get(), equalTo(offsetLimitDelta * 2));&lt;br/&gt;
+        assertThat(store.approximateNumEntries(), equalTo((long) numberOfKeys - offsetCheckpointed * 2));&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void createStateForRestoration(final String changelogTopic) {&lt;br/&gt;
+        final Properties producerConfig = new Properties();&lt;br/&gt;
+        producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+&lt;br/&gt;
+        try (final KafkaProducer&amp;lt;Integer, Integer&amp;gt; producer =&lt;br/&gt;
+                     new KafkaProducer&amp;lt;&amp;gt;(producerConfig, new IntegerSerializer(), new IntegerSerializer())) {&lt;br/&gt;
+&lt;br/&gt;
+            for (int i = 0; i &amp;lt; numberOfKeys; i++) &lt;/p&gt;
{
+                producer.send(new ProducerRecord&amp;lt;&amp;gt;(changelogTopic, i, i));
+            }
&lt;p&gt;+        }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private void setCommittedOffset(final String topic, final int limitDelta) {&lt;br/&gt;
+        final Properties consumerConfig = new Properties();&lt;br/&gt;
+        consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());&lt;br/&gt;
+        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APPID);&lt;br/&gt;
+        consumerConfig.put(ConsumerConfig.CLIENT_ID_CONFIG, &quot;commit-consumer&quot;);&lt;br/&gt;
+        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);&lt;br/&gt;
+        consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);&lt;br/&gt;
+&lt;br/&gt;
+        final Consumer consumer = new KafkaConsumer(consumerConfig);&lt;br/&gt;
+        final List&amp;lt;TopicPartition&amp;gt; partitions = Arrays.asList(&lt;br/&gt;
+                new TopicPartition(topic, 0),&lt;br/&gt;
+                new TopicPartition(topic, 1));&lt;br/&gt;
+&lt;br/&gt;
+        consumer.assign(partitions);&lt;br/&gt;
+        consumer.seekToEnd(partitions);&lt;br/&gt;
+&lt;br/&gt;
+        for (TopicPartition partition : partitions) &lt;/p&gt;
{
+            final long position = consumer.position(partition);
+            consumer.seek(partition, position - limitDelta);
+        }
&lt;p&gt;+&lt;br/&gt;
+        consumer.commitSync();&lt;br/&gt;
+        consumer.close();&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16513969" author="guozhang" created="Fri, 15 Jun 2018 15:29:14 +0000"  >&lt;p&gt;Cherry-picked the bug fix to 0.10.2, 0.11.0, 1.0, 1.1.&lt;/p&gt;</comment>
                            <comment id="16514001" author="githubbot" created="Fri, 15 Jun 2018 15:49:48 +0000"  >&lt;p&gt;guozhangwang closed pull request #5195: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7021&quot; title=&quot;Source KTable checkpoint is not correct&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7021&quot;&gt;&lt;del&gt;KAFKA-7021&lt;/del&gt;&lt;/a&gt;: Update upgrade guide section for reusing source topic&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/kafka/pull/5195&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/5195&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html&lt;br/&gt;
index 07f85446424..cd9278262b7 100644&lt;br/&gt;
&amp;#8212; a/docs/streams/upgrade-guide.html&lt;br/&gt;
+++ b/docs/streams/upgrade-guide.html&lt;br/&gt;
@@ -34,7 +34,7 @@ &amp;lt;h1&amp;gt;Upgrade Guide and API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
     &amp;lt;/div&amp;gt;&lt;/p&gt;

&lt;p&gt;     &amp;lt;p&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Upgrading from any older version to 2.0.0 is possible: (1) you need to make sure to update you code accordingly, because there are some minor non-compatible API changes since older&lt;br/&gt;
+        Upgrading from any older version to 2.0.0 is possible: (1) you need to make sure to update you code and config accordingly, because there are some minor non-compatible API changes since older&lt;br/&gt;
         releases (the code changes are expected to be minimal, please see below for the details),&lt;br/&gt;
         (2) upgrading to 2.0.0 in the online mode requires two rolling bounces.&lt;br/&gt;
         For (2), in the first rolling bounce phase users need to set config &amp;lt;code&amp;gt;upgrade.from=&quot;older version&quot;&amp;lt;/code&amp;gt; (possible values are &amp;lt;code&amp;gt;&quot;0.10.0&quot;, &quot;0.10.1&quot;, &quot;0.10.2&quot;, &quot;0.11.0&quot;, &quot;1.0&quot;, and &quot;1.1&quot;&amp;lt;/code&amp;gt;)&lt;br/&gt;
@@ -59,6 +59,16 @@ &amp;lt;h1&amp;gt;Upgrade Guide and API Changes&amp;lt;/h1&amp;gt;&lt;br/&gt;
         For Kafka Streams 0.10.0, broker version 0.10.0 or higher is required.&lt;br/&gt;
     &amp;lt;/p&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    &amp;lt;p&amp;gt;&lt;br/&gt;
+        Another important thing to keep in mind: in deprecated &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt; class, when a &amp;lt;code&amp;gt;KTable&amp;lt;/code&amp;gt; is created from a source topic via &amp;lt;code&amp;gt;KStreamBuilder.table()&amp;lt;/code&amp;gt;, its materialized state store&lt;br/&gt;
+        will reuse the source topic as its changelog topic for restoring, and will disable logging to avoid appending new updates to the source topic; in the &amp;lt;code&amp;gt;StreamsBuilder&amp;lt;/code&amp;gt; class introduced in 1.0, this behavior was changed&lt;br/&gt;
+        accidentally: we still reuse the source topic as the changelog topic for restoring, but will also create a separate changelog topic to append the update records from source topic to. In the 2.0 release, we have fixed this issue and now users&lt;br/&gt;
+        can choose whether or not to reuse the source topic based on the &amp;lt;code&amp;gt;StreamsConfig#TOPOLOGY_OPTIMIZATION&amp;lt;/code&amp;gt;: if you are upgrading from the old &amp;lt;code&amp;gt;KStreamBuilder&amp;lt;/code&amp;gt; class and hence you need to change your code to use&lt;br/&gt;
+        the new &amp;lt;code&amp;gt;StreamsBuilder&amp;lt;/code&amp;gt;, you should set this config value to &amp;lt;code&amp;gt;StreamsConfig#OPTIMIZE&amp;lt;/code&amp;gt; to continue reusing the source topic; if you are upgrading from 1.0 or 1.1 where you are already using &amp;lt;code&amp;gt;StreamsBuilder&amp;lt;/code&amp;gt; and hence have already&lt;br/&gt;
+        created a separate changelog topic, you should set this config value to &amp;lt;code&amp;gt;StreamsConfig#NO_OPTIMIZATION&amp;lt;/code&amp;gt; when upgrading to 2.0.0 in order to use that changelog topic for restoring the state store.&lt;br/&gt;
+        More details about the new config &amp;lt;code&amp;gt;StreamsConfig#TOPOLOGY_OPTIMIZATION&amp;lt;/code&amp;gt; can be found in &amp;lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-295%3A+Add+Streams+Configuration+Allowing+for+Optional+Topology+Optimization&quot;&amp;gt;KIP-295&amp;lt;/a&amp;gt;.&lt;br/&gt;
+    &amp;lt;/p&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
     &amp;lt;p&amp;gt;&lt;br/&gt;
         In 2.0.0 we have added a few new APIs on the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface (for details please read &amp;lt;a href=&quot;#streams_api_changes_200&quot;&amp;gt;Streams API changes&amp;lt;/a&amp;gt; below).&lt;br/&gt;
         If you have customized window store implementations that extends the &amp;lt;code&amp;gt;ReadOnlyWindowStore&amp;lt;/code&amp;gt; interface you need to make code changes.&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 22 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3un5b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>