<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 16:52:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[KAFKA-2143] Replicas get ahead of leader and fail</title>
                <link>https://issues.apache.org/jira/browse/KAFKA-2143</link>
                <project id="12311720" key="KAFKA">Kafka</project>
                    <description>&lt;p&gt;On a cluster of 6 nodes, we recently saw a case where a single under-replicated partition suddenly appeared, replication lag spiked, and network IO spiked. The cluster appeared to recover eventually on its own,&lt;/p&gt;

&lt;p&gt;Looking at the logs, the thing which failed was partition 7 of the topic &lt;tt&gt;background_queue&lt;/tt&gt;. It had an ISR of 1,4,3 and its leader at the time was 3. Here are the interesting log lines:&lt;/p&gt;

&lt;p&gt;On node 3 (the leader):&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2015-04-23 16:50:05,879] ERROR [Replica Manager on Broker 3]: Error when processing fetch request for partition [background_queue,7] offset 3722949957 from follower with correlation id 148185816. Possible cause: Request for offset 3722949957 but we only have log segments in the range 3648049863 to 3722949955. (kafka.server.ReplicaManager)
[2015-04-23 16:50:05,879] ERROR [Replica Manager on Broker 3]: Error when processing fetch request for partition [background_queue,7] offset 3722949957 from follower with correlation id 156007054. Possible cause: Request for offset 3722949957 but we only have log segments in the range 3648049863 to 3722949955. (kafka.server.ReplicaManager)
[2015-04-23 16:50:13,960] INFO Partition [background_queue,7] on broker 3: Shrinking ISR for partition [background_queue,7] from 1,4,3 to 3 (kafka.cluster.Partition)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that both replicas suddenly asked for an offset &lt;b&gt;ahead&lt;/b&gt; of the available offsets.&lt;/p&gt;

&lt;p&gt;And on nodes 1 and 4 (the replicas) many occurrences of the following:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[2015-04-23 16:50:05,935] INFO Scheduling log segment 3648049863 for log background_queue-7 for deletion. (kafka.log.Log) (edited)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Based on my reading, this looks like the replicas somehow got &lt;b&gt;ahead&lt;/b&gt; of the leader, asked for an invalid offset, got confused, and re-replicated the entire topic from scratch to recover (this matches our network graphs, which show 3 sending a bunch of data to 1 and 4).&lt;/p&gt;

&lt;p&gt;Taking a stab in the dark at the cause, there appears to be a race condition where replicas can receive a new offset before the leader has committed it and is ready to replicate?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12823502">KAFKA-2143</key>
            <summary>Replicas get ahead of leader and fail</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="becket_qin">Jiangjie Qin</assignee>
                                    <reporter username="eapache">Evan Huus</reporter>
                        <labels>
                    </labels>
                <created>Thu, 23 Apr 2015 18:17:13 +0000</created>
                <updated>Fri, 10 Feb 2017 05:02:16 +0000</updated>
                            <resolved>Fri, 29 Jan 2016 04:20:05 +0000</resolved>
                                    <version>0.8.2.1</version>
                                    <fixVersion>0.9.0.1</fixVersion>
                                    <component>replication</component>
                        <due></due>
                            <votes>8</votes>
                                    <watches>27</watches>
                                                                                                                <comments>
                            <comment id="14648444" author="bugzmanov" created="Thu, 30 Jul 2015 23:02:33 +0000"  >&lt;p&gt;I would like to add that we are experiencing this issue quite often. &lt;br/&gt;
And the situation get worse when we have configs like this&lt;/p&gt;

&lt;p&gt;replication factor: 2&lt;br/&gt;
unclean.leader.election.enable: true &lt;/p&gt;

&lt;p&gt;This leads to loss of a whole partition in case of two network glitches happening in a row. &lt;/p&gt;

&lt;p&gt;The failure scenario looks like this:&lt;br/&gt;
we have replica.lag.max.messages: 4000 so I&apos;m guessing follower is always little bit behind leader for high volume topic (which means leader has a larger offset value than the follower)&lt;/p&gt;

&lt;p&gt;1) Broker A (leader) has committed offset up-to 5000 &lt;br/&gt;
2) Broker B (follower) has committed offset up to 3000 (he is still in ISR because of  replica.lag.max.messages)&lt;br/&gt;
**&lt;b&gt;network glitch happens&lt;/b&gt;**&lt;br/&gt;
3) Broker B becomes a leader, Broker A becomes a follower&lt;br/&gt;
4) Broker A (follower) asks leader for messages starting from 5000 &lt;br/&gt;
5) Broker A (follower)  receives message that this is invalid offset (Broker B has only 3000) and drops partition to 0&lt;br/&gt;
**&lt;b&gt;network glitch happens&lt;/b&gt;**&lt;br/&gt;
6) Broker A becomes a leader (unclean election), Broker B becomes a follower&lt;br/&gt;
7) Broker B (follower) ask leader for messages starting from 3000&lt;br/&gt;
8) Broker B (follower) receives message that this is invalid offset (Broker A has only 0) and drops partition to 0&lt;/p&gt;

&lt;p&gt;As a result we lost partition because of 2 network glitches. &lt;/p&gt;

&lt;p&gt;And if the configs are &lt;br/&gt;
replication factor: 2&lt;br/&gt;
unclean.leader.election.enable: false &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/warning.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;the scenario repeats up to 5th step, but then Broker A got kicked out of ISR  and unclean election is not happening. &lt;/p&gt;</comment>
                            <comment id="14650146" author="becket_qin" created="Sat, 1 Aug 2015 04:08:32 +0000"  >&lt;p&gt;Hmm, with current design when the follower got OffsetOutOfRangeException during fetch, and if &lt;b&gt;leader&apos;s log end offset is smaller than follower&apos;s log end offset&lt;/b&gt;, follower will not truncate to zero but the current leader&apos;s log end offsets. That means in step (5), broker A should start to fetch from offset 3000 instead of drop to 0 unless broker B really have nothing.&lt;/p&gt;

&lt;p&gt;However, if the follower&apos;s log end offsets is smaller than the leader&apos;s log end offsets but we received offset out of range exception. We assume it was because the follower&apos;s log end offsets is too small and behind the earliest available offset of leader. But it might not necessarily be true, because we are fetching the latest offset again after we receive the exception. So it is possible that when the exception was returned, follower was ahead of the leader, but when it tries to handle the exception, since the new leader has got some new messages appended, the follower is now behind the leader. In that case, the follower will truncate its log to the log start offset of leader, which is the problem you described.&lt;/p&gt;

&lt;p&gt;I think what happened is as below:&lt;/p&gt;

&lt;p&gt;1) Broker A (leader) has committed offset up-to 5000&lt;br/&gt;
2) Broker B (follower) has committed offset up to 3000 (he is still in ISR because of replica.lag.max.messages)&lt;br/&gt;
**&lt;b&gt;network glitch happens&lt;/b&gt;**&lt;br/&gt;
3) Broker B becomes a leader, Broker A becomes a follower&lt;br/&gt;
4) Broker A fetch from Broker B and got an OffsetOutOfRangeException.&lt;br/&gt;
4.1) Broker B got some new message and its log end offset become 6000.&lt;br/&gt;
4.2) Broker A tries to handle the Exception so it checks the log end offset on broker B and found it is greater than broker A&apos;s log end offset so it truncate itself to the starting offset of Broker B.&lt;/p&gt;

&lt;p&gt;And the rest is pretty what you described.&lt;/p&gt;

&lt;p&gt;By design at step (3) when Broker A become follower, it will truncate its log to its high watermark, which should be 3000. There is a possible related issue &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2334&quot; title=&quot;Prevent HW from going back during leader failover &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2334&quot;&gt;&lt;del&gt;KAFKA-2334&lt;/del&gt;&lt;/a&gt;, which might make 3000 not the real high watermark on Broker B. And that issue triggered the offset out of range in step (4).&lt;/p&gt;

&lt;p&gt;So it looks to me &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2334&quot; title=&quot;Prevent HW from going back during leader failover &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2334&quot;&gt;&lt;del&gt;KAFKA-2334&lt;/del&gt;&lt;/a&gt; and the ReplicaFetcherThread bug together caused the issue. I&apos;ll take both of the issues and see if they can be solved together.&lt;/p&gt;</comment>
                            <comment id="14651344" author="becket_qin" created="Mon, 3 Aug 2015 02:31:47 +0000"  >&lt;p&gt;Actually I realized &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2334&quot; title=&quot;Prevent HW from going back during leader failover &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2334&quot;&gt;&lt;del&gt;KAFKA-2334&lt;/del&gt;&lt;/a&gt; is orthogonal to this issue. I submitted patch for this ticket and will leave &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2334&quot; title=&quot;Prevent HW from going back during leader failover &quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2334&quot;&gt;&lt;del&gt;KAFKA-2334&lt;/del&gt;&lt;/a&gt; to be solved separately.&lt;/p&gt;</comment>
                            <comment id="14659277" author="wushujames" created="Thu, 6 Aug 2015 00:51:08 +0000"  >&lt;p&gt;I&apos;m trying to understand part of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bugzmanov&quot; class=&quot;user-hover&quot; rel=&quot;bugzmanov&quot;&gt;bugzmanov&lt;/a&gt;&apos;s description. If unclean.leader.election.enable=false, then in Step 5, as you say, Broker A is kicked out of the ISR list.&lt;/p&gt;

&lt;p&gt;What happens, then, to the messages 3000-5000, that were originally in Broker A? Are they just lost?&lt;/p&gt;

&lt;p&gt;What was the producer settings for request.required.acks (old producer) or acks (new producer) in this scenario? Was it 0 or 1? If it was -1 (old producer) or &quot;all&quot; (new producer), is it possible for the replica lag to get that large?&lt;/p&gt;</comment>
                            <comment id="14659477" author="becket_qin" created="Thu, 6 Aug 2015 04:33:44 +0000"  >&lt;p&gt;In that case, yes, messages 3000-5000 will be lost. If producer is not producing with acks=-1, that is the expected behavior. If acks=-1, as long as producer got callback from broker, that means the message has been appended to all the brokers in ISR (NOT all the replicas!)&lt;/p&gt;

&lt;p&gt;Here is a slide describing how to avoid data loss if you are interested:&lt;br/&gt;
&lt;a href=&quot;http://www.slideshare.net/JiangjieQin/no-data-loss-pipeline-with-apache-kafka-49753844&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.slideshare.net/JiangjieQin/no-data-loss-pipeline-with-apache-kafka-49753844&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14659503" author="wushujames" created="Thu, 6 Aug 2015 05:15:15 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ewencp&quot; class=&quot;user-hover&quot; rel=&quot;ewencp&quot;&gt;ewencp&lt;/a&gt; sent me that slidedeck of yours. I added all your suggested settings to my team&apos;s &quot;Kafka recommended settings&quot; wiki. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Even with ack=-1, I think it&apos;s still possible for a replica to lag behind the leader by 2000 messages, right? It would be lagging behind by 2000 messages that were produced but not-yet-acknowledged. Not sure if losing those would count as message-loss, since they weren&apos;t actually ack&apos;d?&lt;/p&gt;

&lt;p&gt;replica.lag.max.messages is a broker-level config setting, not a topic level config setting. I think that means that it&apos;s hard to find a setting that works for high-volume topics and low-volume topics, right?&lt;/p&gt;

&lt;p&gt;And... a bit of Googling around, and I think my observation is the exact thing that inspired &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-16+-+Automated+Replica+Lag+Tuning&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-16+-+Automated+Replica+Lag+Tuning&lt;/a&gt; and  &lt;a href=&quot;http://www.confluent.io/blog/hands-free-kafka-replication-a-lesson-in-operational-simplicity/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.confluent.io/blog/hands-free-kafka-replication-a-lesson-in-operational-simplicity/&lt;/a&gt;, right?&lt;/p&gt;

&lt;p&gt;Is it possible for broker to be in the ISR for one topic, and to be out of the ISR for another topic?&lt;/p&gt;</comment>
                            <comment id="14660629" author="ewencp" created="Thu, 6 Aug 2015 19:06:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wushujames&quot; class=&quot;user-hover&quot; rel=&quot;wushujames&quot;&gt;wushujames&lt;/a&gt; I think one easy way for a broker to be in ISR for one topic and out of ISR for another is a partition &amp;#8211; if topic A has broker 1 as leader and topic B has broker 2 as leader, then broker 3 replicating A and B could be in ISR for A, but unable to replicate data for B if there is a partition between brokers 1 and 3.&lt;/p&gt;</comment>
                            <comment id="14680467" author="githubbot" created="Mon, 10 Aug 2015 18:04:30 +0000"  >&lt;p&gt;GitHub user becketqin opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/129&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/129&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2143&quot; title=&quot;Replicas get ahead of leader and fail&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2143&quot;&gt;&lt;del&gt;KAFKA-2143&lt;/del&gt;&lt;/a&gt;: fix replica offset truncate to beginning during leader migration.&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/becketqin/kafka&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/becketqin/kafka&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2143&quot; title=&quot;Replicas get ahead of leader and fail&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2143&quot;&gt;&lt;del&gt;KAFKA-2143&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/129.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/129.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #129&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 71f8a4716e1f0b4fc2bd88aa30fe38aef8a9f92e&lt;br/&gt;
Author: Jiangjie Qin &amp;lt;becket.qin@gmail.com&amp;gt;&lt;br/&gt;
Date:   2015-08-03T02:22:02Z&lt;/p&gt;

&lt;p&gt;    Fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2134&quot; title=&quot;Producer blocked on metric publish&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2134&quot;&gt;&lt;del&gt;KAFKA-2134&lt;/del&gt;&lt;/a&gt;, fix replica offset truncate to beginning during leader migration.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14730109" author="junrao" created="Fri, 4 Sep 2015 00:41:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;, since before step (3), both A and B are in ISR, the last committed offset in A can&apos;t be larger than 3000. So, in step (3), if A becomes a follower, it has to first truncate its log to last committed offset before fetching. So, at that point, A&apos;s fetch offset can&apos;t be larger than 3000 and therefore won&apos;t be out of range.&lt;/p&gt;

&lt;p&gt;The following is a alternative scenario that can cause this.&lt;/p&gt;

&lt;p&gt;1) Broker A (leader) receives messages to 5000&lt;br/&gt;
2) Broker B (follower) receives messages to 3000 (it is still in ISR because of replica.lag.max.messages)&lt;br/&gt;
3) For some reason, B is dropped out of ISR.&lt;br/&gt;
4) Broker A (the only one in ISR) commits messages to 5000.&lt;br/&gt;
5) For some reason, Broker A is considered dead and Broker B is live.&lt;br/&gt;
6) Broker B is selected as the new leader (unclean leader election) and is the only one in ISR.&lt;br/&gt;
7) Broker A is considered live again and starts fetching from 5000 (last committed offset) and gets OffsetOutOfRangeException.&lt;br/&gt;
8) In the mean time, B receives more messages to offset 6000.&lt;br/&gt;
9) Broker A tries to handle OffsetOutOfRangeException and finds out leader B&apos;s log end offset is now larger than its log end offset and truncates all its log.&lt;/p&gt;

&lt;p&gt;Your patch reduces the amount of the data that Broker A needs to replicate in step 9, which is probably fine. However, we probably should first verify if this is indeed what&apos;s happening since it seems that it should happen rarely. Also, &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt; reports a similar issue w/o any leadership change. So, may be there is something else that can cause this.&lt;/p&gt;</comment>
                            <comment id="14730117" author="becket_qin" created="Fri, 4 Sep 2015 00:54:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=junrao&quot; class=&quot;user-hover&quot; rel=&quot;junrao&quot;&gt;junrao&lt;/a&gt;, you are right. I realized there should be unclean leader election in this case. I&apos;ll check the code further to see if there is any finding for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15056672" author="ottomata" created="Mon, 14 Dec 2015 20:36:46 +0000"  >&lt;p&gt;I&apos;d like to add that Wikimedia is experiencing this as well.  It only started happening to us recently when we started producing to Kafka via a PHP process serving a web request.  This process is short lived, so only connects to Kafka in order to produce a single message, and then disconnects.  The only topic that this happens to is the one that this PHP process produces to.&lt;/p&gt;

&lt;p&gt;Everything recovers as it should, but something is definitely wrong here.  Why is a follower trying to consume an offset ahead of what the leader has?&lt;/p&gt;

&lt;p&gt;We&apos;re running 0.8.2.1 (with the snappy compression bugfix backported).&lt;/p&gt;</comment>
                            <comment id="15056999" author="becket_qin" created="Mon, 14 Dec 2015 23:52:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ottomata&quot; class=&quot;user-hover&quot; rel=&quot;ottomata&quot;&gt;ottomata&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt; is a more likely cause. You may want to take a look at that.&lt;/p&gt;</comment>
                            <comment id="15122919" author="githubbot" created="Fri, 29 Jan 2016 04:19:41 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/kafka/pull/129&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/129&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15122920" author="guozhang" created="Fri, 29 Jan 2016 04:20:05 +0000"  >&lt;p&gt;Issue resolved by pull request 129&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/kafka/pull/129&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/pull/129&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15123866" author="becket_qin" created="Fri, 29 Jan 2016 17:48:22 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guozhang&quot; class=&quot;user-hover&quot; rel=&quot;guozhang&quot;&gt;guozhang&lt;/a&gt;. I almost forgot I had this ticket &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15242412" author="jbrosenberg@gmail.com" created="Fri, 15 Apr 2016 04:31:21 +0000"  >&lt;p&gt;Hello, I recently experienced this issue, as described in the original description above.  We are running 0.8.2.2.&lt;br/&gt;
Note, in the original description above (and in my case as well), there was no evidence of a network glitch, and no evidence of any partition leadership change.  So, I&apos;m concerned whether the discussion that follows in this ticket (which describes network glitches and leadership changes almost exclusively) really addressed the original issue as reported (and my recent observation of this as well).&lt;/p&gt;

&lt;p&gt;Can someone comment on this please?  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=becket_qin&quot; class=&quot;user-hover&quot; rel=&quot;becket_qin&quot;&gt;becket_qin&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15242428" author="jbrosenberg@gmail.com" created="Fri, 15 Apr 2016 04:44:57 +0000"  >&lt;p&gt;After looking at &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt;, I think that is the actual issue reported in this ticket (as opposed to the unrelated issue the first commenter raised), and I see that &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2477&quot; title=&quot;Replicas spuriously deleting all segments in partition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-2477&quot;&gt;&lt;del&gt;KAFKA-2477&lt;/del&gt;&lt;/a&gt; seems to have been fixed.  So upgrading to 0.9.X is the thing to do, it would appear.&lt;/p&gt;</comment>
                            <comment id="15860712" author="mathieu.filotto" created="Fri, 10 Feb 2017 04:59:39 +0000"  >&lt;p&gt;Is the upgrade the only option ?&lt;br/&gt;
Is 0.10 upgrade working too ?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12826749">KAFKA-2165</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 40 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2doc7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>gwenshap</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>