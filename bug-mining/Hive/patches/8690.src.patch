diff --git a/data/conf/hive-site.xml b/data/conf/hive-site.xml
index eec7648483..cd716824bb 100644
--- a/data/conf/hive-site.xml
+++ b/data/conf/hive-site.xml
@@ -326,11 +326,6 @@
   <value>false</value>
 </property>
 
-<property>
-  <name>hive.strict.timestamp.conversion</name>
-  <value>false</value>
-</property>
-
 <property>
   <name>hive.cbo.fallback.strategy</name>
   <value>TEST</value>
diff --git a/hbase-handler/src/test/queries/positive/hbase_timestamp.q b/hbase-handler/src/test/queries/positive/hbase_timestamp.q
index 5972f49d95..4f040770e0 100644
--- a/hbase-handler/src/test/queries/positive/hbase_timestamp.q
+++ b/hbase-handler/src/test/queries/positive/hbase_timestamp.q
@@ -1,4 +1,6 @@
 --! qt:dataset:src
+set hive.strict.timestamp.conversion=false;
+
 DROP TABLE hbase_table;
 CREATE EXTERNAL TABLE hbase_table (key string, value string, `time` timestamp)
   STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index 7e138e1346..8e91c7a73a 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -27,12 +27,14 @@ minitez.query.files=\
   flatten_union_subdir.q,\
   limit_bailout.q,\
   mapjoin_addjar.q,\
+  non_strict_numeric_to_timestamp_conversion.q,\
   orc_merge12.q,\
   orc_vectorization_ppd.q,\
   tez_complextype_with_null.q,\
   tez_tag.q,\
   tez_union_udtf.q,\
   tez_union_with_udf.q,\
+  udf_configurable.q,\
   update_orig_table.q,\
   vector_join_part_col_char.q,\
   vector_non_string_partition.q
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTimestamp.java
index 8cbcbf55b0..5504341383 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTimestamp.java
@@ -20,6 +20,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.MapredContext;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToTimestamp;
@@ -31,7 +32,6 @@
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -71,7 +71,6 @@ public class GenericUDFTimestamp extends GenericUDF {
    * otherwise, it's interpreted as timestamp in seconds.
    */
   private boolean intToTimestampInSeconds = false;
-  private boolean strict = true;
 
   @Override
   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
@@ -79,24 +78,31 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
     checkArgPrimitive(arguments, 0);
     checkArgGroups(arguments, 0, tsInputTypes, STRING_GROUP, DATE_GROUP, NUMERIC_GROUP, VOID_GROUP, BOOLEAN_GROUP);
 
-    strict = SessionState.get() != null ? SessionState.get().getConf()
-        .getBoolVar(ConfVars.HIVE_STRICT_TIMESTAMP_CONVERSION) : new HiveConf()
-        .getBoolVar(ConfVars.HIVE_STRICT_TIMESTAMP_CONVERSION);
-    intToTimestampInSeconds = SessionState.get() != null ? SessionState.get().getConf()
-        .getBoolVar(ConfVars.HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS) : new HiveConf()
-        .getBoolVar(ConfVars.HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS);
-
-    if (strict) {
-      if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(tsInputTypes[0]) == PrimitiveGrouping.NUMERIC_GROUP) {
+    final SessionState ss = SessionState.get();
+    if (ss != null) {
+      final boolean strict = ss.getConf().getBoolVar(ConfVars.HIVE_STRICT_TIMESTAMP_CONVERSION);
+      final PrimitiveGrouping grouping = PrimitiveObjectInspectorUtils.getPrimitiveGrouping(tsInputTypes[0]);
+      if (strict && grouping == PrimitiveGrouping.NUMERIC_GROUP) {
         throw new UDFArgumentException(
             "Casting NUMERIC types to TIMESTAMP is prohibited (" + ConfVars.HIVE_STRICT_TIMESTAMP_CONVERSION + ")");
       }
+      intToTimestampInSeconds = ss.getConf().getBoolVar(ConfVars.HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS);
     }
 
     obtainTimestampConverter(arguments, 0, tsInputTypes, tsConvertors);
     return PrimitiveObjectInspectorFactory.writableTimestampObjectInspector;
   }
 
+  @Override
+  public void configure(MapredContext context) {
+    if (context == null) {
+      return;
+    }
+
+    intToTimestampInSeconds = HiveConf.getBoolVar(context.getJobConf(),
+        ConfVars.HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS);
+  }
+
   @Override
   public Object evaluate(DeferredObject[] arguments) throws HiveException {
     PrimitiveObjectInspectorConverter.TimestampConverter ts =
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterCompare.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterCompare.java
index ac3c0abae8..47104a1bf1 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterCompare.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterCompare.java
@@ -84,6 +84,7 @@ public TestVectorFilterCompare() {
     // Arithmetic operations rely on getting conf from SessionState, need to initialize here.
     SessionState ss = new SessionState(new HiveConf());
     ss.getConf().setVar(HiveConf.ConfVars.HIVE_COMPAT, "latest");
+    ss.getConf().setBoolVar(HiveConf.ConfVars.HIVE_STRICT_TIMESTAMP_CONVERSION, false);
     SessionState.setCurrentSessionState(ss);
   }
 
diff --git a/ql/src/test/queries/clientnegative/strict_numeric_to_timestamp_conversion.q b/ql/src/test/queries/clientnegative/strict_numeric_to_timestamp_conversion.q
new file mode 100644
index 0000000000..0ebf29a153
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/strict_numeric_to_timestamp_conversion.q
@@ -0,0 +1,13 @@
+set hive.strict.timestamp.conversion=true;
+
+create table test_num_ts_input_n(begin string, ts string);
+
+insert into test_num_ts_input_n values('1653209895687','2022-05-22T15:58:15.931+07:00'),('1653209938316','2022-05-22T15:58:58.490+07:00'),('1653209962021','2022-05-22T15:59:22.191+07:00'),('1653210021993','2022-05-22T16:00:22.174+07:00');
+
+
+CREATE TABLE t_date_ctas_n AS
+select
+  CAST( CAST( `begin` AS BIGINT) / 1000  AS TIMESTAMP ) `begin`,
+  CAST( DATE_FORMAT(CAST(regexp_replace(`ts`,'(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2}).(\\d{3})\\+(\\d{2}):(\\d{2})','$1-$2-$3 $4:$5:$6.$7') AS TIMESTAMP ),'yyyyMMdd') as BIGINT ) `par_key`
+FROM  test_num_ts_input_n;
+
diff --git a/ql/src/test/queries/clientpositive/non_strict_numeric_to_timestamp_conversion.q b/ql/src/test/queries/clientpositive/non_strict_numeric_to_timestamp_conversion.q
new file mode 100644
index 0000000000..f576c69557
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/non_strict_numeric_to_timestamp_conversion.q
@@ -0,0 +1,14 @@
+set hive.strict.timestamp.conversion=false;
+
+create table test_num_ts_input(begin string, ts string);
+
+insert into test_num_ts_input values('1653209895687','2022-05-22T15:58:15.931+07:00'),('1653209938316','2022-05-22T15:58:58.490+07:00'),('1653209962021','2022-05-22T15:59:22.191+07:00'),('1653210021993','2022-05-22T16:00:22.174+07:00');
+
+set hive.vectorized.execution.enabled=false;
+
+CREATE TABLE t_date_ctas AS
+select
+  CAST( CAST( `begin` AS BIGINT) / 1000  AS TIMESTAMP ) `begin`,
+  CAST( DATE_FORMAT(CAST(regexp_replace(`ts`,'(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2}).(\\d{3})\\+(\\d{2}):(\\d{2})','$1-$2-$3 $4:$5:$6.$7') AS TIMESTAMP ),'yyyyMMdd') as BIGINT ) `par_key`
+FROM  test_num_ts_input;
+
diff --git a/ql/src/test/queries/clientpositive/udf_configurable.q b/ql/src/test/queries/clientpositive/udf_configurable.q
new file mode 100644
index 0000000000..9f8bb3180c
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/udf_configurable.q
@@ -0,0 +1,55 @@
+CREATE TABLE test_udf_configurable (cint1 INT, cint2 INT, ctimestamp TIMESTAMP, text_timestamp STRING);
+INSERT INTO test_udf_configurable VALUES
+  (10000, 3, CAST('1970-01-01 01:02:03' AS TIMESTAMP), '1970-01-01 01:02:03 4'),
+  (20000, 5, CAST('1970-01-02 04:05:06' AS TIMESTAMP), '1970-01-02 04:05:06 5'),
+  (30000, 7, CAST('1970-01-03 07:08:09' AS TIMESTAMP), '1970-01-03 07:08:09 6');
+
+set hive.compat=latest;
+set hive.strict.timestamp.conversion=false;
+set hive.int.timestamp.conversion.in.seconds=true;
+set hive.local.time.zone=Asia/Bangkok;
+set hive.datetime.formatter=SIMPLE;
+set hive.masking.algo=sha512;
+set hive.use.googleregex.engine=true;
+
+-- On HiveServer2
+SELECT
+  -- DECIMAL because of hive.compat=latest
+  cint1 / cint2,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  cint1 = ctimestamp,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  -- Interpreted as seconds because of hive.int.timestamp.conversion.in.seconds=true
+  CAST(cint1 AS TIMESTAMP),
+  -- The semantics of "u" is different between SimpleDateFormat and DateTimeFormatter
+  DATE_FORMAT(ctimestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  FROM_UNIXTIME(cint1, 'yyyy-MM-dd HH:mm:ss u'),
+  TO_UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  -- SHA512 is used
+  MASK_HASH(text_timestamp),
+  -- Java's Pattern doesn't support it, then it fails with hive.use.googleregex.engine=false
+  text_timestamp RLIKE '\\p{Katakana}+'
+FROM test_udf_configurable;
+
+-- On Tez without vectorization
+set hive.fetch.task.conversion=none;
+set hive.vectorized.execution.enabled=false;
+SELECT
+  -- DECIMAL because of hive.compat=latest
+  cint1 / cint2,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  cint1 = ctimestamp,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  -- Interpreted as seconds because of hive.int.timestamp.conversion.in.seconds=true
+  CAST(cint1 AS TIMESTAMP),
+  -- formatter
+  DATE_FORMAT(ctimestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  FROM_UNIXTIME(cint1, 'yyyy-MM-dd HH:mm:ss u'),
+  TO_UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  -- SHA512 is used
+  MASK_HASH(text_timestamp),
+  -- Java's Pattern doesn't support it, then it fails with hive.use.googleregex.engine=false
+  text_timestamp RLIKE '\\p{Katakana}+'
+FROM test_udf_configurable;
diff --git a/ql/src/test/results/clientnegative/strict_numeric_to_timestamp_conversion.q.out b/ql/src/test/results/clientnegative/strict_numeric_to_timestamp_conversion.q.out
new file mode 100644
index 0000000000..b9032edb3c
--- /dev/null
+++ b/ql/src/test/results/clientnegative/strict_numeric_to_timestamp_conversion.q.out
@@ -0,0 +1,19 @@
+PREHOOK: query: create table test_num_ts_input_n(begin string, ts string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_num_ts_input_n
+POSTHOOK: query: create table test_num_ts_input_n(begin string, ts string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_num_ts_input_n
+PREHOOK: query: insert into test_num_ts_input_n values('1653209895687','2022-05-22T15:58:15.931+07:00'),('1653209938316','2022-05-22T15:58:58.490+07:00'),('1653209962021','2022-05-22T15:59:22.191+07:00'),('1653210021993','2022-05-22T16:00:22.174+07:00')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@test_num_ts_input_n
+POSTHOOK: query: insert into test_num_ts_input_n values('1653209895687','2022-05-22T15:58:15.931+07:00'),('1653209938316','2022-05-22T15:58:58.490+07:00'),('1653209962021','2022-05-22T15:59:22.191+07:00'),('1653210021993','2022-05-22T16:00:22.174+07:00')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@test_num_ts_input_n
+POSTHOOK: Lineage: test_num_ts_input_n.begin SCRIPT []
+POSTHOOK: Lineage: test_num_ts_input_n.ts SCRIPT []
+FAILED: SemanticException Line 0:-1 Wrong arguments '1000': Casting NUMERIC types to TIMESTAMP is prohibited (hive.strict.timestamp.conversion)
diff --git a/ql/src/test/results/clientpositive/tez/non_strict_numeric_to_timestamp_conversion.q.out b/ql/src/test/results/clientpositive/tez/non_strict_numeric_to_timestamp_conversion.q.out
new file mode 100644
index 0000000000..7525d91f48
--- /dev/null
+++ b/ql/src/test/results/clientpositive/tez/non_strict_numeric_to_timestamp_conversion.q.out
@@ -0,0 +1,38 @@
+PREHOOK: query: create table test_num_ts_input(begin string, ts string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_num_ts_input
+POSTHOOK: query: create table test_num_ts_input(begin string, ts string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_num_ts_input
+PREHOOK: query: insert into test_num_ts_input values('1653209895687','2022-05-22T15:58:15.931+07:00'),('1653209938316','2022-05-22T15:58:58.490+07:00'),('1653209962021','2022-05-22T15:59:22.191+07:00'),('1653210021993','2022-05-22T16:00:22.174+07:00')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@test_num_ts_input
+POSTHOOK: query: insert into test_num_ts_input values('1653209895687','2022-05-22T15:58:15.931+07:00'),('1653209938316','2022-05-22T15:58:58.490+07:00'),('1653209962021','2022-05-22T15:59:22.191+07:00'),('1653210021993','2022-05-22T16:00:22.174+07:00')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@test_num_ts_input
+POSTHOOK: Lineage: test_num_ts_input.begin SCRIPT []
+POSTHOOK: Lineage: test_num_ts_input.ts SCRIPT []
+PREHOOK: query: CREATE TABLE t_date_ctas AS
+select
+  CAST( CAST( `begin` AS BIGINT) / 1000  AS TIMESTAMP ) `begin`,
+  CAST( DATE_FORMAT(CAST(regexp_replace(`ts`,'(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2}).(\\d{3})\\+(\\d{2}):(\\d{2})','$1-$2-$3 $4:$5:$6.$7') AS TIMESTAMP ),'yyyyMMdd') as BIGINT ) `par_key`
+FROM  test_num_ts_input
+PREHOOK: type: CREATETABLE_AS_SELECT
+PREHOOK: Input: default@test_num_ts_input
+PREHOOK: Output: database:default
+PREHOOK: Output: default@t_date_ctas
+POSTHOOK: query: CREATE TABLE t_date_ctas AS
+select
+  CAST( CAST( `begin` AS BIGINT) / 1000  AS TIMESTAMP ) `begin`,
+  CAST( DATE_FORMAT(CAST(regexp_replace(`ts`,'(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2}).(\\d{3})\\+(\\d{2}):(\\d{2})','$1-$2-$3 $4:$5:$6.$7') AS TIMESTAMP ),'yyyyMMdd') as BIGINT ) `par_key`
+FROM  test_num_ts_input
+POSTHOOK: type: CREATETABLE_AS_SELECT
+POSTHOOK: Input: default@test_num_ts_input
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@t_date_ctas
+POSTHOOK: Lineage: t_date_ctas.begin EXPRESSION [(test_num_ts_input)test_num_ts_input.FieldSchema(name:begin, type:string, comment:null), ]
+POSTHOOK: Lineage: t_date_ctas.par_key EXPRESSION [(test_num_ts_input)test_num_ts_input.FieldSchema(name:ts, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/tez/udf_configurable.q.out b/ql/src/test/results/clientpositive/tez/udf_configurable.q.out
new file mode 100644
index 0000000000..69b9e8446a
--- /dev/null
+++ b/ql/src/test/results/clientpositive/tez/udf_configurable.q.out
@@ -0,0 +1,116 @@
+PREHOOK: query: CREATE TABLE test_udf_configurable (cint1 INT, cint2 INT, ctimestamp TIMESTAMP, text_timestamp STRING)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_udf_configurable
+POSTHOOK: query: CREATE TABLE test_udf_configurable (cint1 INT, cint2 INT, ctimestamp TIMESTAMP, text_timestamp STRING)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_udf_configurable
+PREHOOK: query: INSERT INTO test_udf_configurable VALUES
+  (10000, 3, CAST('1970-01-01 01:02:03' AS TIMESTAMP), '1970-01-01 01:02:03 4'),
+  (20000, 5, CAST('1970-01-02 04:05:06' AS TIMESTAMP), '1970-01-02 04:05:06 5'),
+  (30000, 7, CAST('1970-01-03 07:08:09' AS TIMESTAMP), '1970-01-03 07:08:09 6')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@test_udf_configurable
+POSTHOOK: query: INSERT INTO test_udf_configurable VALUES
+  (10000, 3, CAST('1970-01-01 01:02:03' AS TIMESTAMP), '1970-01-01 01:02:03 4'),
+  (20000, 5, CAST('1970-01-02 04:05:06' AS TIMESTAMP), '1970-01-02 04:05:06 5'),
+  (30000, 7, CAST('1970-01-03 07:08:09' AS TIMESTAMP), '1970-01-03 07:08:09 6')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@test_udf_configurable
+POSTHOOK: Lineage: test_udf_configurable.cint1 SCRIPT []
+POSTHOOK: Lineage: test_udf_configurable.cint2 SCRIPT []
+POSTHOOK: Lineage: test_udf_configurable.ctimestamp SCRIPT []
+POSTHOOK: Lineage: test_udf_configurable.text_timestamp SCRIPT []
+PREHOOK: query: SELECT
+  -- DECIMAL because of hive.compat=latest
+  cint1 / cint2,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  cint1 = ctimestamp,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  -- Interpreted as seconds because of hive.int.timestamp.conversion.in.seconds=true
+  CAST(cint1 AS TIMESTAMP),
+  -- The semantics of "u" is different between SimpleDateFormat and DateTimeFormatter
+  DATE_FORMAT(ctimestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  FROM_UNIXTIME(cint1, 'yyyy-MM-dd HH:mm:ss u'),
+  TO_UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  -- SHA512 is used
+  MASK_HASH(text_timestamp),
+  -- Java's Pattern doesn't support it, then it fails with hive.use.googleregex.engine=false
+  text_timestamp RLIKE '\\p{Katakana}+'
+FROM test_udf_configurable
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_udf_configurable
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: SELECT
+  -- DECIMAL because of hive.compat=latest
+  cint1 / cint2,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  cint1 = ctimestamp,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  -- Interpreted as seconds because of hive.int.timestamp.conversion.in.seconds=true
+  CAST(cint1 AS TIMESTAMP),
+  -- The semantics of "u" is different between SimpleDateFormat and DateTimeFormatter
+  DATE_FORMAT(ctimestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  FROM_UNIXTIME(cint1, 'yyyy-MM-dd HH:mm:ss u'),
+  TO_UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  -- SHA512 is used
+  MASK_HASH(text_timestamp),
+  -- Java's Pattern doesn't support it, then it fails with hive.use.googleregex.engine=false
+  text_timestamp RLIKE '\\p{Katakana}+'
+FROM test_udf_configurable
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_udf_configurable
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+3333.33333333333	false	1970-01-01 02:46:40	1970-01-01 01:02:03 4	1970-01-01 09:46:40 4	-21477	-21477	ca764b57c635a893c91b0edaea84bca142e065990e1a66b9c60cf855777af51070444210d30e359fa81e5a77c68a073224e0c7343d957f556816618a10baa37c	false
+4000.00000000000	false	1970-01-01 05:33:20	1970-01-02 04:05:06 5	1970-01-01 12:33:20 4	75906	75906	7882085a4ed36e7c675fcd12083eefa9a208f1fdcfba10b3a986bb5b1d43e23da3ba87b3933feebfed80ba9e05e4e73f72231cb8022453e562e034e8b5c603b9	false
+4285.71428571429	false	1970-01-01 08:20:00	1970-01-03 07:08:09 6	1970-01-01 15:20:00 4	173289	173289	d9899e12fce35a4f65cd63aee1e953dacd9e91229d52dd17ddad8e44c17d2d081c0a5519a96637663bc6baa2d7c96d6f1a9d6944ac7668c7926ebc10c9f3b81b	false
+PREHOOK: query: SELECT
+  -- DECIMAL because of hive.compat=latest
+  cint1 / cint2,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  cint1 = ctimestamp,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  -- Interpreted as seconds because of hive.int.timestamp.conversion.in.seconds=true
+  CAST(cint1 AS TIMESTAMP),
+  -- formatter
+  DATE_FORMAT(ctimestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  FROM_UNIXTIME(cint1, 'yyyy-MM-dd HH:mm:ss u'),
+  TO_UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  -- SHA512 is used
+  MASK_HASH(text_timestamp),
+  -- Java's Pattern doesn't support it, then it fails with hive.use.googleregex.engine=false
+  text_timestamp RLIKE '\\p{Katakana}+'
+FROM test_udf_configurable
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_udf_configurable
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: SELECT
+  -- DECIMAL because of hive.compat=latest
+  cint1 / cint2,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  cint1 = ctimestamp,
+  -- Allowed by hive.strict.timestamp.conversion=false
+  -- Interpreted as seconds because of hive.int.timestamp.conversion.in.seconds=true
+  CAST(cint1 AS TIMESTAMP),
+  -- formatter
+  DATE_FORMAT(ctimestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  FROM_UNIXTIME(cint1, 'yyyy-MM-dd HH:mm:ss u'),
+  TO_UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  UNIX_TIMESTAMP(text_timestamp, 'yyyy-MM-dd HH:mm:ss u'),
+  -- SHA512 is used
+  MASK_HASH(text_timestamp),
+  -- Java's Pattern doesn't support it, then it fails with hive.use.googleregex.engine=false
+  text_timestamp RLIKE '\\p{Katakana}+'
+FROM test_udf_configurable
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_udf_configurable
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+3333.33333333333	false	1970-01-01 02:46:40	1970-01-01 01:02:03 4	1970-01-01 09:46:40 4	-21477	-21477	ca764b57c635a893c91b0edaea84bca142e065990e1a66b9c60cf855777af51070444210d30e359fa81e5a77c68a073224e0c7343d957f556816618a10baa37c	false
+4000.00000000000	false	1970-01-01 05:33:20	1970-01-02 04:05:06 5	1970-01-01 12:33:20 4	75906	75906	7882085a4ed36e7c675fcd12083eefa9a208f1fdcfba10b3a986bb5b1d43e23da3ba87b3933feebfed80ba9e05e4e73f72231cb8022453e562e034e8b5c603b9	false
+4285.71428571429	false	1970-01-01 08:20:00	1970-01-03 07:08:09 6	1970-01-01 15:20:00 4	173289	173289	d9899e12fce35a4f65cd63aee1e953dacd9e91229d52dd17ddad8e44c17d2d081c0a5519a96637663bc6baa2d7c96d6f1a9d6944ac7668c7926ebc10c9f3b81b	false
