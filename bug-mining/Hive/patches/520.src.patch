diff --git a/CHANGES.txt b/CHANGES.txt
index fb63f6b184..267a784d14 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -602,6 +602,9 @@ Trunk -  Unreleased
     HIVE-1843 Add an option in dynamic partition inserts to throw an error if
     0 partitions are created (Ning Zhang via namit)
 
+    HIVE-1846 Hive should not assume that local mode mappers run in same JVM
+    (rvadali via jssarma)
+
   TESTS
 
     HIVE-1464. improve  test query performance
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 6134b51319..25f7a06bb3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -169,7 +169,15 @@ public static MapredWork getMapRedWork(Configuration job) {
       assert jobID != null;
       gWork = gWorkMap.get(jobID);
       if (gWork == null) {
-        InputStream in = new FileInputStream("HIVE_PLAN" + jobID);
+        String jtConf = HiveConf.getVar(job, HiveConf.ConfVars.HADOOPJT);
+        String path;
+        if (jtConf == "local") {
+          String planPath = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);
+          path = new Path(planPath).toUri().getPath();
+        } else {
+          path = "HIVE_PLAN" + jobID;
+        }
+        InputStream in = new FileInputStream(path);
         MapredWork ret = deserializeMapRedWork(in, job);
         gWork = ret;
         gWork.initialize();
@@ -302,15 +310,15 @@ public static void setMapRedWork(Configuration job, MapredWork w, String hiveScr
       Path planPath = new Path(hiveScratchDir, jobID);
       HiveConf.setVar(job, HiveConf.ConfVars.PLAN, planPath.toUri().toString());
 
+      // use the default file system of the job
+      FileSystem fs = planPath.getFileSystem(job);
+      FSDataOutputStream out = fs.create(planPath);
+      serializeMapRedWork(w, out);
+
       // Serialize the plan to the default hdfs instance
       // Except for hadoop local mode execution where we should be
       // able to get the plan directly from the cache
       if (!HiveConf.getVar(job, HiveConf.ConfVars.HADOOPJT).equals("local")) {
-        // use the default file system of the job
-        FileSystem fs = planPath.getFileSystem(job);
-        FSDataOutputStream out = fs.create(planPath);
-        serializeMapRedWork(w, out);
-
         // Set up distributed cache
         DistributedCache.createSymlink(job);
         String uriWithLink = planPath.toUri().toString() + "#HIVE_PLAN" + jobID;
