diff --git a/CHANGES.txt b/CHANGES.txt
index 57374e0c76..417383d20d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -25,6 +25,8 @@ Trunk (unreleased changes)
 
   BUG FIXES
 
+    HIVE-77. Thread safe query execution. (Joydeep through zshao)
+
     HIVE-86. Drop table should not delete data for external tables.
     (Johan Oskarsson through zshao)
 
diff --git a/build-common.xml b/build-common.xml
index 88cf1453d0..e99d12f0cf 100644
--- a/build-common.xml
+++ b/build-common.xml
@@ -52,7 +52,6 @@
   <property name="test.build.classes" value="${test.build.dir}/classes"/>
   <property name="test.build.javadoc" value="${test.build.dir}/docs/api"/>
   <property name="test.include" value="Test*"/>
-  <property name="test.exclude" value="TestSerDe"/>
   <property name="test.classpath.id" value="test.classpath"/>
   <property name="test.output" value="true"/>
   <property name="test.timeout" value="900000"/>
@@ -226,12 +225,15 @@
       <sysproperty key="log4j.configuration" value="file://${test.data.dir}/conf/hive-log4j.properties"/>
       <sysproperty key="derby.stream.error.file" value="${test.build.dir}/derby.log"/>
       <sysproperty key="hive.aux.jars.path" value="${build.dir.hive}/ql/test/test-udfs.jar,${test.src.data.dir}/files/TestSerDe.jar"/>
+      <sysproperty key="ql.test.query.clientpositive.dir" value="${ql.test.query.clientpositive.dir}"/>
+      <sysproperty key="ql.test.results.clientpositive.dir" value="${ql.test.results.clientpositive.dir}"/>
+
       <classpath refid="${test.classpath.id}"/>
       <formatter type="${test.junit.output.format}" usefile="${test.junit.output.usefile}" />
       <batchtest todir="${test.build.dir}" unless="testcase">
         <fileset dir="${test.build.classes}"
                  includes="**/${test.include}.class"
-                 excludes="**/${test.exclude}.class" />
+                 excludes="**/TestMTQueries.class,**/TestSerDe.class" />
       </batchtest>
       <batchtest todir="${test.build.dir}" if="testcase">
         <fileset dir="${test.build.classes}" includes="**/${testcase}.class"/>
diff --git a/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java b/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
index 9ad0b53c36..a61afedb9a 100644
--- a/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
+++ b/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
@@ -24,6 +24,7 @@
 import java.util.*;
 
 import org.apache.hadoop.fs.FsShell;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.exec.Utilities.StreamPrinter;
@@ -39,31 +40,22 @@ public class CliDriver {
   public final static String prompt = "hive";
   public final static String prompt2 = "    "; // when ';' is not yet seen
 
-  public static SetProcessor sp;
-  public static Driver qp;
-  public static FsShell dfs;
-  public static Log LOG = LogFactory.getLog("CliDriver");
-
-  /**
-   * delay console initialization until session has been initialized
-   */
-  public static LogHelper console;
-  public static LogHelper getConsole() {
-    if(console == null)
-      console = new LogHelper(LOG);
-    return (console);
-  }
-  
-  public CliDriver(CliSessionState ss) {
-    SessionState.start(ss);
+  private SetProcessor sp;
+  private Driver qp;
+  private FsShell dfs;
+  private LogHelper console;
+
+  public CliDriver() {
+    SessionState ss = SessionState.get();
     sp = new SetProcessor();
     qp = new Driver();
+    dfs = new FsShell(ss != null ? ss.getConf() : new Configuration ());
+    Log LOG = LogFactory.getLog("CliDriver");
+    console = new LogHelper(LOG);
   }
   
-  public static int processCmd(String cmd) {
-
+  public int processCmd(String cmd) {
     SessionState ss = SessionState.get();
-    LogHelper console = getConsole();
 
     String[] tokens = cmd.split("\\s+");
     String cmd_1 = cmd.substring(tokens[0].length());
@@ -106,10 +98,6 @@ public static int processCmd(String cmd) {
 
     } else if (tokens[0].toLowerCase().equals("dfs")) {
 
-      // dfs shell commands
-      if(dfs == null)
-        dfs = new FsShell(ss.getConf());
-
       String [] alt_tokens = new String [tokens.length-1];
       System.arraycopy(tokens, 1, alt_tokens, 0, tokens.length-1);
       tokens = alt_tokens;
@@ -180,6 +168,8 @@ public static int processCmd(String cmd) {
     } else {
       PrintStream out = ss.out;
 
+      long start = System.currentTimeMillis();
+
       ret = qp.run(cmd);
       Vector<String> res = new Vector<String>();
       while (qp.getResults(res)) {
@@ -193,12 +183,18 @@ public static int processCmd(String cmd) {
       if (ret == 0) {
         ret = cret;
       }
+
+      long end = System.currentTimeMillis();
+      if (end > start) {
+        double timeTaken = (double)(end-start)/1000.0;
+        console.printInfo("Time taken: " + timeTaken + " seconds", null);
+      }
     }
 
     return ret;
   }
 
-  public static int processLine(String line) {
+  public int processLine(String line) {
     int ret = 0;
     for(String oneCmd: line.split(";")) {
       oneCmd = oneCmd.trim();
@@ -214,7 +210,7 @@ public static int processLine(String line) {
     return 0;
   }
 
-  public static int processReader(BufferedReader r) throws IOException {
+  public int processReader(BufferedReader r) throws IOException {
     String line;
     int ret = 0;
     while((line = r.readLine()) != null) {
@@ -258,17 +254,15 @@ public static void main(String[] args) throws IOException {
       conf.set((String) item.getKey(), (String) item.getValue());
     }
 
-    sp = new SetProcessor();
-    qp = new Driver();
-    dfs = new FsShell(ss.getConf());
+    CliDriver cli = new CliDriver ();
 
     if(ss.execString != null) {
-      System.exit(processLine(ss.execString));
+      System.exit(cli.processLine(ss.execString));
     }
 
     try {
       if(ss.fileName != null) {
-        System.exit(processReader(new BufferedReader(new FileReader(ss.fileName))));
+        System.exit(cli.processReader(new BufferedReader(new FileReader(ss.fileName))));
       }
     } catch (FileNotFoundException e) {
       System.err.println("Could not open input file for reading. ("+e.getMessage()+")");
@@ -298,10 +292,9 @@ public static void main(String[] args) throws IOException {
     String prefix = "";
     String curPrompt = prompt;
     while ((line = reader.readLine(curPrompt+"> ")) != null) {
-      long start = System.currentTimeMillis();
       if(line.trim().endsWith(";")) {
         line = prefix + " " + line;
-        ret = processLine(line);
+        ret = cli.processLine(line);
         prefix = "";
         curPrompt = prompt;
       } else {
@@ -309,11 +302,6 @@ public static void main(String[] args) throws IOException {
         curPrompt = prompt2;
         continue;
       }
-      long end = System.currentTimeMillis();
-      if (end > start) {
-        double timeTaken = (double)(end-start)/1000.0;
-        getConsole().printInfo("Time taken: " + timeTaken + " seconds", null);
-      }
     }
 
     System.exit(ret);
diff --git a/ql/build.xml b/ql/build.xml
index 7b9a62fda8..a350222578 100644
--- a/ql/build.xml
+++ b/ql/build.xml
@@ -26,6 +26,9 @@
   <property name="ql.test.template.dir" location="${basedir}/src/test/templates"/>
   <property name="ql.test.results.dir" location="${basedir}/src/test/results"/>
 
+  <property name="ql.test.query.clientpositive.dir" location="${ql.test.query.dir}/clientpositive"/>
+  <property name="ql.test.results.clientpositive.dir" location="${ql.test.results.dir}/clientpositive"/>
+
   <import file="../build-common.xml"/>
 
   <path id="test.classpath">
@@ -59,9 +62,9 @@
 
     <qtestgen outputDirectory="${test.build.src}/org/apache/hadoop/hive/cli" 
               templatePath="${ql.test.template.dir}" template="TestCliDriver.vm" 
-              queryDirectory="${ql.test.query.dir}/clientpositive" 
+              queryDirectory="${ql.test.query.clientpositive.dir}" 
               queryFile="${qfile}"
-              resultsDirectory="${ql.test.results.dir}/clientpositive" className="TestCliDriver"
+              resultsDirectory="${ql.test.results.clientpositive.dir}" className="TestCliDriver"
               logFile="${test.log.dir}/testclidrivergen.log"/>
 
     <qtestgen outputDirectory="${test.build.src}/org/apache/hadoop/hive/cli" 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
index 9d62b68999..84a5e9384b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
@@ -415,24 +415,47 @@ public static void main(String[] args) throws IOException, HiveException {
    * fragment for passing such configuration information to ExecDriver
    */
   public static String generateCmdLine(HiveConf hconf) {
-    StringBuilder sb = new StringBuilder ();
-    Properties deltaP = hconf.getChangedProperties();
+    try {
+      StringBuilder sb = new StringBuilder ();
+      Properties deltaP = hconf.getChangedProperties();
+      boolean localMode = hconf.getVar(HiveConf.ConfVars.HADOOPJT).equals("local");
+      String hadoopSysDir = "mapred.system.dir";
+      String hadoopWorkDir = "mapred.local.dir";
+
+      for(Object one: deltaP.keySet()) {
+        String oneProp = (String)one;
+      
+        if(localMode && (oneProp.equals(hadoopSysDir) || oneProp.equals(hadoopWorkDir)))
+          continue;
 
-    for(Object one: deltaP.keySet()) {
-      String oneProp = (String)one;
-      String oneValue = deltaP.getProperty(oneProp);
+        String oneValue = deltaP.getProperty(oneProp);
 
-      sb.append("-jobconf ");
-      sb.append(oneProp);
-      sb.append("=");
-      try {
+        sb.append("-jobconf ");
+        sb.append(oneProp);
+        sb.append("=");
         sb.append(URLEncoder.encode(oneValue, "UTF-8"));
-      } catch (UnsupportedEncodingException e) {
-        throw new RuntimeException(e);
+        sb.append(" ");
       }
-      sb.append(" ");
+
+      // Multiple concurrent local mode job submissions can cause collisions in working dirs
+      // Workaround is to rename map red working dir to a temp dir in such a case
+      if(localMode) {
+        sb.append("-jobconf ");
+        sb.append(hadoopSysDir);
+        sb.append("=");
+        sb.append(URLEncoder.encode(hconf.get(hadoopSysDir) + "/" + Utilities.randGen.nextInt(), "UTF-8"));
+
+        sb.append(" ");
+        sb.append("-jobconf ");
+        sb.append(hadoopWorkDir);
+        sb.append("=");
+        sb.append(URLEncoder.encode(hconf.get(hadoopWorkDir) + "/" + Utilities.randGen.nextInt(), "UTF-8"));
+      }
+
+      return sb.toString();
+    } catch (UnsupportedEncodingException e) {
+      throw new RuntimeException(e);
     }
-    return sb.toString();
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
index 9f78b32083..740013dc1d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
@@ -41,7 +41,6 @@ public taskTuple(Class<T> workClass, Class<? extends Task<T>> taskClass) {
 
   public static ArrayList<taskTuple<? extends Serializable>> taskvec;
   static {
-    id = 0;
     taskvec = new ArrayList<taskTuple<? extends Serializable>>();
     taskvec.add(new taskTuple<moveWork>(moveWork.class, MoveTask.class));
     taskvec.add(new taskTuple<fetchWork>(fetchWork.class, FetchTask.class));
@@ -54,10 +53,21 @@ public taskTuple(Class<T> workClass, Class<? extends Task<T>> taskClass) {
     // taskvec.add(new taskTuple<mapredWork>(mapredWork.class, ExecDriver.class));
   }
 
-  private static int id;
+  private static ThreadLocal<Integer> tid = new ThreadLocal<Integer> () {
+    protected synchronized Integer initialValue() {
+        return new Integer(0);
+      }
+  };
+
+  public static int getAndIncrementId() {
+    int curValue = tid.get().intValue();
+    tid.set(new Integer(curValue+1));
+    return curValue;
+  }
+
   
   public static void resetId() {
-    id = 0;
+    tid.set(new Integer(0));
   }
   
   @SuppressWarnings("unchecked")
@@ -67,7 +77,7 @@ public static <T extends Serializable> Task<T> get(Class<T> workClass, HiveConf
       if(t.workClass == workClass) {
         try {
           Task<T> ret = (Task<T>)t.taskClass.newInstance();
-          ret.setId("Stage-" + Integer.toString(id++));
+          ret.setId("Stage-" + Integer.toString(getAndIncrementId()));
           return ret;
         } catch (Exception e) {
           throw new RuntimeException(e);
@@ -90,7 +100,7 @@ public static <T extends Serializable> Task<T> get(Class<T> workClass, HiveConf
         } else {
           ret = (Task<T>)ExecDriver.class.newInstance();
         }
-        ret.setId("Stage-" + Integer.toString(id++));
+        ret.setId("Stage-" + Integer.toString(getAndIncrementId()));
         return ret;
       } catch (Exception e) {
         throw new RuntimeException (e.getMessage(), e);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 3276aceb69..85382ec65c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -34,11 +34,15 @@
 
 import org.apache.commons.lang.StringUtils;
 
+/**
+ * SessionState encapsulates common data associated with a session
+ * 
+ * Also provides support for a thread static session object that can
+ * be accessed from any point in the code to interact with the user
+ * and to retrieve configuration information
+ */
 public class SessionState {
 
-  public static Log LOG = LogFactory.getLog("SessionState");
-  public static LogHelper console = new LogHelper(LOG);
-
   /**
    * current configuration
    */ 
@@ -137,41 +141,42 @@ public String getSessionId() {
     return (conf.getVar(HiveConf.ConfVars.HIVESESSIONID));
   }
 
-
   /**
-   * Singleton Session object
+   * Singleton Session object per thread.
    *
-   * For multiple sessions - we could store in a hashmap or have a thread local var
    **/
-  private static SessionState ss;
+  private static ThreadLocal<SessionState> tss = new ThreadLocal<SessionState> ();
 
   /**
-   * start a new session
+   * start a new session and set it to current session
    */
   public static SessionState start(HiveConf conf) {
-    ss = new SessionState (conf);
+    SessionState ss = new SessionState (conf);
     ss.getConf().setVar(HiveConf.ConfVars.HIVESESSIONID, makeSessionId());
-    console = new LogHelper(LOG);
+    tss.set(ss);
     return (ss);
   }
 
+  /**
+   * set current session to existing session object
+   * if a thread is running multiple sessions - it must call this method with the new
+   * session object when switching from one session to another
+   */
   public static SessionState start(SessionState startSs) {
-    ss = startSs;
-    console = new LogHelper(LOG);
-    ss.getConf().setVar(HiveConf.ConfVars.HIVESESSIONID, makeSessionId());
-    return ss;
+    tss.set(startSs);
+    if(StringUtils.isEmpty(startSs.getConf().getVar(HiveConf.ConfVars.HIVESESSIONID))) {
+      startSs.getConf().setVar(HiveConf.ConfVars.HIVESESSIONID, makeSessionId());
+    }
+    return startSs;
   }
 
   /**
    * get the current session
    */
   public static SessionState get() {
-    return ss;
+    return tss.get();
   }
 
-  public static LogHelper getConsole() {
-    return console;
-  }
 
   private static String makeSessionId() {
     GregorianCalendar gc = new GregorianCalendar();
@@ -198,37 +203,42 @@ public static void initHiveLog4j () {
     }
   }
 
+  /**
+   * This class provides helper routines to emit informational and error messages to the user
+   * and log4j files while obeying the current session's verbosity levels.
+   * 
+   * NEVER write directly to the SessionStates standard output other than to emit result data
+   * DO use printInfo and printError provided by LogHelper to emit non result data strings
+   * 
+   * It is perfectly acceptable to have global static LogHelper objects (for example - once per module)
+   * LogHelper always emits info/error to current session as required.
+   */
   public static class LogHelper {
 
     protected Log LOG;
     protected boolean isSilent;
-    protected SessionState ss;
     
-    public LogHelper(SessionState ss, boolean isSilent, Log LOG) {
-      this.LOG = LOG;
-      this.isSilent = isSilent;
-      this.ss = ss;
-    }
-
     public LogHelper(Log LOG) {
-      // the session control silent or not
-      this(SessionState.get(), false, LOG);
+      this(LOG, false);
     }
 
     public LogHelper(Log LOG, boolean isSilent) {
-      // no session info - use isSilent setting passed in
-      this(null, isSilent, LOG);
+      this.LOG = LOG;
+      this.isSilent = isSilent;
     }
 
     public PrintStream getOutStream() {
+      SessionState ss = SessionState.get();
       return ((ss != null) && (ss.out != null)) ? ss.out : System.out;   
     }
 
     public PrintStream getErrStream() {
+      SessionState ss = SessionState.get();
       return ((ss != null) && (ss.err != null)) ? ss.err : System.err;
     }
 
     public boolean getIsSilent() {
+      SessionState ss = SessionState.get();
       // use the session or the one supplied in constructor
       return (ss != null) ? ss.getIsSilent() : isSilent;
     }
@@ -254,9 +264,21 @@ public void printError(String error, String detail) {
     }
   }
 
+  private static LogHelper _console;
+  /**
+   * initialize or retrieve console object for SessionState
+   */
+  private static LogHelper getConsole() {
+    if(_console == null) {
+      Log LOG = LogFactory.getLog("SessionState");
+      _console = new LogHelper(LOG);
+    }
+    return _console;
+  }
+
   public static String validateFile(Set<String> curFiles, String newFile) {
     SessionState ss = SessionState.get();
-    LogHelper console = SessionState.getConsole();
+    LogHelper console = getConsole();
     Configuration conf = (ss == null) ? new Configuration() : ss.getConf();
 
     try {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
index 46293a4d7f..8bce04c3bb 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -173,11 +173,18 @@ public QTestUtil(String outDir) throws Exception {
 
     qMap = new TreeMap<String, String>();
     srcTables = new LinkedList<String>();
+    init();
   }
+  
+
 
   public void addFile(String qFile) throws Exception {
 
     File qf = new File(qFile);
+    addFile(qf);
+  }
+
+  public void addFile(File qf) throws Exception {
 
     FileInputStream fis = new FileInputStream(qf);
     BufferedInputStream bis = new BufferedInputStream(fis);
@@ -194,19 +201,15 @@ public void addFile(String qFile) throws Exception {
   public void cleanUp() throws Exception {
     String warehousePath = ((new URI(testWarehouse)).getPath());
     // Drop any tables that remain due to unsuccessful runs
-    db.dropTable("src", true, true);
-    db.dropTable("src1", true, true);
-    db.dropTable("src_thrift", true, true);
-    db.dropTable("src_sequencefile", true, true);
-    db.dropTable("srcpart", true, true);
-    db.dropTable("srcbucket", true, true);
-    db.dropTable("dest1", true, true);
-    db.dropTable("dest2", true, true);
-    db.dropTable("dest3", true, true);
-    db.dropTable("dest4", true, true);
-    db.dropTable("dest4_sequencefile", true, true);
-    deleteDirectory(new File(warehousePath, "dest4.out"));
-    deleteDirectory(new File(warehousePath, "union.out"));    
+    for(String s: new String [] {"src", "src1", "src_thrift", "src_sequencefile", 
+                                 "srcpart", "srcbucket", "dest1", "dest2", 
+                                 "dest3", "dest4", "dest4_sequencefile",
+                                 "dest_j1", "dest_j2", "dest_g1", "dest_g2"}) {
+      db.dropTable(s);
+    }
+    for(String s: new String [] {"dest4.out", "union.out"}) {
+      deleteDirectory(new File(warehousePath, s));
+    }
   }
 
   public void createSources() throws Exception {
@@ -252,26 +255,21 @@ public void createSources() throws Exception {
     bucketCols.add("key");
     db.createTable("srcbucket", cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class, 2, bucketCols);
     srcTables.add("srcbucket");
-    fpath = new Path(testFiles, "kv1.txt");
-    newfpath = new Path(tmppath, "kv1.txt");
-    fs.copyFromLocalFile(false, true, fpath, newfpath);
-    loadCmd = "LOAD DATA INPATH '" +  newfpath.toString() + "' INTO TABLE srcbucket";
-    ecode = drv.run(loadCmd);
-    if(ecode != 0) {
-      throw new Exception("load command: " + loadCmd + " failed with exit code= " + ecode);
+    for (String fname: new String [] {"kv1.txt", "kv2.txt"}) {
+      fpath = new Path(testFiles, fname);
+      newfpath = new Path(tmppath, fname);
+      fs.copyFromLocalFile(false, true, fpath, newfpath);
+      loadCmd = "LOAD DATA INPATH '" +  newfpath.toString() + "' INTO TABLE srcbucket";
+      ecode = drv.run(loadCmd);
+      if(ecode != 0) {
+        throw new Exception("load command: " + loadCmd + " failed with exit code= " + ecode);
+      }
     }
-    fpath = new Path(testFiles, "kv2.txt");
-    newfpath = new Path(tmppath, "kv2.txt");
-    fs.copyFromLocalFile(false, true, fpath, newfpath);
-    loadCmd = "LOAD DATA INPATH '" +  newfpath.toString() + "' INTO TABLE srcbucket";
-    ecode = drv.run(loadCmd);
-    if(ecode != 0) {
-      throw new Exception("load command: " + loadCmd + " failed with exit code= " + ecode);
+    
+    for (String tname: new String [] {"src", "src1"}) {
+      db.createTable(tname, cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
+      srcTables.add(tname);
     }
-    db.createTable("src", cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
-    srcTables.add("src");
-    db.createTable("src1", cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
-    srcTables.add("src1");
     db.createTable("src_sequencefile", cols, null, SequenceFileInputFormat.class, SequenceFileOutputFormat.class);
     srcTables.add("src_sequencefile");
     
@@ -328,22 +326,20 @@ public void init() throws Exception {
   }
 
   public void init(String tname) throws Exception {
-
-    init();
     cleanUp();
     createSources();
 
     LinkedList<String> cols = new LinkedList<String>();
     cols.add("key");
     cols.add("value");
-    
+   
     LinkedList<String> part_cols = new LinkedList<String>();
     part_cols.add("ds");
     part_cols.add("hr");
 
     db.createTable("dest1", cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
     db.createTable("dest2", cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
-    
+   
     db.createTable("dest3", cols, part_cols, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
     Table dest3 = db.getTable("dest3");
 
@@ -351,18 +347,22 @@ public void init(String tname) throws Exception {
     part_spec.put("ds", "2008-04-08");
     part_spec.put("hr", "12");
     db.createPartition(dest3, part_spec);
-    
+   
     db.createTable("dest4", cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
     db.createTable("dest4_sequencefile", cols, null, SequenceFileInputFormat.class, SequenceFileOutputFormat.class);
   }
 
   public void cliInit(String tname) throws Exception {
-    
-    init();
-    cleanUp();
-    createSources();
+    cliInit(tname, true);
+  }
+
+  public void cliInit(String tname, boolean recreate) throws Exception {
+    if(recreate) {
+      cleanUp();
+      createSources();
+    }
 
-    CliSessionState ss = new CliSessionState(new HiveConf(SessionState.class));
+    CliSessionState ss = new CliSessionState(conf);
 
     ss.in = System.in;
 
@@ -374,8 +374,22 @@ public void cliInit(String tname) throws Exception {
     ss.out = new PrintStream(fo, true, "UTF-8");
     ss.err = ss.out;
     ss.setIsSilent(true);
-    cliDriver = new CliDriver(ss);
     SessionState.start(ss);
+    cliDriver = new CliDriver();
+  }
+
+  public int executeOne(String tname) {
+    String q = qMap.get(tname);
+
+    if(q.indexOf(";") == -1)
+      return -1;
+
+    String q1 = q.substring(0, q.indexOf(";") + 1);
+    String qrest = q.substring(q.indexOf(";")+1);
+    qMap.put(tname, qrest);
+
+    System.out.println("Executing " + q1);
+    return cliDriver.processLine(q1);
   }
 
   public int execute(String tname) {
@@ -383,7 +397,7 @@ public int execute(String tname) {
   }
 
   public int executeClient(String tname) {
-    return CliDriver.processLine(qMap.get(tname));
+    return cliDriver.processLine(qMap.get(tname));
   }
 
   public void convertSequenceFileToTextFile() throws Exception {
@@ -672,5 +686,110 @@ public List<Task<? extends Serializable>> analyzeAST(CommonTree ast) throws Exce
   public TreeMap<String, String> getQMap() {
     return qMap;
   }
+
+
+  /**
+   * QTRunner: Runnable class for running a a single query file
+   * 
+   **/
+  public static class QTRunner implements Runnable {
+    private QTestUtil qt;
+    private String fname;
+
+    public QTRunner(QTestUtil qt, String fname) {
+      this.qt = qt;
+      this.fname = fname;
+    }
+    
+    public void run() {
+      try {
+        // assumption is that environment has already been cleaned once globally
+        // hence each thread does not call cleanUp() and createSources() again
+        qt.cliInit(fname, false);
+        /*
+          XXX Ugly hack - uncomment this to test without DDLs.
+          Should be removed once DDL/metastore mt issues are resolved
+          synchronized (this.getClass()) {
+          qt.executeOne(fname);
+          }
+        */
+        qt.executeClient(fname);
+      } catch (Throwable e) {
+        System.err.println("Query file " + fname + " failed with exception " + e.getMessage());
+        e.printStackTrace();
+        System.err.flush();
+      }
+    }
+  }
+
+  /**
+   * executes a set of query files either in sequence or in parallel.
+   * Uses QTestUtil to do so
+   *
+   * @param qfiles array of input query files containing arbitrary number of hive queries
+   * @param resDirs array of output directories one corresponding to each input query file
+   * @param mt whether to run in multithreaded mode or not
+   * @return true if all the query files were executed successfully, else false
+   *
+   * In multithreaded mode each query file is run in a separate thread. the caller has to 
+   * arrange that different query files do not collide (in terms of destination tables)
+   */
+  public static boolean queryListRunner(File [] qfiles, String [] resDirs, boolean mt) {
+
+    assert(qfiles.length == resDirs.length);
+    boolean failed = false;        
+
+    try {
+      QTestUtil[] qt = new QTestUtil [qfiles.length];
+      for(int i=0; i<qfiles.length; i++) {
+        qt[i] = new QTestUtil(resDirs[i]);
+        qt[i].addFile(qfiles[i]);
+      }
+
+      if (mt) {
+        // in multithreaded mode - do cleanup/initialization just once
+
+        qt[0].cleanUp();
+        qt[0].createSources();
+
+        QTRunner [] qtRunners = new QTestUtil.QTRunner [qfiles.length];
+        Thread [] qtThread = new Thread [qfiles.length];
+
+        for(int i=0; i<qfiles.length; i++) {
+          qtRunners[i] = new QTestUtil.QTRunner (qt[i], qfiles[i].getName());
+          qtThread[i] = new Thread (qtRunners[i]);
+        }
+
+        for(int i=0; i<qfiles.length; i++) {
+          qtThread[i].start();
+        }
+
+        for(int i=0; i<qfiles.length; i++) {
+          qtThread[i].join();
+          int ecode = qt[i].checkCliDriverResults(qfiles[i].getName());
+          if (ecode != 0) {
+            failed = true;
+            System.err.println("Test " + qfiles[i].getName() + " results check failed with error code " + ecode);
+          }
+        }
+
+      } else {
+        
+        for(int i=0; i<qfiles.length && !failed; i++) {
+          qt[i].cliInit(qfiles[i].getName());
+          qt[i].executeClient(qfiles[i].getName());
+          int ecode = qt[i].checkCliDriverResults(qfiles[i].getName());
+          if (ecode != 0) {
+            failed = true;
+            System.err.println("Test " + qfiles[i].getName() + " results check failed with error code " + ecode);
+          }
+        }
+      }
+    } catch (Exception e) {
+      e.printStackTrace();
+      return false;
+    }
+    return (!failed);
+  }
 }
 
diff --git a/ql/src/test/queries/clientpositive/groupby1.q b/ql/src/test/queries/clientpositive/groupby1.q
index ccca48d61b..2c65e4b441 100755
--- a/ql/src/test/queries/clientpositive/groupby1.q
+++ b/ql/src/test/queries/clientpositive/groupby1.q
@@ -1,8 +1,8 @@
-CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE;
+CREATE TABLE dest_g1(key INT, value DOUBLE) STORED AS TEXTFILE;
 
 EXPLAIN
-FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;
+FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;
 
-FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;
+FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;
 
-SELECT dest1.* FROM dest1;
+SELECT dest_g1.* FROM dest_g1;
diff --git a/ql/src/test/queries/clientpositive/groupby2.q b/ql/src/test/queries/clientpositive/groupby2.q
index 8134ba450c..90c95647c3 100755
--- a/ql/src/test/queries/clientpositive/groupby2.q
+++ b/ql/src/test/queries/clientpositive/groupby2.q
@@ -1,10 +1,10 @@
-CREATE TABLE dest1(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;
+CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;
 
 EXPLAIN
 FROM src
-INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,0,1), count(DISTINCT substr(src.value,4)), concat(substr(src.key,0,1),sum(substr(src.value,4))) GROUP BY substr(src.key,0,1);
+INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,0,1), count(DISTINCT substr(src.value,4)), concat(substr(src.key,0,1),sum(substr(src.value,4))) GROUP BY substr(src.key,0,1);
 
 FROM src
-INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,0,1), count(DISTINCT substr(src.value,4)), concat(substr(src.key,0,1),sum(substr(src.value,4))) GROUP BY substr(src.key,0,1);
+INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,0,1), count(DISTINCT substr(src.value,4)), concat(substr(src.key,0,1),sum(substr(src.value,4))) GROUP BY substr(src.key,0,1);
 
-SELECT dest1.* FROM dest1;
+SELECT dest_g2.* FROM dest_g2;
diff --git a/ql/src/test/queries/clientpositive/join1.q b/ql/src/test/queries/clientpositive/join1.q
index 63dd104b42..54fab8c7d0 100644
--- a/ql/src/test/queries/clientpositive/join1.q
+++ b/ql/src/test/queries/clientpositive/join1.q
@@ -1,10 +1,10 @@
-CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
+CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;
 
 EXPLAIN
 FROM src src1 JOIN src src2 ON (src1.key = src2.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value;
+INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;
 
 FROM src src1 JOIN src src2 ON (src1.key = src2.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value;
+INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;
 
-SELECT dest1.* FROM dest1;
+SELECT dest_j1.* FROM dest_j1;
diff --git a/ql/src/test/queries/clientpositive/join2.q b/ql/src/test/queries/clientpositive/join2.q
index c1d66f0b90..7bb547bcb9 100644
--- a/ql/src/test/queries/clientpositive/join2.q
+++ b/ql/src/test/queries/clientpositive/join2.q
@@ -1,10 +1,10 @@
-CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
+CREATE TABLE dest_j2(key INT, value STRING) STORED AS TEXTFILE;
 
 EXPLAIN
 FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;
+INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;
 
 FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;
+INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;
 
-SELECT dest1.* FROM dest1;
+SELECT dest_j2.* FROM dest_j2;
diff --git a/ql/src/test/results/clientpositive/groupby1.q.out b/ql/src/test/results/clientpositive/groupby1.q.out
index b65d13c313..4fb11b03c0 100644
--- a/ql/src/test/results/clientpositive/groupby1.q.out
+++ b/ql/src/test/results/clientpositive/groupby1.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_FUNCTION substr (TOK_COLREF src value) 4)))) (TOK_GROUPBY (TOK_COLREF src key))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_g1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_FUNCTION substr (TOK_COLREF src value) 4)))) (TOK_GROUPBY (TOK_COLREF src key))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -41,7 +41,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-jssarma/669676736/1737584634.10001 
+        /tmp/hive-jssarma/69808444/374659791.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
@@ -74,7 +74,7 @@ STAGE PLANS:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                   serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                  name: dest1
+                  name: dest_g1
 
   Stage: Stage-0
     Move Operator
@@ -84,7 +84,7 @@ STAGE PLANS:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                 serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                name: dest1
+                name: dest_g1
 
 
 0	0.0
diff --git a/ql/src/test/results/clientpositive/groupby2.q.out b/ql/src/test/results/clientpositive/groupby2.q.out
index 8cc02690e1..f2ee952832 100644
--- a/ql/src/test/results/clientpositive/groupby2.q.out
+++ b/ql/src/test/results/clientpositive/groupby2.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION substr (TOK_COLREF src key) 0 1)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_FUNCTION substr (TOK_COLREF src value) 4))) (TOK_SELEXPR (TOK_FUNCTION concat (TOK_FUNCTION substr (TOK_COLREF src key) 0 1) (TOK_FUNCTION sum (TOK_FUNCTION substr (TOK_COLREF src value) 4))))) (TOK_GROUPBY (TOK_FUNCTION substr (TOK_COLREF src key) 0 1))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_g2)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION substr (TOK_COLREF src key) 0 1)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_FUNCTION substr (TOK_COLREF src value) 4))) (TOK_SELEXPR (TOK_FUNCTION concat (TOK_FUNCTION substr (TOK_COLREF src key) 0 1) (TOK_FUNCTION sum (TOK_FUNCTION substr (TOK_COLREF src value) 4))))) (TOK_GROUPBY (TOK_FUNCTION substr (TOK_COLREF src key) 0 1))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -43,7 +43,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-jssarma/836051436/1775451122.10001 
+        /tmp/hive-jssarma/190784876/1532193204.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
@@ -81,7 +81,7 @@ STAGE PLANS:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                   serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                  name: dest1
+                  name: dest_g2
 
   Stage: Stage-0
     Move Operator
@@ -91,7 +91,7 @@ STAGE PLANS:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                 serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                name: dest1
+                name: dest_g2
 
 
 0	1	00.0
diff --git a/ql/src/test/results/clientpositive/join1.q.out b/ql/src/test/results/clientpositive/join1.q.out
index 2c161b599d..55811babd4 100644
--- a/ql/src/test/results/clientpositive/join1.q.out
+++ b/ql/src/test/results/clientpositive/join1.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF src src1) (TOK_TABREF src src2) (= (TOK_COLREF src1 key) (TOK_COLREF src2 key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src1 key)) (TOK_SELEXPR (TOK_COLREF src2 value)))))
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF src src1) (TOK_TABREF src src2) (= (TOK_COLREF src1 key) (TOK_COLREF src2 key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_j1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src1 key)) (TOK_SELEXPR (TOK_COLREF src2 value)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -60,7 +60,7 @@ STAGE PLANS:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                   serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                  name: dest1
+                  name: dest_j1
 
   Stage: Stage-0
     Move Operator
@@ -70,7 +70,7 @@ STAGE PLANS:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                 serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                name: dest1
+                name: dest_j1
 
 
 0	val_0
diff --git a/ql/src/test/results/clientpositive/join2.q.out b/ql/src/test/results/clientpositive/join2.q.out
index dd6939ce9a..8b195f2a9a 100644
--- a/ql/src/test/results/clientpositive/join2.q.out
+++ b/ql/src/test/results/clientpositive/join2.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF src src1) (TOK_TABREF src src2) (= (TOK_COLREF src1 key) (TOK_COLREF src2 key))) (TOK_TABREF src src3) (= (+ (TOK_COLREF src1 key) (TOK_COLREF src2 key)) (TOK_COLREF src3 key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src1 key)) (TOK_SELEXPR (TOK_COLREF src3 value)))))
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF src src1) (TOK_TABREF src src2) (= (TOK_COLREF src1 key) (TOK_COLREF src2 key))) (TOK_TABREF src src3) (= (+ (TOK_COLREF src1 key) (TOK_COLREF src2 key)) (TOK_COLREF src3 key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_j2)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src1 key)) (TOK_SELEXPR (TOK_COLREF src3 value)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -110,7 +110,7 @@ STAGE PLANS:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                   serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                  name: dest1
+                  name: dest_j2
 
   Stage: Stage-0
     Move Operator
@@ -120,7 +120,7 @@ STAGE PLANS:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                 serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
-                name: dest1
+                name: dest_j2
 
 
 0	val_0
