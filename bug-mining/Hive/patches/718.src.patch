diff --git a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java
index f5a56d86d6..4173796128 100644
--- a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java
+++ b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java
@@ -76,7 +76,7 @@ public HiveConnection(String uri, Properties info) throws SQLException {
         client = new HiveServer.HiveServerHandler();
       } catch (MetaException e) {
         throw new SQLException("Error accessing Hive metastore: "
-            + e.getMessage(), "08S01");
+            + e.getMessage(), "08S01",e);
       }
     } else {
       // parse uri
diff --git a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDataSource.java b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDataSource.java
index 39d6824fb3..21599ff356 100644
--- a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDataSource.java
+++ b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDataSource.java
@@ -58,7 +58,7 @@ public Connection getConnection(String username, String password)
     try {
       return new HiveConnection("", null);
     } catch (Exception ex) {
-      throw new SQLException();
+      throw new SQLException("Error in getting HiveConnection",ex);
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
index b6ad5e2b4a..fec278aa24 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
@@ -34,14 +34,14 @@
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.Date;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.HashMap;
+import java.util.Map.Entry;
 import java.util.Set;
 import java.util.SortedSet;
 import java.util.TreeSet;
-import java.util.Map.Entry;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -98,6 +98,7 @@
 import org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc;
 import org.apache.hadoop.hive.ql.plan.AlterIndexDesc;
 import org.apache.hadoop.hive.ql.plan.AlterTableDesc;
+import org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableTypes;
 import org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc;
 import org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc;
 import org.apache.hadoop.hive.ql.plan.CreateIndexDesc;
@@ -131,7 +132,6 @@
 import org.apache.hadoop.hive.ql.plan.ShowTablesDesc;
 import org.apache.hadoop.hive.ql.plan.SwitchDatabaseDesc;
 import org.apache.hadoop.hive.ql.plan.UnlockTableDesc;
-import org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableTypes;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.security.authorization.Privilege;
 import org.apache.hadoop.hive.serde.Constants;
@@ -837,7 +837,7 @@ private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException
     Index idx = db.getIndex(dbName, baseTableName, indexName);
 
     switch(alterIndex.getOp()) {
-      case ADDPROPS: 
+      case ADDPROPS:
         idx.getParameters().putAll(alterIndex.getProps());
         break;
       case UPDATETIMESTAMP:
@@ -1819,12 +1819,12 @@ private int showPartitions(Hive db, ShowPartitionsDesc showParts) throws HiveExc
       outStream = null;
     } catch (FileNotFoundException e) {
       LOG.info("show partitions: " + stringifyException(e));
-      throw new HiveException(e.toString());
+      throw new HiveException(e);
     } catch (IOException e) {
       LOG.info("show partitions: " + stringifyException(e));
-      throw new HiveException(e.toString());
+      throw new HiveException(e);
     } catch (Exception e) {
-      throw new HiveException(e.toString());
+      throw new HiveException(e);
     } finally {
       IOUtils.closeStream((FSDataOutputStream) outStream);
     }
@@ -2039,7 +2039,7 @@ private int showFunctions(ShowFunctionsDesc showFuncs) throws HiveException {
       LOG.warn("show function: " + stringifyException(e));
       return 1;
     } catch (Exception e) {
-      throw new HiveException(e.toString());
+      throw new HiveException(e);
     } finally {
       IOUtils.closeStream((FSDataOutputStream) outStream);
     }
@@ -2300,7 +2300,7 @@ private int describeFunction(DescFunctionDesc descFunc) throws HiveException {
       LOG.warn("describe function: " + stringifyException(e));
       return 1;
     } catch (Exception e) {
-      throw new HiveException(e.toString());
+      throw new HiveException(e);
     } finally {
       IOUtils.closeStream((FSDataOutputStream) outStream);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
index c93feda690..052aefe9d3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
@@ -151,7 +151,7 @@ public Partition(Table tbl, Map<String, String> partSpec, Path location)
       sd.read(prot);
     } catch (TException e) {
       LOG.error("Could not create a copy of StorageDescription");
-      throw new HiveException("Could not create a copy of StorageDescription");
+      throw new HiveException("Could not create a copy of StorageDescription",e);
     }
 
     tpart.setSd(sd);
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/SimpleCharStream.java b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/SimpleCharStream.java
index 8e95bf97db..9717c61d88 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/SimpleCharStream.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/SimpleCharStream.java
@@ -76,7 +76,7 @@ protected void ExpandBuff(boolean wrapAround) {
         maxNextCharInd = (bufpos -= tokenBegin);
       }
     } catch (Throwable t) {
-      throw new Error(t.getMessage());
+      throw new Error("Error in ExpandBuff",t);
     }
 
     bufsize += 2048;
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TBinarySortableProtocol.java b/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TBinarySortableProtocol.java
index 2a4d0b7002..696a488a61 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TBinarySortableProtocol.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TBinarySortableProtocol.java
@@ -41,7 +41,7 @@
 
 /**
  * An implementation of the Thrift Protocol for binary sortable records.
- * 
+ *
  * The data format: NULL: a single byte \0 NON-NULL Primitives: ALWAYS prepend a
  * single byte \1, and then: Boolean: FALSE = \1, TRUE = \2 Byte: flip the
  * sign-bit to make sure negative comes before positive Short: flip the sign-bit
@@ -55,16 +55,16 @@
  * as Int (see above), then one key by one value, and then the next pair and so
  * on. Binary: size stored as Int (see above), then the binary data in its
  * original form
- * 
+ *
  * Note that the relative order of list/map/binary will be based on the size
  * first (and elements one by one if the sizes are equal).
- * 
+ *
  * This protocol takes an additional parameter SERIALIZATION_SORT_ORDER which is
  * a string containing only "+" and "-". The length of the string should equal
  * to the number of fields in the top-level struct for serialization. "+" means
  * the field should be sorted ascendingly, and "-" means descendingly. The sub
  * fields in the same top-level field will have the same sort order.
- * 
+ *
  * This is not thrift compliant in that it doesn't write out field ids so things
  * cannot actually be versioned.
  */
@@ -340,7 +340,7 @@ public void writeString(String str) throws TException {
     try {
       dat = str.getBytes("UTF-8");
     } catch (UnsupportedEncodingException uex) {
-      throw new TException("JVM DOES NOT SUPPORT UTF-8: " + uex.getMessage());
+      throw new TException("JVM DOES NOT SUPPORT UTF-8: ",uex);
     }
     writeTextBytes(dat, 0, dat.length);
   }
@@ -635,7 +635,7 @@ public String readString() throws TException {
       String r = new String(stringBytes, 0, i, "UTF-8");
       return r;
     } catch (UnsupportedEncodingException uex) {
-      throw new TException("JVM DOES NOT SUPPORT UTF-8: " + uex.getMessage());
+      throw new TException("JVM DOES NOT SUPPORT UTF-8: ",uex);
     }
   }
 
