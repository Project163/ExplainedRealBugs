diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 94924e82fa..2b6be65e6b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -407,8 +407,7 @@ public static ExprNodeDesc deserializeExpression(String s, Configuration conf) {
 
     ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
 
-    XMLDecoder decoder = new XMLDecoder(bais, null, null,
-                                       addResourceFilesToClassPath(conf));
+    XMLDecoder decoder = new XMLDecoder(bais, null, null);
     try {
       ExprNodeDesc expr = (ExprNodeDesc) decoder.readObject();
       return expr;
@@ -481,8 +480,7 @@ public void exceptionThrown(Exception e) {
   public static QueryPlan deserializeQueryPlan(InputStream in, Configuration conf) {
     XMLDecoder d = null;
     try {
-      d = new XMLDecoder(in, null, null,
-                         addResourceFilesToClassPath(conf));
+      d = new XMLDecoder(in, null, null);
       QueryPlan ret = (QueryPlan) d.readObject();
       return (ret);
     } finally {
@@ -515,8 +513,7 @@ public static void serializeMapRedWork(MapredWork w, OutputStream out) {
   public static MapredWork deserializeMapRedWork(InputStream in, Configuration conf) {
     XMLDecoder d = null;
     try {
-      d = new XMLDecoder(in, null, null,
-                         addResourceFilesToClassPath(conf));
+      d = new XMLDecoder(in, null, null);
       MapredWork ret = (MapredWork) d.readObject();
       return (ret);
     } finally {
@@ -548,8 +545,7 @@ public static void serializeMapRedLocalWork(MapredLocalWork w, OutputStream out)
   public static MapredLocalWork deserializeMapRedLocalWork(InputStream in, Configuration conf) {
     XMLDecoder d = null;
     try {
-      d = new XMLDecoder(in, null, null,
-                         addResourceFilesToClassPath(conf));
+      d = new XMLDecoder(in, null, null);
       MapredLocalWork ret = (MapredLocalWork) d.readObject();
       return (ret);
     } finally {
@@ -1789,7 +1785,7 @@ public void run() {
                 throw new IOException(e);
               }
             } while (!executorDone);
-	  }
+    }
           executor.shutdown();
         }
         HiveInterruptUtils.checkInterrupted();
@@ -1951,7 +1947,7 @@ public static void validatePartSpec(Table tbl, Map<String, String> partSpec)
   public static String generatePath(String baseURI, String dumpFilePrefix,
       Byte tag, String bigBucketFileName) {
     String path = new String(baseURI + Path.SEPARATOR + "MapJoin-" + dumpFilePrefix + tag +
-    	"-" + bigBucketFileName + suffix);
+      "-" + bigBucketFileName + suffix);
     return path;
   }
 
@@ -2132,15 +2128,15 @@ public static <T> T executeWithRetry(SQLCommand<T> cmd, PreparedStatement stmt,
         result = cmd.run(stmt);
         return result;
       } catch (SQLTransientException e) {
-      	LOG.warn("Failure and retry #" + failures +  " with exception " + e.getMessage());
+        LOG.warn("Failure and retry #" + failures +  " with exception " + e.getMessage());
         if (failures >= maxRetries) {
           throw e;
         }
         long waitTime = getRandomWaitTime(baseWindow, failures, r);
-      	try {
-      	  Thread.sleep(waitTime);
-      	} catch (InterruptedException iex) {
-     	  }
+        try {
+          Thread.sleep(waitTime);
+        } catch (InterruptedException iex) {
+         }
       } catch (SQLException e) {
         // throw other types of SQLExceptions (SQLNonTransientException / SQLRecoverableException)
         throw e;
@@ -2232,14 +2228,14 @@ public static PreparedStatement prepareWithRetry(Connection conn, String stmt,
 
   /**
    * Introducing a random factor to the wait time before another retry.
-	 * The wait time is dependent on # of failures and a random factor.
-	 * At the first time of getting an exception , the wait time
-	 * is a random number between 0..baseWindow msec. If the first retry
-	 * still fails, we will wait baseWindow msec grace period before the 2nd retry.
-	 * Also at the second retry, the waiting window is expanded to 2*baseWindow msec
-	 * alleviating the request rate from the server. Similarly the 3rd retry
-	 * will wait 2*baseWindow msec. grace period before retry and the waiting window is
-	 * expanded to 3*baseWindow msec and so on.
+   * The wait time is dependent on # of failures and a random factor.
+   * At the first time of getting an exception , the wait time
+   * is a random number between 0..baseWindow msec. If the first retry
+   * still fails, we will wait baseWindow msec grace period before the 2nd retry.
+   * Also at the second retry, the waiting window is expanded to 2*baseWindow msec
+   * alleviating the request rate from the server. Similarly the 3rd retry
+   * will wait 2*baseWindow msec. grace period before retry and the waiting window is
+   * expanded to 3*baseWindow msec and so on.
    * @param baseWindow the base waiting window.
    * @param failures number of failures so far.
    * @param r a random generator.
@@ -2248,7 +2244,7 @@ public static PreparedStatement prepareWithRetry(Connection conn, String stmt,
   public static long getRandomWaitTime(int baseWindow, int failures, Random r) {
     return (long) (
           baseWindow * failures +     // grace period for the last round of attempt
-      	  baseWindow * (failures + 1) * r.nextDouble()); // expanding time window for each failure
+          baseWindow * (failures + 1) * r.nextDouble()); // expanding time window for each failure
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/AddResourceProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/AddResourceProcessor.java
index ea69b946a0..f555981252 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/AddResourceProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/AddResourceProcessor.java
@@ -21,6 +21,7 @@
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.parse.VariableSubstitution;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
@@ -57,6 +58,10 @@ public CommandProcessorResponse run(String command) {
         return new CommandProcessorResponse(1,errMsg,null);
       }
     }
+
+    Thread.currentThread().setContextClassLoader(
+        Utilities.addResourceFilesToClassPath(ss.getConf()));
+
     return new CommandProcessorResponse(0);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/DeleteResourceProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/DeleteResourceProcessor.java
index 83fadeb205..003527a561 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/DeleteResourceProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/DeleteResourceProcessor.java
@@ -21,6 +21,7 @@
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.parse.VariableSubstitution;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
@@ -59,6 +60,9 @@ public CommandProcessorResponse run(String command) {
       ss.delete_resource(t);
     }
 
+    Thread.currentThread().setContextClassLoader(
+        Utilities.addResourceFilesToClassPath(ss.getConf()));
+
     return new CommandProcessorResponse(0);
   }
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
index ee25a8e6ce..e39ea3dd83 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
@@ -23,6 +23,8 @@
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
@@ -61,9 +63,12 @@ public final class SerDeUtils {
   private static ConcurrentHashMap<String, Class<?>> serdes =
     new ConcurrentHashMap<String, Class<?>>();
 
+  public static final Log LOG = LogFactory.getLog(SerDeUtils.class.getName());
+
   public static void registerSerDe(String name, Class<?> serde) {
     if (serdes.containsKey(name)) {
-      throw new RuntimeException("double registering serde " + name);
+      LOG.warn("double registering serde " + name);
+      return;
     }
     serdes.put(name, serde);
   }
