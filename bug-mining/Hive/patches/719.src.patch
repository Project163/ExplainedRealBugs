diff --git a/data/files/x.txt b/data/files/x.txt
new file mode 100644
index 0000000000..6407b72051
--- /dev/null
+++ b/data/files/x.txt
@@ -0,0 +1,2 @@
+Joe	2
+Hank	2
diff --git a/data/files/y.txt b/data/files/y.txt
new file mode 100644
index 0000000000..7eedbcfae2
--- /dev/null
+++ b/data/files/y.txt
@@ -0,0 +1 @@
+2	Tie
diff --git a/data/files/z.txt b/data/files/z.txt
new file mode 100644
index 0000000000..7eedbcfae2
--- /dev/null
+++ b/data/files/z.txt
@@ -0,0 +1 @@
+2	Tie
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
index 0115a12c11..575059951c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.util.ReflectionUtils;
 
@@ -403,7 +404,7 @@ public void closeOp(boolean abort) throws HiveException {
             bigBucketFileName = "-";
           }
           // get the tmp URI path; it will be a hdfs path if not local mode
-          String tmpURIPath = Utilities.generatePath(tmpURI, tag, bigBucketFileName);
+          String tmpURIPath = Utilities.generatePath(tmpURI, conf.getDumpFilePrefix(), tag, bigBucketFileName);
           hashTable.isAbort(rowNumber, console);
           console.printInfo(Utilities.now() + "\tDump the hashtable into file: " + tmpURIPath);
           // get the hashtable file and path
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
index c8ed0439f7..df30b24236 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
@@ -188,7 +188,7 @@ private void loadHashTable() throws HiveException {
           .entrySet()) {
         Byte pos = entry.getKey();
         HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashtable = entry.getValue();
-        String filePath = Utilities.generatePath(baseDir, pos, currentFileName);
+        String filePath = Utilities.generatePath(baseDir, conf.getDumpFilePrefix(), pos, currentFileName);
         Path path = new Path(filePath);
         LOG.info("\tLoad back 1 hashtable file from tmp file uri:" + path.toString());
         hashtable.initilizePersistentHash(path.toUri().getPath());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
index 68bb0e99ae..dc084d01e1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
@@ -403,7 +403,9 @@ private void generateDummyHashTable(String alias, String bigBucketFileName) thro
     if (bigBucketFileName == null || bigBucketFileName.length() == 0) {
       bigBucketFileName = "-";
     }
-    String tmpURIPath = Utilities.generatePath(tmpURI, tag, bigBucketFileName);
+    HashTableSinkOperator htso = (HashTableSinkOperator)childOp;
+    String tmpURIPath = Utilities.generatePath(tmpURI, htso.getConf().getDumpFilePrefix(),
+        tag, bigBucketFileName);
     console.printInfo(Utilities.now() + "\tDump the hashtable into file: " + tmpURIPath);
     Path path = new Path(tmpURIPath);
     FileSystem fs = path.getFileSystem(job);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 71f3bc4c91..54fcbcb8d0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -118,8 +118,8 @@
 import org.apache.hadoop.hive.ql.plan.MapredWork;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
-import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.PlanUtils.ExpressionTypes;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
@@ -134,8 +134,8 @@
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.mapred.FileOutputFormat;
@@ -1909,9 +1909,10 @@ public static void validatePartSpec(Table tbl, Map<String, String> partSpec)
 
   public static String suffix = ".hashtable";
 
-  public static String generatePath(String baseURI, Byte tag, String bigBucketFileName) {
-    String path = new String(baseURI + Path.SEPARATOR + "MapJoin-" + tag + "-" + bigBucketFileName
-        + suffix);
+  public static String generatePath(String baseURI, String dumpFilePrefix,
+      Byte tag, String bigBucketFileName) {
+    String path = new String(baseURI + Path.SEPARATOR + "MapJoin-" + dumpFilePrefix + tag +
+    	"-" + bigBucketFileName + suffix);
     return path;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
index 98bb2a5e27..60f7f7c6d8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
@@ -432,9 +432,18 @@ public static MapJoinOperator convertMapJoin(
       valueTableDescs.add(valueTableDesc);
       valueFiltedTableDescs.add(valueFilteredTableDesc);
     }
+    String dumpFilePrefix = "";
+    if( joinTree.getMapAliases() != null ) {
+      for(String mapAlias : joinTree.getMapAliases()) {
+        dumpFilePrefix = dumpFilePrefix + mapAlias;
+      }
+      dumpFilePrefix = dumpFilePrefix+"-"+PlanUtils.getCountForMapJoinDumpFilePrefix();
+    } else {
+      dumpFilePrefix = "mapfile"+PlanUtils.getCountForMapJoinDumpFilePrefix();
+    }
     MapJoinDesc mapJoinDescriptor = new MapJoinDesc(keyExprMap, keyTableDesc, valueExprMap,
         valueTableDescs, valueFiltedTableDescs, outputColumnNames, mapJoinPos, joinCondns,
-        filterMap, op.getConf().getNoOuterJoin());
+        filterMap, op.getConf().getNoOuterJoin(), dumpFilePrefix);
     mapJoinDescriptor.setTagOrder(tagOrder);
 
     MapJoinOperator mapJoinOp = (MapJoinOperator) OperatorFactory.getAndMakeChild(
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
index 7ae75d678b..742cea2a3a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
@@ -58,7 +58,6 @@
 import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
 
 /**
  * GenMRSkewJoinProcessor.
@@ -280,10 +279,11 @@ public static void processSkewJoin(JoinOperator joinOp,
       assert reducer instanceof JoinOperator;
       JoinOperator cloneJoinOp = (JoinOperator) reducer;
 
+      String dumpFilePrefix = "mapfile"+PlanUtils.getCountForMapJoinDumpFilePrefix();
       MapJoinDesc mapJoinDescriptor = new MapJoinDesc(newJoinKeys, keyTblDesc,
           newJoinValues, newJoinValueTblDesc, newJoinValueTblDesc,joinDescriptor
           .getOutputColumnNames(), i, joinDescriptor.getConds(),
-          joinDescriptor.getFilters(), joinDescriptor.getNoOuterJoin());
+          joinDescriptor.getFilters(), joinDescriptor.getNoOuterJoin(), dumpFilePrefix);
       mapJoinDescriptor.setTagOrder(tags);
       mapJoinDescriptor.setHandleSkewJoin(false);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java
index 4a729e8d58..947ca901f1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java
@@ -25,8 +25,8 @@
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 import java.util.Map.Entry;
+import java.util.Set;
 
 /**
  * Map Join operator Descriptor implementation.
@@ -81,6 +81,9 @@ public class HashTableSinkDesc extends JoinDesc implements Serializable {
   private LinkedHashMap<String, Integer> bucketFileNameMapping;
   private float hashtableMemoryUsage;
 
+  //map join dump file name
+  private String dumpFilePrefix;
+
   public HashTableSinkDesc() {
     bucketFileNameMapping = new LinkedHashMap<String, Integer>();
   }
@@ -109,6 +112,7 @@ public HashTableSinkDesc(MapJoinDesc clone) {
     this.bigTableAlias = clone.getBigTableAlias();
     this.aliasBucketFileNameMapping = clone.getAliasBucketFileNameMapping();
     this.bucketFileNameMapping = clone.getBucketFileNameMapping();
+    this.dumpFilePrefix = clone.getDumpFilePrefix();
   }
 
 
@@ -134,6 +138,21 @@ public void setHashtableMemoryUsage(float hashtableMemoryUsage) {
     this.hashtableMemoryUsage = hashtableMemoryUsage;
   }
 
+  /**
+   * @return the dumpFilePrefix
+   */
+  public String getDumpFilePrefix() {
+    return dumpFilePrefix;
+  }
+
+  /**
+   * @param dumpFilePrefix
+   *          the dumpFilePrefix to set
+   */
+  public void setDumpFilePrefix(String dumpFilePrefix) {
+    this.dumpFilePrefix = dumpFilePrefix;
+  }
+
   public boolean isHandleSkewJoin() {
     return handleSkewJoin;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
index ada9826d67..6d0a6a544a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
@@ -25,8 +25,8 @@
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 import java.util.Map.Entry;
+import java.util.Set;
 
 /**
  * Map Join operator Descriptor implementation.
@@ -50,6 +50,9 @@ public class MapJoinDesc extends JoinDesc implements Serializable {
   private LinkedHashMap<String, LinkedHashMap<String, ArrayList<String>>> aliasBucketFileNameMapping;
   private LinkedHashMap<String, Integer> bucketFileNameMapping;
 
+  //map join dump file name
+  private String dumpFilePrefix;
+
   public MapJoinDesc() {
     bucketFileNameMapping = new LinkedHashMap<String, Integer>();
   }
@@ -64,13 +67,14 @@ public MapJoinDesc(MapJoinDesc clone) {
     this.bigTableAlias = clone.bigTableAlias;
     this.aliasBucketFileNameMapping = clone.aliasBucketFileNameMapping;
     this.bucketFileNameMapping = clone.bucketFileNameMapping;
+    this.dumpFilePrefix = clone.dumpFilePrefix;
   }
 
   public MapJoinDesc(final Map<Byte, List<ExprNodeDesc>> keys,
       final TableDesc keyTblDesc, final Map<Byte, List<ExprNodeDesc>> values,
       final List<TableDesc> valueTblDescs,final List<TableDesc> valueFilteredTblDescs,  List<String> outputColumnNames,
       final int posBigTable, final JoinCondDesc[] conds,
-      final Map<Byte, List<ExprNodeDesc>> filters, boolean noOuterJoin) {
+      final Map<Byte, List<ExprNodeDesc>> filters, boolean noOuterJoin, String dumpFilePrefix) {
     super(values, outputColumnNames, noOuterJoin, conds, filters);
     this.keys = keys;
     this.keyTblDesc = keyTblDesc;
@@ -78,6 +82,7 @@ public MapJoinDesc(final Map<Byte, List<ExprNodeDesc>> keys,
     this.valueFilteredTblDescs = valueFilteredTblDescs;
     this.posBigTable = posBigTable;
     this.bucketFileNameMapping = new LinkedHashMap<String, Integer>();
+    this.dumpFilePrefix = dumpFilePrefix;
     initRetainExprList();
   }
 
@@ -103,6 +108,21 @@ public void setRetainList(Map<Byte, List<Integer>> retainList) {
     this.retainList = retainList;
   }
 
+  /**
+   * @return the dumpFilePrefix
+   */
+  public String getDumpFilePrefix() {
+    return dumpFilePrefix;
+  }
+
+  /**
+   * @param dumpFilePrefix
+   *          the dumpFilePrefix to set
+   */
+  public void setDumpFilePrefix(String dumpFilePrefix) {
+    this.dumpFilePrefix = dumpFilePrefix;
+  }
+
   /**
    * @return the keys
    */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
index 66a76d52a7..0ad0889492 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
@@ -69,6 +69,8 @@ public final class PlanUtils {
 
   protected static final Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.plan.PlanUtils");
 
+  private static long countForMapJoinDumpFilePrefix = 0;
+
   /**
    * ExpressionTypes.
    *
@@ -77,6 +79,10 @@ public static enum ExpressionTypes {
     FIELD, JEXL
   };
 
+  public static long getCountForMapJoinDumpFilePrefix() {
+    return countForMapJoinDumpFilePrefix++;
+  }
+
   @SuppressWarnings("nls")
   public static MapredWork getMapRedWork() {
     try {
diff --git a/ql/src/test/queries/clientpositive/mapjoin_subquery2.q b/ql/src/test/queries/clientpositive/mapjoin_subquery2.q
new file mode 100644
index 0000000000..20e053fcba
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/mapjoin_subquery2.q
@@ -0,0 +1,39 @@
+drop table x;
+drop table y;
+drop table z;
+
+CREATE TABLE x (name STRING, id INT)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
+
+CREATE TABLE y (id INT, name STRING)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
+
+CREATE TABLE z (id INT, name STRING)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
+
+load data local inpath '../data/files/x.txt' INTO TABLE x;
+load data local inpath '../data/files/y.txt' INTO TABLE y;
+load data local inpath '../data/files/z.txt' INTO TABLE z;
+
+SELECT subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id);
+
+EXPLAIN
+SELECT /*+ MAPJOIN(z) */ subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT /*+ MAPJOIN(x) */ x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id);
+
+SELECT /*+ MAPJOIN(z) */ subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT /*+ MAPJOIN(x) */ x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id);
+
+drop table x;
+drop table y;
+drop table z;
diff --git a/ql/src/test/results/clientpositive/mapjoin_subquery2.q.out b/ql/src/test/results/clientpositive/mapjoin_subquery2.q.out
new file mode 100644
index 0000000000..41ff23350d
--- /dev/null
+++ b/ql/src/test/results/clientpositive/mapjoin_subquery2.q.out
@@ -0,0 +1,273 @@
+PREHOOK: query: drop table x
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table x
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table y
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table y
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table z
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table z
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE x (name STRING, id INT)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE x (name STRING, id INT)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@x
+PREHOOK: query: CREATE TABLE y (id INT, name STRING)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE y (id INT, name STRING)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@y
+PREHOOK: query: CREATE TABLE z (id INT, name STRING)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE z (id INT, name STRING)
+ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@z
+PREHOOK: query: load data local inpath '../data/files/x.txt' INTO TABLE x
+PREHOOK: type: LOAD
+PREHOOK: Output: default@x
+POSTHOOK: query: load data local inpath '../data/files/x.txt' INTO TABLE x
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@x
+PREHOOK: query: load data local inpath '../data/files/y.txt' INTO TABLE y
+PREHOOK: type: LOAD
+PREHOOK: Output: default@y
+POSTHOOK: query: load data local inpath '../data/files/y.txt' INTO TABLE y
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@y
+PREHOOK: query: load data local inpath '../data/files/z.txt' INTO TABLE z
+PREHOOK: type: LOAD
+PREHOOK: Output: default@z
+POSTHOOK: query: load data local inpath '../data/files/z.txt' INTO TABLE z
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@z
+PREHOOK: query: SELECT subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@x
+PREHOOK: Input: default@y
+PREHOOK: Input: default@z
+PREHOOK: Output: file:/tmp/tianzhao/hive_2011-10-24_06-01-49_774_6691053797137524902/-mr-10000
+POSTHOOK: query: SELECT subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@x
+POSTHOOK: Input: default@y
+POSTHOOK: Input: default@z
+POSTHOOK: Output: file:/tmp/tianzhao/hive_2011-10-24_06-01-49_774_6691053797137524902/-mr-10000
+2	Joe	2	Tie	2	Tie
+2	Hank	2	Tie	2	Tie
+PREHOOK: query: EXPLAIN
+SELECT /*+ MAPJOIN(z) */ subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT /*+ MAPJOIN(x) */ x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id)
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT /*+ MAPJOIN(z) */ subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT /*+ MAPJOIN(x) */ x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id)
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME y)) (TOK_TABREF (TOK_TABNAME x)) (= (. (TOK_TABLE_OR_COL x) id) (. (TOK_TABLE_OR_COL y) id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST x))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL x) id) key1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL x) name) value1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL y) id) key2) (TOK_SELEXPR (. (TOK_TABLE_OR_COL y) name) value2)))) subq) (TOK_TABREF (TOK_TABNAME z)) (= (. (TOK_TABLE_OR_COL subq) key1) (. (TOK_TABLE_OR_COL z) id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST z))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL subq) key1)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL subq) value1)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL subq) key2)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL subq) value2)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL z) id)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL z) name)))))
+
+STAGE DEPENDENCIES:
+  Stage-4 is a root stage
+  Stage-1 depends on stages: Stage-4
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-4
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        subq:x 
+          Fetch Operator
+            limit: -1
+        z 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        subq:x 
+          TableScan
+            alias: x
+            HashTable Sink Operator
+              condition expressions:
+                0 {id} {name}
+                1 {name} {id}
+              handleSkewJoin: false
+              keys:
+                0 [Column[id]]
+                1 [Column[id]]
+              Position of Big Table: 0
+        z 
+          TableScan
+            alias: z
+            HashTable Sink Operator
+              condition expressions:
+                0 {_col0} {_col1} {_col2} {_col3}
+                1 {id} {name}
+              handleSkewJoin: false
+              keys:
+                0 [Column[_col0]]
+                1 [Column[id]]
+              Position of Big Table: 0
+
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        subq:y 
+          TableScan
+            alias: y
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+              condition expressions:
+                0 {id} {name}
+                1 {name} {id}
+              handleSkewJoin: false
+              keys:
+                0 [Column[id]]
+                1 [Column[id]]
+              outputColumnNames: _col0, _col1, _col4, _col5
+              Position of Big Table: 0
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: int
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: int
+                outputColumnNames: _col0, _col1, _col4, _col5
+                Select Operator
+                  expressions:
+                        expr: _col5
+                        type: int
+                        expr: _col4
+                        type: string
+                        expr: _col0
+                        type: int
+                        expr: _col1
+                        type: string
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0} {_col1} {_col2} {_col3}
+                      1 {id} {name}
+                    handleSkewJoin: false
+                    keys:
+                      0 [Column[_col0]]
+                      1 [Column[id]]
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                    Position of Big Table: 0
+                    Select Operator
+                      expressions:
+                            expr: _col0
+                            type: int
+                            expr: _col1
+                            type: string
+                            expr: _col2
+                            type: int
+                            expr: _col3
+                            type: string
+                            expr: _col4
+                            type: int
+                            expr: _col5
+                            type: string
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                      Select Operator
+                        expressions:
+                              expr: _col0
+                              type: int
+                              expr: _col1
+                              type: string
+                              expr: _col2
+                              type: int
+                              expr: _col3
+                              type: string
+                              expr: _col4
+                              type: int
+                              expr: _col5
+                              type: string
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                        File Output Operator
+                          compressed: false
+                          GlobalTableId: 0
+                          table:
+                              input format: org.apache.hadoop.mapred.TextInputFormat
+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: SELECT /*+ MAPJOIN(z) */ subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT /*+ MAPJOIN(x) */ x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@x
+PREHOOK: Input: default@y
+PREHOOK: Input: default@z
+PREHOOK: Output: file:/tmp/tianzhao/hive_2011-10-24_06-01-55_615_1052117020867998139/-mr-10000
+POSTHOOK: query: SELECT /*+ MAPJOIN(z) */ subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
+FROM
+(SELECT /*+ MAPJOIN(x) */ x.id as key1, x.name as value1, y.id as key2, y.name as value2 
+ FROM y JOIN x ON (x.id = y.id)) subq
+ JOIN z ON (subq.key1 = z.id)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@x
+POSTHOOK: Input: default@y
+POSTHOOK: Input: default@z
+POSTHOOK: Output: file:/tmp/tianzhao/hive_2011-10-24_06-01-55_615_1052117020867998139/-mr-10000
+2	Joe	2	Tie	2	Tie
+2	Hank	2	Tie	2	Tie
+PREHOOK: query: drop table x
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@x
+PREHOOK: Output: default@x
+POSTHOOK: query: drop table x
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@x
+POSTHOOK: Output: default@x
+PREHOOK: query: drop table y
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@y
+PREHOOK: Output: default@y
+POSTHOOK: query: drop table y
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@y
+POSTHOOK: Output: default@y
+PREHOOK: query: drop table z
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@z
+PREHOOK: Output: default@z
+POSTHOOK: query: drop table z
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@z
+POSTHOOK: Output: default@z
