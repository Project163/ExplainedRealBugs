diff --git a/CHANGES.txt b/CHANGES.txt
index 1ac4583211..bf9cad497d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -51,6 +51,9 @@ Trunk -  Unreleased
     HIVE-853. Provide hints for controlling join order
     (Emil Ibrishimov via namit)
 
+    HIVE-31. Support Create Table As Select
+    (Ning Zhang via namit)
+
   IMPROVEMENTS
 
     HIVE-760. Add version info to META-INF/MANIFEST.MF.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java
index 3668ec740d..731e3c42dd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java
@@ -35,6 +35,8 @@ public class ColumnInfo implements Serializable {
   private static final long serialVersionUID = 1L;
 
   private String internalName;
+  
+  private String alias = null; // [optional] alias of the column (external name as seen by the users) 
 
   /**
    * Store the alias of the table where available.
@@ -96,4 +98,13 @@ public boolean getIsPartitionCol() {
   public String toString() {
     return internalName + ": " + type;
   }
+  
+  public void setAlias(String col_alias) {
+    alias = col_alias;
+  }
+  
+  public String getAlias() {
+    return alias;
+  }
+  
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index d63b26c8f6..7edb97ec15 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -37,6 +37,10 @@
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Order;
+import org.apache.hadoop.hive.serde.Constants;
+
 public abstract class BaseSemanticAnalyzer {
   protected final Hive db;
   protected final HiveConf conf;
@@ -253,6 +257,92 @@ public Set<WriteEntity> getOutputs() {
     return outputs;
   }
 
+  /**
+   *  Get the list of FieldSchema out of the ASTNode. 
+   */
+  protected List<FieldSchema> getColumns(ASTNode ast) throws SemanticException
+  {
+    List<FieldSchema> colList = new ArrayList<FieldSchema>();
+    int numCh = ast.getChildCount();
+    for (int i = 0; i < numCh; i++) {
+      FieldSchema col = new FieldSchema();
+      ASTNode child = (ASTNode)ast.getChild(i);
+      
+      // child 0 is the name of the column
+      col.setName(unescapeIdentifier(child.getChild(0).getText()));
+      // child 1 is the type of the column
+      ASTNode typeChild = (ASTNode)(child.getChild(1));
+      col.setType(getTypeStringFromAST(typeChild));
+       
+      // child 2 is the optional comment of the column
+      if (child.getChildCount() == 3)
+        col.setComment(unescapeSQLString(child.getChild(2).getText()));
+      colList.add(col);
+    }
+    return colList;
+  }
+  
+  protected List<String> getColumnNames(ASTNode ast)
+  {
+    List<String> colList = new ArrayList<String>();
+    int numCh = ast.getChildCount();
+    for (int i = 0; i < numCh; i++) {
+      ASTNode child = (ASTNode)ast.getChild(i);
+      colList.add(unescapeIdentifier(child.getText()));
+    }
+    return colList;
+  }
+  
+  protected List<Order> getColumnNamesOrder(ASTNode ast)
+  {
+    List<Order> colList = new ArrayList<Order>();
+    int numCh = ast.getChildCount();
+    for (int i = 0; i < numCh; i++) {
+      ASTNode child = (ASTNode)ast.getChild(i);
+      if (child.getToken().getType() == HiveParser.TOK_TABSORTCOLNAMEASC)
+        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()), 1));
+      else
+        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()), 0));
+    }
+    return colList;
+  }
+  
+  protected static String getTypeStringFromAST(ASTNode typeNode) throws SemanticException {
+    switch (typeNode.getType()) {
+    case HiveParser.TOK_LIST:
+      return Constants.LIST_TYPE_NAME + "<"
+        + getTypeStringFromAST((ASTNode)typeNode.getChild(0)) + ">";
+    case HiveParser.TOK_MAP:
+      return Constants.MAP_TYPE_NAME + "<"
+        + getTypeStringFromAST((ASTNode)typeNode.getChild(0)) + ","
+        + getTypeStringFromAST((ASTNode)typeNode.getChild(1)) + ">";
+    case HiveParser.TOK_STRUCT:
+      return getStructTypeStringFromAST(typeNode);
+    default:
+      return DDLSemanticAnalyzer.getTypeName(typeNode.getType());
+    }
+  }
+  
+  private static String getStructTypeStringFromAST(ASTNode typeNode)
+      throws SemanticException {
+    String typeStr = Constants.STRUCT_TYPE_NAME + "<";
+    typeNode = (ASTNode) typeNode.getChild(0);
+    int children = typeNode.getChildCount();
+    if(children <= 0)
+      throw new SemanticException("empty struct not allowed.");
+    for (int i = 0; i < children; i++) {
+      ASTNode child = (ASTNode) typeNode.getChild(i);
+      typeStr += unescapeIdentifier(child.getChild(0).getText()) + ":";
+      typeStr += getTypeStringFromAST((ASTNode) child.getChild(1));
+      if (i < children - 1)
+        typeStr += ",";
+    }
+      
+    typeStr += ">";
+    return typeStr;
+  }
+ 
+ 
   public static class tableSpec {
     public String tableName;
     public Table tableHandle;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index bc7294d69e..98178b7b22 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -40,17 +40,12 @@
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
-import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
-import org.apache.hadoop.hive.ql.io.RCFileInputFormat;
-import org.apache.hadoop.hive.ql.io.RCFileOutputFormat;
 import org.apache.hadoop.hive.ql.plan.AddPartitionDesc;
 import org.apache.hadoop.hive.ql.plan.DDLWork;
 import org.apache.hadoop.hive.ql.plan.MsckDesc;
 import org.apache.hadoop.hive.ql.plan.alterTableDesc;
-import org.apache.hadoop.hive.ql.plan.createTableDesc;
-import org.apache.hadoop.hive.ql.plan.createTableLikeDesc;
 import org.apache.hadoop.hive.ql.plan.descFunctionDesc;
 import org.apache.hadoop.hive.ql.plan.descTableDesc;
 import org.apache.hadoop.hive.ql.plan.dropTableDesc;
@@ -62,11 +57,7 @@
 import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.ql.plan.alterTableDesc.alterTableTypes;
 import org.apache.hadoop.hive.serde.Constants;
-import org.apache.hadoop.hive.serde2.SerDeUtils;
-import org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
-import org.apache.hadoop.mapred.SequenceFileInputFormat;
-import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.mapred.TextInputFormat;
 
 public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
@@ -85,14 +76,6 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
     TokenToTypeName.put(HiveParser.TOK_DATETIME, Constants.DATETIME_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_TIMESTAMP, Constants.TIMESTAMP_TYPE_NAME);
   }
-  private static final String TEXTFILE_INPUT = TextInputFormat.class.getName();
-  private static final String TEXTFILE_OUTPUT = IgnoreKeyTextOutputFormat.class.getName();
-  private static final String SEQUENCEFILE_INPUT = SequenceFileInputFormat.class.getName();
-  private static final String SEQUENCEFILE_OUTPUT = SequenceFileOutputFormat.class.getName();
-  private static final String RCFILE_INPUT = RCFileInputFormat.class.getName();
-  private static final String RCFILE_OUTPUT = RCFileOutputFormat.class.getName();
-
-  private static final String COLUMNAR_SERDE = ColumnarSerDe.class.getName();
 
   public static String getTypeName(int token) {
     return TokenToTypeName.get(token);
@@ -104,9 +87,7 @@ public DDLSemanticAnalyzer(HiveConf conf) throws SemanticException {
 
   @Override
   public void analyzeInternal(ASTNode ast) throws SemanticException {
-    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE)
-       analyzeCreateTable(ast);
-    else if (ast.getToken().getType() == HiveParser.TOK_DROPTABLE)
+    if (ast.getToken().getType() == HiveParser.TOK_DROPTABLE)
        analyzeDropTable(ast);
     else if (ast.getToken().getType() == HiveParser.TOK_DESCTABLE)
     {
@@ -155,245 +136,7 @@ else if (ast.getToken().getType() == HiveParser.TOK_SHOWPARTITIONS)
     }
   }
 
-  private void analyzeCreateTable(ASTNode ast)
-    throws SemanticException {
-    String            tableName     = unescapeIdentifier(ast.getChild(0).getText());
-    String            likeTableName = null;
-    List<FieldSchema> cols          = null;
-    List<FieldSchema> partCols      = null;
-    List<String>      bucketCols    = null;
-    List<Order>       sortCols      = null;
-    int               numBuckets    = -1;
-    String            fieldDelim    = null;
-    String            fieldEscape   = null;
-    String            collItemDelim = null;
-    String            mapKeyDelim   = null;
-    String            lineDelim     = null;
-    String            comment       = null;
-    String            inputFormat   = TEXTFILE_INPUT;
-    String            outputFormat  = TEXTFILE_OUTPUT;
-    String            location      = null;
-    String            serde         = null;
-    Map<String, String> mapProp     = null;
-    boolean           ifNotExists   = false;
-    boolean           isExt         = false;
-
-    if ("SequenceFile".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVEDEFAULTFILEFORMAT))) {
-      inputFormat = SEQUENCEFILE_INPUT;
-      outputFormat = SEQUENCEFILE_OUTPUT;
-    }
-
-    LOG.info("Creating table" + tableName);
-    int numCh = ast.getChildCount();
-    for (int num = 1; num < numCh; num++)
-    {
-      ASTNode child = (ASTNode)ast.getChild(num);
-      switch (child.getToken().getType()) {
-        case HiveParser.TOK_IFNOTEXISTS:
-          ifNotExists = true;
-          break;
-        case HiveParser.KW_EXTERNAL:
-          isExt = true;
-          break;
-        case HiveParser.TOK_LIKETABLE:
-          if (child.getChildCount() > 0) {
-            likeTableName = unescapeIdentifier(child.getChild(0).getText());
-          }
-          break;
-        case HiveParser.TOK_TABCOLLIST:
-          cols = getColumns(child);
-          break;
-        case HiveParser.TOK_TABLECOMMENT:
-          comment = unescapeSQLString(child.getChild(0).getText());
-          break;
-        case HiveParser.TOK_TABLEPARTCOLS:
-          partCols = getColumns((ASTNode)child.getChild(0));
-          break;
-        case HiveParser.TOK_TABLEBUCKETS:
-          bucketCols = getColumnNames((ASTNode)child.getChild(0));
-          if (child.getChildCount() == 2)
-            numBuckets = (Integer.valueOf(child.getChild(1).getText())).intValue();
-          else
-          {
-            sortCols = getColumnNamesOrder((ASTNode)child.getChild(1));
-            numBuckets = (Integer.valueOf(child.getChild(2).getText())).intValue();
-          }
-          break;
-        case HiveParser.TOK_TABLEROWFORMAT:
-
-          child = (ASTNode)child.getChild(0);
-          int numChildRowFormat = child.getChildCount();
-          for (int numC = 0; numC < numChildRowFormat; numC++)
-          {
-            ASTNode rowChild = (ASTNode)child.getChild(numC);
-            switch (rowChild.getToken().getType()) {
-              case HiveParser.TOK_TABLEROWFORMATFIELD:
-                fieldDelim = unescapeSQLString(rowChild.getChild(0).getText());
-                if (rowChild.getChildCount()>=2) {
-                  fieldEscape = unescapeSQLString(rowChild.getChild(1).getText());
-                }
-                break;
-              case HiveParser.TOK_TABLEROWFORMATCOLLITEMS:
-                collItemDelim = unescapeSQLString(rowChild.getChild(0).getText());
-                break;
-              case HiveParser.TOK_TABLEROWFORMATMAPKEYS:
-                mapKeyDelim = unescapeSQLString(rowChild.getChild(0).getText());
-                break;
-              case HiveParser.TOK_TABLEROWFORMATLINES:
-                lineDelim = unescapeSQLString(rowChild.getChild(0).getText());
-                break;
-              default: assert false;
-            }
-          }
-          break;
-        case HiveParser.TOK_TABLESERIALIZER:
-
-          child = (ASTNode)child.getChild(0);
-          serde = unescapeSQLString(child.getChild(0).getText());
-          if (child.getChildCount() == 2) {
-            mapProp = new HashMap<String, String>();
-            ASTNode prop = (ASTNode)((ASTNode)child.getChild(1)).getChild(0);
-            for (int propChild = 0; propChild < prop.getChildCount(); propChild++) {
-              String key = unescapeSQLString(prop.getChild(propChild).getChild(0).getText());
-              String value = unescapeSQLString(prop.getChild(propChild).getChild(1).getText());
-              mapProp.put(key,value);
-            }
-          }
-          break;
-        case HiveParser.TOK_TBLSEQUENCEFILE:
-          inputFormat = SEQUENCEFILE_INPUT;
-          outputFormat = SEQUENCEFILE_OUTPUT;
-          break;
-        case HiveParser.TOK_TBLTEXTFILE:
-          inputFormat = TEXTFILE_INPUT;
-          outputFormat = TEXTFILE_OUTPUT;
-          break;
-        case HiveParser.TOK_TBLRCFILE:
-          inputFormat = RCFILE_INPUT;
-          outputFormat = RCFILE_OUTPUT;
-          serde = COLUMNAR_SERDE;
-          break;
-        case HiveParser.TOK_TABLEFILEFORMAT:
-          inputFormat = unescapeSQLString(child.getChild(0).getText());
-          outputFormat = unescapeSQLString(child.getChild(1).getText());
-          break;
-        case HiveParser.TOK_TABLELOCATION:
-          location = unescapeSQLString(child.getChild(0).getText());
-          break;
-        default: assert false;
-      }
-    }
-    if (likeTableName == null) {
-      createTableDesc crtTblDesc =
-        new createTableDesc(tableName, isExt, cols, partCols, bucketCols,
-                            sortCols, numBuckets,
-                            fieldDelim, fieldEscape,
-                            collItemDelim, mapKeyDelim, lineDelim,
-                            comment, inputFormat, outputFormat, location, serde,
-                            mapProp, ifNotExists);
-
-      validateCreateTable(crtTblDesc);
-      rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), crtTblDesc), conf));
-    } else {
-      createTableLikeDesc crtTblLikeDesc =
-        new createTableLikeDesc(tableName, isExt, location, ifNotExists, likeTableName);
-      rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), crtTblLikeDesc), conf));
-    }
-
-  }
-
-  private void validateCreateTable(createTableDesc crtTblDesc) throws SemanticException {
-    // no duplicate column names
-    // currently, it is a simple n*n algorithm - this can be optimized later if need be
-    // but it should not be a major bottleneck as the number of columns are anyway not so big
-
-    if((crtTblDesc.getCols() == null) || (crtTblDesc.getCols().size() == 0)) {
-      // for now make sure that serde exists
-      if(StringUtils.isEmpty(crtTblDesc.getSerName()) || SerDeUtils.isNativeSerDe(crtTblDesc.getSerName())) {
-        throw new SemanticException(ErrorMsg.INVALID_TBL_DDL_SERDE.getMsg());
-      }
-      return;
-    }
-
-    try {
-      Class<?> origin = Class.forName(crtTblDesc.getOutputFormat(), true, JavaUtils.getClassLoader());
-      Class<? extends HiveOutputFormat> replaced = HiveFileFormatUtils.getOutputFormatSubstitute(origin);
-      if(replaced == null)
-        throw new SemanticException(ErrorMsg.INVALID_OUTPUT_FORMAT_TYPE.getMsg());
-    } catch (ClassNotFoundException e) {
-      throw new SemanticException(ErrorMsg.INVALID_OUTPUT_FORMAT_TYPE.getMsg());
-    }
-
-    Iterator<FieldSchema> iterCols = crtTblDesc.getCols().iterator();
-    List<String> colNames = new ArrayList<String>();
-    while (iterCols.hasNext()) {
-      String colName = iterCols.next().getName();
-      Iterator<String> iter = colNames.iterator();
-      while (iter.hasNext()) {
-        String oldColName = iter.next();
-        if (colName.equalsIgnoreCase(oldColName))
-          throw new SemanticException(ErrorMsg.DUPLICATE_COLUMN_NAMES.getMsg());
-      }
-      colNames.add(colName);
-    }
-
-    if (crtTblDesc.getBucketCols() != null)
-    {
-      // all columns in cluster and sort are valid columns
-      Iterator<String> bucketCols = crtTblDesc.getBucketCols().iterator();
-      while (bucketCols.hasNext()) {
-        String bucketCol = bucketCols.next();
-        boolean found = false;
-        Iterator<String> colNamesIter = colNames.iterator();
-        while (colNamesIter.hasNext()) {
-          String colName = colNamesIter.next();
-          if (bucketCol.equalsIgnoreCase(colName)) {
-            found = true;
-            break;
-          }
-        }
-        if (!found)
-          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg());
-      }
-    }
-
-    if (crtTblDesc.getSortCols() != null)
-    {
-      // all columns in cluster and sort are valid columns
-      Iterator<Order> sortCols = crtTblDesc.getSortCols().iterator();
-      while (sortCols.hasNext()) {
-        String sortCol = sortCols.next().getCol();
-        boolean found = false;
-        Iterator<String> colNamesIter = colNames.iterator();
-        while (colNamesIter.hasNext()) {
-          String colName = colNamesIter.next();
-          if (sortCol.equalsIgnoreCase(colName)) {
-            found = true;
-            break;
-          }
-        }
-        if (!found)
-          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg());
-      }
-    }
-
-    if (crtTblDesc.getPartCols() != null)
-    {
-      // there is no overlap between columns and partitioning columns
-      Iterator<FieldSchema> partColsIter = crtTblDesc.getPartCols().iterator();
-      while (partColsIter.hasNext()) {
-        String partCol = partColsIter.next().getName();
-        Iterator<String> colNamesIter = colNames.iterator();
-        while (colNamesIter.hasNext()) {
-          String colName = unescapeIdentifier(colNamesIter.next());
-          if (partCol.equalsIgnoreCase(colName))
-            throw new SemanticException(ErrorMsg.COLUMN_REPEATED_IN_PARTITIONING_COLS.getMsg());
-        }
-      }
-    }
-  }
-
-  private void analyzeDropTable(ASTNode ast)
+  private void analyzeDropTable(ASTNode ast) 
     throws SemanticException {
     String tableName = unescapeIdentifier(ast.getChild(0).getText());
     dropTableDesc dropTblDesc = new dropTableDesc(tableName);
@@ -441,84 +184,6 @@ private HashMap<String, String> getProps(ASTNode prop) {
     return mapProp;
   }
 
-  private static String getTypeStringFromAST(ASTNode typeNode) throws SemanticException {
-    switch (typeNode.getType()) {
-    case HiveParser.TOK_LIST:
-      return Constants.LIST_TYPE_NAME + "<"
-        + getTypeStringFromAST((ASTNode)typeNode.getChild(0)) + ">";
-    case HiveParser.TOK_MAP:
-      return Constants.MAP_TYPE_NAME + "<"
-        + getTypeStringFromAST((ASTNode)typeNode.getChild(0)) + ","
-        + getTypeStringFromAST((ASTNode)typeNode.getChild(1)) + ">";
-    case HiveParser.TOK_STRUCT:
-      return getStructTypeStringFromAST(typeNode);
-    default:
-      return getTypeName(typeNode.getType());
-    }
-  }
-
-  private static String getStructTypeStringFromAST(ASTNode typeNode)
-      throws SemanticException {
-    String typeStr = Constants.STRUCT_TYPE_NAME + "<";
-    typeNode = (ASTNode) typeNode.getChild(0);
-    int children = typeNode.getChildCount();
-    if(children <= 0)
-      throw new SemanticException("empty struct not allowed.");
-    for (int i = 0; i < children; i++) {
-      ASTNode child = (ASTNode) typeNode.getChild(i);
-      typeStr += unescapeIdentifier(child.getChild(0).getText()) + ":";
-      typeStr += getTypeStringFromAST((ASTNode) child.getChild(1));
-      if (i < children - 1)
-        typeStr += ",";
-    }
-
-    typeStr += ">";
-    return typeStr;
-  }
-
-  private List<FieldSchema> getColumns(ASTNode ast) throws SemanticException
-  {
-    List<FieldSchema> colList = new ArrayList<FieldSchema>();
-    int numCh = ast.getChildCount();
-    for (int i = 0; i < numCh; i++) {
-      FieldSchema col = new FieldSchema();
-      ASTNode child = (ASTNode)ast.getChild(i);
-      col.setName(unescapeIdentifier(child.getChild(0).getText()));
-      ASTNode typeChild = (ASTNode)(child.getChild(1));
-      col.setType(getTypeStringFromAST(typeChild));
-
-      if (child.getChildCount() == 3)
-        col.setComment(unescapeSQLString(child.getChild(2).getText()));
-      colList.add(col);
-    }
-    return colList;
-  }
-
-  private List<String> getColumnNames(ASTNode ast)
-  {
-    List<String> colList = new ArrayList<String>();
-    int numCh = ast.getChildCount();
-    for (int i = 0; i < numCh; i++) {
-      ASTNode child = (ASTNode)ast.getChild(i);
-      colList.add(unescapeIdentifier(child.getText()));
-    }
-    return colList;
-  }
-
-  private List<Order> getColumnNamesOrder(ASTNode ast)
-  {
-    List<Order> colList = new ArrayList<Order>();
-    int numCh = ast.getChildCount();
-    for (int i = 0; i < numCh; i++) {
-      ASTNode child = (ASTNode)ast.getChild(i);
-      if (child.getToken().getType() == HiveParser.TOK_TABSORTCOLNAMEASC)
-        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()), 1));
-      else
-        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()), 0));
-    }
-    return colList;
-  }
-
   /**
    * Get the fully qualified name in the ast. e.g. the ast of the form ^(DOT ^(DOT a b) c)
    * will generate a name of the form a.b.c
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
index 7ff18cf1a8..48840bbc4e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
@@ -96,8 +96,16 @@ public enum ErrorMsg {
   INVALID_MAPJOIN_HINT("neither table specified as map-table"),
   INVALID_MAPJOIN_TABLE("result of a union cannot be a map table"),
   NON_BUCKETED_TABLE("Sampling Expression Needed for Non-Bucketed Table"),
-  NEED_PARTITION_ERROR("need to specify partition columns because the destination table is partitioned.");
-
+  NEED_PARTITION_ERROR("need to specify partition columns because the destination table is partitioned."),
+  CTAS_CTLT_COEXISTENCE("Create table command does not allow LIKE and AS-SELECT in the same command"),
+  CTAS_COLLST_COEXISTENCE("Create table as select command cannot specify the list of columns for the target table."),
+  CTLT_COLLST_COEXISTENCE("Create table like command cannot specify the list of columns for the target table."),
+  INVALID_SELECT_SCHEMA("Cannot derive schema from the select-clause."),
+  CTAS_PARCOL_COEXISTENCE("CREATE-TABLE-AS-SELECT does not support partitioning in the target table."),
+  CTAS_MULTI_LOADFILE("CREATE-TABLE-AS-SELECT results in multiple file load."),
+  CTAS_EXTTBL_COEXISTENCE("CREATE-TABLE-AS-SELECT cannot create external table."),
+  TABLE_ALREADY_EXISTS("Table already exists:", "42S02");
+  
   private String mesg;
   private String SQLState;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g b/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
index c95b65f106..5a90f618f6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
@@ -215,8 +215,28 @@ createStatement
 @init { msgs.push("create statement"); }
 @after { msgs.pop(); }
     : KW_CREATE (ext=KW_EXTERNAL)? KW_TABLE ifNotExists? name=Identifier
-      ( like=KW_LIKE likeName=Identifier | (LPAREN columnNameTypeList RPAREN)? tableComment? tablePartition? tableBuckets? tableRowFormat? tableFileFormat? ) tableLocation?
-    -> ^(TOK_CREATETABLE $name $ext? ifNotExists? ^(TOK_LIKETABLE $likeName?) columnNameTypeList? tableComment? tablePartition? tableBuckets? tableRowFormat? tableFileFormat? tableLocation?)
+      (  like=KW_LIKE likeName=Identifier
+         tableLocation?
+       | (LPAREN columnNameTypeList RPAREN)? 
+         tableComment? 
+         tablePartition? 
+         tableBuckets? 
+         tableRowFormat? 
+         tableFileFormat? 
+         tableLocation?
+         (KW_AS selectStatement)?
+      )
+    -> ^(TOK_CREATETABLE $name $ext? ifNotExists? 
+         ^(TOK_LIKETABLE $likeName?) 
+         columnNameTypeList? 
+         tableComment? 
+         tablePartition? 
+         tableBuckets? 
+         tableRowFormat? 
+         tableFileFormat? 
+         tableLocation?
+         selectStatement?
+        )
     ;
 
 dropStatement
@@ -635,6 +655,11 @@ regular_body
                      selectClause whereClause? groupByClause? orderByClause? clusterByClause?
                      distributeByClause? sortByClause? limitClause?))
    |
+   selectStatement
+   ;
+
+selectStatement
+   :
    selectClause
    fromClause
    whereClause?
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java
index 75df6037d4..bd4666a193 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java
@@ -22,6 +22,7 @@
 
 import org.apache.hadoop.hive.ql.parse.QBParseInfo;
 import org.apache.hadoop.hive.ql.parse.QBMetaData;
+import org.apache.hadoop.hive.ql.plan.createTableDesc;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -46,6 +47,7 @@ public class QB {
   private QBJoinTree qbjoin;
   private String id;
   private boolean isQuery;
+  private createTableDesc tblDesc = null;   // table descriptor of the final results
 
   public void print(String msg) {
     LOG.info(msg + "alias=" + qbp.getAlias());
@@ -160,6 +162,21 @@ public boolean getIsQuery() {
   }
 
   public boolean isSelectStarQuery() {
-    return qbp.isSelectStarQuery() && aliasToSubq.isEmpty();
+    return qbp.isSelectStarQuery() && aliasToSubq.isEmpty() && !isCTAS();
+  }
+  
+  public createTableDesc getTableDesc() {
+    return tblDesc;
+  }
+
+  public void setTableDesc(createTableDesc desc) {
+    tblDesc = desc;
+  }
+  
+  /**
+   * Whether this QB is for a CREATE-TABLE-AS-SELECT.
+   */
+  public boolean isCTAS() {
+    return tblDesc != null;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 50edde83ef..bdf19c864f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -143,6 +143,28 @@
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 
+import java.util.regex.Pattern;
+import java.util.regex.Matcher;
+import org.apache.hadoop.hive.metastore.api.Order;
+import org.apache.hadoop.mapred.SequenceFileInputFormat;
+import org.apache.hadoop.mapred.SequenceFileOutputFormat;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.hive.ql.io.RCFileInputFormat;
+import org.apache.hadoop.hive.ql.io.RCFileOutputFormat;
+import org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe;
+import org.apache.hadoop.hive.ql.plan.DDLWork;
+import org.apache.hadoop.hive.ql.plan.createTableDesc;
+import org.apache.hadoop.hive.ql.plan.createTableLikeDesc;
+import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.serde2.SerDeUtils;
+import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.FetchOperator;
+import java.util.Collection;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.ql.hooks.WriteEntity;
+
 /**
  * Implementation of the semantic analyzer
  */
@@ -162,6 +184,14 @@ public class SemanticAnalyzer extends BaseSemanticAnalyzer {
   private int destTableId;
   private UnionProcContext uCtx;
   List<MapJoinOperator> listMapJoinOpsNoReducer;
+  
+  private static final String TEXTFILE_INPUT = TextInputFormat.class.getName();
+  private static final String TEXTFILE_OUTPUT = IgnoreKeyTextOutputFormat.class.getName();
+  private static final String SEQUENCEFILE_INPUT = SequenceFileInputFormat.class.getName();
+  private static final String SEQUENCEFILE_OUTPUT = SequenceFileOutputFormat.class.getName();
+  private static final String RCFILE_INPUT = RCFileInputFormat.class.getName();
+  private static final String RCFILE_OUTPUT = RCFileOutputFormat.class.getName();
+  private static final String COLUMNAR_SERDE = ColumnarSerDe.class.getName();
 
   private static class Phase1Ctx {
     String dest;
@@ -450,12 +480,11 @@ public void doPhase1(ASTNode ast, QB qb, Phase1Ctx ctx_1)
             doPhase1GetDistinctFuncExpr(aggregations));
         break;
 
-      case HiveParser.TOK_WHERE: {
+      case HiveParser.TOK_WHERE:
         qbp.setWhrExprForClause(ctx_1.dest, ast);
-      }
         break;
 
-      case HiveParser.TOK_DESTINATION: {
+      case HiveParser.TOK_DESTINATION:
         ctx_1.dest = "insclause-" + ctx_1.nextNum;
         ctx_1.nextNum++;
 
@@ -468,10 +497,9 @@ public void doPhase1(ASTNode ast, QB qb, Phase1Ctx ctx_1)
         }
 
         qbp.setDestForClause(ctx_1.dest, (ASTNode) ast.getChild(0));
-      }
         break;
 
-      case HiveParser.TOK_FROM: {
+      case HiveParser.TOK_FROM:
         int child_count = ast.getChildCount();
         if (child_count != 1)
           throw new SemanticException("Multiple Children " + child_count);
@@ -487,17 +515,15 @@ else if (isJoinToken(frm))
           processJoin(qb, frm);
           qbp.setJoinExpr(frm);
         }
-      }
         break;
 
-      case HiveParser.TOK_CLUSTERBY: {
+      case HiveParser.TOK_CLUSTERBY:
         // Get the clusterby aliases - these are aliased to the entries in the
         // select list
         qbp.setClusterByExprForClause(ctx_1.dest, ast);
-      }
         break;
 
-      case HiveParser.TOK_DISTRIBUTEBY: {
+      case HiveParser.TOK_DISTRIBUTEBY:
         // Get the distribute by  aliases - these are aliased to the entries in the
         // select list
         qbp.setDistributeByExprForClause(ctx_1.dest, ast);
@@ -507,10 +533,9 @@ else if (isJoinToken(frm))
         else if (qbp.getOrderByForClause(ctx_1.dest) != null) {
           throw new SemanticException(ErrorMsg.ORDERBY_DISTRIBUTEBY_CONFLICT.getMsg(ast));
         }
-      }
         break;
 
-      case HiveParser.TOK_SORTBY: {
+      case HiveParser.TOK_SORTBY:
         // Get the sort by aliases - these are aliased to the entries in the
         // select list
         qbp.setSortByExprForClause(ctx_1.dest, ast);
@@ -521,20 +546,18 @@ else if (qbp.getOrderByForClause(ctx_1.dest) != null) {
           throw new SemanticException(ErrorMsg.ORDERBY_SORTBY_CONFLICT.getMsg(ast));
         }
 
-      }
         break;
 
-      case HiveParser.TOK_ORDERBY: {
+      case HiveParser.TOK_ORDERBY:
         // Get the order by aliases - these are aliased to the entries in the
         // select list
         qbp.setOrderByExprForClause(ctx_1.dest, ast);
         if (qbp.getClusterByForClause(ctx_1.dest) != null) {
           throw new SemanticException(ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg(ast));
         }
-      }
         break;
 
-      case HiveParser.TOK_GROUPBY: {
+      case HiveParser.TOK_GROUPBY:
         // Get the groupby aliases - these are aliased to the entries in the
         // select list
         if (qbp.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
@@ -542,13 +565,10 @@ else if (qbp.getOrderByForClause(ctx_1.dest) != null) {
         }
         qbp.setGroupByExprForClause(ctx_1.dest, ast);
         skipRecursion = true;
-      }
         break;
 
       case HiveParser.TOK_LIMIT:
-        {
-          qbp.setDestLimit(ctx_1.dest, new Integer(ast.getChild(0).getText()));
-        }
+        qbp.setDestLimit(ctx_1.dest, new Integer(ast.getChild(0).getText()));
         break;
 
       case HiveParser.TOK_UNION:
@@ -673,7 +693,12 @@ public void getMetaData(QB qb) throws SemanticException {
             {
               fname = ctx.getMRTmpFileURI();
               ctx.setResDir(new Path(fname));
-              qb.setIsQuery(true);
+              
+              if ( qb.isCTAS() ) {
+                qb.setIsQuery(false);
+              } else {
+                qb.setIsQuery(true);
+              }
             }
             qb.getMetaData().setDestForAlias(name, fname,
                                              (ast.getToken().getType() == HiveParser.TOK_DIR));
@@ -2578,10 +2603,33 @@ private Operator genFileSinkPlan(String dest, QB qb,
         String cols = new String();
         String colTypes = new String();
         Vector<ColumnInfo> colInfos = inputRR.getColumnInfos();
+        
+        // CTAS case: the file output format and serde are defined by the create table command
+        // rather than taking the default value
+        List<FieldSchema> field_schemas = null;
+        createTableDesc tblDesc = qb.getTableDesc();
+        if ( tblDesc != null )
+          field_schemas = new ArrayList<FieldSchema>();
 
         boolean first = true;
         for (ColumnInfo colInfo:colInfos) {
           String[] nm = inputRR.reverseLookup(colInfo.getInternalName());
+          
+          if ( nm[1] != null ) { // non-null column alias
+            colInfo.setAlias(nm[1]);
+          }
+          
+          if ( field_schemas != null ) {
+            FieldSchema col = new FieldSchema();
+            if ( nm[1] != null ) {
+              col.setName(colInfo.getAlias());
+            } else {
+              col.setName(colInfo.getInternalName());
+            }
+            col.setType(colInfo.getType().getTypeName());
+            field_schemas.add(col);
+          }
+          
           if (!first) {
             cols = cols.concat(",");
             colTypes = colTypes.concat(":");
@@ -2605,6 +2653,10 @@ private Operator genFileSinkPlan(String dest, QB qb,
           else
             colTypes = colTypes.concat(tName);
         }
+        
+        // update the create table descriptor with the resulting schema. 
+        if ( tblDesc != null )
+          tblDesc.setCols(field_schemas);
 
         if (!ctx.isMRTmpFileURI(destStr)) {
           this.idToTableNameMap.put( String.valueOf(this.destTableId), destStr);
@@ -2616,8 +2668,13 @@ private Operator genFileSinkPlan(String dest, QB qb,
         this.loadFileWork.add(new loadFileDesc(queryTmpdir, destStr,
                                                isDfsDir, cols, colTypes));
 
-        table_desc = PlanUtils.getDefaultTableDesc(Integer.toString(Utilities.ctrlaCode),
-                                                   cols, colTypes, false);
+        if ( tblDesc == null ) {
+          table_desc = PlanUtils.getDefaultTableDesc(Integer.toString(Utilities.ctrlaCode),
+                                                     cols, colTypes, false);
+        } else {
+          table_desc = PlanUtils.getTableDesc(tblDesc, cols, colTypes);
+        } 
+         
         outputs.add(new WriteEntity(destStr, !isDfsDir));
         break;
     }
@@ -4322,15 +4379,33 @@ private void genMapRedTasks(QB qb) throws SemanticException {
 
       fetchTask = TaskFactory.get(fetch, this.conf);
       setFetchTask(fetchTask);
-    }
-    else {
+    } else {
       // First we generate the move work as this needs to be made dependent on all
       // the tasks that have a file sink operation
       List<moveWork>  mv = new ArrayList<moveWork>();
       for (loadTableDesc ltd : loadTableWork)
         mvTask.add(TaskFactory.get(new moveWork(null, null, ltd, null, false), this.conf));
-      for (loadFileDesc lfd : loadFileWork)
+      
+      boolean oneLoadFile = true;
+      for (loadFileDesc lfd : loadFileWork) {
+        if ( qb.isCTAS() ) {
+          assert(oneLoadFile); // should not have more than 1 load file for CTAS
+          // make the movetask's destination directory the table's destination.
+          String location = qb.getTableDesc().getLocation();
+          if ( location == null ) {
+            // get the table's default location
+            location = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
+            assert(location.length() > 0 );
+            if ( location.charAt(location.length()-1) != '/' ) {
+              location += '/';
+            }
+            location += qb.getTableDesc().getTableName();
+          }
+          lfd.setTargetDir(location);
+          oneLoadFile = false;
+        } 
         mvTask.add(TaskFactory.get(new moveWork(null, null, null, lfd, false), this.conf));
+      }
     }
 
     // generate map reduce plans
@@ -4380,8 +4455,53 @@ private void genMapRedTasks(QB qb) throws SemanticException {
     if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEJOBPROGRESS))
       for (Task<? extends Serializable> rootTask: rootTasks)
         generateCountersTask(rootTask);
+    
+    if ( qb.isCTAS() ) {
+      // generate a DDL task and make it a dependent task of the leaf
+      createTableDesc crtTblDesc = qb.getTableDesc();
+
+      validateCreateTable(crtTblDesc);
+          
+      // Clear the output for CTAS since we don't need the output from the mapredWork, the
+      // DDLWork at the tail of the chain will have the output
+      getOutputs().clear(); 
+      
+      Task<? extends Serializable> crtTblTask = 
+        TaskFactory.get(new DDLWork(getInputs(), getOutputs(), crtTblDesc), this.conf);
+        
+      // find all leaf tasks and make the DDLTask as a dependent task of all of them
+      HashSet<Task<? extends Serializable>> leaves = new HashSet<Task<? extends Serializable>>();
+      getLeafTasks(rootTasks, leaves);    
+      assert(leaves.size() > 0);
+      for ( Task<? extends Serializable> task: leaves ) {
+        task.addDependentTask(crtTblTask);
+      }
+    }
+  }
+
+  /**
+   * Find all leaf tasks of the list of root tasks.
+   */
+  private void getLeafTasks(   List<Task<? extends Serializable>> rootTasks,
+                            HashSet<Task<? extends Serializable>> leaves)   {
+
+    for ( Task<? extends Serializable> root : rootTasks ) {
+      getLeafTasks(root, leaves);
+    }
+  }
+  
+  private void getLeafTasks(        Task<? extends Serializable> task,
+                            HashSet<Task<? extends Serializable>> leaves) {
+    if ( task.getChildTasks() == null ) {
+      if ( ! leaves.contains(task) ) {
+        leaves.add(task);
+      }
+    } else {
+      getLeafTasks(task.getChildTasks(), leaves);
+    }
   }
 
+
   // loop over all the tasks recursviely
   private void generateCountersTask(Task<? extends Serializable> task) {
     if ((task instanceof MapRedTask) || (task instanceof ExecDriver)) {
@@ -4496,13 +4616,23 @@ public Phase1Ctx initPhase1Ctx() {
   @SuppressWarnings("nls")
   public void analyzeInternal(ASTNode ast) throws SemanticException {
     reset();
-
+    
     QB qb = new QB(null, null, false);
     this.qb = qb;
     this.ast = ast;
-
+    ASTNode child = ast;
+    
     LOG.info("Starting Semantic Analysis");
-    doPhase1(ast, qb, initPhase1Ctx());
+    
+    // analyze create table command
+    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) {
+      // if it is not CTAS, we don't need to go further and just return
+      if ( (child = analyzeCreateTable(ast, qb)) == null )
+         return;
+    }
+
+    // continue analyzing from the child ASTNode. 
+    doPhase1(child, qb, initPhase1Ctx());
     LOG.info("Completed phase 1 of Semantic Analysis");
 
     getMetaData(qb);
@@ -4511,7 +4641,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     genPlan(qb);
 
 
-    ParseContext pCtx = new ParseContext(conf, qb, ast, opToPartPruner, aliasToSamplePruner, topOps,
+    ParseContext pCtx = new ParseContext(conf, qb, child, opToPartPruner, aliasToSamplePruner, topOps,
                                          topSelOps, opParseCtx, joinContext, topToTable, loadTableWork, loadFileWork,
                                          ctx, idToTableNameMap, destTableId, uCtx, listMapJoinOpsNoReducer);
 
@@ -4652,4 +4782,339 @@ private void validate(Task<? extends Serializable> task) throws SemanticExceptio
       validate(childTask);
   }
 
+  
+  /**
+   * Get the row resolver given an operator. 
+   */
+  public RowResolver getRowResolver(Operator opt) {
+    return opParseCtx.get(opt).getRR(); 
+  }
+  
+  /**
+   * Analyze the create table command. If it is a regular create-table or create-table-like 
+   * statements, we create a DDLWork and return true. If it is a create-table-as-select, we get the
+   * necessary info such as the SerDe and Storage Format and put it in QB, and return false, indicating
+   * the rest of the semantic analyzer need to deal with the select statement with respect to the
+   * SerDe and Storage Format.
+   */
+  private ASTNode analyzeCreateTable(ASTNode ast, QB qb) 
+    throws SemanticException {
+    String            tableName     = unescapeIdentifier(ast.getChild(0).getText());
+    String            likeTableName = null;
+    List<FieldSchema> cols          = null;
+    List<FieldSchema> partCols      = null;
+    List<String>      bucketCols    = null;
+    List<Order>       sortCols      = null;
+    int               numBuckets    = -1;
+    String            fieldDelim    = null;
+    String            fieldEscape   = null;
+    String            collItemDelim = null;
+    String            mapKeyDelim   = null;
+    String            lineDelim     = null;
+    String            comment       = null;
+    String            inputFormat   = TEXTFILE_INPUT;
+    String            outputFormat  = TEXTFILE_OUTPUT;
+    String            location      = null;
+    String            serde         = null;
+    Map<String, String> mapProp     = null;
+    boolean           ifNotExists   = false;
+    boolean           isExt         = false;
+    ASTNode           selectStmt    = null;
+    final int         CREATE_TABLE  = 0;       // regular CREATE TABLE
+    final int         CTLT          = 1;       // CREATE TABLE LIKE ...      (CTLT)
+    final int         CTAS          = 2;       // CREATE TABLE AS SELECT ... (CTAS)
+    int               command_type  = CREATE_TABLE;
+
+    if ("SequenceFile".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVEDEFAULTFILEFORMAT))) {
+      inputFormat = SEQUENCEFILE_INPUT;
+      outputFormat = SEQUENCEFILE_OUTPUT;
+    }
+
+    LOG.info("Creating table" + tableName + " positin=" + ast.getCharPositionInLine());    
+    int numCh = ast.getChildCount();
+
+    /* Check the 1st-level children and do simple semantic checks:
+     * 1) CTLT and CTAS should not coexists.
+     * 2) CTLT or CTAS should not coexists with column list (target table schema).
+     * 3) CTAS does not support partitioning (for now).
+     */
+    for (int num = 1; num < numCh; num++)
+    {
+      ASTNode child = (ASTNode)ast.getChild(num);
+      switch (child.getToken().getType()) {
+        case HiveParser.TOK_IFNOTEXISTS:
+          ifNotExists = true;
+          break;
+        case HiveParser.KW_EXTERNAL:
+          isExt = true;
+          break;
+        case HiveParser.TOK_LIKETABLE:
+          if (child.getChildCount() > 0) {
+            likeTableName = unescapeIdentifier(child.getChild(0).getText());
+            if ( likeTableName != null ) {
+              if ( command_type == CTAS ) {
+                throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());
+              }
+              if ( cols != null ) {
+                throw new SemanticException(ErrorMsg.CTLT_COLLST_COEXISTENCE.getMsg());
+              }
+            }
+            command_type = CTLT;
+          }
+          break;
+        case HiveParser.TOK_QUERY:         // CTAS
+          if ( command_type == CTLT ) {
+            throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());
+          }
+          if ( cols != null ) {
+            throw new SemanticException(ErrorMsg.CTAS_COLLST_COEXISTENCE.getMsg());
+          }
+          // TODO: support partition for CTAS? 
+          if ( partCols != null || bucketCols != null ) {
+            throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());
+          }
+          if ( isExt ) {
+            throw new SemanticException(ErrorMsg.CTAS_EXTTBL_COEXISTENCE.getMsg());
+          }
+          command_type = CTAS; 
+          selectStmt   = child;
+          break;
+        case HiveParser.TOK_TABCOLLIST:
+          cols = getColumns(child);
+          break;
+        case HiveParser.TOK_TABLECOMMENT:
+          comment = unescapeSQLString(child.getChild(0).getText());
+          break;
+        case HiveParser.TOK_TABLEPARTCOLS:
+          partCols = getColumns((ASTNode)child.getChild(0));
+          break;
+        case HiveParser.TOK_TABLEBUCKETS:
+          bucketCols = getColumnNames((ASTNode)child.getChild(0));
+          if (child.getChildCount() == 2)
+            numBuckets = (Integer.valueOf(child.getChild(1).getText())).intValue();
+          else
+          {
+            sortCols = getColumnNamesOrder((ASTNode)child.getChild(1));
+            numBuckets = (Integer.valueOf(child.getChild(2).getText())).intValue();
+          }
+          break;
+        case HiveParser.TOK_TABLEROWFORMAT:
+
+          child = (ASTNode)child.getChild(0);
+          int numChildRowFormat = child.getChildCount();
+          for (int numC = 0; numC < numChildRowFormat; numC++)
+          {
+            ASTNode rowChild = (ASTNode)child.getChild(numC);
+            switch (rowChild.getToken().getType()) {
+              case HiveParser.TOK_TABLEROWFORMATFIELD:
+                fieldDelim = unescapeSQLString(rowChild.getChild(0).getText());
+                if (rowChild.getChildCount()>=2) {
+                  fieldEscape = unescapeSQLString(rowChild.getChild(1).getText());
+                }
+                break;
+              case HiveParser.TOK_TABLEROWFORMATCOLLITEMS:
+                collItemDelim = unescapeSQLString(rowChild.getChild(0).getText());
+                break;
+              case HiveParser.TOK_TABLEROWFORMATMAPKEYS:
+                mapKeyDelim = unescapeSQLString(rowChild.getChild(0).getText());
+                break;
+              case HiveParser.TOK_TABLEROWFORMATLINES:
+                lineDelim = unescapeSQLString(rowChild.getChild(0).getText());
+                break;
+              default: assert false;
+            }
+          }
+          break;
+        case HiveParser.TOK_TABLESERIALIZER:
+          
+          child = (ASTNode)child.getChild(0);
+          serde = unescapeSQLString(child.getChild(0).getText());
+          if (child.getChildCount() == 2) {
+            mapProp = new HashMap<String, String>();
+            ASTNode prop = (ASTNode)((ASTNode)child.getChild(1)).getChild(0);
+            for (int propChild = 0; propChild < prop.getChildCount(); propChild++) {
+              String key = unescapeSQLString(prop.getChild(propChild).getChild(0).getText());
+              String value = unescapeSQLString(prop.getChild(propChild).getChild(1).getText());
+              mapProp.put(key,value);
+            }
+          }
+          break;
+        case HiveParser.TOK_TBLSEQUENCEFILE:
+          inputFormat = SEQUENCEFILE_INPUT;
+          outputFormat = SEQUENCEFILE_OUTPUT;
+          break;
+        case HiveParser.TOK_TBLTEXTFILE:
+          inputFormat = TEXTFILE_INPUT;
+          outputFormat = TEXTFILE_OUTPUT;
+          break;
+        case HiveParser.TOK_TBLRCFILE:
+          inputFormat = RCFILE_INPUT;
+          outputFormat = RCFILE_OUTPUT;
+          serde = COLUMNAR_SERDE;
+          break;
+        case HiveParser.TOK_TABLEFILEFORMAT:
+          inputFormat = unescapeSQLString(child.getChild(0).getText());
+          outputFormat = unescapeSQLString(child.getChild(1).getText());
+          break;
+        case HiveParser.TOK_TABLELOCATION:
+          location = unescapeSQLString(child.getChild(0).getText());
+          break;
+        default: assert false;
+      }
+    }
+    
+    // check for existence of table
+    if ( ifNotExists ) {
+      try {
+        List<String> tables = this.db.getTablesByPattern(tableName);
+        if ( tables != null && tables.size() > 0 ) { // table exists
+          return null;
+        }
+      } catch (HiveException e) {
+        e.printStackTrace();
+      }
+    }
+
+    // Handle different types of CREATE TABLE command
+    createTableDesc crtTblDesc = null;
+    switch ( command_type ) {
+      
+      case CREATE_TABLE: // REGULAR CREATE TABLE DDL
+        crtTblDesc = 
+          new createTableDesc(tableName, isExt, cols, partCols, bucketCols, 
+                              sortCols, numBuckets,
+                              fieldDelim, fieldEscape,
+                              collItemDelim, mapKeyDelim, lineDelim,
+                              comment, inputFormat, outputFormat, location, serde, 
+                              mapProp, ifNotExists);
+        
+        validateCreateTable(crtTblDesc);
+        rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), crtTblDesc), conf));
+        break;
+        
+      case CTLT: // create table like <tbl_name>
+        createTableLikeDesc crtTblLikeDesc = 
+          new createTableLikeDesc(tableName, isExt, location, ifNotExists, likeTableName);
+        rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), crtTblLikeDesc), conf));
+        break;
+        
+      case CTAS: // create table as select
+      
+        // check for existence of table. Throw an exception if it exists.
+        try {
+          Table tab = this.db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName, 
+                                       false); // do not throw exception if table does not exist
+          
+          if ( tab != null ) {
+            throw new SemanticException(ErrorMsg.TABLE_ALREADY_EXISTS.getMsg(tableName)); 
+          }
+        } catch (HiveException e) { // may be unable to get meta data
+          throw new SemanticException(e); 
+        }
+                
+        crtTblDesc = 
+          new createTableDesc(tableName, isExt, cols, partCols, bucketCols, 
+                              sortCols, numBuckets,
+                              fieldDelim, fieldEscape,
+                              collItemDelim, mapKeyDelim, lineDelim,
+                              comment, inputFormat, outputFormat, location, serde, 
+                              mapProp, ifNotExists);
+        qb.setTableDesc(crtTblDesc);
+        
+        return selectStmt;
+      default: assert false; // should never be unknown command type
+    }
+    return null;
+  }
+ 
+  private void validateCreateTable(createTableDesc crtTblDesc) throws SemanticException {
+    // no duplicate column names
+    // currently, it is a simple n*n algorithm - this can be optimized later if need be
+    // but it should not be a major bottleneck as the number of columns are anyway not so big
+    
+    if((crtTblDesc.getCols() == null) || (crtTblDesc.getCols().size() == 0)) {
+      // for now make sure that serde exists
+      if(StringUtils.isEmpty(crtTblDesc.getSerName()) || SerDeUtils.isNativeSerDe(crtTblDesc.getSerName())) {
+        throw new SemanticException(ErrorMsg.INVALID_TBL_DDL_SERDE.getMsg());
+      }
+      return;
+    }
+    
+    try {
+      Class<?> origin = Class.forName(crtTblDesc.getOutputFormat(), true, JavaUtils.getClassLoader());
+      Class<? extends HiveOutputFormat> replaced = HiveFileFormatUtils.getOutputFormatSubstitute(origin);
+      if(replaced == null)
+        throw new SemanticException(ErrorMsg.INVALID_OUTPUT_FORMAT_TYPE.getMsg());
+    } catch (ClassNotFoundException e) {
+      throw new SemanticException(ErrorMsg.INVALID_OUTPUT_FORMAT_TYPE.getMsg());
+    }
+    
+    Iterator<FieldSchema> iterCols = crtTblDesc.getCols().iterator();
+    List<String> colNames = new ArrayList<String>();
+    while (iterCols.hasNext()) {
+      String colName = iterCols.next().getName();
+      Iterator<String> iter = colNames.iterator();
+      while (iter.hasNext()) {
+        String oldColName = iter.next();
+        if (colName.equalsIgnoreCase(oldColName)) 
+          throw new SemanticException(ErrorMsg.DUPLICATE_COLUMN_NAMES.getMsg());
+      }
+      colNames.add(colName);
+    }
+
+    if (crtTblDesc.getBucketCols() != null)
+    {    
+      // all columns in cluster and sort are valid columns
+      Iterator<String> bucketCols = crtTblDesc.getBucketCols().iterator();
+      while (bucketCols.hasNext()) {
+        String bucketCol = bucketCols.next();
+        boolean found = false;
+        Iterator<String> colNamesIter = colNames.iterator();
+        while (colNamesIter.hasNext()) {
+          String colName = colNamesIter.next();
+          if (bucketCol.equalsIgnoreCase(colName)) {
+            found = true;
+            break;
+          }
+        }
+        if (!found)
+          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg());
+      }
+    }
+
+    if (crtTblDesc.getSortCols() != null)
+    {
+      // all columns in cluster and sort are valid columns
+      Iterator<Order> sortCols = crtTblDesc.getSortCols().iterator();
+      while (sortCols.hasNext()) {
+        String sortCol = sortCols.next().getCol();
+        boolean found = false;
+        Iterator<String> colNamesIter = colNames.iterator();
+        while (colNamesIter.hasNext()) {
+          String colName = colNamesIter.next();
+          if (sortCol.equalsIgnoreCase(colName)) {
+            found = true;
+            break;
+          }
+        }
+        if (!found)
+          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg());
+      }
+    }
+    
+    if (crtTblDesc.getPartCols() != null)
+    {
+      // there is no overlap between columns and partitioning columns
+      Iterator<FieldSchema> partColsIter = crtTblDesc.getPartCols().iterator();
+      while (partColsIter.hasNext()) {
+        String partCol = partColsIter.next().getName();
+        Iterator<String> colNamesIter = colNames.iterator();
+        while (colNamesIter.hasNext()) {
+          String colName = unescapeIdentifier(colNamesIter.next());
+          if (partCol.equalsIgnoreCase(colName)) 
+            throw new SemanticException(ErrorMsg.COLUMN_REPEATED_IN_PARTITIONING_COLS.getMsg());
+        }
+      }
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
index 9b3f8a9ec8..863884f9c0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
@@ -62,8 +62,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree) throws Seman
       switch (tree.getToken().getType()) {
       case HiveParser.TOK_EXPLAIN: return new ExplainSemanticAnalyzer(conf);
       case HiveParser.TOK_LOAD: return new LoadSemanticAnalyzer(conf);
-      case HiveParser.TOK_CREATETABLE:
-      case HiveParser.TOK_DROPTABLE:
+      case HiveParser.TOK_DROPTABLE: 
       case HiveParser.TOK_DESCTABLE:
       case HiveParser.TOK_DESCFUNCTION:
       case HiveParser.TOK_MSCK:
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
index 67a51a3494..e48960e4e9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
@@ -38,9 +38,15 @@
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.mapred.TextInputFormat;
 import org.apache.hadoop.hive.serde2.Deserializer;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 
 public class PlanUtils {
-
+  
+  protected final static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.plan.PlanUtils");
+  
   public static enum ExpressionTypes {FIELD, JEXL};
 
   @SuppressWarnings("nls")
@@ -141,6 +147,42 @@ public static tableDesc getTableDesc(Class<? extends Deserializer> serdeClass,
       properties);    
   }
   
+  /**
+   * Generate a table descriptor from a createTableDesc.
+   */
+  public static tableDesc getTableDesc(createTableDesc crtTblDesc, String cols, String colTypes) {
+    
+    Class<? extends Deserializer> serdeClass = LazySimpleSerDe.class;
+    String separatorCode                     = Integer.toString(Utilities.ctrlaCode);
+    String columns                           = cols;
+    String columnTypes                       = colTypes;
+    boolean lastColumnTakesRestOfTheLine     = false;
+    tableDesc ret;
+
+    try {
+      if ( crtTblDesc.getSerName() != null ) {
+        Class c = Class.forName(crtTblDesc.getSerName());
+        serdeClass = c;
+      }
+    
+      ret = getTableDesc(serdeClass, separatorCode, columns, columnTypes, 
+                                   lastColumnTakesRestOfTheLine, false);
+      
+      // replace the default input & output file format with those found in crtTblDesc
+      Class c1 = Class.forName(crtTblDesc.getInputFormat());
+      Class c2 = Class.forName(crtTblDesc.getOutputFormat());
+      Class<? extends InputFormat>      in_class  = c1;
+      Class<? extends HiveOutputFormat> out_class = c2;
+    
+      ret.setInputFileFormatClass(in_class);
+      ret.setOutputFileFormatClass(out_class);
+    } catch (ClassNotFoundException e) {
+      e.printStackTrace();
+      return null;
+    }
+    return ret;
+  }
+  
   /** 
    * Generate the table descriptor of MetadataTypedColumnsetSerDe with the separatorCode.
    * MetaDataTypedColumnsetSerDe is used because LazySimpleSerDe does not support a table
@@ -382,6 +424,7 @@ public static reduceSinkDesc getReduceSinkDesc(
     return getReduceSinkDesc(keyCols, valueCols, outputColumnNames, includeKey, tag, partitionCols, order.toString(),
          numReducers);
   }
+  
 
 }
   
diff --git a/ql/src/test/queries/clientnegative/ctas.q b/ql/src/test/queries/clientnegative/ctas.q
new file mode 100644
index 0000000000..8685b1abfc
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/ctas.q
@@ -0,0 +1,5 @@
+drop table nzhang_ctas4;
+
+create external table nzhang_ctas4 as select key, value from src;
+
+drop table nzhang_ctas4;
diff --git a/ql/src/test/queries/clientpositive/ctas.q b/ql/src/test/queries/clientpositive/ctas.q
new file mode 100644
index 0000000000..8d49674fd5
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/ctas.q
@@ -0,0 +1,34 @@
+drop table nzhang_ctas1;
+drop table nzhang_ctas2;
+drop table nzhang_ctas3;
+
+explain create table nzhang_ctas1 as select key k, value from src sort by k, value limit 10;
+
+create table nzhang_ctas1 as select key k, value from src sort by k, value limit 10;
+
+select * from nzhang_ctas1;
+
+
+explain create table nzhang_ctas2 as select * from src sort by key, value limit 10;
+
+create table nzhang_ctas2 as select * from src sort by key, value limit 10;
+
+select * from nzhang_ctas2;
+
+
+explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;
+
+create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;
+
+select * from nzhang_ctas3;
+
+
+explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;
+
+create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;
+
+select * from nzhang_ctas3;
+
+drop table nzhang_ctas1;
+drop table nzhang_ctas2;
+drop table nzhang_ctas3;
diff --git a/ql/src/test/results/clientnegative/ctas.q.out b/ql/src/test/results/clientnegative/ctas.q.out
new file mode 100644
index 0000000000..53cd41728b
--- /dev/null
+++ b/ql/src/test/results/clientnegative/ctas.q.out
@@ -0,0 +1,5 @@
+PREHOOK: query: drop table nzhang_ctas4
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table nzhang_ctas4
+POSTHOOK: type: DROPTABLE
+FAILED: Error in semantic analysis: CREATE-TABLE-AS-SELECT cannot create external table.
diff --git a/ql/src/test/results/clientpositive/create_1.q.out b/ql/src/test/results/clientpositive/create_1.q.out
index 48cd673e39..8ddb626a92 100644
--- a/ql/src/test/results/clientpositive/create_1.q.out
+++ b/ql/src/test/results/clientpositive/create_1.q.out
@@ -28,12 +28,11 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:njain, createTime:1253779809, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:nzhang, createTime:1254242350, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
 PREHOOK: query: CREATE TABLE IF NOT EXISTS table1 (a STRING, b STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE IF NOT EXISTS table1 (a STRING, b STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@table1
 PREHOOK: query: CREATE TABLE IF NOT EXISTS table2 (a STRING, b INT) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE IF NOT EXISTS table2 (a STRING, b INT) STORED AS TEXTFILE
@@ -52,7 +51,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:table2, dbName:default, owner:njain, createTime:1253779809, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:table2, dbName:default, owner:nzhang, createTime:1254242351, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
 PREHOOK: query: CREATE TABLE table3 (a STRING, b STRING)
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
 STORED AS TEXTFILE
@@ -75,7 +74,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table3, dbName:default, owner:njain, createTime:1253779810, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=9,field.delim=	}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})
+Detailed Table Information	Table(tableName:table3, dbName:default, owner:nzhang, createTime:1254242351, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=9,field.delim=	}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})
 PREHOOK: query: DROP TABLE table1
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: DROP TABLE table1
diff --git a/ql/src/test/results/clientpositive/create_like.q.out b/ql/src/test/results/clientpositive/create_like.q.out
index 3016daeece..a962eb7e44 100644
--- a/ql/src/test/results/clientpositive/create_like.q.out
+++ b/ql/src/test/results/clientpositive/create_like.q.out
@@ -28,7 +28,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:njain, createTime:1253779841, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:nzhang, createTime:1254243861, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
 PREHOOK: query: CREATE TABLE table2 LIKE table1
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE table2 LIKE table1
@@ -47,17 +47,15 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table2, dbName:default, owner:njain, createTime:1253779841, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE})	
+Detailed Table Information	Table(tableName:table2, dbName:default, owner:nzhang, createTime:1254243861, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE})	
 PREHOOK: query: CREATE TABLE IF NOT EXISTS table2 LIKE table1
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE IF NOT EXISTS table2 LIKE table1
 POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@table2
 PREHOOK: query: CREATE EXTERNAL TABLE IF NOT EXISTS table2 LIKE table1
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE EXTERNAL TABLE IF NOT EXISTS table2 LIKE table1
 POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@table2
 PREHOOK: query: CREATE EXTERNAL TABLE IF NOT EXISTS table3 LIKE table1
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE EXTERNAL TABLE IF NOT EXISTS table3 LIKE table1
@@ -76,7 +74,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table3, dbName:default, owner:njain, createTime:1253779841, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=TRUE})	
+Detailed Table Information	Table(tableName:table3, dbName:default, owner:nzhang, createTime:1254243861, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=TRUE})	
 PREHOOK: query: INSERT OVERWRITE TABLE table1 SELECT key, value FROM src WHERE key = 86
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -96,20 +94,20 @@ POSTHOOK: Output: default@table2
 PREHOOK: query: SELECT * FROM table1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/20199794/10000
+PREHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1104038861/10000
 POSTHOOK: query: SELECT * FROM table1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/20199794/10000
+POSTHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1104038861/10000
 86	val_86
 PREHOOK: query: SELECT * FROM table2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table2
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1027337637/10000
+PREHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1673347492/10000
 POSTHOOK: query: SELECT * FROM table2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table2
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1027337637/10000
+POSTHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1673347492/10000
 100	val_100
 100	val_100
 PREHOOK: query: DROP TABLE table1
diff --git a/ql/src/test/results/clientpositive/ctas.q.out b/ql/src/test/results/clientpositive/ctas.q.out
new file mode 100644
index 0000000000..edaacbb88c
--- /dev/null
+++ b/ql/src/test/results/clientpositive/ctas.q.out
@@ -0,0 +1,421 @@
+PREHOOK: query: drop table nzhang_ctas1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table nzhang_ctas1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table nzhang_ctas2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table nzhang_ctas2
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table nzhang_ctas3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table nzhang_ctas3
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: explain create table nzhang_ctas1 as select key k, value from src sort by k, value limit 10
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: explain create table nzhang_ctas1 as select key k, value from src sort by k, value limit 10
+POSTHOOK: type: CREATETABLE
+ABSTRACT SYNTAX TREE:
+  (TOK_CREATETABLE nzhang_ctas1 TOK_LIKETABLE (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key) k) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL k)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 10))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+              outputColumnNames: _col0, _col1
+              Reduce Output Operator
+                key expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                sort order: ++
+                tag: -1
+                value expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+      Reduce Operator Tree:
+        Extract
+          Limit
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1066079488/10002 
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: string
+                    expr: _col1
+                    type: string
+              sort order: ++
+              tag: -1
+              value expressions:
+                    expr: _col0
+                    type: string
+                    expr: _col1
+                    type: string
+      Reduce Operator Tree:
+        Extract
+          Limit
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Move Operator
+      files:
+          hdfs directory: true
+          destination: file:///data/users/nzhang/work/31/apache-hive-trunk/ql/../build/ql/test/data/warehouse//nzhang_ctas1
+
+  Stage: Stage-3
+      Create Table Operator:
+        Create Table
+          columns: k string, value string
+          if not exists: false
+          input format: org.apache.hadoop.mapred.TextInputFormat
+          # buckets: -1
+          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+          name: nzhang_ctas1
+          isExternal: false
+
+
+PREHOOK: query: create table nzhang_ctas1 as select key k, value from src sort by k, value limit 10
+PREHOOK: type: CREATETABLE
+PREHOOK: Input: default@src
+POSTHOOK: query: create table nzhang_ctas1 as select key k, value from src sort by k, value limit 10
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@nzhang_ctas1
+PREHOOK: query: select * from nzhang_ctas1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@nzhang_ctas1
+PREHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1419754075/10000
+POSTHOOK: query: select * from nzhang_ctas1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@nzhang_ctas1
+POSTHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1419754075/10000
+0	val_0
+0	val_0
+0	val_0
+10	val_10
+100	val_100
+100	val_100
+103	val_103
+103	val_103
+104	val_104
+104	val_104
+PREHOOK: query: explain create table nzhang_ctas2 as select * from src sort by key, value limit 10
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: explain create table nzhang_ctas2 as select * from src sort by key, value limit 10
+POSTHOOK: type: CREATETABLE
+ABSTRACT SYNTAX TREE:
+  (TOK_CREATETABLE nzhang_ctas2 TOK_LIKETABLE (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 10))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+              outputColumnNames: _col0, _col1
+              Reduce Output Operator
+                key expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                sort order: ++
+                tag: -1
+                value expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+      Reduce Operator Tree:
+        Extract
+          Limit
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/574523558/10002 
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: string
+                    expr: _col1
+                    type: string
+              sort order: ++
+              tag: -1
+              value expressions:
+                    expr: _col0
+                    type: string
+                    expr: _col1
+                    type: string
+      Reduce Operator Tree:
+        Extract
+          Limit
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Move Operator
+      files:
+          hdfs directory: true
+          destination: file:///data/users/nzhang/work/31/apache-hive-trunk/ql/../build/ql/test/data/warehouse//nzhang_ctas2
+
+  Stage: Stage-3
+      Create Table Operator:
+        Create Table
+          columns: key string, value string
+          if not exists: false
+          input format: org.apache.hadoop.mapred.TextInputFormat
+          # buckets: -1
+          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+          name: nzhang_ctas2
+          isExternal: false
+
+
+PREHOOK: query: create table nzhang_ctas2 as select * from src sort by key, value limit 10
+PREHOOK: type: CREATETABLE
+PREHOOK: Input: default@src
+POSTHOOK: query: create table nzhang_ctas2 as select * from src sort by key, value limit 10
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@nzhang_ctas2
+PREHOOK: query: select * from nzhang_ctas2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@nzhang_ctas2
+PREHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1229264510/10000
+POSTHOOK: query: select * from nzhang_ctas2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@nzhang_ctas2
+POSTHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1229264510/10000
+0	val_0
+0	val_0
+0	val_0
+10	val_10
+100	val_100
+100	val_100
+103	val_103
+103	val_103
+104	val_104
+104	val_104
+PREHOOK: query: explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
+POSTHOOK: type: CREATETABLE
+ABSTRACT SYNTAX TREE:
+  (TOK_CREATETABLE nzhang_ctas3 TOK_LIKETABLE (TOK_TABLESERIALIZER (TOK_SERDENAME "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe")) TOK_TBLRCFILE (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (/ (TOK_TABLE_OR_COL key) 2) half_key) (TOK_SELEXPR (TOK_FUNCTION concat (TOK_TABLE_OR_COL value) "_con") conb)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL half_key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL conb))) (TOK_LIMIT 10))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Select Operator
+              expressions:
+                    expr: (key / 2)
+                    type: double
+                    expr: concat(value, '_con')
+                    type: string
+              outputColumnNames: _col0, _col1
+              Reduce Output Operator
+                key expressions:
+                      expr: _col0
+                      type: double
+                      expr: _col1
+                      type: string
+                sort order: ++
+                tag: -1
+                value expressions:
+                      expr: _col0
+                      type: double
+                      expr: _col1
+                      type: string
+      Reduce Operator Tree:
+        Extract
+          Limit
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1141672779/10002 
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: double
+                    expr: _col1
+                    type: string
+              sort order: ++
+              tag: -1
+              value expressions:
+                    expr: _col0
+                    type: double
+                    expr: _col1
+                    type: string
+      Reduce Operator Tree:
+        Extract
+          Limit
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+
+  Stage: Stage-0
+    Move Operator
+      files:
+          hdfs directory: true
+          destination: file:///data/users/nzhang/work/31/apache-hive-trunk/ql/../build/ql/test/data/warehouse//nzhang_ctas3
+
+  Stage: Stage-3
+      Create Table Operator:
+        Create Table
+          columns: half_key double, conb string
+          if not exists: false
+          input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+          # buckets: -1
+          output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+          serde name: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+          name: nzhang_ctas3
+          isExternal: false
+
+
+PREHOOK: query: create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
+PREHOOK: type: CREATETABLE
+PREHOOK: Input: default@src
+POSTHOOK: query: create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@nzhang_ctas3
+PREHOOK: query: select * from nzhang_ctas3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@nzhang_ctas3
+PREHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1336526193/10000
+POSTHOOK: query: select * from nzhang_ctas3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@nzhang_ctas3
+POSTHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1336526193/10000
+0.0	val_0_con
+0.0	val_0_con
+0.0	val_0_con
+1.0	val_2_con
+2.0	val_4_con
+2.5	val_5_con
+2.5	val_5_con
+2.5	val_5_con
+4.0	val_8_con
+4.5	val_9_con
+PREHOOK: query: explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
+POSTHOOK: type: CREATETABLE
+ABSTRACT SYNTAX TREE:
+  (TOK_CREATETABLE nzhang_ctas3 TOK_IFNOTEXISTS TOK_LIKETABLE (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 2))))
+
+STAGE DEPENDENCIES:
+
+STAGE PLANS:
+STAGE PLANS:
+PREHOOK: query: create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
+POSTHOOK: type: CREATETABLE
+PREHOOK: query: select * from nzhang_ctas3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@nzhang_ctas3
+PREHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1592752406/10000
+POSTHOOK: query: select * from nzhang_ctas3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@nzhang_ctas3
+POSTHOOK: Output: file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/tmp/1592752406/10000
+0.0	val_0_con
+0.0	val_0_con
+0.0	val_0_con
+1.0	val_2_con
+2.0	val_4_con
+2.5	val_5_con
+2.5	val_5_con
+2.5	val_5_con
+4.0	val_8_con
+4.5	val_9_con
+PREHOOK: query: drop table nzhang_ctas1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table nzhang_ctas1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Output: default@nzhang_ctas1
+PREHOOK: query: drop table nzhang_ctas2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table nzhang_ctas2
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Output: default@nzhang_ctas2
+PREHOOK: query: drop table nzhang_ctas3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table nzhang_ctas3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Output: default@nzhang_ctas3
diff --git a/ql/src/test/results/compiler/plan/cast1.q.xml b/ql/src/test/results/compiler/plan/cast1.q.xml
index f7dc6ad2da..8f81a2f09a 100644
--- a/ql/src/test/results/compiler/plan/cast1.q.xml
+++ b/ql/src/test/results/compiler/plan/cast1.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -100,7 +100,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/2039536853/10001</string> 
+                           <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/1021766579/10001</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -647,6 +647,9 @@
                        <object class="java.util.Vector"> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c0</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col0</string> 
                           </void> 
@@ -657,6 +660,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c1</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col1</string> 
                           </void> 
@@ -667,6 +673,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c2</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col2</string> 
                           </void> 
@@ -677,6 +686,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c3</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col3</string> 
                           </void> 
@@ -687,6 +699,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c4</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col4</string> 
                           </void> 
@@ -697,6 +712,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c5</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col5</string> 
                           </void> 
@@ -707,6 +725,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c6</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col6</string> 
                           </void> 
@@ -962,7 +983,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -974,7 +995,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/groupby2.q.xml b/ql/src/test/results/compiler/plan/groupby2.q.xml
index 5ba23d29d0..e8a85213e2 100755
--- a/ql/src/test/results/compiler/plan/groupby2.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby2.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -851,7 +851,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -863,7 +863,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
@@ -904,7 +904,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/265864502/10001</string> 
+                 <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/103545573/10001</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1141,6 +1141,9 @@
              <object class="java.util.Vector"> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c0</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col0</string> 
                 </void> 
@@ -1151,6 +1154,9 @@
               </void> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c1</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col1</string> 
                 </void> 
@@ -1161,6 +1167,9 @@
               </void> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c2</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col2</string> 
                 </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby3.q.xml b/ql/src/test/results/compiler/plan/groupby3.q.xml
index b607c641e7..d7e3c68f19 100644
--- a/ql/src/test/results/compiler/plan/groupby3.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby3.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -1035,7 +1035,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1047,7 +1047,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
@@ -1088,7 +1088,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/316288666/10001</string> 
+                 <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/1108997379/10001</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1351,6 +1351,9 @@
              <object class="java.util.Vector"> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c0</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col0</string> 
                 </void> 
@@ -1361,6 +1364,9 @@
               </void> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c1</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col1</string> 
                 </void> 
@@ -1371,6 +1377,9 @@
               </void> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c2</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col2</string> 
                 </void> 
@@ -1381,6 +1390,9 @@
               </void> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c3</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col3</string> 
                 </void> 
@@ -1391,6 +1403,9 @@
               </void> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c4</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col4</string> 
                 </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby4.q.xml b/ql/src/test/results/compiler/plan/groupby4.q.xml
index e962447c70..655c5c7c0c 100644
--- a/ql/src/test/results/compiler/plan/groupby4.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby4.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -525,7 +525,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -537,7 +537,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
@@ -578,7 +578,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/1892370332/10001</string> 
+                 <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/343558840/10001</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -721,6 +721,9 @@
              <object class="java.util.Vector"> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c0</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col0</string> 
                 </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby5.q.xml b/ql/src/test/results/compiler/plan/groupby5.q.xml
index 179a46a957..548f2ce976 100644
--- a/ql/src/test/results/compiler/plan/groupby5.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby5.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -608,7 +608,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -620,7 +620,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
@@ -661,7 +661,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/247531555/10001</string> 
+                 <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/278829648/10001</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -834,6 +834,9 @@
              <object class="java.util.Vector"> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>key</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col0</string> 
                 </void> 
@@ -844,6 +847,9 @@
               </void> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c1</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col1</string> 
                 </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby6.q.xml b/ql/src/test/results/compiler/plan/groupby6.q.xml
index 83cc498d3b..df53391b0e 100644
--- a/ql/src/test/results/compiler/plan/groupby6.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby6.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -525,7 +525,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -537,7 +537,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
@@ -578,7 +578,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/1272715718/10001</string> 
+                 <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/681541560/10001</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -721,6 +721,9 @@
              <object class="java.util.Vector"> 
               <void method="add"> 
                <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                <void property="alias"> 
+                 <string>_c0</string> 
+                </void> 
                 <void property="internalName"> 
                  <string>_col0</string> 
                 </void> 
diff --git a/ql/src/test/results/compiler/plan/input20.q.xml b/ql/src/test/results/compiler/plan/input20.q.xml
index 453e3d6425..9f5da90156 100644
--- a/ql/src/test/results/compiler/plan/input20.q.xml
+++ b/ql/src/test/results/compiler/plan/input20.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -672,7 +672,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>tmap:src</string> 
@@ -684,7 +684,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
@@ -729,7 +729,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/1182298917/10001</string> 
+                     <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/1082858174/10001</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -921,6 +921,9 @@
                  <object class="java.util.Vector"> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>key</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>key</string> 
                     </void> 
@@ -931,6 +934,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>value</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>value</string> 
                     </void> 
diff --git a/ql/src/test/results/compiler/plan/input3.q.xml b/ql/src/test/results/compiler/plan/input3.q.xml
index 6e107da9f8..b159269b86 100755
--- a/ql/src/test/results/compiler/plan/input3.q.xml
+++ b/ql/src/test/results/compiler/plan/input3.q.xml
@@ -3464,6 +3464,9 @@
                    <object class="java.util.Vector"> 
                     <void method="add"> 
                      <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                      <void property="alias"> 
+                       <string>value</string> 
+                      </void> 
                       <void property="internalName"> 
                        <string>_col0</string> 
                       </void> 
diff --git a/ql/src/test/results/compiler/plan/input8.q.xml b/ql/src/test/results/compiler/plan/input8.q.xml
index 75226a7e76..671e265598 100644
--- a/ql/src/test/results/compiler/plan/input8.q.xml
+++ b/ql/src/test/results/compiler/plan/input8.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src1</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src1</string> 
            </void> 
           </object> 
          </void> 
@@ -92,7 +92,7 @@
                 <void property="conf"> 
                  <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                   <void property="dirName"> 
-                   <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/1997202856/10001</string> 
+                   <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/633479289/10001</string> 
                   </void> 
                   <void property="tableInfo"> 
                    <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -375,6 +375,9 @@
                <object class="java.util.Vector"> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c0</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col0</string> 
                   </void> 
@@ -385,6 +388,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c1</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col1</string> 
                   </void> 
@@ -395,6 +401,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c2</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col2</string> 
                   </void> 
@@ -476,7 +485,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src1</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src1</string> 
@@ -488,7 +497,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src1</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/input_part1.q.xml b/ql/src/test/results/compiler/plan/input_part1.q.xml
index d60ed18951..2cf5c80899 100644
--- a/ql/src/test/results/compiler/plan/input_part1.q.xml
+++ b/ql/src/test/results/compiler/plan/input_part1.q.xml
@@ -77,7 +77,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/srcpart</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/srcpart</string> 
            </void> 
           </object> 
          </void> 
@@ -113,7 +113,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/1086554636/10001</string> 
+                           <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/1946178726/10001</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -350,6 +350,9 @@
                        <object class="java.util.Vector"> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>key</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col0</string> 
                           </void> 
@@ -360,6 +363,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>value</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col1</string> 
                           </void> 
@@ -370,6 +376,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>hr</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col2</string> 
                           </void> 
@@ -380,6 +389,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>ds</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col3</string> 
                           </void> 
@@ -952,7 +964,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>srcpart</string> 
@@ -964,7 +976,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/input_testxpath.q.xml b/ql/src/test/results/compiler/plan/input_testxpath.q.xml
index c75197c710..083e671692 100644
--- a/ql/src/test/results/compiler/plan/input_testxpath.q.xml
+++ b/ql/src/test/results/compiler/plan/input_testxpath.q.xml
@@ -68,7 +68,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src_thrift</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
            </void> 
           </object> 
          </void> 
@@ -96,7 +96,7 @@
                 <void property="conf"> 
                  <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                   <void property="dirName"> 
-                   <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/677556742/10001</string> 
+                   <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/328853814/10001</string> 
                   </void> 
                   <void property="tableInfo"> 
                    <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -407,6 +407,9 @@
                <object class="java.util.Vector"> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c0</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col0</string> 
                   </void> 
@@ -417,6 +420,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>mystring</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col1</string> 
                   </void> 
@@ -427,6 +433,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c2</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col2</string> 
                   </void> 
@@ -558,7 +567,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src_thrift</string> 
@@ -570,7 +579,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/input_testxpath2.q.xml b/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
index dfe8f582ac..d07befefcf 100644
--- a/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
+++ b/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
@@ -68,7 +68,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src_thrift</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
            </void> 
           </object> 
          </void> 
@@ -104,7 +104,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>file:/data/users/njain/hive3/hive3/build/ql/tmp/1791992572/10001</string> 
+                           <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/28255874/10001</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -372,6 +372,9 @@
                        <object class="java.util.Vector"> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c0</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col0</string> 
                           </void> 
@@ -382,6 +385,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c1</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col1</string> 
                           </void> 
@@ -392,6 +398,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c2</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col2</string> 
                           </void> 
@@ -807,7 +816,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src_thrift</string> 
@@ -819,7 +828,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/join4.q.xml b/ql/src/test/results/compiler/plan/join4.q.xml
index 40926f0ed7..5e71bc2d28 100644
--- a/ql/src/test/results/compiler/plan/join4.q.xml
+++ b/ql/src/test/results/compiler/plan/join4.q.xml
@@ -1846,6 +1846,9 @@
                  <object class="java.util.Vector"> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c1</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col0</string> 
                     </void> 
@@ -1856,6 +1859,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c2</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col1</string> 
                     </void> 
@@ -1866,6 +1872,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c3</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col2</string> 
                     </void> 
@@ -1876,6 +1885,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c4</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col3</string> 
                     </void> 
diff --git a/ql/src/test/results/compiler/plan/join5.q.xml b/ql/src/test/results/compiler/plan/join5.q.xml
index 830fa28b67..2e48d40901 100644
--- a/ql/src/test/results/compiler/plan/join5.q.xml
+++ b/ql/src/test/results/compiler/plan/join5.q.xml
@@ -1846,6 +1846,9 @@
                  <object class="java.util.Vector"> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c1</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col0</string> 
                     </void> 
@@ -1856,6 +1859,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c2</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col1</string> 
                     </void> 
@@ -1866,6 +1872,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c3</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col2</string> 
                     </void> 
@@ -1876,6 +1885,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c4</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col3</string> 
                     </void> 
diff --git a/ql/src/test/results/compiler/plan/join6.q.xml b/ql/src/test/results/compiler/plan/join6.q.xml
index 94052b5b39..7a196eede4 100644
--- a/ql/src/test/results/compiler/plan/join6.q.xml
+++ b/ql/src/test/results/compiler/plan/join6.q.xml
@@ -1846,6 +1846,9 @@
                  <object class="java.util.Vector"> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c1</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col0</string> 
                     </void> 
@@ -1856,6 +1859,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c2</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col1</string> 
                     </void> 
@@ -1866,6 +1872,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c3</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col2</string> 
                     </void> 
@@ -1876,6 +1885,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c4</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col3</string> 
                     </void> 
diff --git a/ql/src/test/results/compiler/plan/join7.q.xml b/ql/src/test/results/compiler/plan/join7.q.xml
index 2f255151b2..e8acbd6297 100644
--- a/ql/src/test/results/compiler/plan/join7.q.xml
+++ b/ql/src/test/results/compiler/plan/join7.q.xml
@@ -2669,6 +2669,9 @@
                  <object class="java.util.Vector"> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c1</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col0</string> 
                     </void> 
@@ -2679,6 +2682,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c2</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col1</string> 
                     </void> 
@@ -2689,6 +2695,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c3</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col2</string> 
                     </void> 
@@ -2699,6 +2708,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c4</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col3</string> 
                     </void> 
@@ -2709,6 +2721,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c5</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col4</string> 
                     </void> 
@@ -2719,6 +2734,9 @@
                   </void> 
                   <void method="add"> 
                    <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                    <void property="alias"> 
+                     <string>c6</string> 
+                    </void> 
                     <void property="internalName"> 
                      <string>_col5</string> 
                     </void> 
diff --git a/ql/src/test/results/compiler/plan/join8.q.xml b/ql/src/test/results/compiler/plan/join8.q.xml
index 01fb1ed120..2b333aa945 100644
--- a/ql/src/test/results/compiler/plan/join8.q.xml
+++ b/ql/src/test/results/compiler/plan/join8.q.xml
@@ -1850,6 +1850,9 @@
                      <object class="java.util.Vector"> 
                       <void method="add"> 
                        <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                        <void property="alias"> 
+                         <string>c1</string> 
+                        </void> 
                         <void property="internalName"> 
                          <string>_col0</string> 
                         </void> 
@@ -1860,6 +1863,9 @@
                       </void> 
                       <void method="add"> 
                        <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                        <void property="alias"> 
+                         <string>c2</string> 
+                        </void> 
                         <void property="internalName"> 
                          <string>_col1</string> 
                         </void> 
@@ -1870,6 +1876,9 @@
                       </void> 
                       <void method="add"> 
                        <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                        <void property="alias"> 
+                         <string>c3</string> 
+                        </void> 
                         <void property="internalName"> 
                          <string>_col2</string> 
                         </void> 
@@ -1880,6 +1889,9 @@
                       </void> 
                       <void method="add"> 
                        <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                        <void property="alias"> 
+                         <string>c4</string> 
+                        </void> 
                         <void property="internalName"> 
                          <string>_col3</string> 
                         </void> 
diff --git a/ql/src/test/results/compiler/plan/sample1.q.xml b/ql/src/test/results/compiler/plan/sample1.q.xml
index 95b713c095..981fbadefd 100644
--- a/ql/src/test/results/compiler/plan/sample1.q.xml
+++ b/ql/src/test/results/compiler/plan/sample1.q.xml
@@ -77,7 +77,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/srcpart</string> 
            </void> 
           </object> 
          </void> 
@@ -117,7 +117,7 @@
                             <void property="conf"> 
                              <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                               <void property="dirName"> 
-                               <string>file:/data/users/njain/hive4/hive4/build/ql/tmp/1901117107/10001</string> 
+                               <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/100926076/10001</string> 
                               </void> 
                               <void property="tableInfo"> 
                                <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -357,6 +357,9 @@
                            <object class="java.util.Vector"> 
                             <void method="add"> 
                              <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                              <void property="alias"> 
+                               <string>key</string> 
+                              </void> 
                               <void property="internalName"> 
                                <string>_col0</string> 
                               </void> 
@@ -367,6 +370,9 @@
                             </void> 
                             <void method="add"> 
                              <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                              <void property="alias"> 
+                               <string>value</string> 
+                              </void> 
                               <void property="internalName"> 
                                <string>_col1</string> 
                               </void> 
@@ -377,6 +383,9 @@
                             </void> 
                             <void method="add"> 
                              <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                              <void property="alias"> 
+                               <string>ds</string> 
+                              </void> 
                               <void property="internalName"> 
                                <string>_col2</string> 
                               </void> 
@@ -387,6 +396,9 @@
                             </void> 
                             <void method="add"> 
                              <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                              <void property="alias"> 
+                               <string>hr</string> 
+                              </void> 
                               <void property="internalName"> 
                                <string>_col3</string> 
                               </void> 
@@ -991,7 +1003,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -1003,7 +1015,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/subq.q.xml b/ql/src/test/results/compiler/plan/subq.q.xml
index b4d5a4d885..dcaf45b868 100644
--- a/ql/src/test/results/compiler/plan/subq.q.xml
+++ b/ql/src/test/results/compiler/plan/subq.q.xml
@@ -788,6 +788,9 @@
                            <object class="java.util.Vector"> 
                             <void method="add"> 
                              <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                              <void property="alias"> 
+                               <string>key</string> 
+                              </void> 
                               <void property="internalName"> 
                                <string>_col0</string> 
                               </void> 
@@ -798,6 +801,9 @@
                             </void> 
                             <void method="add"> 
                              <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                              <void property="alias"> 
+                               <string>value</string> 
+                              </void> 
                               <void property="internalName"> 
                                <string>_col1</string> 
                               </void> 
diff --git a/ql/src/test/results/compiler/plan/udf1.q.xml b/ql/src/test/results/compiler/plan/udf1.q.xml
index 5991d387d5..5c6a1e0435 100644
--- a/ql/src/test/results/compiler/plan/udf1.q.xml
+++ b/ql/src/test/results/compiler/plan/udf1.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -100,7 +100,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>file:/data/users/njain/hive4/hive4/build/ql/tmp/1903343068/10001</string> 
+                           <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/929999119/10001</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1286,6 +1286,9 @@
                        <object class="java.util.Vector"> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c0</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col0</string> 
                           </void> 
@@ -1296,6 +1299,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c1</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col1</string> 
                           </void> 
@@ -1306,6 +1312,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c2</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col2</string> 
                           </void> 
@@ -1316,6 +1325,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c3</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col3</string> 
                           </void> 
@@ -1326,6 +1338,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c4</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col4</string> 
                           </void> 
@@ -1336,6 +1351,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c5</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col5</string> 
                           </void> 
@@ -1346,6 +1364,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c6</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col6</string> 
                           </void> 
@@ -1356,6 +1377,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c7</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col7</string> 
                           </void> 
@@ -1366,6 +1390,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c8</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col8</string> 
                           </void> 
@@ -1376,6 +1403,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c9</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col9</string> 
                           </void> 
@@ -1386,6 +1416,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c10</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col10</string> 
                           </void> 
@@ -1396,6 +1429,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c11</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col11</string> 
                           </void> 
@@ -1406,6 +1442,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c12</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col12</string> 
                           </void> 
@@ -1416,6 +1455,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c13</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col13</string> 
                           </void> 
@@ -1426,6 +1468,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c14</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col14</string> 
                           </void> 
@@ -1436,6 +1481,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c15</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col15</string> 
                           </void> 
@@ -1446,6 +1494,9 @@
                         </void> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                          <void property="alias"> 
+                           <string>_c16</string> 
+                          </void> 
                           <void property="internalName"> 
                            <string>_col16</string> 
                           </void> 
@@ -1701,7 +1752,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1713,7 +1764,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/udf4.q.xml b/ql/src/test/results/compiler/plan/udf4.q.xml
index a48aaa9b1d..f31493e295 100644
--- a/ql/src/test/results/compiler/plan/udf4.q.xml
+++ b/ql/src/test/results/compiler/plan/udf4.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/dest1</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/dest1</string> 
            </void> 
           </object> 
          </void> 
@@ -92,7 +92,7 @@
                 <void property="conf"> 
                  <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                   <void property="dirName"> 
-                   <string>file:/data/users/njain/hive4/hive4/build/ql/tmp/981463087/10001</string> 
+                   <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/383611566/10001</string> 
                   </void> 
                   <void property="tableInfo"> 
                    <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1260,6 +1260,9 @@
                <object class="java.util.Vector"> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c0</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col0</string> 
                   </void> 
@@ -1270,6 +1273,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c1</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col1</string> 
                   </void> 
@@ -1280,6 +1286,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c2</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col2</string> 
                   </void> 
@@ -1290,6 +1299,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c3</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col3</string> 
                   </void> 
@@ -1300,6 +1312,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c4</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col4</string> 
                   </void> 
@@ -1310,6 +1325,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c5</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col5</string> 
                   </void> 
@@ -1320,6 +1338,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c6</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col6</string> 
                   </void> 
@@ -1330,6 +1351,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c7</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col7</string> 
                   </void> 
@@ -1340,6 +1364,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c8</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col8</string> 
                   </void> 
@@ -1350,6 +1377,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c9</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col9</string> 
                   </void> 
@@ -1360,6 +1390,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c10</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col10</string> 
                   </void> 
@@ -1370,6 +1403,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c11</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col11</string> 
                   </void> 
@@ -1380,6 +1416,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c12</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col12</string> 
                   </void> 
@@ -1390,6 +1429,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c13</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col13</string> 
                   </void> 
@@ -1400,6 +1442,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c14</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col14</string> 
                   </void> 
@@ -1410,6 +1455,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c15</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col15</string> 
                   </void> 
@@ -1420,6 +1468,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c16</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col16</string> 
                   </void> 
@@ -1430,6 +1481,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c17</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col17</string> 
                   </void> 
@@ -1440,6 +1494,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c18</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col18</string> 
                   </void> 
@@ -1521,7 +1578,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/dest1</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/dest1</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>dest1</string> 
@@ -1533,7 +1590,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/dest1</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/dest1</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/udf6.q.xml b/ql/src/test/results/compiler/plan/udf6.q.xml
index 8b54a12af8..7e64adb911 100644
--- a/ql/src/test/results/compiler/plan/udf6.q.xml
+++ b/ql/src/test/results/compiler/plan/udf6.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -92,7 +92,7 @@
                 <void property="conf"> 
                  <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                   <void property="dirName"> 
-                   <string>file:/data/users/njain/hive4/hive4/build/ql/tmp/948218645/10001</string> 
+                   <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/1465554839/10001</string> 
                   </void> 
                   <void property="tableInfo"> 
                    <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -336,6 +336,9 @@
                <object class="java.util.Vector"> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c0</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col0</string> 
                   </void> 
@@ -346,6 +349,9 @@
                 </void> 
                 <void method="add"> 
                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                  <void property="alias"> 
+                   <string>_c1</string> 
+                  </void> 
                   <void property="internalName"> 
                    <string>_col1</string> 
                   </void> 
@@ -423,7 +429,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -435,7 +441,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/udf_case.q.xml b/ql/src/test/results/compiler/plan/udf_case.q.xml
index 6de3f3f45f..6d9b044b80 100644
--- a/ql/src/test/results/compiler/plan/udf_case.q.xml
+++ b/ql/src/test/results/compiler/plan/udf_case.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -96,7 +96,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>file:/data/users/njain/hive4/hive4/build/ql/tmp/2019530918/10001</string> 
+                       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/2034861252/10001</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -224,6 +224,9 @@
                    <object id="Vector0" class="java.util.Vector"> 
                     <void method="add"> 
                      <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                      <void property="alias"> 
+                       <string>_c0</string> 
+                      </void> 
                       <void property="internalName"> 
                        <string>_col0</string> 
                       </void> 
@@ -234,6 +237,9 @@
                     </void> 
                     <void method="add"> 
                      <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                      <void property="alias"> 
+                       <string>_c1</string> 
+                      </void> 
                       <void property="internalName"> 
                        <string>_col1</string> 
                       </void> 
@@ -517,7 +523,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -529,7 +535,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/udf_when.q.xml b/ql/src/test/results/compiler/plan/udf_when.q.xml
index 1e58feb41c..af5deae224 100644
--- a/ql/src/test/results/compiler/plan/udf_when.q.xml
+++ b/ql/src/test/results/compiler/plan/udf_when.q.xml
@@ -64,7 +64,7 @@
            </void> 
            <void method="put"> 
             <string>location</string> 
-            <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+            <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
            </void> 
           </object> 
          </void> 
@@ -96,7 +96,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>file:/data/users/njain/hive4/hive4/build/ql/tmp/365879746/10001</string> 
+                       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/tmp/591430521/10001</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -224,6 +224,9 @@
                    <object id="Vector0" class="java.util.Vector"> 
                     <void method="add"> 
                      <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                      <void property="alias"> 
+                       <string>_c0</string> 
+                      </void> 
                       <void property="internalName"> 
                        <string>_col0</string> 
                       </void> 
@@ -234,6 +237,9 @@
                     </void> 
                     <void method="add"> 
                      <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                      <void property="alias"> 
+                       <string>_c1</string> 
+                      </void> 
                       <void property="internalName"> 
                        <string>_col1</string> 
                       </void> 
@@ -637,7 +643,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -649,7 +655,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/nzhang/work/31/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object idref="LinkedHashMap0"/> 
diff --git a/ql/src/test/results/compiler/plan/union.q.xml b/ql/src/test/results/compiler/plan/union.q.xml
index 035298d91e..4a8f00c68b 100644
--- a/ql/src/test/results/compiler/plan/union.q.xml
+++ b/ql/src/test/results/compiler/plan/union.q.xml
@@ -860,6 +860,9 @@
                                <object class="java.util.Vector"> 
                                 <void method="add"> 
                                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                                  <void property="alias"> 
+                                   <string>key</string> 
+                                  </void> 
                                   <void property="internalName"> 
                                    <string>_col0</string> 
                                   </void> 
@@ -870,6 +873,9 @@
                                 </void> 
                                 <void method="add"> 
                                  <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                                  <void property="alias"> 
+                                   <string>value</string> 
+                                  </void> 
                                   <void property="internalName"> 
                                    <string>_col1</string> 
                                   </void> 
