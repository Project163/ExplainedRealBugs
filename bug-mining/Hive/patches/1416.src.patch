diff --git a/build-common.xml b/build-common.xml
index 0807827581..ad5ac2372a 100644
--- a/build-common.xml
+++ b/build-common.xml
@@ -59,7 +59,7 @@
   <property name="test.output" value="true"/>
   <property name="test.junit.output.format" value="xml"/>
   <property name="test.junit.output.usefile" value="true"/>
-  <property name="minimr.query.files" value="list_bucket_dml_10.q,input16_cc.q,scriptfile1.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,bucket5.q,load_fs2.q,bucket_num_reducers2.q,infer_bucket_sort_merge.q,infer_bucket_sort_reducers_power_two.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_num_buckets.q,leftsemijoin_mr.q,schemeAuthority.q,truncate_column_buckets.q,remote_script.q,,load_hdfs_file_with_space_in_the_name.q,parallel_orderby.q"/>
+  <property name="minimr.query.files" value="list_bucket_dml_10.q,input16_cc.q,scriptfile1.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,bucket5.q,load_fs2.q,bucket_num_reducers2.q,infer_bucket_sort_merge.q,infer_bucket_sort_reducers_power_two.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_num_buckets.q,leftsemijoin_mr.q,schemeAuthority.q,schemeAuthority2.q,truncate_column_buckets.q,remote_script.q,,load_hdfs_file_with_space_in_the_name.q,parallel_orderby.q"/>
   <property name="minimr.query.negative.files" value="cluster_tasklog_retrieval.q,minimr_broken_pipe.q,mapreduce_stack_trace.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff_hadoop20.q" />
   <property name="test.silent" value="true"/>
   <property name="hadoopVersion" value="${hadoop.version.ant-internal}"/>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index b5c85d2a0d..78213460c0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -480,7 +480,7 @@ public void cleanUpInputFileChangedOp() throws HiveException {
   }
 
   private Path normalizePath(String onefile) {
-    return new Path(new Path(onefile).toUri().getPath());
+    return new Path(onefile);
   }
 
   public void process(Writable value) throws HiveException {
diff --git a/ql/src/test/queries/clientpositive/schemeAuthority2.q b/ql/src/test/queries/clientpositive/schemeAuthority2.q
new file mode 100644
index 0000000000..ecd4d13d0e
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/schemeAuthority2.q
@@ -0,0 +1,12 @@
+dfs -mkdir file:///tmp/test;
+dfs -mkdir hdfs:///tmp/test;
+
+create external table dynPart (key string) partitioned by (value string, value2 string) row format delimited fields terminated by '\\t' stored as textfile;
+insert overwrite local directory "/tmp/test" select key from src where (key = 10) order by key;
+insert overwrite directory "/tmp/test" select key from src where (key = 20) order by key;
+alter table dynPart add partition (value='0', value2='clusterA') location 'file:///tmp/test';
+alter table dynPart add partition (value='0', value2='clusterB') location 'hdfs:///tmp/test';
+select value2, key from dynPart where value='0';
+
+dfs -rmr file:///tmp/test;
+dfs -rmr hdfs:///tmp/test;
diff --git a/ql/src/test/results/clientpositive/schemeAuthority2.q.out b/ql/src/test/results/clientpositive/schemeAuthority2.q.out
new file mode 100644
index 0000000000..4248270c33
--- /dev/null
+++ b/ql/src/test/results/clientpositive/schemeAuthority2.q.out
@@ -0,0 +1,46 @@
+PREHOOK: query: create external table dynPart (key string) partitioned by (value string, value2 string) row format delimited fields terminated by '\\t' stored as textfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create external table dynPart (key string) partitioned by (value string, value2 string) row format delimited fields terminated by '\\t' stored as textfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@dynPart
+#### A masked pattern was here ####
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: default@dynpart
+#### A masked pattern was here ####
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: default@dynpart
+POSTHOOK: Output: default@dynpart@value=0/value2=clusterA
+#### A masked pattern was here ####
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: default@dynpart
+#### A masked pattern was here ####
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: default@dynpart
+POSTHOOK: Output: default@dynpart@value=0/value2=clusterB
+PREHOOK: query: select value2, key from dynPart where value='0'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dynpart
+PREHOOK: Input: default@dynpart@value=0/value2=clusterA
+PREHOOK: Input: default@dynpart@value=0/value2=clusterB
+#### A masked pattern was here ####
+POSTHOOK: query: select value2, key from dynPart where value='0'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dynpart
+POSTHOOK: Input: default@dynpart@value=0/value2=clusterA
+POSTHOOK: Input: default@dynpart@value=0/value2=clusterB
+#### A masked pattern was here ####
+clusterB	20
+clusterA	10
+#### A masked pattern was here ####
