diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GlobalLimitOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GlobalLimitOptimizer.java
index e368570fca..4e47aea562 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GlobalLimitOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GlobalLimitOptimizer.java
@@ -92,41 +92,44 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {
       //    SELECT * FROM (SELECT col1 as col2 (SELECT * FROM ...) t1 LIMIT ...) t2);
       //
       TableScanOperator ts = topOps.values().iterator().next();
-      LimitOperator tempGlobalLimit = checkQbpForGlobalLimit(ts);
-
-      // query qualify for the optimization
-      if (tempGlobalLimit != null) {
-        LimitDesc tempGlobalLimitDesc = tempGlobalLimit.getConf();
-        Table tab = ts.getConf().getTableMetadata();
-        Set<FilterOperator> filterOps = OperatorUtils.findOperators(ts, FilterOperator.class);
-
-        if (!tab.isPartitioned()) {
-          if (filterOps.size() == 0) {
-            Integer tempOffset = tempGlobalLimitDesc.getOffset();
-            globalLimitCtx.enableOpt(tempGlobalLimitDesc.getLimit(),
-                (tempOffset == null) ? 0 : tempOffset);
-          }
-        } else {
-          // check if the pruner only contains partition columns
-          if (onlyContainsPartnCols(tab, filterOps)) {
-
-            String alias = (String) topOps.keySet().toArray()[0];
-            PrunedPartitionList partsList = pctx.getPrunedPartitions(alias, ts);
-
-            // If there is any unknown partition, create a map-reduce job for
-            // the filter to prune correctly
-            if (!partsList.hasUnknownPartitions()) {
+      Table tab = ts.getConf().getTableMetadata();
+      // StorageHandlers will always have empty tablePath.
+      // GenMapRedUtils.setMapWork removes empty tablePath from input dir with select-Limit
+      // InputFormat.getSplits wont be called if no input path & TS Vertex will have 0 task parallelism
+      if (tab.getStorageHandler() == null) {
+        LimitOperator tempGlobalLimit = checkQbpForGlobalLimit(ts);
+        // query qualify for the optimization
+        if (tempGlobalLimit != null) {
+          LimitDesc tempGlobalLimitDesc = tempGlobalLimit.getConf();
+          Set<FilterOperator> filterOps = OperatorUtils.findOperators(ts, FilterOperator.class);
+          if (!tab.isPartitioned()) {
+            if (filterOps.size() == 0) {
               Integer tempOffset = tempGlobalLimitDesc.getOffset();
               globalLimitCtx.enableOpt(tempGlobalLimitDesc.getLimit(),
                   (tempOffset == null) ? 0 : tempOffset);
             }
+          } else {
+            // check if the pruner only contains partition columns
+            if (onlyContainsPartnCols(tab, filterOps)) {
+
+              String alias = (String) topOps.keySet().toArray()[0];
+              PrunedPartitionList partsList = pctx.getPrunedPartitions(alias, ts);
+
+              // If there is any unknown partition, create a map-reduce job for
+              // the filter to prune correctly
+              if (!partsList.hasUnknownPartitions()) {
+                Integer tempOffset = tempGlobalLimitDesc.getOffset();
+                globalLimitCtx.enableOpt(tempGlobalLimitDesc.getLimit(),
+                        (tempOffset == null) ? 0 : tempOffset);
+              }
+            }
+          }
+          if (globalLimitCtx.isEnable()) {
+            LOG.info("Qualify the optimize that reduces input size for 'offset' for offset "
+                + globalLimitCtx.getGlobalOffset());
+            LOG.info("Qualify the optimize that reduces input size for 'limit' for limit "
+                + globalLimitCtx.getGlobalLimit());
           }
-        }
-        if (globalLimitCtx.isEnable()) {
-          LOG.info("Qualify the optimize that reduces input size for 'offset' for offset "
-              + globalLimitCtx.getGlobalOffset());
-          LOG.info("Qualify the optimize that reduces input size for 'limit' for limit "
-              + globalLimitCtx.getGlobalLimit());
         }
       }
     }
diff --git a/ql/src/test/queries/clientpositive/external_jdbc_table4.q b/ql/src/test/queries/clientpositive/external_jdbc_table4.q
index eb5997e000..ca29b0ce2e 100644
--- a/ql/src/test/queries/clientpositive/external_jdbc_table4.q
+++ b/ql/src/test/queries/clientpositive/external_jdbc_table4.q
@@ -140,6 +140,11 @@ SELECT * FROM db1_ext_auth2;
 
 set hive.cbo.enable=true;
 
+set hive.limit.optimize.enable=true;
+explain select * from db1_ext_auth1 limit 10;
+select * from db1_ext_auth1 limit 10;
+set hive.limit.optimize.enable=false;
+
 DROP TABLE db1_ext_auth1;
 DROP TABLE db2_ext_auth2;
 DROP TABLE db1_ext_auth2;
diff --git a/ql/src/test/results/clientpositive/llap/external_jdbc_table4.q.out b/ql/src/test/results/clientpositive/llap/external_jdbc_table4.q.out
index 5993da1f82..8786aa499f 100644
--- a/ql/src/test/results/clientpositive/llap/external_jdbc_table4.q.out
+++ b/ql/src/test/results/clientpositive/llap/external_jdbc_table4.q.out
@@ -618,6 +618,49 @@ POSTHOOK: Input: default@db1_ext_auth2
 101	-16	66.0	-75.0
 20	20	20.0	20.0
 40	50	-455.4543	330.767
+PREHOOK: query: explain select * from db1_ext_auth1 limit 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@db1_ext_auth1
+#### A masked pattern was here ####
+POSTHOOK: query: explain select * from db1_ext_auth1 limit 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@db1_ext_auth1
+#### A masked pattern was here ####
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        TableScan
+          alias: db1_ext_auth1
+          properties:
+            hive.sql.query SELECT "IKEY", "bkey", "fkey", "dkey"
+FROM (SELECT *
+FROM "EXTERNAL_JDBC_SIMPLE_DERBY2_TABLE1"
+FETCH NEXT 10 ROWS ONLY) AS "t"
+            hive.sql.query.fieldNames IKEY,bkey,fkey,dkey
+            hive.sql.query.fieldTypes int,bigint,float,double
+            hive.sql.query.split false
+          Select Operator
+            expressions: ikey (type: int), bkey (type: bigint), fkey (type: float), dkey (type: double)
+            outputColumnNames: _col0, _col1, _col2, _col3
+            ListSink
+
+PREHOOK: query: select * from db1_ext_auth1 limit 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@db1_ext_auth1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from db1_ext_auth1 limit 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@db1_ext_auth1
+#### A masked pattern was here ####
+-20	-20	-20.0	-20.0
+100	-15	65.0	-74.0
+20	20	20.0	20.0
+44	53	-455.454	330.76
 PREHOOK: query: DROP TABLE db1_ext_auth1
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@db1_ext_auth1
