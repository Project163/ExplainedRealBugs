diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java
index de28766914..5199071d5e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java
@@ -61,6 +61,8 @@
 public class ColumnStatsTask extends Task<ColumnStatsWork> implements Serializable {
   private static final long serialVersionUID = 1L;
   private FetchOperator ftOp;
+  private int totalRows;
+  private int numRows = 0;
   private static transient final Log LOG = LogFactory.getLog(ColumnStatsTask.class);
 
   public ColumnStatsTask() {
@@ -70,6 +72,7 @@ public ColumnStatsTask() {
   @Override
   public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {
     super.initialize(conf, queryPlan, ctx);
+    work.initializeForFetch();
     try {
       JobConf job = new JobConf(conf, ExecDriver.class);
       ftOp = new FetchOperator(work.getfWork(), job);
@@ -294,14 +297,16 @@ private int persistPartitionStats() throws HiveException {
       e.printStackTrace();
     }
 
-    // Construct a column statistics object from the result
-    ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);
+    if (io != null) {
+      // Construct a column statistics object from the result
+      ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);
 
-    // Persist the column statistics object to the metastore
-    try {
-      db.updatePartitionColumnStatistics(colStats);
-    } catch (Exception e) {
-      e.printStackTrace();
+      // Persist the column statistics object to the metastore
+      try {
+        db.updatePartitionColumnStatistics(colStats);
+      } catch (Exception e) {
+        e.printStackTrace();
+      }
     }
     return 0;
   }
@@ -317,14 +322,16 @@ private int persistTableStats() throws HiveException {
       e.printStackTrace();
     }
 
-    // Construct a column statistics object from the result
-    ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);
+    if (io != null) {
+      // Construct a column statistics object from the result
+      ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);
 
-    // Persist the column statistics object to the metastore
-    try {
-      db.updateTableColumnStatistics(colStats);
-    } catch (Exception e) {
-      e.printStackTrace();
+      // Persist the column statistics object to the metastore
+      try {
+        db.updateTableColumnStatistics(colStats);
+      } catch (Exception e) {
+        e.printStackTrace();
+      }
     }
     return 0;
   }
@@ -344,10 +351,25 @@ public int execute(DriverContext driverContext) {
   }
 
   private InspectableObject fetchColumnStats() throws IOException, CommandNeedRetryException {
+    InspectableObject io = null;
+
     try {
-      InspectableObject io = ftOp.getNextRow();
-      if (io == null) {
-        throw new CommandNeedRetryException();
+      int rowsRet = work.getLeastNumRows();
+      if (rowsRet <= 0) {
+        rowsRet = ColumnStatsWork.getLimit() >= 0 ?
+            Math.min(ColumnStatsWork.getLimit() - totalRows, 1) : 1;
+      }
+      if (rowsRet <= 0) {
+        ftOp.clearFetchContext();
+        return null;
+      }
+      while (numRows < rowsRet) {
+        if ((io = ftOp.getNextRow()) == null) {
+          if (work.getLeastNumRows() > 0) {
+            throw new CommandNeedRetryException();
+          }
+        }
+        numRows++;
       }
       return io;
     } catch (CommandNeedRetryException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java
index 0e3846e4b0..3cae7273a7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java
@@ -20,6 +20,8 @@
 
 import java.io.Serializable;
 
+import org.apache.hadoop.hive.ql.exec.ListSinkOperator;
+
 /**
  * ColumnStats Work.
  *
@@ -29,6 +31,8 @@ public class ColumnStatsWork implements Serializable {
   private static final long serialVersionUID = 1L;
   private FetchWork fWork;
   private ColumnStatsDesc colStats;
+  private static final int LIMIT = -1;
+
 
   public ColumnStatsWork() {
   }
@@ -61,4 +65,21 @@ public ColumnStatsDesc getColStats() {
   public void setColStats(ColumnStatsDesc colStats) {
     this.colStats = colStats;
   }
+
+  public ListSinkOperator getSink() {
+    return fWork.getSink();
+  }
+
+  public void initializeForFetch() {
+    fWork.initializeForFetch();
+  }
+
+  public int getLeastNumRows() {
+    return fWork.getLeastNumRows();
+  }
+
+  public static int getLimit() {
+    return LIMIT;
+  }
+
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java
index e09efb79ef..f97e460116 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java
@@ -229,24 +229,32 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       Object p = parameters[0];
       BooleanStatsAgg myagg = (BooleanStatsAgg) agg;
-      if (p == null) {
-        myagg.countNulls++;
+      boolean emptyTable = false;
+
+      if (parameters[1] == null) {
+        emptyTable = true;
       }
-      else {
-        try {
-          boolean v = PrimitiveObjectInspectorUtils.getBoolean(p, inputOI);
-          if (v == false) {
-            myagg.countFalses++;
-          } else if (v == true){
-            myagg.countTrues++;
-          }
-        } catch (NumberFormatException e) {
-          if (!warned) {
-            warned = true;
-            LOG.warn(getClass().getSimpleName() + " "
-                + StringUtils.stringifyException(e));
-            LOG.warn(getClass().getSimpleName()
-                + " ignoring similar exceptions.");
+
+      if (!emptyTable) {
+        if (p == null) {
+          myagg.countNulls++;
+        }
+        else {
+          try {
+            boolean v = PrimitiveObjectInspectorUtils.getBoolean(p, inputOI);
+            if (v == false) {
+              myagg.countFalses++;
+            } else if (v == true){
+              myagg.countTrues++;
+            }
+          } catch (NumberFormatException e) {
+            if (!warned) {
+              warned = true;
+              LOG.warn(getClass().getSimpleName() + " "
+                  + StringUtils.stringifyException(e));
+              LOG.warn(getClass().getSimpleName()
+                  + " ignoring similar exceptions.");
+            }
           }
         }
       }
@@ -472,14 +480,24 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       Object p = parameters[0];
       LongStatsAgg myagg = (LongStatsAgg) agg;
+      boolean emptyTable = false;
+
+      if (parameters[1] == null) {
+        emptyTable = true;
+      }
 
       if (myagg.firstItem) {
-        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);
+        int numVectors = 0;
+        if (!emptyTable) {
+          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);
+        }
         initNDVEstimator(myagg, numVectors);
         myagg.firstItem = false;
         myagg.numBitVectors = numVectors;
       }
 
+      if (!emptyTable) {
+
       //Update null counter if a null value is seen
       if (p == null) {
         myagg.countNulls++;
@@ -511,6 +529,7 @@ public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveExcep
           }
         }
       }
+      }
     }
 
     @Override
@@ -572,7 +591,11 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {
     @Override
     public Object terminate(AggregationBuffer agg) throws HiveException {
       LongStatsAgg myagg = (LongStatsAgg) agg;
-      long numDV = myagg.numDV.estimateNumDistinctValues();
+
+      long numDV = 0;
+      if (myagg.numBitVectors != 0) {
+        numDV = myagg.numDV.estimateNumDistinctValues();
+      }
 
       // Serialize the result struct
       ((Text) result[0]).set(myagg.columnType);
@@ -770,42 +793,54 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       Object p = parameters[0];
       DoubleStatsAgg myagg = (DoubleStatsAgg) agg;
+      boolean emptyTable = false;
+
+      if (parameters[1] == null) {
+        emptyTable = true;
+      }
 
       if (myagg.firstItem) {
-        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);
+        int numVectors = 0;
+        if (!emptyTable) {
+          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);
+        }
         initNDVEstimator(myagg, numVectors);
         myagg.firstItem = false;
         myagg.numBitVectors = numVectors;
       }
 
-      //Update null counter if a null value is seen
-      if (p == null) {
-        myagg.countNulls++;
-      }
-      else {
-        try {
-          double v = PrimitiveObjectInspectorUtils.getDouble(p, inputOI);
+      if (!emptyTable) {
 
-          //Update min counter if new value is less than min seen so far
-          if (v < myagg.min) {
-            myagg.min = v;
-          }
-
-          //Update max counter if new value is greater than max seen so far
-          if (v > myagg.max) {
-            myagg.max = v;
-          }
-
-          // Add value to NumDistinctValue Estimator
-          myagg.numDV.addToEstimator(v);
-
-        } catch (NumberFormatException e) {
-          if (!warned) {
-            warned = true;
-            LOG.warn(getClass().getSimpleName() + " "
-                + StringUtils.stringifyException(e));
-            LOG.warn(getClass().getSimpleName()
-                + " ignoring similar exceptions.");
+        //Update null counter if a null value is seen
+        if (p == null) {
+          myagg.countNulls++;
+        }
+        else {
+          try {
+
+            double v = PrimitiveObjectInspectorUtils.getDouble(p, inputOI);
+
+            //Update min counter if new value is less than min seen so far
+            if (v < myagg.min) {
+              myagg.min = v;
+            }
+
+            //Update max counter if new value is greater than max seen so far
+            if (v > myagg.max) {
+              myagg.max = v;
+            }
+
+            // Add value to NumDistinctValue Estimator
+            myagg.numDV.addToEstimator(v);
+
+          } catch (NumberFormatException e) {
+            if (!warned) {
+              warned = true;
+              LOG.warn(getClass().getSimpleName() + " "
+                  + StringUtils.stringifyException(e));
+              LOG.warn(getClass().getSimpleName()
+                  + " ignoring similar exceptions.");
+            }
           }
         }
       }
@@ -870,7 +905,11 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {
     @Override
     public Object terminate(AggregationBuffer agg) throws HiveException {
       DoubleStatsAgg myagg = (DoubleStatsAgg) agg;
-      long numDV = myagg.numDV.estimateNumDistinctValues();
+      long numDV = 0;
+
+      if (myagg.numBitVectors != 0) {
+        numDV = myagg.numDV.estimateNumDistinctValues();
+      }
 
       // Serialize the result struct
       ((Text) result[0]).set(myagg.columnType);
@@ -1082,44 +1121,56 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       Object p = parameters[0];
       StringStatsAgg myagg = (StringStatsAgg) agg;
+      boolean emptyTable = false;
+
+      if (parameters[1] == null) {
+        emptyTable = true;
+      }
 
       if (myagg.firstItem) {
-        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);
+        int numVectors = 0;
+        if (!emptyTable) {
+          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);
+        }
         initNDVEstimator(myagg, numVectors);
         myagg.firstItem = false;
         myagg.numBitVectors = numVectors;
       }
 
-      // Update null counter if a null value is seen
-      if (p == null) {
-        myagg.countNulls++;
-      }
-      else {
-        try {
-          String v = PrimitiveObjectInspectorUtils.getString(p, inputOI);
+      if (!emptyTable) {
 
-          // Update max length if new length is greater than the ones seen so far
-          int len = v.length();
-          if (len > myagg.maxLength) {
-            myagg.maxLength = len;
-          }
-
-          // Update sum length with the new length
-          myagg.sumLength += len;
-
-          // Increment count of values seen so far
-          myagg.count++;
-
-          // Add string value to NumDistinctValue Estimator
-          myagg.numDV.addToEstimator(v);
-
-        } catch (NumberFormatException e) {
-          if (!warned) {
-            warned = true;
-            LOG.warn(getClass().getSimpleName() + " "
-                + StringUtils.stringifyException(e));
-            LOG.warn(getClass().getSimpleName()
-                + " ignoring similar exceptions.");
+        // Update null counter if a null value is seen
+        if (p == null) {
+          myagg.countNulls++;
+        }
+        else {
+          try {
+
+            String v = PrimitiveObjectInspectorUtils.getString(p, inputOI);
+
+            // Update max length if new length is greater than the ones seen so far
+            int len = v.length();
+            if (len > myagg.maxLength) {
+              myagg.maxLength = len;
+            }
+
+            // Update sum length with the new length
+            myagg.sumLength += len;
+
+            // Increment count of values seen so far
+            myagg.count++;
+
+            // Add string value to NumDistinctValue Estimator
+            myagg.numDV.addToEstimator(v);
+
+          } catch (NumberFormatException e) {
+            if (!warned) {
+              warned = true;
+              LOG.warn(getClass().getSimpleName() + " "
+                  + StringUtils.stringifyException(e));
+              LOG.warn(getClass().getSimpleName()
+                  + " ignoring similar exceptions.");
+            }
           }
         }
       }
@@ -1186,8 +1237,18 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {
     @Override
     public Object terminate(AggregationBuffer agg) throws HiveException {
       StringStatsAgg myagg = (StringStatsAgg) agg;
-      long numDV = myagg.numDV.estimateNumDistinctValues();
-      double avgLength = (double)(myagg.sumLength/(1.0 * (myagg.count + myagg.countNulls)));
+
+      long numDV = 0;
+      double avgLength = 0.0;
+      long total = myagg.count + myagg.countNulls;
+
+      if (myagg.numBitVectors != 0) {
+        numDV = myagg.numDV.estimateNumDistinctValues();
+      }
+
+      if (total != 0) {
+         avgLength = (double)(myagg.sumLength / (1.0 * total));
+      }
 
       // Serialize the result struct
       ((Text) result[0]).set(myagg.columnType);
@@ -1347,34 +1408,41 @@ public void reset(AggregationBuffer agg) throws HiveException {
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       Object p = parameters[0];
       BinaryStatsAgg myagg = (BinaryStatsAgg) agg;
+      boolean emptyTable = false;
 
-      // Update null counter if a null value is seen
-      if (p == null) {
-        myagg.countNulls++;
+      if (parameters[1] == null) {
+        emptyTable = true;
       }
-      else {
-        try {
-          BytesWritable v = PrimitiveObjectInspectorUtils.getBinary(p, inputOI);
-
-          // Update max length if new length is greater than the ones seen so far
-          int len = v.getLength();
-          if (len > myagg.maxLength) {
-            myagg.maxLength = len;
-          }
 
-          // Update sum length with the new length
-          myagg.sumLength += len;
-
-          // Increment count of values seen so far
-          myagg.count++;
-
-        } catch (NumberFormatException e) {
-          if (!warned) {
-            warned = true;
-            LOG.warn(getClass().getSimpleName() + " "
-                + StringUtils.stringifyException(e));
-            LOG.warn(getClass().getSimpleName()
-                + " ignoring similar exceptions.");
+      if (!emptyTable) {
+        // Update null counter if a null value is seen
+        if (p == null) {
+          myagg.countNulls++;
+        }
+        else {
+          try {
+            BytesWritable v = PrimitiveObjectInspectorUtils.getBinary(p, inputOI);
+
+            // Update max length if new length is greater than the ones seen so far
+            int len = v.getLength();
+            if (len > myagg.maxLength) {
+              myagg.maxLength = len;
+            }
+
+            // Update sum length with the new length
+            myagg.sumLength += len;
+
+            // Increment count of values seen so far
+            myagg.count++;
+
+          } catch (NumberFormatException e) {
+            if (!warned) {
+              warned = true;
+              LOG.warn(getClass().getSimpleName() + " "
+                  + StringUtils.stringifyException(e));
+              LOG.warn(getClass().getSimpleName()
+                  + " ignoring similar exceptions.");
+            }
           }
         }
       }
@@ -1440,7 +1508,12 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {
     @Override
     public Object terminate(AggregationBuffer agg) throws HiveException {
       BinaryStatsAgg myagg = (BinaryStatsAgg) agg;
-      double avgLength = (double)(myagg.sumLength/(1.0 * (myagg.count + myagg.countNulls)));
+      double avgLength = 0.0;
+      long count = myagg.count + myagg.countNulls;
+
+      if (count != 0) {
+        avgLength = (double)(myagg.sumLength / (1.0 * (myagg.count + myagg.countNulls)));
+      }
 
       // Serialize the result struct
       ((Text) result[0]).set(myagg.columnType);
diff --git a/ql/src/test/queries/clientpositive/columnstats_tbllvl.q b/ql/src/test/queries/clientpositive/columnstats_tbllvl.q
index 187e8be5d4..72d88a67b5 100644
--- a/ql/src/test/queries/clientpositive/columnstats_tbllvl.q
+++ b/ql/src/test/queries/clientpositive/columnstats_tbllvl.q
@@ -23,3 +23,16 @@ analyze table UserVisits_web_text_none compute statistics for columns sourceIP,
 
 analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;
 
+CREATE TABLE empty_tab(
+   a int,
+   b double,
+   c string, 
+   d boolean,
+   e binary)
+row format delimited fields terminated by '|'  stored as textfile;
+
+explain 
+analyze table empty_tab compute statistics for columns a,b,c,d,e;
+
+analyze table empty_tab compute statistics for columns a,b,c,d,e;
+
diff --git a/ql/src/test/queries/clientpositive/compute_stats_empty_table.q b/ql/src/test/queries/clientpositive/compute_stats_empty_table.q
new file mode 100644
index 0000000000..d4ed93fa63
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/compute_stats_empty_table.q
@@ -0,0 +1,12 @@
+create table tab_empty(a boolean, b int, c double, d string, e binary);
+
+select count(*) from tab_empty;
+
+-- compute statistical summary of data
+select compute_stats(a, 16) from tab_empty;
+select compute_stats(b, 16) from tab_empty;
+select compute_stats(c, 16) from tab_empty;
+select compute_stats(d, 16) from tab_empty;
+select compute_stats(e, 16) from tab_empty;
+
+
diff --git a/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out b/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out
index 3390ca0ed8..007bc31c06 100644
--- a/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out
+++ b/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out
@@ -264,3 +264,124 @@ POSTHOOK: query: analyze table UserVisits_web_text_none compute statistics for c
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@uservisits_web_text_none
 #### A masked pattern was here ####
+PREHOOK: query: CREATE TABLE empty_tab(
+   a int,
+   b double,
+   c string, 
+   d boolean,
+   e binary)
+row format delimited fields terminated by '|'  stored as textfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE empty_tab(
+   a int,
+   b double,
+   c string, 
+   d boolean,
+   e binary)
+row format delimited fields terminated by '|'  stored as textfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@empty_tab
+PREHOOK: query: explain 
+analyze table empty_tab compute statistics for columns a,b,c,d,e
+PREHOOK: type: QUERY
+POSTHOOK: query: explain 
+analyze table empty_tab compute statistics for columns a,b,c,d,e
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME empty_tab)) (TOK_TABCOLNAME a b c d e))
+
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+  Stage-1 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Map Reduce
+      Alias -> Map Operator Tree:
+        empty_tab 
+          TableScan
+            alias: empty_tab
+            Select Operator
+              expressions:
+                    expr: a
+                    type: int
+                    expr: b
+                    type: double
+                    expr: c
+                    type: string
+                    expr: d
+                    type: boolean
+                    expr: e
+                    type: binary
+              outputColumnNames: a, b, c, d, e
+              Group By Operator
+                aggregations:
+                      expr: compute_stats(a, 16)
+                      expr: compute_stats(b, 16)
+                      expr: compute_stats(c, 16)
+                      expr: compute_stats(d, 16)
+                      expr: compute_stats(e, 16)
+                bucketGroup: false
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Reduce Output Operator
+                  sort order: 
+                  tag: -1
+                  value expressions:
+                        expr: _col0
+                        type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
+                        expr: _col1
+                        type: struct<columntype:string,min:double,max:double,countnulls:bigint,bitvector:string,numbitvectors:int>
+                        expr: _col2
+                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
+                        expr: _col3
+                        type: struct<columntype:string,counttrues:bigint,countfalses:bigint,countnulls:bigint>
+                        expr: _col4
+                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint>
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: compute_stats(VALUE._col0)
+                expr: compute_stats(VALUE._col1)
+                expr: compute_stats(VALUE._col2)
+                expr: compute_stats(VALUE._col3)
+                expr: compute_stats(VALUE._col4)
+          bucketGroup: false
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>
+                  expr: _col1
+                  type: struct<columntype:string,min:double,max:double,countnulls:bigint,numdistinctvalues:bigint>
+                  expr: _col2
+                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>
+                  expr: _col3
+                  type: struct<columntype:string,counttrues:bigint,countfalses:bigint,countnulls:bigint>
+                  expr: _col4
+                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint>
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-1
+    Column Stats Work
+      Column Stats Desc:
+          Columns: a, b, c, d, e
+          Column Types: int, double, string, boolean, binary
+          Table: empty_tab
+
+
+PREHOOK: query: analyze table empty_tab compute statistics for columns a,b,c,d,e
+PREHOOK: type: QUERY
+PREHOOK: Input: default@empty_tab
+#### A masked pattern was here ####
+POSTHOOK: query: analyze table empty_tab compute statistics for columns a,b,c,d,e
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@empty_tab
+#### A masked pattern was here ####
diff --git a/ql/src/test/results/clientpositive/compute_stats_empty_table.q.out b/ql/src/test/results/clientpositive/compute_stats_empty_table.q.out
new file mode 100644
index 0000000000..8dffaf3db5
--- /dev/null
+++ b/ql/src/test/results/clientpositive/compute_stats_empty_table.q.out
@@ -0,0 +1,61 @@
+PREHOOK: query: create table tab_empty(a boolean, b int, c double, d string, e binary)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table tab_empty(a boolean, b int, c double, d string, e binary)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tab_empty
+PREHOOK: query: select count(*) from tab_empty
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+POSTHOOK: query: select count(*) from tab_empty
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+0
+PREHOOK: query: -- compute statistical summary of data
+select compute_stats(a, 16) from tab_empty
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+POSTHOOK: query: -- compute statistical summary of data
+select compute_stats(a, 16) from tab_empty
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+{"columntype":"Boolean","counttrues":0,"countfalses":0,"countnulls":0}
+PREHOOK: query: select compute_stats(b, 16) from tab_empty
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+POSTHOOK: query: select compute_stats(b, 16) from tab_empty
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+{"columntype":"Long","min":0,"max":0,"countnulls":0,"numdistinctvalues":0}
+PREHOOK: query: select compute_stats(c, 16) from tab_empty
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+POSTHOOK: query: select compute_stats(c, 16) from tab_empty
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+{"columntype":"Double","min":0.0,"max":0.0,"countnulls":0,"numdistinctvalues":0}
+PREHOOK: query: select compute_stats(d, 16) from tab_empty
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+POSTHOOK: query: select compute_stats(d, 16) from tab_empty
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+{"columntype":"String","maxlength":0,"avglength":0.0,"countnulls":0,"numdistinctvalues":0}
+PREHOOK: query: select compute_stats(e, 16) from tab_empty
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+POSTHOOK: query: select compute_stats(e, 16) from tab_empty
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab_empty
+#### A masked pattern was here ####
+{"columntype":"Binary","maxlength":0,"avglength":0.0,"countnulls":0}
