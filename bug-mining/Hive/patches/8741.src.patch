diff --git a/common/src/java/org/apache/hadoop/hive/conf/Constants.java b/common/src/java/org/apache/hadoop/hive/conf/Constants.java
index efbee20c55..4b0e75fafb 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/Constants.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/Constants.java
@@ -99,7 +99,8 @@ public class Constants {
   public static final String ORC_INPUT_FORMAT = "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat";
   public static final String ORC_OUTPUT_FORMAT = "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat";
   
-  public static final Pattern COMPACTION_POOLS_PATTERN = Pattern.compile("hive\\.compactor\\.worker\\.(.*)\\.threads");
+  public static final Pattern COMPACTION_POOLS_PATTERN = Pattern.compile("^hive\\.compactor\\.worker\\.(.+)\\.threads$");
+  public static final String COMPACTION_DEFAULT_POOL = "default";
   public static final String HIVE_COMPACTOR_WORKER_POOL = "hive.compactor.worker.pool";
 
   public static final String HTTP_HEADER_REQUEST_TRACK = "X-Request-ID";
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 1b181c5ff4..635d54415a 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -62,7 +62,6 @@
 import java.net.URLEncoder;
 import java.nio.charset.StandardCharsets;
 import java.time.ZoneId;
-import java.util.AbstractMap;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.EnumSet;
@@ -7245,15 +7244,4 @@ public void syncFromConf(HiveConf conf) {
       set(e.getKey(), e.getValue());
     }
   }
-
-  public List<Map.Entry<String, String>> getMatchingEntries(Pattern regex) {
-    List<Map.Entry<String, String>> matchingEntries = new ArrayList<>();
-    for (Map.Entry<String, String> entry : this) {
-      Matcher matcher = regex.matcher(entry.getKey());
-      if (matcher.matches()) {
-        matchingEntries.add(new AbstractMap.SimpleEntry<>(entry.getKey(), matcher.group(0)));
-      }
-    }
-    return matchingEntries;
-  }
 }
diff --git a/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
index 12dfd1b8e0..1348ecb2c7 100644
--- a/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
+++ b/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -491,6 +491,7 @@ public enum ErrorMsg {
   UNEXPECTED_PARTITION_TRANSFORM_SPEC(10437, "Partition transforms are only supported by Iceberg storage handler", true),
   NONICEBERG_COMPACTION_WITH_FILTER_NOT_SUPPORTED(10440, "Compaction with filter is not allowed on non-Iceberg table {0}.{1}", true),
   ICEBERG_COMPACTION_WITH_PART_SPEC_AND_FILTER_NOT_SUPPORTED(10441, "Compaction command with both partition spec and filter is not supported on Iceberg table {0}.{1}", true),
+  COMPACTION_THREAD_INITIALIZATION(10442, "Compaction thread failed during initialization", false),
 
   //========================== 20000 range starts here ========================//
 
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionPoolOnTezTest.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionPoolOnTezTest.java
index ee552208bb..e63afeb3da 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionPoolOnTezTest.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionPoolOnTezTest.java
@@ -17,7 +17,6 @@
  */
 package org.apache.hadoop.hive.ql.txn.compactor;
 
-import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.hive.conf.Constants;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.MetaException;
@@ -27,7 +26,6 @@
 import org.apache.hadoop.hive.metastore.txn.TxnStore;
 import org.apache.hadoop.hive.metastore.txn.TxnUtils;
 import org.apache.hadoop.hive.ql.TxnCommandsBaseForTests;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
 import org.junit.Assert;
 import org.junit.Test;
 
@@ -36,7 +34,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.regex.Pattern;
-import java.io.*;
 
 import static org.apache.hadoop.hive.ql.txn.compactor.TestCompactorBase.executeStatementOnDriver;
 
@@ -171,4 +168,25 @@ public void testShowCompactionsRespectPoolName() throws Exception {
     Assert.assertTrue(p.matcher(results.get(1).toString()).matches());
   }
 
+  @Test
+  public void testCompactionWithCustomPool() throws Exception {
+    String poolName = "pool1";
+    conf.setInt(String.format("hive.compactor.worker.%s.threads", poolName), 1);
+
+    Map<String, String> properties = new HashMap<>();
+    properties.put(Constants.HIVE_COMPACTOR_WORKER_POOL, poolName);
+    provider.createFullAcidTable(null, DEFAULT_TABLE_NAME, false, false, properties);
+    provider.insertTestData(DEFAULT_TABLE_NAME, false);
+
+    TxnCommandsBaseForTests.runInitiator(conf);
+
+    checkCompactionRequest("initiated", poolName);
+
+    Map<String, Integer> customPools = CompactorUtil.getPoolConf(conf);
+    Assert.assertEquals(1, customPools.size());
+    Assert.assertEquals(Integer.valueOf(1), customPools.get(poolName));
+
+    TxnCommandsBaseForTests.runWorker(conf, poolName);
+    checkCompactionRequest("ready for cleaning", poolName);
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionException.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionException.java
new file mode 100644
index 0000000000..302345f028
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionException.java
@@ -0,0 +1,94 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.txn.compactor;
+
+import org.apache.hadoop.hive.common.classification.InterfaceAudience;
+import org.apache.hadoop.hive.common.classification.InterfaceStability;
+import org.apache.hadoop.hive.ql.ErrorMsg;
+
+@InterfaceAudience.Public
+@InterfaceStability.Stable
+public class CompactionException extends RuntimeException {
+
+  /**
+   * Standard predefined message with error code and possibly SQL State, etc.
+   */
+  private final ErrorMsg canonicalErrorMsg;
+
+  /**
+   * Error Messages returned from remote exception (eg. hadoop error)
+   */
+  private final String remoteErrorMsg;
+
+  public CompactionException() {
+    this(null, null, ErrorMsg.GENERIC_ERROR);
+  }
+
+  public CompactionException(String message) {
+    this(message, null);
+  }
+
+  public CompactionException(Throwable cause) {
+    this(cause, null, ErrorMsg.GENERIC_ERROR);
+  }
+
+  public CompactionException(String message, Throwable cause) {
+    super(message, cause);
+    canonicalErrorMsg = ErrorMsg.GENERIC_ERROR;
+    remoteErrorMsg = null;
+  }
+
+  public CompactionException(ErrorMsg message, String... msgArgs) {
+    this(null, null, message, msgArgs);
+  }
+
+  public CompactionException(Throwable cause, ErrorMsg errorMsg, String... msgArgs) {
+    this(cause, null, errorMsg, msgArgs);
+  }
+
+  public CompactionException(Throwable cause, ErrorMsg errorMsg) {
+    this(cause, null, errorMsg);
+  }
+
+  public CompactionException(ErrorMsg errorMsg) {
+    this(null, null, errorMsg);
+  }
+
+  /**
+   * This is the recommended constructor to use since it helps use
+   * canonical messages throughout and propagate remote errors.
+   *
+   * @param errorMsg Canonical error message
+   * @param msgArgs message arguments if message is parametrized; must be {@code null} is message takes no arguments
+   */
+  public CompactionException(Throwable cause, String remErrMsg, ErrorMsg errorMsg, String... msgArgs) {
+    super(errorMsg.format(msgArgs), cause);
+    canonicalErrorMsg = errorMsg;
+    remoteErrorMsg = remErrMsg;
+  }
+
+  /**
+   * @return {@link ErrorMsg#GENERIC_ERROR} by default
+   */
+  public ErrorMsg getCanonicalErrorMsg() {
+    return canonicalErrorMsg;
+  }
+
+  public String getRemoteErrorMsg() { return remoteErrorMsg; }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java
index 09296293f0..ec0619f799 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java
@@ -29,6 +29,7 @@
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.txn.entities.CompactionInfo;
 
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.thrift.TException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -111,11 +112,14 @@ protected String tableName(Table t) {
     return Warehouse.getQualifiedName(t);
   }
 
-  public static void initializeAndStartThread(CompactorThread thread,
-      Configuration conf) throws Exception {
+  public static void initializeAndStartThread(CompactorThread thread, Configuration conf) {
     LOG.info("Starting compactor thread of type " + thread.getClass().getName());
     thread.setConf(conf);
-    thread.init(new AtomicBoolean());
+    try {
+      thread.init(new AtomicBoolean());
+    } catch (Exception e) {
+      throw new CompactionException(e, ErrorMsg.COMPACTION_THREAD_INITIALIZATION);
+    }
     thread.start();
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java
index 7bcd4ffd55..6f953a63b9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java
@@ -17,7 +17,9 @@
  */
 package org.apache.hadoop.hive.ql.txn.compactor;
 
+import com.google.common.collect.Maps;
 import org.apache.commons.lang3.StringUtils;
+import org.apache.commons.lang3.math.NumberUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -25,6 +27,7 @@
 import org.apache.hadoop.hive.common.ValidReadTxnList;
 import org.apache.hadoop.hive.common.ValidTxnList;
 import org.apache.hadoop.hive.common.ValidWriteIdList;
+import org.apache.hadoop.hive.conf.Constants;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.LockComponentBuilder;
@@ -76,6 +79,7 @@
 import java.util.concurrent.ForkJoinWorkerThread;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Function;
+import java.util.regex.Matcher;
 import java.util.stream.Collectors;
 
 import static java.lang.String.format;
@@ -549,4 +553,15 @@ public static CompactionResponse initiateCompactionForPartition(Table table, Par
     }
     return compactionResponse;
   }
+
+  public static Map<String, Integer> getPoolConf(HiveConf hiveConf) {
+    Map<String, Integer> poolConf = Maps.newHashMap();
+    for (Map.Entry<String, String> entry : hiveConf) {
+      Matcher matcher = Constants.COMPACTION_POOLS_PATTERN.matcher(entry.getKey());
+      if (matcher.matches()) {
+        poolConf.put(matcher.group(1), NumberUtils.toInt(entry.getValue(), 0));
+      }
+    }
+    return poolConf;
+  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java b/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java
index f32a1d47d1..a0ae8f8603 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java
@@ -35,6 +35,7 @@
 import org.apache.hadoop.fs.LocatedFileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.RemoteIterator;
+import org.apache.hadoop.hive.conf.Constants;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.utils.TestTxnDbUtil;
@@ -246,12 +247,18 @@ public static void runInitiator(HiveConf hiveConf) throws Exception {
   public static void runWorker(HiveConf hiveConf) throws Exception {
     runCompactorThread(hiveConf, CompactorThreadType.WORKER);
   }
+  public static void runWorker(HiveConf hiveConf, String poolName) throws Exception {
+    runCompactorThread(hiveConf, CompactorThreadType.WORKER, poolName);
+  }
   public static void runCleaner(HiveConf hiveConf) throws Exception {
     // Wait for the cooldown period so the Cleaner can see the last committed txn as the highest committed watermark
     Thread.sleep(MetastoreConf.getTimeVar(hiveConf, MetastoreConf.ConfVars.TXN_OPENTXN_TIMEOUT, TimeUnit.MILLISECONDS));
     runCompactorThread(hiveConf, CompactorThreadType.CLEANER);
   }
-  private static void runCompactorThread(HiveConf hiveConf, CompactorThreadType type)
+  private static void runCompactorThread(HiveConf hiveConf, CompactorThreadType type) throws Exception {
+    runCompactorThread(hiveConf, type, Constants.COMPACTION_DEFAULT_POOL);
+  }
+  private static void runCompactorThread(HiveConf hiveConf, CompactorThreadType type, String poolName)
       throws Exception {
     AtomicBoolean stop = new AtomicBoolean(true);
     CompactorThread t;
@@ -261,6 +268,9 @@ private static void runCompactorThread(HiveConf hiveConf, CompactorThreadType ty
         break;
       case WORKER:
         t = new Worker();
+        if (poolName != null && !poolName.equals(Constants.COMPACTION_DEFAULT_POOL)) {
+          ((Worker)t).setPoolName(poolName); 
+        }
         break;
       case CLEANER:
         t = new Cleaner();
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCompactorUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCompactorUtil.java
new file mode 100644
index 0000000000..33a3a38a9a
--- /dev/null
+++ b/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCompactorUtil.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.txn.compactor;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.Map;
+
+public class TestCompactorUtil {
+
+  @Test
+  public void testGetPoolConf() {
+    HiveConf conf = new HiveConf();
+    conf.setInt("hive.compactor.worker.iceberg1.threads", 4);
+    conf.setInt("hive.compactor.worker.iceberg2", 4);
+    conf.setInt("hive.compactor.worker.iceberg3.thread.zzz", 4);
+    conf.setInt("aaa.hive.compactor.worker.iceberg4.threads", 4);
+
+    Map<String, Integer> entries = CompactorUtil.getPoolConf(conf);
+ 
+    Assert.assertEquals(1, entries.size());
+    Assert.assertEquals(Integer.valueOf(4), entries.get("iceberg1"));
+  }
+}
diff --git a/service/src/java/org/apache/hive/service/server/HiveServer2.java b/service/src/java/org/apache/hive/service/server/HiveServer2.java
index 4af82548ac..b006d49cf0 100644
--- a/service/src/java/org/apache/hive/service/server/HiveServer2.java
+++ b/service/src/java/org/apache/hive/service/server/HiveServer2.java
@@ -35,6 +35,7 @@
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.stream.IntStream;
 
 import org.apache.commons.cli.GnuParser;
 import org.apache.commons.cli.HelpFormatter;
@@ -91,12 +92,14 @@
 import org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.txn.compactor.CompactorThread;
+import org.apache.hadoop.hive.ql.txn.compactor.CompactorUtil;
 import org.apache.hadoop.hive.ql.txn.compactor.Worker;
 import org.apache.hadoop.hive.registry.impl.ZookeeperUtils;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.hive.shims.Utils;
 import org.apache.hive.common.util.HiveStringUtils;
 import org.apache.hive.common.util.HiveVersionInfo;
+import org.apache.hive.common.util.Ref;
 import org.apache.hive.common.util.ShutdownHookManager;
 import org.apache.hive.http.HttpServer;
 import org.apache.hive.http.JdbcJarDownloadServlet;
@@ -1222,63 +1225,62 @@ private static void startHiveServer2() throws Throwable {
     }
   }
 
-  private void maybeStartCompactorThreads(HiveConf hiveConf) throws Exception {
+  public Map<String, Integer> maybeStartCompactorThreads(HiveConf hiveConf) {
+    Map<String, Integer> startedWorkers = new HashMap<>();
     if (MetastoreConf.getVar(hiveConf, MetastoreConf.ConfVars.HIVE_METASTORE_RUNWORKER_IN).equals("hs2")) {
-      int numWorkers = MetastoreConf.getIntVar(hiveConf, MetastoreConf.ConfVars.COMPACTOR_WORKER_THREADS);
-      List<Map.Entry<String, String>> entries = hiveConf.getMatchingEntries(Constants.COMPACTION_POOLS_PATTERN);
+      Ref<Integer> numWorkers = new Ref<>(MetastoreConf.getIntVar(hiveConf, MetastoreConf.ConfVars.COMPACTOR_WORKER_THREADS));
+      Map<String, Integer> customPools = CompactorUtil.getPoolConf(hiveConf);
 
       StringBuilder sb = new StringBuilder(2048);
       sb.append("This HS2 instance will act as Compactor with the following worker pool configuration:\n");
-      sb.append("Global pool size: ").append(numWorkers).append("\n");
-
-      LOG.info("Initializing the compaction pools with using the global worker limit: {} ", numWorkers);
-      while (numWorkers > 0 && entries.size() > 0) {
-        Map.Entry<String, String> entry = entries.remove(0);
-        String poolName = entry.getValue();
-        int poolWorkers = hiveConf.getInt(entry.getKey(), 0);
+      sb.append("Global pool size: ").append(numWorkers.value).append("\n");
 
+      LOG.info("Initializing the compaction pools with using the global worker limit: {} ", numWorkers.value);
+      customPools.forEach((poolName, poolWorkers) -> {
         if (poolWorkers == 0) {
-          LOG.warn("Compaction pool ({}) configured with zero workers. Skipping pool initialization", poolName);
-          sb.append("Pool not initialized, 0 size: ").append(poolName).append("\n");
-          continue;
+          LOG.warn("Pool not initialized, configured with zero workers: {}", poolName);
         }
-        if (poolWorkers > numWorkers) {
-          LOG.warn("Global worker pool exhausted, compaction pool ({}) will be configured with less workers than the " +
-              "required number. ({} -> {})", poolName, poolWorkers, numWorkers);
-          poolWorkers = numWorkers;
+        else if (numWorkers.value == 0) {
+          LOG.warn("Pool not initialized, no available workers remained: {}", poolName);
         }
+        else {
+          if (poolWorkers > numWorkers.value) {
+            LOG.warn("Global worker pool exhausted, compaction pool ({}) will be configured with less workers than the " +
+                "required number. ({} -> {})", poolName, poolWorkers, numWorkers.value);
+            poolWorkers = numWorkers.value;
+          }
 
-        LOG.info("Initializing compaction pool ({}) with {} workers.", poolName, poolWorkers);
-        for (int i = 0; i < poolWorkers; i++) {
-          Worker w = new Worker();
-          w.setPoolName(poolName);
-          CompactorThread.initializeAndStartThread(w, hiveConf);
-          sb.append("Worker - Name: ").append(w.getName()).append(", Pool: ").append(poolName)
-              .append(", Priority: ").append(w.getPriority()).append("\n");
+          LOG.info("Initializing compaction pool ({}) with {} workers.", poolName, poolWorkers);
+          IntStream.range(0, poolWorkers).forEach(i -> {
+            Worker w = new Worker();
+            w.setPoolName(poolName);
+            CompactorThread.initializeAndStartThread(w, hiveConf);
+            startedWorkers.compute(poolName, (k, v) -> (v == null) ? 1 : v + 1);
+            sb.append(
+                String.format("Worker - Name: %s, Pool: %s, Priority: %d", w.getName(), poolName, w.getPriority())
+            );
+          });
+          numWorkers.value -= poolWorkers;
         }
-        numWorkers -= poolWorkers;
-      }
+      });
 
-      if (numWorkers == 0) {
+      if (numWorkers.value == 0) {
         LOG.warn("No default compaction pool configured, all non-labeled compaction requests will remain unprocessed!");
-        if (entries.size() > 0) {
-          for (Map.Entry<String, String> entry : entries) {
-            String poolName = entry.getValue();
-            LOG.warn("There are no available workers for the following compaction pool: {} ", poolName);
-            sb.append("Pool not initialized, no remaining free workers: ").append(poolName).append("\n");
-          }
+        if (customPools.size() > 0) {
           sb.append("Pool not initialized, no remaining free workers: default\n");
         }
       } else {
-        LOG.info("Initializing default compaction pool with {} workers.", numWorkers);
-        for (int i = 0; i < numWorkers; i++) {
+        LOG.info("Initializing default compaction pool with {} workers.", numWorkers.value);
+        IntStream.range(0, numWorkers.value).forEach(i -> {
           Worker w = new Worker();
           CompactorThread.initializeAndStartThread(w, hiveConf);
+          startedWorkers.compute(Constants.COMPACTION_DEFAULT_POOL, (k, v) -> (v == null) ? 1 : v + 1);
           sb.append("Worker - Name: ").append(w.getName()).append(", Pool: default, Priority: ").append(w.getPriority()).append("\n");
-        }
+        });
       }
       LOG.info(sb.toString());
     }
+    return startedWorkers;
   }
 
   /**
diff --git a/service/src/test/org/apache/hive/service/server/TestHiveServer2.java b/service/src/test/org/apache/hive/service/server/TestHiveServer2.java
new file mode 100644
index 0000000000..7393dd6a77
--- /dev/null
+++ b/service/src/test/org/apache/hive/service/server/TestHiveServer2.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.server;
+
+import org.apache.hadoop.hive.conf.Constants;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
+import org.junit.Test;
+import java.util.Map;
+import static org.junit.Assert.assertEquals;
+
+public class TestHiveServer2 {
+
+  @Test
+  public void testMaybeStartCompactorThreadsOneCustomPool() {
+    HiveServer2 hs2 = new HiveServer2();
+
+    HiveConf conf = new HiveConf();
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.HIVE_METASTORE_RUNWORKER_IN, "hs2");
+    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.COMPACTOR_WORKER_THREADS, 1);
+    conf.setInt("hive.compactor.worker.pool1.threads", 1);
+
+    Map<String, Integer> startedWorkers = hs2.maybeStartCompactorThreads(conf);
+    assertEquals(1, startedWorkers.size());
+    assertEquals(Integer.valueOf(1), startedWorkers.get("pool1"));
+  }
+
+  @Test
+  public void testMaybeStartCompactorThreadsZeroTotalWorkers() {
+    HiveServer2 hs2 = new HiveServer2();
+
+    HiveConf conf = new HiveConf();
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.HIVE_METASTORE_RUNWORKER_IN, "hs2");
+    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.COMPACTOR_WORKER_THREADS, 0);
+    conf.setInt("hive.compactor.worker.pool1.threads", 5);
+
+    Map<String, Integer> startedWorkers = hs2.maybeStartCompactorThreads(conf);
+    assertEquals(0, startedWorkers.size());
+  }
+
+  @Test
+  public void testMaybeStartCompactorThreadsZeroCustomWorkers() {
+    HiveServer2 hs2 = new HiveServer2();
+
+    HiveConf conf = new HiveConf();
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.HIVE_METASTORE_RUNWORKER_IN, "hs2");
+    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.COMPACTOR_WORKER_THREADS, 5);
+
+    Map<String, Integer> startedWorkers = hs2.maybeStartCompactorThreads(conf);
+    assertEquals(1, startedWorkers.size());
+    assertEquals(Integer.valueOf(5), startedWorkers.get(Constants.COMPACTION_DEFAULT_POOL));
+  }
+
+  @Test
+  public void testMaybeStartCompactorThreadsMultipleCustomPools() {
+    HiveServer2 hs2 = new HiveServer2();
+
+    HiveConf conf = new HiveConf();
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.HIVE_METASTORE_RUNWORKER_IN, "hs2");
+    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.COMPACTOR_WORKER_THREADS, 12);
+    conf.setInt("hive.compactor.worker.pool1.threads", 3);
+    conf.setInt("hive.compactor.worker.pool2.threads", 4);
+    conf.setInt("hive.compactor.worker.pool3.threads", 5);
+
+    Map<String, Integer> startedWorkers = hs2.maybeStartCompactorThreads(conf);
+    assertEquals(3, startedWorkers.size());
+    assertEquals(Integer.valueOf(3), startedWorkers.get("pool1"));
+    assertEquals(Integer.valueOf(4), startedWorkers.get("pool2"));
+    assertEquals(Integer.valueOf(5), startedWorkers.get("pool3"));
+  }
+
+  @Test
+  public void testMaybeStartCompactorThreadsMultipleCustomPoolsAndDefaultPool() {
+    HiveServer2 hs2 = new HiveServer2();
+
+    HiveConf conf = new HiveConf();
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.HIVE_METASTORE_RUNWORKER_IN, "hs2");
+    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.COMPACTOR_WORKER_THREADS, 15);
+    conf.setInt("hive.compactor.worker.pool1.threads", 3);
+    conf.setInt("hive.compactor.worker.pool2.threads", 4);
+    conf.setInt("hive.compactor.worker.pool3.threads", 5);
+
+    Map<String, Integer> startedWorkers = hs2.maybeStartCompactorThreads(conf);
+    assertEquals(4, startedWorkers.size());
+    assertEquals(Integer.valueOf(3), startedWorkers.get("pool1"));
+    assertEquals(Integer.valueOf(4), startedWorkers.get("pool2"));
+    assertEquals(Integer.valueOf(5), startedWorkers.get("pool3"));
+    assertEquals(Integer.valueOf(3), startedWorkers.get(Constants.COMPACTION_DEFAULT_POOL));
+  }
+}
