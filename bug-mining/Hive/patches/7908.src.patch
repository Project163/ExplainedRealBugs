diff --git a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
index 50bca663d8..5c828fac02 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
@@ -3338,4 +3338,30 @@ public void testReplDBLocationDelete() throws Exception {
       fs.delete(cmQualPath, true);
     }
   }
+
+  @Test
+  public void testHeaderFooterNonTextFiles() throws Exception {
+    HiveStatement stmt = (HiveStatement) con.createStatement();
+    try {
+      // Test with header for non text file.
+      stmt.execute(
+          "CREATE EXTERNAL TABLE parquet_emp (id int) STORED AS PARQUET "
+              + "TBLPROPERTIES ('skip.header.line.count'='1')");
+      stmt.execute("insert into parquet_emp values(1),(2),(3),(4)");
+      ResultSet result = stmt.executeQuery("select count(*) from parquet_emp");
+      assertTrue(result.next());
+      assertEquals(4, result.getInt("_c0"));
+
+      // Test with footer for non text file
+      stmt.execute(
+          "CREATE EXTERNAL TABLE parquetf_emp (id int) STORED AS PARQUET "
+              + "TBLPROPERTIES ('skip.footer.line.count'='1')");
+      stmt.execute("insert into parquetf_emp values(1),(2),(3),(4)");
+      result = stmt.executeQuery("select count(*) from parquetf_emp");
+      assertTrue(result.next());
+      assertEquals(4, result.getInt("_c0"));
+    } finally {
+      stmt.close();
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index cccd922a52..b627303bd2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -169,7 +169,6 @@
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
-import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -4012,7 +4011,8 @@ public static <K, V> boolean skipHeader(RecordReader<K, V> currRecReader, int he
   public static int getHeaderCount(TableDesc table) throws IOException {
     int headerCount;
     try {
-      headerCount = Integer.parseInt(table.getProperties().getProperty(serdeConstants.HEADER_COUNT, "0"));
+      headerCount =
+          getHeaderOrFooterCount(table, serdeConstants.HEADER_COUNT);
     } catch (NumberFormatException nfe) {
       throw new IOException(nfe);
     }
@@ -4031,7 +4031,8 @@ public static int getHeaderCount(TableDesc table) throws IOException {
   public static int getFooterCount(TableDesc table, JobConf job) throws IOException {
     int footerCount;
     try {
-      footerCount = Integer.parseInt(table.getProperties().getProperty(serdeConstants.FOOTER_COUNT, "0"));
+      footerCount =
+          getHeaderOrFooterCount(table, serdeConstants.FOOTER_COUNT);
       if (footerCount > HiveConf.getIntVar(job, HiveConf.ConfVars.HIVE_FILE_MAX_FOOTER)) {
         throw new IOException("footer number exceeds the limit defined in hive.file.max.footer");
       }
@@ -4043,6 +4044,20 @@ public static int getFooterCount(TableDesc table, JobConf job) throws IOExceptio
     return footerCount;
   }
 
+  private static int getHeaderOrFooterCount(TableDesc table,
+      String propertyName) {
+    int count =
+        Integer.parseInt(table.getProperties().getProperty(propertyName, "0"));
+    if (count > 0 && table.getInputFileFormatClass() != null
+        && !TextInputFormat.class
+        .isAssignableFrom(table.getInputFileFormatClass())) {
+      LOG.warn(propertyName
+          + "  is only valid for TextInputFormat, ignoring the value.");
+      count = 0;
+    }
+    return count;
+  }
+
   /**
    * Convert path to qualified path.
    *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java
index 0a29762d37..c2a330f0b7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.plan.Explain.Vectorization;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.mapred.TextInputFormat;
 
 import java.io.Serializable;
 import java.util.ArrayList;
@@ -480,7 +481,9 @@ public void setNumBuckets(int numBuckets) {
 
   public boolean isNeedSkipHeaderFooters() {
     boolean rtn = false;
-    if (tableMetadata != null && tableMetadata.getTTable() != null) {
+    if (tableMetadata != null && tableMetadata.getTTable() != null
+        && TextInputFormat.class
+        .isAssignableFrom(tableMetadata.getInputFormatClass())) {
       Map<String, String> params = tableMetadata.getTTable().getParameters();
       if (params != null) {
         String skipHVal = params.get(serdeConstants.HEADER_COUNT);
