diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java
index 41a65ceb45..cdbc7f235a 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java
@@ -53,7 +53,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
@@ -148,7 +148,7 @@ public void setConf(Configuration conf) {
 
   @SuppressWarnings("deprecation")
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return AccumuloSerDe.class;
   }
 
diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/AccumuloSerDe.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/AccumuloSerDe.java
index 40c95530e6..fcd819b72b 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/AccumuloSerDe.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/AccumuloSerDe.java
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.accumulo.LazyAccumuloRow;
 import org.apache.hadoop.hive.accumulo.columns.ColumnMapping;
 import org.apache.hadoop.hive.accumulo.columns.HiveAccumuloRowIdColumnMapping;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
 import org.apache.hadoop.hive.serde2.lazy.LazyFactory;
@@ -45,7 +45,7 @@
  * Deserialization from Accumulo to LazyAccumuloRow for Hive.
  *
  */
-public class AccumuloSerDe implements SerDe {
+public class AccumuloSerDe extends AbstractSerDe {
 
   private AccumuloSerDeParameters accumuloSerDeParameters;
   private LazyAccumuloRow cachedRow;
diff --git a/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java b/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java
index 639fc3ae94..62e5c81865 100644
--- a/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java
+++ b/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java
@@ -23,7 +23,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -36,7 +36,7 @@
  */
 public class TestRegexSerDe extends TestCase {
 
-  private SerDe createSerDe(String fieldNames, String fieldTypes,
+  private AbstractSerDe createSerDe(String fieldNames, String fieldTypes,
       String inputRegex, String outputFormatString) throws Throwable {
     Properties schema = new Properties();
     schema.setProperty(serdeConstants.LIST_COLUMNS, fieldNames);
@@ -55,7 +55,7 @@ private SerDe createSerDe(String fieldNames, String fieldTypes,
   public void testRegexSerDe() throws Throwable {
     try {
       // Create the SerDe
-      SerDe serDe = createSerDe(
+      AbstractSerDe serDe = createSerDe(
           "host,identity,user,time,request,status,size,referer,agent",
           "string,string,string,string,string,string,string,string,string",
           "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") " 
diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java
index ac03099188..8242385af0 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java
@@ -25,7 +25,7 @@
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.OutputFormat;
 import org.slf4j.Logger;
@@ -50,7 +50,7 @@ public Class<? extends OutputFormat> getOutputFormatClass() {
   }
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return DruidSerDe.class;
   }
 
diff --git a/druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidStorageHandler.java b/druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidStorageHandler.java
index 0a44aaac5d..6db13c3799 100644
--- a/druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidStorageHandler.java
+++ b/druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidStorageHandler.java
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.druid;
 
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 
 /**
  * Storage handler for Druid to be used in tests. It cannot connect to
@@ -27,7 +27,7 @@
 public class QTestDruidStorageHandler extends DruidStorageHandler {
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return QTestDruidSerDe.class;
   }
 
diff --git a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java
index 466aabef4a..c2e7808f16 100644
--- a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -117,7 +117,7 @@ public HBaseSerDe() throws SerDeException {
 
   /**
    * Initialize the SerDe given parameters.
-   * @see SerDe#initialize(Configuration, Properties)
+   * @see AbstractSerDe#initialize(Configuration, Properties)
    */
   @Override
   public void initialize(Configuration conf, Properties tbl)
@@ -268,7 +268,7 @@ public HBaseSerDeParameters getHBaseSerdeParam() {
    * Deserialize a row from the HBase Result writable to a LazyObject
    * @param result the HBase Result Writable containing the row
    * @return the deserialized object
-   * @see SerDe#deserialize(Writable)
+   * @see AbstractSerDe#deserialize(Writable)
    */
   @Override
   public Object deserialize(Writable result) throws SerDeException {
diff --git a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
index 1a1f780694..9cad97ad4b 100644
--- a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
@@ -65,7 +65,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping;
@@ -315,7 +315,7 @@ public Class<? extends OutputFormat> getOutputFormatClass() {
   }
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return HBaseSerDe.class;
   }
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java
index 81c79438fe..235d1863d5 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java
@@ -27,7 +27,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -56,7 +56,7 @@
 @SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS,
                           serdeConstants.LIST_COLUMN_TYPES})
 
-public class HCatRecordSerDe implements SerDe {
+public class HCatRecordSerDe extends AbstractSerDe {
 
   private static final Logger LOG = LoggerFactory.getLogger(HCatRecordSerDe.class);
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java
index 1b47b28a30..ef1707917d 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java
@@ -38,7 +38,7 @@
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -89,7 +89,7 @@
                           serdeConstants.LIST_COLUMN_TYPES,
                           serdeConstants.TIMESTAMP_FORMATS})
 
-public class JsonSerDe implements SerDe {
+public class JsonSerDe extends AbstractSerDe {
 
   private static final Logger LOG = LoggerFactory.getLogger(JsonSerDe.class);
   private List<String> columnNames;
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DefaultRecordWriterContainer.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DefaultRecordWriterContainer.java
index 209d7bcef5..13c4354e0b 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DefaultRecordWriterContainer.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DefaultRecordWriterContainer.java
@@ -22,7 +22,7 @@
 import java.io.IOException;
 
 import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.Writable;
@@ -39,7 +39,7 @@
 class DefaultRecordWriterContainer extends RecordWriterContainer {
 
   private final HiveStorageHandler storageHandler;
-  private final SerDe serDe;
+  private final AbstractSerDe serDe;
   private final OutputJobInfo jobInfo;
   private final ObjectInspector hcatRecordOI;
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java
index a7c9f29ecc..b53dcf197a 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java
@@ -27,7 +27,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.Writable;
@@ -56,7 +56,7 @@ class DynamicPartitionFileRecordWriterContainer extends FileRecordWriterContaine
   private int maxDynamicPartitions;
 
   private final Map<String, RecordWriter<? super WritableComparable<?>, ? super Writable>> baseDynamicWriters;
-  private final Map<String, SerDe> baseDynamicSerDe;
+  private final Map<String, AbstractSerDe> baseDynamicSerDe;
   private final Map<String, org.apache.hadoop.mapred.OutputCommitter> baseDynamicCommitters;
   private final Map<String, org.apache.hadoop.mapred.TaskAttemptContext> dynamicContexts;
   private final Map<String, ObjectInspector> dynamicObjectInspectors;
@@ -81,7 +81,7 @@ public DynamicPartitionFileRecordWriterContainer(
           + "HCatOutputFormat. Please make sure that method is called.");
     }
 
-    this.baseDynamicSerDe = new HashMap<String, SerDe>();
+    this.baseDynamicSerDe = new HashMap<String, AbstractSerDe>();
     this.baseDynamicWriters =
         new HashMap<String, RecordWriter<? super WritableComparable<?>, ? super Writable>>();
     this.baseDynamicCommitters = new HashMap<String, org.apache.hadoop.mapred.OutputCommitter>();
@@ -159,7 +159,7 @@ protected LocalFileWriter getLocalFileWriter(HCatRecord value) throws IOExceptio
       localJobInfo = HCatBaseOutputFormat.getJobInfo(currTaskContext.getConfiguration());
 
       // Setup serDe.
-      SerDe currSerDe =
+      AbstractSerDe currSerDe =
           ReflectionUtils.newInstance(storageHandler.getSerDeClass(), currTaskContext.getJobConf());
       try {
         InternalUtil.initializeOutputSerDe(currSerDe, currTaskContext.getConfiguration(),
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputFormatContainer.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputFormatContainer.java
index 95ee3b4d1a..3ecb6080e6 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputFormatContainer.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputFormatContainer.java
@@ -29,7 +29,7 @@
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
@@ -82,8 +82,8 @@ public RecordWriter<WritableComparable<?>, HCatRecord> getRecordWriter(TaskAttem
     StorerInfo storeInfo = jobInfo.getTableInfo().getStorerInfo();
     HiveStorageHandler storageHandler = HCatUtil.getStorageHandler(
       context.getConfiguration(), storeInfo);
-    Class<? extends SerDe> serde = storageHandler.getSerDeClass();
-    SerDe sd = (SerDe) ReflectionUtils.newInstance(serde,
+    Class<? extends AbstractSerDe> serde = storageHandler.getSerDeClass();
+    AbstractSerDe sd = (AbstractSerDe) ReflectionUtils.newInstance(serde,
       context.getConfiguration());
     context.getConfiguration().set("mapred.output.value.class",
       sd.getSerializedClass().getName());
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileRecordWriterContainer.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileRecordWriterContainer.java
index 2a883d6517..b2abc5fbb3 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileRecordWriterContainer.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileRecordWriterContainer.java
@@ -28,7 +28,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.NullWritable;
@@ -54,7 +54,7 @@
 abstract class FileRecordWriterContainer extends RecordWriterContainer {
 
   protected final HiveStorageHandler storageHandler;
-  protected final SerDe serDe;
+  protected final AbstractSerDe serDe;
   protected final ObjectInspector objectInspector;
 
   private final List<Integer> partColsToDel;
@@ -110,7 +110,7 @@ public void write(WritableComparable<?> key, HCatRecord value) throws IOExceptio
     LocalFileWriter localFileWriter = getLocalFileWriter(value);
     RecordWriter localWriter = localFileWriter.getLocalWriter();
     ObjectInspector localObjectInspector = localFileWriter.getLocalObjectInspector();
-    SerDe localSerDe = localFileWriter.getLocalSerDe();
+    AbstractSerDe localSerDe = localFileWriter.getLocalSerDe();
     OutputJobInfo localJobInfo = localFileWriter.getLocalJobInfo();
 
     for (Integer colToDel : partColsToDel) {
@@ -129,11 +129,11 @@ public void write(WritableComparable<?> key, HCatRecord value) throws IOExceptio
   class LocalFileWriter {
     private RecordWriter localWriter;
     private ObjectInspector localObjectInspector;
-    private SerDe localSerDe;
+    private AbstractSerDe localSerDe;
     private OutputJobInfo localJobInfo;
 
     public LocalFileWriter(RecordWriter localWriter, ObjectInspector localObjectInspector,
-        SerDe localSerDe, OutputJobInfo localJobInfo) {
+        AbstractSerDe localSerDe, OutputJobInfo localJobInfo) {
       this.localWriter = localWriter;
       this.localObjectInspector = localObjectInspector;
       this.localSerDe = localSerDe;
@@ -148,7 +148,7 @@ public ObjectInspector getLocalObjectInspector() {
       return localObjectInspector;
     }
 
-    public SerDe getLocalSerDe() {
+    public AbstractSerDe getLocalSerDe() {
       return localSerDe;
     }
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java
index b970153e34..040906f34d 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.OutputFormat;
@@ -62,17 +62,17 @@ public class FosterStorageHandler extends DefaultStorageHandler {
 
   private Class<? extends InputFormat> ifClass;
   private Class<? extends OutputFormat> ofClass;
-  private Class<? extends SerDe> serDeClass;
+  private Class<? extends AbstractSerDe> serDeClass;
 
   public FosterStorageHandler(String ifName, String ofName, String serdeName) throws ClassNotFoundException {
     this((Class<? extends InputFormat>) JavaUtils.loadClass(ifName),
       (Class<? extends OutputFormat>) JavaUtils.loadClass(ofName),
-      (Class<? extends SerDe>) JavaUtils.loadClass(serdeName));
+      (Class<? extends AbstractSerDe>) JavaUtils.loadClass(serdeName));
   }
 
   public FosterStorageHandler(Class<? extends InputFormat> ifClass,
                 Class<? extends OutputFormat> ofClass,
-                Class<? extends SerDe> serDeClass) {
+                Class<? extends AbstractSerDe> serDeClass) {
     this.ifClass = ifClass;
     this.ofClass = ofClass;
     this.serDeClass = serDeClass;
@@ -89,7 +89,7 @@ public Class<? extends OutputFormat> getOutputFormatClass() {
   }
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return serDeClass;  //To change body of implemented methods use File | Settings | File Templates.
   }
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java
index 3100181409..1230795d04 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
@@ -141,7 +141,7 @@ private static ObjectInspector getObjectInspector(TypeInfo type) throws IOExcept
 
   //TODO this has to find a better home, it's also hardcoded as default in hive would be nice
   // if the default was decided by the serde
-  static void initializeOutputSerDe(SerDe serDe, Configuration conf, OutputJobInfo jobInfo)
+  static void initializeOutputSerDe(AbstractSerDe serDe, Configuration conf, OutputJobInfo jobInfo)
     throws SerDeException {
     SerDeUtils.initializeSerDe(serDe, conf,
                                getSerdeProperties(jobInfo.getTableInfo(),
diff --git a/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java b/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java
index 24b952e17f..e409e75571 100644
--- a/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java
+++ b/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java
@@ -32,7 +32,7 @@
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.ql.io.AcidOutputFormat;
 import org.apache.hadoop.hive.ql.io.RecordUpdater;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -152,7 +152,7 @@ private List<Integer> getBucketColIDs(List<String> bucketCols, List<FieldSchema>
    * @return serde
    * @throws SerializationError
    */
-  public abstract SerDe getSerde() throws SerializationError;
+  public abstract AbstractSerDe getSerde() throws SerializationError;
 
   /**
    * Encode a record as an Object that Hive can read with the ObjectInspector associated with the
diff --git a/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java b/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java
index 87eb4c4d89..58fba4f6a9 100644
--- a/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java
+++ b/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters;
@@ -270,7 +270,7 @@ public void write(long transactionId, byte[] record)
   }
 
   @Override
-  public SerDe getSerde() {
+  public AbstractSerDe getSerde() {
     return serde;
   }
 
diff --git a/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java b/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java
index 31212ee7ee..13756e281d 100644
--- a/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java
+++ b/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -98,7 +98,7 @@ public StrictJsonWriter(HiveEndPoint endPoint, HiveConf conf, StreamingConnectio
   }
 
   @Override
-  public SerDe getSerde() {
+  public AbstractSerDe getSerde() {
     return serde;
   }
 
diff --git a/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java b/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
index 4f6985cd13..3efe424c37 100644
--- a/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
+++ b/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;
 import org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector;
 import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -248,12 +248,12 @@ private Object createRandomRow(final String columnTypes) throws SerDeException {
    * methods.
    */
   private class StorageFormatTest {
-    private SerDe serDe;
+    private AbstractSerDe serDe;
     private JobConf jobConf;
     private HiveOutputFormat outputFormat;
     private InputFormat inputFormat;
 
-    public StorageFormatTest(SerDe serDeImpl, HiveOutputFormat outputFormatImpl, InputFormat inputFormatImpl) throws SerDeException {
+    public StorageFormatTest(AbstractSerDe serDeImpl, HiveOutputFormat outputFormatImpl, InputFormat inputFormatImpl) throws SerDeException {
       jobConf = new JobConf();
       serDe = serDeImpl;
       outputFormat = outputFormatImpl;
diff --git a/llap-client/src/java/org/apache/hadoop/hive/llap/LlapRowRecordReader.java b/llap-client/src/java/org/apache/hadoop/hive/llap/LlapRowRecordReader.java
index 10d7c947b9..ee92f3e231 100644
--- a/llap-client/src/java/org/apache/hadoop/hive/llap/LlapRowRecordReader.java
+++ b/llap-client/src/java/org/apache/hadoop/hive/llap/LlapRowRecordReader.java
@@ -37,7 +37,7 @@
 import org.apache.hadoop.hive.llap.Schema;
 import org.apache.hadoop.hive.llap.TypeDesc;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.io.HiveCharWritable;
 import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
@@ -60,7 +60,7 @@ public class LlapRowRecordReader implements RecordReader<NullWritable, Row> {
   protected final Configuration conf;
   protected final RecordReader<NullWritable, Text> reader;
   protected final Schema schema;
-  protected final SerDe serde;
+  protected final AbstractSerDe serde;
   protected final Text textData = new Text();
 
   public LlapRowRecordReader(Configuration conf, Schema schema, RecordReader<NullWritable, Text> reader) throws IOException {
@@ -147,7 +147,7 @@ public Schema getSchema() {
     return schema;
   }
 
-  protected SerDe initSerDe(Configuration conf) throws SerDeException {
+  protected AbstractSerDe initSerDe(Configuration conf) throws SerDeException {
     Properties props = new Properties();
     StringBuffer columnsBuffer = new StringBuffer();
     StringBuffer typesBuffer = new StringBuffer();
@@ -166,7 +166,7 @@ protected SerDe initSerDe(Configuration conf) throws SerDeException {
     props.put(serdeConstants.LIST_COLUMNS, columns);
     props.put(serdeConstants.LIST_COLUMN_TYPES, types);
     props.put(serdeConstants.ESCAPE_CHAR, "\\");
-    SerDe serde = new LazySimpleSerDe();
+    AbstractSerDe serde = new LazySimpleSerDe();
     serde.initialize(conf, props);
 
     return serde;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
index deb7c76730..ac5331e427 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
@@ -46,7 +46,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -180,7 +180,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     }
     try {
       TableDesc keyTableDesc = conf.getKeyTblDesc();
-      SerDe keySerde = (SerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),
+      AbstractSerDe keySerde = (AbstractSerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),
           null);
       SerDeUtils.initializeSerDe(keySerde, null, keyTableDesc.getProperties(), null);
       MapJoinObjectSerDeContext keyContext = new MapJoinObjectSerDeContext(keySerde, false);
@@ -190,7 +190,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
         }
         mapJoinTables[pos] = new HashMapWrapper(hconf, -1);
         TableDesc valueTableDesc = conf.getValueTblFilteredDescs().get(pos);
-        SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(), null);
+        AbstractSerDe valueSerDe = (AbstractSerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(valueSerDe, null, valueTableDesc.getProperties(), null);
         mapJoinTableSerdes[pos] = new MapJoinTableContainerSerDe(keyContext, new MapJoinObjectSerDeContext(
             valueSerDe, hasFilter(pos)));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
index 0aaa51a809..6cbcab6991 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.ql.plan.JoinDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -276,13 +276,13 @@ public static TableDesc getSpillTableDesc(Byte alias, TableDesc[] spillTableDesc
     return spillTableDesc[alias];
   }
 
-  public static SerDe getSpillSerDe(byte alias, TableDesc[] spillTableDesc,
+  public static AbstractSerDe getSpillSerDe(byte alias, TableDesc[] spillTableDesc,
       JoinDesc conf, boolean noFilter) {
     TableDesc desc = getSpillTableDesc(alias, spillTableDesc, conf, noFilter);
     if (desc == null) {
       return null;
     }
-    SerDe sd = (SerDe) ReflectionUtil.newInstance(desc.getDeserializerClass(),
+    AbstractSerDe sd = (AbstractSerDe) ReflectionUtil.newInstance(desc.getDeserializerClass(),
         null);
     try {
       SerDeUtils.initializeSerDe(sd, null, desc.getProperties(), null);
@@ -344,7 +344,7 @@ public static RowContainer<List<Object>> getRowContainer(Configuration hconf,
       JoinDesc conf,boolean noFilter, Reporter reporter) throws HiveException {
 
     TableDesc tblDesc = JoinUtil.getSpillTableDesc(alias,spillTableDesc,conf, noFilter);
-    SerDe serde = JoinUtil.getSpillSerDe(alias, spillTableDesc, conf, noFilter);
+    AbstractSerDe serde = JoinUtil.getSpillSerDe(alias, spillTableDesc, conf, noFilter);
 
     if (serde == null) {
       containerSize = -1;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
index 416606eaf8..07aa2ea6a3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
@@ -58,7 +58,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -275,7 +275,7 @@ public void generateMapMetaData() throws HiveException {
 
     try {
       TableDesc keyTableDesc = conf.getKeyTblDesc();
-      SerDe keySerializer = (SerDe) ReflectionUtil.newInstance(
+      AbstractSerDe keySerializer = (AbstractSerDe) ReflectionUtil.newInstance(
           keyTableDesc.getDeserializerClass(), null);
       SerDeUtils.initializeSerDe(keySerializer, null, keyTableDesc.getProperties(), null);
       MapJoinObjectSerDeContext keyContext = new MapJoinObjectSerDeContext(keySerializer, false);
@@ -289,7 +289,7 @@ public void generateMapMetaData() throws HiveException {
         } else {
           valueTableDesc = conf.getValueFilteredTblDescs().get(pos);
         }
-        SerDe valueSerDe = (SerDe) ReflectionUtil.newInstance(
+        AbstractSerDe valueSerDe = (AbstractSerDe) ReflectionUtil.newInstance(
             valueTableDesc.getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(valueSerDe, null, valueTableDesc.getProperties(), null);
         MapJoinObjectSerDeContext valueContext =
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java
index 8366ea7c83..f418a7f26b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java
@@ -37,7 +37,7 @@
 import org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag;
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
@@ -403,7 +403,7 @@ private void createInputPartition() throws HiveException {
       ObjectInspector inputOI = conf.getStartOfChain() == tabDef ?
           inputObjInspectors[0] : inputDef.getOutputShape().getOI();
 
-      SerDe serde = conf.isMapSide() ? tabDef.getInput().getOutputShape().getSerde() :
+      AbstractSerDe serde = conf.isMapSide() ? tabDef.getInput().getOutputShape().getSerde() :
         tabDef.getRawInputShape().getSerde();
       StructObjectInspector outputOI = conf.isMapSide() ? tabDef.getInput().getOutputShape().getOI() :
         tabDef.getRawInputShape().getOI();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java
index 0d0211f841..edcb8f76c2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java
@@ -29,7 +29,7 @@
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
@@ -42,20 +42,20 @@
 public class PTFPartition {
   protected static Logger LOG = LoggerFactory.getLogger(PTFPartition.class);
 
-  SerDe serDe;
+  AbstractSerDe serDe;
   StructObjectInspector inputOI;
   StructObjectInspector outputOI;
   private final PTFRowContainer<List<Object>> elems;
 
   protected PTFPartition(Configuration cfg,
-      SerDe serDe, StructObjectInspector inputOI,
+      AbstractSerDe serDe, StructObjectInspector inputOI,
       StructObjectInspector outputOI)
       throws HiveException {
     this(cfg, serDe, inputOI, outputOI, true);
   }
   
   protected PTFPartition(Configuration cfg,
-      SerDe serDe, StructObjectInspector inputOI,
+      AbstractSerDe serDe, StructObjectInspector inputOI,
       StructObjectInspector outputOI,
       boolean createElemContainer)
       throws HiveException {
@@ -76,7 +76,7 @@ public void reset() throws HiveException {
     elems.clearRows();
   }
 
-  public SerDe getSerDe() {
+  public AbstractSerDe getSerDe() {
     return serDe;
   }
 
@@ -239,7 +239,7 @@ public static interface PTFPartitionIterator<T> extends Iterator<T> {
   }
 
   public static PTFPartition create(Configuration cfg,
-      SerDe serDe,
+      AbstractSerDe serDe,
       StructObjectInspector inputOI,
       StructObjectInspector outputOI)
       throws HiveException {
@@ -247,7 +247,7 @@ public static PTFPartition create(Configuration cfg,
   }
   
   public static PTFRollingPartition createRolling(Configuration cfg,
-      SerDe serDe,
+      AbstractSerDe serDe,
       StructObjectInspector inputOI,
       StructObjectInspector outputOI,
       int precedingSpan,
@@ -256,7 +256,7 @@ public static PTFRollingPartition createRolling(Configuration cfg,
     return new PTFRollingPartition(cfg, serDe, inputOI, outputOI, precedingSpan, followingSpan);
   }
 
-  public static StructObjectInspector setupPartitionOutputOI(SerDe serDe,
+  public static StructObjectInspector setupPartitionOutputOI(AbstractSerDe serDe,
       StructObjectInspector tblFnOI) throws SerDeException {
     return (StructObjectInspector) ObjectInspectorUtils.getStandardObjectInspector(tblFnOI,
         ObjectInspectorCopyOption.WRITABLE);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFRollingPartition.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFRollingPartition.java
index ad1cf2451d..67b3255be6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFRollingPartition.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFRollingPartition.java
@@ -24,7 +24,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
@@ -70,7 +70,7 @@ public class PTFRollingPartition extends PTFPartition {
    */
   List<Object> currWindow;
 
-  protected PTFRollingPartition(Configuration cfg, SerDe serDe,
+  protected PTFRollingPartition(Configuration cfg, AbstractSerDe serDe,
       StructObjectInspector inputOI, StructObjectInspector outputOI,
       int startPos, int endPos) throws HiveException {
     super(cfg, serDe, inputOI, outputOI, false);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SkewJoinHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SkewJoinHandler.java
index 0ff6659862..7fad34f408 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SkewJoinHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SkewJoinHandler.java
@@ -36,7 +36,7 @@
 import org.apache.hadoop.hive.ql.plan.JoinDesc;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -83,7 +83,7 @@ public class SkewJoinHandler {
 
   private int skewKeyDefinition = -1;
   private Map<Byte, StructObjectInspector> skewKeysTableObjectInspector = null;
-  private Map<Byte, SerDe> tblSerializers = null;
+  private Map<Byte, AbstractSerDe> tblSerializers = null;
   private Map<Byte, TableDesc> tblDesc = null;
 
   private Map<Byte, Boolean> bigKeysExistingMap = null;
@@ -113,7 +113,7 @@ public void initiliaze(Configuration hconf) {
     skewKeysTableObjectInspector = new HashMap<Byte, StructObjectInspector>(
         numAliases);
     tblDesc = desc.getSkewKeysValuesTables();
-    tblSerializers = new HashMap<Byte, SerDe>(numAliases);
+    tblSerializers = new HashMap<Byte, AbstractSerDe>(numAliases);
     bigKeysExistingMap = new HashMap<Byte, Boolean>(numAliases);
     taskId = Utilities.getTaskId(hconf);
 
@@ -137,7 +137,7 @@ public void initiliaze(Configuration hconf) {
           .getStandardStructObjectInspector(keyColNames, skewTableKeyInspectors);
 
       try {
-        SerDe serializer = (SerDe) ReflectionUtils.newInstance(tblDesc.get(
+        AbstractSerDe serializer = (AbstractSerDe) ReflectionUtils.newInstance(tblDesc.get(
             alias).getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(serializer, null, tblDesc.get(alias).getProperties(), null);
         tblSerializers.put((byte) i, serializer);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java
index c491df3f5c..9b1af1bd38 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java
@@ -31,7 +31,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -69,7 +69,7 @@ public FlatRowContainer() {
   /** Called when loading the hashtable. */
   public void add(MapJoinObjectSerDeContext context,
       BytesWritable value) throws HiveException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     isAliasFilterSet = !context.hasFilterTag(); // has tag => need to set later
     if (rowLength == UNKNOWN) {
       try {
@@ -197,7 +197,7 @@ public List<Object> next() {
     }
   }
 
-  private void read(SerDe serde, Writable writable, int rowOffset) throws HiveException {
+  private void read(AbstractSerDe serde, Writable writable, int rowOffset) throws HiveException {
     try {
       ObjectInspectorUtils.copyStructToArray(
           serde.deserialize(writable), serde.getObjectInspector(),
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
index 573dc080e0..04e89e8e74 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
@@ -47,7 +47,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.WriteBuffers;
 import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
@@ -1166,7 +1166,7 @@ public int size() {
   @Override
   public void setSerde(MapJoinObjectSerDeContext keyCtx, MapJoinObjectSerDeContext valCtx)
       throws SerDeException {
-    SerDe keySerde = keyCtx.getSerDe(), valSerde = valCtx.getSerDe();
+    AbstractSerDe keySerde = keyCtx.getSerDe(), valSerde = valCtx.getSerDe();
 
     if (writeHelper == null) {
       LOG.info("Initializing container with " + keySerde.getClass().getName() + " and "
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
index a8aa71a62c..c86e5f541e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
@@ -35,7 +35,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.ByteStream.RandomAccessOutput;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.WriteBuffers;
 import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
@@ -154,14 +154,14 @@ public static interface KeyValueHelper extends BytesBytesMultiHashMap.KvSource {
   }
 
   private static class KeyValueWriter implements KeyValueHelper {
-    private final SerDe keySerDe, valSerDe;
+    private final AbstractSerDe keySerDe, valSerDe;
     private final StructObjectInspector keySoi, valSoi;
     private final List<ObjectInspector> keyOis, valOis;
     private final Object[] keyObjs, valObjs;
     private final boolean hasFilterTag;
 
     public KeyValueWriter(
-        SerDe keySerDe, SerDe valSerDe, boolean hasFilterTag) throws SerDeException {
+        AbstractSerDe keySerDe, AbstractSerDe valSerDe, boolean hasFilterTag) throws SerDeException {
       this.keySerDe = keySerDe;
       this.valSerDe = valSerDe;
       keySoi = (StructObjectInspector)keySerDe.getObjectInspector();
@@ -221,10 +221,10 @@ public int getHashFromKey() throws SerDeException {
   static class LazyBinaryKvWriter implements KeyValueHelper {
     private final LazyBinaryStruct.SingleFieldGetter filterGetter;
     private Writable key, value;
-    private final SerDe keySerDe;
+    private final AbstractSerDe keySerDe;
     private Boolean hasTag = null; // sanity check - we should not receive keys with tags
 
-    public LazyBinaryKvWriter(SerDe keySerDe, LazyBinaryStructObjectInspector valSoi,
+    public LazyBinaryKvWriter(AbstractSerDe keySerDe, LazyBinaryStructObjectInspector valSoi,
         boolean hasFilterTag) throws SerDeException {
       this.keySerDe = keySerDe;
       if (hasFilterTag) {
@@ -366,7 +366,7 @@ public int getHashFromKey() throws SerDeException {
   @Override
   public void setSerde(MapJoinObjectSerDeContext keyContext, MapJoinObjectSerDeContext valueContext)
       throws SerDeException {
-    SerDe keySerde = keyContext.getSerDe(), valSerde = valueContext.getSerDe();
+    AbstractSerDe keySerde = keyContext.getSerDe(), valSerde = valueContext.getSerDe();
     if (writeHelper == null) {
       LOG.info("Initializing container with " + keySerde.getClass().getName() + " and "
           + valSerde.getClass().getName());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinEagerRowContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinEagerRowContainer.java
index eaeae3120c..bb3c4befc3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinEagerRowContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinEagerRowContainer.java
@@ -26,7 +26,7 @@
 import java.util.ConcurrentModificationException;
 import java.util.List;
 
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -134,7 +134,7 @@ public void read(MapJoinObjectSerDeContext context, ObjectInputStream in, Writab
 
   @SuppressWarnings("unchecked")
   public void read(MapJoinObjectSerDeContext context, Writable currentValue) throws SerDeException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     List<Object> value = (List<Object>)ObjectInspectorUtils.copyToStandardObject(serde.deserialize(currentValue),
         serde.getObjectInspector(), ObjectInspectorCopyOption.WRITABLE);
     if(value == null) {
@@ -151,7 +151,7 @@ public void read(MapJoinObjectSerDeContext context, Writable currentValue) throw
   @Override
   public void write(MapJoinObjectSerDeContext context, ObjectOutputStream out)
   throws IOException, SerDeException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     ObjectInspector valueObjectInspector = context.getStandardOI();
     long numRows = rowCount();
     long numRowsWritten = 0L;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
index 9f27f5635a..1cd90212be 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
@@ -29,7 +29,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe;
@@ -56,7 +56,7 @@ public abstract void write(MapJoinObjectSerDeContext context, ObjectOutputStream
   @SuppressWarnings("deprecation")
   public static MapJoinKey read(Output output, MapJoinObjectSerDeContext context,
       Writable writable) throws SerDeException, HiveException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     Object obj = serde.deserialize(writable);
     MapJoinKeyObject result = new MapJoinKeyObject();
     result.read(serde.getObjectInspector(), obj);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
index 7592f9e3be..ad7bd5d4e8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -119,7 +119,7 @@ protected void read(ObjectInspector oi, Object obj) throws SerDeException {
   @Override
   public void write(MapJoinObjectSerDeContext context, ObjectOutputStream out)
       throws IOException, SerDeException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     ObjectInspector objectInspector = context.getStandardOI();
     Writable container = serde.serialize(key, objectInspector);
     container.write(out);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectSerDeContext.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectSerDeContext.java
index f47d481a73..a112a68e8a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectSerDeContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectSerDeContext.java
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.ql.exec.persistence;
 
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -26,10 +26,10 @@
 @SuppressWarnings("deprecation")
 public class MapJoinObjectSerDeContext {
   private final ObjectInspector standardOI;
-  private final SerDe serde;
+  private final AbstractSerDe serde;
   private final boolean hasFilter;
 
-  public MapJoinObjectSerDeContext(SerDe serde, boolean hasFilter)
+  public MapJoinObjectSerDeContext(AbstractSerDe serde, boolean hasFilter)
       throws SerDeException {
     this.serde = serde;
     this.hasFilter = hasFilter;
@@ -47,7 +47,7 @@ public ObjectInspector getStandardOI() {
   /**
    * @return the serde
    */
-  public SerDe getSerDe() {
+  public AbstractSerDe getSerDe() {
     return serde;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainerSerDe.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainerSerDe.java
index eb48dd758f..83a4612b08 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainerSerDe.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainerSerDe.java
@@ -35,7 +35,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.Writable;
@@ -66,8 +66,8 @@ public MapJoinObjectSerDeContext getValueContext() {
    */
   public MapJoinPersistableTableContainer load(ObjectInputStream in)
       throws HiveException {
-    SerDe keySerDe = keyContext.getSerDe();
-    SerDe valueSerDe = valueContext.getSerDe();
+    AbstractSerDe keySerDe = keyContext.getSerDe();
+    AbstractSerDe valueSerDe = valueContext.getSerDe();
     MapJoinPersistableTableContainer tableContainer;
     try {
       String name = in.readUTF();
@@ -120,8 +120,8 @@ public MapJoinTableContainer load(
         return getDefaultEmptyContainer(keyContext, valueContext);
       }
 
-      SerDe keySerDe = keyContext.getSerDe();
-      SerDe valueSerDe = valueContext.getSerDe();
+      AbstractSerDe keySerDe = keyContext.getSerDe();
+      AbstractSerDe valueSerDe = valueContext.getSerDe();
       Writable keyContainer = keySerDe.getSerializedClass().newInstance();
       Writable valueContainer = valueSerDe.getSerializedClass().newInstance();
 
@@ -225,8 +225,8 @@ public MapJoinTableContainer loadFastContainer(MapJoinDesc mapJoinDesc,
 
         FileStatus[] fileStatuses = fs.listStatus(folder);
         if (fileStatuses != null && fileStatuses.length > 0) {
-          SerDe keySerDe = keyContext.getSerDe();
-          SerDe valueSerDe = valueContext.getSerDe();
+          AbstractSerDe keySerDe = keyContext.getSerDe();
+          AbstractSerDe valueSerDe = valueContext.getSerDe();
           Writable key = keySerDe.getSerializedClass().newInstance();
           Writable value = valueSerDe.getSerializedClass().newInstance();
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java
index e928719c33..c8a1a0dc66 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java
@@ -39,7 +39,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
@@ -97,7 +97,7 @@ public class RowContainer<ROW extends List<Object>>
   private int itrCursor; // iterator cursor in the currBlock
   private int readBlockSize; // size of current read block
   private int addCursor; // append cursor in the lastBlock
-  private SerDe serde; // serialization/deserialization for the row
+  private AbstractSerDe serde; // serialization/deserialization for the row
   private ObjectInspector standardOI; // object inspector for the row
 
   private List<Object> keyObject;
@@ -160,7 +160,7 @@ private JobConf getLocalFSJobConfClone(Configuration jc) {
   }
 
 
-  public void setSerDe(SerDe sd, ObjectInspector oi) {
+  public void setSerDe(AbstractSerDe sd, ObjectInspector oi) {
     this.serde = sd;
     this.standardOI = oi;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
index 7e41b7a36b..d7264c2c8d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
@@ -41,7 +41,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
@@ -79,7 +79,7 @@ public class ReduceRecordSource implements RecordSource {
 
   // Input value serde needs to be an array to support different SerDe
   // for different tags
-  private SerDe inputValueDeserializer;
+  private AbstractSerDe inputValueDeserializer;
 
   private TableDesc keyTableDesc;
   private TableDesc valueTableDesc;
@@ -151,7 +151,7 @@ void init(JobConf jconf, Operator<?> reducer, boolean vectorized, TableDesc keyT
 
       // We should initialize the SerDe with the TypeInfo when available.
       this.valueTableDesc = valueTableDesc;
-      inputValueDeserializer = (SerDe) ReflectionUtils.newInstance(
+      inputValueDeserializer = (AbstractSerDe) ReflectionUtils.newInstance(
           valueTableDesc.getDeserializerClass(), null);
       SerDeUtils.initializeSerDe(inputValueDeserializer, null,
           valueTableDesc.getProperties(), null);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedSerde.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedSerde.java
index bff6200c3e..9675cc8145 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedSerde.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedSerde.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.Writable;
@@ -25,11 +26,10 @@
 /**
  * Serdes that support vectorized {@link VectorizedRowBatch} must implement this interface.
  */
-public interface VectorizedSerde {
+public abstract class VectorizedSerde extends AbstractSerDe {
+  public abstract Writable serializeVector(
+      VectorizedRowBatch vrg, ObjectInspector objInspector) throws SerDeException;
 
-  Writable serializeVector(VectorizedRowBatch vrg, ObjectInspector objInspector)
-      throws SerDeException;
-
-  void deserializeVector(Object rowBlob, int rowsInBlob, VectorizedRowBatch reuseBatch)
-      throws SerDeException;
+  public abstract void deserializeVector(
+      Object rowBlob, int rowsInBlob, VectorizedRowBatch reuseBatch) throws SerDeException;
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java
index 59876e2dec..3ec9105c81 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedSerde;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -45,7 +45,7 @@
  * It transparently passes the object to/from the ORC file reader/writer.
  */
 @SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS, serdeConstants.LIST_COLUMN_TYPES, OrcSerde.COMPRESSION})
-public class OrcSerde implements SerDe, VectorizedSerde {
+public class OrcSerde extends VectorizedSerde {
 
   private static final Logger LOG = LoggerFactory.getLogger(OrcSerde.class);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/DefaultStorageHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/DefaultStorageHandler.java
index e183bf3c53..82b78b879c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/DefaultStorageHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/DefaultStorageHandler.java
@@ -23,7 +23,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.HiveMetaHook;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
@@ -54,7 +54,7 @@ public Class<? extends OutputFormat> getOutputFormatClass() {
   }
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return LazySimpleSerDe.class;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
index 1eec32cbc7..5975d0cf07 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
@@ -23,7 +23,7 @@
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.hive.metastore.HiveMetaHook;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
@@ -59,9 +59,9 @@ public interface HiveStorageHandler extends Configurable {
   public Class<? extends OutputFormat> getOutputFormatClass();
 
   /**
-   * @return Class providing an implementation of {@link SerDe}
+   * @return Class providing an implementation of {@link AbstractSerDe}
    */
-  public Class<? extends SerDe> getSerDeClass();
+  public Class<? extends AbstractSerDe> getSerDeClass();
 
   /**
    * @return metadata hook implementation, or null if this
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java
index f32d02b952..519f10d4a3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java
@@ -81,7 +81,7 @@
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator;
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver;
 import org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe;
@@ -643,7 +643,7 @@ private ShapeDetails setupShape(StructObjectInspector OI,
       List<String> columnNames,
       RowResolver rr) throws SemanticException {
     Map<String, String> serdePropsMap = new LinkedHashMap<String, String>();
-    SerDe serde = null;
+    AbstractSerDe serde = null;
     ShapeDetails shp = new ShapeDetails();
 
     try {
@@ -806,13 +806,13 @@ private ObjectInspector initExprNodeEvaluator(ExprNodeEvaluator exprEval,
    * OI & Serde helper methods
    */
 
-  protected static SerDe createLazyBinarySerDe(Configuration cfg,
+  protected static AbstractSerDe createLazyBinarySerDe(Configuration cfg,
       StructObjectInspector oi, Map<String, String> serdePropsMap) throws SerDeException {
     serdePropsMap = serdePropsMap == null ? new LinkedHashMap<String, String>() : serdePropsMap;
 
     PTFDeserializer.addOIPropertiestoSerDePropsMap(oi, serdePropsMap);
 
-    SerDe serDe = new LazyBinarySerDe();
+    AbstractSerDe serDe = new LazyBinarySerDe();
     Properties p = new Properties();
     p.setProperty(org.apache.hadoop.hive.serde.serdeConstants.LIST_COLUMNS,
         serdePropsMap.get(org.apache.hadoop.hive.serde.serdeConstants.LIST_COLUMNS));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java
index cfddb220b9..a793fea4cf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java
@@ -49,7 +49,7 @@
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator;
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver;
 import org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -265,8 +265,8 @@ protected void initialize(ShapeDetails shp, StructObjectInspector OI) throws Hiv
       serDeProps.setProperty(serdeName, serdePropsMap.get(serdeName));
     }
     try {
-      SerDe serDe =  ReflectionUtils.newInstance(hConf.getClassByName(serdeClassName).
-          asSubclass(SerDe.class), hConf);
+      AbstractSerDe serDe =  ReflectionUtils.newInstance(hConf.getClassByName(serdeClassName).
+          asSubclass(AbstractSerDe.class), hConf);
       SerDeUtils.initializeSerDe(serDe, hConf, serDeProps, null);
       shp.setSerde(serDe);
       StructObjectInspector outOI = PTFPartition.setupPartitionOutputOI(serDe, OI);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ptf/ShapeDetails.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ptf/ShapeDetails.java
index bc2ee831bc..7e3cebd2d2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ptf/ShapeDetails.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ptf/ShapeDetails.java
@@ -24,7 +24,7 @@
 import org.apache.hadoop.hive.ql.exec.PTFUtils;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.TypeCheckCtx;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 
 public class ShapeDetails {
@@ -32,7 +32,7 @@ public class ShapeDetails {
   Map<String, String> serdeProps;
   List<String> columnNames;
   transient StructObjectInspector OI;
-  transient SerDe serde;
+  transient AbstractSerDe serde;
   transient RowResolver rr;
   transient TypeCheckCtx typeCheckCtx;
 
@@ -68,11 +68,11 @@ public void setOI(StructObjectInspector oI) {
     OI = oI;
   }
 
-  public SerDe getSerde() {
+  public AbstractSerDe getSerde() {
     return serde;
   }
 
-  public void setSerde(SerDe serde) {
+  public void setSerde(AbstractSerDe serde) {
     this.serde = serde;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java
index e9f8ff959e..5cc84a04ef 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java
@@ -56,7 +56,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AggregationBuffer;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer;
 import org.apache.hadoop.hive.ql.udf.generic.ISupportStreamingModeForWindowing;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -1618,7 +1618,7 @@ class StreamingState {
     StreamingState(Configuration cfg, StructObjectInspector inputOI,
         boolean isMapSide, WindowTableFunctionDef tabDef, int precedingSpan,
         int followingSpan) throws HiveException {
-      SerDe serde = isMapSide ? tabDef.getInput().getOutputShape().getSerde()
+      AbstractSerDe serde = isMapSide ? tabDef.getInput().getOutputShape().getSerde()
           : tabDef.getRawInputShape().getSerde();
       StructObjectInspector outputOI = isMapSide ? tabDef.getInput()
           .getOutputShape().getOI() : tabDef.getRawInputShape().getOI();
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestPTFRowContainer.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestPTFRowContainer.java
index 0611072569..e5a5bff48d 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestPTFRowContainer.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestPTFRowContainer.java
@@ -27,7 +27,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -47,7 +47,7 @@ public class TestPTFRowContainer {
   private static final String COL_NAMES = "x,y,z,a,b,v";
   private static final String COL_TYPES = "int,string,double,int,string,string";
 
-  static SerDe serDe;
+  static AbstractSerDe serDe;
   static Configuration cfg;
 
   @BeforeClass
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
index 2c1bb6fe2f..4aac90a53c 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
@@ -89,7 +89,7 @@
 import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -1682,7 +1682,7 @@ public void testInOutFormat() throws Exception {
           ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,
               ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
     }
-    SerDe serde = new OrcSerde();
+    AbstractSerDe serde = new OrcSerde();
     HiveOutputFormat<?, ?> outFormat = new OrcOutputFormat();
     org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter writer =
         outFormat.getHiveRecordWriter(conf, testFilePath, MyRow.class, true,
@@ -1816,7 +1816,7 @@ public void testMROutput() throws Exception {
           ObjectInspectorFactory.getReflectionObjectInspector(NestedRow.class,
               ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
     }
-    SerDe serde = new OrcSerde();
+    AbstractSerDe serde = new OrcSerde();
     OutputFormat<?, ?> outFormat = new OrcOutputFormat();
     RecordWriter writer =
         outFormat.getRecordWriter(fs, conf, testFilePath.toString(),
@@ -1875,7 +1875,7 @@ public void testEmptyFile() throws Exception {
         outFormat.getHiveRecordWriter(conf, testFilePath, MyRow.class, true,
             properties, Reporter.NULL);
     writer.close(true);
-    SerDe serde = new OrcSerde();
+    AbstractSerDe serde = new OrcSerde();
     SerDeUtils.initializeSerDe(serde, conf, properties, null);
     InputFormat<?,?> in = new OrcInputFormat();
     FileInputFormat.setInputPaths(conf, testFilePath.toString());
@@ -1941,7 +1941,7 @@ public void testDefaultTypes() throws Exception {
           ObjectInspectorFactory.getReflectionObjectInspector(StringRow.class,
               ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
     }
-    SerDe serde = new OrcSerde();
+    AbstractSerDe serde = new OrcSerde();
     HiveOutputFormat<?, ?> outFormat = new OrcOutputFormat();
     org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter writer =
         outFormat.getHiveRecordWriter(conf, testFilePath, StringRow.class,
@@ -2495,7 +2495,7 @@ public void testSplitElimination() throws Exception {
           ObjectInspectorFactory.getReflectionObjectInspector(NestedRow.class,
               ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
     }
-    SerDe serde = new OrcSerde();
+    AbstractSerDe serde = new OrcSerde();
     OutputFormat<?, ?> outFormat = new OrcOutputFormat();
     conf.setInt("mapred.max.split.size", 50);
     RecordWriter writer =
@@ -2529,7 +2529,7 @@ public void testSplitElimination() throws Exception {
   public void testSplitEliminationNullStats() throws Exception {
     Properties properties = new Properties();
     StructObjectInspector inspector = createSoi();
-    SerDe serde = new OrcSerde();
+    AbstractSerDe serde = new OrcSerde();
     OutputFormat<?, ?> outFormat = new OrcOutputFormat();
     conf.setInt("mapred.max.split.size", 50);
     RecordWriter writer =
@@ -3631,7 +3631,7 @@ public void testRowNumberUniquenessInDifferentSplits() throws Exception {
     conf.setLong(HiveConf.ConfVars.HIVE_ORC_DEFAULT_STRIPE_SIZE.varname, newStripeSize);
     conf.setLong(HiveConf.ConfVars.MAPREDMAXSPLITSIZE.varname, newMaxSplitSize);
 
-    SerDe serde = new OrcSerde();
+    AbstractSerDe serde = new OrcSerde();
     HiveOutputFormat<?, ?> outFormat = new OrcOutputFormat();
     org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter writer =
       outFormat.getHiveRecordWriter(conf, testFilePath, MyRow.class, true,
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/AbstractSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/AbstractSerDe.java
index 9434e916d3..049b35dc4f 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/AbstractSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/AbstractSerDe.java
@@ -32,7 +32,7 @@
  * new methods can be added in the underlying interface, SerDe, and only implementations
  * that need those methods overwrite it.
  */
-public abstract class AbstractSerDe implements SerDe {
+public abstract class AbstractSerDe implements Deserializer, Serializer {
 
   protected String configErrors;
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/DefaultFetchFormatter.java b/serde/src/java/org/apache/hadoop/hive/serde2/DefaultFetchFormatter.java
index 3038037caa..a21509218f 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/DefaultFetchFormatter.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/DefaultFetchFormatter.java
@@ -35,24 +35,24 @@
  */
 public class DefaultFetchFormatter<T> implements FetchFormatter<String> {
 
-  private SerDe mSerde;
+  private AbstractSerDe mSerde;
 
   @Override
   public void initialize(Configuration hconf, Properties props) throws SerDeException {
     mSerde = initializeSerde(hconf, props);
   }
 
-  private SerDe initializeSerde(Configuration conf, Properties props) throws SerDeException {
+  private AbstractSerDe initializeSerde(Configuration conf, Properties props) throws SerDeException {
     String serdeName = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEFETCHOUTPUTSERDE);
-    Class<? extends SerDe> serdeClass;
+    Class<? extends AbstractSerDe> serdeClass;
     try {
       serdeClass =
-          Class.forName(serdeName, true, JavaUtils.getClassLoader()).asSubclass(SerDe.class);
+          Class.forName(serdeName, true, JavaUtils.getClassLoader()).asSubclass(AbstractSerDe.class);
     } catch (ClassNotFoundException e) {
       throw new SerDeException(e);
     }
     // cast only needed for Hadoop 0.17 compatibility
-    SerDe serde = ReflectionUtil.newInstance(serdeClass, null);
+    AbstractSerDe serde = ReflectionUtil.newInstance(serdeClass, null);
     Properties serdeProps = new Properties();
     if (serde instanceof DelimitedJSONSerDe) {
       serdeProps.put(SERIALIZATION_FORMAT, props.getProperty(SERIALIZATION_FORMAT));
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/Deserializer.java b/serde/src/java/org/apache/hadoop/hive/serde2/Deserializer.java
index df27db2e85..a1d3dd8766 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/Deserializer.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/Deserializer.java
@@ -31,10 +31,9 @@
  * HiveDeserializer also provides the ObjectInspector which can be used to
  * inspect the internal structure of the object (that is returned by deserialize
  * function).
- * All deserializers should extend the abstract class AbstractDeserializer, and eventually
- * Deserializer interface should be removed
+ * All deserializers should extend the abstract class AbstractDeserializer.
+ * The interface is necessary for SerDes to be able to implement both Serializer and Deserializer.
  */
-@Deprecated
 public interface Deserializer {
 
   /**
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/SerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/SerDe.java
deleted file mode 100644
index db15ce5d5d..0000000000
--- a/serde/src/java/org/apache/hadoop/hive/serde2/SerDe.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.serde2;
-
-/**
- * A union of HiveDeserializer and HiveSerializer interface.
- *
- * If a developer wants his hive table to be read-only, then he just want to
- * return
- *
- * both readable and writable, then
- *
- * All serdes should extend the abstract class AbstractSerDe, and eventually SerDe interface
- * should be removed
- */
-@Deprecated
-public interface SerDe extends Deserializer, Serializer {
-
-}
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/Serializer.java b/serde/src/java/org/apache/hadoop/hive/serde2/Serializer.java
index b39db892ba..3f07a86e31 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/Serializer.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/Serializer.java
@@ -28,10 +28,9 @@
  * HiveSerializer is used to serialize data to a Hadoop Writable object. The
  * serialize In addition to the interface below, all implementations are assume
  * to have a ctor that takes a single 'Table' object as argument.
- * All serializers should extend the abstract class AbstractSerializer, and eventually
- * Serializer interface should be removed
+ * All serializers should extend the abstract class AbstractSerializer.
+ * The interface is necessary for SerDes to be able to implement both Serializer and Deserializer.
  */
-@Deprecated
 public interface Serializer {
 
   /**
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
index e32d9a6d1a..36beaee932 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
@@ -88,7 +88,7 @@ public ColumnarSerDe() throws SerDeException {
   /**
    * Initialize the SerDe given the parameters.
    *
-   * @see SerDe#initialize(Configuration, Properties)
+   * @see AbstractSerDe#initialize(Configuration, Properties)
    */
   @Override
   public void initialize(Configuration conf, Properties tbl) throws SerDeException {
@@ -123,7 +123,7 @@ public void initialize(Configuration conf, Properties tbl) throws SerDeException
    * @param objInspector
    *          The ObjectInspector for the row object
    * @return The serialized Writable object
-   * @see SerDe#serialize(Object, ObjectInspector)
+   * @see AbstractSerDe#serialize(Object, ObjectInspector)
    */
   @Override
   public Writable serialize(Object obj, ObjectInspector objInspector) throws SerDeException {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
index ac2d39fe74..17ecff15bb 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
@@ -105,7 +105,7 @@ public LazySimpleSerDe() throws SerDeException {
    * ","-separated column names columns.types: ",", ":", or ";"-separated column
    * types
    *
-   * @see SerDe#initialize(Configuration, Properties)
+   * @see AbstractSerDe#initialize(Configuration, Properties)
    */
   @Override
   public void initialize(Configuration job, Properties tbl)
@@ -141,7 +141,7 @@ public void initialize(Configuration job, Properties tbl)
    * @param field
    *          the Writable that contains the data
    * @return The deserialized row Object.
-   * @see SerDe#deserialize(Writable)
+   * @see AbstractSerDe#deserialize(Writable)
    */
   @Override
   public Object doDeserialize(Writable field) throws SerDeException {
@@ -167,7 +167,7 @@ public ObjectInspector getObjectInspector() throws SerDeException {
   /**
    * Returns the Writable Class after serialization.
    *
-   * @see SerDe#getSerializedClass()
+   * @see AbstractSerDe#getSerializedClass()
    */
   @Override
   public Class<? extends Writable> getSerializedClass() {
@@ -186,7 +186,7 @@ public Class<? extends Writable> getSerializedClass() {
    *          The ObjectInspector for the row object
    * @return The serialized Writable object
    * @throws IOException
-   * @see SerDe#serialize(Object, ObjectInspector)
+   * @see AbstractSerDe#serialize(Object, ObjectInspector)
    */
   @Override
   public Writable doSerialize(Object obj, ObjectInspector objInspector)
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java b/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
index 036be4e96d..646a29dd77 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
@@ -137,7 +137,7 @@ public void testLazyBinarySerDe() throws Throwable {
     }
   }
 
-  private void deserializeAndSerializeLazyBinary(SerDe serDe, Object[] rows, ObjectInspector rowOI)
+  private void deserializeAndSerializeLazyBinary(AbstractSerDe serDe, Object[] rows, ObjectInspector rowOI)
       throws Throwable {
 
     BytesWritable bytes[] = new BytesWritable[rows.length];
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
index f1eeb2dbe8..1c84fe642b 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
@@ -24,7 +24,7 @@
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerdeRandomRowSource;
 import org.apache.hadoop.hive.serde2.VerifyFast;
 import org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead;
@@ -45,8 +45,8 @@ public class TestBinarySortableFast extends TestCase {
   private void testBinarySortableFast(
           SerdeRandomRowSource source, Object[][] rows,
           boolean[] columnSortOrderIsDesc, byte[] columnNullMarker, byte[] columnNotNullMarker,
-          SerDe serde, StructObjectInspector rowOI,
-          SerDe serde_fewer, StructObjectInspector writeRowOI,
+          AbstractSerDe serde, StructObjectInspector rowOI,
+          AbstractSerDe serde_fewer, StructObjectInspector writeRowOI,
           boolean ascending, PrimitiveTypeInfo[] primitiveTypeInfos,
           boolean useIncludeColumns, boolean doWriteFewerColumns, Random r) throws Throwable {
 
@@ -311,9 +311,9 @@ private void testBinarySortableFastCase(int caseNum, boolean doNonRandomFill, Ra
     order = StringUtils.leftPad("", columnCount, '+');
     String nullOrder;
     nullOrder = StringUtils.leftPad("", columnCount, 'a');
-    SerDe serde_ascending = TestBinarySortableSerDe.getSerDe(fieldNames, fieldTypes, order, nullOrder);
+    AbstractSerDe serde_ascending = TestBinarySortableSerDe.getSerDe(fieldNames, fieldTypes, order, nullOrder);
 
-    SerDe serde_ascending_fewer = null;
+    AbstractSerDe serde_ascending_fewer = null;
     if (doWriteFewerColumns) {
       String partialFieldNames = ObjectInspectorUtils.getFieldNames(writeRowStructObjectInspector);
       String partialFieldTypes = ObjectInspectorUtils.getFieldTypes(writeRowStructObjectInspector);
@@ -323,9 +323,9 @@ private void testBinarySortableFastCase(int caseNum, boolean doNonRandomFill, Ra
 
     order = StringUtils.leftPad("", columnCount, '-');
     nullOrder = StringUtils.leftPad("", columnCount, 'z');
-    SerDe serde_descending = TestBinarySortableSerDe.getSerDe(fieldNames, fieldTypes, order, nullOrder);
+    AbstractSerDe serde_descending = TestBinarySortableSerDe.getSerDe(fieldNames, fieldTypes, order, nullOrder);
 
-    SerDe serde_descending_fewer = null;
+    AbstractSerDe serde_descending_fewer = null;
     if (doWriteFewerColumns) {
       String partialFieldNames = ObjectInspectorUtils.getFieldNames(writeRowStructObjectInspector);
       String partialFieldTypes = ObjectInspectorUtils.getFieldTypes(writeRowStructObjectInspector);
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java
index 935313b7f2..6db2093f64 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java
@@ -24,7 +24,7 @@
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestPrimitiveClass.ExtraTypeInfo;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -62,7 +62,7 @@ public static String hexString(BytesWritable bytes) {
     return sb.toString();
   }
 
-  public static SerDe getSerDe(String fieldNames, String fieldTypes, String order, String nullOrder)
+  public static AbstractSerDe getSerDe(String fieldNames, String fieldTypes, String order, String nullOrder)
       throws Throwable {
     Properties schema = new Properties();
     schema.setProperty(serdeConstants.LIST_COLUMNS, fieldNames);
@@ -76,7 +76,7 @@ public static SerDe getSerDe(String fieldNames, String fieldTypes, String order,
   }
 
   private void testBinarySortableSerDe(Object[] rows, ObjectInspector rowOI,
-      SerDe serde, boolean ascending) throws Throwable {
+      AbstractSerDe serde, boolean ascending) throws Throwable {
 
     ObjectInspector serdeOI = serde.getObjectInspector();
 
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java
index a1828c90cc..e62a80a1d6 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java
@@ -23,7 +23,7 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerdeRandomRowSource;
 import org.apache.hadoop.hive.serde2.VerifyFast;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestClass;
@@ -39,8 +39,8 @@ public class TestLazyBinaryFast extends TestCase {
 
   private void testLazyBinaryFast(
       SerdeRandomRowSource source, Object[][] rows,
-      SerDe serde, StructObjectInspector rowOI,
-      SerDe serde_fewer, StructObjectInspector writeRowOI,
+      AbstractSerDe serde, StructObjectInspector rowOI,
+      AbstractSerDe serde_fewer, StructObjectInspector writeRowOI,
       PrimitiveTypeInfo[] primitiveTypeInfos,
       boolean useIncludeColumns, boolean doWriteFewerColumns, Random r) throws Throwable {
 
@@ -242,9 +242,9 @@ public void testLazyBinaryFastCase(int caseNum, boolean doNonRandomFill, Random
     String fieldNames = ObjectInspectorUtils.getFieldNames(rowStructObjectInspector);
     String fieldTypes = ObjectInspectorUtils.getFieldTypes(rowStructObjectInspector);
 
-    SerDe serde = TestLazyBinarySerDe.getSerDe(fieldNames, fieldTypes);
+    AbstractSerDe serde = TestLazyBinarySerDe.getSerDe(fieldNames, fieldTypes);
 
-    SerDe serde_fewer = null;
+    AbstractSerDe serde_fewer = null;
     if (doWriteFewerColumns) {
       String partialFieldNames = ObjectInspectorUtils.getFieldNames(writeRowStructObjectInspector);
       String partialFieldTypes = ObjectInspectorUtils.getFieldTypes(writeRowStructObjectInspector);
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
index e54db9517a..0cd573642d 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
@@ -32,7 +32,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestClass;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
@@ -93,7 +93,7 @@ static List<MyTestInnerStruct> getRandStructArray(Random r) {
    * @return the initialized LazyBinarySerDe
    * @throws Throwable
    */
-  protected static SerDe getSerDe(String fieldNames, String fieldTypes) throws Throwable {
+  protected static AbstractSerDe getSerDe(String fieldNames, String fieldTypes) throws Throwable {
     Properties schema = new Properties();
     schema.setProperty(serdeConstants.LIST_COLUMNS, fieldNames);
     schema.setProperty(serdeConstants.LIST_COLUMN_TYPES, fieldTypes);
@@ -115,7 +115,7 @@ protected static SerDe getSerDe(String fieldNames, String fieldTypes) throws Thr
    * @throws Throwable
    */
   private void testLazyBinarySerDe(Object[] rows, ObjectInspector rowOI,
-      SerDe serde) throws Throwable {
+      AbstractSerDe serde) throws Throwable {
 
     ObjectInspector serdeOI = serde.getObjectInspector();
 
@@ -183,7 +183,7 @@ private void testShorterSchemaDeserialization(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames1 = ObjectInspectorUtils.getFieldNames(rowOI1);
     String fieldTypes1 = ObjectInspectorUtils.getFieldTypes(rowOI1);
-    SerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
+    AbstractSerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
     serde1.getObjectInspector();
 
     StructObjectInspector rowOI2 = (StructObjectInspector) ObjectInspectorFactory
@@ -191,7 +191,7 @@ private void testShorterSchemaDeserialization(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames2 = ObjectInspectorUtils.getFieldNames(rowOI2);
     String fieldTypes2 = ObjectInspectorUtils.getFieldTypes(rowOI2);
-    SerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
+    AbstractSerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
     ObjectInspector serdeOI2 = serde2.getObjectInspector();
 
     int num = 100;
@@ -226,7 +226,7 @@ private void testShorterSchemaDeserialization1(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames1 = ObjectInspectorUtils.getFieldNames(rowOI1);
     String fieldTypes1 = ObjectInspectorUtils.getFieldTypes(rowOI1);
-    SerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
+    AbstractSerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
     serde1.getObjectInspector();
 
     StructObjectInspector rowOI2 = (StructObjectInspector) ObjectInspectorFactory
@@ -234,7 +234,7 @@ private void testShorterSchemaDeserialization1(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames2 = ObjectInspectorUtils.getFieldNames(rowOI2);
     String fieldTypes2 = ObjectInspectorUtils.getFieldTypes(rowOI2);
-    SerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
+    AbstractSerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
     ObjectInspector serdeOI2 = serde2.getObjectInspector();
 
     int num = 100;
@@ -269,7 +269,7 @@ void testLongerSchemaDeserialization(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames1 = ObjectInspectorUtils.getFieldNames(rowOI1);
     String fieldTypes1 = ObjectInspectorUtils.getFieldTypes(rowOI1);
-    SerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
+    AbstractSerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
     serde1.getObjectInspector();
 
     StructObjectInspector rowOI2 = (StructObjectInspector) ObjectInspectorFactory
@@ -277,7 +277,7 @@ void testLongerSchemaDeserialization(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames2 = ObjectInspectorUtils.getFieldNames(rowOI2);
     String fieldTypes2 = ObjectInspectorUtils.getFieldTypes(rowOI2);
-    SerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
+    AbstractSerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
     ObjectInspector serdeOI2 = serde2.getObjectInspector();
 
     int num = 100;
@@ -313,7 +313,7 @@ void testLongerSchemaDeserialization1(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames1 = ObjectInspectorUtils.getFieldNames(rowOI1);
     String fieldTypes1 = ObjectInspectorUtils.getFieldTypes(rowOI1);
-    SerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
+    AbstractSerDe serde1 = getSerDe(fieldNames1, fieldTypes1);
     serde1.getObjectInspector();
 
     StructObjectInspector rowOI2 = (StructObjectInspector) ObjectInspectorFactory
@@ -321,7 +321,7 @@ void testLongerSchemaDeserialization1(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames2 = ObjectInspectorUtils.getFieldNames(rowOI2);
     String fieldTypes2 = ObjectInspectorUtils.getFieldTypes(rowOI2);
-    SerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
+    AbstractSerDe serde2 = getSerDe(fieldNames2, fieldTypes2);
     ObjectInspector serdeOI2 = serde2.getObjectInspector();
 
     int num = 100;
@@ -351,7 +351,7 @@ void testLazyBinaryMap(Random r) throws Throwable {
         ObjectInspectorOptions.JAVA);
     String fieldNames = ObjectInspectorUtils.getFieldNames(rowOI);
     String fieldTypes = ObjectInspectorUtils.getFieldTypes(rowOI);
-    SerDe serde = getSerDe(fieldNames, fieldTypes);
+    AbstractSerDe serde = getSerDe(fieldNames, fieldTypes);
     ObjectInspector serdeOI = serde.getObjectInspector();
 
     StructObjectInspector soi1 = (StructObjectInspector) serdeOI;
diff --git a/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java b/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
index abdf8cd2f7..ba02c9cb8a 100644
--- a/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
+++ b/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
@@ -60,7 +60,7 @@
 import org.apache.hadoop.hive.ql.session.OperationLog;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
@@ -93,7 +93,7 @@ public class SQLOperation extends ExecuteStatementOperation {
   private CommandProcessorResponse response;
   private TableSchema resultSchema = null;
   private Schema mResultSchema = null;
-  private SerDe serde = null;
+  private AbstractSerDe serde = null;
   private boolean fetchStarted = false;
   private volatile MetricsScope currentSQLStateScope;
   // Display for WebUI.
@@ -575,7 +575,7 @@ private RowSet decodeFromString(List<Object> rows, RowSet rowSet)
     return rowSet;
   }
 
-  private SerDe getSerDe() throws SQLException {
+  private AbstractSerDe getSerDe() throws SQLException {
     if (serde != null) {
       return serde;
     }
