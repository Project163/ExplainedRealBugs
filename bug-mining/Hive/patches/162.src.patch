diff --git a/CHANGES.txt b/CHANGES.txt
index 36674ce639..f6a6c8a293 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -11,7 +11,7 @@ Trunk - Unreleased
 
     HIVE-354. UDF for length of a string. (Neil Conway via namit).
 
-    HIVE-402. Implement UDF regexp. (Raghu Murthy via namit)
+    HIVE-402. Implement UDF regexp. (Raghotham Murthy via namit)
 
     HIVE-250. Shared memory java dbm for map-side joins.
     (Joydeep Sen Sarma via zshao)
@@ -162,10 +162,10 @@ Trunk - Unreleased
 
     HIVE-501. Fix UDFLower() bug. (Zheng Shao via prasadc)
 
-    HIVE-505. console.info prints to stderr. (Raghu Murthy via namit)
+    HIVE-505. console.info prints to stderr. (Raghotham Murthy via namit)
 
     HIVE-504. Fix script operator with empty input file.
-    (Raghu Murthy via zshao)
+    (Raghotham Murthy via zshao)
 
     HIVE-507. Handle empty files properly. (Namit Jain via zshao)
 
@@ -177,6 +177,9 @@ Trunk - Unreleased
 
     HIVE-451. Fix ORDER BY xxx DESC (He Yongqiang via njain)
 
+    HIVE-467. Scratch data location should be on different filesystems for
+    different types of intermediate data. (Joydeep Sen Sarma via rmurthy)
+
 Release 0.3.1 - Unreleased
 
   INCOMPATIBLE CHANGES
@@ -383,7 +386,7 @@ Release 0.2.0 - Unreleased
     HIVE-98. Dependency management with hadoop core using ivy.
     (Ashish Thusoo through zshao)
 
-    HIVE-73. Thrift Server and Client for Hive (Raghu through zshao)
+    HIVE-73. Thrift Server and Client for Hive (Raghotham Murthy through zshao)
 
     HIVE-113. Distribute by and sort by support. (zshao)
 
diff --git a/common/src/java/org/apache/hadoop/hive/common/FileUtils.java b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
new file mode 100644
index 0000000000..7b6483590b
--- /dev/null
+++ b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
@@ -0,0 +1,79 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.common;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.conf.Configuration;
+import java.io.IOException;
+import java.net.URI;
+
+/**
+ * Collection of file manipulation utilities common across Hive
+ */
+public class FileUtils {
+
+  /**
+   * Variant of Path.makeQualified that qualifies the input
+   * path against the default file system indicated by the 
+   * configuration
+   *
+   * This does not require a FileSystem handle in most cases
+   * - only requires the Filesystem URI. This saves the cost
+   * of opening the Filesystem - which can involve RPCs - as
+   * well as cause errors
+   *
+   * @param path path to be fully qualified
+   * @param conf Configuration file
+   * @return path qualified relative to default file system
+   */
+  public static Path makeQualified(Path path, Configuration conf) 
+    throws IOException {
+
+    if (!path.isAbsolute()) {
+      // in this case we need to get the working directory
+      // and this requires a FileSystem handle. So revert to
+      // original method.
+      return path.makeQualified(FileSystem.get(conf));
+    }
+
+    URI fsUri = FileSystem.getDefaultUri(conf);
+    URI pathUri = path.toUri();
+
+    String scheme = pathUri.getScheme();
+    String authority = pathUri.getAuthority();
+
+    if (scheme != null &&
+        (authority != null || fsUri.getAuthority() == null))
+      return path;
+    
+    if (scheme == null) {
+      scheme = fsUri.getScheme();
+    }
+    
+    if (authority == null) {
+      authority = fsUri.getAuthority();
+      if (authority == null) {
+        authority = "";
+      }
+    }
+
+    return new Path(scheme+":"+"//"+authority + pathUri.getPath());
+  }
+}
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index f114f51d6c..a8c8991f27 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -283,14 +283,16 @@ public void create_table(Table tbl) throws AlreadyExistsException, MetaException
           getMS().openTransaction();
           if(tbl.getSd().getLocation() == null || tbl.getSd().getLocation().isEmpty()) {
             tblPath = wh.getDefaultTablePath(tbl.getDbName(), tbl.getTableName());
-            tbl.getSd().setLocation(tblPath.toString());
           } else {
             if (!isExternal(tbl)) {
               LOG.warn("Location: " + tbl.getSd().getLocation() +
                        "specified for non-external table:" + tbl.getTableName());
             }
-            tblPath = new Path(tbl.getSd().getLocation());
+            tblPath = wh.getDnsPath(new Path(tbl.getSd().getLocation()));
           }
+
+          tbl.getSd().setLocation(tblPath.toString());
+
           // get_table checks whether database exists, it should be moved here
           if(is_table_exists(tbl.getDbName(), tbl.getTableName())) {
             throw new AlreadyExistsException("Table " + tbl.getTableName() + " already exists");
@@ -488,14 +490,19 @@ public Partition add_partition(Partition part) throws InvalidObjectException,
           if(tbl == null) {
             throw new InvalidObjectException("Unable to add partition because table or database do not exist");
           }
-          partLocation = new Path(part.getSd().getLocation());
-          if (partLocation == null) {
+
+          String partLocationStr = part.getSd().getLocation();
+          if (partLocationStr == null || partLocationStr.isEmpty()) {
             // set default location if not specified
-            String partLocStr = Warehouse.makePartName(tbl.getPartitionKeys(), part.getValues());
-            partLocation = new Path(partLocStr);
-            part.getSd().setLocation(partLocStr);
+            partLocation = new Path(tbl.getSd().getLocation(),
+                                    Warehouse.makePartName(tbl.getPartitionKeys(), part.getValues()));
+            
+          } else {
+            partLocation = wh.getDnsPath(new Path(partLocationStr));
           }
 
+          part.getSd().setLocation(partLocation.toString());
+
           if(!wh.isDir(partLocation)) {
             if(!wh.mkdirs(partLocation)) {
               throw new MetaException (partLocation + " is not a directory or unable to create one");
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
index 91c5b0b415..9e41bb9071 100755
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.common.FileUtils;
 
 /**
  * This class represents a warehouse where data of Hive tables is stored
@@ -44,48 +45,74 @@
 public class Warehouse {
   private Path whRoot;
   private Configuration conf;
+  String whRootString;
 
   public static final Log LOG = LogFactory.getLog("hive.metastore.warehouse");
 
   public Warehouse(Configuration conf) throws MetaException {
     this.conf = conf;
-    String whRootString =  HiveConf.getVar(conf, HiveConf.ConfVars.METASTOREWAREHOUSE);
+    whRootString =  HiveConf.getVar(conf, HiveConf.ConfVars.METASTOREWAREHOUSE);
     if(StringUtils.isBlank(whRootString)) {
-      throw new MetaException(HiveConf.ConfVars.METASTOREWAREHOUSE.varname + " is not set in the config or blank");
+      throw new MetaException(HiveConf.ConfVars.METASTOREWAREHOUSE.varname
+                              + " is not set in the config or blank");
     }
-    whRoot = new Path(whRootString);
-    URI uri = whRoot.toUri();
+  }
 
-    // if the METASTOREWAREHOUSE value does not specify the schema and the authority
-    // then use the default file system as specified by the Configuration
+  /**
+   * Helper function to convert IOException to MetaException
+   */
+  private FileSystem getFs(Path f) throws MetaException {
     try {
-      if ((uri.getScheme() == null) && (uri.getAuthority() == null)) {
-        FileSystem fs = FileSystem.get(conf);
-        whRoot = new Path(fs.getUri().toString(), whRootString);
-      }
+      return f.getFileSystem(conf);
     } catch (IOException e) {
       MetaStoreUtils.logAndThrowMetaException(e);
     }
+    return null;
   }
 
-  public Path getDefaultDatabasePath(String dbName) {
-    if(dbName.equalsIgnoreCase(MetaStoreUtils.DEFAULT_DATABASE_NAME)) {
+  /**
+   * Hadoop File System reverse lookups paths with raw ip addresses
+   * The File System URI always contains the canonical DNS name of the
+   * Namenode. Subsequently, operations on paths with raw ip addresses
+   * cause an exception since they don't match the file system URI.
+   *
+   * This routine solves this problem by replacing the scheme and authority
+   * of a path with the scheme and authority of the FileSystem that it
+   * maps to.
+   *
+   * @param path Path to be canonicalized
+   * @return Path with canonical scheme and authority
+   */
+  public Path getDnsPath(Path path) throws MetaException {
+    FileSystem fs  = getFs(path);
+    return (new Path(fs.getUri().getScheme(), fs.getUri().getAuthority(),
+                     path.toUri().getPath()));
+  }
+
+
+  /**
+   * Resolve the configured warehouse root dir with respect to the configuration
+   * This involves opening the FileSystem corresponding to the warehouse root dir
+   * (but that should be ok given that this is only called during DDL statements
+   * for non-external tables).
+   */
+  private Path getWhRoot() throws MetaException {
+    if (whRoot != null) {
       return whRoot;
     }
-    return new Path(this.whRoot, dbName.toLowerCase() + ".db");
-  }
-  
-  public Path getDefaultTablePath(String dbName, String tableName) {
-    return new Path(getDefaultDatabasePath(dbName), tableName.toLowerCase());
+    whRoot = getDnsPath(new Path(whRootString));
+    return whRoot;
   }
 
-  private FileSystem getFs(Path f) throws MetaException {
-    try {
-      return f.getFileSystem(conf);
-    } catch (IOException e) {
-      MetaStoreUtils.logAndThrowMetaException(e);
+  public Path getDefaultDatabasePath(String dbName) throws MetaException {
+    if (dbName.equalsIgnoreCase(MetaStoreUtils.DEFAULT_DATABASE_NAME)) {
+      return getWhRoot();
     }
-    return null;
+    return new Path(getWhRoot(), dbName.toLowerCase() + ".db");
+  }
+  
+  public Path getDefaultTablePath(String dbName, String tableName) throws MetaException {
+    return new Path(getDefaultDatabasePath(dbName), tableName.toLowerCase());
   }
 
   public boolean mkdirs(Path f) throws MetaException {
@@ -106,7 +133,14 @@ public boolean deleteDir(Path f, boolean recursive) throws MetaException {
       if(!fs.exists(f)) {
         return false;
       }
-      Trash trashTmp = new Trash(fs.getConf());
+
+      // older versions of Hadoop don't have a Trash constructor based on the 
+      // Path or FileSystem. So need to achieve this by creating a dummy conf.
+      // this needs to be filtered out based on version
+      Configuration dupConf = new Configuration(conf);
+      FileSystem.setDefaultUri(dupConf, fs.getUri());
+
+      Trash trashTmp = new Trash(dupConf);
       if (trashTmp.moveToTrash(f)) {
         LOG.info("Moved to trash: " + f);
         return true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Context.java b/ql/src/java/org/apache/hadoop/hive/ql/Context.java
index 5578693178..930207a5e6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Context.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Context.java
@@ -22,6 +22,10 @@
 import java.io.DataInput;
 import java.io.IOException;
 import java.io.FileNotFoundException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.Random;
+import java.util.ArrayList;
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -30,8 +34,15 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.util.StringUtils;
-import java.util.Random;
 
+import org.apache.hadoop.hive.common.FileUtils;
+
+/**
+ * Context for Semantic Analyzers.
+ * Usage:
+ * not reusable - construct a new one for each query
+ * should call clear() at end of use to remove temporary folders
+ */
 public class Context {
   private Path resFile;
   private Path resDir;
@@ -40,33 +51,219 @@ public class Context {
   private Path[] resDirPaths;
   private int    resDirFilesNum;
   boolean initialized;
-  private String scratchDir;
+  private String scratchPath;
+  private Path MRScratchDir;
+  private Path localScratchDir;
+  private ArrayList<Path> allScratchDirs = new ArrayList<Path> ();
   private HiveConf conf;
-  
+  Random rand = new Random ();
+  protected int randomid = Math.abs(rand.nextInt());
+  protected int pathid = 10000;
+  protected boolean explain = false;
+
   public Context(HiveConf conf) {
     this.conf = conf;
-    initialized = false;
-    resDir = null;
-    resFile = null;
+    Path tmpPath = new Path(conf.getVar(HiveConf.ConfVars.SCRATCHDIR));
+    scratchPath = tmpPath.toUri().getPath();
+  }
+
+  /**
+   * Set the context on whether the current query is an explain query
+   * @param value true if the query is an explain query, false if not
+   */
+  public void setExplain(boolean value) {
+    explain = value;
+  }
+
+  /**
+   * Find out whether the current query is an explain query
+   * @return true if the query is an explain query, false if not
+   */
+  public boolean getExplain() {
+    return explain;
+  }
+
+  /**
+   * Make a tmp directory on the local filesystem
+   */
+  private void makeLocalScratchDir() throws IOException {
+    while (true) {
+      localScratchDir = new Path(System.getProperty("java.io.tmpdir")
+                                 + File.separator + Math.abs(rand.nextInt()));
+      FileSystem fs = FileSystem.getLocal(conf);
+      if (fs.mkdirs(localScratchDir)) {
+        localScratchDir = fs.makeQualified(localScratchDir);
+        allScratchDirs.add(localScratchDir);
+        break;
+      }
+    }
+  }
+
+  /**
+   * Make a tmp directory for MR intermediate data
+   * If URI/Scheme are not supplied - those implied by the default filesystem
+   * will be used (which will typically correspond to hdfs instance on hadoop cluster)
+   */
+  private void makeMRScratchDir() throws IOException {
+    while(true) {
+      MRScratchDir = FileUtils.makeQualified
+        (new Path(conf.getVar(HiveConf.ConfVars.SCRATCHDIR),
+                  Integer.toString(Math.abs(rand.nextInt()))), conf);
+
+      if (explain) {
+        allScratchDirs.add(MRScratchDir);
+        return;
+      }
+
+      FileSystem fs = MRScratchDir.getFileSystem(conf);
+      if (fs.mkdirs(MRScratchDir)) {
+        allScratchDirs.add(MRScratchDir);
+        return;
+      }
+    }
+  }
+  
+  /**
+   * Make a tmp directory on specified URI
+   * Currently will use the same path as implied by SCRATCHDIR config variable
+   */
+  private Path makeExternalScratchDir(URI extURI) throws IOException {
+    while(true) {
+      String extPath = scratchPath + File.separator + 
+        Integer.toString(Math.abs(rand.nextInt()));
+      Path extScratchDir = new Path(extURI.getScheme(), extURI.getAuthority(),
+                                    extPath);
+
+      if (explain) {
+        allScratchDirs.add(extScratchDir);        
+        return extScratchDir;
+      }
+
+      FileSystem fs = extScratchDir.getFileSystem(conf);
+      if (fs.mkdirs(extScratchDir)) {
+        allScratchDirs.add(extScratchDir);
+        return extScratchDir;
+      }
+    }
+  }
+
+  /**
+   * Get a tmp directory on specified URI
+   * Will check if this has already been made
+   * (either via MR or Local FileSystem or some other external URI
+   */
+  private String getExternalScratchDir(URI extURI) {
+    try {
+      // first check if we already made a scratch dir on this URI
+      for (Path p: allScratchDirs) {
+        URI pURI = p.toUri();
+        if (strEquals(pURI.getScheme(), extURI.getScheme()) &&
+            strEquals(pURI.getAuthority(), extURI.getAuthority())) {
+          return p.toString();
+        }
+      }
+      return makeExternalScratchDir(extURI).toString();
+    } catch (IOException e) {
+      throw new RuntimeException (e);
+    }
+  }
+  
+  /**
+   * Create a map-reduce scratch directory on demand and return it
+   */
+  private String getMRScratchDir() {
+    if (MRScratchDir == null) {
+      try {
+        makeMRScratchDir();
+      } catch (IOException e) {
+        throw new RuntimeException (e);
+      }
+    }
+    return MRScratchDir.toString();
+  }
+
+  /**
+   * Create a local scratch directory on demand and return it
+   */
+  private String getLocalScratchDir() {
+    if (localScratchDir == null) {
+      try {
+        makeLocalScratchDir();
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    return localScratchDir.toString();
+  }
+
+  /**
+   * Remove any created scratch directories
+   */
+  private void removeScratchDir() {
+    if (explain) {
+      try {
+        if (localScratchDir != null)
+          FileSystem.getLocal(conf).delete(localScratchDir);
+      } catch (Exception e) {
+        LOG.warn("Error Removing Scratch: " + StringUtils.stringifyException(e));
+      }
+    } else {
+      for (Path p: allScratchDirs) {
+        try {
+          p.getFileSystem(conf).delete(p);
+        } catch (Exception e) {
+          LOG.warn("Error Removing Scratch: " + StringUtils.stringifyException(e));
+        }
+      }
+    }
+    MRScratchDir = null;
+    localScratchDir = null;
+  }
+
+  /**
+   * Return the next available path in the current scratch dir
+   */
+  private String nextPath(String base) {
+    return base + File.separator + Integer.toString(pathid++);
+  }
+  
+  /**
+   * check if path is tmp path. the assumption is that all uri's relative
+   * to scratchdir are temporary
+   * @return true if a uri is a temporary uri for map-reduce intermediate
+   *         data, false otherwise
+   */
+  public boolean isMRTmpFileURI(String uriStr) {
+    return (uriStr.indexOf(scratchPath) != -1);
   }
 
-  public void makeScratchDir() throws Exception {
-    Random rand = new Random();
-    int randomid = Math.abs(rand.nextInt()%rand.nextInt());
-    scratchDir = conf.getVar(HiveConf.ConfVars.SCRATCHDIR) + File.separator + randomid;
-    Path tmpdir = new Path(scratchDir);
-    FileSystem fs = tmpdir.getFileSystem(conf);
-    fs.mkdirs(tmpdir);
+  /**
+   * Get a path to store map-reduce intermediate data in
+   * @return next available path for map-red intermediate data
+   */
+  public String getMRTmpFileURI() {
+    return nextPath(getMRScratchDir());
   }
 
-  public String getScratchDir() {
-    return scratchDir;
+
+  /**
+   * Get a tmp path on local host to store intermediate data
+   * @return next available tmp path on local fs
+   */
+  public String getLocalTmpFileURI() {
+    return nextPath(getLocalScratchDir());
   }
+  
 
-  public void removeScratchDir() throws Exception {
-    Path tmpdir = new Path(scratchDir);
-    FileSystem fs = tmpdir.getFileSystem(conf);
-    fs.delete(tmpdir, true);
+  /**
+   * Get a path to store tmp data destined for external URI
+   * @param extURI external URI to which the tmp data has to be 
+   *               eventually moved
+   * @return next available tmp path on the file system corresponding
+   *              extURI
+   */
+  public String getExternalTmpFileURI(URI extURI) {
+    return nextPath(getExternalScratchDir(extURI));
   }
 
   /**
@@ -104,8 +301,7 @@ public void setResDir(Path resDir) {
     resDirPaths = null;
   }  
   
-  public void clear() {
-    initialized = false;
+  public void clear() throws IOException {
     if (resDir != null)
     {
       try
@@ -127,11 +323,7 @@ public void clear() {
         LOG.info("Context clear error: " + StringUtils.stringifyException(e));
       }
     }
-
-    resDir = null;
-    resFile = null;
-    resDirFilesNum = 0;
-    resDirPaths = null;
+    removeScratchDir();
   }
 
   public DataInput getStream() {
@@ -186,5 +378,12 @@ private DataInput getNextStream() {
     
     return null;
   }
+
+  /**
+   * Little abbreviation for StringUtils
+   */
+  private static boolean strEquals(String str1, String str2) {
+    return org.apache.commons.lang.StringUtils.equals(str1, str2);
+  }
 }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 4f60f1bd09..3735c5cfa0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -140,14 +140,12 @@ public boolean hasReduceTasks(List<Task<? extends Serializable>> tasks) {
   public Driver(HiveConf conf) {
     console = new LogHelper(LOG);
     this.conf = conf;
-    ctx = new Context(conf);
   }
 
   public Driver() {
     console = new LogHelper(LOG);
     if (SessionState.get() != null) {
       conf = SessionState.get().getConf();
-      ctx = new Context(conf);
     }
   }
 
@@ -165,8 +163,7 @@ public int compile(String command) {
     TaskFactory.resetId();
 
     try {
-      ctx.clear();
-      ctx.makeScratchDir();
+      ctx = new Context (conf);
 
       ParseDriver pd = new ParseDriver();
       ASTNode tree = pd.parse(command);
@@ -415,8 +412,6 @@ else if (ss == Utilities.streamStatus.TERMINATED)
 
   public int close() {
     try {
-      // Delete the scratch directory from the context
-      ctx.removeScratchDir();
       ctx.clear();
     } catch (Exception e) {
       console.printError("FAILED: Unknown exception : " + e.getMessage(), "\n"
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
index d73440efc3..6c95e8d9ac 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
@@ -629,7 +629,6 @@ public boolean hasReduce() {
   }
 
   private void addInputPaths(JobConf job, mapredWork work, String hiveScratchDir) throws Exception {
-    FileSystem inpFs = FileSystem.get(job);
     int numEmptyPaths = 0;
     
     // If the query references non-existent partitions
@@ -672,6 +671,7 @@ private void addInputPaths(JobConf job, mapredWork work, String hiveScratchDir)
         
         // If the input file does not exist, replace it by a empty file
         Path dirPath = new Path(onefile);
+        FileSystem inpFs = dirPath.getFileSystem(job);
         boolean emptyInput = true;
         
         if (inpFs.exists(dirPath)) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
index 4a773a78c7..fa503aa701 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
@@ -47,8 +47,8 @@ public class ExplainTask extends Task<explainWork> implements Serializable {
   public int execute() {
     
     try {
-    	OutputStream outS = FileSystem.get(conf).create(work.getResFile());
-    	PrintStream out = new PrintStream(outS);
+      OutputStream outS = work.getResFile().getFileSystem(conf).create(work.getResFile());
+      PrintStream out = new PrintStream(outS);
     	
       // Print out the parse AST
       outputAST(work.getAstStringTree(), out, 0);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
index 313e594a91..0ff8663911 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
@@ -223,7 +223,12 @@ private RecordReader<WritableComparable, Writable> getRecordReader() throws Exce
       if (currPath == null)
         return null;
 
-      FileInputFormat.setInputPaths(job, currPath);
+      // not using FileInputFormat.setInputPaths() here because it forces a connection
+      // to the default file system - which may or may not be online during pure metadata
+      // operations
+      job.set("mapred.input.dir",
+              org.apache.hadoop.util.StringUtils.escapeString(currPath.toString()));
+
       tableDesc tmp = currTbl;
       if (tmp == null)
         tmp = currPart.getTableDesc();
@@ -322,7 +327,7 @@ public boolean fetch(Vector<String> res) {
       return true;
     }
     catch (Exception e) {
-      console.printError("Failed with exception " +   e.getMessage(), "\n" + StringUtils.stringifyException(e));
+      console.printError("Failed with exception " + e.getClass().getName() + ":" +   e.getMessage(), "\n" + StringUtils.stringifyException(e));
       return false;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 0e6f51f756..a6bc3f60cd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -68,7 +68,7 @@ public static enum TableIdEnum {
   transient protected boolean autoDelete = false;
 
   private void commit() throws IOException {
-    if(!fs.rename(outPath, finalPath)) {
+   if (!fs.rename(outPath, finalPath)) {
       throw new IOException ("Unable to rename output to: " + finalPath);
     }
     LOG.info("Committed to output file: " + finalPath);
@@ -120,9 +120,12 @@ public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[]
         statsMap.put(tabIdEnum, row_count);
         
       }
-      fs = FileSystem.get(hconf);
-      finalPath = new Path(Utilities.toTempPath(conf.getDirName()), Utilities.getTaskId(hconf));
-      outPath = new Path(Utilities.toTempPath(conf.getDirName()), Utilities.toTempPath(Utilities.getTaskId(hconf)));
+      String specPath = conf.getDirName();
+      Path tmpPath = Utilities.toTempPath(specPath);
+      String taskId =  Utilities.getTaskId(hconf);
+      fs =(new Path(specPath)).getFileSystem(hconf);
+      finalPath = new Path(tmpPath, taskId);
+      outPath = new Path(tmpPath, Utilities.toTempPath(taskId));
 
       LOG.info("Writing to temp file: " + outPath);
 
@@ -134,7 +137,7 @@ public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[]
       // OutputFormat.getRecordWriter() is that
       // getRecordWriter does not give us enough control over the file name that
       // we create.
-      Path parent = Utilities.toTempPath(conf.getDirName());
+      Path parent = Utilities.toTempPath(specPath);
       finalPath = HiveFileFormatUtils.getOutputFormatFinalPath(parent, jc, hiveOutputFormat, isCompressed, finalPath);
       tableDesc tableInfo = conf.getTableInfo();
 
@@ -195,9 +198,10 @@ public String getName() {
   public void jobClose(Configuration hconf, boolean success) throws HiveException { 
     try {
       if(conf != null) {
-        fs = FileSystem.get(hconf);
-        Path tmpPath = Utilities.toTempPath(conf.getDirName());
-        Path finalPath = new Path(conf.getDirName());
+        String specPath = conf.getDirName();
+        fs = (new Path(specPath)).getFileSystem(hconf);
+        Path tmpPath = Utilities.toTempPath(specPath);
+        Path finalPath = new Path(specPath);
         if(success) {
           if(fs.exists(tmpPath)) {
             // Step1: rename tmp output folder to final path. After this point, 
@@ -227,9 +231,12 @@ public void jobClose(Configuration hconf, boolean success) throws HiveException
    * @param dst the target directory
    * @throws IOException 
    */
-  static public void renameOrMoveFiles(FileSystem fs, Path src, Path dst) throws IOException {
+  static public void renameOrMoveFiles(FileSystem fs, Path src, Path dst)
+    throws IOException, HiveException {
     if (!fs.exists(dst)) {
-      fs.rename(src, dst);
+      if (!fs.rename(src, dst)) {
+        throw new HiveException ("Unable to move: " + src + " to: " + dst);
+      }
     } else {
       // move file by file
       FileStatus[] files = fs.listStatus(src);
@@ -244,7 +251,9 @@ static public void renameOrMoveFiles(FileSystem fs, Path src, Path dst) throws I
             dstFilePath = new Path(dst, fileName + "_" + suffix);
           } while (fs.exists(dstFilePath));
         }
-        fs.rename(srcFilePath, dstFilePath);
+        if (!fs.rename(srcFilePath, dstFilePath)) {
+          throw new HiveException ("Unable to move: " + src + " to: " + dst);
+        }
       }
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index 0abef97588..0386a756f6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -49,16 +49,14 @@ public class MoveTask extends Task<moveWork> implements Serializable {
   public int execute() {
 
     try {
-      // Create a file system handle
-      FileSystem fs = FileSystem.get(conf);
-
       // Do any hive related operations like moving tables and files
       // to appropriate locations
       for(loadFileDesc lfd: work.getLoadFileWork()) {
         Path targetPath = new Path(lfd.getTargetDir());
         Path sourcePath = new Path(lfd.getSourceDir());
+        FileSystem fs = sourcePath.getFileSystem(conf);
         if (lfd.getIsDfsDir()) {
-          // Just do a rename on the URIs
+          // Just do a rename on the URIs, they belong to the same FS
           String mesg = "Moving data to: " + lfd.getTargetDir();
           String mesg_detail = " from " +  lfd.getSourceDir();
           console.printInfo(mesg, mesg_detail);
@@ -66,10 +64,13 @@ public int execute() {
           // delete the output directory if it already exists
           fs.delete(targetPath, true);
           // if source exists, rename. Otherwise, create a empty directory
-          if (fs.exists(sourcePath))
-            fs.rename(sourcePath, targetPath);
-          else
-            fs.mkdirs(targetPath);
+          if (fs.exists(sourcePath)) {
+            if (!fs.rename(sourcePath, targetPath))
+              throw new HiveException ("Unable to rename: " + sourcePath + " to: "
+                                       + targetPath);
+          } else
+            if (!fs.mkdirs(targetPath))
+              throw new HiveException ("Unable to make directory: " + targetPath);
         } else {
           // This is a local file
           String mesg = "Copying data to local directory " + lfd.getTargetDir();
@@ -77,15 +78,17 @@ public int execute() {
           console.printInfo(mesg, mesg_detail);
 
           // delete the existing dest directory
-          LocalFileSystem dstFs = FileSystem.getLocal(fs.getConf());
+          LocalFileSystem dstFs = FileSystem.getLocal(conf);
 
           if(dstFs.delete(targetPath, true) || !dstFs.exists(targetPath)) {
             console.printInfo(mesg, mesg_detail);
             // if source exists, rename. Otherwise, create a empty directory
             if (fs.exists(sourcePath))
               fs.copyToLocalFile(sourcePath, targetPath);
-            else
-              dstFs.mkdirs(targetPath);
+            else {
+              if (!dstFs.mkdirs(targetPath)) 
+                throw new HiveException ("Unable to make local directory: " + targetPath);
+            }
           } else {
             console.printInfo("Unable to delete the existing destination directory: " + targetPath);
           }
@@ -105,9 +108,10 @@ public int execute() {
           // Get all files from the src directory
           FileStatus [] dirs;
           ArrayList<FileStatus> files;
+          FileSystem fs;
           try {
-            fs = FileSystem.get(db.getTable(tbd.getTable().getTableName()).getDataLocation(),
-                Hive.get().getConf());
+            fs = FileSystem.get
+              (db.getTable(tbd.getTable().getTableName()).getDataLocation(),conf);
             dirs = fs.globStatus(new Path(tbd.getSourceDir()));
             files = new ArrayList<FileStatus>();
             for (int i=0; (dirs != null && i<dirs.length); i++) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.java
index 54112f26ba..706c43135e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.java
@@ -76,8 +76,9 @@ public RecordWriter getHiveRecordWriter(JobConf jc, Path outPath,
     }
 
     final int finalRowSeparator = rowSeparator;
+    FileSystem fs = outPath.getFileSystem(jc);
     final OutputStream outStream = Utilities.createCompressedStream(jc,
-        FileSystem.get(jc).create(outPath), isCompressed);
+        fs.create(outPath), isCompressed);
     return new RecordWriter() {
       public void write(Writable r) throws IOException {
         if (r instanceof Text) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveSequenceFileOutputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveSequenceFileOutputFormat.java
index 417a2bfb03..6fc231579c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveSequenceFileOutputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveSequenceFileOutputFormat.java
@@ -62,8 +62,9 @@ public RecordWriter getHiveRecordWriter(JobConf jc, Path finalOutPath,
       Class<? extends Writable> valueClass, boolean isCompressed,
       Properties tableProperties, Progressable progress) throws IOException {
 
+    FileSystem fs = finalOutPath.getFileSystem(jc);
     final SequenceFile.Writer outStream = Utilities.createSequenceWriter(jc,
-        FileSystem.get(jc), finalOutPath, BytesWritable.class, valueClass,
+        fs, finalOutPath, BytesWritable.class, valueClass,
         isCompressed);
 
     return new RecordWriter() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
index 32a3b8f5bf..d798b97fdf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
@@ -505,28 +505,47 @@ public void loadPartition(Path loadPath, String tableName,
   throws HiveException {
     Table tbl = getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName);
     try {
-      FileSystem fs = FileSystem.get(tbl.getDataLocation(), getConf());
-      Path partPath = new Path(tbl.getDataLocation().getPath(), Warehouse.makePartName(partSpec));
       /** Move files before creating the partition since down stream processes check 
        *  for existence of partition in metadata before accessing the data. If partition
        *  is created before data is moved, downstream waiting processes might move forward
        *  with partial data
        */
+
+      FileSystem fs;
+      Path partPath;
+
+      // check if partition exists without creating it
+      Partition part = getPartition (tbl, partSpec, false);
+      if (part == null) {
+        // Partition does not exist currently. The partition name is extrapolated from
+        // the table's location (even if the table is marked external)
+        fs = FileSystem.get(tbl.getDataLocation(), getConf());
+        partPath = new Path(tbl.getDataLocation().getPath(), Warehouse.makePartName(partSpec));
+      } else {
+        // Partition exists already. Get the path from the partition. This will
+        // get the default path for Hive created partitions or the external path
+        // when directly created by user
+        partPath = part.getPath()[0];
+        fs = partPath.getFileSystem(getConf());
+      }
       if(replace) {
         Hive.replaceFiles(loadPath, partPath, fs, tmpDirPath);
       } else {
         Hive.copyFiles(loadPath, partPath, fs);
       }
-      // create a partition if it doesn't exist
-      getPartition(tbl, partSpec, true);
+
+      if (part == null) {
+        // create the partition if it didn't exist before
+        getPartition(tbl, partSpec, true);
+      }
     } catch (IOException e) {
       LOG.error(StringUtils.stringifyException(e));
       throw new HiveException(e);
     } catch (MetaException e) {
       LOG.error(StringUtils.stringifyException(e));
       throw new HiveException(e);
-    } 
-  }
+    }
+}
 
   /**
    * Load a directory into a Hive Table.
@@ -762,14 +781,18 @@ static protected void replaceFiles(Path srcf, Path destf, FileSystem fs,
           for(int i=0; i<srcs.length; i++) {
               FileStatus[] items = fs.listStatus(srcs[i].getPath());
               for(int j=0; j<items.length; j++) {
-                  boolean b = fs.rename(items[j].getPath(), new Path(tmppath, items[j].getPath().getName()));
-                  LOG.debug("Renaming:"+items[j]+",Status:"+b);
+                if (!fs.rename(items[j].getPath(),
+                               new Path(tmppath, items[j].getPath().getName()))) {
+                  throw new HiveException ("Error moving: " + items[j].getPath()
+                                           + " into: " + tmppath);
+                }
               }
           }
 
           // point of no return
           boolean b = fs.delete(destf, true);
           LOG.debug("Deleting:"+destf.toString()+",Status:"+b);
+
           // create the parent directory otherwise rename can fail if the parent doesn't exist
           if (!fs.mkdirs(destf.getParent())) {
             throw new HiveException("Unable to create destination directory: " 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
index 2afc35dcf0..baef9a81ae 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
@@ -128,9 +128,6 @@ public List<tableDesc> getTTDesc() {
   private ParseContext                          parseCtx;
   private Task<? extends Serializable>          mvTask;
   private List<Task<? extends Serializable>>    rootTasks;
-  private String scratchDir;
-  private int randomid;
-  private int pathid;
 
   private Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx; 
   private Task<? extends Serializable>         currTask;
@@ -156,7 +153,6 @@ public List<tableDesc> getTTDesc() {
    * @param parseCtx   current parse context
    * @param rootTasks  root tasks for the plan
    * @param mvTask     the final move task
-   * @param scratchDir directory for temp destinations   
    * @param mapCurrCtx operator to task mappings
    * @param inputs     the set of input tables/partitions generated by the walk
    * @param outputs    the set of destinations generated by the walk
@@ -167,7 +163,6 @@ public GenMRProcContext (
     ParseContext                           parseCtx,
     Task<? extends Serializable>           mvTask,
     List<Task<? extends Serializable>>     rootTasks,
-    String scratchDir, int randomid, int pathid,
     Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx,
     Set<ReadEntity> inputs,
     Set<WriteEntity> outputs) 
@@ -178,9 +173,6 @@ public GenMRProcContext (
     this.mvTask     = mvTask;
     this.parseCtx   = parseCtx;
     this.rootTasks  = rootTasks;
-    this.scratchDir = scratchDir;
-    this.randomid   = randomid;
-    this.pathid     = pathid;
     this.mapCurrCtx = mapCurrCtx;
     this.inputs = inputs;
     this.outputs = outputs;
@@ -277,48 +269,6 @@ public void setRootTasks(List<Task<? extends Serializable>>  rootTasks) {
     this.rootTasks = rootTasks;
   }
 
-  /**
-   * @return directory for temp destinations   
-   */
-  public String getScratchDir() {
-    return scratchDir;
-  }
-
-  /**
-   * @param scratchDir directory for temp destinations   
-   */
-  public void setScratchDir(String scratchDir) {
-    this.scratchDir = scratchDir;
-  }
-
-  /**
-   * @return   identifier used for temp destinations   
-   */
-  public int getRandomId() {
-    return randomid;
-  }
-
-  /**
-   * @param randomid   identifier used for temp destinations   
-   */
-  public void setRandomId(int randomid) {
-    this.randomid = randomid;
-  }
-
-  /**
-   * @return   identifier used for temp destinations   
-   */
-  public int getPathId() {
-    return pathid;
-  }
-
-  /**
-   * @param pathid   identifier used for temp destinations   
-   */
-  public void setPathId(int pathid) {
-    this.pathid = pathid;
-  }
-
   /**
    * @return operator to task mappings
    */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
index 2bfeab4a8c..6af4e8ca80 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.ql.plan.partitionDesc;
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
@@ -117,14 +118,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Ob
       PlanUtils.getBinaryTableDesc(PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol")); 
     
     // generate the temporary file
-    String scratchDir = ctx.getScratchDir();
-    int randomid = ctx.getRandomId();
-    int pathid   = ctx.getPathId();
-    
-    String taskTmpDir = (new Path(scratchDir + File.separator + randomid + '.' + pathid)).toString();
-    
-    pathid++;
-    ctx.setPathId(pathid);
+    Context baseCtx = parseCtx.getContext();
+    String taskTmpDir = baseCtx.getMRTmpFileURI();
     
     // Add the path to alias mapping
     uCtxTask.addTaskTmpDir(taskTmpDir);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index e75d79d179..f4ecca88ad 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
 import org.apache.hadoop.hive.ql.metadata.*;
 import org.apache.hadoop.hive.ql.parse.*;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
@@ -376,13 +377,8 @@ private static void splitTasks(ReduceSinkOperator op,
       rootTasks.remove(childTask);
 
     // generate the temporary file
-    String scratchDir = opProcCtx.getScratchDir();
-    int randomid = opProcCtx.getRandomId();
-    int pathid   = opProcCtx.getPathId();
-      
-    String taskTmpDir = (new Path(scratchDir + File.separator + randomid + '.' + pathid)).toString();
-    pathid++;
-    opProcCtx.setPathId(pathid);
+    Context baseCtx = parseCtx.getContext();
+    String taskTmpDir = baseCtx.getMRTmpFileURI();
     
     Operator<? extends Serializable> parent = op.getParentOperators().get(0);
     tableDesc tt_desc = 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 11d6305c35..dca14a38aa 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -20,6 +20,7 @@
 
 import java.util.*;
 import java.io.File;
+import java.io.IOException;
 import java.io.Serializable;
 import java.io.UnsupportedEncodingException;
 
@@ -37,10 +38,6 @@
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 
 public abstract class BaseSemanticAnalyzer {
-  protected String scratchDir;
-  protected int randomid;
-  protected int pathid;
-
   protected final Hive db;
   protected final HiveConf conf;
   protected List<Task<? extends Serializable>> rootTasks;
@@ -60,11 +57,6 @@ public BaseSemanticAnalyzer(HiveConf conf) throws SemanticException {
       rootTasks = new ArrayList<Task<? extends Serializable>>();
       LOG = LogFactory.getLog(this.getClass().getName());
       console = new LogHelper(LOG);
-
-      this.scratchDir = this.db.getConf().getVar(HiveConf.ConfVars.SCRATCHDIR);
-      Random rand = new Random();
-      this.randomid = Math.abs(rand.nextInt()%rand.nextInt());
-      this.pathid = 10000;
       this.idToTableNameMap = new  HashMap<String, String>();
     } catch (Exception e) {
       throw new SemanticException (e);
@@ -77,11 +69,11 @@ public HashMap<String, String> getIdToTableNameMap() {
   }
 
   
-  public abstract void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException;
+  public abstract void analyzeInternal(ASTNode ast) throws SemanticException;
 
   public void analyze(ASTNode ast, Context ctx) throws SemanticException {
-    scratchDir = ctx.getScratchDir();
-    analyzeInternal(ast, ctx);
+    this.ctx = ctx;
+    analyzeInternal(ast);
   }
 
   public void validate() throws SemanticException {
@@ -93,34 +85,34 @@ public List<Task<? extends Serializable>> getRootTasks() {
   }
 
   /**
-	 * @return the fetchTask
-	 */
-	public Task<? extends Serializable> getFetchTask() {
-		return fetchTask;
-	}
+   * @return the fetchTask
+   */
+  public Task<? extends Serializable> getFetchTask() {
+    return fetchTask;
+  }
 
-	/**
-	 * @param fetchTask the fetchTask to set
-	 */
-	public void setFetchTask(Task<? extends Serializable> fetchTask) {
-		this.fetchTask = fetchTask;
-	}
+  /**
+   * @param fetchTask the fetchTask to set
+   */
+  public void setFetchTask(Task<? extends Serializable> fetchTask) {
+    this.fetchTask = fetchTask;
+  }
 
-	public boolean getFetchTaskInit() {
-		return fetchTaskInit;
-	}
+  public boolean getFetchTaskInit() {
+    return fetchTaskInit;
+  }
 
-	public void setFetchTaskInit(boolean fetchTaskInit) {
-		this.fetchTaskInit = fetchTaskInit;
-	}
+  public void setFetchTaskInit(boolean fetchTaskInit) {
+    this.fetchTaskInit = fetchTaskInit;
+  }
 
-	protected void reset() {
+  protected void reset() {
     rootTasks = new ArrayList<Task<? extends Serializable>>();
   }
 
   public static String stripQuotes(String val) throws SemanticException {
     if ((val.charAt(0) == '\'' && val.charAt(val.length() - 1) == '\'')
-      || (val.charAt(0) == '\"' && val.charAt(val.length() - 1) == '\"')) {
+        || (val.charAt(0) == '\"' && val.charAt(val.length() - 1) == '\"')) {
       val = val.substring(1, val.length() - 1);
     } 
     return val;
@@ -129,31 +121,31 @@ public static String stripQuotes(String val) throws SemanticException {
   public static String charSetString(String charSetName, String charSetString) 
     throws SemanticException {
     try
-    {
-      // The character set name starts with a _, so strip that
-      charSetName = charSetName.substring(1);
-      if (charSetString.charAt(0) == '\'')
-        return new String(unescapeSQLString(charSetString).getBytes(), charSetName);
-      else                                       // hex input is also supported
       {
-        assert charSetString.charAt(0) == '0';
-        assert charSetString.charAt(1) == 'x';
-        charSetString = charSetString.substring(2);
+        // The character set name starts with a _, so strip that
+        charSetName = charSetName.substring(1);
+        if (charSetString.charAt(0) == '\'')
+          return new String(unescapeSQLString(charSetString).getBytes(), charSetName);
+        else                                       // hex input is also supported
+          {
+            assert charSetString.charAt(0) == '0';
+            assert charSetString.charAt(1) == 'x';
+            charSetString = charSetString.substring(2);
         
-        byte[] bArray = new byte[charSetString.length()/2];
-        int j = 0;
-        for (int i = 0; i < charSetString.length(); i += 2)
-        {
-          int val = Character.digit(charSetString.charAt(i), 16) * 16 + Character.digit(charSetString.charAt(i+1), 16);
-          if (val > 127)
-            val = val - 256;
-          bArray[j++] = new Integer(val).byteValue();
-        }
-
-        String res = new String(bArray, charSetName);
-        return res;
-      } 
-    } catch (UnsupportedEncodingException e) {
+            byte[] bArray = new byte[charSetString.length()/2];
+            int j = 0;
+            for (int i = 0; i < charSetString.length(); i += 2)
+              {
+                int val = Character.digit(charSetString.charAt(i), 16) * 16 + Character.digit(charSetString.charAt(i+1), 16);
+                if (val > 127)
+                  val = val - 256;
+                bArray[j++] = new Integer(val).byteValue();
+              }
+
+            String res = new String(bArray, charSetName);
+            return res;
+          } 
+      } catch (UnsupportedEncodingException e) {
       throw new SemanticException(e);
     }
   }
@@ -175,7 +167,7 @@ public static String unescapeIdentifier(String val) {
   }
 
   @SuppressWarnings("nls")
-  public static String unescapeSQLString(String b) {
+    public static String unescapeSQLString(String b) {
 
     Character enclosure = null;
 
@@ -206,15 +198,15 @@ public static String unescapeSQLString(String b) {
         if ((i1 >= '0' && i1 <= '1') &&
             (i2 >= '0' && i2 <= '7') &&
             (i3 >= '0' && i3 <= '7'))
-        {
-          byte bVal = (byte)((i3 - '0') + ((i2 - '0') * 8 ) + ((i1 - '0') * 8 * 8));
-          byte[] bValArr = new byte[1];
-          bValArr[0] = bVal;
-          String tmp = new String(bValArr);
-          sb.append(tmp);
-          i += 3;
-          continue;
-        }
+          {
+            byte bVal = (byte)((i3 - '0') + ((i2 - '0') * 8 ) + ((i1 - '0') * 8 * 8));
+            byte[] bValArr = new byte[1];
+            bValArr[0] = bVal;
+            String tmp = new String(bValArr);
+            sb.append(tmp);
+            i += 3;
+            continue;
+          }
       }
 
       if (currentChar == '\\' && (i+2 < b.length())) {
@@ -229,7 +221,7 @@ public static String unescapeSQLString(String b) {
         case 't': sb.append("\t"); break;
         case 'Z': sb.append("\u001A"); break;
         case '\\': sb.append("\\"); break;
-        // The following 2 lines are exactly what MySQL does
+          // The following 2 lines are exactly what MySQL does
         case '%': sb.append("\\%"); break;
         case '_': sb.append("\\_"); break;
         default: sb.append(n);
@@ -242,14 +234,6 @@ public static String unescapeSQLString(String b) {
     return sb.toString();
   }
   
-  public String getTmpFileName() 
-  {
-    // generate the temporary file
-    String taskTmpDir = this.scratchDir + File.separator + this.randomid + '.' + this.pathid;
-    this.pathid++;
-    return taskTmpDir;
-  }
-  
   public Set<ReadEntity> getInputs() {
     return new LinkedHashSet<ReadEntity>();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index fa1c720ac4..142a28df0f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -100,8 +100,7 @@ public DDLSemanticAnalyzer(HiveConf conf) throws SemanticException {
   }
 
   @Override
-  public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
-    this.ctx = ctx;
+  public void analyzeInternal(ASTNode ast) throws SemanticException {
     if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE)
        analyzeCreateTable(ast, false);
     else if (ast.getToken().getType() == HiveParser.TOK_CREATEEXTTABLE)
@@ -110,15 +109,15 @@ else if (ast.getToken().getType() == HiveParser.TOK_DROPTABLE)
        analyzeDropTable(ast);
     else if (ast.getToken().getType() == HiveParser.TOK_DESCTABLE)
     {
-      ctx.setResFile(new Path(getTmpFileName()));
+      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));
       analyzeDescribeTable(ast);
     }
     else if (ast.getToken().getType() == HiveParser.TOK_SHOWTABLES)
     {
-      ctx.setResFile(new Path(getTmpFileName()));
+      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));
       analyzeShowTables(ast);
     } else if (ast.getToken().getType() == HiveParser.TOK_MSCK) {
-      ctx.setResFile(new Path(getTmpFileName()));
+      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));
       analyzeMetastoreCheck(ast);    
     } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_RENAME)
       analyzeAlterTableRename(ast);
@@ -138,7 +137,7 @@ else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_SERIALIZER)
       analyzeAlterTableSerde(ast);
     else if (ast.getToken().getType() == HiveParser.TOK_SHOWPARTITIONS)
     {
-      ctx.setResFile(new Path(getTmpFileName()));
+      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));
       analyzeShowPartitions(ast);
     }
     else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
index 8202308308..34ee4b54de 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
@@ -36,8 +36,9 @@ public ExplainSemanticAnalyzer(HiveConf conf) throws SemanticException {
     super(conf);
   }
 
-  public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
-    
+  public void analyzeInternal(ASTNode ast) throws SemanticException {
+    ctx.setExplain(true);
+
     // Create a semantic analyzer for the query
     BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, (ASTNode)ast.getChild(0));
     sem.analyze((ASTNode)ast.getChild(0), ctx);
@@ -47,7 +48,7 @@ public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
       extended = true;
     }
     
-    ctx.setResFile(new Path(getTmpFileName()));
+    ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));
     List<Task<? extends Serializable>> tasks = sem.getRootTasks();
     Task<? extends Serializable> fetchTask = sem.getFetchTask();
     if (tasks == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
index ad62481a0a..5702e63027 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
@@ -34,7 +34,7 @@ public FunctionSemanticAnalyzer(HiveConf conf) throws SemanticException {
     super(conf);
   }
   
-  public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
+  public void analyzeInternal(ASTNode ast) throws SemanticException {
     String functionName = ast.getChild(0).getText();
     String className = unescapeSQLString(ast.getChild(1).getText());
     createFunctionDesc desc = new createFunctionDesc(functionName, className);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
index 7050f810d0..cdd0f9be31 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
@@ -148,7 +148,7 @@ private void applyConstraints(URI fromURI, URI toURI, Tree ast, boolean isLocal)
   }
 
   @Override
-  public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
+  public void analyzeInternal(ASTNode ast) throws SemanticException {
     isLocal = isOverWrite = false;
     Tree from_t = ast.getChild(0);
     Tree table_t = ast.getChild(1);
@@ -189,29 +189,22 @@ public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
     if(isLocal) {
       // if the local keyword is specified - we will always make a copy. this might seem redundant in the case 
       // that the hive warehouse is also located in the local file system - but that's just a test case.
-      URI copyURI;
-      try {
-        // extract out the path name only from the scratchdir configuration
-        String scratchPath = (new Path(conf.getVar(HiveConf.ConfVars.SCRATCHDIR))).toUri().getPath();
-        copyURI = new URI(toURI.getScheme(), toURI.getAuthority(),
-                          scratchPath + "/" + Utilities.randGen.nextInt(),
-                          null, null);                          
-      } catch (URISyntaxException e) {
-        // Has to use full name to make sure it does not conflict with org.apache.commons.lang.StringUtils
-        LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
-        LOG.error("Invalid URI. Check value of variable: " + HiveConf.ConfVars.SCRATCHDIR.toString());
-        throw new SemanticException("Cannot initialize temporary destination URI");
-      }
-      rTask = TaskFactory.get(new copyWork(fromURI.toString(), copyURI.toString()), this.conf);
+      String copyURIStr = ctx.getExternalTmpFileURI(toURI);
+      URI copyURI = URI.create(copyURIStr);
+      rTask = TaskFactory.get(new copyWork(fromURI.toString(), copyURIStr), this.conf);
       fromURI = copyURI;
     }
-
+    
     // create final load/move work
     List<loadTableDesc> loadTableWork =  new ArrayList<loadTableDesc>();
     List<loadFileDesc> loadFileWork = new ArrayList<loadFileDesc>();
 
-    loadTableWork.add(new loadTableDesc(fromURI.toString(), getTmpFileName(), Utilities.getTableDesc(ts.tableHandle),
-                                        (ts.partSpec != null) ? ts.partSpec : new HashMap<String, String> (),
+    String loadTmpPath;
+    loadTmpPath = ctx.getExternalTmpFileURI(toURI);
+    loadTableWork.add(new loadTableDesc(fromURI.toString(), loadTmpPath,
+                                        Utilities.getTableDesc(ts.tableHandle),
+                                        (ts.partSpec != null) ? ts.partSpec :
+                                        new HashMap<String, String> (),
                                         isOverWrite));
 
     if(rTask != null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index a073021af8..6e600b205c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -38,6 +38,8 @@
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
@@ -744,7 +746,7 @@ public void getMetaData(QB qb) throws SemanticException {
             if ((!qb.getParseInfo().getIsSubQ()) &&
                 (((ASTNode)ast.getChild(0)).getToken().getType() == HiveParser.TOK_TMP_FILE))
             {
-              fname = getTmpFileName();
+              fname = ctx.getMRTmpFileURI();
               ctx.setResDir(new Path(fname));
               qb.setIsQuery(true);
             }
@@ -763,39 +765,6 @@ public void getMetaData(QB qb) throws SemanticException {
     }
   }
 
-  @SuppressWarnings("nls")
-  public static String getJEXLOpName(String name) {
-    if (name.equalsIgnoreCase("AND")) {
-      return "&&";
-    }
-    else if (name.equalsIgnoreCase("OR")) {
-      return "||";
-    }
-    else if (name.equalsIgnoreCase("NOT")) {
-      return "!";
-    }
-    else if (name.equalsIgnoreCase("=")) {
-      return "==";
-    }
-    else if (name.equalsIgnoreCase("<>")) {
-      return "!=";
-    }
-    else if (name.equalsIgnoreCase("NULL")) {
-      return "== NULL";
-    }
-    else if (name.equalsIgnoreCase("NOT NULL")) {
-      return "!= NULL";
-    }
-    else {
-      return name;
-    }
-  }
-
-  @SuppressWarnings("nls")
-  public static String getJEXLFuncName(String name) {
-    return "__udf__" + name;
-  }
-
   private boolean isPresent(String[] list, String elem) {
     for (String s : list)
       if (s.equals(elem))
@@ -2236,84 +2205,109 @@ private Operator genConversionOps(String dest, QB qb,
   private Operator genFileSinkPlan(String dest, QB qb,
       Operator input) throws SemanticException {
 
-  	RowResolver inputRR = opParseCtx.get(input).getRR();
-    // Generate the destination file
-    String queryTmpdir = this.scratchDir + File.separator + this.randomid + '.' + this.pathid + '.' + dest ;
-    this.pathid ++;
-
-    // Next for the destination tables, fetch the information
-    // create a temporary directory name and chain it to the plan
-    String dest_path = null;
-    tableDesc table_desc = null;
+    RowResolver inputRR = opParseCtx.get(input).getRR();
+    QBMetaData qbm = qb.getMetaData();
+    Integer dest_type = qbm.getDestTypeForAlias(dest);
     
+    Table dest_tab;     // destination table if any
+    String queryTmpdir; // the intermediate destination directory
+    Path dest_path;     // the final destination directory
+    tableDesc table_desc = null;
     int currentTableId = 0;
-
-    Integer dest_type = qb.getMetaData().getDestTypeForAlias(dest);
+    boolean isLocal = false;
 
     switch (dest_type.intValue()) {
-    case QBMetaData.DEST_TABLE:
-      {
-        
-       
-        Table dest_tab = qb.getMetaData().getDestTableForAlias(dest);
+    case QBMetaData.DEST_TABLE: {
+
+        dest_tab = qbm.getDestTableForAlias(dest);
+        dest_path = dest_tab.getPath();
+        queryTmpdir = ctx.getExternalTmpFileURI(dest_path.toUri());
         table_desc = Utilities.getTableDesc(dest_tab);
         
         this.idToTableNameMap.put( String.valueOf(this.destTableId), dest_tab.getName());
         currentTableId = this.destTableId;
         this.destTableId ++;
 
-        dest_path = dest_tab.getPath().toString();
         // Create the work for moving the table
-        this.loadTableWork.add(new loadTableDesc(queryTmpdir, getTmpFileName(),
-                                            table_desc,
-                                            new HashMap<String, String>()));
+        this.loadTableWork.add
+          (new loadTableDesc(queryTmpdir,
+                             ctx.getExternalTmpFileURI(dest_path.toUri()),
+                             table_desc,
+                             new HashMap<String, String>()));
         outputs.add(new WriteEntity(dest_tab));
         break;
       }
-    case QBMetaData.DEST_PARTITION:
-      {
-        Partition dest_part = qb.getMetaData().getDestPartitionForAlias(dest);
-        Table dest_tab = dest_part.getTable();
+    case QBMetaData.DEST_PARTITION: {
+
+        Partition dest_part = qbm.getDestPartitionForAlias(dest);
+        dest_tab = dest_part.getTable();
+        dest_path = dest_part.getPath()[0];
+        queryTmpdir = ctx.getExternalTmpFileURI(dest_path.toUri());
         table_desc = Utilities.getTableDesc(dest_tab);
-        dest_path = dest_part.getPath()[0].toString();
-        this.idToTableNameMap.put( String.valueOf(this.destTableId), dest_tab.getName());
+
+        this.idToTableNameMap.put(String.valueOf(this.destTableId), dest_tab.getName());
         currentTableId = this.destTableId;
         this.destTableId ++;
         
-        this.loadTableWork.add(new loadTableDesc(queryTmpdir, getTmpFileName(), table_desc, dest_part.getSpec()));
+        this.loadTableWork.add
+          (new loadTableDesc(queryTmpdir,
+                             ctx.getExternalTmpFileURI(dest_path.toUri()),
+                             table_desc, dest_part.getSpec()));
         outputs.add(new WriteEntity(dest_part));
         break;
       }
     case QBMetaData.DEST_LOCAL_FILE:
+        isLocal = true;
+        // fall through
     case QBMetaData.DEST_DFS_FILE: {
-        dest_path = qb.getMetaData().getDestFileForAlias(dest);
+        dest_path = new Path(qbm.getDestFileForAlias(dest));
+        String destStr = dest_path.toString();
+
+        if (isLocal) {
+          // for local directory - we always write to map-red intermediate
+          // store and then copy to local fs
+          queryTmpdir = ctx.getMRTmpFileURI();
+        } else {
+          // otherwise write to the file system implied by the directory
+          // no copy is required. we may want to revisit this policy in future
+
+          try {
+            Path qPath = FileUtils.makeQualified(dest_path, conf);
+            queryTmpdir = ctx.getExternalTmpFileURI(qPath.toUri());
+          } catch (Exception e) {
+            throw new SemanticException("Error creating temporary folder on: "
+                                        + dest_path, e);
+          }
+        }
         String cols = new String();
         Vector<ColumnInfo> colInfos = inputRR.getColumnInfos();
     
         boolean first = true;
         for (ColumnInfo colInfo:colInfos) {
-        	String[] nm = inputRR.reverseLookup(colInfo.getInternalName());
+          String[] nm = inputRR.reverseLookup(colInfo.getInternalName());
           if (!first)
             cols = cols.concat(",");
           
           first = false;
           if (nm[0] == null) 
-          	cols = cols.concat(nm[1]);
+            cols = cols.concat(nm[1]);
           else
-          	cols = cols.concat(nm[0] + "." + nm[1]);
+            cols = cols.concat(nm[0] + "." + nm[1]);
         }
-        if (!dest_path.startsWith(this.scratchDir+File.separator))
-        {
-          this.idToTableNameMap.put( String.valueOf(this.destTableId), dest_path);
+
+        if (!ctx.isMRTmpFileURI(destStr)) {
+          this.idToTableNameMap.put( String.valueOf(this.destTableId), destStr);
           currentTableId = this.destTableId;
           this.destTableId ++;
         }
+
         boolean isDfsDir = (dest_type.intValue() == QBMetaData.DEST_DFS_FILE);
-        this.loadFileWork.add(new loadFileDesc(queryTmpdir, dest_path,
-                                          isDfsDir, cols));
+        this.loadFileWork.add(new loadFileDesc(queryTmpdir, destStr,
+                                               isDfsDir, cols));
+
         table_desc = PlanUtils.getDefaultTableDesc(Integer.toString(Utilities.ctrlaCode),
-            cols);
-        outputs.add(new WriteEntity(dest_path, !isDfsDir));
+                                                   cols);
+        outputs.add(new WriteEntity(destStr, !isDfsDir));
         break;
     }
     default:
@@ -3583,7 +3577,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {
       new GenMRProcContext(
         new HashMap<Operator<? extends Serializable>, Task<? extends Serializable>>(),
         new ArrayList<Operator<? extends Serializable>>(),
-        getParseContext(), mvTask, this.rootTasks, this.scratchDir, this.randomid, this.pathid,
+        getParseContext(), mvTask, this.rootTasks,
         new HashMap<Operator<? extends Serializable>, GenMapRedCtx>(),
         inputs, outputs);
 
@@ -3675,8 +3669,7 @@ public Phase1Ctx initPhase1Ctx() {
 
   @Override
   @SuppressWarnings("nls")
-  public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
-    this.ctx = ctx;
+  public void analyzeInternal(ASTNode ast) throws SemanticException {
     reset();
 
     QB qb = new QB(null, null, false);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
index 625e69ae64..5239303912 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -745,13 +745,12 @@ public List<Task<? extends Serializable>> analyzeAST(ASTNode ast) throws Excepti
 
     // Do semantic analysis and plan generation
     Context ctx = new Context(conf);
-    ctx.makeScratchDir();
     while((ast.getToken() == null) && (ast.getChildCount() > 0)) {
       ast = (ASTNode)ast.getChild(0);
     }
     
     sem.analyze(ast, ctx);
-    ctx.removeScratchDir();
+    ctx.clear();
     return sem.getRootTasks();
   }
 
diff --git a/ql/src/test/queries/clientpositive/create_1.q b/ql/src/test/queries/clientpositive/create_1.q
index 817ac6da09..82630885fe 100644
--- a/ql/src/test/queries/clientpositive/create_1.q
+++ b/ql/src/test/queries/clientpositive/create_1.q
@@ -1,3 +1,5 @@
+set fs.default.name=invalidscheme:///;
+
 DROP TABLE table1;
 DROP TABLE table2;
 DROP TABLE table3;
diff --git a/ql/src/test/queries/clientpositive/groupby1.q b/ql/src/test/queries/clientpositive/groupby1.q
index 25eee098fd..1275eab281 100755
--- a/ql/src/test/queries/clientpositive/groupby1.q
+++ b/ql/src/test/queries/clientpositive/groupby1.q
@@ -3,9 +3,13 @@ set hive.groupby.skewindata=true;
 
 CREATE TABLE dest_g1(key INT, value DOUBLE) STORED AS TEXTFILE;
 
+set fs.default.name=invalidscheme:///;
+
 EXPLAIN
 FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;
 
+set fs.default.name=file:///;
+
 FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;
 
 SELECT dest_g1.* FROM dest_g1;
diff --git a/ql/src/test/queries/clientpositive/insertexternal1.q b/ql/src/test/queries/clientpositive/insertexternal1.q
new file mode 100644
index 0000000000..466fcbf650
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/insertexternal1.q
@@ -0,0 +1,13 @@
+drop table texternal;
+
+create table texternal(key string, val string) partitioned by (insertdate string);
+
+!rm -fr /tmp/texternal;
+!mkdir -p /tmp/texternal/2008-01-01;
+
+alter table texternal add partition (insertdate='2008-01-01') location 'file:///tmp/texternal/2008-01-01';
+from src insert overwrite table texternal partition (insertdate='2008-01-01') select *;
+
+select * from texternal where insertdate='2008-01-01';
+
+!rm -fr /tmp/texternal;
diff --git a/ql/src/test/results/clientpositive/insertexternal1.q.out b/ql/src/test/results/clientpositive/insertexternal1.q.out
new file mode 100644
index 0000000000..4c192c2d59
--- /dev/null
+++ b/ql/src/test/results/clientpositive/insertexternal1.q.out
@@ -0,0 +1,509 @@
+query: drop table texternal
+query: create table texternal(key string, val string) partitioned by (insertdate string)
+query: alter table texternal add partition (insertdate='2008-01-01') location 'file:///tmp/texternal/2008-01-01'
+query: from src insert overwrite table texternal partition (insertdate='2008-01-01') select *
+Input: default/src
+Output: default/texternal/insertdate=2008-01-01
+query: select * from texternal where insertdate='2008-01-01'
+Input: default/texternal/insertdate=2008-01-01
+Output: file:/d1/jssarma/src/hive-scratchdir/build/ql/tmp/1849466824/10000
+238	val_238	2008-01-01
+86	val_86	2008-01-01
+311	val_311	2008-01-01
+27	val_27	2008-01-01
+165	val_165	2008-01-01
+409	val_409	2008-01-01
+255	val_255	2008-01-01
+278	val_278	2008-01-01
+98	val_98	2008-01-01
+484	val_484	2008-01-01
+265	val_265	2008-01-01
+193	val_193	2008-01-01
+401	val_401	2008-01-01
+150	val_150	2008-01-01
+273	val_273	2008-01-01
+224	val_224	2008-01-01
+369	val_369	2008-01-01
+66	val_66	2008-01-01
+128	val_128	2008-01-01
+213	val_213	2008-01-01
+146	val_146	2008-01-01
+406	val_406	2008-01-01
+429	val_429	2008-01-01
+374	val_374	2008-01-01
+152	val_152	2008-01-01
+469	val_469	2008-01-01
+145	val_145	2008-01-01
+495	val_495	2008-01-01
+37	val_37	2008-01-01
+327	val_327	2008-01-01
+281	val_281	2008-01-01
+277	val_277	2008-01-01
+209	val_209	2008-01-01
+15	val_15	2008-01-01
+82	val_82	2008-01-01
+403	val_403	2008-01-01
+166	val_166	2008-01-01
+417	val_417	2008-01-01
+430	val_430	2008-01-01
+252	val_252	2008-01-01
+292	val_292	2008-01-01
+219	val_219	2008-01-01
+287	val_287	2008-01-01
+153	val_153	2008-01-01
+193	val_193	2008-01-01
+338	val_338	2008-01-01
+446	val_446	2008-01-01
+459	val_459	2008-01-01
+394	val_394	2008-01-01
+237	val_237	2008-01-01
+482	val_482	2008-01-01
+174	val_174	2008-01-01
+413	val_413	2008-01-01
+494	val_494	2008-01-01
+207	val_207	2008-01-01
+199	val_199	2008-01-01
+466	val_466	2008-01-01
+208	val_208	2008-01-01
+174	val_174	2008-01-01
+399	val_399	2008-01-01
+396	val_396	2008-01-01
+247	val_247	2008-01-01
+417	val_417	2008-01-01
+489	val_489	2008-01-01
+162	val_162	2008-01-01
+377	val_377	2008-01-01
+397	val_397	2008-01-01
+309	val_309	2008-01-01
+365	val_365	2008-01-01
+266	val_266	2008-01-01
+439	val_439	2008-01-01
+342	val_342	2008-01-01
+367	val_367	2008-01-01
+325	val_325	2008-01-01
+167	val_167	2008-01-01
+195	val_195	2008-01-01
+475	val_475	2008-01-01
+17	val_17	2008-01-01
+113	val_113	2008-01-01
+155	val_155	2008-01-01
+203	val_203	2008-01-01
+339	val_339	2008-01-01
+0	val_0	2008-01-01
+455	val_455	2008-01-01
+128	val_128	2008-01-01
+311	val_311	2008-01-01
+316	val_316	2008-01-01
+57	val_57	2008-01-01
+302	val_302	2008-01-01
+205	val_205	2008-01-01
+149	val_149	2008-01-01
+438	val_438	2008-01-01
+345	val_345	2008-01-01
+129	val_129	2008-01-01
+170	val_170	2008-01-01
+20	val_20	2008-01-01
+489	val_489	2008-01-01
+157	val_157	2008-01-01
+378	val_378	2008-01-01
+221	val_221	2008-01-01
+92	val_92	2008-01-01
+111	val_111	2008-01-01
+47	val_47	2008-01-01
+72	val_72	2008-01-01
+4	val_4	2008-01-01
+280	val_280	2008-01-01
+35	val_35	2008-01-01
+427	val_427	2008-01-01
+277	val_277	2008-01-01
+208	val_208	2008-01-01
+356	val_356	2008-01-01
+399	val_399	2008-01-01
+169	val_169	2008-01-01
+382	val_382	2008-01-01
+498	val_498	2008-01-01
+125	val_125	2008-01-01
+386	val_386	2008-01-01
+437	val_437	2008-01-01
+469	val_469	2008-01-01
+192	val_192	2008-01-01
+286	val_286	2008-01-01
+187	val_187	2008-01-01
+176	val_176	2008-01-01
+54	val_54	2008-01-01
+459	val_459	2008-01-01
+51	val_51	2008-01-01
+138	val_138	2008-01-01
+103	val_103	2008-01-01
+239	val_239	2008-01-01
+213	val_213	2008-01-01
+216	val_216	2008-01-01
+430	val_430	2008-01-01
+278	val_278	2008-01-01
+176	val_176	2008-01-01
+289	val_289	2008-01-01
+221	val_221	2008-01-01
+65	val_65	2008-01-01
+318	val_318	2008-01-01
+332	val_332	2008-01-01
+311	val_311	2008-01-01
+275	val_275	2008-01-01
+137	val_137	2008-01-01
+241	val_241	2008-01-01
+83	val_83	2008-01-01
+333	val_333	2008-01-01
+180	val_180	2008-01-01
+284	val_284	2008-01-01
+12	val_12	2008-01-01
+230	val_230	2008-01-01
+181	val_181	2008-01-01
+67	val_67	2008-01-01
+260	val_260	2008-01-01
+404	val_404	2008-01-01
+384	val_384	2008-01-01
+489	val_489	2008-01-01
+353	val_353	2008-01-01
+373	val_373	2008-01-01
+272	val_272	2008-01-01
+138	val_138	2008-01-01
+217	val_217	2008-01-01
+84	val_84	2008-01-01
+348	val_348	2008-01-01
+466	val_466	2008-01-01
+58	val_58	2008-01-01
+8	val_8	2008-01-01
+411	val_411	2008-01-01
+230	val_230	2008-01-01
+208	val_208	2008-01-01
+348	val_348	2008-01-01
+24	val_24	2008-01-01
+463	val_463	2008-01-01
+431	val_431	2008-01-01
+179	val_179	2008-01-01
+172	val_172	2008-01-01
+42	val_42	2008-01-01
+129	val_129	2008-01-01
+158	val_158	2008-01-01
+119	val_119	2008-01-01
+496	val_496	2008-01-01
+0	val_0	2008-01-01
+322	val_322	2008-01-01
+197	val_197	2008-01-01
+468	val_468	2008-01-01
+393	val_393	2008-01-01
+454	val_454	2008-01-01
+100	val_100	2008-01-01
+298	val_298	2008-01-01
+199	val_199	2008-01-01
+191	val_191	2008-01-01
+418	val_418	2008-01-01
+96	val_96	2008-01-01
+26	val_26	2008-01-01
+165	val_165	2008-01-01
+327	val_327	2008-01-01
+230	val_230	2008-01-01
+205	val_205	2008-01-01
+120	val_120	2008-01-01
+131	val_131	2008-01-01
+51	val_51	2008-01-01
+404	val_404	2008-01-01
+43	val_43	2008-01-01
+436	val_436	2008-01-01
+156	val_156	2008-01-01
+469	val_469	2008-01-01
+468	val_468	2008-01-01
+308	val_308	2008-01-01
+95	val_95	2008-01-01
+196	val_196	2008-01-01
+288	val_288	2008-01-01
+481	val_481	2008-01-01
+457	val_457	2008-01-01
+98	val_98	2008-01-01
+282	val_282	2008-01-01
+197	val_197	2008-01-01
+187	val_187	2008-01-01
+318	val_318	2008-01-01
+318	val_318	2008-01-01
+409	val_409	2008-01-01
+470	val_470	2008-01-01
+137	val_137	2008-01-01
+369	val_369	2008-01-01
+316	val_316	2008-01-01
+169	val_169	2008-01-01
+413	val_413	2008-01-01
+85	val_85	2008-01-01
+77	val_77	2008-01-01
+0	val_0	2008-01-01
+490	val_490	2008-01-01
+87	val_87	2008-01-01
+364	val_364	2008-01-01
+179	val_179	2008-01-01
+118	val_118	2008-01-01
+134	val_134	2008-01-01
+395	val_395	2008-01-01
+282	val_282	2008-01-01
+138	val_138	2008-01-01
+238	val_238	2008-01-01
+419	val_419	2008-01-01
+15	val_15	2008-01-01
+118	val_118	2008-01-01
+72	val_72	2008-01-01
+90	val_90	2008-01-01
+307	val_307	2008-01-01
+19	val_19	2008-01-01
+435	val_435	2008-01-01
+10	val_10	2008-01-01
+277	val_277	2008-01-01
+273	val_273	2008-01-01
+306	val_306	2008-01-01
+224	val_224	2008-01-01
+309	val_309	2008-01-01
+389	val_389	2008-01-01
+327	val_327	2008-01-01
+242	val_242	2008-01-01
+369	val_369	2008-01-01
+392	val_392	2008-01-01
+272	val_272	2008-01-01
+331	val_331	2008-01-01
+401	val_401	2008-01-01
+242	val_242	2008-01-01
+452	val_452	2008-01-01
+177	val_177	2008-01-01
+226	val_226	2008-01-01
+5	val_5	2008-01-01
+497	val_497	2008-01-01
+402	val_402	2008-01-01
+396	val_396	2008-01-01
+317	val_317	2008-01-01
+395	val_395	2008-01-01
+58	val_58	2008-01-01
+35	val_35	2008-01-01
+336	val_336	2008-01-01
+95	val_95	2008-01-01
+11	val_11	2008-01-01
+168	val_168	2008-01-01
+34	val_34	2008-01-01
+229	val_229	2008-01-01
+233	val_233	2008-01-01
+143	val_143	2008-01-01
+472	val_472	2008-01-01
+322	val_322	2008-01-01
+498	val_498	2008-01-01
+160	val_160	2008-01-01
+195	val_195	2008-01-01
+42	val_42	2008-01-01
+321	val_321	2008-01-01
+430	val_430	2008-01-01
+119	val_119	2008-01-01
+489	val_489	2008-01-01
+458	val_458	2008-01-01
+78	val_78	2008-01-01
+76	val_76	2008-01-01
+41	val_41	2008-01-01
+223	val_223	2008-01-01
+492	val_492	2008-01-01
+149	val_149	2008-01-01
+449	val_449	2008-01-01
+218	val_218	2008-01-01
+228	val_228	2008-01-01
+138	val_138	2008-01-01
+453	val_453	2008-01-01
+30	val_30	2008-01-01
+209	val_209	2008-01-01
+64	val_64	2008-01-01
+468	val_468	2008-01-01
+76	val_76	2008-01-01
+74	val_74	2008-01-01
+342	val_342	2008-01-01
+69	val_69	2008-01-01
+230	val_230	2008-01-01
+33	val_33	2008-01-01
+368	val_368	2008-01-01
+103	val_103	2008-01-01
+296	val_296	2008-01-01
+113	val_113	2008-01-01
+216	val_216	2008-01-01
+367	val_367	2008-01-01
+344	val_344	2008-01-01
+167	val_167	2008-01-01
+274	val_274	2008-01-01
+219	val_219	2008-01-01
+239	val_239	2008-01-01
+485	val_485	2008-01-01
+116	val_116	2008-01-01
+223	val_223	2008-01-01
+256	val_256	2008-01-01
+263	val_263	2008-01-01
+70	val_70	2008-01-01
+487	val_487	2008-01-01
+480	val_480	2008-01-01
+401	val_401	2008-01-01
+288	val_288	2008-01-01
+191	val_191	2008-01-01
+5	val_5	2008-01-01
+244	val_244	2008-01-01
+438	val_438	2008-01-01
+128	val_128	2008-01-01
+467	val_467	2008-01-01
+432	val_432	2008-01-01
+202	val_202	2008-01-01
+316	val_316	2008-01-01
+229	val_229	2008-01-01
+469	val_469	2008-01-01
+463	val_463	2008-01-01
+280	val_280	2008-01-01
+2	val_2	2008-01-01
+35	val_35	2008-01-01
+283	val_283	2008-01-01
+331	val_331	2008-01-01
+235	val_235	2008-01-01
+80	val_80	2008-01-01
+44	val_44	2008-01-01
+193	val_193	2008-01-01
+321	val_321	2008-01-01
+335	val_335	2008-01-01
+104	val_104	2008-01-01
+466	val_466	2008-01-01
+366	val_366	2008-01-01
+175	val_175	2008-01-01
+403	val_403	2008-01-01
+483	val_483	2008-01-01
+53	val_53	2008-01-01
+105	val_105	2008-01-01
+257	val_257	2008-01-01
+406	val_406	2008-01-01
+409	val_409	2008-01-01
+190	val_190	2008-01-01
+406	val_406	2008-01-01
+401	val_401	2008-01-01
+114	val_114	2008-01-01
+258	val_258	2008-01-01
+90	val_90	2008-01-01
+203	val_203	2008-01-01
+262	val_262	2008-01-01
+348	val_348	2008-01-01
+424	val_424	2008-01-01
+12	val_12	2008-01-01
+396	val_396	2008-01-01
+201	val_201	2008-01-01
+217	val_217	2008-01-01
+164	val_164	2008-01-01
+431	val_431	2008-01-01
+454	val_454	2008-01-01
+478	val_478	2008-01-01
+298	val_298	2008-01-01
+125	val_125	2008-01-01
+431	val_431	2008-01-01
+164	val_164	2008-01-01
+424	val_424	2008-01-01
+187	val_187	2008-01-01
+382	val_382	2008-01-01
+5	val_5	2008-01-01
+70	val_70	2008-01-01
+397	val_397	2008-01-01
+480	val_480	2008-01-01
+291	val_291	2008-01-01
+24	val_24	2008-01-01
+351	val_351	2008-01-01
+255	val_255	2008-01-01
+104	val_104	2008-01-01
+70	val_70	2008-01-01
+163	val_163	2008-01-01
+438	val_438	2008-01-01
+119	val_119	2008-01-01
+414	val_414	2008-01-01
+200	val_200	2008-01-01
+491	val_491	2008-01-01
+237	val_237	2008-01-01
+439	val_439	2008-01-01
+360	val_360	2008-01-01
+248	val_248	2008-01-01
+479	val_479	2008-01-01
+305	val_305	2008-01-01
+417	val_417	2008-01-01
+199	val_199	2008-01-01
+444	val_444	2008-01-01
+120	val_120	2008-01-01
+429	val_429	2008-01-01
+169	val_169	2008-01-01
+443	val_443	2008-01-01
+323	val_323	2008-01-01
+325	val_325	2008-01-01
+277	val_277	2008-01-01
+230	val_230	2008-01-01
+478	val_478	2008-01-01
+178	val_178	2008-01-01
+468	val_468	2008-01-01
+310	val_310	2008-01-01
+317	val_317	2008-01-01
+333	val_333	2008-01-01
+493	val_493	2008-01-01
+460	val_460	2008-01-01
+207	val_207	2008-01-01
+249	val_249	2008-01-01
+265	val_265	2008-01-01
+480	val_480	2008-01-01
+83	val_83	2008-01-01
+136	val_136	2008-01-01
+353	val_353	2008-01-01
+172	val_172	2008-01-01
+214	val_214	2008-01-01
+462	val_462	2008-01-01
+233	val_233	2008-01-01
+406	val_406	2008-01-01
+133	val_133	2008-01-01
+175	val_175	2008-01-01
+189	val_189	2008-01-01
+454	val_454	2008-01-01
+375	val_375	2008-01-01
+401	val_401	2008-01-01
+421	val_421	2008-01-01
+407	val_407	2008-01-01
+384	val_384	2008-01-01
+256	val_256	2008-01-01
+26	val_26	2008-01-01
+134	val_134	2008-01-01
+67	val_67	2008-01-01
+384	val_384	2008-01-01
+379	val_379	2008-01-01
+18	val_18	2008-01-01
+462	val_462	2008-01-01
+492	val_492	2008-01-01
+100	val_100	2008-01-01
+298	val_298	2008-01-01
+9	val_9	2008-01-01
+341	val_341	2008-01-01
+498	val_498	2008-01-01
+146	val_146	2008-01-01
+458	val_458	2008-01-01
+362	val_362	2008-01-01
+186	val_186	2008-01-01
+285	val_285	2008-01-01
+348	val_348	2008-01-01
+167	val_167	2008-01-01
+18	val_18	2008-01-01
+273	val_273	2008-01-01
+183	val_183	2008-01-01
+281	val_281	2008-01-01
+344	val_344	2008-01-01
+97	val_97	2008-01-01
+469	val_469	2008-01-01
+315	val_315	2008-01-01
+84	val_84	2008-01-01
+28	val_28	2008-01-01
+37	val_37	2008-01-01
+448	val_448	2008-01-01
+152	val_152	2008-01-01
+348	val_348	2008-01-01
+307	val_307	2008-01-01
+194	val_194	2008-01-01
+414	val_414	2008-01-01
+477	val_477	2008-01-01
+222	val_222	2008-01-01
+126	val_126	2008-01-01
+90	val_90	2008-01-01
+169	val_169	2008-01-01
+403	val_403	2008-01-01
+400	val_400	2008-01-01
+200	val_200	2008-01-01
+97	val_97	2008-01-01
